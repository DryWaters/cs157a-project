Mining Top-K Frequent Closed Patterns without Minimum Support Jiawei Han Jianyong Wang Ying Lu Petre Tzvetkov University of Illinois at Urbana-Champaign, Illinois USA hanj, wangj, yinglu tzvetkov}@cs.uiuc.edu ABSTRACT In this paper we propose a new mining task mining top-k frequent closed patterns of length no less than minl where k is the desired number of frequent closed patterns to he mined and minl is the minimal length of each pattern An efficient algorithm called TFP is developed for min ing such patterns without minimum support Two meth ods closdnode-count and descendantsum are proposed to effectively raise support threshold and prune FP-tree both dunng and aftw the construction of FP-tree 
During the mining process a novel top-down and bottom-up combined FP-tree mining strategy is developed to speed-up support raising and closed frequent pattern discovering In addition a fast hash-based closed pattern verification scheme has been employed to check efficiently if a potential closed pattern is really closed Our performance study shows that in most cases TFP outperforms CLOSET and CHARM two efficient frequent closed pattern mining algorithms even when both are run ning with the best tuned minsupport Furthermore the method can be extended to generate association rules and to incorporate user-specified constraints Thus we conclude that for frequent pattern mining, mining topk frequent closed patterns without minsvpport is 
more preferable than the traditional minsupport-based mining 1 INTRODUCTION As me of several essential data mining tasks mining fre quent patterns has been studied extensively in literature From the implementation methodology point of view re cently developed frequent pattern mining algorithms can be categorized into three classes 1 Apriori-based hori zontal formatting method with Apriori I as its represen tative 2 Apriori-based verticai formatting method such as CHARM 181 and 3 projection-based pattern growth method which may explore some compressed data struc ture such as FP-tree as in FP-growth 3 The common framework is to use a minuupport threshold to ensure the generation of 
the correct and complete set of frequent patterns based on the popular Apriori property I every subpattern of a frequent pattern must be firpent also called the downward closurr property Unfortunately this framework though simple leads to the following two problems that may hinder its popular use First Setting mixsupport is quite subtle a too small The work was supported in part by U.S National Science  Foundation University of Illinois and Microsoft Research threshold may lead to the genemtion of thousands of pat terns whewos a too big one may often genemte no an 
swers Our awn experience at mining shopping transaction databases tells us this is by no means an easy task Second frequent pattern mining often leads to the gen eration of a large number of patterns and an even larger number of mined rules And mining a long pattern may unavoidably generate an exponential number of subpatterns due to the downward closure property of the mining process The second problem has been noted and examined by re searchers recently proposing to mine \(frequent closed pat terns 15 7 81 instead Since a closed pattern is the pattern that covers all of its subpatterns with the same support one just need to mine the set of closed 
patterns often much smaller than the whole set of frequent patterns without los ing information Therefore, mining closed patterns should be the default task for mining frequent patterns The above observations indicate that it is often preferable to change the task of mining frequent patterns to mining top k frequent closed patterns of minimum length minl where k is a user-desired number of frequent closed patterns to he mined which is easy to specify or set default top-k refers to the k most frequent closed patterns and mid the min imal length of closed patterns is another parameter easy to set Notice that without min the 
patterns found will he of Iength one or their corresponding closed superpatterns since a pattern can never occur more frequently than its cor responding shorter one i.e subpatterns in any database In this paper we study the problem of mining topk fre quent closed patterns of minimal length minl efficiently without minsupport i.e., starting with minsupport  0 Our study is focused on the FP-treebased algorithm An efficient algorithm called TFP is developed by taking ad vantage of a few interesting properties of topk frequent closed patterns with minimum length mid including 1 any transactions shorter than mid will not be included in the pattern mining 
2 minsupport can be raised dynami cally in the FP-tree construction which will help pruning the tree before mining and 3 the most promising tree branches can he mined first to raise minsupport further and the raised rninsupport is then used to effectively prune the remaining branches Performance study shows that TFP has SWprisingly high performance In most cases it is better than two efficient frequent closed pattern mining algorithms CLOSET and CHARM with the best tuned minsupport Moreover association rules can be extracted by minor ex tension of the method, and constraints can o be incorpe 0-7695-1754-4/02 17.00 0 2002 IEEE 211 


rated into topk closed pattern mining Therefore we conclude that mining tOpk frequent closed patterns without minimum support is more preferable from both usability and efficiency points of view than traditi0n.d minsupport-based mining The remaining of the paper is organized as follows In Section 2 the basic concept of topk closed pattern mining is introduced and the problem is analyzed with the related properties identified Section 3 presents the algorithm for mining topk closed patterns A performance study of the algorithm is reported in Section 4 Extensions of the method are discussed in Section 5 and we conclude our study in Section 6 2 PROBLEM DEFINITION In this section we first introduce the basic concepts of top k closed patterns then analyze the problems and present an interesting method for mining topk closed pattern Let I  i,,iz   i be a set of items An itemset X is a non-empty subset of I The length of itemset X is the number of items contained in X and X is called an l-itemset if its length is 1 A tuple lid X is called a transaction where tid is a transaction identifier and X is an itemset A transaction database TDB is a set of transactions An itemset X is contained in transaction tid,Y if X 2 Y Given a transaction database TDB the support of an itemset X denoted as sup\(X is the number of transactions in TDB which contain X DEFINITION 1 topk closed itemset An itemset X is a closed itemset if there ezists no itemset X such that 1 c X and 2 V tmnsaction T X E T  X E T A closed itemset X 1s I top-k closed itemset of minimal length mind iJ them enst no more than k  1 closed itemsets oJ length at least minS whose support is higher than that of X  Our task is to mine topk closed itemsets of minimal length minl efficiently in a large transaction database EXAMPLE 1 A transaction dataset example Let Table 1 be our transaction database TDB Suppose our task is to find top-4 Jreguent closed paltem with mind  4 m Items I Ordered Items I Table 1 A transaction database TDB FP-growth Without minsupport threshold one can still use Apriori to mine all the I-itemsets level-by-level for I from 1 to minl However since one cannot use the downward closure property to prune infmpent Citemsets for genera tion of I  l candidates Apriori bas to join all the length 1 itemsets to generate length I  1 candidates for all 1 from 1 to minl 1 This is inefficient CHARM loses its pruning power as well since it has to generate transaction idJist for every item and find their intersected transaction idlist for every pair of such items since there is no itemset that can be pruned Will the fate he the same for FP-growth Since FP-growth uses a compressed data structure FP-tree to register TDB all the possible itemsets of a transaction and their corresponding length information are preserved in the corresponding branch of the FP-tree Moreover, FP-tree pre serves the support information of the itemsets as well Thus it is possible to utilize such information to speed up mining Of the three possible methods we will examine FP-growth in detail The question then becomes how can we eztend FP-growth for eficient top-k fmpent closed pattern mining We have the following ideas 1 O-mindupport forces us to construct the full FP-tree however with topk in mind one can capture sufficient higher support closed nodes in tree con struction and dynamically raise minaupport to prune the tree and 2 in FP-tree mining one can first mine the most promising subtrees so that high support patterns can be de rived earlier which can be used to prune low-support sub trees In the following section we will develop the method step-by-step 3 MINING TOP-K FREQUENT CLOSED PAT TERNS METHOD DEVELOPMENT In this section we perform stepby-step analysis to de velop an efficient method for mining topk frequent closed patterns 3.1 Short transactions and 1-counts Before studying FP-tree construction we notice REMARK 3.1 Short transactions If a tmnsaction T contains less than mind dlstinct items none of the item in T con contribute to a pattern of minimum length minl For the discussions below we will consider only the trans actions that satisfy the minimum length requirement   Our first question is which mining methodologg should be chosen fmm among the thme choices Apriori CHARM and Since there could be more than one itemset having the same support in a transaction database to ensure the set mined is independent of the ordering of the items and transactions our method will mine every closed itemset whose support is no less than the k-th frequent closed itemset b Count and a The FP-tree constructed from TDB 1-count of items Figure 1 FP-tree and its header table Let the occurrence frequency of each item be stored as Here we introduce in count in the global header table 212 


the header table another counter l\(ow\which records the total occurrences of an item at the level no higher than minl in the FP-tree as shown in Figure 1 b REMARK 3.2 1-count If the 1-count of an item t is lower than minsupport t connot be used to genemte Jw Fen1 itemet of length no less than mind Rationale Based on the rules forgenemtion offrepent item set in FP-tree Si only a node residing at the level mind or lower i.e deeper in the tree moy genemte a pmfiz path no shorter than minl Based on Remark 3.1 short prefLzpoths will not contebute to the genemtion of itemset with length greoter or equal to minl Thus only the nodes with I-count no lower than minsupport may generate lkrpuent itemet of length no less than minl  People may wonder that ou assumption is to start with minsupport  0 how could we still use the notion of minsupport Notice that if we can find a goad number i.e no less than k of clased nodes with nontrivial sup port during the FP-tree construction or before tree mining the minsupport can be raised which can be used to prune other items with low support 3.2 Raising minsupport for pruning FP-tree Since our goal is to mine topk frequent closed nodes in order to raise minsupport effectively one must ensure that the nodes taken into count are closed LEMMA 3.1 Closed node At any time during the construction of an FP-tree D node n is 1 closed node rep resenting D closed itemset if it falk into one ofthe following three mes 1 n has more than one child and n caries more count than the sum of its children 2 nt comes more count than ib child and 3 nl is a leaf node Rationale This can be easily derived fmm the definition of closed itemet and the rules fo construction of FP-tree 9 As shown in Figure 2 a D node nt  1 denotes an itemet nl    nt with support 1 Any later tmmaction or prefiz-path that contaim ezoetly the same set of item will be represented by the same node in the tree with inerrased support If nt has more than one child and nt caries more count than the sum of its children then n cannot carry the same support as any of its children and thus nt must be 1 closed node The same reason holds ifnt caries more count than its child If nt is 1 leaf node the future insertion of brnnches will make the node either remain as D leaf node OT cam more count than its children thw nt must be a closed node as well  To raise minsupport dynamically during the FP-tree con struction a simple data structure called closednodemount array can be used to register the elurent count of each clased I-node with support node as illustrated in the left The array is constructed as follows Initially all the count of eacb node is initialized to 0 or only the non-zero 1 node is registered, depending on the implementation Each clmed I-node with support m in the FP-tree has one count in the count slot of node m During the construction of an FP-tree suppose inserting one transaction into a branch makes the support of a closed I-node P increases from m to I-node I count U 17 32 86  P b Count of closed I-node with support node a Judging the closed node Figure 2 Closed node and closed node count array m  1 Then the count corresponding to the node m  1 in the array is increased by one whereas that corresponding to the node m is decreased by one Based on how the closed-oderount array is constructed one can easily derive the following lemma LEMMA 3.2 Raise minsupport using closed.nnode At any time during the constmetion of an FP-tree the mini mum support for mining top-k closed itemsets will be at least equal to the node if the sum ofthe closedxnoderount amy fmm the top to node is no less than k  Besides using the closednodelount array to raise mini mum support there is another method to raise minsupport with FP-tree called anchor-node descendant-sum or simply descendant-sum as described below An anchor-node is a node at level mind 1 of an FP-tree It is called an anchor node since it serves as an anchor to the descendant nodes at level mind and below The method is described in the following example iz h18 J  Figure 3 node of an FP-tree Calculate descendantsum for an anchor EXAMPLE 2 As shown in Fipre 3 node b is on nnchor node since it resides at level mid  1  2 At thzJ node we collect the sum of the counts for each distinct iternset of b's descendants For emmple since b has two descendant d nodes d  25 and d  43 b's descendantsum ford is d  68 which means that the support ofitemet abd contnbuted from 6's descendants is 68 From the FP-tree presented in Figure 3 it is easy lo figurn out that b's descendantsum should be c 75 d 68 e  43 h  38 f  2O g  20 Such summaw infomation may mise minsupport 213 


effectiectively For ezample minsupport for top-3 closed nodes should be at least 43 bosed on b's descendantsum  descendantsum Each distinct count in descendant-sum of an anchor node represents the minimum count of one dlstinct closed pattern that can be generated by the FP-tree Rationale Let the path from the root of the FP-tree to on anchor node b be 0 Let D descendant ofb bed and its count be countd Then based on the method for FP-tree construc tion thew must ezist an itemet 0-d whose count is ermntd IfP d zs a closed node then it is the wipe closed node in the FP-tree with count countd othemise them must enst a closed pattern which is its super-pottern dth support countd Since another node in b's descendantsum with the same svpport countd may show such D closed node with 0 d and atso another bmnch may contribute addition01 count to such a closed node thus only D distinct count in descendantsum of b may mpmsent the minimum count of a distinct closed pattern genemted by the FP-tree  We have the following observations regarding the two sup port raising methods First the closednodeaunt method is cheap only one array and is easy to implement, and it can he performed at any time during the tree insertion proces Second, comparing with closednodeaunt descendantsum is more effective at raising minsuppet hut is more costly since there could he many mid  1 level nodes in an FP-tree and each such nude will ueed a deseendontsum structure Moreover before fully scanning the database one does not know which node may eventually have a very high count Thus it is tricky to select the appropriate an chor nodes for support raising too many anchor nodes may waste storage space whereas too few nodes may not be able to register enough count information to raise minsupport effectively Computing descendantsum structure for low count nodes could he a waste since it usually derives small descendantsum and may not raise minsupport effectively Based on the above analysis, our implementation explores both techniques but at difierent times During the FP-tree construction it keeps an closednnodexount array which raises minsupport dynamically prunes some infrequent nodes and reduces the size of the FP-tree to he constructed Af ter scanning the database i.e the FP-tree is constructed we traverse the suhtree of the level mid  1 node with the highest support to calculate descendantsum This will effectively raise minsupport If the so-raised minsupport is still less than the highest support of the remaining level minl 1 nodes the remaining node with the highest sup port will be traversed, and this process continues Based on ow experiments only a small number of nodes need to he 50 traversed if k for topk is less than 1000 in most cases LEMMA 3.3 3.3 Efficientminingof FP-treefor top-k patterns The raise of min-support prunes the FP-tree and speeds up the mining However the overall critical performance gain comes from efficient FP-tree mining We have the following observations REMARK 3.3 Item skipping Ifthe count ofan item in the global header table is less than minsupport then it Is infrequent and its nodes should be mmoved from the FP-tree Woxover if the 1-count of an item in the global heoder ta ble ts less than minsupport the item should not be used to genemte any conditional FP-tree  The TFP-mining with FP-tree is similar to FP-growth However there are a few subtle points 1 Top-down" ordering of the items in theglobal header table for the generation of conditional FP-trees The first subtlety is in what order the conditional FP-trees should be generated for topk mining Notice since FP-growth in 3j is to find the complete set of frequent patterns its min ing may start from any item in the header table For topk mining our goal is to find only the patterns wtth high sup port and mise the minsupport as fast as possible to ovoid unnecessary work Thus mining should start from the item that has the first non-zero 1-count which usually carries the highest 1-count in the header table, and walk down the header table entries to mine subsequent items i.e in the sorteditemlist order This ordering is based on that item with higher 1-count usually produce patterns with higher support With this ordering minsupport can he raised faster and the topk patterns can he discovered earlier In addition an item with 1-count less than mindupport do not have to generate conditional FP-tree for further mining as stated in Remark 3.2 Thus the faster the minsupport can he raised the moIe and earlier pruning can he done 2 Bottom-up ordering of the items in a local header table for mining conditional FP-trees The second subtlety is how to mine conditional FP-trees We have shown that the generation of conditional FP-trees should follow the order of the sorteditemlist which can be viewed as topdown walking through the header table However it is often more beneficial to mine a conditional pattern tree in the bottom-up manner in the sense that we first mine the item that are located at the low end of a tree branch since it tends to produce the longest patterns first then followed hy shorter ones It is more efficient to first generate long closed patterns since the patterns containing only the subset items can be absorbed by them easily 3 Efficient searching and maintaining of closed patterns using a pattern-tree structure The third subtle point is how to efficiently maintain the set of current frequent closed patterns and check whether a new pattern is a closed one During the mining process a pattern-tree i4 used to keep the set of current frequent closed patterns The structure of pattern-tree is similar to that of FP-tree Recall that the items in a branch of the FP-tree are ordered in the support decreasing order This ordering is crucial for closed pat tern verification to be discussed below thus we retain this item ordering in the patterns mined The major difference between FP-tree and pattern-tree is that the former stores transactions in compressed form whereas the latter stores potential closed frequent patterns The bottom-up mining of the conditional PP-trees gener ates patterns in such an order for patterns that share pre fixes longer patterns are generated first In addition, there is a total ordering over the patterns generated This leads to our closed frequent pattern verification scheme presented as follows Let i   it   ij   in he the sortehilemlist where ii is the first non-zero I-count item and ij he the item whose 214 


conditional FP-tree is currently being mined Then the set of already mined closed patterns S can be split into two subsets 1 SDz.i obtained by mining the conditional trees corresponding to items from il to ij i.e none of the item sets contains item if and 2 s obtained so far by min ing ij's conditional tree i.e every itemset contains item i Upon finding a new pattern p during the mining of ij's conditional tree we need to perform new pattern checking checking against Sj and old pattern checking checking The new pattern checking is performed as fallows Since the mining of the conditional tree is in a bottom-up man ner just like CLOSET we need to check whether 1 p is a subpattern of another pattern pi in S  and 2 supp\(p supp\(p If the answer is no i.e p passes new pattern checking p becomes a new closed pattern with respect to S Note that because patterns in Sold do not contain item ij there is no need to check if p is a subpattern of the patterns in Sold The old pattern checking is performed as follows Since the global FP-tree is mined in a topdown manner pattern p may he a super-pattern of another pattern p,\(d in Sold with supp\(p supp\(p.rd In this case parr cannot be a closed pattern since it is absorbed by p Therefore if p has passed both new and old pattern checking it can he used to raise the support threshold Otherwise if p passes only the new pattern checking then it is inserted into the pattern tree, but it cannot be used to raise the support threshold The correctness of the above checking is shown in the following lemmas LEMMA 3.4 against Sold New pattern checking If a pattern p cannot pass the new pattern checking there must edt n pattern p  in S  which must o contain item i with Rationale This con be obtained directlyfmm the newpattern checking method  Let prefis\(p be the prefix pattern of B pattern p i.e obtained by removing the last item ij from p Old pattern checking For oldpattern checking we only need to check if there ezists a pottern prefiz\(p in Sold with supp\(prefis\(p z supp\(p Rationale Since D pattern in Sol does not contoin item i it cannot become D super-pottern ofp Thus we only need to check if it is a subpattern of p In fact we only need to check if them is I potternprefis\(p in Sou with the Same support as p We can pmve this by eontmdiction Let us assume there ls another subpattern ofprefix\(p that an be absorbed by p If thls is the ease omording to ow mining order we know this subpattern must have been absorbed by prefiz\(p either via new pattern checking or old pattern checking 8 Support raise If a newly mined pat tern p can pass both new pattern checking and old pottern checking then it is safe to we p to mise minsuppat Rationale From Lemma 3.5 there will be two possibilities for p First it is D wal closed pattern i.e it will not be absorbed by any patterns later Second it will be absorbed by a later found pattern and this pattern can only absorb pattern p In this case we will not use the later found pot tern to mise support because it has already been wed to mise SUPP\(P 1  SUPP\(P LEMMA 3.5 LEMMA 3.6 support when we found pottern p OT p's precedents Thus it is safe to we p to mise minsuppart  To accelerate both new and old pattern checking we in troduce a tw-level index header table into the pattern-tree structure Notice that if a pattern can absorb or be ab sorbed by another pattern the two patterns must have same support Thus our first index is based on the sup port of a pattern In addition for new pattern checking we only need to check if pattern p can be absorbed by another pattern that also contains ij and for old pattern checking we need to check if p can absorb prefis\(p that ends with the second-last item of p To speed up the checking our second level indexing use5 the last item.ID in a closed pat tern as the index key At each pattern-tree node we also record the length of the pattern in order to judge if the corresponding pattern needs to be checked The twelevel index header table and the checking process are shown in the following example EXAMPLE 3 Closed pattern verification Figure 4 shows a two-level indezing structure for venfiiention of closed patterns Based on the lemmas we only need to indez into U Figure 4 Twc-level indexing for veriRcatlon of closed patterns the fimt structure based on the itemset support and based on its matching ofthe last two items in the indez structurr to find whether the eomspnding closed node is in the tree 8 3.4 Algorithm Now we summarize the entire mining process and present ALGORITHM 1 Mining top-k jwepuent closed itemsets with Input 1 A tmnsoction database DB 2 an integer k i.e the k most freguent closed itemsets to be mined and 3 mind the minimal length of the frequent closed itemsets Output The set offreguent closed items which satisfy the wquirement Method the mining algorithm minimal length mind in a large tmwaetion database 1 Initially minsupport  0 2 Scan DB once Collect the mcumncefreepueney count  This scan can be replaced by a sampling process, which re duces one database scan but increases the chance that items may not be ordered very well due to biased sampling, which may hurt performance later Thus such scan reduction may or may not improve the performance depending on the data characteristics and the ordering of transactions 215 


of every item in tmnsactions and sort them in fre quency descending order which fonns sorteditedist and the header of FP-tree 3 Scan DB again, construct FP-tree updote the I-count in the header of the FP-tree use closed node count amy to mise minsuppol-t and use this support to prune the tree After scanning DB tmuerse FP-tree with descendantsum check which mises minsuppmt further and the mised minsuppol-t is used to prune the tree 4 e mining is perfomzed by tmuersing down the header table, storting with the item with the fimt non-zero 1 count and genemte the conditional FP-tree fov each item M along as its 1-count is no less than the cumnt minsupport Each conditional FP-tree is mined in 223bottom-up\224 i.e long to short manner Each mined closed pattern is inserted into a pottern-tree 5 Output pattern fmm pattern-tree in the order of their support Stop when it outputs le patterns  4 EXPERIMENTAL EVALUATION In this section we report our performance study of TFP In particular we compare the efficiency of TFP with CHARM over a variety of datasets and CLOSET two well known algorithms for mining frequent closed itemsets To give the best possihle credit to CHARM and CLOSET our comparison is always based on assigning the best tuned minsuppmt which is difficult to obtain in practice to the two algorithms so that they can generate the same topk closed patterns for a user-specified k due tu der a condition of mind These optimal minsuppol-t are obtained by running TFP once under each experimental con dition This means that even if TFP has only comparable performance with those algorithms it will still he far more useful than the latter due to its usability and the difficulty to speculate minstqpol-t without mining In addition we also study the scalability of TFP The experiments show that 1 the running time of TFP is shorter than CLOSET and CHARM in most cases when mind is long and is comparable in other cases and 2 TFP has nearly linear scalability 4.1 Datasets Both real and synthetic datasets are used in experiments and they can be grouped into the following two categories 1 Dense datasets that contain many long frequent closed patterns 1 pumsb census data which consists of 49,044 transactions each with an average length of 74 items 2 connect-4 game state information data which consists of 67,557 transactions each with an average length of 43 items and 3 mushmomcharacteristic data which consists of 8,124 transadions having an average length of 23 items All these datasets are obtained from the UC-Irvine Machine Learning Database Repository 2 Sparse datasets 1 gazalle click stream data which consists of 59,601 transactions with an average length of 2.5 items and contains many short length below 10 and some very long closed patterns obtained from BlueMartini Software Inc and 2 TlOI4DlOOKsynthetic data from the IBM dataset generator which consists of 100,000 transac tions with an average length of lO.items, and with many closed frequent patterns having average length of 4 4.2 Performance Results All experiments were performed on a 1.7GHa Pentium-4 PC with 512MB of memory running Windows 2000 The CHARM code was provided to us hy its author The CLOSET is an improved version that uses the same index-based closed node verification scheme as in TFP We compared the performance of TFP with CHARM and CLOSET on the 5 datasets by varying mid and K In most cases K is selected to be either 100 or 500 which covers the range of typical K Iues We also evaluated the scalability of TFP with respect to the size of database For the dense datasets with many long closed patterns TFP performs consistently better than CHARM and CLOSET for longer minl Dense Datasets WWll a K  100 221.dWPI b K  500 Figure 5 Performance on Connect-4 I Figure 5 shows the running time of the three algorithms on the connect-4 dataset for K fixed at 100 and 500 respec tively and minl ranging from 0 to 25 We observe that TFP\222s running time for K at 100 and 500 remains stable over the range of mind When mind reaches 11 or 12 TFP starts to outperform CHARM and the same for CLOSET when minl reaches 17 or 18 The reason is that for long patterns the minsupport is quite low In this case CHARM has to retain many short frequent patterns before forming the required longer patterns and the FP-tree of CLOSET would also contain a large number of items that takes up much mining time On the other hand TFP is able to use the mid length restriction to cut many short frequent pat terns early thus reduce the total running time Figure 6 shows the running time of the three algorithms on the connect-4 dataset with minl set to 10 and 20 respec tively and K ranging from 100 to 2000 For the connect-4 dataset, the average length of the frequent closed patterns is above 10 thus mind at 10 is considered to be a very low length restriction for this dataset From a we can see that even for very low length restriction such as 10 TFP\222s perior mance is comparable to that of CLOSET and CHARM when it runs without giving support threshold For minl equal to 20 the running time for TFP is almost constant over the full range and on average 5 times faster than CHARM and 2 to 3 times faster than CLOSET We also noticed that even for very low mind as K increases the performance gap between TFP CLOSET and CHARM gets smaller 216  


    f      I i    0 0 m m 100 10 am 1m 1110 lam m am m n am a0 la0 IBO Eo Im 1100 am a minl  10 b mind  20 Figure 6 Performance on Connect-4 11 Figure 7 shows the running time of the three algorithms on the mushroom and pumsb datasets with K set to 500 and mind ranges from 0 to 25 For the mushroom dataset when minl is less than 6 all three algorithm have simi lar low running time TFP keeps its low running time for the whole range of mind and starts to outperform CHARM when minl is as low as 6 and starts to outperform CLOSET when minl is equal to 8 Pumsh has very similar results as connect-4 and mushroom datasets i 1 m j i s 21 di I 2 U  0 I IO I3 1 1 I IO I I 1l ndWRI YUdWR a Mushrwm b Pumsb Figure 7 Performance on Mushroom and Pumsb Sparse Dataset Experiments show that TFP can effi ciently mine sparse datasets without minsupport It has comparable performance with CHARM and CLOSET for low mid and outperforms both on higher mind Figure 8a shows the running times of TFP CHARM and CLOSET on T1014D100K with K fixed at 100 and minl ranges from 1 to 10 Again it demonstrates TFP's strength in dealing with long minl At minl  8 the performance of CHARM and CLOSET starts deteriorating while TFP re tains its good performance Figure 8b shows the perfor mance on the same dataset but with minl fixed at 8 and varying K from 200 to 2000 The curves show that when K is above 400 the running times of CHARM and CLOSET are around 3 times slower than TFP The experiments on the gazelle dataset are shown in Fig ure 9 For smaller K TFP outperforms both CHARM and CLOSET for minl greater than or equal to 5 For K  500 TFP continues to outperform CLOSET for mind greater than or equal to 5 and has similar performance as CHARM Rom this performance study we conclude that TFP has good overall performance for both dense and sparse datasets Its running time is nearly constant over a wide range of K and mind values for dense data Unlike CHARM and CLOSETwhose performance deteriorates as mind increases b L=8 Figure 8 Performance on T1014D100K a K  inn b K  500 Figure 9 Performance on Gazelle TFP's running time stays low The reason is inherent from the mining strategy of TFP CHARM and CLOSET In mast time the support for long patterns is lower than that of short patterns Thus even with the optimal support given both CLOSET and CHARM are unable to prune short fre quent patterns early thus causing much time spent on min ing useless patterns On the other hand TFP is able to use the minl length restriction to cut many short frequent patterns early thus improves its running time instantly In addition TFP does not include any nodes that reside above minl level to participate in the mining process As mind increases more nodes reside above the minie level of the tree means that less conditional FP-trees need to he built thus keeps the running time low Besides the good performance over long minl values the performance of TFP over short minl values even when mind  1 i.e no length constraint is still comparable to that of CLOSET and CHARM In such cases the run ning times between the three do not differ much and both CLOSET and CHARM were run with the optimal support threshold while TFP was not given any support threshold Scalability Test Our performance tests showed that the running time of TFP increases linearly with increased dataset size 5 DISCUSSION In this section we discuss the related work how to gener ate association rules from the mined topk frequent patterns and how to push constraints into the mining process 5.1 Related work Recent studies have shown that closed patterns are more desirable 5 and efficient methods for mining closed pat 217 


terns such as CLOSET 7 and CHARM B have been de veloped However these methods all require a user-specified support threshold Our algorithm does not need the user to provide any minimum support and in most cases runs fater than two efficient algorithms CHARM and CLOSET which in turn outperform Apriori substantially 7 81 Fu et al Z studied mining N most interesting item sets for every length 1 which is different from our work in several aspects 1 they mine all the patterns instead of only the closed ones  2 they do not have minimum length constraintssince it mines patterns at all the lengths some heuristics developed here cannot be applied and 3 their philosophy and methodology of FP-tree modification are also different from ours To the best of our knowledge this is the first study on mining topk frequent closed patterns with length constraint therefore we only compare our method with the two best known and well-performed closed pattern mining algorithms 5.2 Generation of association rules Although topk frequent itemsets could he all that a user wants in some mining tasks in some other cases sjhe wants to mine strong association rules from the mined topk fre quent itemsets We examine how to do this efficiently Items in the short transactions, though not contributing to the support of a topk itemset of length no less than mind may contribute to the support of the items in it Thus they need to be included in the computation which has minimal influence on the performance To derive cor rect confidence we have the following observations 1 The support of every Liternset is derived at the start of min ing 2 The set of topk closed itemsets may contain the items forming subsetjsuperset relationships and the rules involving such itemsets can be automatically derived 3 For rules in other forms, one needs to use the derived topk itemsets as probes and the known minsupport as threshold and perform probe constrained mining to find the support only related to those itemsets 4 As an alternative to the above one can set mind 2 which will derive the patterns readily for all the combinations of association rules 5.3 Pushing constraints into TFP mining Constraint-based mining 14.61 is essential to topk mining since users may always want to put constraints on the data and rules to be mined We examine how different kinds of constraints can be pushed into the topk frequent closed pattern mining deep into the TFP-mining process he succint constraints should be pushed deep to select only those itemsets hefore mining starts and the anti-monotonic Constraint should be pushed into the iterative TFP-mining process in a similar way as FP-growth Second for monotone constraints the rule will also be similar to that in traditional frequent pattern mining is if an itemset mined so far e.g okd satisfies a constraint sum 2 loo adding more items such as e still satisfies it and thus the constraints checking can be avoided in further expansion Third for convertible constraints one can arrange items in an appropriate order so that the constraint can be trans formed into an anti-monotone one and the anti-monotone constraint pushing can he applied First succinct and anti-monotone constraints can be pushed Interested readers can easily prove such properties for top k frequent closed pattern mining 6 CONCLUSIONS We have studied a practically interesting problem mining top-k frequent closed patterns of length no less than mind and proposed an efficient algorithm TFP with several opti mizations 1 using closednodexcount and descendantsum to raise mindupport before tree mining 2 exploring the topdown and bottom-up combined FP-tree mining to first mine the most promising parts of the tree in order to raise rninsupport and prune the unpromising tree branches and 3 using a special indexing structure and a novel closed pattern verification scheme to perform efficient closed pat tern verification Our experiments and performance study show that TFP has high performance In most cases it out performs two efficient frequent closed pattern mining algo rithms CLOSET and CHARM even when they are running with the best tuned minsuppwt Furthermore the method can be extended to generate association rules and to incor porate user-specified constraints Based on this study we conclude that mining topk fre quent closed patterns without minsupport should be more preferable than the traditional minsuppwt-based mining for frequent pattern nuning More detailed study along this direction is needed including further improvement of the performance and flexibility at mining topk frequent closed patterns as well as mining topk frequent closed sequential patterns or structured patterns Acknowledgements We are grateful to Dr Mohammed Zaki for providing the code and data conversion package of CHARM and promptly answering many questions 7 REFERENCES I R Agrawal and R Srikant Fast algorithm for mining 2 A W.-C. Fu R W.-W Kwong and J Tang Mining 3 J Han J Pei and Y Yin Mining frequent patterns 4 R Ng L V S Lakshmanan J Han and A Pang association rules VLDB'94 n-most interesting itemsets ISMIS'W without candidate generation SIGMOD'OO Exploratory mining and pruning optimizations of constrained esociations rules SIGMOD'SR Discovering frequent closed itemsets for association rules ICDT'99 161 J Pei J Ha and L V S Lakshmanan Mining frequent itemsets with convertible constraints ICDE'O1 7 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets DMKD'OO 8 M J Zaki and C J Hsiao CHARM An efficient algorithm for closed itemset mining SDM'O2 5 N Pasquier Y Bastide R Taouil and L Lakhal 218 


I Plenary Panel Session J Future Directions in Database Research  456 Chair Surajit Chaudhuri Microsoft Corporation Panelists Hector Garcia-Molina Stanford University Hank Korth, Bell Laboratories Guy Lohman IBM Almaden Research Center David Lomet Microsoft Research David Maier Oregon Graduate Institute I Session 14 Query Processing in Spatial Databases I Chair Sharma Chakravarthy University of Florida Processing Incremental Multidimensional Range Queries in a Direct Manipulation Visual Query Environment  458 High Dimensional Similarity Joins Algorithms and Performance Evaluation  466 S Hibino and E Rundensteiner N Koudas and K.C Sevcik Y Theodoridis E Stefanakis and T Sellis Cost Models for Join Queries in Spatial Databases  476 Mining Association Rules Anti-Skew Algorithms  486 J.-L Lin and M.H Dunham Mining for Strong Negative Associations in a Large Database of Customer Transactions  494 A Savasere E Omiecinski and S Navathe Mining Optimized Association Rules with Categorical and Numeric Attributes  503 R Rastogi and K Shim Chair: Anoop Singhal AT&T Laboratories S Venkataraman J.F Naughton and M Livny Remote Load-Sensitive Caching for Multi-Server Database Systems  514 DB-MAN A Distributed Database System Based on Database Migration in ATM Networks  522 T Hara K Harumoto M Tsukamoto and S Nishio S Banerjee and P.K Chrysanthis Network Latency Optimizations in Distributed Database Systems  532 I Session 17 Visualization of Multimedia Data I Chair Tiziana Catarci, Universita di Roma 223La Sapienza\224 W Chang D Murthy A Zhang and T.F Syeda-Mahmood Global Integration of Visual Databases  542 X 


The Alps at Your Fingertips Virtual Reality and Geoinformation Systeps  550 R Pajarola l Ohler P Stucki K Szabo and P Widmayer C Baral G. Gonzalez and T.C Son Design and Implementation of Display Specifications for Multimedia Answers  558 1 Session 18 Management of Objects I Chair: Arbee Chen National Tsing Hua University P Boncz A.N Wilschut, and M.L. Kersten C Zou B Salzberg, and R Ladin 0 Wolfson S Chamberlain S Dao L Jiang, and G. Mendei Flattening an Object Algebra to Provide Performance  568 Back to the Future Dynamic Hierarchical Clustering  578 Cost and Imprecision in Modeling the Position of Moving Objects  588 ROL A Prototype for Deductive and Object-Oriented Databases  598 A Graphical Editor for the Conceptual Design of Business Rules  599 The Active HYpermedia Delivery System AHYDS using the M Liu W Yu M Guo and R Shan P Lang W Obermair W Kraus and T Thalhammer PHASME Application-Oriented DBMS  600 F Andres and K. Ono S Chakravarthy and R Le S Mudumbai K Shah A Sheth K Parasuraman and C Bertram ECA Rule Support for Distributed Heterogeneous Environments  601 ZEBRA Image Access System  602 Author Index  603 xi 


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


