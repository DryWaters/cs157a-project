Synthesis and decoding of emotionally expressive music performance Roberto Bresin, Anders Friberg Department of Speech, Music and Hearing  Royal Institute of Technology Drottning Kristinas vag 31,10044 Stockholm, Sweden roberto, andersf ABSTRACT A recently developed application of Director Musices DM is presented The DM is a rule-based software tool for automatic music performance developed at the Speech Music and Hearing Dept at the Royal Institute of Technology Stockholm It is written in Common Lisp and is available both for Windows and Macintosh It will be demonstrated that particular combinations of rules defined in the DM can be used for synthesizing performances that differ in emotional quality Different performances of two pieces of music were synthesized 
so as to elicit listeners\222 associations to six different emotions fear anger happiness sadness, tenderness and solemnity\Performance rules and their parameters were selected so as to match previous findings about emotional aspects of music performance Variations of the performance variables IO1 Inter-Onset Interval 001 Offset Onset Interval and L Sound Level will be presented for each rule-setup In a forced-choice listening test 20 listeners were asked to classify the performances with respect to emotions The results showed that the listeners with very few exceptions recognized the intended emotions correctly This shows that a proper selection of rules and rule parameters in DM can indeed produce a wide variety of meaningful emotional performances even extending the scope of the original rule definition INTRODUCTION In recent years an increased effort has been spent in 
the study of verbal as well as non-verbal communication of emotion In the music field in particular Gabrielsson Juslin and others at the Department of Psychology of Uppsala University showed that there are specific parameters in the microstructure of a perform ance that are manipulated by the performers when they are asked to play the same simple piece of music with different prescribed emotions  13 This research has concerned so-called basic emotions anger sadness happiness and fear and also solemnity and tenderness It has been shown that all these emotions as conveyed by players could be clearly recognized by an audience of musically trained and untrained listeners 2 3 Other research has shown that it is possible to communicate also more complex emotional states even though 
it is not completely clear how to define them in term of adjectives performers and listeners often use different terms in describing intentions and perceived emotions 4]-[7 The most recent step in this research trend has been to synthesize performances encoding different emotions by setting the values of certain expressive cues on a commercial sequencer 8 or by using original software 9 Also under these conditions listeners could recognize and identify the intended emotions The aim of the present preliminary investigation was to take a further step in this direction complete automatic performance synthesis was used in an attempt to convey six different emotions SYNTHESIS Gabrielsson l lo and Juslin 2 8 proposed a list of possible expressive cues that seemed characteristic for each of the emotions fear anger happiness sadness solemnity and tenderness These cues are described 
in terms of qualitative changes of tempo, sound level articulation staccato legato tone onsets and decays timbre deviations of IOI Inter-Onset Interval vibrato, final ritardando As a grand piano sound Kurzweil sound samples of the Pinnacle Turtle Beach soundboard was used in the present experiment the cues tone onset and decay timbre and vibrato could not be used The remaining cues manipulated here are listed in Table 1 The Director Musices DM program was utilized for synthesizing the Performances This is an expert system for automatic music performance written in Lisp language and containing about twenty rules These rules attempt to model a performer\222s rendering of for example phrasing intonation or rhythmic patterns 
The rules can affect most of the performance parameters mentioned above It is an implementation of the 223KTH performance rule system\224 l 11 In order to synthesize performances associated with each of the emotions the qualitative description of each expressive cue was interpreted by the authors into a quantitative rule description For each intended emotion a macro rule was written each activating a special subset of previously defined rules In Table 1 the cue profiles for each emotion as outlined by Gabrielsson and Juslin are compared with the rule setup utilized for the synthesis No more than five rules were used Duration contrast articulation rule 12 Duration contrast rule  1 13 13 Punctuation rule 14 Phrase arch 
rule  151 Final ritardando rule  161 The setting of rule parameters was refined with the help of Lars FrydCn expert musician and principal advisor in designing the rules in DM Two clearly different pieces of music were used One was the melody line of a Swedish nursery tune Ekonn satt i granen henceforth Ekorrn 223The squirrel sat on the fir-tree\223 composed by Alice TegnCr written in major tonality Figure 1 The other was a computer generated piece \(henceforth Mazurka written in minor tonality in an attempt to portray the musical style of FrCderic Chopin 17 The phrase structure with regard to the levels of sub-phrase \(level 6 phrase level 5 and piece \(level 4 was marked in each score \(in Figure 
1 the marking for Ekorrn is shown This was needed as an input to the Phrase arch rule Each of the two scores was performed in seven different versions by Director Musices The rule setup shown in Table 1 was applied for fear angry happy sad solemn and tender versions while no rules were used for synthesizing a dead-pan version henceforth referred to as 223no-expression\224 Each macro rule, starting from the no-expression case, produced deviations in the performance parameters The original tempo was 187 quarter notes per minute for Ekorrn and 96 quarter notes per minute for Mazurka Note that the same rule setup was used for both Ekorrn and Mazurka 0-7803-5731-0/99/$10.00 01999 IEEE IV-317 


Table 1 Cue profiles for each emotion as outlined by Gabrielsson and Juslin are compared with the rule set-up utilized for the synthesis with Director Musices Expressive Cue Tempo Sound level Emotion Fear Gabrielsson and Juslin Director Musices Irregular Low Tone 101 is lengthened by 80 Sound level is decreased bv 6 dB Anger Articulation Time deviations Happiness Mostly staccato or non legato Large  Structural reorganizations Final acceleration sometimes Sadness Tempo Sound level   Solemnity Very rapid Loud Tone IOI is shortened by 15 Sound level is increased by 8 dB Tenderness Time deviations m Moderate Structural reorganizations Increased contrast between long and Tempo Sound level Duration contrast articulation rule k  2 Duration contrast rule k  4 Punctuation rule k  2  Phrase arch rule applied on phrase level \(level  5 k  1.5 turn Phrase arch rule applied on sub-phrase level \(level  6 k  1.5, turn Final ritardando k  1 O q  3 position  0.2 next  1.3 amp  4.0 position  0.2 amp  4.0 last  0.2 short notes Fast Moderate or loud position  0.3 amp 4 last  1 Tone IO1 is shortened by 20 Sound level is increased bv 3 dB Articulation Time deviations Tempo Sound level Articulation Time deviations Final ritardando TemDo Articulation I Mostlv non-legato I Duration contrast articulation rule k  1 Airy Moderate Duration contrast articulation rule k  2.5 9 Duration contrast rule k  2 Punctuation rule k  2 9 Final ritardando rule k  0.3 q  4 Tone IOI is lengthened by 30 Sound level is decreased by 6 dB Duration contrast rule k  2 Phrase arch rule applied on phrase level \(level  5 k  1.5 turn Phrase arch rule applied on sub-phrase level \(level  6 k  1.5 turn Obtained from the Phrase rule with the next parameter Tone IO1 is lenethened bv 30 Slow Moderate or loud Legato Moderate position  0.3, next  1.3 amp  2 position  2 amp  4 last  0.2 Yes Slow or moderate Sound level Articulation Time deviations  Duration contrast rule k  2 amp  0 Punctuation rule k  2  Phrase arch rule applied on phrase level \(level  5 k  0.7 turn Phrase arch rule applied on sub-phrase level \(level  6 k  0.7 turn position  0.5 next  1.3 amp  4 Moderate or loud Mostly legato Relativelv small Sound level is increased by 3 dB High loud rule k  1.5 Duration contrast articulation rule k  1 Punctuation rule k  1 Sound level Articulation Time deviations I I High loud rule k  1.5  Mostly low Legato Diminished contrast Sound level is decreased by 6 dB Duration contrast rule k  4 amp  0 Finalritardando I Yes Tempo 1 Slow I Tone 101 is lengthened by 30 I Final ritardando rule k  0.3 q  2 between long and short I notes Finalritardando I Yes 1 Final ritardando rule k  0.5 q  1.5 As an example Figure 2 shows the deviations for the 223irregular tempo\224 Larger time deviations are associated with the synthesized 223fear\224 version of Ekorrn The relative time deviation shorter notes The graph also shows that a strong ritardando for each note\222s IO1 varies between 20 and loo of the appears in the end The Duration contrast articulation rule nominal values as seen in the top graph in Figure 2 Most of the introduced quite large articulation pauses after all comparatively deviations are positive indicating a slower tempo than in a short notes thus rendering a 223mostly staccato neutral performance Note that the per cent IO1 deviation curve lo The Punctuation rule inserts larger articulation pauses at is not a straight line but rather contains quite great and quick automatically detected structural boundaries All this oscillations thus reflecting what Gabrielsson described as IV-318 


  Figure 1 The Ekorm melody including phrase S sub-phrase 6\and piece \(4\markings as used by the phrase arch rules corresponds to the positive deviations in the off-time duration curve of Figure 2 The sound level curve of the afraid version of Ekorrn Figure 2 bottom graph shows negative deviations This reflects the 223lower sound level\224 referred to by Gabrielsson An unusual setting of the Phrase arch rule produced a sequence of decrescendo-crescendo patterns with louder notes at phrase and sub-phrase boundaries This was an attempt to realize the 223structural reorganization\224 proposed by Gabrielsson In figure 5 the average IO1 deviations are plotted versus the average sound level together with their standard deviations for all six performances of each piece Negative values of sound level deviation indicate a softer performance than the no expression one 223Anger\224 and 223happiness\224 performances are thus played quicker and louder while 223tenderness\224 223fear\224 and 223sadness\224 performances are slower and softer relative to a no expression rendering Figure 6 shows relative mean and standard deviation for the deviations of IO1 for all six performances of each piece Negative values of IO1 deviation imply a tempo faster than the original and vice versa The 223fear\224 and 223sadness\224 versions have larger standard deviations obtained mainly by exaggerating the duration contrast and also by applying the phrasing rules DECODING TEST According to Juslin forced-choice judgments and free labeling judgments give similar results in listeners\222 decoding of a performer\222s intended emotional expression 3 Therefore it was considered sufficient to make a forced-choice listening test to assess the efficiency of the emotional communication Fourteen performances 7 emotions x 2 examples originally stored in MIDI files, were recorded as standard sound files and presented to a panel of listeners Twenty listeners 24-50 years old 5 female and 15 male volunteered as subjects None of them was musicians or music students Eighteen of the subjects played or used to play an instrument on a non-regular basis The subjects were all working at the Speech Music Hearing Department at the Royal Institute of Technology, Stockholm The subjects listened to the examples individually Each subject was instructed to identify the emotional expression of each example as one out of seven alternative substantives fear, anger happiness sadness solemnity tenderness no-expression The responses were automatically recorded by means of the Visor software system, specially designed for listening tests  181 Visor presents the sound files in random order as anonymous boxes on the screen The listeners were given instructions directly on the computer screen The subjects listened to the stimuli over headphones Sennheiser HD435 Manhattan and the output level of the soundboard was set to the maximum level possible Each session contained four sub-tests, each presenting the seven performances of 1 Ekorrn 2 Mazurka 3 Ekorrn and 4 Mazurka The order of the performances within each sub-test was automatically randomized for each individual subject by Visor The average duration of an experiment session was approximately 11 minutes Figure 2 Deviations from no-expression case for the 223fear\224 version of Ekorm for per cent IO1 top off-time in milliseconds \(middle\and sound level in decibel bottom RESULTS Four subjects who gave same answers for repeated stimuli in less than 36 of the cases, were eliminated from the subsequent analysis Figure 3 shows the percentage of 223correct\224 responses for the two pieces In all cases but one these percentages well exceeded the 14 chance level The 223tenderness\224 version of Mazurka was the most difficult case to identify 13 For 223happiness\224 the responses differed substantially between Ekorrn 97 and Mazurka 69 The same applied to 223no expression\224 Ekorm 8 1  and Mazurka 66%\This could partly be due to the fact that Mazurka was in a minor tonality which in the western music tradition is often associated with the moods sadness and anger Fear anger happiness and sadness received an average percentage of 69 for Ekorrn and 59% for Mazurka To facilitate a more detailed analysis, the subjects\222 responses for each version of Ekorrn and Mazurka are presented in Tables 2 and 3 For Ekorm the subjects mostly chose the 223correct\224 alternative although for the 223fear\224 version the 223tender\224 alternative was chosen by 34 of the subjects and for the 223tenderness\224 version the 223sadness\224 alternative was selected by 28 This may be due to the unavailability of vibrato in piano synthesis according to Gabrielsson and Juslin vibrato is an important expressive cue in the synthesis of 223fear\224 and 223tenderness\224 performances. Furthermore according to Juslin in press\the 223tenderness\222 version could be performed with a higher sound level than the 223sadness\224 version while the same sound level for both versions was used in this experiment For Mazurka the 223fear\224 version elicited less confusion it was classified as 223tenderness\224 by only 21 of the subjects On the other hand, many subjects classified the 223tenderness\224 version as 223sadness\224 41 or as 223no expression\224 31 and only 13 selected the 223tenderness\224 alternative The reason for this confusion would be the same as for Ekorm lack of vibrato and of differentiation of sound level All other versions of Mazurka were mostly classified according to the intended emotion IV-319 


Figure 3 Effect of piece: percentage of 223right\224 classification The synthesis of each emotional performance was achieved by using a special set-up including a subset of DM rules see Table 1 A total of 17 parameters were involved An attempt was made to reduce the number of dimensions of this space by means of a principal component analysis Two principal factors emerged explaining 61 Factor 1 and 29 Factor 2 of the total variance Figure 4 presents the main results in terms of the distribution of the different setups in the two-dimensional space Factor 1 was closely related to deviations of sound pressure level and tempo louder and quicker performances had coordinates between Anger and Solemnity while softer and slower performances had coordinates between Sadness and Fear Factor 2 was closely related to the articulation and phrasing variables The distribution of setups resembles those presented in previous works and obtained with other methods 6 7 9 The k-values for the Duration Contrast rule in the setups also shown in Figure 4 were highest in the first quadrant lowest in the second and intermediate in the third and fourth The figure also shows an attempt to a qualitative interpretation of variation of this rule in the space DISCUSSION The main result from this exploratory experiment is that the emotions associated with the DM macro rules where classified correctly by the listeners in most cases This suggests that it is possible to group DM performance rules into macro rules producing performances that can be associated with different emotional states An important observation is that the same macro rules could be successfully applied both to Ekorrn and Mazurka two completely different compositions This is not surprising since the used performance rules previously have been modeled so as to work correctly in different musical contexts On the other hand it may be more advantageous to use somewhat different versions of the macro rules for scores of differing characters for example different quantities of staccato and phrase marking may be preferable depending on music style Using the DM system is not the only possible way for evoking associations with emotions many other recipes certainly exist However the findings in the present work also indicate interesting new potentials for the DM system One of its limitations has been that it produced only one performance of each piece due to its deterministic structure The results of the present study indicate that by complementing the DM with macro rules performances can be obtained that significantly differ in emotional quality The range of fiiture applications is of course unpredictable One possibility would be to design an 223emotional tool-box\224 where users can chose different ways of playing the same pieces of music by selecting a button or a combination of buttons Table 2 Confusion matrix YO for the classification test of seven synthesized performances of Ekorrn I 5 88 3 0 5 0 0 8 5 85 0 3 0 0 3 0 0 63 3 28 5 3 10 3 5 65 3 13 5 0 3 33 3 45 13 0 0 18 0 10 5 68 Table 3 Confusion matrix  for the classification test of seven synthesized performances of Mazurka I ii 6 HZ4 6i 3 i SOLEMN 16 13 69 TENDERNESS 13 0 0 41 13 31 NO EXPRESSION 0 6 016 9 66 associated with different emotions This could be applied to large MIDI music databases on the Internet Another possibility would be to use the new macro rules as a tool for objective analysis of emotional aspects of performances This possibility may be interesting from a musicological point of view The macro rules could be used in reverse in order to analyze the emotional content of a performance For example the parameters of the rules can be automatically fitted to a performance c.f. Friberg 1995b In this way, it may be possible to classify deviations in various performance parameters according to intended emotion This possibility seems tempting to explore in the future The DM rules are all triggered by the structure of the music i.e the combination of note values intervals etc Hence the rules can only reflect the structure Nevertheless by varying the selection of rules and their quantities performance were generated that could be readily interpreted in emotional terms This indicates that an emotionally expressive performance can be directly derived from the musical structure ACKNOWLEDGEMENTS This work was supported by the Swedish National Council for Research in the Humanities and Social Sciences and by the Bank of Sweden Tercentenary Foundation The authors are grateful to Johan Sundberg who assisted in editing the manuscript to Patrik Juslin for valuable discussions and to the twenty anonymous subjects who participated in the listening test IV  320 


Fear Dur.Contr 4 i Solemnity fast/lom I DurConb 0  Anger 222  DurConb 2  221x   slow soft Factor 1 222  Tenderness  DurContr 4 222 Sadness DurConb 2 1 F U 10 c 3  25 B 20  m Figure 4 Two-dimensional space of the substantives derived from principal component analysis of the different emotional rule setups a Ekom o Mazurka Anger Happiness i a U  5 H  10 m q I Tenderness I Sadness 15 1 I I 100 50 0 50 100 Average 101 Deviation 224 Figure 5 Average 101 deviations versus average sound level deviations for all six performances of each piece The bars represent the standard deviations REFERENCES Gabrielsson A Intention and emotional expression in music performance In A. Friberg J lwarsson E Jansson  J Sundberg \(Eds Proceedings of the Stockholm Music Acoustics Conference 1993 Stockholm Royal Swedish Academy of Music, 1994, 108- 1 1 1 Juslin P.N Emotional communication in music performance a functionalist perspective and some data Music Perception 1997 I4 4 383-418 Juslin P.N Can results from studies of perceived expression in musical performances be generalized across response formats Psychomusicology in press Canazza S De Poli G Rinaldin S  Vidolin A Sonological analysis of clarinet expressivity In M. Leman Ed Music gestalt and computing: studies in cognitive and systematic musicology Berlin, Heidelberg New York Springer Verlag 1997 43 1-440 Battel G.U  Fimbianti R How communicate expressive intentions in piano performance In A Argentini  C Mirolo Eds Proceedings of the XII Colloqium on Musical Informatics Udine AIMI 1998 De Poli G Rod4 A  Vidolin A A model of dynamic profile variation depending on expressive intention in piano performance of classical music In A Argentini  C Mirolo Eds Proceedings of the XI1 Colloqium on Musical Informatics Udine: AIMI 1998,79-82 Orio N  Canazza S How are expressive deviations related to musical instruments? Analysis of tenor sax and piano performances of 223How High the Moon\224 theme In Argentini A  Mirolo C Eds Proceedings of the XII Colloqium on Musical Informatics Udine AIMI 1998 Juslin P.N Perceived emotional expression in synthesized performances of a short melody capturing the listener\222s judgment policy Musicae Scientiae 1997b 1 2 225 256 Canazza S De Poli G Di Sanzo G  Vidolin A Adding expressiveness to automatic musical performance In A Argentini  C Mirolo \(Eds Proceedings of the XU Colloqium on Musical Informatics Udine AIMI 1998 7 1-74 Gabrielsson A Expressive intention and performance In R Steinberg Ed Music and the Mind Machine the Psychophysiology and the Psychopathology of the Sense of Music Berlin Heidelberg New York Springer Verlag 1995,35-47 Friberg A A Quantitative Rule System for Musical Expression Doctoral dissertation Stockholm Royal Institute of Technology 1995a Bresin R Friberg A Emotional expression in music performance: synthesis and decoding TMH-QPSR Speech Music and Hearing Quarterly Progress and Status Report 411998 Stockholm pp 85-94 Friberg A Generative for music performance a formal description of a rule system Computer Music Journal Friberg A Bresin, R Frydtn L  Sundberg J Musical punctuation on the microlevel Automatic identification and performance of small melodic units Journal of New Music Research 1998,27 3 271-292 Friberg A Matching the rule parameters of Phrase arch to performances of 223Tr2umerei\224 A preliminary study In A Friberg  J Sundberg Eds Proceedings of the KTH Symposium on Grammars for Music Performance Stockholm Speech Music and Hearing Department Friberg A  Sundberg J Does music performance allude to locomotion A model of final ritardandi derived from measurements of stopping runners Journal of the Acoustical Society ofAmerica 1999 IO5 3 1469 1484 Cope D Computer modeling of musical intelligence in experiments in musical intelligence Computer Music Journal 1992,16 2 69-83 Granqvist S Enhancements to the Visual Analogue Scale VAS for listening tests Quarterly Progress and Status Report Stockholm Royal Institute of Technology  Speech Music and Hearing Department 1996,4,61-65 67-70 75-78 1991 I5\(2 56-71 1995b 37-44 LINKS Sound and MIDI fires of the seven versions of Ekorrn and Mazurka http://www.speech.kth.se/-robertolemotion KTHperformance rules description and sofmare http://www speech.kth.se/music/performance IV-321 


Fear 180  7 c 120 m 60 a U    0 eo z 60 0 200 400 6M 800 1000 Nominal 101 ms Anger 180 0 MO 4Gu 600 Boo Nominal 101 ms Happiness 180   120 0 V  m  d60 0 Eo 2  a 40 0 400 6w lo00 Nominal 101 ms Solemnity 0 200 400 600 800 loo0 Nominal 101 ms Sadness 180 a Eo s 60 Nominal 101 ms 1 00 Tenderness 180  4 120 0 e   IM P so 3 a m 60 0 400 600 Boo lo00 Nominal 101 ms Figure 6 Relative deviations of IO1 and for all six performances of each piece Negative values imply a tempo faster than the non expressive versions, while positive deviations indicate lengthening of tone duration and thus a slower tempo. The bars show the standard deviations IV-322 


main\(atgc. argv  cvm_startup\(argc, argv spacep  cvm-alloc\(size cvm-create-procs\(apriori apriorio cvm-finish 1 number of 1 nodes 2 4 VISIONA 1 2 type CVM apriori  r Apriori algorithm 221I k=l while \(candidate item sets of k-th pass are not emply  r step of k-th pass  while \(\(read data  emply  while item sets of k items are selectable from the data  search the itemset in the hash tree cvm-lock\(&mt[fieldp  4001 222fieldp cvm_unlock\(\(Lmt[fieldp  4001 1 1 cvm-barrier\(0 make the large itemsets of k by comparing the support values of the candidate itemsets with the minimum support value make the candidate item-sets of k+l out of the large itemsets of k k I I pass-1 pass-2 pass-3 pass-4 total 1 4 1 rscc sec ec e 15 21 4 1 41 39 44 9 1 93 1 4 2 1 8 1 2 1 1 5 step-I step-9 step-3 step-4 step-5 step4 step-7 step-8 step-9 step-IC Figure 5 Pseudo code for association rule mining executed in CVM consistency mechanism on the DSM for scalable data mining in both parallel and distributed computing en vironments l and 2 show the basic algorithm of as sociation rule mining which is popular among the data mining research community 9 shows a modification of l and 2 to decrease the number of the candidate item-sets to increase the performance lo shows the parallel algorithm of l and a for cluster-type dis tributed systems with message passing programming model 51 shows bayesian network generation which is a new technique of data mining New techniques for data mining and speedup are important research areas In the past new techniques or speedup is performed for each application indepen dently Our approach is different from the past re search We use DSM as a common tool for scalable data mining programs The objective is to develop scalable Table 6 Performance of association rule mining in CVM and VISIONA I I 4 1 21 11 1 5 data mining programs efficiently To decrease the over head of the DSM we proposed the LBC mechanism on the DSM. Also this paper described VISIONA a pro totype to implement the DSM with LBC To evaluate the effectiveness of the DSM with LBC we have implemented VISIONA both in PC clusters and UNIX computer clusters Programs of association rule mining and bayesian network generation has been also implemented in VISIONA According to t,he results of the evaluation of the DSM with LBC speedup by increasing the number of processors on the DSM with LBC is greater than or equal to that of SMP-type parallel computer In the future we will improve the VISIONA as a DSM system with multiple consistency protocol including the LBC mechanism The goal is to increase the adaptability to many different data mining programs References l R Agrawal T Imielinski A Swami 224Min ing Association Rules between Sets of Items in Large Databases,\224 Proceedings of ACM SIGMOD pp.207-216 May 1993 a R Agrawal R Srikant 224Fast Algorithms for Min ing Association Rules,\224 Proceedings of the 20th VLDB Conference pp.487-499 September 1994 3 C Amza A L Cox S Dwarkadas P Keleher H Lu R Rajamony W Yu W Zwaenepoel 224Tread Marks Shared Memory Computing on Networks of Workstations,\224 IEEE COMPUTER Vol 29 No 2 pp.18-28 February 1996 4 P Keleher 224The relative importance of concur rent writers and weak consistency models,\224 Pro ceedings of the 16th International Conference on Distributed Computing Systems pp 9 1-98 Nl ay 1996 5 D Heckerman 224Bayesian Networks for Knowl edge Discovery,\224 Advances in Knowledge Discov 149 


ery and Data Mining AAA1 Press/The MIT Press pp.273-305 1996 6 D Lenosla J Laudon T Joe D Nakahira L Stevens A Gupta J Hennessy 224The DASH Pro totype Implementation and Performance,\224 Pro ceedings of the 19th International Symposium on Computer Architecture pp.92-103 May 1992 7 F T Chong B Lim R Bianchini J Kubiatow icz 224Application Performance on the MIT Alewife Machine,\224 IEEE COMPUTER Vol 29 No 12 pp.57-64 December 1996 8 P Keleher A.L Cox S Dwarkadas W Zwaenepoel 224An Evaluation of Software-Based Release Consistent Protocols 224 Journal of Parallel and Distributed Computing Vol 29 pp.126-141 October 1995 9 J S Park M Chen P S Yu 224An Effective Hash Based 4lgorithni for Mining Association Rules,\224 Proceedings of ACM SIGMOD pp.175-186 June 1995 lo E Han, G Karypis V Kumar 224Scalable Parallel Data Mining for Association Rules,\224 Proceedings of ACM SIGMOD pp.277-288 May 1997 150 


Figure 8 Visual interface for Moridou system Search EngineTest Page 0 UI 0 5 5 Keyword plealet Figure 9 Prototype system in hcterogeneous environment 283 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


