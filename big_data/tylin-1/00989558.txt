LPMiner An Algorithm for Finding Frequent Itemsets Using Length-Decreasing Support Constraint Masakazu Sen0 and George Karypis Department of Computer Science and Engineering Army HPC Research Center University of Minnesota 4 192 EE/CS Building, 200 Union Street SE, Minneapolis MN 55455 Fax 612 625-0572 seno, karypis} @cs.umn.edu Abstract Over the years a variety of algorithms forfinding fre quent itemsets in very large transaction databases have been developed. The key feature in most of 
these algorithms is that they use a constant support constraint to control the inherently exponential complexity of the problem In gen eral, itemsets that contain only a few items will tend to be interesting ifthey have a high support, whereas long item sets can still be interesting even if their support is relatively small Ideally we desire to have an algorithm thatjnds all the frequent itemsets whose support decreases as a func tion of their length In this paper we present an algorithm called LPMinel thatfinds 
all itemsets that satisfy a length decreasing support constraint Our experimental evalua tion shows that LPMiner is up to two orders of magnitude faster than the FP-growth algorithm for$nding itemsets at a constant support constraint, and that its runtime increases gradually as the average length of the transactions and the discovered itemsets increases 1 Introduction Data mining research during the last eight years has led to the development of a variety of algorithms for find ing frequent itemsets in very large transaction databases 2 1 4 81 These itemsets can 
be used to find association rules or extract prevalent patterns that exist in the transac tions and have been effectively used in many different do mains and applications The key feature in most of these algorithms is that they This work was supported by NSF CCR-99725 19 EIA-9986042 ACI 9982274 by Army Research Office contract DA/DAAG55-98-1-0441 by the DOE ASCI program and by Army High Performance Computing Re search Center contract number DAAH04-95-C-0008 
Access to computing facilities was provided by the Minnesota Supercomputing Institute 0-7695-1 119-8/01 17.00 0 2001 IEEE control the inherently exponential complexity of the prob lem by finding only the itemsets that occur in a sufficiently large fraction of the transactions, called the support A lim itation of this paradigm for generating frequent itemsets is that it uses a constant value of support, irrespective of the length of the discovered itemsets In general, itemsets that contain only a few items will tend to 
be interesting if they have a high support, whereas long itemsets can still be in teresting even if their support is relatively small Unfortu nately if constant-support-based frequent itemset discovery algorithms are used to find some of the longer but infrequent itemsets they will end up generating an exponentially large number of short itemsets. Maximal frequent itemset discov ery algorithms 8 can potentially be used to find some of these longer itemsets but these algorithms can still gener ate a very large number of short infrequent itemsets if these 
itemsets are maximal Ideally we desire to have an algo rithm that finds all the frequent itemsets whose support de creases as a function of their length. Developing such an al gorithm is particularly challenging because the downward closure property of the constant support constraint cannot be used to prune short infrequent itemsets In this paper we present another property, called small est valid extension SVE that can be used to prune the search space of potential itemsets in the case where the sup port decreases as a function of the itemset length Using 
this property we developed an algorithm called LPMiner that finds all itemsets that satisfy a length-decreasing sup port constraint LPMiner uses the recently proposed FP tree 4 data structure to compactly store the database trans actions in main memory and the SVE property to prune certain portions of the conditional FP-trees that are being generated during itemset discovery. Our experimental eval uation shows that LPMiner is up to two orders of magnitude faster than the FP-growth algorithm for finding itemsets at a constant support constraint and that its runtime increases 505 


gradually as the average length of the transactions \(and the discovered itemsets\increases The rest of this paper is organized as follows. Section 2 provides some background information and related research work. Section 3 describes the FP-growth algorithm 4 on which LPMiner is based In Section 4 we describe how the length-decreasing support constraint can be exploited to prune the search space of frequent itemsets The experimen tal results of our algorithm are shown in Section 5 followed by the conclusion in Section 6 2 Background and related works The problem of finding frequent itemsets is formally de fined as follows: Given a set of transactions T each con taining a set of items from the set I and a support o we want to find all subsets of items that occur in at least o\(TI transactions. These subsets are calledfrequent itemsets Over the years a number of algorithms have been de veloped for finding all frequent itemsets The first compu tationally efficient algorithm for finding itemsets in large databases was Apriori 2 which finds frequent itemsets of length 1 based on previously generated 1  1 fre quent itemsets The key idea of Apriori is to use the down ward closure property of the support constraint to prune the space of frequent itemsets The FP-growth algorithm 4 finds frequent itemsets by using a data structure called FP tree that can compactly store in memory the transactions of the original database, thus eliminating the need to access the disks more than twice Another efficient way to repre sent transaction database is to use vertical tid-list database format The vertical database format associates each item with all the transactions that include the item Eclat in 7 uses this data format to find all frequent itemsets Even though to our knowledge no work has been pub lished for finding frequent itemsets in which the support decreases as a function of the length of the itemset, there has been some work in developing itemset discovery algo rithms that use multiple support constraints Liu et al 5 presented an algorithm in which each item has its own min imum item support or MIS The minimum support of an itemset is the lowest MIS among those items in the item set By sorting items in ascending order of their MIS val ues, the minimum support of the itemset never decreases as the length of itemset grows, making the support of itemsets downward closed Thus an Apriori-based algorithm can be applied Wang et al 6 allow a set of more general support constraints In particular they associate a support constraint for each one of the itemsets By introducing a new function called Pminsup that has \223Apriori-like\224 property they pro posed an Apriori-based algorithm for finding the frequent itemsets It is possible to represent a length-decreasing sup port constraint by using the formulation in 6 However the 223pushed\224 minimum support of each itemset is forced to be equal to the support value corresponding to the longest itemset. Thus it cannot prune the search space. Finally Co hen et al 3 adopt a different approach in that they do not use any support constraint. Instead they search for similar itemsets using probabilistic algorithms that do not guaran tee that all frequent itemsets can be found 3 FP-growth algorithm In this section we describe how the FP-growth algorithm works because our approach is based on this algorithm The description here is based on 4 The key idea behind FP-growth is to use a data struc ture called FP-tree to obtain a compact representation of the original transactions so that they can fit into the main mem ory As a result any subsequent operations that are required to find the frequent itemsets can be performed quickly, with out having to access the disks The FP-growth algorithm achieves that by performing just two passes over the trans actions Figure 1 shows how the FP-tree generation algo rithm works given an input transaction database that has five transactions with a total of six different items First it scans the transaction database to count how many times each item occurs in the database to get an \223Item Support Ta ble\224 \(step a The 223Item Support Table\224 has a set of item name, support pairs For example, item A occurs twice in the database namely in a transaction with tid 1 and another one with tid 5 therefore its support is 2/5  40 In step b those items in the Item Support Table are sorted accord ing to their support The result is stored in item-name field of Node-Link header table NL Notice that item F is not included in NL because the support of item F is less than the minimum support constraint 40 In step c items in each transaction in the input transaction database are sorted in the same order as items in the Node-Link header table NL. While transaction tid 5 is sorted, item F is discarded because the item is infrequent and has no need of consider ation In step \(d the FP-tree is generated by inserting those sorted transactions one by one The initial FP-tree has only its root When the first transaction is inserted nodes that represent item B C E A and D are generated, forming a path from the root in this order The count of each node is set to 1 because each node represents only one transaction tid 1 so far Next when the second transaction is inserted a node representing item B is nor generated Instead, the node already generated is reused In this case, because the root node has a child that represents item B the count of the node is incremented by one As for item E since there is no child representing item E under the current node a new node with item-name E is generated as a child of the current node Similar processes are repeated until all the sorted transactions are inserted into the FP-tree Once an FP-tree is generated from the input transaction database, the algorithm mines frequent itemsets from the 506 


sort items in each transaction Item Support Table Figure 1 Flow of FP-tree generation Conditional Pattern Base of conditional pattern D item count E  _____ ____ Node-Link NL Conditional FP-tree of conditional pattern D Single path FP-tree Figure 2 Conditional FP-tree FP-tree The algorithm generates itemsets from shorter to longer ones adding items one-by-one to those itemsets al ready generated It divides mining the FP-tree into mining smaller FP-trees each of which is based on an item on the Node-Link header table in Figure 1 Let us choose item D as an example For item D we generate a new transac tion database called conditional pattern base Each trans action in the conditional pattern base consists of items on the paths from parent nodes whose child nodes have item name D to the root node The conditional pattern base for item D is shown in Figure 2 Each transaction in the con ditional pattern base also has its count of occurrence corre sponding to the count of the node with item-name D in the original FP-tree. Note that item D itself is a frequent item set consisting of one item Let us call this frequent itemset 223D\224 a conditionalpattern A conditional pattern base is a set of transactions each of which includes the conditional pattern What we do next is to forget the original FP-tree in Figure 1 for a while and then focus on the conditional pattern base we got just now to generate frequent itemsets that include this conditional pattern 223D\222 For this purpose we generate a smaller FP-tree than the original one based on the conditional pattern 223D\222 This new FP-tree, called conditional FP-tree is generated from the conditional pat tern base using the FP-tree generation algorithm again If the conditional FP-tree is not a single path tree we divide mining this conditional FP-tree to mining even smaller con ditional FP-trees recursively This is repeated until we ob tain a conditional FP-tree with only a single path During those recursively repeated processes all selected items are added to the conditional pattern Once we obtain a single path conditional FP-tree like the one in Figure 2 we gen erate all possible combinations of items along the path and combine each of these sets of items to the conditional pat tern For example from those three nodes in the conditional FP-tree in Figure 2 we have 23  8 combinations of item B C and E 223 224 no item 223B\224 223C\224 223E\224 223BC\222  223CE\222 223EB\224 and 223BCE\224 Then we obtain frequent itemsets based on conditional pattern base 223D\224 223D\222 223DB\224 223DC\224, \223DE\222 223DBC\224, \223DCE\224, \223DEB\224 and 223DBCE\222 4 LPMiner algorithm LPMiner is an itemset discovery algorithm based on the FP-growth algorithm which finds all the itemsets that sat isfy a particular length-decreasing support constraint f\(1 where 1 is the length of the itemset More precisely f\(1 Satisfies f\(la 2 f\(lb for any Za,lb such that I  lb The idea of introducing this kind of support constraint is 507 


that by using a support that decreases with the length of the itemset we may be able to find long itemsets that may be of interest without generating an exponentially large num ber of shorter itemsets Figure 3 shows a typical length decreasing support constraint In this example the support constraint decreases linearly to the minimum value and then stays the same for itemsets of longer length Our problem is restated as finding those itemsets located above the curve determined by length-decreasing support constraint f 1 support  I support I length of itemset Figure 4 Smallest valid extension SVE 1 10 length of itemset Figure 3 An example of typical length-decreasing support constraint A simple way of finding such itemsets is to use any of the traditional constant-support frequent itemset discovery algorithms in which the support was set to minl>o f\(Z and then discard the itemsets that do not satisfy the length decreasing support constraint This approach however does not reduce the number of infrequent itemsets being discovered and as our experiments will show requires a large amount of time As discussed in the introduction finding the complete set of itemsets that satisfy a length-decreasing support func tion is particularly challenging because we cannot use the downward closure property of the constant support frequent itemsets This property states that in order for an itemset of length Z to be frequent all of its subsets have to be frequent as well As a result once we find that an itemset of length I is infrequent, we know that any longer itemsets that include this particular itemset cannot be frequent and thus elim inate such itemsets from further consideration However because in our problem the support of an itemset decreases as its length increases an itemset can be frequent even if its subsets are infrequent A key property regarding the itemset whose support decreases as a function of their length is the following Given a particular itemset I with a support of TI such that TI  f\(lIl then TI  min\({llf\(Z  TI is the minimum length that an itemset I such that I 3 I must have before it can potentially become frequent Figure 4 illustrates this relation graphically The length of I is noth ing more than the point at which a line parallel to the x-axis at y  TI intersects the support curve; here we essentially assume the best case in which I exists and it is supported by the same set of transactions as its subset I We will refer to this property as the smallest valid extension property or WE for short LPMiner uses this property as much as it can to prune the conditional FP-trees, that are generated during the itemset discovery phase In particular it uses three different prun ing methods that when combined substantially reduce the search space and the overall runtime These methods are described in the rest of this section 4.1 Transaction pruning TP The first pruning scheme implemented in LPMiner uses the smallest valid extension property to eliminate entire candi date transactions of a conditional pattern base Recall from Section 3 that during frequent itemset generation the FP growth algorithm builds a separate FP-tree for all the trans actions that contain the conditional pattern currently under consideration Let CP be that conditional pattern lCPl be its length and T\(CP be its support If CP is infre quent, we know from the WE property that in order for this conditional pattern to grow to something indeed frequent it must have a length of at least f-'\(o\(CP Using this requirement, before building the FP-tree corresponding to this conditional pattern we can eliminate any transactions whose length is shorter than f-l\(c~\(CP  ICPI as these transactions cannot contribute to a valid frequent itemset in which CP is part of it We will refer to this as the trunsac tion pruning method and denote it by TP We evaluated the complexity of this method in compari son with the complexity of inserting a transaction to a con ditional pattern base There are three parameters we have to know to prune a transaction: the length of each transac tion being inserted f-'\(o\(CP and CPI The length of each transaction is calculated in a constant time because we can count each item when the transaction is actually being generated As T\(CP and lCPl are common values 508 


for all transactions in a conditional pattern base, these val ues need to be calculated only once for the conditional pat tern base It takes a constant time added to the original FP growth algorithm to calculate JCPI As for f-'\(a\(CP evaluating f takes O\(log 111 to execute binary search on the support table determined by f\(l Let cpb be the con ditional pattern base and rn  CtranEcpb Itranl The com plexity per inserting a transaction is O\(log\(lIl Under an assumption that all items in I are contained in cpb this value is nothing more than O\(1 Thus the complexity of this method is just a constant time per inserting a transac tion 4.2 Node pruning NP The second pruning method focuses on pruning certain nodes of a conditional FP-tree on which the next condi tional pattern base is about to be generated. Let us consider a node w of the FP-tree. Let I\(v be the item stored at this node a\(l\(w be the support of the item in the conditional pattern base and h\(v be the height of the longest path from the root through v to a leaf node From the SVE property we know that the node w will contribute to a valid frequent itemset only if 1 h\(v  ICPl 2 f-'\(.\(w where lCPl is the length of conditional pattern of the cur rent conditional FP-tree The reason that equation 1 is correct is because, among the transactions that go through node w the longest itemset that I\(v can participate in has a length of h\(v Now if the support of I\(v is small such that it requires an itemset whose length f-l\(a\(I\(v is greater than h\(v  JCPI then that itemset cannot be supported by any of the transactions that go through node U Thus if equation 1 does not hold node v can be pruned from the FP-tree Once node w is pruned then a\(I\(w will de crease as well as the height of the nodes through U possibly allowing further pruning We will refer to this as the node pruning method, or NP for short A key observation to make is that both the TP and NP methods can be used together as each one of them prunes portions of the FP-tree that the other one does not In par ticular, the NP methods can prune a node in a path that is longer than f-'\(a\(CP  lCPl because the item of that node has lower support than CP On the other hand TP reduces the frequency of some itemsets in the FP-tree by removing entire short transactions. For example, consider two transactions A B C D and A B Let's assume that f-l\(a\(CP  lCPl  4 and each one of the items A,B,C,D has a support equal to that of CP In that case, the NP will not remove any nodes, whereas TP will eliminate the second transaction In order to perform the node pruning we need to com pute the height of each node and then traverse each node v to see if it violates equation 1 If it does then the node v can be pruned The height of all the nodes whose longest path goes through v must be decremented by one and the support of I\(v needs to be decremented to take account of the removal of U Every time we make such changes in the tree nodes that could not have been pruned before may now become eligible for pruning In particular, all the rest of the nodes that have the same item I\(v needs to be rechecked, as well as all the nodes whose height was decre mented upon the removal of w Our initial experiments with such an implementation showed that the cost of perform ing the pruning was often quite higher than the saving we achieved when used in conjunction with the TP scheme. For this reason we implemented an approximate but fast version of this scheme that achieves a comparable degree of prun ing Our approximate NP algorithm initially sorts the trans actions of the conditional pattern base in decreasing trans action length then traverses each transaction in that order and tries to insert them in the FP-tree Let t be one such transaction and l\(t be its length When t is inserted into the FP-tree it may share a prefix with some transactions al ready in the FP-tree However as soon as the insertion oft results in a new node being created we check to see if we can prune it using equation I In particular if w is that newly created node then h\(v  l\(t because the trans actions are inserted into the FP-tree in decreasing length Thus v can be pruned if If that can be done the new node is eliminated and the inser tion of t continues to the next item Now if one of the next items inserts a new node U then that one may be pruned us ing equation 2 In equation 2 we use the original length of the transaction l\(t not the length after the removal of the item previously pruned The reason is that Z\(t is the cor rect upper bound of h\(u because one of the transactions inserted later may have a length of at most l\(t the same as the length of the current transaction and can modify its height The above approach is approximate because i the elim ination of a node affects only the nodes that can be elimi nated in the subsequent transactions not the nodes already in the tree; \(ii we use pessimistic bounds on the height of a node as discussed in the previous paragraph\This approx imate approach however does not increase the complexity of generating the conditional FP-tree beyond the sorting of the transactions in the conditional pattern base Since the length of the transaction falls within a small range they can be sorted in linear time using bucket sort 509 


4.3 Path pruning PP Once the tree becomes a single path the original FP growth algorithm generates all possible combinations of items along the path and concatenates each of those com binations with its conditional pattern If the path contains k items, there exist a total of 2k such combinations. How ever using the SVE property we can limit the number of combinations that we may need to consider Let il i2   ik be the k items such that a\(ij 2 a\(ij+l One way of generating all possible 2k combina tions is to grow them incrementally as follows First we create two sets one that contains il and the other that does not Next, for each of these sets we generate two new sets such that in each pair of them, one contains i2 and the other does not leading to four different sets By continuing this process a total of k times we will obtain all possible 2k combinations of items This approach essentially builds a binary tree with IC levels of edges in which the nodes cor respond to the possible combinations. One such binary tree for k  4 is shown in Figure 5 To see how the SVE property can be used to prune cer tain subgraphs of this tree and hence combinations to be explored\consider a particular internal node v of that tree Let h\(v be the height of the node root has a height of zero\and let P\(v be the number of edges that were one on the path from the root to w In other words P\(v is the number of items that have been included so far in the set Using the SVE property we can stop expanding the tree un der node v if and only if P\(v  k   lCPl  f-l\(44L\(v  Essentially, the above formula states that based on the fre quency of the current item the set must have a sufficiently large number of items before it can be frequent If the num ber of items that were already inserted in the set 3\(v plus the number of items that are left for possible insertion k h\(v is not sufficiently large then no frequent itemsets can be generated from this branch of the tree and hence it can be pruned We will refer to this method as path pruning or PP for short The complexity of PP per one binary tree is k log 111 be cause we need to evaluate f for k items On the other hand, the original FP-growth algorithm has the complexity of O\(2 for one binary tree The former is much smaller for large k For small k this analysis tells that PP may cost more than the saving Our experimental result however suggests that the effect of pruning is bigger than the cost ID1 IT1 A IOOK lOOK 3 to35 3 to30 5 Experimental results We experimentally evaluated the various search space prun ing methods of LPMiner using a variety of datasets gener ated by the synthetic transaction generator that is provided IT1 2 10000 1000 ill ILI N Figure 5 Binary tree when k  4 IT1 2 loo00 5000 by the IBM Quest group and was used in evaluating the Apriori algorithm All of our experiments were per formed on Intel-based Linux workstations with Pentium 111 at 600MHz and 1GB of main memory. All the reported run times are in seconds We used two classes of datasets DS1 and DS2 Both of them contained lOOK transactions For each of the two classes we generated different problem instances in which we varied the average size of the transactions from 3 items to 35 items for DS1 obtaining a total of 33 dif ferent datasets DS1.3    DS1.35 and from 3 items to 30 items for DS2, obtaining DS2.3    DS2.30 For each problem instance in both of DS 1 z and DS2.2 we set the average size of the maximal long itemset to be 2/2 so as z increases, the dataset contains longer frequent itemsets The difference between DS1.z and DS2.z is that each problem instance DS 1 x consists of 1 K items, whereas each problem instance DS2.2 consists of 5K items The characteristics of these datasets are summarized in Table 1 Table 1 Parameters for datasets used in our tests JDI Number of transactions IT Average size of the transactions 11 Average size of the maximal po tentially long itemsets IL Number of maximal po tentially large itemsets N Number of items I Darameter I DS1 1 DS2 1 In all of our experiments we used minimum support con straint that decreases linearly with the length of the frequent itemsets In particular for each of the DS1.x datasets, the initial value of support was set to 0.5 and it was decreased linearly down to 0.01 for itemsets up to length 2 For the rest of the itemsets, the support was kept fixed at 0.01 The left graph of Figure 6 shows the shape of the support curve for DS1.20 In the case of the DS2 class of datasets we used a similar approach to generate the constraint however in stead of using 0.01 as the minimum support we used 0.005 510 


The right graph of Figure 6 shows the shape of the support curve for DS2.20 Length of Patterns Figure 6 Support curve for DS1.20 and DS2.20 5.1 Results Tables 2 and 3 show the experimental results that we ob tained for the DSl and DS2 datasets respectively Each row of the tables shows the results obtained for a differ ent DS1.z or DS2.z dataset specified on the first column The remaining columns show the amount of time required by different itemset discovery algorithms The column la beled 223FP-growth\224 shows the amount of time taken by the original FP-growth algorithm using a constant support con straint that corresponds to the smallest support of the sup port curve, 0.01 for DSl and 0.005 for DS2 The columns under the heading 223LPMiner\224 show the amount of time required by the proposed itemset discovery algorithm that uses the decreasing support curve to prune the search space A total of seven different variations of the LPMiner algo rithm are presented that use different combinations of the pruning methods described in Section 4 For example, the column label 223NP\224 corresponds to the scheme that uses only node pruning Section 4.2 whereas the column la beled 223NP+TP+PP\224 corresponds to the scheme that uses all the three different schemes described in Section 4 Note that values with a 223-\224 correspond to experiments that were aborted because they were taking too long time A number of interesting observations can be made from the results in these tables First either one of the LPMiner methods performs better than the FP-growth algorithm In particular the LPMiner that uses all three pruning methods does the best requiring substantially smaller time than the FP-growth algorithm For DSI it is about 2.2 times faster for DSl.10 8.2 times faster for DS1.20, 33.4 times faster for DS 1.30 and 1 15 times faster for DS 1.35. Similar trends can be observed for DS2 in which the performance of LP Miner is 4.2 times faster for DS2.10, 21.0 times faster for DS2.20 and 55.6 times faster for DS2.27 Second the performance gap between FP-growth and LPMiner increases as the length of the discovered frequent itemset increases recall that for both DS1.z and DS2.2 the length of the frequent itemsets increases with z This is due to the fact that the overall itemset space that LPMiner can prune becomes larger leading to improved relative per formance Third comparing the different pruning methods in isola tion we can see that NP and TF\222 lead to the largest runtime reduction and PP achieves the smallest reduction This is not surprising as PP can only prune itemsets during the late stages of itemset generation Finally the runtime with three pruning methods in creases gradually as the average length of the transactions and the discovered itemsets increases, whereas the run time of the original FP-growth algorithm increases expo nentially 6 Conclusion In this paper we presented an algorithm that can efficiently find all frequent itemsets that satisfy a length-decreasing support constraint The key insight that enabled us to achieve high performance was the smallest valid extension property of the length decreasing support curve So far, we have dealt with a common length-decreasing support for all the items However the proposed algorithm can be easily extended to allow different length-decreasing support constraint to be specified for each item or itemset References R Agarwal C Aggarwal V Prasad and V Crestana A tree projection algorithm for generation of large itemsets for as sociation rules IBM Research Report RC21341 November 1998 R Agrawal and R Srikant. Fast algorithms for mining asso ciation rules In Proc of the 20th Int\222l Conference on Very Large Databases Santiago, Chile, September 1994 E Cohen M Datar S Fujiwara A Gionis P Indyk R Mot wani J D Ullman and C Yang Finding interesting asso ciations without support pruning In ICDE pages 489499 2000 J Han J Pei and Y Yin Mining frequent patterns without candidate generation In Proc 2000 ACM-SIGMOD Int. Con5 Management of Data SIGMOD\222OO pages 1-12 Dallas TX May 2000 B Liu W Hsu and Y Ma Mining association rules with multiple minimum supports In SIGKDD 1999 1999 K Wang Y He and J Han Mining frequent itemsets us ing support constraints In The VLDB Journal pages 43-52 2000 M J Zaki Scalable algorithms for association mining IEEE Transactions on Knowledge and Data Engineering 12\(3 May/June 2000 M J Zaki and K Gouda Fast vertical mining using diffsets Technical Report 01-1 RPI 2001 511 


Table 2 Comparison of pruning methods using DS1 Table 3 Comparison of pruning methods using DS2 512 


 A A A A A A A A B B B B B B B A B A B A B A B AB A B A A A A B B B A B A B A A B B B B A B A B A B A B A B A B A disjoint B A inside B A contains B A equals B A meets B A covered by B A covers B A overlaps B A B A B A B A B A B AB Figure 4 Topology and resolution increase with minimum bounding circles 64Mb of main memory Since the Apriori algorithm uses the number of transactions as support and we wanted to compare our algorithm with Apriori we have implemented MaxOccur and the na\250 021ve with transaction based support MaxOccur1 The second version of MaxOccur MaxOccur2 used the object-based support as presented in Algorithm 3.1 Table 9 shows the average execution times for the four algorithms with different image set sizes and 033 0 0  05 for Apriori 223Na\250 021ve\224 and MaxOccur1 and 0  0035 for MaxOccur2 The results are graphically illustrated in Figure 5 Clearly MaxOccur scales well with both versions treating one thousand images in 1.3 seconds on average regardless of the size of the data set The running time for 002ltering the frequent item-sets with 033 0  the maximum support threshold line 16 of Algorithm 3.1 is negligible since it is done in main memory once the frequent item-sets are determined Moreover the calculation of the total number of items line 4 of Algorithm 3.1 is done during the 002rst scan of the data set and has limited repercussion on the algorithms execution time The major difference between Apriori and MaxOccur is in ascertaining the candidate item-sets and counting their repeated occurrences in the images Obviously MaxOccur discovers more frequent item-sets The na\250 021ve algorithm also 002nds the same frequent item-sets but is visibly capable of less performance in execution time The left graphic in Figure 6 shows the average number of frequent item-sets discovered with the three algorithms Apriori found on average 109 different frequent k-item-sets while MaxOccur1 and Na\250 021ve found 148 on the same data sets and MaxOccur2 found 145 on average The discrepancy between MaxOccur1 and MaxOccur2 is basically due to the different de\002nition of support The price we pay in performance loss with MaxOccur is gained by more frequent item-sets and thus more potentially useful association rules with recurrent items discovered ofimages Apriori Na\250 021ve MaxOccur1 MaxOccur2 10K 6.43 70.91 13.62 13.68 25K 15.66 176.69 32.35 34.11 50K 30.54 359.38 66.07 67.44 75K 44.93 514.33 97.27 101.23 100K 60.75 716.01 130.12 137.81 Table 9 Average execution times in seconds with different number of images 0 100 200 300 400 500 600 700 800 10K 25K 50K 75K 100K Apriori MaxOccur1 MaxOccur2 Na\357ve time images Figure 5 Scale up of the algorithms 6 Discussion and conclusion We have introduced in this paper multimedia association rules based on image content and spatial relationships between visual features in images using coarse to 002ne resolution approach and we have demonstrated the preservation and changes in topological features during resolution re\002nement We have put forth a Progressive Resolution Re\002nement approach for mining visual media at different resolution levels and have presented two algorithms for the discovery of content-based multimedia association rules These rules would be meaningful only in a homogeneous image collection a collection of semantically similar images or received from the same source channel Many improvements could still be added to the multimedia mining process to speed up the discovery or to re\002ne or generalize the discovered results 017 One major enhancement in the performance of the multimedia association rule discovery algorithms is the addition of some restrictions on the rules to be discovered Such restrictions could be given in a metarule form Meta-rule guided mining consists of dis#ofimages 033 0 0  25 0  20 0  15 0  10 0  05 10K 1.43 2.20 2.70 5.06 13.51 25K 2.80 4.78 6.31 11.20 32.35 50K 6.27 9.28 11.59 22.74 66.07 75K 8.24 13.57 17.69 33.94 97.27 100K 11.32 17.63 23.13 46.74 130.12 Table 10 Average execution time in seconds of MaxOccur with different thresholds 


 0 20 40 60 80 100 120 140 160 MaxOccur2 MaxOccur1 Na\357ve Apriori Apriori MaxOccur1 MaxOccur2 Na\357ve F k  Figure 6 Frequent item\255sets found by the dif\255 ferent algorithms covering rules that not only are frequent and con\002dent but also comply with the meta-rule template For example with a meta-rule such as 223 H-Next-to X Y   Colour x red  Overlap Y Z   P  Y Z  224 one need only to 002nd frequent 3-item-sets of the form f HNext-to\(red Y  Overlap Y 003  P  Y 003  g where Y is an attribute value and P a visual descriptor or spatial relationship predicate Obviously such a 002lter would greatly reduce the complexity of the search problem A method for exploiting meta-rules for mining multilevel association rules is given in  017 We have approximated an object in an image to a locale which is an area with a consistent visual feature such as colour Objects in images and videos are obviously more complex In a recent paper 9 re gions and their signatures are used as objects in a similarity retrieval system A computationally ef\002cient way to identify distinct objects in images is however still to be proposed Automatically identifying real objects and using spatial relationships between real objects would reduce the number of rules discovered and make them more signi\002cant for some multimedia applications 017 Object recognition or identi\002cation in image processing and computer vision is a very active research 002eld Accurately identifying an object in a video for example as being an object in itself is a very dif\002cult task We believe that data mining techniques can help in this perspective Multimedia association rules with spatial relationships using the motion vector of locales as a conditional 002lter can be used to discover whether locales moving together in a video sequence are part of the same object with a high con\002dence 017 There are many application domains where multimedia association rules could be applied and should be tested such as global weather analysis and weather forecast medical imaging solar surface activity understanding etc We are investigating the application with Magnetic Resonance Imaging MRI to discover associations between lesioned structures in the brain or between lesions and pathological characteristics Further development and experiments with mining multimedia data will be reported in the future References 1 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules In Proc VLDB  pages 487\226499 1994 2 M  J  E genhof er  Spatial Query Languages  PhD thesis University of Maine 1989 3 M  J  E genhof er and J  S har ma T opol ogi cal r e l a t i ons between regions in r 2 and z 2 In Advances in Spatial Databases SSD'93  Singapore 1993 4 U  M  F ayyad S  G  D j or go vski  a nd N  W e i r  A ut omat i n g the analysis and cataloging of sky surveys In U Fayyad G Piatetsky-Shapiro P Smyth and R Uthurusamy editors Advances in Knowledge Discovery and Data Mining  pages 471\226493 AAAI/MIT Press 1996 5 Y  F u a n d J Han  M e ta-ru le-g u i d e d m in in g o f a sso ciatio n rules in relational databases In Proc 1st Int Workshop Integration of Knowledge Discovery with Deductive and ObjectOriented Databases  pages 39\22646 Singapore Dec 1995 6 J  H an an d Y  F u  Disco v e ry o f mu ltip le-le v el asso ciatio n r u l es from large databases In Proc VLDB  pages 420\226431 1995 7 Z  N  L i  O R Z a 250 021ane and Z Tauber Illumination invariance and object model in content-based image and video retrieval Journal of Visual Communication and Image Representation  10\(3\:219\226244 September 1999 8 R  M iller a n d Y  Y a n g  Asso ciatio n r u l es o v e r i n t erv a l d ata In Proc ACM-SIGMOD  pages 452\226461 Tucson 1997 9 A  N atse v  R Rasto g i  a n d K Sh im W ALR U S A s imilar ity retrieval algorithm for image databases In Proc ACMSIGMOD  pages 395\226406 Philadelphia 1999  R Ng L  V  S  L akshmanan J  H an a nd A Pang E x ploratory mining and pruning optimizations of constrained associations rules In Proc ACM-SIGMOD  Seattle 1998 11 R Srik an t a n d R Ag ra w a l M i n i n g q u a n titati v e asso ciatio n rules in large relational tables In Proc ACM-SIGMOD  pages 1\22612 Montreal 1996  P  S t ol or z H  N a kamur a  E  M esr obi an R  M unt z E  S h ek J Santos J Yi K Ng S Chien C Mechoso and J Farrara Fast spatio-temporal data mining of large geophysical datasets In Proc Int Conf on KDD  pages 300\226305 1995  O  R  Z a 250 021ane Resource and Knowledge Discovery from the Internet and Multimedia Repositories  PhD thesis School of Computing Science Simon Fraser University March 1999  O  R  Z a 250 021ane,J.Han,Z.-N.Li,J.Y.Chiang,andS.Chee MultiMediaMiner A system prototype for multimedia data mining In Proc ACM-SIGMOD  Seattle 1998  O  R  Z a 250 021ane J Han Z.-N Li and J Hou Mining multimedia data In CASCON'98 Meeting of Minds  Toronto 1998 


18001  balancing mechanism which requires further investi gation 4.5 Speedup Figure 12 shows the speedup ratio for pass 2 vary ing the number of processors used, 16 32 48 and 64 where the curve is normalized with the 16 processor execution time The minimum support value was set to 0.4 4.5 0.5 1 1 0 I 10 20 30 40 50 60 70 number of mxessors Figure 12 Speedup curve NPA HPA and HPA-ELD attain much higher lin earity than SPA HPA-ELD an extension of HPA for extremely large itemset decomposition further in creases the linearity HPA-ELD attains satisfactory speed up ratio This algorithm just focuses on the item distribution of the transaction file and picks up the extremely frequently occurring items Transferring such items could result in network hot spots HPA-ELD tries not to send such items but to process them locally. Such a small mod ification to the original HPA algorithm could improve the linearity substantially 4.6 Effect of increasing transaction Figure 13 shows the effect of increasing transac tion database sue as the number of transactions is increased from 256,000 to 2 million transactions We used the data set t15.14 The behavior of the results does not change with increased database size The minimum support value was set to 0.4 The num ber of processors is kept at 16 As shown each of the parallel algorithms attains linearity 5 Summary and related work In this paper we proposed four parallel algorithms for mining association rules A summary of the four database size Sizeup 0 I 0 500 loo0 1500 uxw amount of transaction thousands Figure 13 Sizeup curve algorithms is shown in Table 5 In NPA the candi date itemsets are just copied amongst all the proces sors Each processor works on the entire candidate itemsets NPA requires no data transfer when the supports are counted However in the case where the entire candidate itemsets do not fit within the mem ory of a single processor the candidate itemsets are divided and the supports are counted by scanning the transaction database repeatedly Thus Disk 1/0 cost of NPA is high PDM, proposed in 6 is the same as NPA which copies the candidate itemsets among all the processors Disk 1/0 for PDM should be also high The remaining three algorithms SPA HPA and HPA-ELD partition the candidate itemsets over the memory space of all the processors Because it better exploits the total system's memory, disk 1/0 cost is low SPA arbitrarily partitions the candidate itemsets equally among the processors Since each processor broadcasts its local transaction data to all other pro cessors the communication cost is high HPA and HPA-ELD partition the candidate itemsets using a hash function which eliminates the need for transac tion data broadcasting and can reduce the comparison workload significantly HPA-ELD detects frequently occurring itemsets and handles them separately which can reduce the influence of the workload skew 6 Conclusions Since mining association rules requires several scans of the transaction file its computational requirements are too large for a single processor to have a reasonable response time This motivates our research In this paper we proposed four different parallel algorithms for mining association rules on a shared nothing parallel machine and examined their viabil 29 


Table 5 characteristics of algorithms ity through implementation on a 64 node parallel ma chine the Fujitsu AP1000DDV If a single processor can hold all the candidate item sets parallelization is straightforward It is just suf ficient to partition the transaction over the proces sors and for each processor to process the allocated transaction data in parallel We named this algo rithm NPA However when we try to do large scale data mining against a very large transaction file the candidate itemsets become too large to fit within the main memory of a single processor In addition to the size of a transaction file a small minimum support also increases the size of the candidate itemsets As we decrease the minimum support computation time grows rapidly but in many cases we can discover more interesting association rules SPA HPA and HPA-ELD not only partition the transaction file but partition the candidate itemsets among all the processors We implemented these al gorithms on a shard-nothing parallel machine Per formance evaluations show that the best algorithm HPA-ELD attains good linearity on speedup by fully utilizing all the available memory space which is also effective for skew handling At present we are doing the parallelization of mining generalized association rules described in 9 which includes the taxonomy is-a hierarchy Each item belongs to its own class hierarchy In such mining associations between the higher class and the lower class are also examined Thus the candidate itemset space becomes much larger and its computation time also takes even longer than the naive single level association mining Parallel pro cessing is essential for such heavy mining processing Acknowledgments This research is partially supported as a priority research program by ministry of education We would like to thank the F\221ujitsu Parallel Computing Research Center for allowing us to use their APlOOODDV sys tems References l R.Agrawal T.Imielinski and ASwami 223Min ing Association Rules between Sets of Items in Large Databases\224 In Proc of the 1993 ACM SIGMOD International Conference on Manage ment of Data pp207-216 May 1993 2 R.Agrawal and RSrikant 223Fast Algorithms for Mining Association Rules\224 In Proc of the 20th International Conference on Very Large Data Bases pp.487-499 September 1994 3 J.S.Park M.-S.Chen and P.S.Yu 223An Effec tive Hash-Based Algorithm for Mining Associ ation Rules\224 In Proc of the 1995 ACM SIG MOD International Conference on the Manage ment of Data SIGMOD Record Vo1.24 pp.175 186 June 1995 4 H.Mannila H.Toivonen and A.I.Verkamo 223Ef ficient Algorithms for Discovering Association Rules\224 In KDD-94:AAAI Workshop on Knowl edge Discovery in Databases pp.181-192 July 1994 5 A.Savasere, E.Omiecinski and S.Navathe 223An Effective Algorithm for Mining Association Rules in Large Databases\224 In Proc of the 21th International Conference on Very Large Data Bases pp.432-444 September 1995 6 J.S.Park M.-S.Chen and P.S.Yu 223Efficient Parallel Data Mining for Association Rules\224 In Proc of the 4th International Conference on In formation and Knowledge Management pp.31 36 November 1995 7 T.Shintani and M.Kitsuregawa 223Considera tion on Parallelization of Database Mining\224 In Institute of Electronics Information and Com munication Engineering Japan SIG CPS Y95 88 Technical Report Vo1.95 No.47 pp.57-62 December 1995 8 T.Shimizu T.Horie and H.Ishihata 223Perfor mance Evaluation of the APlOOO Effects of message handling broadcast and barrier syn chronization on benchmark performance-\224  In S WO PP 22292 9.2 ARC 95 Information Processing Society of Japan Vo1.92 No.64 1992 9 R.Srikant and R.Agrawal 223Mining Generalized Association Rules\224 In Proc of the 21th Inter national Conference on Very Large Data Bases pp.407-419 September 1995 30 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


