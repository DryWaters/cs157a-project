Fuzzy Clustering for Categorical Multivariate Data Chi-Hyon Oh Katsuhiro Honda and Hidetomo Ichihashi Graduate School of Engineering Osaka Prefecture University Sakai Osaka Japan E-mail:oh@ ie-osakafu-u.ac.jp Abstract This paper proposes a new fuzzy clustering algorithm for categorical multivariate data The conventional fuzzy clustering algorithms form fuzzy clusters so as to minimize the total distance from cluster centers to data points However they cannot be applied to the caSe where only cmcurrence relations among 
individ uals and categories are given and the criterion to ob tain clusters is not available The proposed method en ables us to handle that kind of data set by maximizing the degree of aggregation among clusters The cluster ing results by the proposed method show similarity to those of Correspondence Analysis or Hayashi\222s Quan tification Method Qpe III Numerical examples show the usefulness of our method 1 Introduction We have been facing an explosive increase in the amount of information or data being 
stored in the database electrically In response to the increase of the data storage importance of extracting useful infor mation which is even implicit potential or previously unlarown from the database is getting up-and-coming The extracted information can be put to use in the ar eas such as decision support prediction forecasting and estimation Data Mining 1][2 which is also known as Knowledge Discovery from Database KDD is this kind of paradigm It encompasses a number of different technical approaches. Most 
of them devote themselves to unearth association rules which represent association relationships among different attributes They find out such rules according to similarity or correlation among data There exists a classical technique called Conespon dence Analysis 3 or Hayashi\222s Quantification method type III 141 which analyzes data according to correla tion It is one of the multivariate analysis techniques and can be seen as one of the Data Mining approaches It deals with categorical multivariate data The categori cal multivariate data 
set is provided in the form of cross classification table contingency table or cooccurrence matrix Each individual is described by a set of quali tative variables with several categories The categorical variables are defined by several quantifications of qual itative data binary indicator frequency or scaled vari able The Correspondence Analysis quantifies the multi categorical data so as to maximize correlation among data This technique provides us with useful knowledge from the data set and makes it possible to visualize var ious 
criteria of principal component analysis We can employ the Correspondence Analysis as a way of di mension reduction Cluster analysis is a technique which discovers the sub structure of a data set by dividing it into several clus ters It is also known as one of the Data Mining ap proaches There have been many researches for cluster analysis Fuzzy clustering is an extension of the cluster analysis which represents the affiliation of data points to clusters by memberships Introducing fuzziness to clustering gives us the flexible 
representations of sub structures of the data set Fuzzy c-Means 51 Fuzzy c Lines 6 Fuzzy c-varieties 7 and Fuzzy c-Regression Models 8 are varieties of the fuzzy clustering algo rithms They have different shapes of cluster centers prototypes of clusters Most of them conduct clustering in accordance with similarity or dissimilarity derived from distances from cluster centers to data points They employ Euclidian Mahalanobis or Manhattan distance as the metric Few fuzzy clustering approaches 
how ever exisf which can realize fuzzy clustering where the data vectors are not available and only the similarity or correlation among items is given This paper proposes a new fuzzy clustering algorithm when a categorical multivariate data set is given. By the virtue of its capability of handling categorical multivari ate data the proposed method can conduct not only the cluster analysis but also data analysis similar to the Cor 0-7803-7078-3/0u$l0.00 C EEE Page 2154 


respondence Analysis Yamakawa et al has proposed a hybridization of the fuzzy clustering and the Conespon dence Analysis 9 Their proposed method implement two different data analysis techniques simultaneously and can obtain local relationships among data and scat ter diagrams Though our proposed method also focuses on the same kind of data set it conducts only fuzzy clus tering Therefore our method essentially differs from Yamakawa\222s method And another thing, Inoue et al has already proposed a fuzzy clustering algorithm that can also handle with categorical multivariate data 1101 However as their algorithm form the clusters one after another the volume of the cluster gradually decmses in turn Moreover in the process of assigning data points to clusters, calculation of eigen vectors is needed which is computationally demanding Our proposed algorithm is effectuated by maximizing a simple objective function The objective function rep resents the degree of aggregation of each cluster The solution algorithm for the objective function is based on iterative procedure through necessary conditions for local minima We can obtain clusters for the overall data set at a time by using the propose method In the objective function we introduce entropy maximization as a regularization proposed by Miyamoto et al  111 to obtain fuzzy clusters Afkr the fuzzy clustering we are supposed to obtain memberships for individuals and categories each and all Looking into obtained clusters mixed up with individ uals and categories, the similar result of data analysis to the Correspondence Analysis can be rived Since our proposed method can easily provide fuzzy clusters by solving simple algebraic equations that are far easier than the eigen value problems and doesn\222t require the calculation of cluster centers it provides us with useful way to analyze categorical multivariate data Numerid examples show the usefulness of our method 2 Fuzzy Clustering for Categorical Multivariate Data FCCM 2.1 Categorical Multivariate Data Let us consider a categorical multivariate data set M individuals described by a set of qualitative variables j with N categories In many cases each category con sists of some few subcategories They, however are not taken into consideration here The quahtative variables can be responses to some questionnaires or COOCCW fence relations among individuals and categories The categorical variables are defined by several quantifica tions e.g binary indicator frequency or scaled vari able The categorical multivariate data set is often given in the form of a table We show an example of the table in Table 1 Table 1 an ezample of categorical multivari ate data set In the table the rows are the individuals and the columns are the categories This kind of table is called crossclassification table, contingency table or cooccur rence matrix in a general way The conventional technique handling the data set in Ta ble l i.e the Correspondence Analysis, intends to dis cover relations between individuals and categories It quantifies the individuals and the categories by solving an eigen value problem After the quantification, we can plot the individuals and the categories on one or two di mensional space. Then, they are divided in some groups in accordance with the coordinates and characteristics of the data set are detected The analysis of the cate gorical multivariate data set is commonly conducted in this manner in the Correspondence Analysis The divi sion of the data set after the quantification is done by the analysts observing the data plots Therefore the Corre spondencx Analysis is not a clustering technique but a quantification method. Well-known fuzzy clustering al gorithms 51-[8 however can not handle the data set lie in Table 1 because of the clustering criteria they employ. They form clusters according to distances from cluster centers to data points It is impossible to calcu late those distances with respect to the data set in Table 1 In this paper we propose a new fuzzy clustering al gorithm which is able to handle the data set in Table 1 We call it FCCM Fuzzy Clustering for Categorical Multivariate Data 2.2 Degree of Aggregation Firstly we define two different memberships for the proposed method One is for the individuals, the other is 0-7803-7@78-3/0l/$10.00 C IEEE Page 2155 


for the categories The definitions of two memberships are shown as follows c XU  1 U E O 11 i  1  M 1 c=1 N cwcj=l WcjEIO,l c=l lC 2 j I where uCi is the membership of the i-th individual for the c-th cluster and wcj is that of the j-th category for the c-th cluster C denotes the number of clusters Though it seems that U and wcj have the same con straints since the memberships sum to one they are dif ferent actually For U the total amount of member ships of the i-th individual to the clusters has to be one On the other hand 2 indicates the total membership of the c-th cluster to the categories should be one Secondly we give a definition of the clustering criterion of the FCCM to obtain fuzzy clusters It should be pro vided so as to pup the individuals and the categories which have high correlations each other In this sense we regard the following degree of aggregation as the clustering criterion of the FCCM MN uciwcjdij c  1    c 3 i=l j=1 The degree of aggregation for each cluster is the to tal amount of products of qualitative variables dij and memberships for individuals and categories ue and wcj We maximix the degree of aggregation in 3 to fonn fuzzy clusters by assigning memberships to indi viduals and categories Furthermore if we define the total amount of member ships wcj of the j-th category to the clusters as one in such a way as in 4 we are unable to obtain proper clusters c Cwcj=1 W~~E[O,I I,**.,N 4 j=l The degree of aggregation will be maximized by allo cating individuals and categories to only one cluster un der the constraints provided in 4 That is why we em Ploy 2 2.3 Objective Function The FCCM can be driven by optimization of an objec tive function to maximize the degree of aggregation We use Lagrange's method of indeterminate multiplier to derive the objective function for the FCCM The objec tive function can be written as follows CMN c=l i=l c=l j=1 M IC  CA\266 xu  1 where Ai and yc are Lagrangian multipliers respectively The second and third terms in 5 represent entropy maximization as a regularization which was introduced in Fuzzy c-Means by Miyamoto et al l 11 for the first time It enables us to obtain fuzzy clusters Tu and T are the weighting parameters which specify the degree of fuzziness The remaining terms describe the con srraints of memberships i.e 1 and 2 respectively From the necessary conditions for the optimality of the objective function L i.e aL/aUe  0 and L3L/aWcj  0 we have the following equations N j=1 C N Ud   c=l j=l M The optimization algorithm is based on Picard iteration through necessary conditions for local minima of the objective function Therefore the proposed algorithm can be written as follows The FCCM Algorithm Step I Set values of parameters C Tu Tw and E Initialize memberships U randomly Step 2 Update membership wcj using 7 0-7803-7078-3/0v$l0.00 C IEEE Page 2156 


Step 3 Update memberships uCi using 6 Step 4 If max IuzEW  U  e then stop Otherwise, return to Step 2 3 Numerical Example 3.1 Literature Retrieval Data Set In numerical example we apply our proposed method to literature retrieval data set used in Cl01 and 121 We also apply the Correspondence Analysis to the data set and compare it with the proposed method The data set is shown in Table 2 The rows represent the literatures and the columns are the key words. The data set shows the cOOccurrence relations among the literatures and the key words Each entry denotes the number of appear ances of the key word in the corresponding literature For example, the key word 5 appears twice in the liter ature 4 The retrieval of literatures would be done in a system as to the cOOccuTfence relations For instance the literatures 67 and 8 might be retrieved if the key word 10 is entered into the retrieval system accordmg to Table 2 However the literature 9 should be retrieved in focusing the attention on the coowmnce relations to other key words i.e key words 11 and 12 Table 3 menbershzps of literatures Table 4 memberships of key words I Kev word 11 Cluster 1 I Cluster 2 1 3.2 Numerical Results We used the following values of parameters for the FCCM The number of clsuters C 2 The degree of fuzziness Tu 0.1 The degree of fuzziness T 1.5 Stopping condition of the FCCM E O.OOO1 The results are shown in Table 3 and Table 4 In Ta ble 3 and Table 4 we underlined larger memberships of literatures and key words we assume that literatures and key words are more likely to belong to the cluster to which they have larger memberships. From Table 3 we can see that literes are divided into  1,2,3,4,5 and 6,7,8,9 On the one hand key words are partitioned into 1,2,3,4,5,6,7 8 and 9, 10,11,12 These re sults are reasonable in accordance with Table 2 Figure 1 shows the result of the Correspondence Analy sis applied to Table 2 and represents the scatter diagram of literatures and key words after quantification The values corresponding to the first and second eigen val ues were plotted on the diagram The horizontal axis corresponds to the first eigen value and the vertical axis does the second one In Figure 1,0 and indicate lit eratures and key words respectively We can divide lit eram and key words into two groups according to the observation of Figure 1 One is literatures 1 2,3,4,5 and key words 12,3,4,5,6,7,8 and the other is liter atures 6,7,8,9 and key words 10,11,12 Only the key word 9 belongs to neither group The two groups are circled in Figure 1 Comparing the result of the Cor respondence Analysis with that of the proposed method we can observe that the similar results are obtained ex cept for the key word 9 4 Conclusions In this paper we proposed a new fuzzy clustering al gorithm the FCCM for categorical multivariate data to which the conventional fuzzy clustering algorithms could not be applied The FCCM was applied to the liten retrieval data set which was a kind of categor 0-7803-7078-3/0U$l0.00 C IEEE Page 2157 


Table 2 literature retrieval data set 0.3 0.2 0.1 0 0.1 0.2 0.3 a4 0.4 0.3 0.2 0.1 0 0.1 0.2 0.3 0.4 Fagure 1 the result of the Correspondence Analysis id multivariate data set in the numerical example The Correspondence Analysis was also applied to the data set and compared with the FCCM The FCCM showed the similar result to that of the Correspondence Analy sis While the Correspondence Analysis requires solv ing eigen value problem which is computationally de manding the FCCM needs simple algebraic calcula tions Therefore we can conclude that the FCCM is not only a fuzzy clustering algorithm handling categm id multivariate data but also a simple alternative of the Correspondent Analysis Besides if we modify the definition of memberships in 2 we can apply our proposed method to the case where only similarities among data are given In that case we redefine the constraint 2 as in 8 M cwcj=l WcjE\(O,l c=l C 8 j=l In 8 wcj is not for categories but individuals This modification leads to the similar result of Hayashi\222s Quantification Method 222Qpe IV 4 which also handles the same kind of similaritiy data set References l P Adriaans and D Zantinge Date Mining Addi son Wesley Longman 19 2 M J A Berry and G S Linoff Dare Mining Techniques John Wiley  Sons 1997 3 M Tenenhaus and E W Young 223An analysis and synthesis of multiple correspondence analysis, op timal scaling dual scaling homogeneity analy sis and other methods for quantifymg categorical multivariate data,\223 Psychornetrika V01.50 No.1 4 C Hayashi 223On the prediction of phenomena from qualitative data and the quantification of qualitative data from the mathematical statistical point of view,\224 Annals of the Insrimre of Statistical Mathematics Vo1.3 1952 pp 69-98 5 J C Bezdek Puffem recognition with fuzzy objec rivefunction algorithms Plenum Press New York 1981 6 J C Bezdek C Coray R Gundenon and J Watson 223Detection and characterization of clus ter substructure I linear structure fuzzy c-lines,\224 SIAMJ Appl Math V01.40 N0.2 1981 pp 339 357 1985 pp 91-1 19 0-7803-7078-3/OU$l0.00 C IEEE Page 2158 


71 J C Bezdek C Cony R Gundenon and J Watson 223Detection and characterization of cluster substructure XI fuzzy c-varieties and convex com binations thereof,\224 SIAM J Appl Math V01.40 8 R J Hathaway and J C Bezdek 223Switching regression models and fuzzy clustering,\224 IEEE Trans on Fuzzy System Vol.1 No.3 1993 pp N0.2 1981 pp 358-372 195-204 9 A Yamakwa Y Kanaumi H Ichihashi and T Miyoshi 223Simultaneous Application of Clus tering and Correspondence Analysis,\224 Proc of IJCNN\22299 Paper 625,1999 pp 1-6 lo K Inoue and K Urahama 221\221 Fuzzy Clustering Based on Cooccurrence Mamx and Its Application to Data Retrieval,\224 Tmns of IEICE D-If Vol.J-81 DII No.12,2000 pp 957-966 in Japanese ll S Miyamoto and M Mukitidono 223Fuzzy c means as a regularization and maximum entropy approach,\224 Proc of lFSA\22297 Vol.lI 1997 pp 86 92 121 T K Landauer and S T Dumais 223The latent semantic analysis theory of acquisition induction and representation of knowledge,\224 Psychot Rev Vol 104 N0.2 1997 pp 21 1-240 0-7803-7078-3/0li$l0.00 C IEEE Page 2159 


I I CBC I CNN I Reuters carp statistics I distinctterms 16.5K 44.7K 37.1K 2 COrpuStems 471K 3.6M 1.3M 3 distinct sp's I.2M 5M 3.7M 4 corpussp's 3.9M 28.8M 16.3M Mog Statistics i 5 threshold I 0.002 0.001 I 0.015 IO 11  8 I collected 2.798 3.006 2,699 Figure 7 Pruning for Reuters and CNN corDus I 9 I naive computed rpf's 19.lM 80 47M 70 9 2M 57 zero spJ 22.5M 60.6M 13.6M SHORT-WAM Statistics wlo high prUniag I2 prunedcorpuslerms 45K\(IO O.ZM\(54a O.IM\(78 13 gensp's 14 distinct p3 963K\(77 3 6M\(72 2.IM 57 SHORT-WAM Statistics with high pruning I5 Dmnedcarnustermr 134K\(29 I 1.2M\(32 I O.IM\(71 5M 91s 266M 921 I4'IM 86s portance of high end pruning we implemented two versions of SHORT-WAM one that applies high end pruning and one that does not In the table lines 12 and 15 show the percent age of the comus terms that are pruned with and without while the bottom line was determined by low end pruning Table 3 shows the statistics for the two algorithms when mining for pairs for all three corpora In the table sp stands for sentence pair and corpus sp's is the total number of sen tence pairs in the corpus We count the appearance of a term in a sentence only once In all cases we selected the thresh old so that around 3,000 associations are collected line 8 Pruning eliminates at least 58 of the terms and as much as 8490 for the Reuters corpus line 6 Most terms are pruned from the low end of the distribution high end pruning re moves just 20 terms for the CBC corpus 57 for the CNN corpus and none for the Reuters corpus line 7 The above observations indicate that our theoretical estimates for prun ing may be too conservative To study how pruning varies with corpus size we performed the following experiment We sub-sampled the CNN and Reuters corpora, creating syn theticcollections withsizes Nd  2s,~9,2'0,21',212,213 For each run we selected the threshold so that the percentage of pairs above the threshold \(over all distinct pairs in the cor pus is approximately the same for all runs The results are shown in Figure 7 The x axis is the log of the corpus size while the y axis is the fraction of terms that were pruned Matrix mining improves the performance significantly compared to the naive algorithm that computes the spf val ues for all 7 pairs of the terms that survive pruning line 9 the MATRIX-WAM algorithm computes only a fraction of these maximum 80 minimum 57 line 10 Note however that most of the spf's are actually zero line 1 I The SHORT-WAM algorithm considers only a fraction of pairs that actually appear in the corpus To study the im 16 17  high end pruning Obviously high end pruning is respon sible for most of the removed corpus For the CNN corpus the 57 terms removed due to high end pruning cause 28 of the corpus to be removed The decrease is even more impressive when we consider the pairs generated by SHORT-WAh4 lines 13 16 For the CNN corpus the algorithm generates only S6 of all possi ble corpus sp's ratio of lines 4 and 16 This decrease be comes more important when we mine higher order tuples since the generated pairs will be given as input to the next iteration. Again high end pruning is responsible for most of the pruning of the corpus sp's Finally, our algorithm gener ates at most 72 of all possible disrinct sentence pairs line 17\These pairs are stored in the hash table and they reside in main memory while performing the data pass it is impor tant to keep their number low Note that ApriorinD gen erates all painvise combinations of the terms that survived pruning line 9 threshold pruned terms gensp's 2.4M\(60 16.3M\(56 14.1M\(86 distinct 9p.s 898K 72 3.3M 67 2.IM 57 Table 4 MATRIX-WAM for triples We also implemented the algorithms for higher order tu ples Table 4 shows the statistics for MATRIX-WAM for triples Clearly we still obtain significant pruning Further more, the volume of sentence pairs generated is not large keeping the computation in control We implemented SHORT-WAM for k-tuples for arbitrar ily large k In Figure 8 we plot as a function of the iteration numberi thesizeofthecorpusC figureontheleft well 408 


as the number of candidate tuples and the number of these tuples that survived each pruning phase figure on the right The thresholdis set to0.07 and we mine 8,335 5-tuples Al though the sizes initially grow significantly they fall fast at subsequent iterations This is consistent with the observa tions in 2         e  Figure 8 Statistics for SHORT-WAM 4.5 Sample associations At http://www.cs.toronto.edu/^.tsap~ex~inin~~ere is a full list of the associations. Table 5 shows a sample of as sociations from all three corpora that attracted our interest  deutsche telekom hong kong chevron fexaco department justice mci waddcom aol warner france telecom greenspan tar oats quaker chap ten indigo nestle putina oil opec books indigo leaf maple states united germany west arahia Saudi gas oil exxon juzy capriati hingis chaleau empress frontmac indigo reisman schwmz del monte sun-rype cirque du soleil bribery economics scandal fuel spills lanker, escapes hijack yemen SI hall mcguire baker james secretary, chancellor lawson nigel community ec eumpean arahia ope Saudi chief executive off cer child famering jesse ncaa seth tournament eurabond issuing priced falun gong self-immolation doughnuts kreme krispy laser lasik vision leaf maple schneider Mpls Table 5 Sample associations 5 Conclusions In this paper we introduced a new measure of interest ingness for mining word associations in text and we pro posed new algorithms for pruning and mining under this non-monotone measure We provided theoretical and em pirical analyses of the algorithms The experimental evalua tion demonstrates that our measure produces interesting as sociations and our algorithms perform well in practice We are currently investigating applications of our pruning tech niques to other non-monotone cases Furthermore we are interested in examining if the analysis in Section 4.1 can he applied IO other settings References I R Agrawal T Imielinski A N Swami Mining Associa tion Rules between Sets of Items in Large Databases SIGMOD 1993  R Agrawal R Srikant. Fast Algorithms for Mining Associa tion Rules in Large Databases VLDB 1994 31 H Ahonen 0 Hein0nen.M. Klemettinen A Inkeri Verkamo Applying Data Mining Techniques for Descriptive Phrase Ex traction in Digital Document Collections ADL 1998 41 R Bayardo R Agrawal, D. Gunopulos, Constraint-based rule mining in large, dense databases ICDE 1999 5 S Brin R Motwani I D Ullman S Tsur. Dynamic ltemset Counting and Implication Rules for Market Basket Data SIG MOD 1997 6 S Brin R Motwani C Silventein Beyond Market Bas kets Generalizing Association Rules to Correlations SIGMOD 1997 I71 E Cohen M Datar S Fujiwara A Gionis P Indyk R Motwani I Ullman, C Yang Finding Interesting Associations without Support Pruning ICDE 2ooO 181 D.R Cutting D Karger I Pedenen and J.W Tukey. Scat tedGather A cluster-based approach to browsing large docu ment collections 15th ACM SIGIR, 1992 91 W DuMouchel and D Pregibon, Empirical Bayes Screening for Multi-Item Associations KDD ZOO1 IO R Feldman 1 Dagan and W Klosgen. Efficient algorithms for mining and manipulating associations in texts 13rh Euro pean meering on Cybernaricsand Systems Research 1996 Ill R Feldman W Klosgen and A Zilberstein Document ex plorer Discovering knowledge in document collections IOrh lnurnarional Symposium on Methodologies for Inrelligenr Sys rems Springer-Verlag LNCS 1325,1997 1121 R Feldman I Dagan H Hirsh Mining text using keyword distributions Journal of Intelligent InJormation Systems IO 1998 13 B.Lent R AgrawalandR.Srikant Discoveringtrends in text databases KDD, 1997 I41 D.D Lewis and K Sparck Jones Natural language pro cessing for information retrieval Communicorions of the ACM 39\(1 1996.92-101 I51 A I Lolka The frequency distribution of scientific produc tivity J of the Washingron Acad of Sci 16:317, 1926 I161 H Mannila and H Toivonen Discovering generalized episodes using minimal occurrences KDD 1996 171 C Manning and H Schulze Foundalions oJSratistical Nar ural Language Processing 1999 The MIT Press, Cambridge MA I 81 E. Riloff Little words can make a big difference for text clas sification 18th ACM SIGIR, 1995 1191 E Smadja. Retrieving collocationsfrom text Xtract Compu rariomILiflguisrics 19\(1 1993 143-177 I201 G Webb Efficient Search for association rules KDD 2000 21 1 Witten A.Moffat andT Bell Managing Gigabytes Mor 1221 G K Zipf Human behavior and the principle of least effort gan Kaufman, 1999 New York Hafner 1949 409 


Figure 3 Alternatives to this paper's ZIT2 timing not mnsidered here a Measurements are taken at random times b A more sophisticated periodic scheme with inter-scan intervals TI T2 and T3 Figure 1  Illustration of uniform and nonuniform sampling timing schemes l F  I Figure 2 Possible tig of measurements from two F igUre 4 Overlay of true and false detections from two con secutive scans Notice that the false measurements are unlikely to coincide However given that the inter-scan time is not to great the true measure ments are close chronous but equal-period sensors At fusion cen ter the effect is the same as in Figure 1 4-1605 


Figure 5 Notional uncertainties with uniform and nonuni form sampling In the former case the uncer tainties \(illustrated by covariance ellipses are rel atively constant in size In the latter nonuni form case the ellipses increase significantly in size between pairs of samples and allow more false alarms to enter Figure 6 Representation of the one-step tracking as per formed by the PDAF Figure 7 Procedure of track split filter a Uniform sampling scheme  Nom sampling shew 0.W OW 0.W 0.WB 0.01 0.012 0.014 0.016 0.018 Fabe Alarm Density 2 Figure 8 Uniform and nonuniform sampling schemes in PDAF with Tl  O.ls T2  1.9s and T  1s PD  0.9 4  10 0  1 4-1606 


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


