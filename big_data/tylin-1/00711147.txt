Associative Memory System Using Fuzzy Sets Yukinori Suzuki Muroran Institute of Technology Department of Computer Science & Systems Engineering 27-1, Mizumoto-cho, Muroran 050-8585, Japan yuki@csse.muroran-it.ac.jp N orio Konno Yokohama National University Department of Applied mathmatics, Faculty of Engineering 79-5, Tokiwadai, Hodogaya, Yokohana, Japan norio@mathlab.sci.ynu.ac.jp Junji Maeda Muroran Institute of Technology Department of Computer Science & Systems Engineering 27-1, Mizumoto-cho, Muroran 050-8585, Japan yuki@csse.muroran-it.ac.jp Abstract  An associative memory \(AM presented in this paper.  Vectors to be stored in the AM are transformed into fuzzy sets.  Fuzzy rules are constructed from the fuzzified vectors to associate vector when a stimulus vector is input to the AM system.  Computational experiments are carried out to evaluate performance of the AM system.  The experiments show that the AM system is responsive to feature of the stimulus vectors.  In addition the AM system performs more than 95% correct association rate until 20% of the binary valued elements of the stored vectors are flipped randomly from either zero or one 1.  Introduction An associative memory \(AM whole area of a memory system, unlike a standard computer memory system where information corresponds to a specific address \(Kohonen, 1972 and 1987\This distributive memory system makes AM tolerant to noisy and/or partial input, and it is most attractive features for applying the AM to real-world problems As we intend to apply the AM systems to real-world problems such as pattern recognition, data mining, image processing, and so on, relevant available information involves imprecision, uncertainty, and partial truth.  We are looking for a design method of the AM system which is tolerant to the problems related to the real-world.  Zadeh introduced soft computing \(SC imprecision, uncertainty, and partial truth, and approximation to achieve tractability, robustness, low solution cost and better rapport of reality \(Zadeh, 1997 In this paper, we will present a design method of an AM system using fuzzy sets.  In the AM systems, the imprecision, the uncertainty, and the partial truth of the given information are included into membership functions of fuzzy sets, and fuzzified information is stored in the AM system.  The paper is organized as follows.  Procedures to form  fuzzy stimulus and response vectors are described in section 2.  In section 3, association rules to associate a stimulus vector with response vector is introduced.  In section 4, the performance of the AM system is evaluated by 


computational experiments 2.   Transforming stimulus and response vectors into fuzzy sets We assume that elements of vectors are binary values as either zero or one.  We transform stimulus vectors to be stored in the AM system into fuzzy set based on the following idea:  when we observe an element of the binary valued as zero, we cannot negate the possibility that the observed value is one by an uncertainty and/or imprecision related to the real-world problems.  The converse is also true.  This transformation enables us information processing while maintaining the possibility of the binary value which is not observed A binary value of an element of stimulus vector is transformed into the fuzzy set as  304         304 Axxjp i j Al l l L i j   345 m 0 1 012  where m 304  Al i j x is the membership function and xl L l      012 1 is a discrete value between 0.0 and 1.0 \(Mizumoto, 1989 m 304  Al i j x is determined as follows:  if the value of the A i j is zero, a grade of membership function is 1.0 at x l  00  and decreases linearly along the hypotenuse of the rightangled triangle until the grade becomes zero at xx l  a  On the other hand, if  the A i j is one, the grade of the membership function is 1.0 at x l  10  and decreases linearly along the hypotenuse of the right-angled triangle until the grade becomes zero at xx l  b  The elements of the response vector to be stored in the AM system are transformed into fuzzy sets according to the following idea.  We assume that if an element at a location occurs as either zero or one at a low rate of frequency for all stimulus vectors stored in the AM system, the value of the element at that location is essential.  Conversely, if an element at a location occurs as either zero or one at a high rate of frequency, the value of the element at that location is not essential.  The fewer the occurring frequency of the value of the element, the more essential.  A right-angled triangle is used to form the membership function which is right-angle triangle formed.  We determined shape of the triangle as follows.  If the element of the response vector is essential, uncertainty of the membership function is small On the other hand, the element is not essential, uncertainty of the membership function is large.  The more essential the element of the response vector, the larger the membership function.  Therefore, the length of the base of the triangle is given as   length n n  242 where 242 n is the occurring frequency of the element at a location as either zero or one n is the number of stimulus vectors stored in the AM system.  The height of the triangle is also given as 1 242 n  3.  Association Rules Association rules consists of an exciting rule and inhibiting rule.  The association rule generates fuzzy output vector when a stimulus input vector is given to the AM system The exciting rule gives a large grade of membership function in the area around 1.0 of the element of the fuzzy output vector, when an element of an incoming vector is almost the same as the element of the stored vector in the AM system.  On the other hand, the inhibiting rule gives a large grade of membership function in the area around 0.0 of the element of the fuzzy output vector, when an element of input vector is different from element of the stored vectors We made the exciting AM rule using Cartesian product 


between each element of stimulus and response vectors to be stored in the AM.  To make an inhibiting rule, element of each stimulus and response vectors stored in the AM system is flipped either zero or one or vice versa.  The inhibiting AM rule is made from these flipped stimulus and response vectors by the same procedures to make exciting AM rules When a stimulus vector is input to the AM system, the AM rules generate n fuzzy vectors, respectively.  The membership function of each element of the output vector is composed by max-min operation.  If the present vector is close to one of the stored vector, the grade of membership function of the elements of the output vector will be larger in the area around 1.0 than the area around 0.0.  Converse is also true The membership functions of all elements of the output vectors are summed and normalized to produce averaged output vector.  Then, we compute a centroid of averaged membership function which gives the similarity between the presented vector and vectors stored in the AM system.  The AM system chooses the largest centroid out of n centroids each of which gives the similarity between the presented vector and n stored vectors.  The vector showing the largest centriod is associated with the input vector 4.  Performance of the AM system Performance of the AM system was evaluated by computational experiments.  Alphabetical characters consists of 64 elements are prepared for the experiments These characters are stored in the AM system as stimulus vectors Since C, O, and G are very similar patterns when  they are represented in 64 size, it is difficult to distiguish pattern C from O and it is also difficult to distiguish pattern C from G.  We flipped one elemet of C which is part of elements composing the pattern O.  Since the rate of ocurring frequency of the element is low, the element is an essential element of the pattern O.  The AM system associates vector pattern O, however the AM with correlation matrix method CMM\We also flipped two elements of patern C which are essential elements to represent pattern G.  The AM system associates pattern G, but the AM with CMM associates pattern C.  The same experiment is carried out using similar pattern O, G, and Q.  The AM system associated correct pattern when a part of essential elements are flipped.  In the similar pattern such as P and R, the AM system associated correct pattern.  These experiments show that the AM system is robust  when the essential elements are flipped.  We speculate that this robustness is advantage to use the AM system for real-world problems References T. Kohonen \(1972\Trans IEEE Computers, vol. 21, No. 4, pp.353-359 T. Kohonen \(1987\ganizing and associative memory", New York: Springer-Verlag L.A. Zadeh \(1997 puting, vol. 1/1, pp.1-2 M. Mizumoto \(1989 Tokyo: Science Publisher\(in Japanese 


H\(g A wIW E T\(g and w 2 f lc is incremented Thus after considering all of the records in the database the supports for all of the conjunctions in the candidate sets of each of the groups in G have been computed The purpose of collecting this support information is to maximise the pruning opportunities as described in section 2.7 2.6 Extract Function Extract-rules extracts from the set of evaluated groups rules that can potentially satisfy the constraints these rules are added to the set of rules R For each g E G the rules that are considered are A rule can potentially satisfy the constraints and is H\(g C v t\200 T\(g added to R iff AND sup\(H\(g t c sup\(H\(g t A c 2 min sup conf\(H\(g c suP\(H\(g t A 4 2 Conf sup g  t  AND conf\(H\(g 3 c g 2 minlmp  where Cmax\(g is the maximum confidence of any rule represented by the head of an ancestor of g or g itself The minlmp constraint is fully enforced in the post processing stage as described in SI 2.7 Prune Function The pruning function is applied to a set of groups after evaluation and after a new level of the SE-Tree has been created A group g is said to be pruneable if it can be determined that no rule derivable from g can satisfy the constraints If g is pruneable and is a member of G then g can be removed from G If a group g is not pruneable it may be possible to remove some ATs from its tail A new group g can be formed from g by moving a single AT v from the tail into the head If g is pruneable then no rule derivable from g that includes v can satisfy the constraints and v can be removed from the tail of g If some ATs are removed from the tail of g then tighter bounds for pruning can be achieved Therefore the pruning function is re applied until either the whole group is pruned or no tail ATs are removed 2.7.1 Pruning based on support If sup\(H\(g  A c  min Sup then no rule derivable from g can meet the minSup constraint and g is pruneable The value sup\(H\(g c is available from the candidate set of g where g is the last evaluated ancestor of g 2.7.2 Pruning based on confidence Let r be the rule with maximum confidence that is derivable from a group g The maximum confidence of any rule derivable from g is max Conf\(g  conf\(r If max Conf\(g  min Conf then g is pruneable maxConf\(g can be expressed as maxConf\(g X Y where x  sup\(r and y  sup  c sup\(r  maxConf\(g is monotonically increasing in x and monotonically decreasing in y Therefore x may be replaced with an upper bound x and y with a lower bound X X y then maxConf\(g  x'+y When attempting to prune an evaluated group Dense Miner uses the bounds x sup\(H\(g c and y y where y  sup\(H\(g AT\(g ARA however uses the maxATs constraint It is then possible to calculate another lower bound on y y2 Lemma I y2 is a lower bound on y  sup  c sup\(r where y2  sup\(H\(g A 1c sup\(H\(g A d A 7c where S comprises the n tail ATs t of T\(g with the highest values of sup\(H\(g A TC and n  min\(rnaxATs IH\(gl,lT\(g The proof is initially given for the general case y I sup\(u  c sup\(u where U is any rule derivable from a group g as this is required for improvement pruning and then for the particular case y2 I sup  c sup\(r I\200 s Proof Rule U may be written as H\(g M a conjunction of ATs from T\(g Then c  where M is sup\(u  c sup\(u  SUp\(H\(g M SUp\(H\(g M A C  SUp\(ff\(g M A iC  sUp\(H\(g 1C sUp\(H\(g iM A IC NOW M m Am A...Am and 44 rn Am2 A...A lm vlm2 v...vlm 468 


Since sup\(u v b sup\(u sup sup\(a b it follows that I sup\(a sup\(b xSUp\(H\(g SUp\(H\(g A-IC l\200M In the presence of maxATs lMll min\(m,ATs-\(H\(gl,lT\(gI IS1 Given that S comprises the n tail ATs with the highest values of sup\(H\(g and ISI2IMI it follows that xsuP\(H\(g r A1 c 2 xsup  Ay c 1E s l\200M Therefore SUp\(ff\(g C H\(g I I A I C I\200 s I SUp\(H\(g IC SUp\(H\(g 4 A IC Hence SUp\(H\(g IC x SUp\(H\(g i t A I C l\200S I sup\(u  c sup\(u and y I sup\(u  c sup\(u Therefore since r is a rule derivable from g y2 2 sup\(r  c sup  y  When attempting to prune an evaluated group the supports are available to calculate x and both y1 and y2 y may then be set to max{y y2 for use in the pruning function. However when considering a group g that is not evaluated it may not be possible to calculate x'or y1 or y2 Then, since x'is an upper bound it may be set to sup\(H\(h c where h is the last ancestor of g to have this value available in its candidate set since sup\(H h A C 5 sup\(H\(g  A c It is also possible to calculate values y and y such that y I y,and y I y The values yl and y2 are lower bounds on y therefore so are y and y and so y may be set to When y1 cannot be calculated, Dense Miner calculates m~b:lY;l SUPb g  A T\(g A 4 y  max sUp\(H\(g tsTf8'0 xSUp\(ff\(g where g is the. parent of g in the SE-Tree Both of these values are less than or equal to yl and can be calculated from the supports of the conjunctions in the candidate set of g  ARA with its larger candidate set can often find a higher value for yi and also a value for y  Let g be a group that is derived from an evaluated group g Two cases are considered; case 1 when H g  H\(g i.e. some ATs have been removed from the tail of g to create g and case 2 when H\(g  H v where v is a single AT from T\(g i.e g is either a group created for the purpose of tail pruning or g is in the next level of the SE-Tree from g Case1 H\(g  H\(g y  SUp\(H\(g A T\(g c then ARA calculates y;=sup\(H\(g A{w I wcT\(g 5 A~c where p is the minimal AT in T\(g the ordering 2  Provided that the ordering of g is the same as g yi is less than or equal to yI The issue of tail ordering and how the ordering can be maintained is discussed in section 2.8 sup\(H\(g wI w~T\(g r]Alc is If the supports are not available to calculate 8 available for every t E T\(g since these conjuncts are members of the candidate set of g and is therefore available for the minimal AT p y  sup\(Hfg A IC C sup\(H\(g A It A Tc are available from the candidate set of H\(g since H\(g The supports necessary to calculate rs S  H\(g case 2 H\(g  g v yI  sup\(H\(g A T\(g Then ARA calculates y  sup\(H\(g A w I WE T\(g  w r U A IC where U is the minimal AT in T\(g under the ordering 2  y is less than or equal to yl provided that the ordering of g is the same as g sup\(H\(g A{wI weT\(g  tj~~c is available for every t E T\(g v since these conjuncts are members of the candidate set of g and is therefore available for the AT U Again it may not be possible to calculate 469 


It may also not be possible to calculate y2  sup\(H\(g A 7c c sup\(H\(g A d A lc However sup\(H\(g A c  sup\(H\(g   v A 7c and for every t E T\(g Therefore I\200 s sup\(H\(g lt A 1c sup\(H\(g lt A c sup\(H\(g   A v A 1c c sup\(H\(g   A lf A lc I sup\(H\(g A 1c c sup\(H\(g A d A c y2 where S consists of the IS1 tail ATs of g with the maximum values of sup\(H\(g  d A lc ARA therefore calculates t\200 S I\200 s The support values necessary to calculate yi are available from the candidate set of g 2.7.3 Pruning based on improvement Let r be the rule derivable from a group g with the maximum improvement Then imp\(r maxImp\(g If maxlmp g I minlmp then g is pruneable In order to find an upper bound on maxImp\(g the same methods as Dense Miner are used However because the value of y calculated for confidence pruning is re-used and may be higher in ARA than in Dense Miner the upper bound on maxImp\(g may be lower The methods are presented below without proofs These can be found in 51 or 71 Method 1 maxZmp\(g I maxConf\(g conf\(r where r is the highest confidence rule enumerated in any ancestor group of g X X Method 2 maxlmp g I   x'+y x'+y'+/l where x  rnaxkin sup rnin\(sup\(H\(g c J y is defined above and B I sup\(\(H\(g Z lz A lc where z is the AT in H\(g that minimises this expression In order to be used in improvement pruning y must be a lower bound on sup c sup where r is the rule derivable from g with the maximum improvement. It was shown in lemma 1 that y'l sup\(u  c sup\(u where U is any rule derivable from g Therefore y is a lower bound on sup\(r  c sup\(r Example of pruning Let A I B,C D E F,G H be an evaluated group g Assume that g is not pruneable therefore try to remove some tail ATs To do this we form a group g A A B I C D E F,G H It is found that no rule derivable from g can satisfy the constraints, i.e g is pruneable and so B can be removed from the tail of g After considering each of the tail ATs in the same way it is found that ATs B and D may be removed from the tail of g We form a new group g AI{c,E,F,G,H which replaces g in the SE-Tree Now because some ATs have been removed from the tail and tighter bounds may be available for y an attempt is made to prune g When attempting to prune gl and when attempting to prune ATs from the tail of gl ARA will use the value sup A c A D A E A F A G A H A c as a value for y I y sup A c A E A F A G A H A~C which is potentially higher than sup\(A A B A C A D A E A F A G A H A 7c as used in Dense Miner. Also y2 is greater than or equal to the other value used in Dense Miner Assume it is found that g is not pruneable and no ATs can be removed from its tail A new set of groups in the next level of the SE-Tree are created which replace gl Amongst these is the group h  A A F I G H Now in determining if h is pruneable ARA has available and will use y  sup\(A A F A G A H A lc Dense Miner will use sup\(A BA C A DA E A F AG A H A~c which is likely to be lower. Again yz is greater than or equal to the other value used in Dense Miner sup\(H\(g 1c csup\(H\(g  11 A 7 c This example shows how tighter bounds can be achieved for pruning using the additional support information collected by ARA If the maxATs constraint is specified even tighter bounds can be achieved IS Tfx 2.8 Tail ordering As described in 5 tail ordering is essential for efficient algorithm performance ARA requires that the tail of a group g is ordered before evaluation and the order maintained in groups derived from g until pruning is complete Therefore the vector of tail ATs is sorted before a group is evaluated The ordering used to sort the vector of tail ATs of a group g is descending value of sup\(H\(g A 7c g is the last evaluated ancestor of g An initial evaluation is carried out as the basis for sorting the tail of the root group. For clarity this has not been shown in Figure 2-2 470 


2.9 Results ARA was evaluated on two databases; the connect 4 database 9 and a census database IO Due to limitations of space only the results of the connect 4 tests are reported here The results achieved using the census data base were similar and are presented in 7 Connect 4 has 65,577 records and 42 predictive attributes each of which is categorical with 3 possible values, the consequent was defined as 223result  draw\224 of which there are 6,449 records The algorithm was evaluated according to the following measures Execution Time. Times are for the search phase only, post processing time is not included Number of groups evaluated. This represents the size of the search tree Number of tail ATs evaluated. This is required because a small number of groups with large tails may be more time consuming to evaluate than a large number of groups with small tails In order to compare ARA with Dense Miner a version of Dense Miner was implemented in accordance with the description given in 5 this version of Dense Miner is referred to as ARA-Dense ARA-Dense is identical to ARA in all respects except for the support information collected and the pruning functions any variation in performance can thus be attributed to these differences alone The execution times for ARA-Dense were compared with the times for Dense Miner reported in 5 and found to be slower by a factor of 1.5  1.8 This variation may be attributable to hardware or compiler differences or the detail of implementation All tests were carried out on a Dell Dimension XPS PI11 450 with 128MB memory 1 2 3 2.10 Commentary on test results When rninConf was set to zero it was found that there was no significant difference in the execution times of the two algorithms over a wide range of tests; indicating that the advantage of a smaller search tree was offset by the overhead for collecting more information Figure 2.1 shows the effect of using confidence for pruning by varying the values specified for rninConf The size of tree searched by ARA was smaller than ARA-Dense by a factor of 1.2 to 2.5 the higher the setting of rninConf the greater the reduction in tree size. The number of tail items evaluated varied by a similar order Execution time also varied in proportion to the size of the search tree indicating that the overhead for collecting the additional support information was outweighed by the benefit of a smaller tree when confidence was used for pruning The second set of tests, reported in figure 2.2 shows the effect of introducing the rnuxATs Constraint In order to demonstrate the effect of the maxATs constraint a further algorithm ARA-Part was implemented This is identical to ARA but does not include the additional pruning functionality based on the maxATs constraint ARA-Dense and ARA-Part simply stop searching when the number of ATs in the consequent of extracted rules is equal to maxATs The improvement of ARA-Part over ARA-Dense is thus attributable to the additional support information collected The improvement of ARA over ARA-Part is attributable to the additional pruning using the dTs constraint When muxATs is lower than 11 the maximum antecedent length of any rule that satisfies the other constraints there are considerable benefits to using the constraint For example with muxATs set to 6 the execution times for ARA and ARA-Part were 149 and 493 seconds respectively a saving of over 5 minutes. The time for ARA-Dense was 818 seconds which is over 5 times or 11 minutes longer It is concluded that when minConf is specified ARA generally outperforms ARA-Dense When the dTs constraint is specified further significant gains in performance can be achieved 3 Future Work An attribute test in ARA is a 3 tuple attribute operator  where operator is 223=\224 However it is straightforward to construct ATs with different operators For example an AT could be 223age c 50\224 or 221\221colour  red\224 allowing a richer variety of rules to be constructed A problem with increasing the variety of ATs is that the search space and the number of discovered rules may be vastly increased To counteract this additional constraints may be necessary. However these could be problem specific rather than algorithm specific For example, one possible constraint is to define which ATs or combinations of ATs are of interest or conversely which are not of interest\before the search is undertaken As a result some generality of the search will be lost but the search can be targeted at areas of interest, thus making the problem more tractable and avoiding rules that are not likely to be interesting One of the problems of finding all rules is specifying suitable values for rninCov and rninConf In this research we have experimented with the use of the heuristic search techniques in the DataLamp package developed by the University of East Anglia and the Lanner Group ll DataLamp has been used to find a good rule according to its fitness function then ARA used to find all rules with coverage and confidence greater than that rule 47 1 


04 I 250,000  B 200.000  2  100,000  n 5 50,000  A  z UY 150,000     4\222 f 221 07 225 20 30 40 50 60 minConf  I 7,000,000 ui 6,000,000 I 4.000.000 c,j 3,500,000  2 3,000,000  Z 2,500.000     c 5 2,000.000  7  0  1,500,000  2 500,000    n E i,ooo,ooo  L  07 1 4 OC I 20 30 40 50 60 minConf  I  2,500 h 0 g 2,000 g 1,000 Y E 1.500  c  e 8 500 m   maxATs  42 all minSup=65 minlmp=0.02 Figure 2-1 Varying minConf 4 References  11 Agrawal R Imielinski T Swami A Mining association rules between sets of items in large databases Proceedings of the ACM SIGMOD International Conference on Management of Data May 1993 pp 207  216 ACM Press 1993 2 Agrawal R Srikant R Fast Algorithms for Mining Association Rules Proceedings of the 20th International Conference on VLDB September 1994 pp 487  499 Morgan 3 Agrawal R Imielinski T Swami A Mining association rules between sets of items in large databases Proceedings of the ACM SIGMOD International Conference on Management of Data May 1993 pp 207  216 ACM Press 1993 4 Houtsma M Swami A Set Oriented Mining of Association Rules in Relational Databases Proceedings of the Eleventh International Conference on Data Engineering March 1995 pp 5 Bayardo R Agrawal R Gunopulos D Constraint-Based Rule Mining in Large Dense Databases Proc of the 15th 222 Kaufmann 1994 25  33 IEEE 1995 1,400 7 minSup  65 minConf  40 minlmp  0.02 Figure 2-2 Varying maxATs International Con on Data Engineering March 1999 pp 188  197. IEEE, 1999 6 Bayardo R Agrawal R Mining the Most Interesting Rules Proceedings of the 5th International Conference on Knowledge Discovery and Data Mining KDD 99 August 1999 pp 145  153 AAA1 Press 1999 7 Richards G Mining All Rules University of East Anglia Technical report SY S-COO 10 8 Rymon R Search Through Systematic Set Enumeration Proceedings of the 3rd International Conference on Principles of Knowledge Representation and Reasoning KR 92 October 1992 pp 539  550 Morgan Kaufmann 1992 9 Connect 4 data base http://www.ics.uci.edu mlearn/MLSummary.html IO Census data base http://kdd.ics.uci.edu databaseslcensus income/census-income. html  1 I Lanner Web Site http://www.lanner.coml solutions/datamining html 472 


local support count X.sup must be smaller than the local threshold s x D Following from the discus sion in Subsection 3.3 X.supq is bounded by the value min\(maxsupq X s x D  1 Hence an upper bound of X.sup can be computed by the sum x.supj  jEX.large-sites 2 min\(mazsupq\(X s x Dq  1 q=l q+?X.large-sites In FDM-LPP Si calls p-upper-bound to compute an upper bound for X.sup according to the above for mula This upper bound can be used to prune away X if it is smaller than the global support threshold 0 As discussed before both FDM-LUP and FDM LPP may have less candidate sets than FDM-LP How ever they require more storage and communication messages for the local support counts Their efficiency comparing with FDM-LP will depend largely on the data distribution 5 Performance Study of FDM An in-depth performance study has been performed to compare FDM with CD We have chosen to im plement the representative version of FDM FDM LP and compare it against CD Both algorithms are implemented on a distributed system by using PVM Parallel Virtual Machine 6 A series of three to six RS/6000 workstations running the AIX system are connected by a 10Mb LAN to perform the experi ment The databases in the experiment are composed of synthetic data In the experiment result the number of candidate sets found in FDM at each site is between 10  25 of that in CD The total message size in FDM is between 10  15 of that in CD The execution time of FDM is between 65  75 of that in CD The reduction in the number of candidate sets and message size in FDM is very significant The reduction in execution time is also substantial However it is not directly proportional to the reduction in candidate sets and message size This is mainly due to the overhead of running FDM and CD on PVM What we have ob served is that the overhead of PVM in FDM is very close to that in CD even though the amount of mes sage communication is significantly smaller in FDM From the results of our experiments it is also clear that the performance gain of FDM over CD will be higher in distributed systems in which the commu nication bandwidth is an important performance fac tor For example if the mining is being done on a distributed database over wide area or long haul net work The performance of FDM-LP against Apriori in a large database is also compared. In that case the response time of FDM-LP is only about 20 longer Interpretation transaction mean size mean size of maximal potentially large itemsets number of potentially large itemsets Number of items Clustering size Pool size Correlation level Multiplying factor Parameter ITI III ILI N sq Ps Mf Cr Value 10 4 2000 1000 5-6 50  70 0.5 1260  2400 Table 5 Parameter Table than 1/n of the response time of Apriori where n is the number of sites This is a very ideal speed-up In terms of total execution time FDM-LP is very close to Apriori The test bed that we use has six workstations Each one of them has its own local disk, and its partition is loaded on its local disk before the experiment starts The databases used in our experiment are synthetic data generated using the same techniques introduced in 2 lo The parameters used are similar to those in lo Table 5 is a list of the parameters and their values used in our synthetic databases Readers not familiar with these parameters can refer to 2  In the following we use the notation Tx.Iy.Dm to denote a database in which D  m in thousands IT1  x and 111  y T10.14.D200K s  3 4 5 6 Number of Nodes FDM CD Figure 1 Candidate Sets Reduction n  3 4 5 6 5.1 Candidate Sets and Message Size Re duction The sizes of the databases in our study range from 200K to 600K transactions and the minimumsupport threshold ranges from 3 to 3.75 Note that the number of candidate sets at each site are the same in CD and different in FDM In our experiment we witnessed a reduction of 75  90 of candidate sets on 39 


T10.14.D200K, n  3 T10.14.D200K, n  3 60  I S 8 3.00 3.25 3.510 3.75 YO  I YO Minimum support FDM kCD  gs 3.00 3.25 3.50 3.75 Minimum support FDM CD Figure 4 Message Size Reduction Figure 2 Candidate Sets Reduction average at each site when FDM-LP is compared with CD In Figure 1 the average number of candidate sets generated by FDM-LP and CD for a 200K transaction database are plotted against the number of partitions FDM-LP has a 75  90 reduction in the candidate sets The percentage of reduction increases when the number of partitions increases This shows that FDM becomes more effective when the system is scaled up In Figure 2 the same comparison between FDM-LP and CD is presented for the same database with three partitions on different thresholds In this case, FDM LP experienced a similar amount of reduction T10.14.D200K s  30/0 I 150 100 50 0 3 4 5 6 Number of Nodos FDM CB Figure 3 Message Size Reduction n  3 4 5 6 The reduction in candidate sets should have a pro portional impact on the reduction of messages in the comparison Moreover as discussed before the polling site technique guarantees that FDM only requires O\(n messages for each candidate set which is much smaller than the O\(n2 messages required in CD In our experiment FDM has about 90 reduction in the total message size in all cases when it is compared with CD In Figure 3 the total message size in FDM and CD for the same 200K database are plotted against the number of partitions In Figure 4 the same compari son on the same database of three partitions with dif ferent support thresholds are presented Both results confirm our analysis that FDM-LP is very effective in cutting down the number of messages required T10.14.D200K s  3 90 E3 28  U 70 cc Q 8 a c 50 c xs w  I 3 4 5 6 Number of Nodes FDM CD Figure 5 Execution Time n  3 4 5 6 T10.14.D200K n  3 3.00 3.25 3.50 3.75 Minimum Support E-FDM A-CD Figure 6 Execution Time 5.2 Execution Time Reduction We have also compared the execution time between FDM-LP and CD The execution time of FDM-LP and CD on a 200K database are plotted against the number of partitions in Figure 5 FDM-LP is about 40 


25  35 faster than CD in all cases In Figure 6 the comparison is plotted against different thresholds for the same database on three partitions Again FDM LP is shown to have similar amount of speed-up as in Figure 5 n  3 D  60011 s  2 I Apriori I FDM-LP response time sec I 1474 I 387 I total execution time sec I 844.7 I 842.9 I Table 6 Efficiency of FDM-LP We have also compared FDM-LP on three sites against Apriori with respect to a 600K transactions database in order to find out its efficiency in large database The result is shown in Table 6 The re sponse time of FDM-LP is only slightly 20 larger than 1/3 of that of Apriori In terms of the total ex ecution time FDM-LP is very close to Apriori For a large database FDM-LP may have a bigger portion of the database residing in the distributed memory than Apriori Therefore it will be much faster than running Apriori on the same database in a single ma chine This shows that FDM-LP on a scalable dis tributed system is an efficient and effective technique for mining association rules in large databases The performance study has demonstrated that FDM generates a much smaller set of candidate sets and requires a significantly smaller amount of mes sages when comparing with CD The improvement in execution time is also substantial even though the overhead incurred from PVM prevents FDM from achieving a speed-up proportional to the reduction in candidate sets and message size Even though we have only compared CD with FDM-LP there is enough evidence to show that FDM is more efficient than CD in a distributed environment In the follow ing sections we will discuss our future plan of imple menting the other versions of FDM 6 Discussions In this discussion we will first discuss the issue of possible extension of FDM for fast parallel mining of association rules Following that we will discuss two other related issues 1 the relationship between the effectiveness of FDM and the distribution of data and 2 support threshold relaxation for possible reduction of message overhead The CD and PDM algorithms are designed for share-nothing parallel environment. In particular CD has been implemented and tested on the IBM SP2 machine In designing algorithm for parallel mining of association rules not only the number and size of messages required should be minimized but also the number of synchronizations which is the number of rounds of message communication CD has a simple synchronization scheme It requires only one round of message communication in every iteration Besides the second iteration PDM also has the same synchro nization scheme as CD If FDM was used in the paral lel environment it has a shortcoming even though it requires much less message passings then CD it needs more synchronizations However FDM can be modi fied to overcome this problem In fact in each itera tion the candidate set reduction and global pruning techniques can be used to eliminate many candidates and then a broadcast can be used to exchange the local support counts of the remaining candidates This ap proach will generate less candidate sets than CD and has the same number of synchronization Therefore it will perform better than CD in all cases Performance studies has been carried out in a 32-nodes IBM SP2 to study several variations of this approach and the result is very promising Another interesting issue is the relationship be tween the performance of FDM and the distribution of the itemsets among the partitions From both The orem 1 and Example 1 it is clear that the number of candidate sets decreases dramatically if the distribu tion of itemsets is quite skewed among the partitions If most of the globally large itemsets were locally large at most of the sites the reduction of candidate sets in FDM would not have been as significant In the worst case if every globally large itemset is locally large at all the sites the candidate sets in FDM and CD will be the same Therefore data skewness may improve the performance of FDM in general Special partitioning technique can be used to increase the data skewness to optimize the performance of FDM Some further study is required to explore this issue The last issue that we want to discuss is the pos sible usage of the relaxation factor proposed in ll In FDM if a site sends not only those candidate sets which are locally large but also those that are almost locally large to the polling sites the polling sites may have local support counts from more sites to perform the global pruning of candidate sets For example if the support threshold is lo every site can send the candidate sets whose local support counts exceed 5 to their polling sites In this case for some candi date sets their polling sites may receive local sup port counts from more sites than the no relaxation case Hence the global pruning may be more effec tive However there is a trade-off between sending more candidate sets to the polling sites and the prun ing of candidate sets at the polling sites More study is necessary on the detailed relationship between the relaxation factor and the performance of the pruning 7 Conclusions In this paper we proposed and studied an efficient and effective distributed algorithm FDM for mining association rules Some interesting properties between 41 


locally and globally large itemsets are observed which leads to an effective technique for the reduction of can didate sets in the discovery of large itemsets Two powerful pruning techniques local and global prun ings are proposed Furthermore the optimization of the communications among the participating sites is performed in FDM using the polling sites Sev eral variations of FDM using different combination of pruning techniques are described A representative version FDM-LP is implemented and whose perfor mance is compared with the CD algorithm in a dis tributed system The result shows the high perfor mance of FDM at mining association rules Several issues related to the extensions of the method are also discussed The techniques of can didate set reduction and global pruning can be inte grated with CD to perform mining in a parallel envi ronment which will be better than CD when consider ing both message communication and synchronization Further improvement of the performance of the FDM algorithm using the skewness of data distribution and the relaxation of support thresholds is also discussed Recently there have been interesting studies on the mining of generalized association rules multiple level association rules quantitative association rules etc Extension of our method to the min ing of these kinds of rules in a distributed or parallel system are interesting issues for future research Also parallel and distributed data mining of other kinds of rules such as characteristic rules 7 classification rules, clustering 9 etc is an important direction for future studies For our performance studies an im plementation of the different versions of FDM on an IBM SP2 system with 32 nodes has been carried out and the result is very promising References l R Agrawal and J C Shafer Parallel mining of association rules Design implementation and experience In IBM Research Report 1996 2 R Agrawal and R Srikant Fast algorithms for mining association rules In Proc 1994 Int Conf Very Large Data Bases pages 487-499 Santiago Chile, September 1994 3 R Agrawal and R Srikant Mining sequential patterns In Proc 1995 Int Conf Data Engi neering pages 3-14 Taipei, Taiwan March 1995 4 D.W Cheung J Wan V Ng and C.Y Wong Maintenance of discovered association rules in large databases An incremental updating tech nique In Proc 1996 Int\222l Conf on Data Engi neering New Orleans, Louisiana Feb 1996 5 U M Fayyad 6 Piatetsky-Shapiro P Smyth and R Uthurusamy Advances zn Knowledge Dis covery and Data Mining AAAI/MIT Press 1996 6 A Geist A Beguelin J Dongarra W Jiang R Manchek and V Sunderam PVM Parallel Virtual Machine A Users\222 Guide and Tutorial for Networked Parallel Computing MIT Press 1994 7 J Han Y Cai and N Cercone Data driven discovery of quantitative rules in relational databases IEEE Trans Knowledge and Data En gineering 5:29-40 1993 Discovery of multiple-level association rules from large databases In Proc 1995 Int Conf Very Large Data Bases pages 420-431 Zurich Switzerland Sept 1995 8 J Han and Y Fu 9 R Ng and J Han Efficient and effective cluster ing method for spatial data mining In Proc 1994 Int Conf Very Large Data Bases pages 144-155 Santiago Chile, September 1994 lo J.S Park M.S Chen and P.S Yu An effec tive hash-based algorithm for mining association rules In Proc 1995 ACM-SIGMOD Int Conf Management of Data pages 175-186 San Jose CA May 1995 ll J.S Park M.S Chen, and P.S Yu Efficient par allel mining for association rules In Proc 4th Int Conf on Information and Knowledge Manage ment pages 31-36 Baltimore Maryland Nov 1995 12 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases In Proc 1995 Int Conf Very Large Data Bases pages 432-443 Zurich Switzerland Sept 1995 13 A Silberschatz M Stonebraker and J D U11 man Database research Achievements and op portunities into the 21st century In Report of an NSF Workshop on the Future of Database Sys tems Research May 1995 14 R Srikant and R Agrawal Mining general ized association rules In Proc 1995 Int Conf Very Large Data Bases pages 407-419 Zurich Switzerland Sept 1995 association rules in large relational tables In Proc 1996 ACM-SIGMOD Int Conf Manage ment of Data Montreal Canada June 1996 15 R Srikant and R Agrawal Mining quantitative 42 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


