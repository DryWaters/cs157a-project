Proceedings of 1993 International Joint Conference on Neural Networks A theoretical model of the hippocampal-cortical memory system motivated by physiological functions in the hippocampus Minoru Tsukada Department of Idomation  Communication Engineering Faculty of Engineering Tamaga wa University 6-1 1 Tamagawagakuen, Machida Tokyo 194 JAPAN 
ABSTRACT Based on the physiological evidence we propose a hypothesis on how association and dissociation of event memories are made in the hippocampal-cortical memory system It is postulated that the association  dissociation of memory is carried out by indexing the representations of events memory contents\with temporal codes The memory contents are supplied from the sensory association cortices, while the temporal codes are supplied from decision making  motivation area The two inputs are mixed \(indexing\in the ento-perirhinal area. Indexed signals are fed to hippocampus where connection  
disconnection of memory contents occur depending on the kind of index Finally association  dissociation of event memories is made in the association cortex according to a covariance rule: two events memories are associated when direct cortio-cotical inputs and indirect inputs from the hippocampus are positively correlated through the consolidation made in hippocampus, and they are dissociated when two inputs are negatively correlated in consequence of the disconnection made in the hippocampus Keywords A hippocampal-cortical memory system index theory temporal pattern codes 1 The hippocampal-cortical memory system corresponds 
to a neural network with three layers The hippocampal-cortical memory system consists of the following components the neocortex, the hippocampus and the ento-perirhinal \(ento- perirhinal, and parahippocampal cortices\area. The entorhinal cortex EC is the major source of projections to the hippocampus It receives projections from the unimodal and polymodal areas of the frontal, temporal and parietal lobes of the neocortex via the adjacent perirhinal PR and parahippocampal THDH cortical area The entorhinal cortex also receive other direct input from the orbital frontal cortex the cingulate cortex, the insular cortex and 
the superior temporal gyrus. Thus, the ento-perirhinal cortical area makes up a part of the interface between the neocortex and the hippocampus The hippocampal networks consist of the dentate gyrus DG and the CA3 and CA1 regions where cells are interconnected by widespread and overlapping projections l The widespread efferent projections leave the hippocampus by way of the subiculum S flow to the EC area and continue to the neocortex 2 Physiologically Squire and Zola-Morgan 3 posited that the neocortex represents visual object quality \(in area 
TJ3 and object location \(in area PG as long-term memory, while the hippocampus plays am important role at the time of learning in establishing long-term memory which is then stored in the neocortex. The hippocampal process which works only temporary as short-term memory serves as a device for forming conjunctions between ordinarily unrelated events or stimulus features which are processed and represented by distinct cortical sites After learning memory stored in the neocortex becomes independent of the intervening temporal lobe structure Thus we 
assume that the memory system is composed of neural networks with three layers; the input layer, the hidden layer and the output layer. The neocortex may be thought of as the input-output layer, where information is represented in terms of the activities of assemblies of neurons and it serves as long-term memory The hippocampus corresponds to the hidden layer where short-term memory is developed The ento-perirhinal cortical area is an interface between the neocortex and the hippocampus Schematically this memory system is shown in Fig 1 
2 Neural coding in the memory system Almost all of the models of the associative memory studied previously are based upon the algorithm of taking auto- and cross-correlation of different inputs 4-71 This process is a kind of computation of similarity of input patterns To ensure an accurate recall, the above models store different pattems by transforming them into mutually orthogonal signals However it would be generally accepted a simple psychological inspection that the event 1120 


_ c area TA as long-term b The entorhinal and parahippocarnpal between the neoccrtex projections from of projections to cortices also making area DN these projections c The hippocanpus the dentate gyrus plays an import;,nt establishing NEOCORTEX memory cortex EC perirhinal cortex PR gyrus TF/TH are an interface and the hippocampus and receive tk neocortex and are the major source tthe hippocampus The ento-perirhinal receives projections from the decision or the motivational area MO All are reciprocal consists of the following stages DG the CA3 and CA1 regions and role at the time of learning in long-term memory E C RRER HIPPOCAMPUS temporal cod spatio- t emporal code S XT b1 INDEX LAYER P-1 HIDDEN LAYER Fig.2 A proposed model of the hippocampal-cortical memory system The input-output layer of the network corresponds to the neocortex the index layer corresponds to the ento perirhinal cortical area and the hidden layer correspond to the hippocampus a Activation of an assembly of neurons in the neocortex represents a perceptual feature as a spatial code visual object quality in area TE visual object location in area PG information for auditory recognition in area TA etc b In the index layer the spatial vector is indexed by a temporal code sent from the decision making area i.e it is transformed into a spatio-temporal vector of which the temporal part represents the index and the spatial part represents the information content c The hippocampus works as a device for connecting/disconnecting information contents depending on the index The role of this system is only temporary and this hippocampal control becomes consolidated as permanent memory in the neocortex as time passes 1121 


assemblies of neurons as the distribution of synaptic strengths of neural networks Pulm lo and Amari lll theoretically demonstrated that a sparse cording by assmblies of neuron is the most desirable method from the view point of information capacity. Physiological evidence supporting a sparse coding has recently been found in Inferior Temporal SI We postulate that the spatial pattem of the sparse activities of assemblies of neurons in the neocortex represents events. Memories of different modalities and submodalities are stored in different cortical areas: visual object quality in area TE visual object location in area PG, information for auditory recognition in area TA, etc 2 2 A temporal coding in the ento-perirhinal cortical area as an indexer of memory The ento-perirhinal cortical area may be functioning as an indexer to associate  dissociate memories by using temporally modulated patterns In the ento-perirhinal cortical area, inputs from the neocortex represented by spatial patterns events\are modulated temporally by inputs indexes\from the decision making area One of important decision making areas may be the frontal cortex. The interaction between both inputs generates a spatio-temporal pattem One of effective temporal modulation \(temporal code\would be rythms with different second order statistics serial correlation coefficients of the impulse sequence. There is at present no direct evidence that the ento-perirhinal cortices use temporal code as an index. However, the possibility of using temporal code as an index is strongly supported by the fact that rhythmic activities are very common in the entorhinal cortex I21 and related areas 13-161 2.3 Learning rub at the early stage to associate I dissociate events in the hippocampus hased on the index Hippocampus works as a hidden network to construct a new memory structure in the association cortex through the inferface of ento-perirhinal area In this theory it is assumed that hippocampal network associate I dissociate events according to the kind of indexes joined with them We propose that hippocampus uses a temporal code as an index We stress this because homosynaptic and associative LTP of CA1 neurons were shown to be highly sensitive to a temporal modulation of Schaffer collateral stimulation Our observations showed that the stimulus with positive correlation of successive inter-spike intervals ISIS generated a large homo and associative synaptic LTP that of negative correlation was ineffective, and the stimulus with no correlation induced a moderate LTP 17,18,19 Thus we hypothesize that the positive correlation works as the joining index code \(T which associate different events negative correlation works as a disjoining index code T9 which dissociates two events No correlation works as a neutral code which mantains the present state One more assumption is introduced That is to associate  dissociate indexed events rough synchronization spatial correlation of input events is required Our proposal is that when two spatial vectors the representation of events are in synchrony with the index T they are associated and when they are in synchrony with the index To, they axe dissociated This is shematically shown in Fig 3 v c Negatively correlated code To Fig.3 Schematic representation of connencted/neutral/disconnected information in the hippocampal network based on the index theory a When both i and j inputs synchronously include the positively correlated temporal code as an index they produce a large LTP and are connected b When both i and j inputs are the synchronous and uncorrelated temporal code or asynchoronous time code they produce a moderate LTP which provides a neutral state c When both i and j inputs synchronously include the negative correlated temporal code as an index they produce a small LTP and are disconnected 1122 


1 stage to fix the memory in the association cortex as long term memory nctions as the memory store house for the final representations of events This area about association  neutral  dissociation through back projection from hippocampus in the obtained by associative LTP in hippocampus using index T'. The two memory representations the inputs from association cortex representation of event A and hippocampus event B 3 Conclusion temporal patterns given by second-order statistics are used as indexing codes If the event Trans C-21 353 1972 ram SMC-12 380 1972 1123 


more specific applications will be presented Suppose the following additional assumptions hold relativeto the basic situation used to determinett posterior function of 8 a For all y c Rnk  fj  Iy Rnk Ri s a bounded continuous conditional probability density function for k=l,..,M',Lt-,j  for some chosen track histories i,j b For each integer p>l let D k\(p be the pth discretization and truncation of DRwlth A Ut  k S,m k,m,p denoting the mesh of D k\(mp i.e maximal length of any rectangle formed within the truncatlon area so that in any sense,,lim IkD\(tmp Rnk and 1 im A t  0 in addition replace each P+*k,m,p Zkm kt mt  by the approximating probability funct'ion 14 V Z JI LI1 Cz t Z\(L CL p Zk,m k,m k,m k k,m k m k,m Igp and denote accordingly all computations involving this replacement for k=l,..,M  t=i,ibwith p c e 3\(xk,t  is analytic about xk t=0 k=l t=l j d or is an Archimedean t-conorm with generator function h 0,1  R+U{4e}-whlch is continuous non-increasing with h\(l and h\(0   and is such that or\(x  x x min h\(t l x h\(0   or 1 q I w 15 for all x 243 0,1 q=1,2  See,e.g.,[12 and eq.\(23   e Referring to d h\(l-O x,y is analytic about x=y=0 in 0 t  f 61Z J  R is a continuous function in Zm  i allowed to be arbitrary in kpnkXXRnk This is guaranteed if  is continuous in its first argument and mod is continuous over 0,1 for k=l,..,M v=l Next define k=l  4 and  t 16 v\(x  d h\(s sC\(&Cx.y  17 for all x 243 0,1 Theorem 2 Assume the basic situation holds as established in the previous sections Suppose also that assumptions a f hold Then,assunlng or max f+=Lj 1 J 1 R P A v  eIC1'3  p4+m  l-h1\(mln\(p\(e O 18 where p\(e 4 max a\(e,Z"s b 19 c\(esZ i v+\(,s IO  20 a imi  CK'5E m~e where E v denotes ordinary statistical expectation with respect to random vqctor V replacing the argument non-random Z,kisi which has prob ability density function f where at any value O X for V,f has the conditional form f Si13 z\(i 9i s7 f Z m|f m m m Kk m'k,m'k,m 21 Proof Use the canonical expansion for orl as in 15 where q is replaced by a summation over Dm\(iJ and x i's replaced by Gp\(O,Z 9  In turn expand expand h\(l-G esZ h  in terms of variable fp 1 3 Ip  around 0 obtaining h\(1-G 2e 1'3  iJ Zm 4m p 1  H\(5Zm  m r  0f\(6  Z\(i j 1  rjl 2 Also expanding for A L small where mp  p k A 1 k p  M The result then follows from the definition of an integral  Remark An important famnily of t-norms and t-conorms is due to Frank 13 See also 12 These satisfy the basic modular relation or\(x,y  x  y x.y  22 for all x,y 243 0,1 Frank has shown that this family can be characterized by the Archimedean class i.e all  or such that for all x c 0,1 23 x,x and or\(x,x s which i's also DeMorgan i.e 840 


for\(X,Y  1 l-x l-y  24 for all x,y 243 0,13 and as wel by the class of all ordinal sums C typesof affine transfor involving the block diagonal parts of 0,1 see 9],[12 of subsets of the Archimedean class Indeed the Archimedean class of Frank contains many common t-norms and t-conorms and can be conveniently parameterized using parameter s  s\(x b b Xq log'\(lt SX-l  with or\(xl..xq determined from the DeNorgaA25 or modular relations for all x 1.,uxq e 0,1  with generator function hs given as h x  ogtsXl s-l x 243 0,1 26 vaa^;.ey8iC sl"f darG\260aenFIti"B ie ate St Syseus Academic hedt k dep ahSSr8N.b Processe and Fl ter3  Bar iTrachQgLnf.Mtl9gg 4 duf3^g rde,E EzySt Ss,Aaei p e8>E~\247 i,*aProb*^isticMetric Ri 7odau i o2&.4b kIVM c of 841 5 for all Ocsct  where the special cases are s=O non-Archimedean  O X1  xq  min\(x  xq Or,O\(x1'D sxq m max\(xf 6xq 27 ho\(x 40  if Ocxsl S=V O1\(x1 xq  x1 N xq O'or,l  s  xq P robsu\(xilb S xq 28 hl\(x  log\(x  all x 243 0,1 S=4e x1x...xq min\(xi+..+xqIl or co{xl 5  xq  xi  xq-\(q-l 29 h+.Cx  l-x  all x c 0,1 It follows that if all t-norms and t-conorms used in the computations for the posterior function of e are culled from Frank's family and the assumptions a f hold then Theorem 2 is valid and the key computations for K and v are For generator h5 of or si  Cd hs x _ 0<s'<c+Ws'#I CI\(s'=l   30 For sI sX-l s-l a  X.Y  O<S<ce,sOl s=l   31 For  Ps O&MS1  o0 s K O<s 1 4,n lg1 s'-l  32 Finally let us consider briefly the effect of the choice of inference rules and the accuracy of the error distributions upon the asymptotic posterior function for e as given in Theorem 2 So far iatching tables have not been discussed In general although the basic strutures of the inference rules do not depend upon time and the particular pair of track histories in question the matching table does For subjective attributes these could be chosen as some type of symmtrlzation of the corresponding error distributions while for statistical attributes the matching tables can be naturally chosen as M i'J  I-F cxcz\(iJ 33 where k,m nk k m 34 k,m k,m k,m km km k k,m and Fn is the probability di;tribution function nk of x corresponding to the Z\(t being r.v.'s with a connon expectation bqt oflerwise described by independent p.d.f.'s f kt as for V in Theorem 2 Note also by inspection that c e,z.iJ 1  non-hereasing f tion in the vaHablEs x\(Z,|ii 01 Zj1J   kZ;J provided that we cTooWe for example fmod x and mod x V'sk v for some constant positive ao  bv  all xe[O,l It then easily follows that if all error distributions are not in dirac-form but indicate some spread or confusion between observed/smoothed data then if we can legitimately obtain for the inference rules the consistency structure given by all avk approac hing the extreme extensification 0 with all b approachin he extreme intensification W  for O%6<l eIZ ii approaches O for fixed values of X  and imil3rly fort On the other hand for 8=1 the latter is maA mized The consequence of this,applying standard inequalities to the expectation E C in Theorem 2 and making the additional mild asdumption true for Frank's family for saf that v\(x is non-decreasing j  0,1 with v\(0 l that eI di ag,Z  R,P approaches the possibility function  representing complete data association between i afd  i.e.11 iff 8=1 W\(8 f 6~;f f 35 Future work will be directed toward extending further quantifying and establishing empirical bases for the above study References 5.Ge~u La"I  6p41c\243rportipp4f4uzlxseteorytCor B3 1jao-S ezI ranfkifl Stac3leg-2 Li 6 g 


DE&A Update BCD&A Update BCE&A Update BDE&A A,@D E AR AR AR Update CDE&A Rule R 6R A  B  C,@D Rule R 6R A,@B C,@E Rule R 6R A  B  D,@E Rule A,@C AR R 6R 8 9 IO 11 12 13 14 Update BCDE A Support  Count\(C,D,A Confidence8  Count\(C,D A  If Support8  S C,D  A 100 Count\(C,D  100 And Confidence  C Support  Count\(C,E,A Confidence  Count\(C,E A  If Support  S C,E  A 100 Count\(C,E  100 And Confidence 2 C Support  Count\(D,E,A Confidence  Count\(D,E A  If Support t S D,E  A 100 Count\(D,E  100 And Confidence C Support  Confidence Count\(B,C,D,A  If Support S B,C,D  Count\(B,C,D,A 100 Count\(B,C,D  100 And Confidence C A Support  Confidencel2 Count\(B,C,E A  If Supportl2 S B,C,E  Count\(B,C,E,A 100 Count\(B,C,E  100 And Confidencel2 C A Support  Confidence13 Count\(B,C,D A 1 If Support S B,D,E  Count\(B,C,D,A 100 Count\(B,C,D  100 And Confidence C A Support  Confidence,?= \(Count\(C,D,E A  If Supportl4 S C,D,E  Count\(C,D,E,A 100 Count\(C,D,E  100 And Confidencel4 C A A  B  C,@D,@E I I Tuple Tuple Tuple Tuple Tuple GR.A=@A and GR.D=@D and SR.E=@E If Select  from R where GR.A=@A and GR.B=@B and GR.C=@C and FR.D=@D If Select  from R where GR.A=@A and GR.B=@B and SR.C=@C and GR.E=@E If Select  from R where GR.A=@A and GR.B=@B and SR.D=@D and GR.E=@E If Select  from R where GR.A=@A and GR.C=@C and GR.D=@D and GR.E=@E If Select  from R where GR.A=@A and GR.B=@B and GR.C=@C and FR.D=@D and GR.E=@E set count\(D,E\=count\(D,E Set count\(D,E,A  count\(D,E,A where rule=lO Update AR set count\(B,C,D\=count\(B,C,D Set count\(B,C,D,A where rule=l 1 Update AR set count\(B,C,E\=count\(B,C,E Set count\(B,C,E,A  count\(B,C,E,A where rule=l2 Update AR set count\(B,D,E\=count\(B,D,E Set count\(B,D,E,A where rule=l3 Update AR set count\(C,D,E count\(C,D,E Set count C,D,E A count\(C,D,E,A where rule=14  count B C ,D A    count \(B D E A     Update AR set count B C,D ,E count\(B,C,D,E Set count\(B,C,D,E,A count\(B,C,D,E,A where rule=l5  127 


 Confidencel5 Count\(B,C,D,E If Supportl5 S B,C,D,E  A B C a1 bl Cl an bn cn al\222 bl\222 Cl\222    We can create an association rule table AR to hold its I Parameter count Association Rule table AR with timestamp at Date2 transaction occurrence count as follows I Rulecount I Potential association rule D E dl el dn en dl\222 el\222   We can use the same calculation as described in defined in Constraint class error message will be The situation is that an online shopping centre has a computing association rule continuously except returned to the user \(refer to Figure 3 the total transaction record count of source relation Date2 is n  m 5 Prototype     collection of commodities from television to fruits In Figure 4 we process web log file into Main Table Merge Cookie information Datez I bm cm I dm A prototype has been implemented for frame object agent model which is an Application program Interface developed to invoke the data operation in the frame model for data semantics management The API process handles the method defined by Constraint class during data modification The process defined and executed by RDBMS is to provide triggering event fired on the class When RDBMS process is invoked it will check from the Constraint class for any associated method If method is defined in the Constraint class the method definition will be queries The process will invoke the stored procedure defined by the queried method definition If executed result returned by stored procedure violated the rule Figure 4 Process M In figure 5 we transform Main Table into Transaction Table corresponding to procedure 0 in figure 3 In figure 6 we process Transaction Table into Concise Transaction Table corresponding to procedure  and 0 in figure 3 em leb Loe File 128 


 Figure 5 Process Main Table into Transaction Table D Figure 6 Process Transaction Table into Concise Figure 7 I f bl 411 significant association rules for the target attribute television confidence=20 support=0.2 In Figure 7 we select television as our target and get the rules based on confidence level 20 and support 0.2 6 Conclusion This paper presents a solution to the problem by using a frame metadata model to invoke record count program in a metadata whenever relevant new record is created in the target database As a result web mining association rules can be accomplished online or incrementally without the need to re-compute the historical web transaction counts The metadata thus contributes saving of computing time and also the automation of data mining association rules non-stop The future research of this paper is web mining web pages association rules via Internet References I Chen Q Hsu M and Dayal U A Data Warehouse/OLAP Framework for Scalable Telecommunication Tandem Traffic Analysis a report of Software Technology Laboratory HP Labs 1501 Page Mill Road MS 1U4 Palo Alto CA 94303 USA Robert Cooley, Bamshad Mobasher and Jaideep Srivastava Data Preparation for Mining World Wide Web Browsing Patterns Journal of Knowledge and Information Systems Vol 1 No 1 1999 Robert Cooley, Bamshad Mobasher and Jaideep Srivastava Grouping Web Page References into Transactions for Mining World Wide Web Browsing Patterns Proceedings of the 1997 IEEE Knowledge 2 3 41 I51 t61 I71 and Data Engineering Exchange Workshop \(KDEX 97 November 1997 A.G Buchner M.D Mulvenna S.S Anand, J.G Hughes An Internet-enabled Knowledge Discovery Process the 9th International Database Conference Hong Kong pp 13-27, July 1999 Cookie A.G. Buchner, M. Baumgarten S.S Anand, M.D Mulvenna J.G. Hughes Navigation Pattern Discoven from Internet Data ACM Workshop on Web Usage Analysis and User Profiling WebKDD San Diego CA pp. 25-30 1999 Mena Jesus Data Mining Your Website Digital Press 1999 pp 3 13-3 14 Moore J Han E Boley D Gini M Gross R Hstings K Karypis G Kumar V and Mobasher B Web Page Categorization and Feature Selection Using Association rule and Principal Component Clustering University of Minnesota M Mohania S Madria and Y Kambayashi Seu Maintainable Aggregate Views Proceedings of the 91h International Database Conference City University of Hong Kong Press ISBN 962-937-046-8 p.306-3 17 T.Griffin and L Libkin Incremental mainteriance of views with duplicates Proceedings of the International Conference on Management of Data 1995 Sunita Sarawagi Shiby Thomas and Rakesh Agrawal Integrating Association Rule Mining With Relational Database Systems Alternatives and Implications Takeshi Fukuda Yasuhiko Morimoto Shinichi Morishita and Takeshi Tokuyama Data Mining Using Two-Dimensional Optimized Association Rules: Scheme Algorithms and Visualization ACM 199 pp13-23 Eui-Hong Sam Han George Karypis and Vipin Kumar Scalable Parallel Data Mining for Association Rules ACM 1997, pp277-288 Pieter Adriaans and Dolf Zantinge Data Mining Addision Wesley ISDN 0-201-40380-3 Fong J Huang S Architecture of a Universal Database A Frame Model Approach lntemational Journal of Cooperative Information Systems Volume 8 Number 1 March 1999, pp.47-82 Fong J and Pang F Schema Evolution for New Database Applications A Frame Metadata model Approach Proceedings of Systems Cybernetics and Informatics, Volume 5 1999 pp 104-1 1 1 Fong J Huang S Information Systems Reengineering Springer Verlag ISBN 98 1-3083-15 0 R Zaiane M Xin, J Han Discovering Web Access Patterns arid Trends by Applying OLAP and Data Mining Technology on Web Logs Proceedings Advances in Digital Libraries Conference ADL\22298 Santa Barbara CA April 1998 pp 19-29 A.G Buchner M.D Mulvenna Discovering Internet Marketing Intelligence through Online Analytical Web Usage Mining ACM SIGMOD Record ISSN ACM 1998 343-354 8 1997 179-212 0163-5808 Vol. 27 NO 4 pp 54-61 1998 129 


ofimages 50 100 150 200 1 feature 50292 80777 127038 185080 2 obj identif 210 338 547 856 3 aux image 3847 6911 10756 13732 4 assoc rules 6 3 6 4 Table 2 Measured times in seconds for each Image Mining step with different image set sizes colors and contrast When the L was close to another shape its colors were merged making it dissimilar to other L shaped objects This suggests that irregular shapes in general make image mining dif\256cult We worked with color images but it is also possible to use black and white images Color and texture were important in mining the geometric shapes we created However we ignored shape as mentioned above Shape may be more important for black and white images but more accurate shape descriptors are needed than those provided by the blobs 4.3 Performance evaluation We ran our experiments on a Sun Multiprocessor forge.cc.gatech.edu computer with 4 processors each running at 100 MHz and 128 MB of RAM The image mining program was written in Matlab and C The 256rst three steps are performed in Matlab The feature extraction process is done in Matlab by the software we obtained from UCB Object identi\256cation and record creation were also done in Matlab by a program developed by us An html page is created in Matlab to interpret results The association rules were obtained by a program written in C In this section we examine the performance of the various components of the image mining process as shown in Table 2 for several image set sizes These times were obtained by averaging the ellapsed times of executing the image mining program 256ve times 4.4 Running time analysis Feature extraction although linear in the number of images is slow and there are several reasons for this If image size increases performance should degrade considerably since feature extraction is quadratic in image size Nevertheless this step is done only once and does not have to be repeated to run the image mining algorithm several times Object identi\256cation is fast This is because the algorithm only compares unmatched objects and the number of objects per image is bounded For our experimental results time for this step scales up well Auxiliary image creation is relatively slow but its time grows linearly since it is done on a per image basis The time it takes to 256nd rules is the lowest among all steps If the image mining program is run several times over the same image set only the times for the second and the fourth step should be considered since image features already exist and auxiliary images have already been created 5 Application Image mining could have an application with real images The current implementation could be used with a set of images having the following characteristics 017 Homogeneous The images should have the same type of image content For instance the program can give useless results if some images are landscapes other images contain only people and the remaining images have only cars 017 Simple image content If the images are complex they will produce blobs dif\256cult to match Also the association rules obtained will be harder to interpret A high number of colors blurred boundaries between objects large number of objects signi\256cant difference in object size make the image mining process more prone to errors 017 A few objects per image If the number of objects per image is greater than 10 then our current implementation would not give accurate results since Blobworld in most cases generates at most 12 blobs per image 017 New information The image itself should should give information not already known If all the information about the image is contained in associated alphanumeric data then that data could be mined directly 6 Future Work Results obtained so far look promising but we need to improve several aspects in our research effort We are currently working on the following tasks We also need to analyze images with repeated geometric shapes If we want to obtain simple association rules this can make our program more general This can be done without further modi\256cation to what is working However if we want to mine for more speci\256c rules then we would need to modify our algorithm For instance we could try to 


produce rules like the following if there are two rectangles and one square then we are likely to 256nd three triangles The issues are the combinatorial growth of all the possibilities to mine and also a more complex type of condition We will also study more deeply the problem of mining images with more complex shapes such as the irregular one similar to the letter L We need a systematic approach to determine an optimal similarity threshold or at least a close one A very high threshold means only perfect matches are accepted On the other hand a very low similarity threshold may mean any object is similar to any other object Finding the right similarity threshold for each image type l ooks like an interesting problem Right now it is provided by the user but it can be changed to be tuned by the algorithm itself Also there are many ways to tune the eleven parameters to match blobs and the optimal tuning may be speci\256c to image type There also exists the possibility of using other segmentation algorithms that could perform faster or better feature extraction It is important to note that these algorithms should give a means to compare segmented regions and provide suitable parameters to perform object matching in order to be useful for image mining From our experimental results it is clear that this step is a bottleneck for the overall performance of image mining We can change the object identi\256cation algorithms to generate overlapping object associations using more features Our algorithm currently generates partititons of objects that is if one object is considered similar To another one the latter one will not be compared again By generating overlapping associations we can 256nd even more rules For instance a red rectangular object may be considered similar to another rectangular object and at the same time be similar to another red object Mining by position is also possible for instance two objects in a certain position may imply another object to be in some other position Since the software we are using for feature extraction produces eleven parameters to describe blobs we have 2 11 possibilites to match objects 7 Conclusions We presented a new algorithm to perform data mining on images and an initial experimental and performance study The positive points about our algorithm to 256nd association rules in images and its implementation include the following It does not use domain knowledge it is reasonably fast it does not produce meaningless or false rules it is automated for the most part The negative points include some valid rules are discarded because of low s upport there are repeated rules because of different object id's unwanted matches because of blobs representing several objects slow feature extraction step a careful tuning of several parameters is needed it does not work well with complex images We studied this problem in the context of data mining for databases Our image mining algorithm has 4 major steps feature extraction object identi\256cation auxiliary image creation and identi\256ed object mining The slowest part of image mining is the feature extraction step which is really a part of the process of storing images in a CBIR system and is done only once The next slowest operation is creating the auxiliary blob images which is also done once Object identi\256cation and association rule 256nding are fairly fast and scale up well with image set size We also presented several improvements to our initial approach of image mining Our experimental results are promising and show some potential for future study Rules referring to speci\256c objects are obtained regardless of object position object orientation and even object shape when one object is partially hidden Image mining is feasible to obtain simple rules from not complex images with a few simple objects Nevertheless it requires human intervention and some domain knowledge to obtain better results Images contain a great deal of information and thus the amount of knowledge that we can extract from them is enormous This work is an attempt to combine association rules with automatically identi\256ed objects obtained from a matching process on segmented images Although our experimental results are far from perfect we show that it is better to discover some reliable knowledge automatically than not discovering any new knowledge at all Acknowledgments We thank Chad Carson from the University of California at Berkeley for helping us setup the Blobworld system We also thank Sham Navathe and Norberto Ezquerra for their comments to improve the presentation of this paper References 1 R  A g r a w a l  T  I m i e lin s k i a n d A  S w a m i  M in in g a s s o ciation rules between sets of items in large databases In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data  pages 207\261 216 Washington DC May 26-28 1993  R  A gra w a l a n d R  S ri ka nt  F a s t a l gori t h m s for m i n i n g association rules in large databases In Proceedings of the 20th International Conference on Very Large Data Bases  Santiago Chile August 29-September 1 1994  S  B e l ongi e  C Ca rs on H  G r e e n s p a n  a nd J  Ma lik Recognition of images in large databases using a learning framework Technical Report TR 97-939 U.C Berkeley CS Division 1997 


 C  C a r s on S  Be l ongi e  H  G r e e n s p a n  a nd J  Ma l i k  Region-based image querying In IEEE Workshop on Content-Based Access of Image and Video Libraries  1997 5 G  D u n n a n d B  S  E v e r itt An Introduction to Mathematical Taxonomy  Cambridge University Press New York 1982  U  F a yya d  D  H a u s s l e r  a nd P  S t orol t z  M i n i n g s c i e n ti\256c data Communications of the ACM  39\(11\51\26157 November 1996  U  F a yya d G  P i a t e t s k y-S h a p i r o a n d P  S m y t h  T he kdd process for extracting useful knowledge from volumes of data Communications of the ACM  39\(11\:27\261 34 November 1996 8 D  F o r s y t h J M a l i k  M F l e c k H G r e e n s p a n  T L e ung S Belongie C Carson and C Bregler Finding pictures of objects in large collections of images Technical report U.C Berkeley CS Division 1997  W  J  F ra wl e y  G  P i a t e t s k y S ha pi ro a nd C J  Ma t h e u s  Knowledge Discovery in Databases  chapter Knowledge Discovery in Databases An Overview pages 1 261 27 MIT Press 1991  V  G udi v a da a n d V  R a gha v a n Cont e n t ba s e d i m age retrieval systems IEEE Computer  28\(9\18\26122 September 1995 11 R  H a n s o n  J  S t u t z an d P  C h ees eman  B ay es i a n c l a s si\256cation theory Technical Report FIA-90-12-7-01 Arti\256cial Intelligence Research Branch NASA Ames Research Center Moffet Field CA 94035 1990  M H o l s he i m e r a n d A  S i e be s  D a t a m i ni ng T h e search for knowledge in databases Technical Report CS-R9406 CWI Amsterdam The Netherlands 1993  M H out s m a a nd A  S w a m i  S e t ori e nt e d m i ni ng of association rules Technical Report RJ 9567 IBM October 1993  C O r done z a nd E  O m i e c i ns ki  I m a ge m i ni ng A new approach for data mining Technical Report GITCC-98-12 Georgia Institute of Technology College of Computing 1998  J  R Q u i n l a n Induc t i o n o f d e c i s i on t r e e s  Machine Learning  1\(1\81\261106 1986  A  S a v a s e re  E  O m i e c i ns ki  a nd S  N a v a t h e  A n e f 256 cient algorithm for mining association rules In Proceedings of the VLDB Conference  pages 432 261 444 Zurich Switzerland September 1995  O  R Z a i a ne  J  H a n  Z  N  L i  J  Y  Chi a ng a n d S Chee Multimedia-miner A system prototype for multimedia data mining In Proc 1998 ACM-SIGMOD Conf on Management of Data  June 1998 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


