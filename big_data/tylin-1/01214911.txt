Algorithms for Balancing Privacy and Knowledge Discovery in Association Rule Mining Stanley R M Oliveira 1  2 1 Embrapa Inform atica Agropecu aria Av Andr e Tosello 209 13083-886 Campinas SP Brasil oliveira@cs.ualberta.ca Osmar R Za ane 2 2 Department of Computing Science University of Alberta Edmonton AB Canada T6G 2E8 zaiane@cs.ualberta.ca Abstract The discovery of association rules from large databases has proven beneìcial for companies since such rules can be 
very effective in revealing actionable knowledge that leads to strategic decisions In tandem with this beneìt association rule mining can also pose a threat to privacy protection The main problem is that from non-sensitive information or unclassiìed data one is able to infer sensitive information including personal information facts or even patterns that are not supposed to be disclosed This scenario reveals a pressing need for techniques that ensure privacy protection while facilitating proper information accuracy 
and mining In this paper we introduce new algorithms for balancing privacy and knowledge discovery in association rule mining We show that our algorithms require only two scans regardless of the database size and the number of restrictive association rules that must be protected Our performance study compares the effectiveness and scalability of the proposed algorithms and analyzes the fraction of association rules which are preserved after sanitizing a database We also report the main results of our performance evaluation and discuss some open research issues 
1 Introduction The recent advance of data mining technology to analyze vast amount of data has played an important role in marketing business medical analysis and other applications where pattern discovery is paramount for strategic decision making Despite its beneìts in such areas data mining also opens new threats to privacy and information security if not done or used properly Recent advances in data mining and machine learning algorithms have introduced new problems in privacy protection 6 The main problem is that from non-sensitive data one is able to infer sensitive information 
including personal information facts or even patterns that are not supposed to be disclosed The current status in data mining research reveals that one of the current technical challenges is the development of techniques that incorporate security and privacy issues The main reason is that the increasingly popular use of data mining tools has triggered great opportunities in several application areas which also requires special attention regarding privacy protection In this paper we focus on privacy preserving association rule mining We start by considering a motivating example discussed in 3 4  S uppos e a s i t u at i o n e xi s t s i n w hi ch 
one supplier offers products in reduced prices to some consumers and in turn this supplier receives permission to access the database of the consumers customer purchases The threat becomes real whenever the supplier is allowed to derive highly sensitive knowledge from unclassiìed data that is not even known to the database owners consumers In this case the consumers beneìt from reduced prices whereas the supplier is provided with enough information to predict inventory needs and negotiate other products to obtain a better deal for his consumers This implies that the competitors of this supplier start losing business 
The simplistic solution to address the problem of our motivating example is to implement a lter after the mining phase to weed out/hide the restricted discovered association rules However in the context of our research the users are provided with the data and not the association rules and are free to use their own tools and thus the restriction for privacy has to be applied before the mining phase on the data itself For this reason to address this particular problem we need to develop mechanisms that will enable data own 
ers to choose an appropriate balance between privacy and precision in discovered association rules Such mechanisms can lead to new privacy control systems to convert a given database into a new one in such a way to preserve the genProceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


eral rules mined from the original database The released database is called sanitized database The procedure of converting an original database into a sanitized one is called the sanitization process and it was initially introduced in 1 T o do s o  a s m all number of transactions have to be modiìed by deleting one or more items from them or even adding noise to the data by turning some items from 0 to 1 in some transactions This approach relies on boolean association rules On one hand this approach slightly modiìes some data but this is perfectly acceptable in some real applications 3 4 9 On the other hand such an approach must hold the following restrictions 1 the impact on the non-restricted data has to be minimal and 2 an appropriate balance between a need for privacy and knowledge discovery must be guaranteed To accomplish these restrictions we introduce new algorithms for balancing privacy and knowledge discovery in association rule mining Our sanitizing algorithms require only two scans regardless of the database size and the number of restrictive association rules that must be protected The rst scan is required to build the index inverted le for speeding up the sanitization process while the second scan is used to sanitize the original database This represents a signiìcant improvement over the previous algorithms presented in the literature 4 9 which require v a rious scans depending on the number of association rules to be hidden One major novelty with our approach is that we take into account the impact of our sanitization not only on hiding the association rules that should be hidden but also on accidentally hiding legitimate rules that should not be hidden Other approaches presented in the literature focus on the hiding of restrictive rules but do not study the effect of their sanitization on accidentally concealing legitimate rules or even generating artifact rules i.e rules that do not exist in the original database Our algorithms are integrated with the framework for enforcing privacy in association rule mining presented in 7 8 The frame w o rk is compos ed of a t rans action retrie v a l engine relying on an inverted le and Boolean queries for retrieving transaction IDs from a database a set of sanitizing algorithms and performance measures that quantify the fraction of association rules which are preserved after sanitizing a database Our experiments demonstrate that our algorithms are effective and achieve reasonable results when compared with the other approaches presented in 4 9 This paper is organized as follows In Section 2 we provide the basic concepts to understand the issues addressed in this paper In addition the problem deìnition is given We present the idea behind our framework in Section 3 In Section 4 we introduce new sanitizing algorithms In Section 5 we present the experimental results and discussion Related work is reviewed in Section 6 Finally Section 7 presents our conclusions and a discussion of future work 2 Basic Concepts In this section we brieîy review the idea behind transactional databases and association rules After that we present the formulation of the research problem 2.1 Transactional Databases A transactional database is a relation consisting of transactions in which each transaction t is characterized by an ordered pair deìned as t   TID list of elements  where TID is a unique transaction identiìer number and list of items represents a list of items making up the transactions For instance in market basket data a transactional database is composed of business transactions in which the list of elements represents items purchased in a store 2.2 The Basics of Association Rules Association rules provide a very simple but useful form of rule patterns for data mining A rule consists of a lefthand side proposition the antecedent or condition and a right-hand side the consequent Both the left and righthand side consist of Boolean statements or propositions The rules state that if the left-hand side is true then the right-hand side is also true Formally association rules are deìned as follows Let I   i 1  i n  be a set of literals called items Let D be a database of transactions where each transaction t is an itemset such that t  I  A unique identiìer called TID is associated with each transaction A transaction t supports X  a set of items in I if X  t  An association rule is an implication of the form X  Y where X  I  Y  I and X  Y    Thus we say that a rule X  Y holds in the database D with conìdence  if  X  Y   X    where  A  is the number of occurrences of the set of items A in the set of transactions D  Similarly we say that a rule X  Y holds in the database D with support  if  X  Y   N    where N is the number of transactions in D  Association rule mining algorithms rely on support and conìdence and mainly have two major phases 1 based on a support  set by the user frequent itemsets are determined through consecutive scans of the database 2 strong association rules are derived from the frequent item sets and constrained by a minimum conìdence  also set by the user 2.3 Privacy Preservation Problem Deìnition The scenario we address in this paper is one which deals with two parties A and B  A owning a transactional database and B wanting to mine it for association rules The problem is how can A make some restrictive rules hidden Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


regardless of which minimum support threshold B would use Note that A does not know which association rule mining algorithm or support threshold B would use In this context the database owner A needs to look for some sensitive association rules in order to prevent them from being disclosed So A  the owner of the transactional database has full access to the database and would know what should be restricted based on the application and the database content whether these rules to restrict exist in the database or not A only knows that if these rules exist they should not be disclosed to B  The user B has no knowledge that some rules were hidden Once B gets access to the sanitized database B can mine any available rule The restricted rules if they existed in the original database are supposedly removed by the sanitization process by changing some transactions in the database In other words the user B does not have to know about the rules and A only needs to know which rules existing or not should not be disclosed Given these facts the speciìc problem addressed in this paper can be stated as follows If D is the source database of transactions and R is a set of relevant association rules that could be mined from D  the goal is to transform D into a database D  so that the most association rules in R can still be mined from D  while others representing restricted knowledge are hidden In this case D  becomes the released database 3 The Framework for Privacy Preservation As depicted in Figure 1 our framework encompasses a transactional database modeled into a text database an inverted le a set of sanitizing algorithms used for hiding restrictive association rules from the database a transaction retrieval engine for fast retrieval of transactions and performance measures that quantify the fraction of association rules which are preserved after sanitizing a database We describe the inverted le the transaction retrieval engine and the performance measures in this section and the new algorithms in Section 4 3.1 The Inverted File Index Sanitizing a transactional database consists of identifying the sensitive transactions and adjusting them To speed up this process we model transactions into documents in which the items simply become terms This model preserves all the information and provides the basis for our indexing inverted le borrowing from the information retrieval domain We index the transactional database with the purpose of speeding up the sanitization process In our framework the inverted leês vocabulary is composed of all different items in the transactional database   Vocabulary Transaction IDs Transactional Database Performance M easu r es Algorithms Sanitizing Transaction Retrieval Engine Figure 1 Privacy Preservation Framework and for each item there is a corresponding list of transaction IDs in which the item is present Figure 2 shows an example of an inverted le corresponding to the sample transactional database shown in the gure Freq Items A B C D T1, T2, T3, T4, T5 T1, T2, T3, T5, T6 T1, T2, T4, T5 T1, T3, T4, T6 Vocabulary 5 5 4 4 Transaction IDs T1 T2 T3 T4 T5 T6 A  B  C  D A  B  C A  B  D A  C  D A  B  C B  D Docs    Items/Terms Figure 2 An example of transactions modeled by documents and the corresponding inverted le We implemented the vocabulary based on a perfect hash table 5 with n o co llisio n  in sertio n  o r d e letio n  F o r a given item one access sufìces to nd the list of all transaction IDs that contain the item 3.2 The Transaction Retrieval Engine To search for sensitive transactions in the transactional database it is necessary to access manipulate and query transaction IDs The transaction retrieval engine performs these tasks It accepts requests for transactions from a sanitizing algorithm determines how these requests can be lled consulting the inverted le processes the queries using a query language based on Boolean model and returns the results to the sanitizing algorithm The process of searching for sensitive transactions through the transactional database works on the inverted le In general this process follows three steps 1 Vocabulary search  each restrictive association rule is split into single items Isolated Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


items are transformed into basic queries to the inverted index 2 Retrieval of transactions  The lists of all transaction IDs of transactions containing each individual item respectively are retrieved and 3 Intersections of transaction lists  The lists of transactions of all individual items in each restrictive association rule are intersected using a conjunctive Boolean operator on the query tree to nd the sensitive transactions containing a given restrictive association rule 3.3 Performance Measures In this section we introduce our privacy performance measures related to the problems illustrated in Figure 3 Problem 1 occurs when some restrictive association rules are discovered We call this problem Hiding Failure and it is measured in terms of the percentage of restrictive association rules that are discovered from D   Ideally the hiding failure should be 0 The hiding failure is measured by HF   R R  D    R R  D  where  R R  X  denotes the number of restrictive association rules discovered from database X  In our framework the proportion of restrictive association rules that are nevertheless discovered from the sanitized database can be controlled with the disclosure threshold   and this proportion ranges from 0 to 100 Note that  does not control the hiding failure directly but indirectly by controlling the proportion of sensitive transactions to be sanitized for each restrictive association rule Problem 2 occurs when some legitimate association rules are hidden by accident This happens when some nonrestrictive association rules lose support in the database due to the sanitization process We call this problem Misses Cost  and it is measured in terms of the percentage of legitimate association rules that are not discovered from D   In the best case this should also be 0 The misses cost is calculated as follows MC    R R  D     R R  D     R R  D  where   R R  X  denotes the number of non-restrictive association rules discovered from database X  Notice that there is a compromise between the misses cost and the hiding failure The more association rules we hide the more legitimate association rules we miss Problem 3 occurs when some artiìcial association rules are generated from D  as a product of the sanitization process We call this problem Artifactual Patterns andit is measured in terms of the percentage of the discovered association rules that are artifacts This is measured as AP   R   R  R    R   where  X  denotes the cardinality of X  4 Sanitizing Algorithms In this section before we introduce our sanitizing algorithms we present our heuristic approach to sanitize a transactional database RR  1 2 3     R R R R Figure 3 A Visual representation of restrictive and non-restrictive association rules and the rules effectively discovered after transaction sanitization 4.1 Heuristic Approach The goal of our heuristic is to facilitate proper information accuracy and mining while protecting a group of association rules which cont ains highly sensitive knowledge We refer to these rules as restrictive association rules and deìne them as follows Deìnition 1 Restrictive Association Rules Let D be a transactional database  the minimum support threshold R be a set of all association rules that can be mined from D based on a minimum support  and Rules H be a set of decision support rules that need to be hidden according to some security policies A set of association rules denoted by R R  is said to be restrictive if R R  R and if and only if R R would derive the set Rules H   R R is the set of non-restrictive association rules such that  R R 012 R R  R  Figure 3 illustrates the relationship between the set R of all association rules in the database D  the restrictive and non-restrictive association rules as well as the set R  of patterns discovered from the sanitized database D   1,2,and 3 are potential problems that respectively represent the restrictive association rules that were failed to be hidden the legitimate rules accidentally missed and the artiìcial association rules created by the sanitization process We provide performance measures for these potential problems in Section 3.3 A group of restrictive association rules is mined from a database D based on a special group of transactions We refer to these transactions as sensitive transactions and deìne them as follows Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


Deìnition 2 Sensitive Transactions Let T beasetof all transactions in a transactional database D and R R be a set of restrictive association rules mined from D Aset of transactions is said to be sensitive as denoted by S T if S T  T and if and only if all restrictive association rules can be mined from S T and only transactions in S T contain items involved in the restrictive association rules In most cases a sensitive transaction derives more than one restrictive association rule We refer to such transactions as conîicting transactions since modifying one of them causes an impact on other restrictive transactions or even on non-restrictive ones The degree of conîict of a sensitive transaction is deìned as follows Deìnition 3 Degree of Conîict of a Sensitive Transaction Let D be a transactional database and S T be a set of all sensitive transactions in D  The degree of a sensitive transaction t  denoted by degree\(t such that t  S T is deìned as the number of restrictive association rules that have items contained in t  To illustrate the presented concepts let us consider the sample transactional database in Figure 2 Suppose that we have a set of restrictive association rules R R   A,B  D A,C  D   This example yields the following results the sensitive transactions S T containing the restrictive association rules are  T1 T3 T4   The degrees of conîict for the transactions T1 T3 and T4 are 2 1 and 1 respectively Thus the only conîicting transaction is T1 which covers both restrictive association rules at the same time An important observation here is that any association rule that contains a restrictive association rule is also restrictive Hence if A,B  D is a restrictive association rule but not A,C  Das above any association rule derived from the itemset ABCD will also be restrictive since it contains ABD This is because if ABCD is discovered to be a frequent itemset it is straightforward to conclude that ABD is also frequent which should not be disclosed In other words any superset containing ABD should not be allowed to be frequent Our sanitizing algorithms presented in Section 4.2 act on the original database taking into account the degree of conîict of sensitive transactions 4.2 Sanitizing Algorithms Unlike algorithms that hide restrictive rules by modifying existing information in the database our algorithms solely remove information by reducing the support of some items This creates a smaller impact on the database since they do not generate artifacts such as association rules that would not exist had the sanitizing not happened These artifactual rules are generated by a noise addition approach i.e by adding some items in certain transaction Such algorithms create the possibility of discovering some association rules that are not supposed to exist For our hiding strategies Round Robin and Random algorithms the inputs are a transactional database D asetof restrictive association rules R R  and a disclosure threshold   while the output is the sanitized database D  Tosanitize a database each sanitizing algorithm requires only two scans of the original database one initial scan to build the inverted index and an additional scan to alter some sensitive transactions while keeping the other transactions intact All our sanitizing algorithms have essentially four major steps 1 Identify sensitive transactions for each restrictive association rule 2 For each restrictive association rule identify a candidate item that should be eliminated from the sensitive transactions This candidate item is called the victim item  3 Based on the disclosure threshold   calculate for each restrictive association rule the number of sensitive transactions that should be sanitized and 4 Based on the number found in step 3 identify for each restrictive association rule the sensitive transactions that have to be sanitized and remove the victim item from them Our sanitizing algorithms mainly differ in step 2 in the way they identify a victim item to remove from the sensitive transactions for each restrictive rule and in step 4 where the sensitive transactions to be sanitized are selected Steps 1 and 3 remain essentially the same for all approaches The complexity of our sanitization algorithms in main memory is O  n  NlogN  where n is the number of restrictive association rules and N the number of transactions in the database The proof of this is given in 7  In section 5 we compare the effectiveness and scalability of Round Robin and Random algorithms with those ones proposed in 4 9 and wi t h t h e I t e m G roupi ng Al gorithm our best algorithm so far published and presented in 8 The mai n i dea behi nd t h e I t e m G roupi ng Al gori t h m denoted by IGA is to group restricted association rules in groups of rules sharing the same itemsets If two restrictive rules intersect by sanitizing the conîicting sensitive transactions containing both restrictive rules one would take care of hiding these two restrictive rules in one step and consequently reduce the impact on the released database However clustering the restrictive rules based on the intersections between rules leads to groups that overlap since the intersection of itemsets is not transitive By solving the overlap between clusters and t hus isolating the groups we can use a representative of the itemset linking the restrictive rules in the same group as a victim item for all rules in the group By removing the victim item from the sensitive transactions related to the rules in the group all sensitive rules in the group would be hidden in one step 8 Thi s again minimizes the impact on the database and reduces the potential accidental hiding of legitimate rules Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


4.2.1 The Round Robin Algorithm The main idea behind the Round Robin Algorithm denoted by RRA is rather than selecting a unique victim item per given restrictive association rule we select different victim items in turns starting from the rst item then the second and so on in each sensitive transaction The process starts again at the rst item of the restrictive rule as a victim item each time the last item is reached The rationale behind this selection is that by removing one item at a time from the sensitive transactions it would alleviate the impact on the sanitized database and the legitimate association rules to be discovered since this strategy tries to balance the decreasing of the support of the items in restrictive association rules Selecting the sensitive transactions to sanitize is simply based on their degree of conîict Given the number of sensitive transactions to alter based on   this approach selects for each restrictive rule the sensitive transactions whose degree of conîict is sorted in descending order The rationale is that by sanitizing the conîict sensitive transactions that share a common item with more than one restrictive rule this optimizes the hiding strategy of such rules in one step and consequently minimizes the impact of the sanitization on the discovery of the legitimate association rules The sketch of the Round Robin Algorithm is given as follows Round Robin Algorithm Input D  R R   Output D  Step 1 For each association rule rr i  R R do 1 T  rr i   Find Sensitive Transactions rr i  D  Step 2 For each association rule rr i  R R do 1 Victim rr i  item v such that item v  rr i and if there are k items in rr i thei th item is assigned to item v mod k in round robin fashion Step 3 For each association rule rr i  R R do   T  rr i   is the number of sensitive transactions for rr i 1 NumbTrans rr i  T  rr i   1    Step 4 D   D For each association rule rr i  R R do 1 Sort Transactions T  rr i   in descending order of degree of conîict 2 T ransT oSanitize  Select rst NumbTrans rr i transactions from T  rr i  3 in D  foreach transaction t  T ransT oSanitize do 3.1 t   t  Victim rr i  End The four steps of this algorithm correspond to the four steps described above in the beginning of this section The rst step builds an inverted index of the items in D in one scan of the database In step 2 the victim item Victim rr i is selected in a round robin fashion for each restrictive associationrule.Line1instep3showsthat  is used to compute the number NumbTrans rr i of transactions to sanitize This means that the threshold  is actually a measure on the impact of the sanitization rather than a direct measure on the restricted association rules to hide or disclose Indirectly  does have an inîuence on the hiding or disclosure of restricted association rules There is actually only one scan of the database in the implementation of step 4 Transactions that do not need sanitization are directly copied from D to D   while the others are sanitized before copied to D   In our implementation the sensitive transactions to be cleansed are rst marked before the database scan for copying The selection of the sensitive transactions to sanitize T ransT oSanitize is based on their degree of conîict hence the sort in line 1 of step 4 When a transaction is selected for sanitization only the victim items are removed from it line 3.1 in step 4 4.2.2 The Random Algorithm The intuition behind the Random Algorithm denoted by RA is to select as a victim item for a given restrictive association rule one item of such rule randomly Like the Round Robin Algorithm the rationale behind this selection is that removing different items from the sensitive transactions would slightly minimize the support of legitimate association rules that would be available for being mined in the sanitized database Selecting the sensitive transactions to sanitize is simply based on their degree of conîict We evaluated the sanitization through the Random Algorithm by selecting sensitive transactions sorted in ascending and descending order The approach based on descending order in general yielded the best results That is why we have adopted such an approach for our algorithm The sketch of the Random Algorithm is given as follows Random Algorithm Input D  R R   Output D  Step 1 For each association rule rr i  R R do 1 T  rr i   Find Sensitive Transactions rr i  D  Step 2 For each association rule rr i  R R do 1 Victim rr i  item v such that item v  rr i and if there are k items in rr i  the item assigned to item v is random\(k Step 3 For each association rule rr i  R R do   T  rr i   is the number of sensitive transactions for rr i 1 NumbTrans rr i  T  rr i   1    Step 4 D   D For each association rule rr i  R R do 1 Sort Transactions T  rr i   in descending order of degree of conîict 2 T ransT oSanitize  Select rst NumbTrans rr i transactions from T  rr i  3 in D  foreach transaction t  T ransT oSanitize do 3.1 t   t  Victim rr i  End Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


The four steps of this algorithms correspond to those in the Round Robin Algorithm The only difference is that the Random Algorithm selects the victim item randomly while the Round Robin Algorithm selects the victim item taking turns 5 Experimental Results We performed two series of experiments the rst to measure the effectiveness of our sanitization algorithms and the second to measure the efìciency and scalability of the algorithms All the experiments were conducted on a PC AMD Athlon 1900/1600 SPEC CFP2000 588 with 1.2 GB of RAM running a Linux operating system To measure the effectiveness of the algorithms we used a dataset generated by the IBM synthetic data generator to generate a dataset containing 500 different items with 100K transactions in which the average size per transaction is 40 items The effectiveness is measured in terms of the number of restrictive association rules effectively hidden as well as the proportion of legitimate rules accidentally hidden due to the sanitization We selected for our experiments a set of ten restrictive association rules from the dataset ranging from two to ve items in length with support ranging from 20 to 42 and conìdence ranging from 80 to 100 in the database We ran the Apriori algorithm to select such association rules The time required to build the inverted le in main memory was 4.05 seconds Based on this inverted le we retrieved all the sensitive transactions in 1.02 seconds With our ten original restrictive association rules 94701 rules became restricted in the database since any association rule that contains restrictive rules should also be restricted 5.1 Measuring effectiveness In this section we measure the effectiveness of our algorithms taking into account the performance measures introduced in Section 3.3 We compare our algorithms with a similar one proposed in 4 t o h i d e r ul es by reduci ng s upport called Algo2a The algorithm GIH designed by Saygin et al 9 i s s imilar to Algo2a The bas ic dif ference is that in Algo2a some items are removed from sensitive transactions while in GIH a mark  unknowns is placed instead of item deletions Figure 4 shows a special case in which the disclosure threshold  is set to 0 that is no restrictive rule is allowed to be mined from the sanitized database In this situation 30.16 of the legitimate association rules in the case of RRA and RA 24.76 in the case of Algo2a and 20.08 in the case of IGA are accidentally hidden While the algorithms proposed in 4 9 h i d e r ul es reducing their absolute support in the database in our frame        0 5 10 15 20 25 30 35 IGA RRA RA A l g o2a Sanitizing Algorithms Misses Cost   IGA  RRA  RA  Al g o2a  Figure 4 Effect of  on misses cost work the process of modifying transactions satisìes a disclosure threshold  controlled by the database owner This threshold basically expresses how relaxed the privacy preserving mechanisms should be When  0  no restrictive association rules are allowed to be discovered When   100  there are no restrictions on the restrictive association rules The advantage of having this threshold is that it enables a compromise to be found between hiding association rules while missing legitimate ones and nding all legitimate association rules but uncovering restrictive ones Figure 5 shows the effect of the disclosure threshold  on the hiding failure and the misses cost for all three algorithms considering the minimum support threshold   5  Notice that RRA and RA yielded basically the same results That is why their curves are very identical at the scale of the gure As can be observed when  is 0 no restrictive association rule is disclosed for all three algorithms However 30.16 of the legitimate association rules in the case of RRA and RA and 20.08 in the case of IGA are accidentally hidden When  is equal to 100 all restrictive association rules are disclosed and no misses are recorded for legitimate rules What can also be observed is that the hiding failure for RA is slightly better than that for the other approaches On the other hand the impact of IGA on the database is smaller and the misses cost of IGA is the lowest among all approaches before   75  After this value all the algorithms yield similar results Regarding the third performance measure artifactual patterns one may claim that when we decrease the frequencies of some items the relative frequencies in the database may be modiìed by the sanitization process and new rules may emerge However in our experiments the problem artifactual pattern AP was always 0 with all algorithms regardless of the values of   Our sanitization indeed does not remove any transaction The same results can be observed for the algorithms presented in 4 9 We could measure the dissimilarity between the original and sanitized databases by computing the difference between their sizes in bytes However we believe that this Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


 0 20 40 60 80 100 120 0255075100 Disclosure Threshold Hiding Failure IGA RRA RA 0 5 10 15 20 25 30 35 0255075100 Disclosure Threshold Misses Cost IGA RRA RA Figure 5 Effect of  on the hiding failure and misses cost dissimilarity should be measured comparing their contents instead of their sizes Comparing their contents is more intuitive and gouges more accurately the modiìcations made to the transactions in the database To measure the dissimilarity between the original and the sanitized datasets we simply compare the difference of their histograms In this case the horizontal axis of a histogram contains all items in the dataset while the vertical axis corresponds to their frequencies The sum of the frequencies of all items gives the total of the histogram So the dissimilarity between D and D denoted by dif  D D   isgiven by dif  D D   1  n i 1 f D  i   n  i 1  f D  i   f D   i  where f X  i  represents the frequency of the i th item in the dataset X 0 1 2 3 4 5 6 7 IGA RRA RA A l g o2a Sanitizing Algorithms Dissimilarity IGA RRA RA Al g o2a Figure 6 Difference in size between D and D Figure 6 shows the differential between the initial size of the database and the size of the sanitized database when the disclosure threshold  0  To have the smallest impact possible on the database the sanitization algorithm should not reduce the size of the database signiìcantly As can be seen IGA is the one that impacts the least on the database In this particular case 3.55 of the database is lost in the case of IGA 6 in the case of RRA and RA and 5.24 in the case of Algo2a 0 1 2 3 4 5 6 7 0 25 50 75 100 Disclosure Threshold Dissimilarity IGA RRA RA Figure 7 Difference in size between D and D Figure 7 shows the differential between the initial size of the database and the size of the sanitized database for our three algorithms with respect to the disclosure threshold   Again IGA is the one that impacts the least on the database for all values of the disclosure threshold   Thus as can be seen the three algorithms slightly alter the data in the original database while enabling exibility for someone to tune them 5.2 CPU Time for the Sanitization Process We tested the scalability of our sanitization algorithms vis a-vis the size of the database as well as the number of rules to hide Our comparison study also includes the algorithm Algo2a We varied the size of the original database D from 20K transactions to 100K transactions while xing the disclosure threshold  and the support threshold to 0 and keeping the set of restrictive rules constant 10 original patterns Figure 8A shows that IGA RRA and RA increase CPU time linearly with the size of the database while the CPU time in Algo2a grows fast This is due the fact that Algo2a requires various scans over the original database while our algorithms require only two Note that our algorithms yield almost the same CPU time since they are very similar Although IGA sanitizes less sensitive transactions it has an Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


overhead to group restrictive association rules that share the same items and optimizes this process We also varied the number of restrictive rules to hide from approximately 6000 to 29500 while xing the size of the database to 100K transactions and xing the support and disclosure thresholds to   0 Figure 8B shows that our algorithms scale well with the number of rules to hide The gure reports the size of the original set of restricted rules which varied from 2 to 10 This makes the set of all restricted rules range from approximately 6097 to 29558 This scalability is mainly due to the inverted les we use in our approaches for indexing the transactions per item and indexing the sensitive transactions per restrictive rule There is no need to scan the database again whenever we want to access a transaction for sanitization purposes The inverted le gives direct access with pointers to the relevant transactions The CPU time for Algo2a is more expensive due the number of scans over the database 6 Related Work Some effort has been made to address the problem of privacy preservation in association rule mining The class of solutions for this problem has been restricted basically to randomization data partition and data sanitization In this work we focus on the latter category The idea behind data sanitization was introduced in 1 Atallah et al considered the problem of limiting disclosure of sensitive rules aiming at selectively hiding some frequent itemsets from large databases with as little impact on other non-sensitive frequent itemsets as possible Specifically the authors dealt with the problem of modifying a given database so that the support of a given set of sensitive rules mined from the database decreases below the minimum support value The authors focused on the theoretical approach and showed that the optimal sanitization is an NPhard problem In 4 th e a u t h o r s i n v estig ated co n  d e n tiality issu es o f a broad category of association rules and proposed some algorithms to preserve privacy of such rules above a given privacy threshold Although these algorithms ensure privacy preservation they are CPU-intensive since they require multiple scans over a transactional database In addition such algorithms in some way modiìes true data values and relationships by turning some items from 0 to 1 in some transactions In the same direction Saygin et al 9 i nt roduced a method for selectively removing individual values from a database to prevent the discovery of a set of rules while preserving the data for other applications They proposed some algorithms to obscure a given set of sensitive rules by replacing known values with unknowns while minimizing the side effects on non-sensitive rules These algorithms also require various scans to sanitize a database depending on the number of association rules to be hidden Oliveira and Za ane 8 i nt roduced a uni  e d frame w o rk that combines techniques for efìciently hiding restrictive patterns a transaction retrieval engine relying on an inverted le and Boolean queries and a set of algorithms to sanitize a database In this framework the sanitizing algorithms require two scans regardless of the database size and the number of restrictive patterns that must be protected The work presented here differs from the related work in some aspects as follows First we extended our previous work presented in 8 b y addi ng t w o n e w al gori t h ms Round Robin and Random to the set of sanitizing algorithms Second the hiding strategies behind our algorithms deal with the problem 1 and 2 in Figure 3 and most importantly they do not introduce the problem 3 since we do not add noise to the original data Third we study the impact of our hiding strategies in the original database by quantifying how much information is preserved after sanitizing a database So our focus is not only on hiding restrictive association rules but also on maximizing the discovery of rules after sanitizing a database Another difference of our algorithms from the related work is that our algorithms require only two scans over the original database while the algorithms presented in 4 9 requi re v a ri ous s cans depending on the number of association rules to be hidden This is due the fact that our sanitizing algorithms are built on indexes and consequently they achieve a reasonable performance 7 Conclusions In this paper we have introduced two algorithms for balancing privacy and knowledge discovery in association rule mining Our sanitizing algorithms require only two scans regardless of the database size and the number of restrictive association rules that must be protected This rst scan is required to build the index inverted le for speeding up the sanitization process while the second scan is used to sanitize the original database This represents a signiìcant improvement over the previous algorithms presented in the literature 4 9 Our algorithms are integrated to the framework presented in 8 which combines three adv ances for ef ciently hiding restrictive rules inverted les one for indexing the transactions per item and a second for indexing the sensitive transactions per restrictive association rule a transaction retrieval engine relying on Boolean queries for retrieving transaction IDs from the inverted le and combining the resulted lists and a set of sanitizing algorithms The experimental results revealed that our algorithms for sanitizing a transactional database can achieve reasonable Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


 0 20 40 60 80 100 120 140 20 40 60 80 100 DB Size \(transactions in thousands CPU Time \(sec IGA RRA RA Al g o2a 0 10 20 30 40 50 60 70 246810 Set of Restrictive Rules CPU Time \(sec IGA RRA RA Al g o2a AB Figure 8 Results of CPU time for the sanitization process results when compared with the other approaches in the literature Such algorithms slightly alter the data while enabling exibility for someone to tune them In particular the IGA algorithm reached the best performance in terms of dissimilarity and in terms of preservation of legitimate association rules On the other hand the results suggested that RA is slightly better than the other algorithms for hiding failure Although our algorithms guarantee privacy and do not introduce false drops to the data an extra cost is payed because some rules would be removed accidentally since there are functional dependencies between restricted and non-restricted rules The rationale behind this is that privacy preserving association rule mining deals with a tradeoff privacy and accuracy which are contradictory i.e improving one usually incurs a cost for the other It is important to note that our sanitization methods are robust in the sense that there is no de-sanitization possible The alterations to the original database are not saved anywhere since the owner of the database still keeps an original copy of the database intact while distributing the sanitized database Moreover there is no encryption involved There is no possible way to reproduce the original database from the sanitized one Currently we are investigating new optimal sanitization algorithms that minimize the impact in the sanitized database while facilitating proper information accuracy and mining In addition we are working on the optimization of the algorithms RRA and RA specially in terms of preservation of legitimate association rules since their results revealed they are promising 8 Acknowledgments Stanley Oliveira was partially supported by CNPq Conselho Nacional de Desenvolvimento Cient co e Tecnol ogico of Ministry for Science and Technology of Brazil under Grant No 200077/00-7 Osmar Za ane was partially supported by a Research Grant from NSERC Canada We would like to thank Y ucel Saygin and Elena Dasseni for providing us the code of their respective algorithms for our comparison study References 1 M  A tallah  E  Bertin o  A Elmag armid  M  I b r ah im an d V Verykios Disclosure Limitation of Sensitive Rules In Proc of IEEE Knowledge and Data Engineering Workshop  pages 45Ö52 Chicago Illinois November 1999  C  C l i f t on Usi ng S ampl e S i z e t o L i m i t E xposure t o Dat a Mi ning Journal of Computer Security  8\(4\:281Ö307 November 2000 3 C  C lifto n a n d D M a rk s Secu rity an d P ri v a c y Imp licatio n s o f Data Mining In Workshop on Data Mining and Knowledge Discovery  pages 15Ö19 Montreal Canada February 1996 4 E D a s s e n i  V S V e r y k i o s  A K E l m a g a r m i d  a n dE B e r t i n o  Hiding Association Rules by Using Conìdence and Support In Proc of the 4th Information Hiding Workshop  pages 369 383 Pittsburg PA April 2001 5 M  D ietzfelb in g e r  A R Karlin  K  M eh lh o r n  F  M  au f d er Heide H Rohnert and R E Tarjan Dynamic Perfect Hashing Upper and Lower Bounds SIAM Journal on Computing  23\(4\:738Ö761 1994  D E  OêL eary  Kno wledge Disco v e ry as a T hreat to Database Security In G Piatetsky-Shapiro and W J Frawley editors Knowledge Discovery in Databases AAAI/MIT Press pages 507-516 Menlo Park CA 1991 7 S  R M O l i v e i r a a n d O  R  Z a  ane A Framework for Enforcing Privacy in Mining Frequent Patterns Technical report TR02-13 Computer Science Department University of Alberta Canada June 2002 8 S  R M O l i v e i r a a n d O  R  Z a  ane Privacy Preserving Frequent Itemset Mining In Proc of the IEEE ICDM Workshop on Privacy Security and Data Mining  pages 43Ö54 Maebashi City Japan December 2002 9 Y  S aygi n V  S  V e r yki os and C  C l i f t on U s i n g U nkno w n s to Prevent Discovery of Association Rules SIGMOD Record  30\(4\:45Ö54 December 2001 Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


