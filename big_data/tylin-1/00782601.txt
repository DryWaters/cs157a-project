Discovering Pnteresting Prediction Rules with a Genetic Algorithm Edgar Noda 222 Alex A Freitas and Heitor S Lopes CEFET-PR CPGEI AV 7 de Setembro, 3 165 Curitiba  PR 80230-901 Brazil edgar@dainf.cefetpr  br hslopes@cpgei  cefetpr  br Abstract In essence the goal of data mining is to discover knowledge which is highly accurate comprehensible and 223interesting\224 surprising novel Although the literature emphasizes 
predictive accuracy and comprehensibility the discovery of interesting knowledge remains a formidable challenge for data mining algorithms In this paper we present a genetic algouithm designed om the scratch to discover interesting rules Our GA addresses the dependence modelling task where different rules can predict different goal attributes. This task can be regarded as a generalization of the classification task where all rules predict the same goal attribute 1 Imtroduction In essence, the goal of data mining is to discover knowledge which is highly accurate comprehensible and 223interesting\224 
surprising novel Many data mining algorithms are explicitly designed to discover accurate and comprehensible knowledge Actually the goal of discovering comprehensible knowledge is significantly facilitated by the use of rule induction algorithms \(including decision trees This kind of algorithm discovers high-level prediction rules in the form IF some conditions on the values of predicting attributes are true THEN predict a value for some goal attribute  However the discovery of truly interesting rules remains a formidable challenge for 
data mining Recently several authors have presented different viewpoints on what makes a discovered rule interesting For instance Freitas 981 Suzuki  Kodratoff 981 discuss some objective measures of rule interestingness or surprisingness while Liu et al 971 discusses a subjective criterion for evaluating rule interestingness 0-7803-5536-9/99/$10.00 0 1999 IEEE PUC-PR 2 PPGIA-CCET R Imaculada Conceicao 1 155 Curitiba  PR 80.215-901. Brazil alex@ppgia.pucpr.br http://www.ppgia.pucpr br/-alex This paper proposes a Genetic Algorithm GA 
designed to discover interesting prediction rules The proposed algorithm combines some characteristics of the GA-Nuggets algorithm Freitas 991 with some ideas on how to evaluate rule interestingness in an objective data-driven domain independent\manner Freitas 981 Our GA was designed for dependence modelling a data mining task which can be seen as a generalization of the classification task In this latter the aim is to predict the value of a special attribute called the goal attribute given the values of other attributes 
called predicting attributes. Hence, all rules have the same attribute in their consequent 223THEN part\224 In dependence modelling similarly to classification the aim is to discover rules that predict the value of a goal attribute given the values of predicting attributes However unlike classification there is more than one goal attribute Hence different rules can have different attributes in their consequent In our approach for dependence modelling the user specifies a small set of goal attributes which s is interested in predicting. Note 
that this task is different from the discovery of association rules In the latter the task is complety symmetric with respect to the attributes i.e any attribute can occur either in the antecedent or in the consequent of the rule In contrast in our dependence modelling task just a few user selected attributes can occur either in the antecedent or in the consequent of the rule All the other non-goal attributes can occur only in the rule 
antecedent Therefore for a given data set the dependence modelling task has a search space much larger than the classification task This is one of the motivations for using a more robust global-search method like GAS In contrast most data mining methods are based on the rule induction paradigm where the algorithm usually performs a kind of local search \(hill climbing Another related\motivation for using GAS is that they tend to cope better with attribute interaction when 1322 


compared with most rule induction methods Indeed in a CA the fitness hnction evaluates the individual in our case a candidate rule as a whole i.e all the interactions among attributes are taken into account In contrast most rule induction methods select one attribute at time and evaluate a partially constructed candidate rule rather than a hll candidate rule As a result most rule induction methods tend to be quite sensitive to attribute-interaction problems AI A2 A3 2 The Genetic Algorithm This section presents our GA designed for dependence modelling The algorithm is based on CA-Nuggets [Freitas 991 The current version of the system handles only categorical attributes Future enhancements will allow the use of continuous attributes     A\224 2.1 Individual\222s Encoding Each individual in this algorithm represents a candidate rule of the form 223if A then C\221 The antecedent of this rule can be formed by a conjunction of at most n  1 attributes, where n is the number of attributes being mined Each condition is of the form A  Kj where A is the i-th attribute and K is the j-rh value of the i-th attribute\222s domain The consequent consists of a single condition of the form Gk  Vk where Gk is the k-th goal attribute and Vkl is the I-th value of the k-th goal attribute\222s domain. The user specifies a set of potential goal attributes whose prediction is considered interesting Of course when a potential goal attribute occurs in the consequent of the rule it cannot occur in the antecedent of it However if a potential goal attribute does not occur in a rule consequent it can occur in the antecedent of that rule as if it was a predicting attribute A string of fixed size encodes an individual with n genes representing the values that each attribute can assume in the rule \(see Figure 1 The algorithm automatically chooses the best goal attribute to put in the consequent for a given rule antecedent see section 2.3 If an attribute is not present in the rule antecedent the value of its gene is 223-I\224 This value is a flag to indicate that the attribute does not occur in the rule antecedent Hence this encoding effectively represents a variable-length individual rule 2.2 Fitness Function The fitness hnction consists of two parts The first one measures the degree of interestingness of the rule, while the second measures its predictive accuracy The computation of the degree of interestingness of a rule in turn consists of two terms One of them refers to the antecedent of the rule and the other to the consequent. The degree of interestingness of the rule antecedent is calculated by an information-theoretical measure which is a normalized version of the measure propose by Freitas 981 Initially as a preprocessing step the algorithm calculates the information gain of each attribute ZnzoGain Cover  Thomas 911 Then the degree of interestingness of the rule antecedent AntZnt is given by AntInt  1  InfoGain\(A  Where n is the number of attributes in the antecedent and lDom\(Gk the domain cardinality i.e the number of possible values of the goal attribute Gk occurring in the consequent The log term is included in formula I to normalize the value of Ant so that this measure takes on a value between 0 and 1 The ZnfoGain is given by InfoGain\(Ai  Info\(Gk  Info\(Gk\(A 2 Where 31 r=l Where mk is the number of possible values of the goal attribute Gk  n is the number of possible values of the attribute A Pr\(X denotes the probability of X and Pr\(A denotes the conditional probability ofXgiven Y The Antlnt measure can be justified as follows Attributes with high information gain are good predictors of class when these attributes are considered individually i.e one at a time However from a rule interestingness point of view it is likely that the user already knows what are the best predictors individual attributes for its application domain, and rules containing these attributes would tend to have a low degree of interestingness for the user 1323 


On the other hand the user would tend to be more surprised if s saw a rule containing attributes with low information gain The user probably considered these attributes as irrelevant and they are kind of irrelevant for classification when considered individually one at time However attribute interactions can render an individually irrelevant attribute into a relevant one and this phenomenon is intuitively associated with rule interestingness Therefore all other things such as the prediction accuracy coverage and completeness of the rule being equal Freitas 981 argues that rules whose antecedent contain attributes with low information gain are more interesting surprising than rules whose antecedent contain attributes with high information gain The computation of the consequent\222s degree of interestingness is based on the following idea Freitas 991 the larger the relative frequency in the training set of the value being predicted by the consequent the less interesting it is h other words the rarer a value of a goal attribute the more interesting a rule predicting it is For instance a rule predicting a rare disease is much more interesting than a rule predicting a healthy condition, when 99 of the patients are healthy More precisely the formula for measuring the degree of interestingness of the rule consequent ConsInt is ConsInt  1  Pr\(G 151 where Pr\(G,S is the prior probability relative fi-equency of the goal attribute value Gkl and p is a user-specified parameter empirically set to 2 in our experiments The exponent 1/p in the equation 5 can be regarded as a way of reducing the influence of the rule consequent interestingness in the value of the fitness function The second part of the fitness function measures the predictive accuracy PredAcc of the rule and it is given by I A&CI-1/2 PredAcc  C61 I AI where v&CI is the number of examples that satisfy both the rule antecedent and the consequent and is the number of cases that satisfy only the rule antecedent The term  is subtracted in the numerator of equation 6 to penalize rules covering few training examples  see Quinlan 871 Finally the fitness function is AntInt  ConsInt 2   w2 PredAcc r71 W1 Fitness  wl  w2 Where W and W are user-defined weights In our experiment they are set to 1 and 2 repectively Note that PredAcc is given a greater weight than Antlnt and Conslnt All the components of equation 7 are normalized 2.3 Genetic Operators and Rule Consequent Formation Our GA uses the tournament selection where a pair of individuals is randomly picked and the one with the higher fitness is chosen for reproduction The crossover operator follows the idea of uniform crossover Syswerda 891 There is a probability for applying crossover to a pair of individuals and another probability for swapping each gene attribute value in the genome rule antecedent of two individuals After crossover is done the algorithm analyses if any invalid individual was created. If so a repair operator is used to produce valid-genotype individuals The rates used were 0.7 for the crossover operator and 0.5 for attribute value swapping The mutation operator randomly transforms the value of an attribute into another value belonging to the domain of that attribute The mutation rate used was 0.05 Besides crossover and mutation there is the insert and remove operators that directly try to control the size of the rules being evolved so influencing the comprehensibility of the rules These operators randomly insert or remove respectively a condition in the rule antecedent The probability of applying each of these operators depends on the number of attributes in the rule antecedent The insert operator has a null probability of application when the rule antecedent has the maximum number of attributes as specified by the user The removal operator works in the opposite way When the number of attributes in the rule antecedent is minimum as specified by the user it has a null probability of application The insert and remove rates used in our experiment are 0.5 and 0.7 respectively The previous genetic operators act in the rule antecedent Once these operators have been applied and the rule antecedent is formed the algorithm chooses the best consequent for each rule in such a way that maximizes the fitness of an individual candidate rule In other words the rule consequent is carehlly chosen in a deterministic manner to produce the best possible rule for a fixed antecedent In effect this approach gives the algorithm some knowledge of the data mining task being solved A similar approach has been used by some GAS designed to perform a classification task  see e.g Green  Smith 931 A kind of elitism is used in our algorithm In the task of dependence modelling there are several possible pairs of goal attribute goal attribute value that can occur in the rule consequent In each generation the best rule predicting each pair of goal attribute goal attribute value is passed unaltered to the next generation This corresponds to using an elitism factor of K where K is the number of distinct rule consequents occurring in the current population 1324 


3 The Data Sets Used in the Experiments The data sets used to test the algorithm were obtained tiom the UCI repository of machine learning databases http://www ics uci.eddAI/Machine-Learning html The data sets used are the Zoo and Nursery They are normally used for evaluating algorithms performing the classification task In the absence of a specific benchmark data set for the dependence modelling task these data sets were chosen because they seem to contain more than one potential goal attribute The zoo database contains 101 instances and 18 attributes Each instance corresponds to an animal In the pre processing phase the attribute containing the name of the animal was removed since this attribute has no generalization power The attributes in the zoo data set are all categorical The attribute names are as follows hair feathers eggs, milk predator toothed domestic backbone legs, tail, catsize, airborne, aquatic breathes venomous and type Except type and legs the attributes are boolean In our experiments the set of potential goal attributes used was predator domestic and type Predator and domestic are boolean attributes whereas the type attribute can take on seven different values The nursery school data set contains 12960 instances and 9 attributes The attributes are all categorical The attribute names are as follows parents health form children finance housing social has-nurs and recommendation In our experiments the attributes used as potential goal attributes were finance social and recommendation with 2 3,s possible values respectively 4 Results and Discussion In a classification task there are some well-defined measures of predictive accuracy For instance a commonly used measure despite its defects Hand 971 is the accuracy rate i.e the ratio of the number of correctly classified test instances over the total number of test instances The target of this work is the dependence modelling task which as mentioned before is a generalization of the classification task where different rules can predict different attributes In both tasks the evaluation of the discovered rules must take into account their predictive accuracy on a separate test set The difference is as follows In classification we usually aim at discovering a rule set that can classitj any test instance that appears in the future Hence it makes sense to compute an accuracy rate or related measure over all instances in the test set In dependence modelling in the sense addressed in this paper we do not aim to classify the whole rest set Rather the goal is to discover a few interesting rules to be shown to a user We can think of the discovered rules as the most valuable 223knowledge nuggets\224 extracted from the data These knowledge nuggets are valuable even if they do not cover the whole test set In other words the value of the discovered rules depends on their predictive accuracy on the part of the test set covered by those rules but not on the test set as a whole Afier all there are several goal attributes and it is not realistic to expect that the discovered rules can predict the value of all goal attributes for all instances in the test set In fact we could mine such a large rule set by running one classification algorithm for each goal attribute but we would get too many rules and the task being solved would be simply \223multiple classification\224 In contrast in the dependence modelling task addressed in this paper we aim at discovering a much smaller set of interesting rules Hence it does not make much sense to evaluate the performance of the discovered rule set as a whole in test set and the discovered rules are better evaluated on a rule-by rule basis Within this spirits for each data set zoo nursery we performed two experiments The first one consisted of using cross-validation with factor 5 to evaluate the quality of the discovered rules Hence the data set is divided into 5 mutually exclusive and exhaustive partitions The GA is then run 5 times Each time a different partition is used as the test set and other 4 partitions are merged and used as the training set The results of the 5 runs are then averaged The second experiment consisted of using the full data set i.e all the 5 partitions associated with cross validation to discover the final rules to be reported to the user Obviously the predictive power of these final rules is not estimated by their predictive accuracy on the full data set but rather by the average predictive accuracy on the test set computed in the first experiment cross-validation procedure However this second experiment discovering rules from the full data set is necessary and advisable for two reasons First although we can use cross-validation to compute the average of some rule-quality indicators such predictive accuracy we cannot use cross-validation to compute \223average rules\224 \(note that each of 5 runs associated with cross-validation discovers a potentially different set of rules Second The rules discovered from the full data set have the advantage of being discovered from a somewhat larger data set than the rules discovered during cross-validation If the reader is not familiar with above-described usage of cross-validation and subsequent learning i?om the full data set s he is referred to Hand 971 The Results for the Zoo and Nursery data sets are reported and discussed in the next two subsections. For the Zoo data set the population consisted of 100 individuals and the number of generations was 100 For the nursery data set the population consisted of 50 individuals and the number of generations was 100 1325 


4.1 Results for the Zoo Data Set Table 1 shows the average results over the 5 runs of the cross-validation procedure for the zoo data set Each row of this table is associated with the best-discovered rule for a different rule consequent a pair goal attribute goal attribute value In the Zoo data set there are 11 distinct rule consequents since the goal attributes predator domestic and type can take on 2 2 and 7 distinct values respectively Hence table 1 has 11 rows Note that each row is associated with an individual of the last generation since we used an elitism strategy for preserving the best rule for each rule consequent The first column of Table 1 indicates the pair goal attribute goal attribute value characterizing the rule consequent The second column shows the average value of the degree of interestingness of the rule antecedent AnZnt equation l for the rules having the consequent specified in the first column. This average was computed over 5 rules namely the best rule having that consequent in each of the 5 ms of the cross-validation procedure The thud column of table 1 shows the value of the degree of interestingness of the rule consequent ConsZnt equation 5 for all the rules having the consequent specified in the first column. \(Note that the value of ConsZnt is constant for all the rules with a given consequent The fourth and fifth columns of Table 1 show the predictive accuracy on the training and test sets for the rules having the consequent specified in the first column The sixth column shows the average number of examples covered for the rules having the consequent specified in the first column The average results reported in the fourth fifth and sixth columns were computed in a manner similar to the above-described computation of the average for the second column 221fie predictive accuracy on the training set was computed as defined in equation 6 The predictive accuracy on the test was computed by a simplified version of equation 6 namely c4 h CJ  PI Note that in the case of the test set there is no need to subtract  fiom h CJ as it was done for the training set That subtraction was used in training set to compensate for the fact that the measure c4  CJ  is a too optimistic estimate of the predictive accuracy of a rule on unseen data Quinlan 871 Hence this correction is not necessary in the test set which contains data unseen during training As can be seen in Table 1 overall the discovered rules have a very high value of AntZnt i.e their antecedent are considered by the measure specified in equation l to be very interesting surprising In addition Table 1 shows that most of the discovered rules have a good generalization performance on the test set Five out of the 1 1 rules have a predictive accuracy of 100 in the test set Two other rules have a high predictive accuracy in the test set keeping the same performance level as in the training set Only four rules have a predictive accuracy in the test set significantly smaller than the one in the training set Note that the value of JAJ is not as small as it might look at first glance since the test set has only 20 instances The final rules discovered fiom the full data set are show in Table 2 The table shows the best rule for each possible rule consequent As explained before the quality of these rules is best estimated by looking up the corresponding rows in Table 1 In any case for the sake of completeness we have included in table 2 for each rule the antecedent\222s degree of interest AntZnt the consequent\222s degree of interest ConsZnt the number of examples covered by the rule antecedent MI and the number of correctly predicted examples 1.4  CI Note that most of the rules are not only interesting and highly accurate as shown in Table 1 but also comprehensible at least in the sense of representing high level knowledge and not being very long Table 1 Results of cross-validation for the Zoo data set AntZnt I ConsZnt I PredAcc I PredAcc I VI I 1326 


Table 2 Results of learning from the full zoo data set Goal Value predator false predator, true Discovered Rule Antlnt Conslnt p&cl AI If hah=O and eggs and milkO and 0.9843 1 1 0.74461 8 4 4 If\(airbone=O and\(aquatic=l and backbone=l and 0.952456 0.667491 11 11 backbone=l and tail=l and domestic=l Then predator=O I catsize=l Then predato~l I 1 I I domestic, false I lf\(eggs=l airbone=O and predatot-1 and I 0.957968 I 0.358766 I 22 I 22 domestic true type 1 type 2 type 3 type 4 type 5 type 6 type 72 venomous=O Then Domestic=O Then domestic=l lf\(eggs=O aquatic=O and tail=O 0.959166 0.933428 1 1 If\(eggs=O venomous=O domestic=O Then 0.949641 0.770752 32 32 If\(feathers=l venomous=O Domestic=O 0.95521 3 0.895533 17 17 tYPe=l Then type=2 toothed 1 and\(fins=O and\(domestic=O and\(catsize=O Then\(type=3 and\(tail 1 Then\(type=4 breathes 1  catsize=O Then\(type5 If eggs 1  aquatic=O predator 1  0.936044 0.97493 3 3 3 Iflaquatic 1  breathes=O venomous=O 0.939000 0.93 3428 12 12 Iflairbone=O aquatic=l toothed=l 0.92 1090 0.979998 4 4 I f\(predator I  breathes=O tail=O 0.953098 0.949205 7 7 If\(airbone=l fins=O tail=O Then\(type6 0.928637 0.959579 6 6 Dom est ic=O Th en\(type=7 4.2 Results for the Nursery Data Set Table 3 shows the average results over the 5 runs of the cross-validation procedure for the Nursery data set The meaning of the columns of table 3 is analogous to the meaning of Table 1\222s columns as explained in the previous section In the nursery data set there are 10 distinct rule consequents since the goal attribute finance social and recommendation can take on 2 3 and 5 values respectively The results reported in Table 3 are even better than the results reported in Table 1 Again overall the discovered rules have a high value of Antlnt and have a very good generalization performance on the test set Seven out of the tem rules have a predictive accuracy of 100 in the test set The other three rules have a much lower performance in the test set The final rules discovered fiom the full data set are shown in table 4 The table shows the best rule for each possible consequent The meaning of Table 4\222s columns is analogous to the meaning of Table 2\222s columns Again we warn the reader that the last 4 columns of Table 4 were included only for the sake of completeness, since the quality of the rules shown in Table 4 is best estimated by the correspond rows in Table 3 Table 3 Results of cross-validation for the Nursery data set I Goal atiribute Attribute value  I Antlnt recommendation, recommended  recommendation very recorn I 0.9423296 I 0.9805278 I 0.9566708 I 1 o I 3.0 recommendation priority I 0.921443 I 0.7744442 I 0.9949844 I 1 o I 21.4 recommendation, spec prior I 0.92 12322 I 0.8788452 1 0.9949754 I I O 1 22.8 1327 


convenient 0.8 16497 0.816497 0.816497 I 1 0.999882 0.980528 I L I 0.878845 1 0.774445 social nongrob r Discovered rule If\(has-nurs=very-crit and\(children= more and\(health=recommended\and\(class=priority Then financeconvenient If\(has-nurs=very_crit\and\(housing=convenient and\(social=slightlygrob and \(health-7ecommended and class=specqrior Then finan ce=incov If\(form=complete\and\(housing=critical and\(class=ver y-recom Then sociahorn prob social problematie Antlnt 0.998888 0.999 1 1 1 0.996424 recommendation not-recom Then class=n<t-recim If\(children=l Then class=recommended 0.997195 recommendation recommended recommendation very-recom  recommendation priority recommendation specgrior If\(parents=usual hasnurs=critical and 10.997318 If\(parents=pretentious has-nurs=lessgroper and \(housing=convenient\and \(finance=convenient and social=slightlygrob and health=recommended health=recommended\and class=specgrior housing=less-conv and \(finance=incov health=not recom 0.943364 Then \(class=very recom I If parents=pretensious\and has-nurs=lessgroper I 0.93421 1 and \(form=more\and health-riority form=more\and \(fmance=incov and 5 Conclusion and Future Work Our genetic algorithm GA was designed from the scratch to discover a few interesting rules which might be called 223knowledge nuggets\224 rather than to discover a large set of accurate \(but not necessarily interesting\rules The results reported in this paper are very promising since the GA did discover rules which were both highly accurate on an unseen test set and interesting However, a more extensive empirical evaluation of our GA would be useful and will be object of future research We also intend to extend the GA described in this paper to cope with continuous attributes \(the current implementation can handle only categorical attributes Conslnt 0.706733 0.707481 0.8 16497 T 720 2160 7-p Another direction for further research would be to evaluate the performance of the GA with other rule interestingness measures proposed in the literature Bibliography Cover  Thomas 911 Cover T M and Thomas J A Elements of Information Theory John Wiley Sons 1991 Freitas 981 Freitas A A On objective measures of rule surprisingness Roc 2\224 European Symp. on Principles of Data Mining and Knowledge Discovery PKDD-98 Lecture Notes in Artificial Intelligence 15 10 1-9 1 998 1328 


Freitas 991 Freitas A A A genetic algorithm for generalized rule induction In R Roy et al Advances in Soft Computing  Engineering Design and Manufacturing 340-353 Springer-Verlag 1999 Greene  Smith 931 Greene D P  Smith S F Competition-based induction of decision models om examples Machine Learning 13,229-257 1 993 Hand 971 Hand d Construction and Assessment of ClassiJcation Rules John Willey &sons 1 997 Liu et al 971 Liu B Hsu W  Chen S Using general impressions to analyze discovered class fication rules Roc 31d Int Conf on Knowledge Discovery  Data Mining KDD-97\31-36 AAAI Press 1997 Quinlan 871 Quinlan J R Generating production rules from decision trees Roc IJCAI-87,304-307 \(1987 Suzuki  Kodratoff 981 Suzuki E  Kodratoff Y Discovery of surprising exception rules based on intensity of implication Roc 2"d European Symp on Principles of Data Mining and Knowledge Discovery PKDD-98 Lecture Notes in Artificial Intelligence 1510 10-18. \(1998 Syswerda 891 Syswerda G Ungorm Crossover in genetic Algorithms Roc 3 International conference on genetic algorithms \(ICGA89 2  9 1989 1329 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


