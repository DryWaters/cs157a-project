Determination of Rule Weights of Fuzzy Association Rules Hisao Ishibuchi Takashi Yamamoto and Tomoharu Nakashima Department of Industrial Engineering Osaka Prefecture University 1-1 Gakuen-cho Sakai Osaka 599-8531 Japan hisaoi yama nakashi} @ie.osakafu-u.ac.jp Abstract In this paper first we extend two basic measnres of association rules in data mining i.e confidence and support to the case of fimy association rules The main difference between standard and fuzzy association rules is the discretization of continuous variables While continuous variables are divided into intervals for generating standard association rules they are divided into lingnistic valnes in the case of fuzzy association des Next we examine 
two specifications of rule weights of fuzzy association rules for pattern classification problems One is the direct nse of the confidence as a de weight The other is based on a slightly complicated formulation where the rule weight of each frizzy association rule is discounted by the confidence of other rnles with the same antecedent conditions and different consequent classes Through computer simulations on a pattern Classification problem with many continuous attrihutes we compare these two definitions with each other Simulation resnlts show that the direct use of the confidence is inferior to the other defmition of rule weights Then we examine three de selection criteria Le confidence support 
and their prodnct It is shown that good fumy association rules are extracted from nnmerical data using the prodnct criterion Finally we compare the performance of fimy association rules with that of standard association rules I WODUCTION When our knowledge extraction task involves numerical data with continuous attributes each attribute is usually discretized into several intervals 1,2 The discretization into intervals is used in many machine learning techniques such as decision trees 3 In some situations human knowledge exactly corresponds to such discretization of continuous attributes For example the domain interval of our ages is divided into two intervals by the threshold age 20 in the following knowledge 223People under 20 me nof allowed fo smoke\224 In 
other situations the discretization into intervals is not appropriate for describing human knowledge. For example we may have the following knowledge 223Tall people are no comfortable in small cars 224 We cannot appropriately represent this knowledge using the discretization of the domain interval of our height into intervals This is because the linguistic value 223tall\224 cannot be appropriately represented by an interval Mathematical framework for representing linguistic values is fuzzy logic Fuzzy logic has been recognized as a convenient tool for handling continuous attributes by rule-based systems in a human understandable manner 4 Such recognition is supported by many successful applications of fuzzy control methods 5 0-7803-7293-X/01/$17.00 0 2001 IEEE 1555 For hction approximation problems with n inputs we use 
fuzzy if-then rules of the following form Rule R If xl is A,1 and  and x is A then y is B 1 where R is the label of the q-th fuzzy if-then rule x XI  X  is an n-dimensional input vector Aqi is an antecedent linguistic value such as 223small\224 and 223large\224 y is an output variable and B is a consequent linguistic value The fuzzy if-then rule in 1 can be viewed as a fuzzy association rule A 3 B where Ai   Ail   Ai   The main difference between 
fuzzy and non-fuzzy association rules 6 is that the domain of each input and output\variable is fuzzily divided into linguistic values in fuzzy association rules On the other hand we use fuzzy if-then rules of the following form for pattern classification problems Rule R If x1 is A,1 and  and x is A thenclass C 2 where x  XI  x  is an n-dimensional pattern vector and C is a consequent class The fuzzy if-then rule in 2 can be viewed as a fuzzy association rule A 3 C  In this paper we first extend the definitions of the contidence and the support of standard association rules 
6 to the cases of the fuzzy association rules A 3 B and A aC 7 Next we examine two definitions of rule weights of fuzzy association rules for pattern classification problems We also examine three criteria i.e confidence support and their product\that are used for extracting fuzzy association rules om numerical data Finally we compare the performance of fuzzy association rules with that of non-fuzzy association rules 11 FUZZY ASSOCIATION RULES FOR APPROXIMATION Let us assume that we have m input-output pairs  xp  yp   p  1,2  m where xp is an n-dimensional input vector i.e xp 
xpl  xpn   and yp is the corresponding output value Our data set D consists of these m input-output pairs Thus the cardinality of D is m Le ID I m  In the same manner as standard association rules 6 the confidence of the fuzzy association rule A a B is defined as 3 where D\(A  is a fuzzy set of input-output pairs that are compatible with the antecedent part A D Bq  is a fuzzy set of input-output pairs compatible with the consequent part 2001 IEEE International Fuzzy Systems Conference 


B  and I 1 denotes the cardinality of a fuzzy set Eq.\(3 can be calculated from the compatibility grades LA xp  and pclg yp of xp yp with the antecedent part A and the consequent part B as follows 7 m p=l c PAq xp 1 PB up  C\(A s B    4 m P=l z PAq xp 1 The compatibility grade p xp  is calculated fiom the membership function p xi  of A,i as PAq xp xpl  PA xpn In the same manner the support of the fuzzy association rule A a B is calculated as follows 7 I w   i S\(A B   ID1 rn PAq xp PEq yp p=l  6   m III FUZZY ASSOCIATION RULES FOR CLASSIFICATION Let us assume that we have m labeled patterns  xp  tp   p  1,2  m from A4 classes where xp is an n-dimensional pattern vector and tp is the class label of xp  As in the previous section we can define the confidence and the support of the fuzzy association rule A sC from the given training patterns Since C is a non-fuzzy class label the compatibility grade of tp with C is 0 or 1 pq tp  1 if p~ClassC otherwise pcq\(tp The confidence of the fuzzy association rule A s C is calculated as follows 7 p=l The support of A s C is calculated as follows 7 IV PERFORMANCE EVALUATION OF FUZZY RULES In this section we examine the performance of fuzzy association rules for pattern classification problems First we generate fuzzy association rules of the following type from training patterns R If xl is A and  and x is A then C with CF where CFq is a rule weight i.e certainty factor Then the performance of the generated rules is examined by classifying unseen test patterns as well as the training patterns Two definitions of rule weighs and three rule selection criteria are examined through computer simulations A Fuzzy Rule Extractionfiom Numerical Data For simplicity of explanation we assume the pattern space of our pattern classification problem is an n-dimensional hypercube  0,1 Our fuzzy rule extraction is based on the discretization of each continuous attribute into some linguistic values Fig 1 shows an example of the fuzzy discretization which is derived from the uniform discretization into three intervals In this paper we assume that the fuzzy discretization of each continuous attribute is given \(i.e we assume that a set of linguistic values is given for each continuous attribute 9 Membership m l.OW 0.0 I 1 I 0.0 1 O I I Fig 1 An example of the filzzy discretization The antecedent part of the fuzzy association rule R is specified by a combination of the given linguistic values For high-dimensional pattern classification problems with many attributes it is not practical to examine all combinations of antecedent conditions for generating fuzzy association rules Thus we only examine combinations of antecedent conditions including many don't cure conditions Since don't care conditions are omitted fuzzy association rules generated fiom such combhations are short The number of antecedent conditions \(excluding don't care conditions of each rule is referred to as the rule length In our computer simulations we generate association rules of the length three or less i.e., with three or less antecedent conditions The consequent class C and the rule weight CF are determined from the given training patterns for each combination of antecedent conditions It is natural to choose the class with the maximum confidence as the consequent class C for the antecedent part A That is the consequent class C of the fuzzy association rule R in 9 is specified as c\(Aq Cq   max{c\(Aq Class I  c\(A aClass M It is also natural to use the confidence c A 3 C  as the rule weight CF of the fuzzy association rule R  10 1556 


CFq c\(Aq C  1 1 In Cordon et al.[8 rule weights were determined in this manner A different formulation was proposed for determining rule weights in our former study 9 The fornulation can be rewritten using the confidence as CFq c\(A SCq 12 where  C is the average of c\(Aq  Class t over the A4 1 classes except for the consequent class C  B Comparison of Two Definitions of Rule Weights For comparing the two definitions of rule weights with each other we used the wine recognition database in the UCI Machine Learning Repository The wine data set is a three class classification problem with 178 patterns and 13 continuous attributes As a preprocessing procedure we normalized each attribute value into a real number in the unit interval 0 11 The domain of each attribute was discretized into three linguistic values as shown in Fig 1 First we evaluated classification rates on training patterns That is we generated fuzzy association rules using all the given 178 patterns Only fuzzy association rules of the length three or less were considered The consequent class of each rule was determined by 10 The generated rules were divided into three groups according to their consequent classes Fuzzy association rules in each gmup were sorted in a descending order of the product of the confidence and the support When multiple rules had the same value of the product they were randomly sorted. The fist N/3 rules were selected fkom each group In this manner the product of the confidence and the support was used as a rule selection criterion. The performance of the selected rules was examined by classifying all the given 178 patterns In the classification phase the two definitions of rule weights were examined We also examined the case of no rule weights which was simulated by assigning the same rule weight to all rules i.e CFq 1.0 for Vq  Since our rule selection procedure used the random tiebreak mechanism average results were calculated over ten independent runs Simulation results are summarized in Table 1 From this table we can see that better results were obtained fkom the def~tion in 12 13 than the direct use of the confidence on the average We can also see that the performance of fuzzy association rules with no rule weights drastically deteriorated when more than 30 rules were selected Table 1 Comparison among three specifications of rule weights using classification rates on training data We also examined the generalization ability of fuzzy rule based systems using the leaving-one-out LVl technique The whole LVl procedure i.e 178 iterations of the design and test of a fuzzy rule-based system for wine data was iterated five times for calculating the average performance Simulation results are summarized in Table 2 As in Table 1 better results were obtained from the definition of rule weights in 12 13 than the direct use of the confidence Table 2 Cornparison among three specifications of rule weights using classification rates on test data C Comparison of Three Rule Selection Criferin In the previous subsection, we used the product of the confidence and the support for selecting fuzzy association rules We also examined the confidence and the support as a rule selection criterion using the definition of rule weights in 12 13 Simulation results on training data and test data are summarized in Table 3 and Table 4 respectively. From these tables we can see that the best results were obtained kom the product criterion among the three criteria Poor results were obtained kom the confidence criterion because this criterion tends to select association rules with low support Table 3 Comparison among three rule selection criteria using classification rates on training data Table 4 Comparison among three rule selection criteria using classification rates on test data v COMPAREON BETWEEN FUZZY AND STANDARD RULES In this section we examine the performance of standard non-fuzzy association rules. Instead of the fuzzy discretization in Fig 1 we used the corresponding non-fuzzy discretization into three intervals which was also shown in Fig 1 Computer simulations were performed in the same manner as the previous section Classification rates evaluated by the LVl techniques are summarized in Table 5 and Table 6 which correspond to Table 2 and Table 4 respectively 1557 


Table 5 Classification rates on test data by standard association rules with different specifications of rule weights Direct use 86.5 91.6 92.9 92.7 92.7 96.5 95.7 94.4 93.3 93.3 91.0 13 86.5 91.6 92.9 92.7 92.7 96.5 95.7 94.4 93.3 93.3 91.0 No weights 84.3 79.2 80.0 80.3 79.8 79.8 72.5 20.8 16.9 14.0 3.9 Table 6 Classification rates on test data by standard association rules with different rule selection criteria ofrules Confidence Support From Table 5 we can see that the same results were obtained from the two definitions of rule weights This is because the role of the rule weight of each standard association rule is to specify its priority in the classification phase Since the same priority was assigned to each rule by the two definitions of rule weights there was no difference in the simulation results by the two definitions On the other hand in the case of fuzzy association rules, rule weights can adjust the location of classification boundaries \(see lo From the comparison between Table 4 the best classification rate 96.6 and Table 6 the best classification rate 90.9 we can see that higher generalization ability was obtained hm fuzzy association rules While standard association rules did not work well in the case of the uniform discretization in Fig 1 classification performance can be drastically improved by choosing an appropriate pattition for each continuous attribute We used the entropy measure 3 for independently dividing the domain of each continuous attribute into three intervals Two threshold values were selected from 177 candidate values each of which was the mid-point of a pair of neighboring attribute values All the 177 C2 combinations were examined for choosing the discretization with the minimum entropy for each of the 13 continuous attributes Simulation results are summarized in Table 7 and Table 8 From these tables we can see that the generalization ability was improved by choosing an appropriate partition for each continuous attribute 3 6 9 12 15 30 60 90 120 150 300 10.4 20.3 28.1 34.9 40.4 60.7 74.3 78.9 79.4 76.6 67.9 82.2 93.6 97.1 96.7 97.2 98.2 97.5 96.1 95.4 93.8 91.6 VI CONCLUDING REMARKS In this paper we first explained the extension of the deikitions of the confidence and the support to the case of fuzzy association rules. Then we compared two definitions of rule weights with each other by examining the performance of fuzzy association rules Simulation results showed that the direct use of the confidence was inferior to the other definition We also examined three criteria i.e confidence support and their product for selecting fuzzy association rules The best results were obtained by the product criterion Finally we compared fuzzy discretization with non-fuzzy discretization It was shown that the performance of non-fuzzy association rules was sensitive to the choice of partitions than that of fuzzy association rules It was also shown that the generalization ability of non-fuzzy association rules gradually decreased with the number of rules while such deterioration was not clear in the case of fuzzy association rules These observations suggest high robustness of the classification performance of fuzzy rule based systems Table 7 Classification rates on test data by standard association with different specification of rule weights and entropy-based discretization ofrules1 3 I 6 I 9 I12 I15 I30 I60 I90 112011501300 Table 8 Classification rates on test data by standard association rules with different rule selection criteria and entropy-based discretization I Product 186.5191.6192.9192.7192.7196.5195.7194.4193.3193.3191.01 REFERENCES 1]U M Fayyad and K B Irani 223Multi-interval discretization of continuous-valued attributes for classification learning,\224 Pmc of 13th International Joint Confffence on Artificial Intelligence pp 2]J Dougherty R Kohavi and M Sahami 223Supervised and unsupervised discretization of continuous features,\224 Proc of IZth International Conference on Machine Learning pp 194-202 1995 3]J R QuinIan 124.5 Program for Machine Learning Morgan Kaufmann 1993 4]S J Russell and P Norvig Artificial Intelligence A Modem Approach Prentice-Hall 1995 5]C T Leondes Ed Fuzzy Theory Systems Techniques and Applications Vols 1-4 Academic Press 1999 6 R Agrawd and R Srikant, \223Fast algorithms for mining association rules,\224 Proc of 20th International Conference on Very Large Data Bases Santiago, Chile\pp 487-499 September 1994 Expanded version is available as LEM Research Report RJ9839 June 1994 7]H Ishibuchi T Yamamoto and T Nakashima 223Fuzzy data mining Effect of fuzzy discretization,\224 Proc of 2001 IEEE International Conference on Data Mining San Jose November 29  December 2 to appear 8]0 Cordon M J del Jesus F Herren 223A proposal on reasoning methods in fuzzy rule-based classification systems,\224 International Journal ofApproximate Reasoning vol 20 no 1 pp 21-45 1999 9]H Ishibuchi K No and H Tanaka 223Distributed repramtation of film rules and its application to pattern classification,\224 Fuzzy Sets and Systems vol 52 no I 1992 lo H Ishibuchi and T Nakashima 221\221Effect of rule weights in fuzzy rule-based classification systems,\224 IEEE Trans on Ft\(ziy Systems to appear 1022-1027, 1993 1558 


 Memory ptr/File offset  Disk Mini Chunks Chunk Structures Data Cubes Cube Directory Main Memory Paged Paged Paged Processor P Status= F Data Cube *ptr File offset Status= T Data Cube *ptr File offset Totalchunks \(m Status distribution OI U I : In memory O: On Disk U: Unallocated Status Type Ntuples Highmark mem_ptr fileoffset status = INMEM status BESS Value 234.56 001010 sorted by BESS ONDISK Files  0 1 N-1 i Chunk status \(0 <= i < m Figure 4 Data Structures on a processor P i whether the chunk structure for the cube is in memory  status  a pointer to the chunk structure if it is in memory and a 002le offset if it is on disk The total number of chunks for the chunk structure of the cube is in totalchunks  Additionally for each chunk of the chunk structure a chunk status cstatus is maintained to keep track of chunk structure paging The chunk address is a pointer to the chunk structure in memory which stores information for each chunk This is when cstatus is set to INMEM Otherwise cstatus can either be UNALLOCATED or ONDISK In the latter case the chunk address will be a 002le offset value For a multidimensional array the size of the array and the dimension factor in each dimension are stored to lookup for the calculations involving aggregations instead of calculating them on the 003y every time A chunk structure for a sub-cube can either be in its entirety or parts of it can be allocated as they are referred to The cstatus 002eld of the data cube will keep track of allocations Chunk structure keep track of the number of BESS  value pairs  ntuples  in the chunk which are stored in minichunks  Whether a chunk is dense or sparse is tracked by type  A dense chunk has a memory pointer to a dense array whereas a sparse chunk has a memory pointer to a minichunk Chunk index for each dimension in the cube topology is encoded in a 8 byte value cidx  Further dimensions of the chunk are encoded in another 8 byte value cdim  This allows for quick access to these values instead of calculating them on the 003y Minichunks can either be unallocated UNALLOCATED in memory INMEM on disk ONDISK or both in memory and on disk INMEM ONDISK Initially a minichunk for a chunk is allocated memory when a value maps to the chunk UNALLOCATED  INMEM When the minichunk is 002lled it is written to disk and its memory reused for the next minichunk INMEM  INMEM ONDISK Finally when a minichunk is purged to disk it is deallocated INMEM ONDISK  ONDISK A chunk can thus have multiple minichunks Hence choosing the minichunk size is an important parameter to control the number of disk I/O operations for aggregation calculations 5 Algorithms and Analysis Since chunks can either be sparse or dense we need methods to aggregate sparse chunks with sparse chunks sparse with dense chunks and dense with dense chunks The case of dense chunks to sparse chunk does not arise since a dense chunk does not get converted to a sparse chunk ever Also a chunked organization may be converted into a multidimensional array The various options are illustrated in Figure 3 In this section we discuss the algorithms for cube aggregations and chunk mappings 5.1 Ch unk mapping to pro cessors Each chunk in the source cube is processed to map its values to the target chunk The chunk structure carries information about the chunk's dimensional offsets in cidx This along with cdim  the chunk extents is used to calculate the local value in each dimension For distributed dimensions we need to add the start of the processor range to calculate the global value This is then used to calculate the target start and stop values This is used to determine the destination target processor and the target chunk The source can map to the same target chunk on the same processor same target chunk on another processor split among chunks on the same processor or split among chunks on different processors These cases are illustrated in Figure 5 for a two dimensional source to target aggregation of chunks It describes the chunk mapping process and the distinction between split and non-split chunks local mapping and nonlocal mappings For a detailed algorithm refer to A split chunk needs to evaluate each of the index values by decoding the BESS values and adding it to the chunk index values A target processor needs to be evaluated for the distributed dimensions since this can potentially be different For a split source chunk a corrected BESS+value and target chunk id is sent otherwise just the BESS+value is sent Asynchronous send is used to overlap computation and communication Hence before the send buffer to a processor is reused a receive of the previous send must be completed Asynchronous receive operations are posted from all processors and periodically checked to complete the appropriate sends A processor receives the BESS values and the target chunk id and does the aggregation operation For a conversion of a chunked source cube to a multidimensional target array offsets are calculated Dense chunks are similarly treated 


 B Split chunk on single processor A 0 10 20 60 40 30 50 Y X 10 40 60 0 C C 25 B 55 ABC ->  BC P0 P1 P2 P3 P4 P5 P8 P7 P6 P9 P11 P10 P0 P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 15 30 45 Non-split chunk \(Local Split chunk across processor boundary Non-split chunk \(Non local Figure 5 Chunk mapping after aggregation for a 2D cube distribution 5.2 Aggregation Computations The same partitioning dimensions in the source and target sub-cubes result in a local aggregation calculation For example AB C  AB has both A and B partitioned in both sub-cubes and this results in a local aggregation On the other hand AB C  A B  has only A partitioned in the result sub-cube and B goes from being distributed to being undistributed This results in communication and is a non local aggregation Other cases are illustrated in Table 1 The extents of a chunk of the aggregat ing cube can be contained in the extents of a chunk of the aggregat ed cube In this case the BESS+value pairs are directly mapped to the target chunk locally or non-locally However the BESS index values need to be modi\002ed to encode the offsets of the new chunk Figure 6 illustrates a case for A B  A where the chunks with A extents of 10 000 13 on processor P0 map to the chunk with extents 9 000 13 on processor P1 The BESS encoding of A needs to be incremented by 1 to correctly re\003ect the target BESS encoding If the chunk is overlapping over a target chunk boundary then each BESS value has to be extracted to determine its target chunk This is computationally more expensive than the direct case It is to be noted that a 2 dimensional distribution may result in more overlapped chunks than a 1 dimensional distribution because the former has more processor boundary area than the latter Sparse chunks store BESS+value pairs in minichunks Sparse to sparse aggregations involve accessing these minichunks The BESS values are kept sorted in the minichunks to facilitate the aggregation calculations by using sort merge and scan operations used in relational processing A sparse chunk can be aggregated to a dense chunk by converting the BESS dimension encodings to a chunk offset value The detailed algorithms are described in 7 6 Data Mining Data mining can be viewed as an automated application of algorithms to detect patterns and extract knowledge from BESS     +    1 target source BB       05 BESS 0 9 5 13 10 0 5 13 18 23 27 B B AA 05 27 9181823 14 10 13 BB B 10 13 AB source target B P0 P1 P2 P1 P0 Figure 6 Modi\002cation of BESS indices while mapping a non-split chunk data 4 W e b r ie\003y d e scrib e an in te g r atio n o f a sso ciatio n rule mining and classi\002cation with the parallel OLAP and multidimensional analysis infrastructure presented in this paper 6.1 Asso ciation Rule Mining An association rule is an expression A  B where A and B are sets of items contained in a set of transactions This means that a transaction in the database that contains the items in A tend to contain the items in B  with a certain probability These are captured in the metric support and con\002dence  Association rule mining has applications in cross-marketing attached mailing add-on sales store layout and customer segmentation based on buying patterns to name a few Discovery of quantitative rules is associated with quantitative information from the database The data cube represents quantitative summary information for a subset of the attributes Attribute-oriented approaches 1  1 0   1 1  t o data mining are data-driven and can use this summary information to discover association rules Support of a pattern A in a set S is the ratio of the number of transactions containing A and the total number of transactions in S  Con\002dence of a rule A  B is the probability that pattern B occurs in S when pattern A occurs in S and can be de\002ned as the ratio of the support of AB and support of A  The rule is then described as A  B support and a strong association rule has a support greater than a pre-determined minimum support and a con\002dence greater than a predetermined minimum con\002dence This can also be taken as the measure of 223interestingness\224 of the rule Calculation of support and con\002dence for the rule A  B involve the aggregates from the cube AB  A  B and ALL Support is calculated as Prob A  B  N umber of tr ansactions  A  B  T otal tr ansactions  which involve values from cube A B and ALL  Con\002dence is Prob B j A  N umber of tr ansactions  A  B  N umber tr ansactions  A  which involves cubes AB and A  Since the data cube has these summaries for all possible combinations of A and B com 


putation of support and con\002dence can be done for all possible pairs Similarly association between more attributes can be generated by looking at cubes with more attributes Additionally dimension hierarchies can be utilized to provide multiple level data mining by progressive generalization roll-up and deepening drill-down This is useful for data mining at multiple concept levels and interesting information can potentially be obtained at different levels An approach to data mining called Attribute Focusing targets the end-user by using algorithms that lead the user through the analysis of data Attribute Focusing has been successfully applied in discovering interesting patterns in the NBA 1 a nd ot her a ppl i cat i ons  S i n ce dat a cubes h a v e a ggre g ation values on combinations of attributes already calculated the computations of attribute focusing are greatly facilitated by data cubes We present a parallel algorithm to calculate the interestingness function used in attribute focusing on the data cube 6.2 Decision-tree based Classi\014cation Classi\002cation is used for predictive data mining Applications that look at the known answers in the past and leverage from it in the future can take advantage of this technique A set of sample records called the training data set is given consisting of several attributes Attributes can either be continuous  if they come from an ordered domain or categorical  if they are from an unordered domain One of the attributes is the classifying attribute that indicates the class to which the record belongs The objective of classi\002cation is to build a model of the classifying attribute based upon the other attributes of the record We use the decision tree model because they are relatively inexpensive to construct easy to interpret and easy to integrate with data base systems Recent work has focused on using the entire data set in classi\002ers like SLIQ 14 a n d SPRINT  1 6  A p arallel c lassi\002er in the same spirit has been developed in ScalParC 13  The approach of SPRINT SLIQ is to sort the continuous attribute once in the beginning and maintain the sorted order in the subsequent splitting steps Separate lists are kept for each attribute which maintains a record identi\002er for each sorted value In the splitting phase the same records need to be assigned to a node which may be in a different order in the different attribute lists A hash table is used to provide a mapping between record identi\002ers and the node to which it belongs after the split This mapping is then probed to split the attribute lists in a consistent manner Table 2 is an example training set with three attributes Age Car color and Gender and a class attribute Figure 7\(a shows the classi\002cation tree for it At each node the attribute to split is chosen that best divides the training set Several splitting criteria have been used in the past to evaluate the goodness of a split Calculating the Table 2 Training Set Row-id Age Car-Color Gender Class-id 0 10 Green F 0 1 50 Blue M 1 2 40 Yellow F 0 3 30 Green F 0 4 20 Red M 1 5 40 Blue M 0 6 20 Yellow M 1 gini index is commonly used 2  T hi s i n v ol v e s c omput ing the frequency of records of each class in each of the partitions If a parent node having n records and c possible classes is split into p partitions the gini index of the i th partition is g ini i 1 000 P c j 1  n ij n i  2  n i is the total number of records in partition i ofwhich n ij records belong to class j The gini index of the total split is given by g ini split  P p i 1  n i n  g ini i  The attribute with the least value of g ini split is chosen to split the records at that node The matrix n ij is called the count matrix  The count matrix needs to be calculated for each evaluated split point for a continuous attribute Categorical attributes have only one count matrix associated with them hence computation of the gini index is straightforward For the continuous attributes an appropriate splitting value has to be determined by calculating the g ini split and choosing the one with the minimum value If the attribute is sorted then a linear search can be made for the optimal split point by evaluating the gini index at each attribute value The count matrix is calculated at each possible split point to evaluate the g ini split value Splitting the split-attribute is straightforward by adjusting pointer values The challenge is to split the nonsplit attributes ef\002ciently Existing implementations such as SPRINT and ScalParC maintain a mapping of the rowid and class-id with the values assigned to each node The values are split physically among nodes such that the continuous attribute maintain their sorted order in each node to facilitate the sequential scan for the next split determination phase A hash list maintains the mapping of record ids to nodes The record ids in the lists for non-splitting attributes are searched to get the node information and perform the correct split     b             F M Gender Class 0 Class 0 Class 1 Class 1 0110 0111 00 01 0 010 011 Age < 25 Age < 45 30 40 10 20 50 B R G Y M F 0 1 0 1 01 0 1 23 a Figure 7 a Classi\002cation tree for training set b Classi\002cation tree embedded on the base cube 


6.3 Classi\014cation on the cub e structure We propose that classi\002cation trees can be built using structure imposed on data using the multidimensional data model Gini index calculation relies on the count matrix which can be ef\002ciently calculated using the dimensional model Each populated cell represents a record in the array For the base cube which is a multidimensional representation of the records without any aggregation the class value of the record is stored in each cell The gini index calculation uses the count matrix which has information about the number of records in each partition belonging to each possible class      10 20 30 40 50 60 R B G Y W O 1 0 0 1 1 0 1 1 2 1 0 10 0 0 20 2 0 base cube 10 1 0 10 1 0 2 0 1 0 0 1 0 1 10 20 30 40 50 60 O W Y G B R 10 B A A B B A cid O1 30 B 0 50 O 1 1 50 W 60 R 0 0 R 10 20 Y 0 10 G 1 Training Set Multidimensional model cid cid 0 a b Aggregates Figure 8 a Training set records b corresponding multidimensional model To evaluate split points for a continuous attribute the g ini split needs to be evaluated for each possible split point in a continuous attribute and once for a categorical attribute This means the aggregate calculations present in each of the 1 dimensional aggregates can be used if they have number of records belonging to each class Therefore for each aggregate we store the number of records in each class Figure 8\(a gives an example training set with two dimensions A  a continuous dimension and B a categorical dimension and two class values 0 and 1  Figure 8\(b is the corresponding multidimensional model The continuous dimensions A is stored in the sorted order The aggregates store the number of records mapping to that cell for both classes 0 and 1 To calculate the g ini split for the continuous attribute attribute A it is now easy to look at the A aggregate and sum the values belonging to both classes 0 and 1 on both sides of the split point under consideration to get the count matrix Gini index calculation is done on an attribute list which in the case of a multidimensional model is a dimension Count matrix is repeatedly calculated on the sorted attribute list which is readily available in the cube structure as a higher level one dimensional aggregate Each dimension is sorted in the dimensional structure as shown in Figure 8\(b Further details are present in 7 7 Performance Results In this section we present performance results for our system on a 16-node IBM SP-2 distributed memory parallel computer available to us at Northwestern University Assume N tuples and p processors Initially each processor reads N p tuples from a shared disk assuming that the number of unique values is known for each attribute These are partitioned using a sample based partitioning algorithm  Partitioning phase  so that the attribute dimension values are ordered on processors and distributed almost equally To load the base cube tuples are sorted  Sorting phase n the combined key of all the attributes so that the access to chunks is conformant to its layout in memory/disk Sorting in the order A 0  A 1  A 2 A n 000 1  is conformant to the layout of chunks where A 0 is the outer most dimension and A n 000 1 is the inner most for loading a sorted run of values Table 3 Description of datasets and attributes N Numeric S String Data dim  d i   Q i d i  Tuples I 3 1024\(S\,256\(N 512\(N 2 27 10 million II 5 1024\(S\,16\(N\,32\(N\,16\(N\,256\(S 2 31 1 and 10 million III 10 1024\(S\,16\(S\,4\(S\16,4,4,16,4,4,32\(N 2 37 5 and 10 million IV 20 16\(S\,16\(S\,8\(S\2,2,2,2,4,4 2 51 1 million 4,4,4,8,2,8,8,8,2,4,1024 N The base cube is loaded on each processor from these tuples locally on each processor The sub-cubes of the data cube are calculated from here  Building phase  This is followed by the analysis and data mining phase on the computed aggregates We choose four data sets one each of dimensionality 3 5 10 and 20 to illustrate performance Random data with a uniform distribution is currently used for the performance 002gures We have evaluated other types of data e.g skewed with Zip\002an distribution including real OLAP data sets including the OLAP benchmark 3 which w e u sed i n our ear lier study 5 for a better c haracterization o f p erformance under different workloads The number of sub cubes in the datacube for Dataset I is 2 3 8  dataset II is 2 5 32 and dataset III is 2 10  1024  We report the results of complete data cube construction for these sets For Dataset IV we report the results of partial data cube construction in which 1350 sub-cubes are calculated 81632 Number of Processors 0.0 20.0 40.0 60.0 80.0 100.0 120.0 140.0 160.0 180.0 200.0 Time \(in seconds 5 dimensions, 10 million tuples Partitioning Sorting Loading Build 2D Build 1D 81632 Number of Processors 0.0 5.0 10.0 15.0 20.0 25.0 30.0 35.0 40.0 45.0 50.0 55.0 60.0 65.0 Time \(in seconds 10 dimensions, 10 million tuples Partitioning Sorting Loading 81632 Number of Processors 0.0 400.0 800.0 1200.0 1600.0 2000.0 2400.0 2800.0 Time \(in seconds 10 dimensions, 10 million tuples Build \(2D partitioning Build \(1D partitioning Figure 9 Time taken by various phases of the data cube construction algorithm for 5 32 sub cubes and 10 1024 sub cubes dimensions Figure 9 shows the time taken by the various phases of 


the data cube construction algorithm Each phase of the cube construction process shows good speedup for all the data sets when a single dimension is partitioned in the base cube A two dimensional partitioning performs better than a one dimensional partitioning because there are more chunks in the partitioned dimension that allows for better sparsesparse aggregation performance Figure 10 shows the time for full and partial cube build for dataset III and for partial cube building for dataset IV Partial build times are much smaller than the full cube building time  Full Partial 0.0 100.0 200.0 300.0 400.0 500.0 600.0 700.0 800.0 900.0 1000.0 1100.0 1200.0 1300.0 1400.0 1500.0 Time \(in seconds 16 processors 10 dimensions, 10 million tuples 2D distribution 1D distribution 816 Number of Processors 0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 Time \(in seconds Partial Cube 20D , N=1 million Partition Sort Load Build \(2D 956.0 610.7 a b Figure 10 Full and partial cubes a 10D 10 million tuples on 16 processors b partial cubes for 20D on 8  16 processors Other results which are used to evaluate our parallel infrastructure include time to execute OLAP queries notably range queries and queries over dimension hierarchies Association rule mining and classi\002cation tree building on the multidimensional infrastructure are also evaluated with appropriate data sets We do not include them here due to lack of space 8 Conclusions In this paper we have presented the design and implementation of a scalable parallel system for multidimensional analysis OLAP and data mining Using the multidimensional data model data is stored in chunks Sparse chunks are represented by a bit encoding which can be used for ef\002cient aggregation operations on compressed data For maximum ef\002ciency of operations dense regions can also be stored as multidimensional arrays if the cardinalities of the dimensions involved are not large and the cube size is below a speci\002c threshold Operations between chunked and multi-dimensional array cubes are supported The data structures to track the different cubes in a data cube the chunk structures of each cube and the chunks themselves using minichunks use paging to support a large number of cubes a large number of chunks per cube and a large chunk size This framework has been demonstrated for use in OLAP queries which are ad-hoc in nature and require fast computation times by pre-aggregating calculations Data mining uses some of the precomputed aggregated calculations to compute the probabilities needed for calculating support and con\002dence measures for association rules and the split point evaluation in building classi\002cation trees Parallelism has been used to support a large number of dimensions and large data sets for effective data analysis and decision making References  I  B handari A ttrib ute F ocus ing Data m i ning for t he laym an T echnical Report RC 20136 IBM T.J Watson Research Center 1995  L  B reim an J  H Friedm an R  A Ols h en a nd C J S tone Classi\002cation and Regression Trees  Wadsworth Belmont 1984  O L A P C ouncil OL AP Council Benchm ark In http://www.olapcouncil.com  1997  U  M  F ayyad G Piates k y Shapiro P  S m y th a nd R Uthurus am y  Advances in data mining and knowledge discovery  MIT Press 1994  S  G oil a nd A Choudhary  H igh P erform ance OL AP and D ata M ining on Parallel Computers Journal of Data Mining and Knowledge Discovery  1\(4 1997  S  G oil a nd A Choudhary  S pars e d ata s torage s c hem e s f or m u ltidimensional data for OLAP and data mining Technical Report CPDC-9801-005 Northwestern University December 1997  S  G oil a nd A Choudhary  D es ign a nd Im plem entation o f a Scalable System for Multidimensional Analysis and OLAP Technical Report CPDC-9810-020 Northwestern University October 1998  S  G oil a nd A Choudhary  H igh p erform ance m u ltidim ens i onal a nalysis and data mining In Proc SC98 High Performance Networking and Computing Conference  November 1998  J  G ray  A Bos w orth A  L aym a n and H  P irahes h Data Cube A Relational Aggregation Operator Generalizing Group-By CrossTab and Sub-Totals In Proc 12th International Conference on Data Engineering  1996  J  H a n Y  Cai and N  C ercone D ata-dri v en dis c o v e ry of quantitati v e rules in relational databases IEEE Trans on Knowledge and Data Engineering  5\(1 February 1993  J  H a n a nd Y  F u  D is co v e ry of m u ltiple-le v el as s o ciation r ules from large databases In Proc of the 21 st VLDB Conference Zurich  1995  V  Harinarayan A Rajaram a n and J  D  U llm an I m p lem e nting d ata cubes ef\002ciently In Proc SIGMOD International Conference on Management of Data  1996  M J o s h i G  K a r ypis  and V  K um ar  S calpar c  A ne w s calable and ef\002cient parallel classi\002cation algorithm for mining large datasets In Proc International Parallel Processing Symposium  March 1998  M Mehta R A g r a w a l and J  R is s a nen S L I Q  A F a s t S calable Clas si\002er for Data Mining In Proc of the Fifth Int'l Conference on Extending Database Technology Avignon  March 1996  S  S a r a w a gi and M  S tonebr ak er  E f 002 cient o r g anization o f l ar ge multi-dimensional arrays In Proc of the Eleventh International Conference on Data Engineering  February 1994  J  C S h af er  R  A gr a w al a nd M Mehta S P R I N T  A S calable P a r a llel Classi\002er for Data Mining In Proc 22th Int'l Conference on Very Large Databases Mumbai India  September 1996  A Shukla P  M Des hpande J  N aughton and K  R am as w a m y  S tor age estimation for multidimensional aggregates in the resence of hierarchies In Proc of the 22nd International VLDB Conference May 1996  K e nan S oftw are An introduction t o m ulti-dim e ns ional d atabas e technology In http://www.kenan.com/acumate/mddb.htm  1997 


on discovering large itemsets and discovering large se quences respectively We present two algorithms DLG and DSG which need only one database scan for efficient large itemset generation and efficient sequential pattern generation respectively These two algorithms construct an as sociation graph to indicate the associations between items and then traverse the graph to generate large itemsets and large sequences respectively We compare DLG and DSG algorithms to the pre viously known algorithms DHP 16 and AprioriAll 5  respectively The experimental results show that DLG and DSG outperform DHP and AprioriAll re spectively When the minimum support decreases the performance gap increases because the number of candidate itemsets candidate sequences generated by DHP AprioriAll increases and the number of database scans also increases We demonstrate that the execution time of these two algorithms increases linearly as the database size increases and the execution time decreases slightly as the number of items itemsets increases For our graph-based approach the related informa tion may not fit in the main memory when the size of the database is very large In the future we shall de velop a mining algorithm based on our graph-based approach such that in a very large database environ ment the mining algorithm can also be run in the main memory We shall consider mining various dif ferent relationships among data in a large database of customer transactions such as is-a relationships and part-of relationships We shall also apply our graph based approach on different applications such as doc ument retrieval and resource discovery References R Agrawal and et al Database Mining A Per formance Perspective In IEEE Transactions on Knowledge and Data Enganeering pages 914-925 1993 R Agrawal and et al Mining Association Rules Between Sets of items in Large Databases In Proceedzngs of ACM SIGMOD pages 207-216 1993 R Agrawal and et al An Interval Classifier for Database Mining Applications In Proceed ings of International Conference on Very Large Data Bases pages 560-573 Vancouver British Columbia 1992 R Agrawal and R Srikant Fast Algorithm for Mining Association Rules In Proceedzngs of International Conference on Very Large Data Bases pages 487-499 1994 R Agrawal and R Srikant Mining Sequential Patterns In Proceedings of International Confer ence on Data Engineering pages 3-14 1995 Y Cai N Cercone and J Han An Attribute Oriented Approach for Learning Classification Rules from Relational Databases In Proceedzngs of International Conference on Data Engineering Los Angeles pages 281-288 Feb 1990 7 W Chu and et al Using Type inference and Induced Rules to Provide Intensional Answers. In Proceedings of International Conference on Data Engineering pages 396-403 1991 8 M Hammer and S.B Zdondik Knowledge-based query processing In Proceedings of International Conference on Very Large Data Bases pages 137-146 1980 9 J Han and et al Knowledge Discovery in Databases An Attribute-Oriented Approach In Proceedangs of International Conference on Very Large Data Bases pages 547-559 1992 lo J Han and et al Data-Driven Discovery of Quan titative Rules in Relational Databases in IEEE Transactions on Knowledge and Data Engineer ing pages 29-40 1993 ll M Houtsma and A Swami Set-Oriented Mining for Association Rules in Relational Databases In Proceedings of International Conference on Data Engineering pages 25-33 1995 la C Malley and S Zdonik A Knowledge-Based Approach to Query Optimization In Proceedings of the First Expert Database System Conference pages 243-257 1986 13 H Mannila H Toivonen and A.I Verkamo Efficient Algorithm for Discovering Association Rules In Proceedings of AAAI Workshop on Knowledge Discovery zn Databases pages 181 192, 1994 14 A Motro Using Integrity Contraints to Provide Intensional Answers to Relational Queries In Proceedzngs of International Conference on Very Large Data Bases 1989 15 G Oosthuizen Lattice-Based Knowledge Dis covery In Proceedings of AAAI Workshop on Knowledge Discovery in Databases pages 221 235 1991 16 J.S Park M.S Chen and P.S Yu An Effec tive Hash-Based Algorithm for Mining Associa tion Rules In Proceedings of ACM SIGMOD 24 2  175 186, 1995 17 M.S.E Sciore and et al A Method for Automatic Rule Derivation to Support Semantic Query Op timization In ACM Transactions on Database Systems pages 563-600 1992 18 M Siegel Automatic Rule Derivation for Se mantic Query Optimization In Proceedings of the Second International Conference on Expert Database Systems pages 371-385 1988 17 


19 U.Chaliravarthy D Fishman and J Minker Semantic Query Optimization in Expert Sys tems and Database Systems In Proceedzngs of the Fzrst Internatzonal Conference on Expert Database Systems pages 326-340 1984 20 S.J Yen and A.L.P Chen Neighbor hood/Conceptuai Query Answering with Impre cise/Incomplete Data In Proceedzngs of Inter natzonal Conference on Entzty-Relatronshap Ap proach pages 151-162 1993 21 S.J Yen and A.L.P Chen The Analysis of Re lationships in Databases for Rule Derivation In Journal of Intellzgent Informatzon Systems Vol 7 pages 1-24 1996 22 S.J Yen and A.L.P. Chen An Efficient Algorithm 200or Deriving Compact Rules from Databases In Proceedzngs of Internatzonal Conference on Database Systems for Advanced Applzcataons pages 364-371 1995 23 C Yu and W Sun. Automatic Knowledge Acqui sition and Maintenance for Semantic Query Op timization In IEEE Transactzons on Knowledge and Data Enganeerzng pages 362-375 1989 24 W Ziarko The Discovery Analysis and Rep resentation of Data Dependencies in Databases In Proceedzngs of AAAI Workshop on Knowledge Dzscovery zn Databases pages 195-209 1991 18 


addition for some data types search can be further optimized For example if the indexed categorical data have 223xed-dimensionality 000 we know that the area of each indexed signature is 223xed to 000  We can use this property to derive stricter lower bounds for the directory node entries 001  instead of the rather relaxed 002 004 006 006 t 013 r 001 020 022 004 025 027 For this example a better bound is 002 004 006 006 t 013 r 001 020 022 004 025 027 033 t 000 037    t 001 020 022 004 025 r 013 027 027  We plan to study such search optimizations using domain properties or statistics from the indexed data References  C  C  A ggarw al  J  L  W ol f and P  S  Y u A N e w Method for Similarity Indexing of Market Basket Data SIGMOD Conference  pages 407\205418 1999  R  A gra w al and R  S ri kant  F as t A l gori t h ms for M i n ing Association Rules in Large Databases VLDB Conference  pages 487\205499 1994  K  S  B e y er  J  G ol ds t e i n  R  R amakri s hnan and U Shaft When Is 215Nearest Neighbor\216 Meaningful International Conference on Database Theory  pages 217\205235 1999  T  B ri nkhof f H.-P  K ri e g el  a nd B  S e e g er  E f 223 ci ent Processing of Spatial Joins Using R-Trees SIGMOD Conference  pages 237\205246 1993  A  C orral  Y  Manol opoul os  Y  T heodori d i s  a nd M Vassilakopoulos Closest Pair Queries in Spatial Databases SIGMOD Conference  pages 189\205200 2000  A  P  d e V ries N  M amoulis N  N es a nd M K e r sten Ef\223cient k-NN Search on Vertically Decomposed Data SIGMOD Conference  pages 322\205333 2002  U  D eppisch S-T r ee A D ynamic B alanced Signature Index for Of\223ce Retrieval ACM SIGIR Conference  pages 77\20587 1986  V  G aede a nd O G 250 unther Multidimensional Access Methods ACM Computing Surveys  30\(2\170\205231 1998  V  G ant i  J  Gehrk e  a nd R  R a makri s hnan C A C T US 205 clustering categorical data using summaries ACM SIGKDD Conference on Knowledge Discovery and Data mining  pages 73\20583 1999  D Gi bs on J  M Kl ei nber g  a nd P  R a gha v a n C l us tering Categorical Data An Approach Based on Dynamical Systems VLDB Conference  pages 311\205322 1998  A Gi oni s  D Gunopul os  a nd N K oudas  Ef 223 c i e nt and Tunable Similar Set Retrieval SIGMOD Conference  2001  S  Guha R  R as t ogi  a nd K S h i m  R OC K A R obust Clustering Algorithm for Categorical Attributes International Conference on Data Engineering  pages 512\205521 1999  A Gut t m an R T rees  A Dynami c I nde x S t r uct u re for Spatial Searching SIGMOD Conference  pages 47\205 57 1984  S  Hel m er and G  M oerk ot t e  A S t udy of F our Inde x Structures for Set-Valued Attributes of Low Cardinality Technical Report University of Mannheim  number 2/99 1999  G R Hjaltason a nd H Samet Distance Bro w sing in Spatial Databases TODS  24\(2\265\205318 1999  A K J a i n and R  C  D ubes  Algorithms for Clustering Data  Prentice-Hall 1988  I Kamel a nd C  F a louts o s  Hilbert R tree An Improved R-tree using Fractals VLDB Conference  pages 500\205509 1994  F  K o rn N  S i d i r opoul os  C  F al out s o s  E S i e g el  a nd Z Protopapas Fast Nearest Neighbor Search in Medical Image Databases VLDB Conference  pages 215\205 226 1996  N K oudas a nd K C  S e vci k  H i g h D i m ens i onal S i m i larity Joins Algorithms and Performance Evaluation International Conference on Data Engineering  pages 466\205475 1998  N R ous s opoul os  S  K el l e y  and F  V i n cent  Neares t Neighbor Queries SIGMOD Conference  pages 71\205 79 1995  Y  S a kurai  M  Y os hi ka w a  S  U emura and H  K oj i m a The A-tree An Index Structure for High-Dimensional Spaces Using Relative Approximation VLDB Conference  pages 516\205526 2000  The U C I KDD Archi v e ht t p    kdd.i c s  uci  edu 23 R W e b e r  H.-J S ch ek  a n d S Blo tt A Q u a n titative Analysis and e Study for SimilaritySearch Methods in High-Dimensional Spaces VLDB Conference  pages 194\205205 1998  86  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


