Mining Weighted Browsing Patterns with Linguistic Minimum Supports TzunpPei Hong Ming-Jer Chiang and Shyue-Liang Wang Deparment of Electrical Engineering National University of Kaohsiung institute of Infoonnation Engineering IShou University Kaohsiung, Taiwan R.O.C tphong@nuk.edu tw ndmc893009@yahoo.com.tw slwang@isu. edu tw Absrrrrer-All the wtb pages are usually assumed to have the same importance in web mining. Different web pages in a web site may however have different importance to usera in red applications Besides the mining parameten in most conventional data-mining algorithmr are numerical This paper tbun attempts to propose a weighted webmining technique to discover linguistic browsing patterns from log data in web sewers Web pages are first evaluated 
by managers as linguistic terms to reflect their importance which are then transformed as fuzzy rets of weights Linguistic minimum supports are assigned by users, showing a more natural way of human reasoning Fuvy operations including fuzzy ranking are then used to find linguistic weighted large sequences An example is also given to clearly illustrate the proposed approach Keywords Browsing pattern fuzzy set web mining weght I INTRODUCTION Recently world-wide-web applications have grown very rapidly and have made a significant impact on computer systems. Among them web browsing for useful information may be most commonly seen Due to its tremendous amounts of use efficient and effective web retrieval has thus become a very important research topic in this field Techniques ofweb mining have thus been requested and developed to achieve this purpose Cooley 
et al divided web mining into two classes webcontent mining and webusage mining 8 Web-content mining focuses on information discovery from sources across the world-wide-web On the other hand webusage mining emphasizes on the automatic discovery of user access patterns fiom web servers 9 In the past smral webmining approaches for finding sequential patterns and interesting user information from the world-wide-web were proposed 5-81 The fuzzy set theory has been used more and more frequently in intelligent systems because of its simplicity and similarity to human reasoning. The theory has been applied in fields such as manufacturing engineering diagnosis and economics among others Several filay learning algorithms 
for inducing rules from given sets of data have been designed and used to good effect with specific domains 2,4 10 151 In the past all the web pages were usually assumed to have the same importance in web minimg Different web pages in a web site may however, have different importance to users in real applications. For example a web page with merchandise items on it may be more important than that with general introduction Also a web page with expensive merchandise items may be more important than that with cheap ones Different importance for web pages should then be considered in web mining. Besides the minimum support and minimum confidence values in most conventional data-mining algorithms were set numerical In 14 
we proposed a weighted mining algorithm for association rules using linguistic minimum support and nirmtn confidence In this paper we further extend it to mining weighted sequential browsing patterns from log data on web servers The linguistic minimum support values are given, which are more natural and understandable for human beings The browsing sequences of users on web pages are used to analyze the overall retrieval behaviors of a web site Web pages may have different importance, which is evaluated by managers or experts as linguistic terms The proposed approach transforms linguistic importance of web pages and minimum supports into fuzzy sets then filters out weighted large browsing patterns with linguistic supports using fuzzy 
operations 11 REVIEW OF RELATED MININGAPPROACHES Agrawal and Srikant proposed a mining algorithm to discover sequential ptterns from a set of transactions I Five phases are included in their approach In the first phase the transactions are sorted first by customer ID as the major key and then by transaction time as the minor key This phase thus convexts the original transactions into customer sequences In the second phase the set of all large itemsets are found fiom the customer sequences by comparing their counts with a predefmed support parameter a This phase is similar to the process of mining association rules Note that 
when an itemset occus more than one time in a customer sequence it is counted once for this customer sequence In the third phase each large itemset is mapped to a contiguous integer and the original customer sequences are transformed into the mapped integer sequences In the hurth phase the set of transformed integer sequences are used to find large sequences among them In the fifth phase the maximaly large sequences are then derived and output to users Besides Cai er al proposed weighted mining to reflect different importance to different items 31 Each item was attached a numerical weight given by 
users Weighted supports and weighted confidences were then defined to determine interesting association rules Yue er al then extended their concepts to filzzy item vectors  181 111 REVIEW OF RELATED FUZZY SET CONCEPTS Fuzzy set theory was first proposed by Zadeh and Goguen in 1965 9][20 Fuzy set theory is primarily concerned with quantifying and reasoning using natural language n which words can have ambiguous meanings This can be thought of as an extension of traditional crisp sets in which each element must either be in or not in a set 0 2002 IEEE SMC TPlF7 


Triangular membership functions are commonly used and can be denoted by A b c where C  The abscissa b represents the variable value with the maximal grade of membcrship value i.e j a and c are the lower and upper bounds of the available area They are used to reflect the fuzziness of the data 5 b Besides hzy ranking is usually used to determine the order of fuzzy sets and is thus quite important to actual applications Several filzzy ranking methods have been proposed in the literatwe The ranking method using gravities is introduced here Note that other ranking methods can also be used in our filjm algorithm The abscissa of the gravity of a triangular membership function U b c is u+b+c Let A and B be two triangular fuzzy sets A and B can then be represented as follows  A  UA bA CA B  UB bB CBI Let g\(A  A  bA  CA and g\(B  aB bB 4 cg Using the gravity ranking method, we say A  E ifg\(A  m IV NOTATION The notation used in this paper is defined as follows n the total number of log records n 222 the total number of browsing sequences m the total number of web pages A the i-th web page 1 Si S m k the total number of managers w  the transformed fuzzy weight for importance of Page Ai  w,aw  the filav average weight for importance of Page A  w  the fuzzy weight for importance of sequential sequence connt the count of Page A  a the predefined linguistic minimum support value 4 thej-th membership function of importance ZaW the filzzy average weight of all possible linguistic wsup the fuzzy weighted support of Page Ah minsup the tmnsformed fuzzy set from the linguistic minimum support value a wminsup the fuzzy weighted set of minimum supports C the set of candidate sequences with r web pages 5 the set of large sequences with r web pages 222\222 evaluated by thej-th manager s terms of importance v THE PROPOSED ALGORITHM FOR LINGUISTIC WEIGHTED WEB MINING Log data in a web site are used to analyze the browsing pattems on that site Many fields exist in a log schema hong them the fields date time client-ip andfife nume are used in the mining process Only the log data with asp htm ha jva and cgi are considered web pages and used to analyze the mining behavior The other files such as jpg and gif m thought of as inclusion in the pages and are omitted The number of files to be analyzed is thus reduced The log data to be analyzed are sorted first in the order of client-ip and then in the order of date and time The web pages browsed by a client can thus be easily found The importance of web pages are considered and represented as linguistic terms Linguistic minimum support value is assigned in the mining process The proposed webmining algorithm then uses the set of membership functions for importance to transform managers\222 linguistic evaluations of the importance of web pages into fuzzy weights The fuzzy weights of web pages from different mangers are then averaged The algorithm then calculates the weighted counts of web pages from browsing sequences according to the average fuzzy weights of web pages The given linguistic minimum support value is also transformed into a fizzy weighted set All weighted large I-sequences can thus be found by ranking the fuzzy weighted support of each web page with filzzy weighted minimum support After that candidate 3sequences are formed from weighted large 1-sequences and the same procedure is used to find all weighted large 2-sequences This procedure is repeated until all weighted large sequences have been found Details of the proposed mining algorithm are described below The weighted webmining algorithm INPUT A set of n web-log records, a set of m web pages with their importance evaluated by k managers two sets of membership functions respectively for importance and minimum support, and a pre-defmed linguistic minimum support value a OUTPUT A set of weighted linguistic browsing patterns STEP 1: Select the records with file names including asp htm html Jva cgi and closing connection from the log data keep only the fields date time client-ip and file-nume STEP 2 Transform the client-ips into contiguous integers called encoded client ID for convenience according to their first browsing time Note that the same client-ip with two closing connections is given two integers STEP 3 Sort the resulting log data first ty encoded client ID and then by date and time STEP 4 Form a browsing sequence for each client ID by sequentially listing the web pages STEP 5 Transform each linguistic term of importance for web page A I Si I m which is evaluated by the j-th manager into a fuzzy set 4 of weights using the given membership funkions of the importance of web pages STEP 6 Calculate the filzzy average weight wicM of each web page Ai by fky addition as Ik k pj wy   K,j STEP 7 Count the occurrences counti of each web page A appearing in the set of browsing sequences f Ai appears more than one time in a browsing sequence its count addition is still one set the support support of A as counij n 222 where n 222 is the mber of browsing sequences from STEP 4 STEP 8 Calculate the filzzy weighted support wsup of each web page Ai as 


counq x n wsup  A B W xcount n wsup  MANAGER1 MANAGER2 MANAoER3 1-1 ordinsrv ordinuy Vuylmportr lmponani Imponnnt STEP 9 Transform the given linguistic minimum support value o into a fuzzy set minsup using the given membership functions of minimum supports STEP 10 Calculate the fuzzy weighted set wminsup f the given minimum support value as wminsup  minsup X the gravity of I where C OrdiWY D Unimporuat m with I being the j-th membership function of importance Im represents the fuzzv average weight of all possible linguistic tenns of importance STEP 11 Check whether the weighted support supi of each web page Ai is larger than or equal to the filzzy weighted minimum support wminsup by fuzzy ranking Any fuzzy ranking approach can be applied here as long as it can generate a crisp rank If wsup is equal to or greater than wminsup put A in the set of large 1-sequences LI STEP 12 Set r  1 where r is used to represent the number of web pages kept in the current large sequences STEP 13 Generate the candidate set Cr from L in a way similar to that in the aprioriafl algorithm l Restated the algorithm first joins L and L under the condition that r-1 web pages in the two sequences are the same and with the same orders Different permutations represent different candidates. The algorithm then keeps in C the sequences which have all their subsequences of length r existing in  STEP 14 Do the following substeps for each newly formed el s with web browsing pattern a Calculate the fizzy weight W of sequence s st  SI 3   m c as Imponant imponant Unimportant Very Unimponant w  w A w,Y A...Aw where W is the filzzy average weight of web page s  calculated in STEP 6 If the minimm operation is used for the intersection then r+l i=I w  Min wsy b Count the occurrences count of sequence s appearing in the set of browsing sequences f s appears more than one time in a browsing sequence its count addition is still one set the support rupporrr of s as counh n whm n  is the number of browsing sequences c Calculate the weighted support wsup of sequence s as d Check whether the weigh:c.d support wsup of sequence s is greater thz c:r equal to the fuzzy weighted minimum support wminsup by fuzzy ranking If wsup is greater than or equal to wminsup put s in the set of large r+I  STEP 15 If is null then do the next step otherwise set r r  1 and repeat Steps 13-15 STEP 16 For each large r-sequence s r>l with weighted support wsup find the linguistic minimum support region S with wminsup S wsup wminsupi+l by fuzzy ranking where wminsup  rninsupi X the gravity of I minsup is the given membership function for S Output sequence s with linguistic support value S The linguistic sequential browsing patterns output after Step 16 can serve as meta-knowledge concerning the given log data VI AN EXAMPLE In this section iin example is given to illusnate the proposed webmining algorithm. This is a simple example to show how the proposed algorithm can be used to generate weighted linguistic browsing sequential patterns for clients browsing behavior according to the log data on a web server Assume the browsing sequences from a log data after Step 4 is shown in Table 1 I I Also assume the membership functions for importance of the web pages are given in Figure 1 


unimportant Important veryunimportant very Impomt  0 0 0.25 0.5 0.75 1 Weight Figure 1  The membership functions of hportance The linguistic terms for the importance of the web pages given in Table 2 are transformed into fuzzy sets by the membership functions in Figure 1 The average weight of each wb page is calculated by fuzzy addition The average fkzy weights of all the web pages are shown in Table 3 Table 3 The average fuzzy weights of all the web pages WEB PAGE AVERAGE FUZZY WEIGHT 0.333 0.583 0.833 0.583, 0.833 1 0.417 0.667 0.917 0 0.167 0.417 E 0.5 0.75 1 The appearing number of each web page is counted from the browsing sequences in Table 1 If a web page occurs more than one time in a sequence it is counted once for this sequence The weighted support of each web page is calculated with results shown in Table 4 Table 4 The weighted supports of all the web pages WEB PAGE WEIGHTED SUPPORT 0.056 0.097 0.139 0.486 0.694 0.833 0.278 0.444 0.61 1 0.25,0.375,0 The given linguistic minimum support value is transformed into a fuzzy set of minimum supports Assume the membership functions for minimum supports are given in Figure 2 Verypw Low Middle High VeryHigh The gravity of P\223 is calculated as 0.5 The fuzzy weighted set of minimum supports for 223Middle\224 is then 025,0.5,0.75 x 0.5 which is 0.125 025 0.375 The weighted support of each web page is then compared with the filzzy weighted minimum support by fuzzy ranking hy fuzzy ranking approach can be applied here as long as it can generate a crisp rank Assume the gravity ranking approach is adopted in this example B C and E are large weighted I-sequences C is then first generated from L as follows B B B C B E C 4 C 0 C E E 4 E C and E E Among them B C B E and E B are found to be large weighted 2-sequences in Step 14 There are no large 3-sequences in the example The linguistic support value is found for each large r-sequence s r  1 Take the sequential browsing pttern E B as an example Its weighted support is 0.167 0.25 0.333 Since the membership function for linguistic minimum support region 223Middle\223 is 0.25 0.5 0.75 and for 223High\223 is 0.5 0.75 1 The weighted fuzzy set for these two regions are 0.125 0.25 0.375 and 0.25 0.375 0.5 Since 0.125 0.25 0.375 I\(0.167 0.25 0.333  0.25 0.375 0.5 by fizzy ranking the linguistic support value for sequence E B is then 221Middle\224. The linguistic supports of the other two large 2-sequences can be similarly derived VII CONCLUSION In this paper we have proposed a new weighted webmining algorithm which can process webserver logs to discover useful sequential browsing patterns with linguistic supports The web pages are evaluated by managers as linguistic terms which are then transformed and averaged as fuzzy sets of weights Fuzy operations including filzzy ranking are used to find weighted sequential browsing patterns Compared to previous mining approaches the proposed one has linguistic inputs and outputs which are more natural and understandable for human beings Although the proposed method works well in weighted web mining from log data and can effectively manage linguistic minimum supports, it is just a beginning There is still much work to be done in this field Our method assumes that the membership functions are known in advance In 112.13 151 we proposed some fizzy learning methods to automatically derive the membership functions In the future we will attempt to dynamically adjust the membership hctions in the proposed webmining algorithm to avoid the bottleneck of membership function acquisition 0 0.25 0.5 0.75 1 Mini Figure 2 The membership functiom of minimum supports Also assume the given linguistic minimum support value is 224Middle\224 It is then traxuformcd into a fuzzy set of minimum supports 0.25 0.5 0.75 according to the given membership functions in Fip 2 suppoa REFERENCES l R Agrawal R Srikant 223Mining Sequential Patterns\224 The Eleventh International Conaerence on Data Engineering 1995 pp 3 14 2 A F Blishun 223Fuzy leaming models in expert systems,\224 Fuzzy Sets and Systems Vol 22 1987 pp 57-70 3 C H Cai W C Fu C H Cheng and W W Kwong 223Mining association rules with weighted items,\222\222 The International Database Engineering and Applications Symposium 1998 pp 68-77 4 L M de Campos and S Moral 223Learning rules for a filzzy inference model,\224 Fuzzy Sea and Systems Vol 59 1993 pp 247-257 


SI M S Chen J S Park and P S Yu 223Efficient Data Mining for Path Taversal Patterns\224 IEEE Transactions on Knowledge and Data Engineerins Vol 10 1998 pp 209-221 a Liren Chen and Katia Sycara 223WebMate A Personal Agent for Browsing and searching,\224 The Second International Conference on Autonomous Agents ACM 1998 7 Edith Cohen Balachander Krishnamurthy and Jennifer Rexford 224 Efficient Algorithms for Predicting Requests to Web Servers,\224 The Eighteenth IEEE Annual Joint Conference on Computer and Communications Societies Vol 1,1999 pp 284 293 8 R Cooley B Mobasher and J Srivastava, \223Grouping Web Page References into Transactions for Mining World Wide Web Browsing Patterns,\224 Knowledge and Data Engineering Exchange Worhhop 1997 pp 2 9 191 R ley B Mobasher and J Srivastava 223Web Mining Information and Pattern Discovery on the World Wide Web,\224 Ninth IEEE International Conference on Tools with ArtiJciaI Intelligence 1997 pp 558 567 IO M Delgado and A Gonzalez 223An inductive learning procedure to identify fuav systems,\224 Fuzy Sets and Systems Vol 55 1993 pp 121-132 ll A.Gonzalez 223A learning methodology in uncertain and imprecise environments,\224 International Journal of Intelligent Systems Vol 10 1995 pp 57-371 I21 T P Hong and J B Chen 223Finding xelevant attributes and membership functions,\224 Fuzzy Sets and Systems Vo1.103 No 3 1999, pp 38W I31 T P Hong and J B Chen 223Processing individual filzzy attributes for fuzzy rule induction,\224 Furzy Sets and Systems Vol 112,No 1,2OoO,pp.127-140  141 T P Hong M J Chiang and S L Wang 224Mining from quantitative data with inguistic minimum supports and confidences\224 We 2002 IEEE International Conference on Funy System Honolulu Hawaii 2002 pp.494-499 IS T P Hong and C Y Lee 223Induction of fuzzy rules and membership functions from training examples,\224 Fuzzy Sets andSystems Vol 84 1996 pp 33+7 I61 T P Hong and S S Tseng, \223A generalued version space learning algorithm for noisy and uncertain data,\224 IEEE Transactions on Knowledge and Data Engineering Vol 9 NO 2 1997, pp 336-340 I71 J Rives 223FID3 filzzy induction decision tree,\224 The First International symposium on Uncertain Modeling and Analysis 1990 pp 457462 I81 S Yue E Tsang D Yeung and D Shi 223Mining fuzzy association rules with weighted items,\224 The IEEE International Conference on Systems Man and Cybernetics 2000 pp 1906-191 1 I91 L A Zadeh 223Fuzzy logic,\224 IEEE Computer 1988 pp 20 L A Zadeh 223Fuzzy sets,\224 Information and Control Vol 83-93 8 NO 3 1965 pp 338-353 


The second reason wh y 014ne gran ularit y taxonomies are preferred is that the n um b er of candidates generated increases rapidly with the increase in the a v erage fanout Fine gran ularit y taxonomies alleviate this problem 2.2 Algorithm W e assume that the transactions are in the form h TID i j i k i n i and the complete taxonom yis a v ailable Generating negativ e asso ciation rules inv olv es 014nding all the negativ e large itemsets and generating the negativ e rules W e 014rst consider the problem of 014nding negativ e large itemsets As explained in section 2.1 generating negativ e large itemsets inv olv es 014nding generalized large itemsets generating negativ e candidates coun ting supp ort for the candidates and generating the negativ e large itemsets T o 014nd all large itemsets w e can use one the algorithms Basic  Cumulate or EstMer ge  prop osed in 14  T o generate the negativ e large itemsets w e describ e t w o algorithms b elo w Both the algorithms use similar approac hes The 014rst algorithm is a straigh t forw ard implemen tation and the second algorithm incorp orates some optimizations to impro v e the p erformance Both the algorithms require m ultiple iterations 2.2.1 Naiv e Algorithm Eac h iteration of this algorithm consists of t w o phases In the 014rst phase of iteration k w e compute the generalized large itemsets of size k using one of Basic  Cumulate or EstMer ge  In the second phase 014rst the negativ e candidate itemsets of size k are generated as describ e in section 2.1.1 Next supp ort for the candidates is coun ted b y making a pass o v er the data Therefore this algorithm requires t w o passes o v er the data during eac h iteration for a total of 2 002 n passes where n is the total n um b er of iterations The impro v ed algorithm reduces the n um b er of passes o v er the data as describ ed b elo w 2.2.2 An Impro v ed Algorithm This algorithm incorp orates t w o optimizations o v er the naiv e algorithm 014rst all small 1-itemsets are deleted from the taxonom y second the instead of generating the negativ e itemsets during eac h iteration they are generated in a single step after generating the large itemsets of all sizes The 014rst optimization reduces the n um b er of negativ e candidates generated The second optimization reduces the n um b er of passes o v er the data from 2 002 n to n  1 The algorithm is sho wn in Figure 3 2.3 Generating Rules Once the negativ e itemsets are generated 014nding all negativ e rules is straigh t forw ard F or a negativ e itemset n w e output the rule a 6    n 000 a  where a  and  n 000 a  are nonempt y subsets of n ha ving minim um supp ort if its rule in terest measure is greater than sp eci\014ed minim um  MinRI Our algorithm for generating negativ e rules is an extension of the apgenrules algorithm describ ed in 2  The extensions handle the requiremen t that the an teceden t and the L 1  f large 1{itemsets g  k 2  k represen ts the pass n um ber  First generate all large itemsets while  L k 000 1 6    b egin  Generate new candidates of size k using Basic  Cum ulate or EstMerge C k  GenCands  L k 000 1  forall transactions t 2D b egin C t  subset C k  t  forall candidates c 2 C t c coun t end L k  f c 2 C k j c coun t 025 MinSup g k  k 1 end  No w generate negativ e itemsets Delete all small 1-itemsets from the taxonom y k 2 while  L k 6    b egin  Generate negativ e candidates of size k as  describ ed in section 2.1.1 NC k  GenNegCands  L k  NC  NC  NC k  k  k 1 end forall transactions t 2D b egin NC t  subset NC k  t  forall candidates c 2 NC t c coun t end N k  f c 2 NC k j c coun t  MinSup 002 MinR I g Figure 3 An impro v ed algorithm consequen ts of the rule m ust b e large Note that if a turns out to b e small none of the extensions need to b e generated b ecause they will not ha v e the minim um supp ort Similarl y if a rule a 6    n 000 a  do es not ha v e minim um RI then none of the subsets of a need to b e considered b ecause none of those rules will ha v e minim um RI either The rule generation algorithm is sho wn in Figure 4 The apriori-gen function is the join follo w ed b y the prune step describ ed in 2  2.4 Data Structures The algorithm to generate negativ e asso ciation rules is v ery similar to generating generalized association rules with additional steps to generate negativ e itemsets Since ev ery 1-itemset of candidate m ust ha v e minim um supp ort instead of testing ev ery itemset for minim um supp ort while generating candidates it is considerably faster to compress the taxonom yb y 014ltering out all items whic h do not ha v e minim um supp ort Then no candidate in whic h one of the 1-item 


forall negativ e itemsets n k of size k  k 025 2 do H 1  f consequen ts of rules generated from n k with one item in the consequen t g  call genrules  n k  H 1  L 2  L k 000 2  end pro cedure genrules  n k  H m  L m 1  L k 000 m 000 1   n k  negativ e k itemset H m  set of m item consequen ts if  k>m 1 then b egin H m 1  apriori-gen  H m  forall h m 1 2 H m 1 if  h m 1 2 L m 1 then if  n k 000 h m 1  2 L k 000 m 000 1  then RI  E  sup  n k  000 sup  n k  sup  n k 000 h m 1  if  RI 025 MinRI  then output rule  n k 000 h m 1  6   h m 1  else delete h m 1 from H m 1  else delete h m 1 from H m 1  end call genrules  n k  H m 1  end Figure 4 Rule Generation subsets do es not ha v e minim um supp ort will b e generated As men tioned earlier same candidate ma ybe generated in more than one w a y  Therefore all candidates are put in a hash table for fast lo okup Whenev er a candidate is generated 014rst the hash table is c hec k ed If the candidate is not found then the candidate is inserted in the hash table Otherwise the exp ected supp ort for the candidate is set to larger of the t w o During the rule generation pro cess the coun ts of the subsets of the negativ e itemset are required All large itemsets are also placed in a hash table for fast lo okup 2.5 Memory Managemen t Since negativ e candidates of all sizes are generated at the same time and their supp ort is tested in a single pass it is p ossible that the n um b er of candidates generated is to o large to accommo date in a v ailable memory Ho w ev er if suc h a situation arises the negativ e candidate generation pro cess is stopp ed and the supp ort for the candidates already generated is coun ted The generated negativ e itemsets are either written bac k to the disk or if they are su\016cien tly small are k ept in the main memory  The candidate generation pro cess is no w con tin ued This will necessitate more than one pass o v er the database 3 Exp erime n tal Results In this section w e describ e the exp erimen tal results of our tec hnique for generating negativ e asso ciations W e p erformed the exp erimen ts using syn thetic data on Sun SP AR Cstation 5 with 32 MB of main memory  3.1 Syn thetic Data The syn thetic data is generated suc h that it sim ulates customer buying pattern in a retail mark et environmen t W eha v e used the same basic metho d as describ ed in 14  Ho w ev er to sim ulate the buying pattern more accurately w e adapted the hierarc hical mo del of consumer c hoice called the nested logit mo del to generate the data In this mo del consumers 014rst decide on whic h category to buy and then decide whic h particular brand to buy within that category  W e 014rst generate a taxonom yo v er the items F or an yin ternal no de the n um berofc hildren are pic k ed from a P oisson distribution with mean set to F This pro cess is generated starting from the ro ot lev el Then at lev el 2 and so on un til there are no more items W e next generate a set of p oten tially maxim al large itemsets from whic h itemsets are assigned to a transaction T o generate the set of p oten tially maxim al large itemsets w e 014rst generate p oten tially maxim al clusters of categories comprising of items one lev el ab o v e the leaf lev el The clusters sizes are pic k ed from a P oisson distribution with mean equal to a v erage cluster size Next for eac h cluster w e generate a set of p oten tially maxim al itemsets from the c hildren of the items in the cluster The n um b er of suc h itemsets is pic k ed from a P oisson distribution with mean set to a v erage n um b er of itemsets The size of eac h itemset is also pic k ed from a P oisson distribution with mean set to a v erage large itemset size Eac h cluster has an asso ciated w eigh t that determines the probabilit y that this cluster will b e pic k ed The w eigh tispic k ed according to an exp onen tial distribution with mean set to 1 The w eigh ts are normalized suc h that the sum of all w eigh ts equals 1 The itemsets asso ciated with eac h cluster are also giv en w eigh ts whic h determine the probabilit y this itemset will b e pic k ed once that particular cluster is pic k ed The w eigh ts are pic k ed from an exp onen tial distribution with mean set to 1 The w eigh ts are then normalized suc h that the sum of all w eigh ts of all itemsets for a cluster equal 1 The length of a transaction is determined b yP oisson distribution with mean 026 equal to j T j Un til the transaction size is less than the generated length a cluster is pic k ed according to its w eigh t Once the cluster is determined an itemset from that cluster is pic k ed and assigned to the transaction Not all items from the itemset pic k ed are assigned to the transaction Items from the itemset are dropp ed as long as an uniformly generated random n um ber bet w een 0 and 1 is less than a corruption lev el c  The corruption lev el for itemset is determined b y a normal distribution with mean 0.5 and v ariance 0.1 The transactions con tain only leaf items from the taxonom y  The parameters used in the generation of the synthetic data are sho wn in T able 3 3.2 P erformance W e generated t w o sets of data Short and T all based on di\013eren ta v erage fanouts in the taxonom y on the items Both datasets con tain the same n um ber of items leaf items in the taxonom y and the same n um b er of transactions The parameter v alues used to generate the data are sho wn in T able 4 


j D j Num b er of transactions j T j Av erage size of transactions j C j Av erage size of maxim al p oten tially large clusters j I j Av erage size of maxim al p oten tially large itemsets j S j Av erage n um b er of itemsets for eac h cluster j L j Num b er of maxim al p oten tially large clusters N Num b er of items R Num ber of roots F F anout T able 3 P arameters P arameter Short T all j D j 50,000 50,000 j T j 10 10 j C j 5 5 j I j 5 5 j S j 3 3 j L j 2,000 2,000 N 8,000 8,000 R 10 10 F 9 3 T able 4 Data parameters W e ran b oth algorithms on the t w o data sets for v arious v alues of minim um supp ort The minim um RI w as set to 0.5 in all cases The results are sho wn in Figures 5 and 6 The times sho wn include b oth generation of negativ e itemsets and negativ e rules Since our goal w as to study the p erformance of generating negativ e asso ciations w eha v e not included the time tak en to generate the generalized large itemsets The T all dataset whic h has a taxonom y with smaller fanout to ok longer to complete than the Short dataset The reason w as the far larger n um b er of generalized large itemsets that w ere generated for the T all dataset F or example at a supp ort lev el of 1.5  15,476 large itemsets w ere generated for the T all dataset as opp osed to 1,499 for Short If normalized for the n umb er of generalized large itemsets the times for the T all dataset are m uc h smaller than those of Short as p er our exp ectations Next w e compared the e\013ect of the di\013eren t fanouts in the taxonom y  The n um b er of negativ e candidates generated and the n um b er of negativ e rules for eac h data set are sho wn in the Figure 7 T ok eep the results comparable w eha v e normalized these n um b ers with resp ect to the n um b er of large itemsets As can b e seen the exp erimen t con\014rmed that the n um ber of candidates increases with the increase in fanout 4 Conclusion W ein tro duced the problem of mining negativ e asso ciation rules in a large database of retail customer   0 50 100 150 200 250 300 350 400 0.01 0.015 0.025 0.04 0.06 0.1 0.2 Time \(sec Minimum Support Naive  Better  Figure 5 Execution times Short data set   0 200 400 600 800 1000 1200 0.01 0.015 0.025 0.04 0.06 0.1 0.2 Time \(sec Minimum Support Naive  Better  Figure 6 Execution times T all data set transactions Mining negativ e information is nontrivial and in the case of retail transactions data the problem b ecomes imp ossible to solv e Our approac his to use the grouping information suc h as a taxonom y o v er the items and the existing p ositiv e asso ciations in the data to induce negativ e rules b et w een items close to the items in the p ositiv e asso ciations This approac h solv es the problem pruning the com binatorial searc h space to a small subset of cases whic hha v e a high p oten tial of b eing in teresting W e presen tan algorithm for mining negativ e rules and impro v emen ts and study their p erformance on syn thetic data 4.1 F uture W ork Man y problems remain unsolv ed in the problem of mining negativ e rules Tw o of the biggest problems are as follo ws 017 W e assume only the taxonom yo v er the items as the domain kno wledge a v ailable to mine for negativ e rules Ho w ev er there ma y b e man y other 


 10 100 1000 10000 100000 1e+06 2 3 4 5 Number of candidates \(Normalized Size of the itemset Fanout = 9  Fanout = 3  Figure 7 Num b er of negativ e candidates t yp es of information a v ailable F or instance a kno wledge of substitute items Ho w to incorp orate other t yp es of information to impro v e the qualit y of rules needs to b e explored further 017 The n um b er of candidates generated is exp onential o v er the length of the large itemsets b eing considered More e\016cien t candidate generation tec hniques need to b e dev elop ed Ac kno wledgmen t The 014rst author wishes to thank Dr Rak esh Agra w al at IBM Almaden Researc h Cen ter for suggesting the problem and for man y insigh tful discussions References  R Agra w al T Imielinski  and A Sw ami Mining asso ciation rules b et w een sets of items in large databases In Pr o c e e dings of the 1993 A CM SIGMOD International Confer enc e on Management of Data  pages 207{216 W ashington DC Ma y 26-28 1993  R Agra w al and R Srik an t F ast algorithms for mining asso ciation rules in large databases In Pr o c e e dings of the 20th Internationa l Confer enc eonV ery L ar ge Data Bases  San tiago Chile August 29-Septem b er 1 1994  T J Blisc hok Ev ery transaction tells a story Creating customer kno wledge through mark et-bask et analysis Chain Stor eA ge Exe cutive  V71:50  57 Marc h 1995  J Han and Y F u Disco v ery of m ultiple-lev el asso ciation rules from large databases In Pr o c e e dings of the VLDB Confer enc e  pages 420  431 Septem b er 1995  M Houtsma and A Sw ami Set-orien ted mining of asso ciation rules In Pr o c e e dings of the International Confer enc e on Data Engine ering T aip ei T aiw an Marc h 1995  R Krishnam urth y and T Imielinski  Practitioner problems in need of database researc h A CM SIGMOD R e c or d  20\(3 Septem b er 1991  H Mannila H T oiv onen and A I V erk amo E\016cien t algorithms for disco v ering asso ciation rules In KDD-94 AAAI Workshop on Know le dge Disc overy in Datab ases  pages 181  192 Seattle W ashington July 1994  J S P ark M-S Chen and P S Y u An e\013ectiv e hash based algorithm for mining asso ciation rules In Pr oc e e dings of the A CM-SIGMOD Confer enc e on Management of Data  pages 229  248 San Jose California Ma y 1995  G Piatetsky-Shapiro Disc overy A nalysis and Pr esentation of Str ong R ules  pages 229  248 AAAI Press/The MIT Press Menlo P ark California 1991  G Piatetsky-Shapiro and W J F ra wley  editors Know le dge Disc overy in Datab ases  MIT Press 1991  A Sa v asere E Omiecinski and S Na v athe An e\016cien t algorithm for mining asso ciation rules In Pr oc e e dings of the VLDB Confer enc e  pages 432  444 Zuric h Switzerland Septem b er 1995  A Silb ersc hatz M Stonebrak er and J Ullman Database systems ac hiev emen ts and opp ortunities Communic ations of the A CM  34\(10 Octob er 1991  P Sm yth and R M Go o dman R ule Induction Using Information The ory  pages 159  177 AAAI Press/The MIT Press Menlo P ark California 1991  R Srik an t and R Agra w al Mining generalized association rules In Pr o c e e dings of the VLDB Confer enc e  pages 407  419 Septem b er 1995  M Stonebrak er R Agra w al U Da y al E Nuehold and A Reuter Database researc h at a crossroads The vienna up date In Pr o c e e dings of the 19th International Confer enc eon V ery L ar ge Data Bases  pages 688{192 Dublin Ireland August 1993 


with the same parameters but di\013eren t in size ranging from 25K to 100K The v alues of w minsup are set as the ab o v e scale-up exp erimen t In this 014gure the time is giv en in ln  sec  F rom the 014gure the execution time increases with the n um b er of transactions linearly with ln scale implying that the complexit yof the algorithms is exp onen tial in the n um b er of transactions 1  5.2.3 Exp erimen t for sp ecial case In this section w e are in terested in the p erformance in the sp ecial case whic h is the item w eigh ts equal to 0 or 1 only  In this case w e mak e the 014rst 900 w eigh ts b e 0 and the remaining w eigh ts b e 1 Other things including database and threshold equal as ab o v e section W e carried out the exp erimen t for the normalized w eigh ted case to compare the t w o algorithms There are t w o ma jor 014ndings 1 The p erformance of the sp ecial case is m uc h b etter than the general case where item w eigh ts follo w a distribution b et w een 0 and 1 2 Con trary to the previous cases MINW AL\(W p erforms b etter than MINW AL\(O F rom Figure 8 w e notice that the time needed in MINW AL\(W is m uc h less than the MINW AL\(O for all the thresholds This is b ecause in the joining step the n um b er of starting seed candidate itemsets in C 1 to generate itemsets in C 2  is less than MINW AL\(O case In this situation the 0/1 w eigh ts giv e the adv an tage to MINW AL\(W During the 014rst step the algorithm MINW AL\(W will easily prune all the small itemsets with 0 w eigh ts while MINW AL\(O will k eep those small itemsets with 0 w eigh ts As the starting seed is smaller in size MINW AL\(W w ould p erform w ell in this case 6 Conclusion W eha v e prop osed to study a new problem of mining w eigh ted asso ciation rule This is a generalization of the asso ciation rule mining problem In this generalization the items are assigned w eigh ts to re\015ect their imp ortance to the user The main di\013erence b et w een mining w eigh ted asso ciation rules and the mining un w eigh ted asso ciation rules is the do wn w ard closure prop ert y  W e prop osed t w o di\013eren t de\014nition of w eigh ted supp ort without normalization and with normalization W e prop osed new algorithms based on the supp ort b ounds  the algorithms MINW AL\(O and MINW AL\(W MINW AL\(O is applicable to b oth normalized and unnormalized cases and MINW AL\(W is applicable to the normalized case only  The p erformance ev aluation has b een done on these t w o algorithms W e found that MINW AL\(O outp erforms MINW AL\(W in most cases but MINW AL\(W p erforms b etter for the sp ecial case with only 0/1 item w eigh ts So far w eha v e only considered the mining of binary w eigh ted asso ciation rules Some of the researc hers did the researc h for the problem of the quan titativ e assoication rules suc has[4  3 W ema yin v estigate the problem of quan titativ e asso ciation rules with w eigh ted items whic his anin teresting topic in the future References  R Agra w al and R Srik an t F ast algorithms for mining asso ciation rules In Pr o c e e dings of the 20th VLDB Confer enc e  pages 487{499 1994  D Cheung V.T Ng A F u and Y F u Ef\014cien t mining of asso ciation rules in distributed databases In IEEE T r ansactions on Know le dge and Data Engine ering  pages 1{23 1996  T ak eshi F ukuda Y asuhik o Morimoto Shinic hi Morishita and T ak eshi T okuy ama Data mining using t w o-dimensional optimized asso ciation rules Sc heme algorithms an visualization In Pr o c e e dings of A CM SIGMOD  pages 13{23 1996  T ak eshi F ukuda Y asuhik o Morimoto Shinic hi Morishita and T ak eshi T okuy ama Mining optimized asso ciation rules for n umeric attributes T ec hnical Rep ort 1623-14 IBM T oky o Researc h Lab oratory  1996  J Han M Kam b er and J Chiang Mining m ultidimensional asso ciation rules using data cub es T ec hnical rep ort Database Systems Researc h Laboratory Sc ho ol of Science Simon F raser Univ ersit y  1997  J.S P ark M-S Chen and P S Y u An e\013ectiv e hash-based algorithm for mining asso ciation rules In Pr o c e e dings of A CM SIGMOD  pages 175{186 1995  A Sa v asere E Omiecinski and S Na v athe An e\016cien t algorithm for mining asso ciation rules in large databases In Pr o c e e dings of the 21th International Confer enc eon V ery L ar ge Data Bases  pages 432{444 1995 


expect this optimization to be of greatest bene\336t when the transaction sizes are large r example if our transaction is T 000 f A\000 B 000 C\000 D\000 E g  k 000 3 fan-out 000 2 then all the 3-subsets of T are f ABC,ABD,ABE,ACD,ACE,ADE,BCD,BCE,BDE,CDE g  Figure 2 shows the candidate hash tree C 3  We ave to increment the support of every subset of T contained in C 3  We egin with the subset AB C  and hash to node 11 and process all the itemsets In this downward path from the root we mark nodes 1 4 and 11 as visited We then process subset AD B  and mark node 10 Now consider the subset CDE  We see in this case that node 1 has already been marked and we can preempt the processing at this very stage This approach can r consume a lot of memory r a n fan-out F  for iteration k  e need additional memory of size F k to store the 337ags In the parallel implementation we have to keep a VISITED 336eld for each processor bringing the memory requirement to P\000F k  This can still get very large especially with increasing number of processors In we sho w a mechanism by which further reduces the memory requirement to only k 000F  The approach in the parallel setting yields a total requirement of k 000F 000P  5 Experimental Evaluation Database T I D Total Size T5.I2.D100K 5 2 100,000 2.6MB T10.I4.D100K 10 4 100,000 4.3MB T15.I4.D100K 15 4 100,000 6.2MB T20.I6.D100K 20 6 100,000 7.9MB T10.I6.D400K 10 6 400,000 17.1MB T10.I6.D800K 10 6 800,000 34.6MB T10.I6.D1600K 10 6 1,600,000 69.8MB Table 2 Database properties 5.1 Experimental Setup All the experiments were performed on a 12-node SGI Power Challenge shared-memory multiprocessor Each node is a MIPS processor running at 100MHz There\325s a total of 256MB of main memory The primary cache size is 16 KB 64 bytes cache line size with different instruction and data caches while the secondary cache is 1 B 128 bytes cache line size The databases are stored on an attached 2GB disk All processors run IRIX 5.3 and data is obtained from the disk via an NFS 336le server We used different synthetic databases with size ranging form 3MB to 70MB 2  and are generated using the procedure described in These databases mimic the transactions in a retailing en vironment Each transaction has a unique ID followed by a list of items bought in that transaction The 2 While results in this section are only shown for memory resident databases the concepts and optimization are equally applicable for non memory resident databases In non memory resident programs I/O becomes an important problem Solutions to the I/O problem can be applied in combination with the schemes presented in this paper These solutions are part of future research 11 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


  0 500 1000 1500 2000 2500 0 2 4 6 8 10 12 Number of Large Itemsets Iterations Large Itemset at Support = 0.5 222T5.I2.D100K\222  222T10.I4.D100K\222   222T15.I4.D100K\222   222T20.I6.D100K\222   222T10.I6.D400K\222   222T10.I6.D800K\222   222T10.I6.D1600K\222  Figure 3 Large Itemsets per Iteration data-mining provides information about the set of items generally bought together Table 2 shows the databases used and their properties The number of transactions is denoted as jD j  average transaction size as j T j  and the average maximal potentially large itemset size as j I j  The number of maximal potentially large itemsets j L j 000 2000 and the number of items N 000 1000 We refer the reader to for more detail on the database generation All the e xperiments were performed with a minimum support value of 0.5 and a leaf threshold of 2 i.e max of 2 itemsets per leaf We note that the  improvements shown in all the experiments except where indicated do not take into account initial database reading time since we speci\336cally wanted to measure the effects of the optimizations on the computation Figure 3 shows the number of iterations and the number of large itemsets found for different databases In the following sections all the results are reported for the CCPD parallelization We do not present any results for the PCCD approach since it performs very poorly and results in a speed-down on more than one processor 3  5.2 Aggregate Parallel Performance Table 3 s actual running times for the unoptimized sequential and a naive parallelization of the base algorithm Apriori for 2,4 and 8 processors without any f the techniques descibed in sections 3 and 4 In this section all the graphs showing  improvements are with respect to the data for one processor in table 3 Figure 4 presents the speedups obtained on different databases and different processors for the CCPD parallelization The results presented on CCPD use all the optimization discussed 3 Recall that in the PCCD approach every processor has to read the entire database during each iteration The resulting I/O costs on our system were too prohibitive for this method to be  12 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


