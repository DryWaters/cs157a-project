Unifying Decision Tree Induction and Association Based Classification Hongyan Liu Je&y Xu Yu Hongjun Lu3 Jim Chen Depment of Management Science and Engineering Tsinghua University, Haidian District, Beijing 100084 China hyliuchcnj Department of System Engineering and Engineering Management Chinese University of Hong Kong Hong Kon China yu@Se.CUhk.edU.hk Department of Computer Science Hong Kong VnivexsiQ of Science  Technology Hong Kong chi luhj@cs.ust.hk Keyword ehsailicotion dedaion tree association rules RDBMS L nVraODUCllON Classificadon is a process of finding the comnan pmpcnies 
of data objects that belong to the s~m class It has a wide range of applications in d world and has been extensively studied by rescarchm in various fields such as machine ICaming and statistics 17 In the recent surge of data mining mearch various classification techniques have bem re examined including decision tree induction l3 Bayesian cIassifim 4 neural networks 191 and etc Among these methods decision tree is still one of the most popular methods because of 
its fastertraining speed reasonably good accuracy and good scalability IO 14 161 More recently a new classification method based on ass&ations has betn developxi Z 3 7 8 11 15 IS The rationale of association-based classificatioo is rather intuitive Given a wining dataset with n amibutes AI A  A and a class label C an association rule with.the class label as its consequent alir a  a  cb supl con reprssents a classitkation rule iI AI  a 
and A2  olj and _ and A om then class  c supb con It is obvious that association based classifcation approaches aim at discovering all such association rules with the class label as Mnsaluent From this viewpins association-based classifcarion is adopting an exhaustivc search shategy in finding classification rules This is different fium decision wee classification when a decision tree is consttucted we select an attribute to split at each intemal node based on the valw of 
certain goodness functions such as information gain OT gini ioda In other words decision tree is collsrmcfed in a greedy manna While bdstics based grady scroch can find near Opthnal solutions exhaustive search should always find optimal solutions This could be one of the reasons that association bad classification often leads to bigher accuracy wmpared to the decision tree method By discovering all classification rules association-based classification can impm the classification accuracy to certain extent and it also 
brings other issues One of these issues is how to efficiently maintain and search from a large anmunt of rules Most of existing association based classification mahods seem not addressing such issues very well and use unskudwed flat tile to managc thcse rules Urn by flat-style we mean there is no data skucture Iikc trees to mpporl efficient searching A recent pmposal of association based decision me ADP adapts a Ire srmelurc that organizes classification rules discovered based on their parmt-child relationship IS A 
rule a,+c is identified as a parent of ol,bl+cI the purpose of using ADT is to make it easier to pmc those redundant des by a pessimistic error estimation mnhod Although ADT plays an impoWnt rule in rule pruning the ADT tree dos not impose the actual smcture on des It still maintains every rule in di&rcnt tree nodes even though rules at a node have common attributes in their heads with their parent des at their parent node therefore the efficient manipulation the large numh of rules remains an issue The above-mrntioned observations motivated the 
wxk reported in this paper a generalized deciiion lrec Om that unifies the classical decision tree induction with the association-based classificatioa Gm as the name implies has the similar tree shllcfWT as decision Uas I*I internal nodes dcterminc the splits on attribute values and the leave nodes indicate the class to which a sample should classified Given an unseen sample its class label can be detmnined by following the tree branches based on matching attribute values at the intemal nodes at each level of the tree 


Howw unlike decision trees rvhne only one amibute is selected to split at each intcmal node an intemal node of GDT contains eneies for all attributes whose split will lead to meaningful classifration rules In other words just like association-based classification where all classification rules arc discovered, a GDT encodes all classification rules in a tree form A path fmm the mot of a GDT to a leave node is equivalent to an association rule among the attniutes in the passing intemal nods and the class label at the leave node The benefits of using GDT can be summand  asfollowS  A GDT W-b the result of sort of an exhaustive search of classification rules It usually contains more wful classification rules found and dcscnbsd in a classical decision tree which is expected to have highcr accuracyin comparison with decision trce method A common critique to any arhaustivc search method is its computational complexity Evaluating attributes at each node to find good split is the main cost for decision tree conwuctl on To consrmct a GDT wc can w the algorithm developed in mining sssaiation rules in large dah efficiently cym with the support of relational database managemnt systems SI On the other hand techniques developed in decision tree induction such as treepruning can bc applied to GDT which solves one ofthe difficulties in association-bascd classification debmining thc intemtingncss ofrules A GDT maintains classification rules as tree paths It ov the main difficulties in association-bascd classification of managiog large numbcr of rules she cost of storage is redd as rules with cormmn amibntes in their antecedent am shared through comn sub-path in thc tree Insmion deletion and xarching a rule in a tree could bc implrmcnted efficientb fie remainder kthc papa is organized as OIIOWS won U brietly describes the concept of classical decision tree and some of its deficiencies Section III gives the definition of Genaalized Decision Tra and corresponding algorithms to insert and search the tree Our expcrimmgl dts arc presented in Section 1v And finauy we conclude our work in won v IL DECISION AND AssoclAllON RULES A dsision tree is a trec consisting of two rypss of nodes namely decision nodes non-leafnodes and class no leaf nodes and edges A decision node specifies a test to be carried out on a single attribute An edge from a decision node is associated with an nttribuk value Most existing algorithms conma decision trees in two phases consmting the trre and pmning the tree Givm a training dataset a decision tree is constructed level by INCI in a top down manner At each Iml a greedy searching method is used to select the best splitting attributes based on certain goodness function The attribute with the maximum goodness measure will be selected as the split attribute at a decision node One mst widely used goodness measure is information gain 12,13 Such a greedynature of the decision tree classification may cause some undesirable results Let us consider an exme example Table I contains a simplified dataset for an insurance company where amibute Risk is the class label Given such a da-t as the training dags a decision ba TI consrmcted is as shown in Figun I Hen information gain is used as the goodness measun when selscting the splitting attributes Note that forthe training dam shown in Table I because the information gain of the three auributes Age-pup Car fype and Income at the root am actually the same mat is any of them can be selected as the first split attribute The decision trcc shown in Figun I is the result of selecting Age group as the first split attribute If we chaose different attribute as the splitting attributes we will have different Table 1 Sample Trammg dafasn I Aer-group 1 Car-lype I Income I Risk I rr1.ADsision~~l 1 decision trecs Figure 2 and Figure 3 sbow'altematives using Gr-rype and Income as the first split attribute respectively This example well illustrated the grady nature of the decision tree induction Givm a dataset a decision tree may only reflect part of the pmpenies rclatcd to classification In other words some classificalion rules could be neglected in a consbuctcd decision tree because of the selected splitting attributes This can lead to some undesirable results Since SOM classification rules could missing fmm the decision trec constructed For example decision trecs TI R and T3 do not contain any classification information about Income Age and Car-type respcdively As such some unseen sample that can be classified using one tree may not be able to be classified if the tree used happens to be the one with necessaryclassification infomtion For example given samples bung sport low or a sample with missing value   low we am unable to classify themusingTI ButwithdecisiontreeT3,thcsetm samples can be classified as Risk high For an unseen sample the class label may not only depend on the attribute values but also depend on the consmucting pms of the decision tree used In other words given the same training dataset and an uwen sample the classification muh may be different depending on the decision tree used For example, for an unseen sample old sednn low it will be classified   I  


as Avg risk by decision tree TI Figurel as ow risk by TZ Figure 2 and as high risk by T3 Figure 3 Fl-2 Dccisionuul2 y$p$?-$ai avg low avg low Flgmre 3 Dsision trc I3 If we appty the a~soeiation-bascd classification techniques and set the minimum support bcing 0 for a dataset with small number of samples and the minimum mnfidace to l00sb wc can obtain the following scf of classification rules The two fignres in the parenthesis are the confidence and support for the rule R UAge-grwp=okfthenRisk=ovg lWh,33 R2 If Agegroup  middle and Car-rYpe  sport then Risk high 100 17 R3 UAge-group  middle and Income  high then Risk ow 100%4 17 RI UAge-group =young and Cor-rupe  wn  then Risk high 100 17 RI UAge-group young and Income  ovg  then Risk low 100 17 R6 U clu-rype  sport and Income  ovg  then Risk ovg 100 17 R,:U Car-fype=sedan thenRisk=low lOO%,33 R8 UCar-rype  WJI and Income  high then Risk  llvg RgUIncom=IowthenRisk=high l00%&,33 Comparing these set ofrules and the decision trees we can sec that each decision me conshucted using decision tree based algorithms only covers five among the nine classification rules TI covers RI R2, R3 R R R covers RA L Rb R 2 and T3 covcxs Rj RI Rb Ra Rd It is obvious that when we use this complete set of classification rules for classifying unm samples we will not encounter the problem wc mentioned above While association base classification methods can fmd more rules than a decision tree consmcted using an existing decision tree construction algorithm an astute reader may realize that there could be a Iarge numher of rules given a training dataset In this example a six tuple training data generates nine rules Such a large number of rules will obviously raise the issues of computational efficiency 100 17 Ill A GENauuza DECISION TRe We have seen that both decision tree classification and association based classification have their own merits and deficiencies In this section we propose to generalize the classical decision tree to ovmome such deficiencies At the same time to address the issues raised for association-based classification A GDT 7hhe definition An examplc of genmlizcd decision tree GDV for the sample daw in Table I is shown in Figure 4 Like decision trees at each level of a GDT the tree bdes out based on attribute values The leave nodes of the tree arc class labels Unlike decision mcs an intemal node contains entries for mo~c tban one attribute In Figure 4 the mot node contains the amibutcs wd the nodes in the nut level contain either one or two entries There arc branches for each amibute We d&e a genrralizsd decision tree as follows Definition 3.1 A Gad Decision Tree GDV is an ordnsd tree coasisdng of two types of nodes attribute node non-leafnode and class node leaf-node An amibute node contains a xt of attributes An edge fium an amiute node to its child is associated with a pair of attributmms attribute value A path fmm the mot to a leaf node rrpresenrs a classification rule whose consequent is the label of the leaf node The mot node of the tree must be an attribute node The tree shown in Figure 4 is a GDT foUowing the defdtion If we comparr Figure 4 with the set of rules dismvard by an association-based classification algorithm we can sa that the GDT in faa eneodes all classifications in a tree form and the nine rules correspmd to the nine paths in the tne re 4 A GDT far the sample damet in Tabb I Flprrs.AnomcrGDTfarmcsampkdatasaio Like decision trees a number of diffmnt gendized decision trees can be conshuctd Figurc 5 depicts another I The meaning of figurrs in the parenthesis will be explained later 


GDT construned hm the sample datasct in Table I Note that trees in Figure 4 and S look difkrat as the ordcfing of amibutes in which the two tree were constructed is different However the clsssification rules represented by the paths in the tree are the samc This is an important propmy of the gennalized decision ha B Constructing a GDT Since a GDT encodes all classification des a stnightforward appmach to consbuct a GDT is to starl from mining association rules with consequent being a class label We will not discuss the details here since mining of such rules have bem well srudied In panicular with the recent development in relational darabasc managmmt systems association-based classification can discover all classification rules more efficiently than building decision kea SI A GDT can be consmctcd by inserting the discovered association rules into the tree The major funeton we are going to discuss here is U check whder a rule is redundant acwrding to the delinition of widon 3.2 A des Rj    q q cong matches another rule R ail a  c s sup wnf4,i/forevrryg\(p=I.2  k    rruk Rj marchw Rondq  c we say that nJeR is rowredby or corn R FM example rule sedan  low matches but does not cover young s  high while de low high matches and covers rule young low  high Dqidtion3.3 Givsnnvonrler ail au   q supy confJ and R  ail a  4 i supr umi4.w say is redundant y 1 all the samples covered by  are covered by  2 all the samples matched by Rj are matched by Ri and either the cod  confj or cod  con but sup  supp or Forexample inTable I Givens low,young sedan IOW beMm redundant Lunma 3.1 Given r rules R  ail a _   cj supI cod4 and R  31  a  ci supb cod rule R coven R then rule 5 is redundant When a classification rule is to be inscrtcd into a GM trcc function IsRedwuimt listed below is called to check whether the rule is redudant and damninc the indon point Funelion IsRedunak~hf de T GDT 1 begin 2 3 4 5 rrtluoNuLL 6 7 end Travw T in depth first o Check if thm is a rule r\222 in T covers r or matches I with higher cod or sup value if such a rule,r\222 exists then else mtnm the end node position Function IsRdunahnt stam from the mot and traverses the tree in a depth first oh Redundant rules arc discarded For a non-redundant rule it rems the insrrtion point which is the end node of the last shad commm prefix with an existing rule in the ordered tree The insertion pmcedurc is simply to insert the Rmaining path for the de to be inserad into the GDT tree C Clm~fing an unseen sample using a GDT Funccion SmrchGDTfl GDT t sample 1 begin 2 3 max confidence and support value 4 cod travnse T in depth-fmt ordcr find a rule in T that matches t The de gives Function hhGDT outline3 above is used to classify a given sample t The tree is searched hm the root node in a depth-6rst orcler The maximized confidence value and support value variables are initialized to be m Class label is set to be the defauit class in case if thm is no rule found for tuple 1 When searching the tree wl~enevcr a class node is found its corresponding rule will be examined to check if it matches the tuple and has a higher confidence and suppofi value If so it beMmw the mt kt matching rule Note that for a GDT a given sample may match more than one de TbM is, the full tree needs to be searched to fmd the rule with highest maximum cnnfidence To reduce the effort of search fM each 8tUiibutc in the internal nodes the maximm mnfidencc of the paths passing through the node is stored The numbers after the name of amibutes in Figure 4 arc such mfidmce values Aftcr a matching rule is found there is no nccd to search those branches if the maximum confidence of the matched rule is higher than the maximum value stored in the node N A~OWCESTUDY We implemented GDT tree by modifying an association based classification algorih GAGRDB SI to evaluate its effectiveness In this section we report the muits of our experiments All experiments reported in this section were performed on an IBM Thinkpad notebook computer with 700HZ CPU nmning Minosoft Windows 2000 professional and IBM DBZ version 7.0 A Classi$carion occuracy As we discussed it is expected that GDT will givcp higher classifmtion aceura~y than the classical decision tree During our experiments we run C4.S and our classification algorithms using a sct of data from the UCI Repository 6 The continuous amibutes arc discretized using the MLC discretim based on entmpy discrctktion S 191 In most cases IO-fold mss-validation is used The results of our system are shorn in Table 2 Fmm Table 2 we can sec that among 17 dafascb GDT loses only S of them with average accuracy 0.033 higher than C4.S 


TabkZClanrificatonaccvrafyofGDTandC4.5 B Esecutio enq One motivation of GM is to address the issues of manipulating large number of classification rules in association-based classification The second oe*l of expCrimmts focus on the exextion efficiency of GDT To indicate the effcaivcncsa of organizing rules in GM wc usc the association-based classification appmach withouf Wllsrmchn  g GDT GAC-RDB as thc ref of comparison The data set wc wd is synthetic data set de in I Each rsord in the data sct wnsists of 9 atrribuDs and IO classification funetions defined on thex 9 atuihtes Here we condun our qmhcntal studies using hction IO shown blow which is the most complex hction among the ten classification funetions hysars  20  equity=O hay 2 20  equity=O.lx hvalue x fhycarrs-20 disposable  0.67 x  salary wmmission SO00 x elevel  02 xcquity-IOk ClasSA dispos8bleXl Since GDT woks with categorical attributes the non categorical amiiur*l discretizcd first by a simple qui width method for discrehtion The interval width and the number of intervals rcsulted are shown in Table 3 Tabk 3 DiSSriking the Srmbute Val In order to compare the time used to search rule set we pdcd several rule set with diffmnt number of rules 20000 40000 6oMx re 6 Ssarrh be No of da 0 60 120 180 240 300 Number of attributes    Flgmre 7 Ssarrb he  No of amibutcr ranging from 4.921 to 52,133 for datasets with noise level of 0.1 For dataset with 500,ooO training tuples and 10,000 M fupleq the total time used in thc iterations of tinding classification rules to test is shown in Figurc 6 From the figure wc can sec that the searching time saved by using GDT increased bticaUy as the number of rules incnascs The sewnd set of expnimnts is to sa the effects of the number of attributes on the scmhing time We also use the synthetic datarrt but we increase the number of atkibutcs horn 9 to 300 The noiss Iml of the 200.000 training tuples is still 0.1 with the me number of 10,000 test tuples Since those extra amibutcs other than the original 9 attributes are irrelevant to the classification hction IO the nunk of rules produced for those dabsets with di&rcnt number of athibutes rrmains the same and it is about 25,000 rules The searching tim for the testing procedure of all iterations is shown in Figore 7 This figure indicates that the numk of attributes does not afkt the searching time significantly for both data suuctwes This is because we do not need to wmpare those exha attributes as they do not appear in the rule at all At the sam tim the figurc also shows that the tree structure has a faster starching timS than tlat style suucture It is worth noting that in the process of tinding classification rules whenever a rule is found we need to search the existing rule set to how if the current rule is redundant This pmemVe in similar to the searching pro~cdurs for an usem case The diffmnce is that we only need to find the first rule covering it So wmparcd to searching all of matching rules this ph dy spcnds less timc Therefox to so ex GDT can also speed up the rules tinding ph of association based classification algorithms The third sct of experimena demonstnttes it The data.Ws is as the mnd set of ex Figure 8 shows the time spent on searching 


the existing tree for a t rule to see if it is redundant and store it to the tree if not redundant It is obvious that as the number of amibutes incnascs the time increases It is impomnt to know that the me approach spcnt less time than the unstrucNnd approach I 0 w 120 180 240 300 lp a Tic m torr IUIW bber of attributes V CONCLUSIONS In this p~pcr we proposed a gsncralizsd dccision trce GDT which is a natural exhion of decision tra GDT unifies the decision Iree classification and association-based Classification A GM tra sncodcs all classifmton can be disurvsnd using association-bascd classification appmaches to address the issuc that the classical dccision tree classification may miss some classifications On the omer hand GDT makm mipulation of large number of classification rules more efficient as shown by the rcsulfi of OUrpcrfOImanes study VI RE-EmNCEs IIR Agrawdl T Imielinski and A Swami Database mining A prformana paspcnive IEEE Tmsactions on Knowledge and Data Enginemin 5\(6 Daonber 1993 RI Bayardo Brute-Fa mining of high coniidmce classifration rules In heedings of 3rdInmnational Confwence on Knowledge Direowry and Dafa Mining 1997 G Dong X Zhang L Wong, and 1 Li CAEP Classification by aggregating emerging patterns InIernatio~I Confeence on Discowy science 1999 N Friedman D.Geiga and M Goldszmidt Bayesian network classifier Machinehrning 29,1997,131 163 U.M Fayyad and K.B hi Multi-Interval discretization of continuous-valued amibutes for classification learning In Proceedings of the 13th Inmationalfoinf Conference an AmJicial Infelligence 1993 1022-1027 CJ Mesq P Murphy UCI npositoty of machine learning databases 1996 Olnp:lhnvw.cs.uci.cdu/-mleamlMLReposito~.h B Liu W Hsq and Y Ma Intemting classification and association rule mining In Proceedings of the Fowfh Intemafional Conference on Knowledge DiscowyandDafa Mining New York USA 1998 80-86 21 3 4 SJ 61 171 181 H Lu H Liu, Decision tables Sulable Chiincation Exploring EDBM CrpebiliUK lo h Of 26th lo~umlalcQm-m0aoO vqLvgrDat?brPs cab Egypt 2000.373-384 H Lu R Setiono and H Liu NnrroRule A conneztionist approach to data mining In Proceedinas of the 21th International Conference on Very Large Darahes Zurich Switzerland Sqtrmbcr 11-15 1995,478489 IO M Mehla R Agrad and 1 Rissanen SLIQ A fast scalable classifier for data mining In Proceedings of the 5th I~ernnronal Conference on mending Dotabme Techl Avignon France March 1996 I I D Memalds and B Wiithrich Extending naive Bayes classifiers using long itnnsm In Proceedings of Sfh International Conference on Knowledge Discovery and Data Mining San Diego California, August 1999 I21 1 R Quinlan Induction of Decision Trees Machine karnihg 1 81-106,1986 I31 J.R Quinlan C4.5 Program for Machine Learning MorganKaufmann 1993 I41 1 C Shafer R AgnwaL and M Mehta SPRINT A scalable padlel classifier for data mining In heedings of the 22nd InterMtioMl Conference on VayLmgeLJa&bases Mumbai Bombay India Septcdcr 1996 I51 K Wang Y He J Han Pushing support cona&ts into frequent itemut mining In Prmeeding of fhe 26fh hferMtiOMl Conference on Very large Dofabases Cairo Egypt 2OOO 43-52 1161 M Wan5 B lycr and J.S Vie Scalable mining for classifration rules in relational databsses In Proceedings ofthe 1998 IIIferMfiOMIhfabaSe Engineering and Applications Sympasium cardiff Wales U.K July 840.1998 I71 S.M Weiss and C.A K&ou.jki Computer System that Learn Cla&cation andPredidion Method fmrn Sratistics Neural Neb Machine Learning and Sysfemr MorganKaufman 1991 IS K Wang S Zhou Y He Growing decision trees on support-lcss association rules In heeding of fhe sixfh ACMSIGKDD infernational confireme on hnowledge discovery and a mining Boston USA  Aupust2OOO 265-269.3 discmiation of continuous-valued amibutes for classitieation Icammg In Proceedings of the 13th Infernnr'onalfoinf Confmnce on Ampcial hfelligence 1993 1022-1027 9 I91 U.M FayVad and LB Irani Multi-lntmal 


Symbol Meaning Number of unique items Number of transactions Number of maximal potentially large itemsets Average size of the transactions Average size of the maximal potentially large itemsets Figure 9 Defnition of Parameters Time Complexity To compare the time complexity of FOLDARM and Apriori we shall focus only on the scan ning process to obtain the support counts of itemsets This is enough to see the vast improvement of FOLDARM over Apriori For each pass of the Apriori algorithm there is a need to scan the entire database regardless of the desired support threshold Suppose the database is of size a and the average size of each transaction is b Then Apriori takes O\(ab units of time to scan the database at each pass For the Erst two passes of FOLDARM only the SOTrieIT Y needs to be scanned. According to Section 6 I Y has x nodes and since each node has one link from its parent in the worst case it will take at most 2 x 5 units of time to traverse the entire structure where N  a In addition, the time needed also depends on the desired support threshold which would further reduce the number of traversals Therefore the average amount of scanning time for the Erst two passes will be far less than O\(ab N N 7 Performance Evaluation This section evaluates and compares the relative perfor mance of the Apriori and FOLDARM algorithms by con ducting experiments on a Pentium-111 machine with a CPU clock rate of 1.7 GHz 256 MB of main memory and run ning on a Windows 2000 platform The algorithms are im plemented in Java and hence large memory requirements of the Java Virtual Machine prevented us from scaling up the experiments Future experiments will be conducted to tackle this issue The SOTrieIT structure is implemented using a combination of integer arrays and Eles Implemen tation details are omitted due to the lack of space In spite of extra &le U0 requirements, FOLDARM maintains its im pressive performance Figure 9 shows the various parame ters used and their meanings The method used for generating synthetic data is similar to the one used in l To describe an experiment we the notation Tw.1z.Ny.D modiEed from the one used in l where w is the average size of transactions II is the average size of maximal potentially large itemsets y is the number of unique items and z is the size of the database We added the y parameter to represent the databases in a clearer man ner The databases used here are similar to those in SI The Erst one is T25.IlO.NlK.DlOK which is denoted as Dl while the second is T25.120.NlOK.DlOOK which is denoted as D2 The following sections analyze the performance of FOLDARM as compared to the algorithms introduced in Section 2 in different scenarios 7.1 Static Databases Figure 10 shows the execution times excluding pre processing time of FOLDARM\for the two different static databases of both Apriori and FOLDARM The databases are termed static because they are not expected to change over time From the graphs it can be quickly observed that FOLDARM outperforms Apriori in all situations In Figure 10\(a FOLDARM maintains a steady speed-up of about 10 times for support thresholds ranging from 3 to 1.5 in VI However when the support threshold falls below 1.5 the speed-up is signifcantly reduced At a support thresh old of OS FOLDARM only manages a speed-up of 1.2 times The situation changes dramatically in D2 Figure 10\(b uses a log scale for the time axis because of the vast dif ference between the execution times of FOLDARM and Apriori. FOLDARM performs at least 80 times faster than Apriori for support thresholds ranging from 3 to 2 Its performance peaks at a support threshold of 1.5 where it performs more than 160 times faster than Apriori This speed-up falls to 54 times at a support threshold of 0.5 Explanation The poor performance of FOLDARM in VI especially at lower support thresholds is due to the fact that more larger-sized frequent itemsets exist at lower sup port thresholds and FOLDARM uses the Apriori algorithm to discover large k-itemsets for k  3 Hence the com putation savings in the Erst two iterations are insigniEcant compared to the huge computation costs needed in obtain ing larger frequent itemsets Interestingly in larger databases like D2 FOLDARM performs exceptionally well The obvious vast improve ment of FOLDARM in 232 can be explained by Figure 11 which shows the number of candidate k-itemsets generated during the mining of Dl and  for a support threshold of 0.5 From Figure 11 it is clear that the main difference in candidate generation between 21 and V2 is in the num ber of candidate 2-itemsets generated Thus by eliminating the need for candidate 2-itemset generation FOLDARM achieves a much greater speed-up in D2 as it contains more than 6 times the number of candidate 2-itemsets as com pared to VI As for higher support thresholds, FOLDARM is able to outperform Apriori by up to two orders of magni tude because in D2 the maximum size of the large itemsets IC is much lower than that of VI If k increases indef 284 


1 8e+006 16e+006 14e+006 O 800000 e GOMXX D1  x  D2 f3   4m00 200000 Ork initely the performance of FOLDARM will eventually be reduced to that of Apriori However, we can conclude from the experiments that as databases and universal itemsets in crease in size k will decrease and thus FOLDARM will scale up very well against Apriori This Ending is particu larly crucial in an electronic commerce where transactions database size\are expected to arrive by the thousands and a wide variety of products \(universal itemset size is available for purchase Another important point to note is that after the Erst 2 passes FOLDARM actually uses Apriori to mine larger fre quent itemsets without relying on the SOTrieIT structure Despite of the fact that FOLDARM only differs from Apri ori during the Erst 2 passes it is much faster and scalable this congrms that candidate itemset generation during the Erst 2 passes constitutes the main bottleneck in Apriori FP-growth is currently the fastest algorithm for min ing static databases However due to the lack of time FP-growth is not implemented but its performance against FOLDARM can be evaluated using Apriori as a basis for relative comparisons The experiments conducted in  81 re port an overall improvement of only an order of magnitude for FP-growth over Apriori In addition the performance of FP-growth is on par with Apriori for support thresholds ranging from 3 to 1.5 in Dz The poor performance of FP-growth can be attributed to the cost in recursively con structing FP-trees Hence signiEcant speed-ups can only be noticed in lower support thresholds when Apriori can not cope with the exponential increase in candidate itemset generation This is undesirable because we want to mine databases efkiently at a wide range of support thresholds instead of only at low support thresholds FOLDARM over comes this limitation of FP-growth and consistently outper forms Apriori at all support thresholds and it can even per form up to two orders of magnitude faster than Apriori   2   __    __    9  p  9 Q 7.2 Dynamic Databases A dynamic database is one with frequent updates trans actions are added and removed frequently In 5 the FUP2 algorithm is presented to make use of past mining results to mine new transactional updates more efkiently Due to time constraints FUP2 is not implemented for comparison studies with FOLDARM As before with the results in its performance analysis section, we can easily assess its per formance as compared to FOLDARM with the performance of Apriori as our reference point In a database of the type TIO.14.NlK.DlOOK with an addition and deletion of 5000 transactions it is found that FUP2 performs only about twice as fast as Apriori as seen in the experiments in 5 This is its best performance against Apriori through the use of past mined knowledge In ad dition it is discovered that when the size of the updates of a database exceeds 40 of the its original size, Apriori can even outperform FUP2 Both Apriori and FOLDARM mine a dynamic database as if it were a static database because they do not make use of past results Since FOLDARM per forms up to 100 times faster than Apriori in a much larger database we can safely conclude that FOLDARM will def initely outperform FUP2 by a wide margin Explanation In spite of its ability to reuse mined results and hence reduce the number of candidate itemsets gener ated FUP2 still performs slower than FOLDARM because it needs to scan the database during the generation of large 1-itemsets and 2-itemsets FOLDARM can quickly do so simply by scanning the SOTrieIT structure which is de nitely many times smaller than the database In situations when the updates are high, the performance of FUP2 drops because the updated database becomes so different from the original one that past mining results are not helpful in determining new large itemsets In processing useless old information, precious computation time is wasted On the other hand FOLDARM does not need to retain past min ing results. Moreover, frequent database updates do not af fect it because it always mines the database from scratch Note that the SOTrieIT structure does not need to be con structed from scratch when the database is updated because it is continuously updated each time a transaction is added or retired 7.3 Dynamic Support Threshold CARMA is currently the only algorithm that allows the user to modify the support threshold on the ay We will once again use the results presented in 7 as a form of compar ison In a database of the form T10.14.NlOK.D100K it is found that Apriori outperforms CARMA for support thresh olds of 0.5 and above It is only when the support thresh 285 


D1 T25110NlKDlOK Apnon x  350 FOLOARM 0 300 I 250 c 200 F 150 100 50 0 e E 11    0 x x __-__ 0 4 9 __ x _ __    c e 1 E F 150 j Suppon Threshold  a 0 D2 T25.120.NlOK.Dl00K 0 4 m Apnon x FOLOARM 0 100000 1 0.5 1 15 2 2.5 3 Suppon Threshold 46 b Figure 10 Execution times for two databases of the form Tw.1s.Ny.D where w is the average size of transactions 3 is the average size of maximal potentially large itemsets y is the number of unique items and z is the size of the database at varying support thresholds olds are at 0.25 and below that CARMA begins to outper form Apriori only by less than 1.5 times On the other hand FOLDARM consistently outperforms Apriori by wide mar gins at various support thresholds and supports dynamic support thresholds This is because the SOTrieIT can be reused for mining at different support thresholds without additional computation Explanation The poor performance of CARMA is at tributed to its need to maintain a lattice of potentially large itemsets It will be faster only when the user does not need a precise set of large itemsets because in this case CARMA does not need to re-scan the database FOLDARM per forms much faster because the SOTrieIT stores threshold independent information and thus its performance will not be affected even if the user changes the support thresholds frequently to obtain an optimal threshold 7.4 Dynamic Universal itemset As none of the algorithms discussed takes into consider ation of the fact that unique items in the database will vary it is not possible to conduct experiments for comparison However we can approximate the most probable results by examining the characteristics of each algorithm When new transactions with new items are added to a database or when old transactions with obsolete items are retired all the discussed algorithms would have to mine the updated database from scratch. Therefore their performance against FOLDARM in such a scenario can be deduced from the pre vious sections When the universal itemset is changed, the SOTrieIT can be easily updated as seen in Figure 5 Hence it should retain its performance edge and outperform all the algorithms seen in the previous sections 7.5 Pre-processing and Storage Requirements As pre-processing is carried on a transaction at the mo ment it arrives in the database it is distributive by nature and thus will not burden a system excessively FOLDARM spends an average of only 180 ms and 250 ms in pre processing a single transaction found in 731 and D2 respec tively This amount of time is insigniEcant considering that it will result in major speed-ups in the mining process This requirement should not be taken into consideration in com paring the performance of FOLDARM and Apriori because pre-processing is done outside of the actual mining process itself The SOTrieIT structure resides in both memory and fles As primitive integer arrays are employed in memory for storing the Erst-level nodes the SOTrieIT only takes up only 2 KB and 14 KB in 271 and Dz respectively Second level nodes grow exponentially with respect to N as seen in Section 6.1 and as such they cannot be stored in memory Instead they are stored in Eles which are named after the labels of their parents These Eles take up approximately 2 MB and 53 MB for 731 and 732 respectively Therefore, it can be concluded that by distributing the SOTrieIT structure among memory and Eles scalability is ensured as hard disk space is currently in the realm of tens of gigabytes 8 Comparison of features This section focuses on the salient features of FOLDARM and elaborates on why it is more suitable for 286 


use in electronic commerce as compared to existing algo rithms discussed in Section 2 For an association rule min ing algorithm to be useful in electronic commerce it must have the following capabilities 1 General Incremental Mining Support The algorithm must obtain association rules quickly that react the latest changes to the transaction database at a certain point in time This can only be achieved if the algo rithm can perform general incremental mining which means that past mining results are exploited during the mining of a database which has many additions and deletions of transactions since the last mining opera tion that was carried out on it 2 Dynamic Threshold Support The fast-changing na ture of electronic commerce prevents the prediction of a suitable support threshold for the mining process Hence the algorithm must allow the user to change the support thresholds of the mining process until an opti mum value is found without signigcant performance degradation In other words it must be able to create and reuse mining information that is independent of the support threshold In a highly competitive environment like electronic commerce new products must be introduced frequently and old ones be retired to satisfy the changing and demand ing needs of the empowered customer Therefore the algorithm must not assume that items in the database remain xed. It must be able to reuse past mining re sults efgciently regardless of changes made to the set of unique items in the database 3 Dynamic Universal itemset Support Apriori I DHP 3 and FP-growth 8 all attempt to improve mining speed but do not satisfy any of the above criteria FUP  FUP2 5 and the Adaptive algorithm 6 satisfy the Erst criteria but do not take into consideration the other two CARMA fulflls the second criteria but does not satisfy the other two and is extremely slow Finally no algorithms exist to date that satisEes the third criteria With the SOTrieIT FOLDARM satisges all three criteria and even performs faster than all algorithms 9 Conclusions The increasing popularity of electronic commerce presents new challenges to association rule mining Due to the easy availability of huge amount of transactional data there is a urgent need for faster algorithms to mine such rich data We have proposed a new algorithm called FOLDARM which uses an efscient novel structure known as the SOTrieIT By simply eliminating the need for candi date I-itemset and 2-itemset generation, FOLDARM is able to achieve signigcant speed-ups Experiments have shown that FOLDARM is much faster than Apriori By using Apri ori as a basis for comparisons FOLDARM is proven to be faster than some prominent existing algorithms In addition as FOLDARM can maintain its performance while the sup port threshold and universal itemset change it is ideal for use in electronic commerce. Therefore though there are ad ditional pre-processing and storage requirements they are both insignigcant and worthwhile considering the advan tages that they reap As databases and their universal item sets grow in size it will be more difgcult to maintain the SOTrieIT Hence parallel versions of the SOTrieIT and the FOLDARM algorithm need to be researched to meet the rising demands of mining larger databases References I R Agrawal and R Srikant Fast algorithms for mining association rules In Proc 20th tnt Con on Very Large Databases pages 487499 Santiago Chile 1994 2 A Das W K Ng and Y K Woon Rapid association rule mining In Proc 10th Int Con on Information and Knowledge Management Atlanta Georgia 2001 3 J S Park M S Chen and P S Yu Using a hash based method with transaction trimming for mining as sociation rules IEEE Trans. on Knowledge and Data Engineering 9\(5 September 1997 4 D W Cheung J Han V T Ng and C Y Wong Maintenance of discovered association rules in large databases An incremental updating technique In Proc 12th Int Con on Data Engineering pages 106-1 14 New Orleans, Louisiana 1996 5 D W Cheung S D Lee and B. Kao A general in cremental technique for maintaining discovered associ ation rules In Proc. 5th tnt Con on Database Sys tems for Advanced Applications pages 185-1 94 Mel bourne Australia 1997 6 N L Sarda and N V Srinivas An adaptive algorithm for incremental mining of association rules In Proc 9th Int Con5 on Database and Expert Systems pages 240-245 Vienna Austria 1998 7 C Hidber Online association rule mining In Proc ACM SIGMOD tnt Con on Management of Data pages 145-154 Philadephia Pennsylvania USA 1999 8 J Han J Pei and Y Yin Mining frequent patterns without candidate generation In Proc ACM SIGMOD Int Con on Management of Data pages 1-12 Dallas Texas 2000 287 


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


