Nonlinear Hebbian Rule A Statistical Interpretation Agus Sudjianto Mohamad H Hassoun Computation and Neural Nebarks Laboratory Department of Electrical and Computer Enlyneering Wayne State University Detroit MI 48202 Abstract Recently the extension of Hebbian learning to nonlinear units has received increased attention Some sucmsful applications of this learning rule have been reported as well however a hdamental understanding of the capability of this learning 
rule is still lacking In thls paper we pursue a better understanding of what the network is actually doing by exploring the statistical characteristics of the criterion function and interpreting the nonlinear unit as a mility integral transformation To improve the capQl>llity of the nonlinear units data preprocessing is suggested A better data preprocessing leads to the development of a hm layer network which consists of linear units in the first layer and nonlinear units in the second layer 
The linear units capture and filter the linear aspect of the data and the nonlinear units discover the nonlinear effects such as clustering and other general nonlinear associations among the variables Several potential applications are demonstrated through the simulation results given throughout this paper 1 Introduction The objective of unsupervised learning is to discover features or regularities in a set of training data without teacher signals Often a learning rule is designed to transform a set of unlabeled vector data x 
E R i  1 2  m into another lower dimensional set to gain insight and understanding about important features in the data without biasing the results by imposing preconceived structures If the information in the data is adequately captured by linear relations among the variables a learning rule may be developed based on covariance \(correlation among the variables For example one-layer linear feed\200orward neural networks employing Hebbian type learning have been proposed to project data onto the directions of 
maximum variances known as principal components Qa 1982 see also Hassoun et al 1993 However certain topological relationships among the variables may not be adequately captured by such linear relations Softky and Kammen 1991; Taylor and Coombes 1993 Examples of problems which require the discernment of topological relationships in the input data are to handle this class of problems commonly found in pattern recognition clustering and filtering applications. Oja et al 1991\extended the Hebbian learning to 
nonlinear units This extension has drawn increased attention Softky and Kammen 1991 Shapuo and Rugel-Bennett 1992 Taylor and Coombes 1993 Oja and Karhunen 1993 and applications Oja et al 1991 Karhunen and Joutsensalo 1993 Oja and Karhunen 1993 however it remains theoretically unclear what the network is actually doing In this paper we explore the statistical characteristics of this learning rule using the probability integral interpretation The results reveal a better understandmg of the processing capabilities of 
the network and its potential applications Our findings provide analytical justification to the previously reported applications of using nonlinear units for extracting sinusoidal signals buried in noise As suspected by previous works we show that the nonlinear units are indeed capable of capturing nonlinear relationshp among the input variables For some applications such as clustering data preprocessing is necessary to either scale the data or remove linear structures in the input data This preprocessing suggests the use of a 
tsamlayer network which is capable of extracting both linear and nonlinear structures in the data 2 Hebbian Learning in Nonlinear Units As mentioned above Qa et al 1991 generalized the stable form of Hebb's learning rule to nonlinear units One of their learning rules for a single unit has the following form Aw=q I-wwT]L\(y 1 where qk W  wight vector  learning rate at step k X  input pattern 0-7803-1901-X/94 4.00 01994 IEEE 1247 


y  wT x  projection of the input patterns onto the L\(y  learningsignal weight vector The L\(y x term is the realization of a nonlinear version of Hebb's rule and the w wT L\(y x term serves as the stabilizer of Hebb's rule The learning rule in 1 may be considered as a constrained went ascent algorithm by letting VJ L\(Y 2 Depending upon the nonlinearity involved in the learning signal a criterion function for 1 may be synthesized For example if the learning signal is assumed as L\(y  tanh\(y l  tanhv then the appropriate criterion function is J  tanh*\(y Noticing that if the output of the nonlinear unit is z  tanhb the learning rule in 1 may be perceived as a went algorithm to solve an optimization izat ion problem with the ob~ective function being the variance of the output Oja et al 1991 Consider the outputs of a unit z and assume that they are governed by a probability density function Hz Here the criterion function maximized by Hebb's rule may be written as oo subjectto WT w=l 3 Applying constrained went ascent as in l with J as in 3 and using the chain rule leads to the following learning rule 4 where the constant term has been absorbed into qu The stochastic approximation version of 4 may be written as Aw qk I W wT dz dY Note that the learning signal in 5 is given by dz y xz is a linear function ofy say z y dY dz then   1 and the learning rule is reduced to the simple Hebbian rule for a linear unit dY For the case of linear units the ective function in 3 has an obvious interpretation i.e projection of the input patterns in the directions of maximum variances known as the principal components The same cfiterion function applied to nonlinear units on the other hand does not have an apparent meaning Note that both hear and nonlinear learning rules are seeking a set of weight parameters such that the outputs of the unit have the largest variance The nonlinear unit however constraints the output to remain within a bounded range e.g z  tanh\(y limits the output within 1 11 The restriction of the nonlinear unit outputs si@cantly distinguishes nonlinear units from their linear counterparts and dramatically affects the mechanism of variance maximization For linear units the learning rule pushes the weight vector to favor components ofthe input vectors or their linear combinations having the largest variance regardless of the shape of the distribution function p\(z On the ather hand due to the restriction of the nonlinear outputs nonlinear learning will produce outputs with a U-shape distribution in order to maximize the output variance Thus the variance maximization obJective in the criterion function drives the learning rule to prefer a wight vector w such that the output z is distributed near the extremes 1 and 1 As an example suppose z follows a standard beta distribution \(the beta distribution is chosen because it has various shapes depending on its parameters where B\(p q is the beta function The mean and variance of distribution Johnson and Kotz E[z P P+4 var\(z P+4 P+1I2 p+q+1 7 this standard beta 1970 are given as 9 Consider three beta distributions as shown in Figure 1 with the following parameters 1 p  q  3 2 p  q  1 and 3 p  q  0.5 The mean of each of the 1248 


3 distributions is equal to 0.5 where the variance are equal to 0.0536,O 1667 and 0.2222 respe&vely in a uniform distribution within the interval 0 11 Row 1984 For example, consider the case ofy having a standard normal distribution given by 0 0.2 0.4 0.6 0.1 1 2 Figure 1 Beta distribution with the following parameters 1  q  3 2 p  q  1 and 3 p  q  0.5 Note that the U-shape distribution p  q  0.5 has the largest variance compare to the others The criterion function in 3 forces the learning rules in 4 and 5 to prefer ths U-shape distribution 3 Probability Integral Transformation and Nonlinear Units The nonlinear units in addition to restricting the output to withm a certain range also transform the probability density of the weighted sum y  wT x into other probability density functions The probability density function of the output z is determined by the probability density function of the input vector the weight vector and the shape of the activation function In the following discussion we show how the shapes of activation functions govern the behavior of the learning rule Consider a single unit with the following output relation where w is a nonlinear function bounded within 0 11 Clearly z takes on values in the interval 1 I z I 1 If a is chosen as a monotonic increasing function on 0 11 then w may be interpreted as a cumulative probability integral distribution function Hence the output of a nonlinear unit in 10 may be perceived as a linear projection transformation with projection vector w of a multivariate random variable x into a univariate random variable y and followed by a probability integral transformation of y into z If y is randomly distributed with probability density function p\(y and cumulative distribution function w then the probability integral transformation I ofy results the probability integral transformation ofy results in z being umformly distributed in 1 11 On the other hand ify is not normally distributed, then z will not follow a uniform distribution Consider input vectors x E R where each component is generated by a different probability distribution The restriction on the nonlinear output causes the learning rule 4 and 5 in order to maximize the output variance to seek a weight vector w such that p\(z has a U-shape distribution deviates away fiom a uniform distribution The learning rules will suppress the components of x or their linear combinations that follow a normal distribution because they generate a uniformly distributed z and intensify those which depart from normality and produce U-shape distribution of z The nonlinear activation function the probability integral transformation together with w play the role of shaping the distribution of z On the contrary the linear neuron does not involve a probability integral transformation to change the structure of output distributions Hence whatever linear combination of components of x that produces the largest variance is preferable by the linear learning Shapiro and Prugel-Bennett 1992 also observed the important role of the shape of the activation function To illustrate the sigtllficance of the type of activation function used consider input vectors consisting of 2 components a pure sinusoidal x1  sin@\where t is uniformly distributed within 0 2x1 and a gaussian random component xz with zero mean and unit variance The probability density and the cumulative distribution functions of x2 is represented by Equations 1 1 and 12 respectively The probability distribution function of x1 can be easily shown Row 1984 to be 1249 


13 1 g Xl   ZJq This distribution is known as the arc-sine distribution Johnson and Kotz 1970 and the corresponding cumulative distribution function is given by 1 _ I xl  sin  x   0.5 x The shape of the probability density functions of x1 and x2 are shown in Figure 2a Accordingly the activation functions using \(12 and 14\are shown in Figure 2b 0.6 h J 0.4 0.2 we wish to extract x1 from the input signal In this case x2 may be suppressed by transforming it into a uniform distribution and xt may be intensified into a U-shape distribution using the probability integral transformation in 12 To avoid the integratton calculation an approximation to the cumulative normal distribution may be obtained by using a cumulative logistic distribution Johnson and Kotz 1970 z  tanhgy For simplicity we select a logistic distribution with f3  1 For a better approximation f3 may be adjusted such that the difference from cumulative normal distribution is less than one percent see Johnson and Kotz 1970 Since the two components of each input vector have different scales variances a preprocessing calculation is necessary using their standard xi xi deviations x  where cy is the standard deviation of xi The result of the training is shown in Figure 3a I 1 0 2 1 0 1 2 3 X Figure 2a Probability density function of x1 from Equation \(13 and x2 from Equation 1 1 3 4  20 40 0 10 100 3 2 1 0 1 2 3 X Figure 2b Activation functions constructed from probability density functions of x1 and x2 from Equation \(14 and  12 respectively Depending on which component of x is interesting an activation function can be selected to enhance or suppress either x1 or x2 If 12 is chosen as the activation function then x1 and x2 produces z with a U-shape distribution and a umform distribution within 1 11 respectively Similarly the opposite effect occurs with the selection of 14 as the activation function To demonstrate this behavior a set of 100 training samples is generated and a nonlinear unit is trained using the learning rule in 5 with a constant learning rate q  0.01 Suppose Figure 3a The result of nonlinear unit training using the activation function in 12 to extract the sinusoidal signal Similarly to extract x2 we can use a probability integral transformation in \(14 Figure 3b shows that the nonlinear unit is capable of extracting the gaussian component 3 I 3 U 20 40 0 to 100 Figure 3b The result of nonlinear unit training using activation function in 14 to extract gaussian signal 1250 


This result agrees with some early applications for sinusoidal signal extraction qa et al 1991 Karhunen and Joutsensalo, 1992 Qa and Karhunen 1993 In these applications the inputs to the network are L successive lags of samples x\(k  x\(k x\(k+l  4k+L-1 Sigmoidal type functions re chosen for the activation functions Similar to the previous examples the sigmoidal signal drives the learning rule to find projection vectors that suppress the gaussian or near gaussian signals and enhance the U-shape distributed signals i.e the sinusoidal signals Note that either the input data or the activation functions need to be scaled to produce the desired results 4 Clustering Applications and Linear PCA Preprocessing The fad that Hebbian learning in a saturating nonlinear unit tends to yield an output clustered at saturation extremes leads to a natural application for data clustering To demonstrate the capalnlity of nonlinear units for data clustering, forty samples of 2-Qmensional data are generated as shown in Figure 4 The data are generated from uniform distributions with 1 I x2 I 2 for both classes and 0 I x1 I 0.5 and 0.5 I x I 1 for class I and class 11 respectively c 2 X 0 X t o Class II 1 0 0.2 0.4 0.6 0.8 1 X1 Figure 4 Forty samples of 2-dimensional data from 2 classes The data represent two class populations In this problem we are interested in finding a projection that indicates a clear separation betwen the two classes We should not use the class labeling in the training but it is of course interesting to compare that labeling with any structure found by Hebbian learning in a nonlinear unit To produce more than 1 cluster the learning should move away from a projection that produce a unimodal or gaussian 1251 type pattern by applying a cumulative normal distribution as the activation function Data clustering often has little to do with any linear covariance structure in the data By preprocessing the data with a linear PCA network this linear structure may be removed The idea is to carry out a linear transformation--rotation location and scale change--to eliminate all the location scale and correlational structures This preprocessing leads to a layer network architecture see Figure 5 where the first layer is a linear PCA network e.g Sanger's network Sanger 1989 which removes linear structure in the data and the second layer is a nonlinear unit which is responsible for extracting nonlinear structures The variance scaling in between the two layers is essential to assure that the inputs to the nonlinear unit has the same scale The linear layer and the variance scaling implement the following transformation v=S U\(x-E[x 15 where U is a matrix composed of eigenvectors of the covariance matrix of the input vectors and S is the corresponding diagonal matrix of eigenvalues igure 5 A two-layer network with linear PC L units in the hidden layer and a nonlinear PCA unit in the output layer The learning rule in 5 with a constant learning rate q  0.01 is used to train the nonlinear unit Agam for a fast computation an activation function z  tanhw is utilized to approximate the cumulative normal distribution function The initial value of the weight vector w of the nonlinear unit is set equal to the might vector of the first principal component of the linear PCA to provide nonlinear learning with a good starting point The projection plots of the first and second principal components o,=u:x;i=1,2 of the linear PCA outputs are shown in Figure 6a and 6b respectively Notice that linear PCA is distracted by components with large variances and fads to provide genuine clusters 


Figure 6c shows the plot of the outputs of the nonlinear unit z  tanh\(wT v The nonlinear unit is capable of learning a projection vector w that provides good data clustering For comparison Figure 6d shows the plot of the outputs of the nonlinear unit with variance scaling but without the linear PCA preprocessing z  tanh\(w i XI  3 The result demonstrates the necessity of having the linear network removes all linear structure More feeding the data into nonlinear units Qi y xm 2 1 p xw up 2 1 0 1 2 1 0.5 0 0.5 1 wmpmqm d 1 0.5 0 0.5 1 Figure 6 Data clustering using projections to a the first principal component of the linear PCA network b the second principal component of the linear PCA network c the first principal component of the nonlinear unit with a linear PCA preprocessing and d the first principal component of the nonlinear unit without a linear PCA preprocessing 5 Conclusion We have demonstrated the behavior of nonlinear units by employing a probability integral transformation interpretation of the unit activation function This approach leads to a better understanding of the learning caphlities of the nonlinear units Two-layer networks with linear units in the hidden layer are proposed to enhance the caplnlity of nonlinear units The hidden layer extracts all linear structure in the input data and allows the nonlinear units to exclusively manipulate the nonlinear information The approach also suggests and justifies potential applications of Hebbian learning in nonlinear units for signal extraction and data clustering Acknowledgments The authors would like to thank Paul Watta for his critical reading of this manuscript References Hassoun M H Sudjianto A and Mortazavian H 1993 On the Stability of Hebb's Rule submitted to Neural Networks Johnson N L and Kotz S 1970 Distribution in Statistics Continuous Univariate Distributions-2 John Wiley New York Karhunen J and Joutsensalo, J 1992 Nonlinear Hebbian Algorithm for Sinusoidal Frequency Estimation Artrficial Neural Networks 2 Prm ICANN-92 Brighton UK Aleksander I and Taylor, J eds North-Holland 1099-1 102 Qa E 1982 A Simplified Neuron Model as a Principal Component Analyzer Journal of Mathematical Biology 15,267-273 Oja E and Karhunen J 1993 Nonlinear PCA Algorithms and Applications Report A18 Helsinki University of Technology Espoo Finland Oja E Ogawa H and Wangviwattam J 1991 Learning in Nonlinear Constrained Hebbian Networks Arti9cial Neural Networks Proc IC4 91 Espoo Finland Kohonen T Makisara K Simula O and Kangas J eds North-Holland 3 85-390 Rohatgi V K 1984 Statistical Znference John Wiley  Sons New York Sanger D T 1989 Optimal Unsupervised Learning in a Single-layer Linear Feed-fomd Neural Network Neural Networks 2,459-473 Shapiro J L and Prugel-Bennett A 1992 Unsupervised Hebbian Learning and the Shape of the Neuron Activation Function Artificial Neural Network 2 Proc ICANN-92 Brighton UK Aleksander I and Taylor J eds North-Holland 1099-1102 Sow W R and Kammen D M 1991 Correlations in Illgh Dimensional or Asymmetric Data Sets Hebbian Neuronal Processing Neural Networks 4,337-347 Taylor J G and Coombes S 1993 Learning Higher Order Correlations Neural Networh 6 423-427 1252 


because they will not appear in any large itemset in the later iteration At any k-th iteration some items in db or DB which is not needed for finding large itemsets in the next iteration can be identified and hence removed At any k-th iteration during the scan in the incre ment db while FUP is counting the support for sets in the candidate sets C and W for each transaction TI the Reduce-db function is called It counts for each I E TI the number of sets in C and W which contain I This number gives an upper bound on the num ber of large k-itemsets that contain I If this number is smaller than k then I cannot belong to any large k+l and hence can be removed from all the transactions Using this number Reduce-db can prune off some items from db After the set C has been pruned against db it can be seen that any items in DB which does not belong to any set in Lk or C will not belong to any large IC  l-itemset Therefore in the scanning of DB to compute the supports of sets in C all items that do not belong to any set in Lk or C can be removed In the FUP algorithm, the function Reduce-DB performs this reduction In FUP we have also integrated the direct hashing technique in 9 which further reduces the number of the candidate sets used in iteration two 4 Performance Study In order to assess the performance of FUP experi ments are conducted to compare its performance with that of Apriori and DMP The experiments were per formed on an AIX system on an RS/6000 workstation with model 410 As will be presented in the follow ing the result shows that FUP is much faster than the most successful mining algorithm with respect to updating association rules FUP performs 2 to 6 times faster than DHP for a moderate size database of 100,000 transactions When the database is scaled up to 1,000,000 transactions the speed-up is 2 to 16 times As explained before the key of the speed-up lies on the much smaller amount of candidate sets In some cases the number of candidate sets generated were counted, and it was found that the amount gen erated in FUP is reduced to the range of 1.5  5 of that in DHP This is a very significant reduction As mentioned above we also tested FUP with some very large databases It was found that FUP actually performs much better in larger databases 4.1 Generation of synthetic data The databases used in our experiments are syn thetic data generated using the same technique intro duced in l and modified in 9 The parameters used ILI Number of transactions in database DB Number of transactions in the increment d Mean size of the transactions Mean size of the maximal potentially large itemsets Number of potentially large itemsets Number of items Table 1 Parameter Table TlO.l4.DlOO.dl 3 8.00 1 1 a E 6.00 E 4.00 0 z 2*oo 3 0.00 6.00 4.00 2.00 1.00 0.75 W Minimum support DHPFUP  pri01-i Figure 2 Performance Ratio are similar to those in 9 except that the size of the increment is an additional parameter Table 1 is a list of the parameters used in our synthetic database In the following we use the notation Tx.Iy.Dm.dn modified from the one used in 9 to denote a database in which D  m thousands d  n thousands IT1  x and 111  y In our experiments we set ILI  2000 N  1000 and the secondary parameters S  5 P  50 and Mj  2000 S is the clustering size used in the generation of potential large itemsets P is the pool size to store potential large itemsets from which transactions will receive their items Mj is the multiplying factor associated with the pool Readers not familiar with these parameters please refer to l The way we create our increment is a straight for ward extension of the technique used to synthesize the database In order to do comparison on a database of size D with an increment of size d A database of size D  d is first generated and then the first D transactions are stored in the database DB and the remaining d transactions is stored in the increment db Since all the transactions are generated from the same statistical pattern, it models very well real life 91 112 


11 0.14.01 0O.dl 5 0.06 a 0.05 f 0.04 Z 8 0.03 C 0 0 5 0.02 g 8 0.01 K 0.00 6.00% 4.00 2.00 1.00 0.75 Minimum Support 0 FUP/DHP FUP/APRIORI i3 E 3 Q 2.5 e 2 1.5 4 15K 25K 75K 125K 175K 250K 350K In creme n t Size Figure 3 Reduction on Candidate Sets Figure 4 Speed Up Ratio vs Increment Size updates 4.2 We have compared the performance of FUP against that of DHP and Apriori The first comparison was done on an updated database T10.14.DlOO.dl The performance ratios between them are shown in Fig ure 2 In our implementation of the DHP a hash table of size 100 is used and hashing is only used in the generation of the size-2 candidate sets This is the same policy used in 9 For small support FUP is 3 to 6 times faster than DHP and 3 to 7 times faster than Apriori For larger support, it is less costly to re-run the mining algorithm on the updated database since the number of large itemsets is relatively smaller In terestingly FUP is still 2 to 3 times faster in this case 4.3 Reduction on the number of candi As explained before FUP substantially reduces the number of candidate sets generated The effect is par ticularly significant at the first iteration In Figure 3 the chart shows the ratio of the number of candidate sets generated by FUP when comparing with the two mining algorithms The amount of reduction ranges from 98 to 95% when FUP is compared to DHP It is even greater when it is compared with Apriori 4.4 Performance of FUP with large incre ment In general the larger the increment is the longer it would take to do the update Also the gain in speed up would slow down Two sets of experiments have been performed to support this analysis A database T10.14.DlOO.dmwith updates of lK 5K and 10K were generated and different updates with different sup ports were done by FUP and DHP For the same sup FUP versus DHP and Apriori date sets port the speed-up ratio decreases when update size increases For example when the support is 2 the ratio decreases from 5.8 to 3.7 We also want to find out whether the decreas ing of the performance ratio as the size increases in the update would eventually bring the performance of FUP down to that of DHP In the same setting of T10.14.DlOO.dm we increase the increment size m from 10K gradually to 350K for comparison The per formance ratio is plotted in Figure 4 A gradually level off only appears when the increment size is about 3.5 times the size of the original database The fact that FUP still exhibits performance gain when the incre ment is much larger than the original database shows that it is very efficient 4.5 Small overhead of FUP We also have done some experiments for the pur pose of analyzing the overhead incurred by the FUP In general if the time to compute the set L\222 from an updated database DB U db is added to the time to compute the original set L of large itemsets from the database DB by a mining algorithm the sum would be larger than that if the same mining algorithm was applied directly on DBUdb to compute L\222 The differ ence of these two time values is a measurement of the overhead of the update If the overhead is small, then it indicates that the update was done very efficiently We have designed some experiments to analyze the overhead of FUP by measuring this difference It was found that the bigger the increment is the smaller this overhead becomes In our experiment what was dis covered is that when the increment is much smaller than the original database, the 0verhea.d percentage 113 


ranges around 10  15 Once the increment is larger than the original size the overhead decreases very rapidly from 10 to 5 This is a very encouraging result because it shows that FUP not only can benefit update with small increment it actually works very well in the case of large increment 4.6 Performance in scaled-up databases Our last experiment is done in a scaled-up database The database is T10.I4.D1000.d10 which contains 1 million transactions The performance ratio between DHP and FUP in this scaled-up database ranges from 3 to 16 The result shows that the gain from FUP will in fact increase if the database becomes larger This shows that FUP is very adaptive to size increase and can be applied to very large databases 5 Discussion and Conclusions We studied an efficient fast incremental updating technique for maintenance of the association rules dis covered by database mining The developed method strives to determine the promising itemsets and hope less itemsets in the incremental portion and reduce the size of the candidate set to be searched against the original large database The method is implemented and its performance is studied and compared with the best algorithms for mining association rules studied so far The study shows that the proposed incremen tal updating technique has superior performance on database updates in comparison with direct mining from an updated database The incremental updating technique is applicable to the databases which allow frequent or occasional updates when new transaction data are added to a transaction database We have also investigated the cases of deletion and modification of a transaction database Recently there have been some interesting stud ies at finding multiple-level or generalized association rules in large transaction databases 6 111 The exten sion of our incremental updating technique for mainte nance of multiple-level or generalized association rules in transaction databases is an interesting topic for fu ture research References R Agrawal T Imielinski and A Swami Mining Association Rules between Sets of Items in Large Databases In Proc 1993 ACM-SIGMOD Int Conf Management of Data 207-216 May 1993 R Agrawal and R Srikant Fast algorithms for mining association rules. In Proc 1994 Int Conf Very Large Data Bases pages 487-499 Santiago Chile September 1994 D.W Cheung A W.-C Fu and J Han Knowledge discovery in databases A rule-based attribute-oriented approach In Proc 1994 Int 2221 Symp on Methodologies for Intelligent Systems pages 164-173 Charlotte North Carolina Octo ber 1994 U M Fayyad G Piatetsky-Shapiro P Smyth and R Uthurusamy Advances in Knowledge Dis covery and Data Mining AAAI/MIT Press 1995 J Han Y Cai and N Cercone Data driven discovery of quantitative rules in relational databases IEEE Trans. Knowledge and Data En gineering 5:29-40 1993 J Han and Y Fu Discovery of multiple-level association rules from large databases In Proc 1995 Int Conf Very Large Data Bases Zurich Switzerland Sept 1995 M Klemettinen H Mannila P Ronkainen H. Toivonen and A I Verkamo Finding inter esting rules from large sets of discovered associa tion rules In Proc 3rd Int\222I Conf on Informa tion and Knowledge Management pages 401-408 Gaithersburg Maryland Nov 1994 R Ng and J Han Efficient and effective cluster ing method for spatial data mining In Proc 1994 Int Conf Very Large Data Bases pages 144-155 Santiago Chile September 1994 J.S Park M.S Chen and P.S Yu An effec tive hash-based algorithm for mining association rules In Proc 1995 ACM-SIGMOD Int Conf Management of Daia San Jose CA May 1995 G Piatetsky-Shapiro and W J Frawley Knowl edge Discovery in Ratabases AAAI/MIT Press 1991 R Srikant and R Agrawal Mining generalized association rules In Proc 1995 Int Conf Very Large Data Bases Zurich Switzerland Sept 1995 114 


The disadvantage of this rule-oriented control strategy is that it imposes a restriction on the mixing of forward and backward chaining rules such that a forward chaining rule cannot read any data written by backward chaining rules STO87 To describe this problem let the following be a series of rules Ra to Rd and the resuls REa to REd derived by these rules Ra Rb Rc Rd DB  R  REb  R  REd Also let Ra and Rb be defined as backward chaining rules and Rc and Rd as forward chaining rules If the original database DB is updated rules Rc and Rd though they are forward chaining rules will not be triggered to update the result REd until someone requests the data of REb Thus REd may be iriconsistent with the base data To overcome this problem we use a result-oriented control strategy in which we specify for each result derived subdatabase whether it is to be pre-evaluated or post evaluated The same rule may follow the forward or backward chaining strategy depending on whether the derived subdatabae is to be pre or post-evaluated To illustrate by the example above assume that REd is defined as pre-evaluated and REb is defined as post evaluated Whenever the database DB is updated the rules Ra Rb Rc and Rd will be triggered in the forward chaining fashion to keep REM which is explicitly stored up-to-date REb on the other hand will be evaluated whenever a retrieval operation is issued against it In this case the rules Ra and Rb that derive REb are applied in the backward chaining fashion Thus Ra and Rb follow one control strategy when deriving RFxl and the other control straregy when deriving REb This technique offers more flexibility and alleviates the restriction in POSTGRES described above 7 Conclusion In this paper we have introduced the induced generalization association construct and presented a deductive rule-based language for object-oriented databases The world of subdatabases is closed under this language which facilitates defining inference chains in which each rule derives a new subdatabase based on the subdatabases derived by previous rules in the chain The transitive closure operation can be specified in our language in the form of looping rather than in a recursive form A result-oriented control strategy to be used as the underlying implementation technique has also been introduced in this paper ACKNOWLEDGEMENTS Research on the rule-based language was supported by the U.S West Advanced Technologies grant number UPN 88071315 Work on the Object-Oriented Query Language OQL was supported by the Navy Manufacturing Technology Program through the National Institute of Standards and Technology formerly the National Bureau of Standards grant number 60NANB4wO17 and by the National Science Foundation grant number DMC-8814989 The development efforts are supported by the Florida High Technology and Industry Council grant number UPN 85100316 BIBLIOGRAPHY ALA89a A.M Alashqur S.Y.W Su and H Lar OQL A Quy Language for Manipulating Object-onented Datah Accepted for Publication the 15th VLDB Int Con 1989 ALA89b A.M Alashqur A Query Model and Query and Knowledge Definition Langwi~es for Object-oriented Databases a Ph.D BAN87 BAT85 cER86 CHA84 COD79 DEL88 DIT86 HS87 FOR88 GAL84 HAM81 m7 JAR84 LAM89 MAI88 RAS88 STO87 SU89 TY88 U85 VAS84 Thesis Univedty if Florida 1989 Jay Banerjee et al Data Model Issues for Object-Oriented Aplications ACM Trans on Ofice Information Systems January 1987 D Batory and W Kim Modeling Concepts for VLSI CAD objects ACM TODS September 1985, pages 322-346 Stefan0 Ceri George Gottlob and Gio Wiederhold Interfacing Relational Databases and Prolog Efficiently Roc of the 1st Intl Con on Expert Database Systems 1986 C L Chang and A Walker PROSQL a PROLOG Programming Interface with SQLDS F'mxdngs of the 1st Intl Workshop on Expert Database Systems 1984 E Codd Extend~ng the Database Relational Model to Capture More Meaning ACM TODS Vol 4 No 4 1979 Lois ML Delcambre and James N Etheredge A self Controlling Interpreter for the relational Production Language Roceedings of ACM SIGMOD Conference on Management of Data 1988, pages 396403 KR Dimich Object-oriented Database Systems the Notion and Issues Roc of rhe Intl Workshop on Object-Oriented Database Systems califomia September 1986 D.H Fishman et al Iris An Object-Oriented Database Management System ACM Transaction on Oftixe Informarion Systems January 1987 Pages 4869 S Ford et al Zeitgeist Database support for object-oriented rogramming in the F  gs of the Second International Workshq on Object-oriented Database Systems 1988 Heme Gallaire Jack Mier and Jean-Marie Nicolas Logic and Databases A Deductive Approach ACM Computing Surveys June 1984 Pages 153-185 M Hammer and D McLeod Database Description with SDM A Semantic Associon Model ACM TODS Sepember 1981 R Hull and R King Semantic Database Modeling Survey Applications and Research Issues ACM Computing Surveys September 1987 Mauhias Jark Jim Clifford and Yannis Vassiliou An Optimizing hlog Front-End to a Relational Query System Roc of ACM SIGMOD Con on Management of Data 1984 H.M Lam S Su and A.M Alashqur Integrating the Concepts and Techniqws of Semantic Data Modeling and the Objectdented wradigm Roc of the 13th Intl Computex Software and ApptiCationS Conference COMSAC 89 1989 Christcphe de Maindreville and Eric Simon A Production Rule Based Approach to Deductive Databases Roc of the 4th Intl Con on Data Engineering California 1988 L Raschid and S.Y.W Su A Transaction-oriented Mechanism to Control Precessing in a Knowledge Base Management System Pmc of the Intl Con on Expert Database Systems 1988 Michael Stonebraker Eric Hanson and Chin-Heng Hong The Design of the POSTGRES Rules System Roc of the 3rd Intl Con on Data Engineering California 1987 S.Y.W Su V KrishnamurIhy and H Lam An Object oriented Semantic Association Model OsAM appearing in A.I in Indus&l Engineering and Manufacturing Theoretid Issues and Applications S Kumara et al eds American Institute of Industrial Engineering 1989 Frederick Ty G-OQL Graphics Interface to the Object Oriented Query Language OQL Master thesis University of Florida 1988 Jeffrey ullman Implementation of Logical Query Languages for Databases ACM TODS September 1985 Y Vassiliou J Clifford and M Jark Access to Specific Declarative Knowledge by Expert Systems The Impact of hg"'ning Decision Suppat Systems 1 1 1984 67 


 s_suppkey s_nationkey ps_partkey ps_suppkey ps_supplycost p_partkey p_name   l_partkey l_discount l_quantity l_orderkey l_suppkey l_extendedprice o_orderkey o_orderdate n_nationkey n_name p_partkey p_name   246\262 1 2 3 4 5 7 6 8 9 10,#11,#12,#13 14 15 16 17 18 1 2 Figure 11 Execution plan of TPC-D query 9 for transposed files 2 0 20 40 60 80 100 0 50 100 150 200 Time [s CPUusage NetSend NetRecv Disk 10 8 6 4 0 Throughput [MB/s CPU usage 8 9 10 13 Figure 12 Execution trace of TPC-D query 9 with transposed files 11 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


of I/O But the resulting speedup compared with the previous plans exceeds 2 which is quite satisfactory Table 3 shows the results of above right-deep rd left-deep ld and transposed file tp methods along with the reported results of other commercial systems for 100GB TPC-D query 9 Because our system lacks the software and maintenance price metrics the overall system price can\220t be determined accurately Hardware components themselves cost less than 0.5M We can observe that our system achieves fairly good performance Above all the execution time with the transposed files is twelve times as short as the most powerful commercial platform These results strongly support the effectiveness of the commodity PC based massively parallel relational database servers  System Exec Time Price Teradata on NCR 5100M 160 000 133MHz Pentium 20GB Main Memory 400 Disk Drives 953.3 17M Oracle 7 n DEC AlphaServer 8400 12 000 437MHz DECchip 21164 24GB Main Memory 84 Disk Drives 1884.9 1.3M Oracle 7 n SUN UE6000 24 000 167MHz UltraSPARC 5.3GB Main Memory 300 Disk Drives 2639.3 2.1M IBM DB2 PE on RS/6000 SP 306 96 000 112MHz PowerPC 604 24GB Main Memory 96 Disk Drives 2899.4 3.7M Oracle 7 n HP9000 EPS30 12 000 120MHz PA7150 3.75GB Main Memory 320 Disk Drives 7154.8 2.2M Our Pilot System 100 000 200MHz Pentium Pros 6.4GB Main Memory 100 Disk Drives rd 193.7 ld 177.2 tp 77.1 see text Table 3 Execution time of 100 GB TPC-D Q9 on several systems 12 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


4 Data mining 4.1 Association rule mining Data mining which is a recent hot research topic in the database field is a method of discovering useful information such as rules and previously unknown patterns existing behind data items It enables more effective utilization of transaction log data which have been just archived and abandoned Among the major applications of data mining is association rule mining so called 217\217basket analysis.\220\220 Each of the transaction data typically consists of a set of items bought in a transaction By analyzing them one can derive some association rule such as 217\21790 of the customers who buy both A and B also buy C.\220\220 In order to improve the quality of obtained rules a very large amount of transaction data have to be examined requiring quite a long time to complete First we introduce some basic concepts of association rule Let 000 000 001 000 1 001\000 2 001\002\002\002\001\000 000 002 be a set of items and 003 000 001 003 1 001\003 2 001\002\002\002\001\003 001 002 be a set of transactions where each transaction 003 002 is a set of items such that 003 002 004\000  n itemset 004 has support 005 in the transaction set 003 if 005  f transactions in 003 contain 004  here we denote 005 000 005\006\007\007\b\t 003 001 004 002 An association rule is an implication of the form 004 005 n  where 004\001 n 004 000  and 004 006 n 000 007  Each rule has two measures of value support and confidence  The support of the rule 004 005 n is 005\006\007\007\b\t 003 001 004 b n 002  The confidence 013 of the rule 004 005 n in the transaction set 003 means 013 of transactions in 003 that contain 004 also contain n  which can be written as 005\006\007\007\b\t 003 001 004 b n 002 f\005\006\007\007\b\t 003 001 004 002  For example let r 1 000 001 1 001 3 001 4 002  r 2 000 001 1 001 2 001 3 001 5 002  r 3 000 001 2 001 4 002  r 4 000 001 1 001 2 002  r 5 000 001 1 001 3 001 5 002 be the transaction database Let minimum  support and minimum confidence be 60 and 70 respectively First all itemsets that have support above the minimum support called large itemsets  are generated In this case the large itemsets are 001 1 002 001 001 2 002 001 001 3 002 001 001 1 001 3 002  Then for each large itemset 004  n association rule 004 t n 005 n 001 n 004 004 002 is derived if 005\006\007\007\b\t 003 001 004 002 f\005\006\007\007\b\t 003 001 004 t n 002 n minimum confidence  The results are 1 005 3 001 005\006\007\007\b\t 003 000 60 001 b\016\017 000\020\021\016\013\021 000 75 002 and 3 005 1 001 005\006\007\007\b\t 003 000 60 001 013\b\016\017 000\020\021\016\013\021 000 100 002  The most well known algorithm for association rule mining is the Apriori algorithm[1 We have studied several parallel algorithms for mining association based on Apriori One of these algorithms called HPA Hash Partitioned Apriori is discussed here Apriori first generates candidate itemsets and then scans the transaction database to determine whether each of the candidates satisfies the user specified minimum support and minimum confidence Using these results the next candidate itemsets are generated This continues until no itemset satisfies the minimum support and confidence The most naive parallelization of Apriori would copy the candidates over all the processing node and make each processing node scan the transaction database in parallel Although this works fine when the number of candidates is small enough to fit in the local memory of a single processing node memory space utilization efficiency of this method is very poor For large scale data mining the storage required for the candidates exceeds the available memory space of a processing node This causes memory overflow which results in significant performance degradation due to an excessive amount of extra I/Os HPA partitions the candidate itemsets among the processing nodes using a hash function as in the parallel hash join which eliminates broadcasting of all the transaction data and can reduce the comparison workload significantly Hence HPA works much better than the naive parallelization for large scale data mining The 022 th iteration pass 022  f the algorithm is as follows 1 Generate the candidate itemsets Each processing node generates new candidate itemsets from the large itemsets of the last  001 022 t 1 002 th iteration Each of the former itemsets contains 022 items while each of the latter itemsets contains 001 022 t 1 002 items They are called 022 itemsets and 001 022 t 1 002 itemsets respectively The processing node applies the hash function to each of the candidates to determine the destination node ID If the candidate is for the processing node itself it is inserted into the hash table otherwise it is discarded 13 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


 30 40 50 60 70 80 90 100 110 120 30 40 50 60 70 80 90 10 0 Execution Time [s Number of Nodes Figure 13 Execution time of HPA program pass 2 on PC cluster 2 Scan the transaction database and count the support count Each processing node reads the transaction database from its local disk 000 itemsets are generated from that transaction and the same hash function used in phase 1 s applied to each of them Each of the 000 itemsets is sent to certain processing node according the hash value For the itemsets received from the other nodes and those locally generated whose ID equals the node\220s ID the hash table is searched If hit its support count value is incremented 3 Determine the large itemset After reading all the transaction data each processing node can individually determine whether each candidate 000 itemset satisfy user-specified minimum support or not Each processing node sends large 000 itemsets to the coordinator where all the large 000 itemsets are gathered 4 Check the terminal condition If the large 000 itemsets are empty the algorithm terminates Otherwise the coordinator broadcasts large 000 itemsets to all the processing nodes and the algorithm enters the next iteration 4.2 Performance evaluation of HPA algorithm The HPA program explained above is implemented on our PC cluster Each node of the cluster has a transaction data file on its own hard disk Transaction data is produced using data generation program developed by Agrawal designating some parameters such as the number of transaction the number of different items and so on The produced data is divided by the number of nodes and copied to each node\220s hard disk The parameters used in the evaluation is as follows The number of transaction is 5,000,000 the number of different items is 5000 and minimum support is 0.7 The size of the data is about 400MBytes in total The message block size is set to be 16KBytes according to the results of communication characteristics of PC clusters discussed in previous section The disk I/O block size is 64KBytes which seems to be most suitable value for the system Note that the number of candidate itemset in pass 2 s substantially larger than for the other passes which relatively frequently occurs in association rules mining Therefore we have been careful to parallelize the program effectively especially in pass 2 so that unnecessary itemsets to count should not be generated 14 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


The execution time of the HPA program pass 2 is shown in figure 13 as the number of PCs is changed The maximum number of PCs used in this evaluation is 100 Reasonably good speedup is achieved in this application as the number of PCs is increased 5 Conclusion In this paper we presented performance evaluation of parallel database processing on an ATM connected 100 node PC cluster system The latest PCs enabled us to obtain over 110Mbps throughput in point-to-point communication on a 155Mbps ATM network even with the so-called 217\217heavy\220\220 TCP/IP This greatly helped in developing the system in a short period since we were absorbed in fixing many other problems Massively parallel computers now tend to be used in business applications as well as the conventional scientific computation Two major business applications decision support query processing and data mining were picked up and executed on the PC cluster The query processing environment was built using the results of our previous research the super database computer SDC project Performace evaluation results with a query of the standard TPC-D benchmark showed that our system achieved superior performance especially when transposed file organization was employed As for data mining we developed a parallel algorithm for mining association rules and implemented it on the PC cluster By utilizing aggregate memory of the system efficiently the system showed good speedup characteristics as the number of nodes increased The good price/performance ratio makes PC clusters very attractive and promising for parallel database processing applications All these facts support the effectiveness of the commodity PC based massively parallel database servers Acknowledgment This project is supported by NEDO New Energy and Industrial Technology Development Organization in Japan Hitachi Ltd technically helped us extensively for ATM related issues References  R Agrawal T Imielinski and A Swami Mining association rules between sets of items in large databases In Proceedings of ACM SIGMOD International Conference on Management of Data  pages 207--216 1993  R Agrawal and R Srikant Fast algorithms for mining association rules In Proceedings of International Conference on Very Large Data Bases  1994  A C Arpaci-Dusseau R H Arpaci-Dusseau D E Culler J M Hellerstein and D A Patterson High-performance sorting on Networks of Workstations In Proceedings of International Conference on Management of Data  pages 243--254 1997  D.S Batory On searching transposed files ACM TODS  4\(4 1979  P.A Boncz W Quak and M.L Kersten Monet and its geographical extensions A novel approach to high performance GIS processing In Proceedings of International Conference on Extending Database Technology  pages 147--166 1996 15 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


 R Carter and J Laroco Commodity clusters Performance comparison between PC\220s and workstations In Proceedings of IEEE International Symposium on High Performance Distributed Computing  pages 292--304 1995  D.J DeWitt and J Gray Parallel database systems  The future of high performance database systems Communications of the ACM  35\(6 1992  J Gray editor The Benchmark Handbook for Database and Transaction Processing Systems  Morgan Kaufmann Publishers 2nd edition 1993  J Heinanen Multiprotocol encapsulation over ATM adaptation layer 5 Technical Report RFC1483 1993  M Kitsuregawa M Nakano and M Takagi Query execution for large relations on Functional Disk System In Proceedings of International Conference on Data Engineering  5th pages 159--167 IEEE 1989  M Kitsuregawa and Y Ogawa Bucket Spreading Parallel Hash:A new parallel hash join method with robustness for data skew in Super Database Computer SDC In Proceedings of International Conference on Very Large Data Bases  16th pages 210--221 1990  M Laubach Classical IP and ARP over ATM Technical Report RFC1577 1994  D.A Schneider and D.J DeWitt Tradeoffs in processing complex join queries via hashing in multiprocessor database machines In Proceedings of International Conference on Very Large Data Bases  16th pages 469--480 1990  T Shintani and M Kitsuregawa Hash based parallel algorithms for mining association rules In Proceedings of IEEE International Conference on Parallel and Distributed Information Systems  pages 19--30 1996  T Sterling D Saverese D.J Becker B Fryxell and K Olson Communication overhead for space science applications on the Beowulf parallel workstaion In Proceedings of International Symposium on High Performance Distributed Computing  pages 23--30 1995  T Tamura M Nakamura M Kitsuregawa and Y Ogawa Implementation and performance evaluation of the parallel relational database server SDC-II In Proceedings of International Conference on Parallel Processing  25th pages I--212--I--221 1996  TPC TPC Benchmark 000\001 D Decision Support Standard Specification Revision 1.1 Transaction Processing Performance Council 1995 16 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


In accordance with 1910.97 and 1910.209 warning signs are required in microwave areas For work involving power line carrier systems this work is to be conducted according to requirements for work on energized lines Comments s APPA objects to the absolute requirement implied by the word ensure regarding exposure to microwave radiation and recommends revision of s l iii to read when an employee works in an area where electromagnetic radiation levels could exceed the levels specified in the radiation protection guide the employer shall institute measures designed to protect employees from accidental exposure to radiation levels greater than those permitted by that guide  I1 an employee must be stationed at the remote end of the rodding operation Before moving an energized cable it must be inspected for defects which might lead to a fault To prevent accidents from working on the wrong cable would require identification of the correct cable when multiple cables are present Would prohibit an employee from working in a manhole with an energized cable with a defect that could lead to a fault However if the cable cannot be deenergized while another cable is out employees may enter the manhole but must protect against failure by some means for example using a ballistics blanket wrapped around cable Requires bonding around opening in metal sheath while working on cable Underaround EIectrical Installations t Comments t This paragraph addresses safety for underground vaults and manholes The following requirements are contained in this section Ladders must be used in manholes and vaults greater than four feet deep and climbing on cables and hangers in these vaults is prohibited Equipment used to lower materials and tools in manholes must be capable of supporting the weight and should be checked for defects before use An employee in a manhole must have an attendant in the immediate vicinity with facilities greater than 250 volts energized An employee working alone is permitted to enter briefly for inspection housekeeping taking readings or similar assuming work could be done safely Duct rods must be inserted in the direction presenting the least hazard to employees and APPA recommends that OSHA rewrite section 7\regarding working with defective cables This rewrite would include the words shall be given a thorough inspection and a determination made as to whether they represent a hazard to personnel or representative of an impending fault As in Subsection \(e EEI proposes the addition of wording to cover training of employees in emergency rescue procedures and for providing and maintaining rescue equipment Substations U This paragraph covers work performed in substations and contains the following requirements Requires that enough space be provided around electrical equipment to allow ready and safe access for operation and maintenance of equipment OSHA's position A2-16 


is that this requirement is sufficiently performance oriented to meet the requirements for old installations according to the 1987 NEW Requires draw-out circuit breakers to be inserted and removed while in the open position and that if the design permits the control circuits be rendered inoperative while breakers are being inserted and removed stated in the Rules and requests that existing installations not be required to be modified to meet NESC APPA recommends that Section u 4 i which includes requirements for enclosing electric conductors and equipment to minimize unauthorized access to such equipment be modified to refer to only those areas which are accessible to the public Requires conductive fences around substations to be grounded Power Generation v Addresses guarding of energized parts  Fences screens, partitions or walls This section provides additional requirements and related work practices for power generating plants  Entrances locked or attended Special Conditions w  Warning signs posted  Live parts greater than 150 volts to be guarded or isolated by location or be insulated  Enclosures are to be according to the 1987 NESC Sections llOA and 124A1 and in 1993 NESC  Requires guarding of live parts except during an operation and maintenance function when guards are removed barriers must be installed to prevent employees in the area from contacting exposed live parts Requires employees who do not work regularly at the substation to report their presence Requires information to be communicated to employees during job briefings in accordance with Section \(c of the Rules Comments U APPA and EEI provide comments as follows Both believe that some older substations \(and power plants would not meet NESC as This paragraph proposes special conditions that are encountered during electric power generation, transmission and distribution work including the following Capacitors  Requires individual units in a rack to be short circuited and the rack grounded  Require lines with capacitors connected to be short circuited before being considered deenergized Current transformer secondaries may not be opened while energized and must be bridged if the CT circuit is opened Series street lighting circuits with open circuit voltages greater than 600 volts must be worked in accordance with Section q\or t and the series loop may be opened only after the source transformer is deenergized and isolated or after the loop is bridged to avoid open circuit condition Sufficient artificial light must be provided where insufficient naturals illumination is present to enable employee to work safely A2-17 


US Coast Guard approved personal floatation devices must be supplied and inspected where employees are engaged in work where there is danger of drowning Required employee protection in public work areas to include the following  Warning signs or flags and other traffic control devices  Barricades for additional protection to employees  Barricades around excavated areas  Warning lights at night prominently displayed Lines or equipment which may be sub to backfeed from cogeneration or other sources are to be worked as energized in accordance with the applicable paragraphs of the Rules Comments w APPA submits the following comments regarding this Special Conditions section Recommends that the wording regarding capacitors be modified to include a waiting period for five minutes prior to short circuiting and grounding in accordance with industry standards for discharging of capacitors For series street light circuits, recommends that language be added for bridging to either install a bypass conductor or by placement of grounds so that work occurs between the grounds Recommends modification of the section regarding personal floatation devices to not apply to work sites near fountains decorative ponds swimming pools or other bodies of water on residential and commercial property Definitions x This section of the proposed Rules includes definitions of terms Definitions particularly pertinent to understanding the proposal and which have not previously been included are listed as follows Authorized Employee  an employee to whom the authority and responsibility to perform a specific assignment has been given by the employer who can demonstrate by experience or training the ability to recognize potentially hazardous energy and its potential impact on the work place conditions and who has the knowledge to implement adequate methods and means for the control and isolation of such energy CZearance for Work  Authorization to perform specified work or permission to enter a restricted area Clearance from Hazard  Separation from energized lines or equipment Comments x The following summarizes the changes in some of the definitions which APPA recommends Add to the definition for authorized employee It the authorized employee may be an employee assigned to perform the work or assigned to provide the energy control and isolation function  Recommends that OSHA modify the definition for a line clearance tree trimmer to add the word qualified resulting in the complete designation as a qualified line clearance tree trimmer Recommends that OSHA modify the definition of qualified employee" to remove the word construction from the definition since it is felt that knowledge of construction procedures is beyond the scope of the proposed rule resulting in APPA's new A2-18 I 


wording as follows more knowledgeable in operation and hazards associated with electric power generation transmission and/or distribution equipment Recommends that OSHA add a definition for the word practicable and replace the word feasible with practicable wherever it appears in the proposed regulations and that practicable be further defined as capable of being accomplished by reasonably available and economic means OTHER ISSUES Clothing OSHA requested comments on the advisability of adopting requirements regarding the clothing worn by electric utility industry employees EEI has presented comments which indicates research is underway prior to establishing a standard for clothing to be worn by electric utility employees However EEI's position is that this standard has not developed to the extent that it could be included in the OSHA Rules Both APPA and EEI state that they would support a requirement that employers train employees regarding the proper type of clothing to wear to minimize hazards when working in the vicinity of exposed energized facilities Grandfathering Due to the anticipated cost impact on the utility industry of the proposed Rules requiring that existing installations be brought to the requirements of the proposed Rules both APPA and EEI propose that the final Rules include an omnibus grandfather provision This provision would exempt those selected types of facilities from modification to meet the new rules EEI states that if the grandfathering concept is incorporated that electric utility employees will not be deprived of proper protection They propose that employers be required to provide employees with a level of protection equivalent to that which the standard would require in those instances in which the utility does not choose to modify existing facilities to comply with the final standard Rubber Sleeves OSHA requests comments from the industry on whether it would be advisable to require rubber insulating sleeves when gloves are used on lines or equipment energized at more than a given voltage EEI states its position that utilities should continue to have the option of choosing rubber gloves or gloves and sleeves to protect employees when it is necessary to work closer to energized lines than the distances specified in the clearance tables Preemuting State Laws EEI requests that the final Rules be clear in their preempting state rules applicable to the operation and maintenance work rules for electric power systems. This is especially critical since some states now have existing laws which are more stringent than the proposed OSHA Rules Examples are 1 in California and Pennsylvania where electric utility linemen are prohibited from using rubber gloves to work on lines and equipment energized at more than certain voltages and 2 in California and Connecticut where the live line bare hand method of working on high voltage transmission systems is prohibited One utility Pacific Gas  Electric has obtained a variance from the California OSHA to perform live line bare-hand transmission maintenance work on an experimental basis Coiiflicts Between the Rilles and Part 1926 Subpart V Since many of the work procedures in construction work and operation and maintenance work are similar and difficult to distinguish between EEI requests that the final order be clear in establishing which rule has jurisdiction over such similar work areas A2-19 v 


IMPACTS ON COSTS AND ASSOCIATED BENEFITS In its introduction to the proposed rules OSHA has provided an estimate of the annual cost impact on the electric utility industry for the proposed des of approximately 20.7 million OSHA estimates that compliance with this proposed standard would annually prevent between 24 and 28 fatalities and 2,175 injuries per year The utilities which have responded to this proposed standard through their respective associations have questioned the claims both of the magnitude of the cost involved and the benefit to the industry in preventing fatalities and lost-time injuries Both EEI and APPA feel that the annual cost which OSHA estimates are significantly lower than would be realized in practice Factors which APPA and EEI feel were not properly addressed include the following OSHA has not accurately accounted for cost of potential retroactive impacts including retrofitting and modifying existing installations and equipment OSHA has not consistently implemented performance based provisions in proposed rules  many portions require specific approaches which would require utilities to replace procedures already in place with new procedures Estimates were based on an average size investor-owned utility of 2,800 employees and an average rural cooperative of 56 employees, which are not applicable to many smaller systems such as municipal systems OSHA has not adequately addressed the retraining which would be necessary with modifying long-established industry practices to be in accordance with the OSHA rules EEI claims that OSHA's proposed clearance requirements would not allow the use of established maintenance techniques for maintaining high voltage transmission systems and thus would require new techniques For an example of the cost which is estimated to be experienced as a result of the new Rules one of the EEI member companies has estimated that approximately 20,000 transmission towers would need to be modified to accommodate the required step bolts in the Rules at an estimated cost of 6,200,000 Additionally this same company estimates that the annual cost of retesting live line tools for its estimated 1,000 tools would be 265,000 Additionally, both EEI and APPA question the additional benefits which OSHA claims would result from implementation of the new Rules APPA questions the estimates of preventing an additional 24 to 28 fatalities annually and 2,175 injuries per year in that it fails to account for the fact that the industry has already implemented in large part safety measures which are incorporated in the Rules EEI and APPA also point out that many preventable injuries cannot be eliminated despite work rules enforcement and safety awareness campaigns since many such accidents which result in fatalities are due to employee being trained but not following the employer's training and policies PRESENT STATUS OF RULES According to information received from the OSHA office in February 1993 the final Rules are to be published no later than July 1993 and possibly as soon as March 1993 OSHA closed their receipt of comments in March 1991 and no further changes in the rules are thought possible A2-20 


CONCLUSION The OSHA 1910.269 which proposes to cover electric utility operation and maintenance work rules affects a multitude of working procedures as are summarized in this paper It is not possible at the present time to assess the final structure of the Rules as may be proposed in 1993 or subsequent years Since the comments from the utility associations APPA and EEI were made following the initial release of the proposed OSHA Rules in 1989 a significant amount of time has elapsed where other events have occurred which may affect the form of the final Rules The 1993 NESC went into effect in August 1992 and includes some of the requirements to which the commenters objected For example a significant requirement in the Part 4 of the 1993 NESC requires that rubber gloves be utilized on exposed energized parts of facilities operating at 50 to 300 volts This requirement is in conflict with EEl\222s proposed change to the OSHA Rules which would still allow working such secondary facilities without the use of rubber gloves Electric utilities are advised to review the January 31 1989 proposed operation and maintenance Rules as summarized in this paper and to review their procedures which would be affected by application of the Rules Many of the procedures proposed in the Rules provide valuable guidance in electric utilities\222 operation and maintenance activities Where the cost impact is not significant, it is recommended that utilities consider implementing such procedures in expectation of the Rules being published in the next few months Also it would be appropriate for electric utilities to review the 1993 edition of the NESC since there are portions of the Rules which have resulted in changes in the NESC These changes mainly occur in Part 4 Rules for the Operation of Electric Supply and Communications Lines and Equipment The concerns which the commenters have addressed regarding the cost impact and the resulting benefits experienced as a result of the promulgation of the Rules are real ones and must be addressed in the final Rules As a result this paper cannot present a conclusion regarding the full impact of the Rules The development of such Rules continue to be an ongoing matter and will undoubtedly require later analysis when the final rules are published A2-21 


