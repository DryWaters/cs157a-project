Adapting classification rule induction to subgroup discovery Nada LavraE Peter Flach Branko KavSek LjupEo Todorovski Joief Stefan Institute University of Bristol Joief Stefan Institute Ljubljana Slovenia Bristol UK Ljubljana Slovenia Nada.Lavrac@ijs.si Peter.Flach@bristol.ac.uk Branko.Kavsek@ijs.si Ljupco.Todorovski@ijs.si Abstract Rule learning is typically used for solving class$carion and prediction tasks However learning of classijcarion rules can be adapred also IO subgroup discovery This 
pa per shows how this can be achieved by modibing rhe cover ing algorirhm and the search heurisric, performing pmba bilisric class9carion of instances, and using an appropriare measure for evaluating rhe results of subgmup discovery Experimenral evaluarion of rhe CN2-SD subgmup discov ery algorithm on I7 UCI dara sers demonstrates subsranrial reducrion of rhe number of induced rules increased mle coverage and rule significance as well as 
slight impmve menrs in rerms of rhe area under rhe ROC curve 1 Introduction Classical rule learning algorithms were designed to con StNCI classification and prediction rules 16,4,5 In addi tion to this area of machine learning, referred to as predic rive induction developments in descriprive inducrion have recently gained much attention These involve mining of association rules e.g the APRIORI association rule learn ing algorithm I subgroup discovery e.g the MIDOS subgroup discovery systems 241 
and other approaches to non-classificatory induction The methodology presented in this paper can be applied to subgroup discovery As in the MIDOS approach, a sub group discovery task can be defined as follows: given a pop ulation of individuals and a property of those individuals we are interested in find population subgroups that are statis tically 221most interesting\222 e.g are as large as possible and have the most unusual statistical \(distributional\character istics with respect to the property of interest This paper investigates how to adapt classical classifi cation 
rule learning approaches to subgroup discovery by appropriately modifying the heuristics and the covering al gorithm for learning sets of rules The proposed modifi 0-7695-1754-4/02 17.00 Q 2002 IEEE cations of classification rule learners can in principle be used on top of any rule learner using the covering approach for rule set construction In this work we show an upgrade of the well-known CNZ rule learning algorithm 4 31 Al ternatively we could have upgraded RL 15 RIPPER 5 SLIPPER 
6 or other more sophisticated classification rule learners The reason for upgrading CNZ is that other more sophisticated learners include modifications that make them more effective in classification tasks, improving their classi fication accuracy Improved classification accuracy is, how ever, not of ultimate interest for subgroup discovery whose main goal is to find interesting population subgroups We have implemented the new subgroup discovery al gorithm CNZ-SD in Java and incorporated it in the WEKA data mining environment 23 The proposed approach per forms subgroup discovery through the following modifica tions of the classical 
rule learning algorithm CN2 a in corporating example weights into the covering algorithm b\incorporating example weights into the weighted rela tive accuracy search heuristic c probabilistic classifica tion based on the class distribution of covered examples by individual rules, both in the case of unordered rule sets and ordered decision lists and d area under the ROC curve rule set evaluation This paper presents the CNZ-SD subgroup discovery al gorithm together with its experimental evaluation in se lected domains of the UCI Repository of Machine Learning Databases 171 The 
experimental comparison with CN2 demonstrates that the subgroup discovery algorithm CN2 SD produces substantially smaller rule sets, where individ ual rules have higher coverage and significance These three factors are important for subgroup discovery smaller size enables better understanding, higher coverage means larger support and higher significance means that rules describe discovered subgroups that are significantly different from the entire population The appropriateness for subgroup dis covery is confirmed also by slight improvements in terms of the area under the ROC curve This paper is organized as follows In Section 2 the 266 


background for this work is explained the standard CN2 rule induction algorithm, including the covering algorithm the weighted relative accuracy heuristic probabilistic clas sification and rule evaluation in the ROC space Sec tion 3 presents the modified CN2 algorithm, called CN2 SD adapting CN2 for subgroup discovery Section 4 presents the experimental evaluation on selected UCI do mains Some links to the related work are given in Sec tion 5 Section 6 concludes by summarizing the results and presenting plans for further work 2 Background This section presents the backgrounds of our work the classical CN2 rule induction algorithm including the cov ering algorithm for rule set construction, the standard CN2 heuristics the weighted relative accuracy heuristic prob abilistic classification and the basics of rule evaluation in ROC space 2.1 The CN2 rule induction algorithm CN2 is an algorithm for inducing propositional clas sification rules 4 31 Induced rules have the form if Cond then Class where Cond is a conjunction of fea tures attribute values In this paper we use the notation Class c Cad CN2 consists of two main procedures the search pro cedure that performs beam search in order to find a single rule and the control procedure that repeatedly executes the search The search procedure performs beam search us ing classification accuracy of the Nk as a heuristic func tion The accuracy of the propositional classification rule Class  Cmd is equal to the conditional probability of class Class given that the condition Cond is satisfied Acc\(Class c Cond  p\(Class\(Cond Different pmb ability estimates like the Laplace 3 or the m-estimate Z 71 can be used in CN2 for estimating the above prob ability The standard CN2 algorithm used in this work uses the Laplace estimate CN2 can apply a significance test to an induced rule A rule is considered to be significant if it expresses a regularity unlikely to have occurred by chance To test significance CN2 uses the likelihood ratio statistic 4 that measures the difference between the class probability distribution in the set of examples covered by the rule and the class probabil ity distribution in the set of all training examples Empiri cal evaluation in 31 shows that applying a significance test reduces the number of induced rules at a cost of slightly reduced predictive accuracy Two different control procedures are used in CN2 one for inducing an ordered list of rules and the other for the un ordered case In both procedures a default rule providing for majority class assignment is added as the final rule in an induced rule set When inducing an ordered list of rules, the search proce dure lwks for the best rule according to the heuristic mea sure in the current set of training examples The rule pre dicts the most frequent class in the set of examples. covered by the induced rule Before starting another search itera tion all examples covered by the induced rule are removed The control procedure invokes a new search until all the examples are covered In the unordered case the control procedure is iterated inducing rules for each class in turn For each induced rule only covered examples belonging to that class are removed instead of removing all covered examples like in the or dered case The negative training examples i.e examples that belong to other classes remain and positives are re moved in order to prevent CN2 finding the same rule again 2.2 The weighted relative accuracy heuristic Weighted relative accuracy is a variant of rule accuracy that can be meaningfully applied both in the descriptive and predictive induction framework in this paper we apply this heuristic for subgroup discovery We use the following notation Let n\(Cad stand for the number of instances covered by a rule Class  Cond n\(C1ass stand for the number of examples of class Class and n\(Class.Cond stand forthe number of correctly clas sified examples true positives We use p\(Class.Cond etc for the corresponding probabilities In our work the Laplace estimate is used for probability estimation Rule accuracy can be expressed as Acc\(C1ass t Cond  p\(Class\(Cond  w Weighted relative accu racy 14,221 a reformulation of one of the heuristics used in EXPLORA lo and MlDOS 241 is defined as follows WRAcc\(C1ass t Cond  p\(Cond p\(ClasslCond  p\(Class I Like most of the other heuristics used in subgroup dis covery systems weighted relative accuracy consists of two components providing a tradeoff between rule generality or the size of a group p\(Cond and distributional unusu alness \(or relative accuracy, i.e the difference between rule accuracy p\(Class1Cond and default accuracy p\(C1ass This difference can also be seen as the accuracy gain rela tive to the fixed rule Class c true The latter rule pre dicts all instances to satisfy Class a rule is only interesting if it improves upon this 221default\222 accuracy Another way of viewing relative accuracy is that it measures the util ity of connecting rule body Cond with a given rule head Class However it is easy to obtain high relative accu racy with highly specific rules i.e rules with low general ity p\(Cond To this end, generality is used as a 221weight\222 267 


so that weighted relative accuracy trades off generality of the rule p\(Cond i.e rule coverage\and relative accuracy Io IO these quantities are referred to as g generality p rule accuracy\and po default accuracy and the func tion g\(p  po\is investigated in the so-called p-g-space Klosgen also investigates other tradeoffs reducing the in fluence of generality e.g a\(p  Po or fi\(p  Po Here we favor weighted relative accuracy because it has an intuitive interpretation in the ROC space see Section 2.4 2.3 Probabilistic classification p\(ClasslCond  p\(Class The induced rules can be ordered or unordered. Ordered rules are interpreted as a decision list I91 in a straight forward manner when classifying a new example, the rules are sequentially tried and the first rule that covers the exam ple is used for prediction In the case of unordered rule sets, the distribution of cov ered training examples among classes is attached to each rule Rules of the form if Cond then Class ClassDistributim are induced, where numbers in the ClassDistribution list denote for each individual class how many training exam ples of this class are covered by the rule When classify ing a new example all rules are tried and those covering the example are collected If a clash occurs several rules with different class predictions cover the example a vot ing mechanism is used to obtain the final prediction the class distributions attached to the rules are summed to de termine the most probable class If no rule fires a default rule is invoked which predicts the majority class of uncov ered training instances 2.4 ROC analysis for subgroup discovery A point on the ROC curve ROC Rcceiver Operating Characteristic IS shows classifier performance in terms of false alm orfalse positive rare FPT   plot ted on the X-axis that needs to be minimized, and sensi tivity or true posirive rote TPT   plotted on the Y-axis that needs to be maximized In the ROC space an appropriate tradeoff, determined by the expert can be achieved by applying different algorithms as well as by different parameter settings of a selected data mining algo rithm or by taking into the account different misclassifica tion costs The ROC space is appropriate for measuring the suc cess of subgroup discovery since rules/subgroups whose TPTIFPT tradeoff is close to the diagonal can be discarded as insignificant Conversely, significant Nles/subgroups are those sufficiently distant from the diagonal The signifi cant rules define the points in the ROC space from which a convex hull is constructed The area under the ROC curve AUO can he used as a quality measure for comparing the success of different learners or subgroup miners Weighted relative accuracy is appropriate to measure the quality of a single subgroup because it is proportional to the distance from the diagonal in the ROC space To see this, note first that rule accuracy p\(Class1Cond is propor tional to the angle between the X-axis and the line connect ing the origin with the point depicting the rule's TPTIFPT tradeoff So for instance, the X-axis has always rule ac curacy 0 these are purely negative subgroups\the Y-axis has always rule accuracy 1 purely positive subgroups and the diagonal represents subgroups with rule accuracy p\(Class the prior probability of the positive class Relative accuracy re-normalizes this such that all points on the diagonal have relative accuracy 0 all points on the Y axis have relative accuracy 1  C~RSS  p\('lass the prior probability of the negative class\and all points on the X-axis have relative accuracy p\(Class Notice that all points on the diagonal also have WRAcc  0 In terms of subgroup discovery the diagonal represents all subgroups with the same target distribution as the whole population only the generality of these average subgroups increases when moving from left to right along the diagonal This in terpretation is slightly different in classifier learning, where the diagonal represents random classifiers that can he con structed without any training More generally one can show that points with the same WRAcc value lie on straight lines parallel to the diagonal In particular, a point on the line TPT  FPT  a 1 I a 5 1 has WRAcc  ap\(Class Class Thus given a fixed class distribution WRAccis proportional lo the veni cal or horizontal\distance a between the line parallel to the diagonal on which the point lies and the diagonal In fact the quantity TPT  FPT would he an alternative quality measure for subgroups with the additional advantage that we can use it to compare subgroups from populations with different class distributions However in this paper we are only concerned with comparing subgroups from the same population, and we prefer WRAcc because it is a familiar measure in subgroup discovery 3 The CN2-SD subgroup discovery algorithm The main modifications of the CN2 algorithm making it appropriate for subgroup discovery involve the imple mentation of the weighted covering algorithm incorpora tion of example weights into the weighted relative accuracy heuristic probabilistic classification also in the case of the  Any of hose subgroups may be the best according to some expen defined optrating conditions 268 


222ordered\222 induction algorithm. and the area under the ROC curve rule set evaluation 3.1 Weighted covering algorithm Subgroup discovery is in general, aimed at discovering interesting properties of subgroups of the entire population If used for subgroup discovery the problem of standard rule learners like CNZ and RIPPER is in the use ofthe covering algorithm for rule set construction The main deficiency of the covering algorithm is that only the first few induced rules may be of interest as subgroup descriptors with suf ficient coverage and significance Subsequently induced rules are induced from biased example subsets i.e subsets including only positive examples not covered by previously induced rules which inappropriately biases the subgroup discovery process As a remedy to this problem we propose to use the weighted covering algorithm in which the subsequently induced rules allow for discovering interesting subgroup properties of the entire population The weighted covering algorithm modifies the classical covering algorithm in such a way that covered positive examples are not deleted from the current training set. Instead in each run of the covering loop the algorithm stores with each example a count that shows how many times with how many rules induced so far\the example has been covered so far Weights derived from these example counts then appear in the computation of WRAcc Initial weights of all positive examples ej equal w\(ej,O  1 We have implemented two approaches a Multiplicative weights In the first approach weights decrease multiplicatively For a given parameter y  1 weights of covered positive examples decrease as follows w\(ej,i  7\221 where w\(ej,i is the weight of ex ample ej being covered i times Note that the weighted cov ering algorithm with y  1 would result in finding the same rule over and over again, whereas with y  0 the algorithm would perform the same as the standard CN2 algorithm b Additive weights In the second approach, weights of covered positive examples decrease according to the for mulaw\(ej,i   3.2 Modified WRAcc heuristic with example weights The modification of CNZ reported in ZZ affected only the heuristic function weighted relative accuracy was used as search heuristic instead of the accuracy heuristic of the original CN2 while everything else remained the same In this work the heuristic function was further modified to en able handling example weights which provide the means to consider different pans of the instance space in each itera tion of the weighted covering algorithm In the WRAcc computation \(Equation I all probabilities are computed by relative frequencies An example weight measures how important it is to cover this example in the next iteration The initial example weight w\(ej,O  1 means that the example hasn\222t been covered by any rule meaning \221please cover this example it hasn\222t been covered before\222 while lower weights mean 221don\222t try too hard on this example\222 The modified WRAcc measure is then de fined as follows n\222\(Cond n\222\(C1.Cond n\221\(C1 WRAcc\(C1c Cond   ___ N\222  n\222\(Cmdl N\221  222 2 In this equation N\222 is the sum of the weights of all ex amples n\222\(Cond is the sum of the weights of all covered examples, and n\222\(C1.Cond is the sum of the weights of all correctly covered examples To add a rule to the generated rule set, the rule with the maximum WRAcc measure is chosen out of those rules in the search space which are not yet present in the rule set produced so far all rules in the final rule set are thus dis tinct, without duplicates 3.3 Probabilistic classification Each CN2 rule returns a class distribution in terms of the numbers of examples covered as distributed over classes The CN2 algorithm uses class distribution in classifying un seen instances only in the case of unordered rule sets, where rules are induced separately for each class. In the case of or dered decision lists the first rule that fires provides the clas sification In our modified CNZ-SD algorithm, also in the ordered case all applicable rules are taken into the account hence the same probabilistic classification is used in both classifiers This means that the terminology 221ordered\221 and 221unordered\222 which in CN2 distinguished between decision list and rule set induction has a different meaning in our setting: the 221unordered\222 algorithm refers to learning classes one by one while the 222ordered\222 algorithm refers to finding best rule conditions and assigning the majority class in the head 3.4 Area under the ROC curve evaluation In subgroup discovery there are two ways in which a rule learner can give rise to a ROC curve a AUC-Method-1 The first method treats each rule as a separate subgroup which is plotted in the ROC space with its true and false positive rates We then calculate the convex hull of this set of points selecting the subgroups which per form optimally under a particular range of operating char acteristics The area under this ROC convex hull AUC indicates the combined quality of the optimal subgroups in the sense that it does evaluate whether a particular subgroup 269 


has anything to add in the context of all the other subgroups However the method does not take account of any overlap between subgroups. and subgroups not on the convex hull are simply ignored \(the existence of many such subgroups may indicate overfitting as we illustrate in Section 4 Note that CN2-SD learns rules both for the positive and negative target class rules for the positive target class rep resent subgroups with a higher proponion of positives than average and rules for the negative target class represent subgroups with a lower than average proportion of posi tives Consequently this method constructs two convex hulls one above the diagonal and one below see Figure I in Section 4 b AUC-Method-2 The second method employs the combined probabilistic classifications of all subgroups as indicated below If we always choose the most likely pre dicted class this corresponds to setting a fixed threshold 0.5 on the positive probability if the positive probability is larger than this threshold we predict positive else negative A ROC curve can be constructed by varying this threshold from I all predictions negative, corresponding to 0,O in the ROC space to 0 all predictions positive correspond ing to 1.1 in the ROC space This results inn  1 points in the ROC space, where n is the total number of classified examples Equivalently we can order all the examples by decreasing the predicted probability of being positive and tracing the ROC curve by starting in O,O stepping up when the example is actually positive and stepping to the right when it is negative until we reach l,l The area under this ROC curve indicates the combined quality of all sub groups i.e the quality of the entire rules set This method can be used with a test set or in cross-validation but the resulting curve is not necessarily convex A detailed de scription of this method applied to decision tree induction can be found in Ill Which of the two methods is more appropriate for sub group discovery is open for debate The second method seems more appropriate if the discovered subgroups are also used for classification, while the first seems more appropri ate for the selection and evaluation of a subset of potentially optimal individual subgroups One advantage of the second method is that it is easier to apply cross-validation. A disad vantage of the first method is that it ignores redundant sub groups which may indicate overfitting In the experimental evaluation in Section 4 we used AUC-Method-2 while the ROC convex hull obtained by AUC-Method-l is only illus trated in one domain in Figure 1 In Lhe cse of ties we make he appmpnate number of steps up and to the fight at once drawing a diagonal line segment 4 Experimental evaluation For subgroup discovery expen's evaluation of results is of ultimate interest Nevertheless before applying the proposed approach to a particular problem of interest we wanted to verify our claims that the mechanisms imple mented in the CN2-SD algorithm are indeed appropriate for subgroup discovery We experimentally evaluated our approach on 17 data sets from the UCI Repository of Machine Learning Databases 171 In Table 1 the selected data sets are sum marized in terms of the number of attributes, the number of examples and the percentage of examples of the ma jority class These data sets have been widely used in other comparative studies Since currently our lava re implementation of CN2 in WEKA does not support con tinuous attributes and can not handle missing values all continuous attributes were discretized and data sets that contain no missing values were chosen The discretiza tion described in SI was performed using the WEKA tool 1231 Moreover in our experiments all of the data sets have two classes, either originally or by selecting one class as positive and joining all the other in the negative class \(in Table I the selected positive class is indicated by ClassName this was done for the purpose of enabling he area under the ROC curve evaluation Table 1 Data set characteristics 8 768 65.1 7 Glar~\(hild~uindnon-floa 9 214 35.51 8 HCarMWI I3 270 55.56 9 lonosphire 34 351 I 11 Lyrnphlmelasfaus 18 148 54.72 12 Scrment{hns!duel 19 2310 14.29 10 Itis{ltir-setosl 4Iso 33.33 13 Soh  60 208 53.36 14 lic-lac-LoE 9 958 6594 I5 Vohicle\(hun 18 846 25.77 13 178 39.89 16 Winell 17 k[mammal 17 101 40.59 The comparison of CN2-SD with CN2 was performed in 17 UCI domains with AUC-Method-2 evaluation based on IO-fold stratified cross validation Table 2 compares the CNZ-SD subgroup discovery algorithm with the stan dard CN2 algorithm CN2-sfundard described in 3 and the CN2 algorithm using WRAcc CNZ-WRAcc described in in terms of area under the ROC curve AUC All these variants of the CN2 algorithm were first re implemented in the WEKA data mining environment 23 270 


because the use of the same system makes the comparisons more impartial Due to space restrictions we only include the results of the unordered algorithms The results of the CN2-SD algorithm were computed using both the multi plicative weights with y  0.5 0.7 0.9 and the additive weights Results withy  0.7 are not listed as they are al ways between those of 7  0.5 and y  0.9 as expected All other parameters of the CN2 algorithm were set to their de fault values bean-size  5 significance-threshold  99 Table 2 Area under the ROC curve with stan dard deviation AUC  sd for different vari ants of the unordered algorithm using 10-fold stratified cross-validation 3.5 and on CN2-WRAcc with a factor of 2 Note however that rules obtained with additive weights and multiplicative weights with high y are highly overlapping due to the rela tively modest decrease of example weights In addition there is also a substantial increase in the av erage likelihood ratio while the ratios achieved by CNZ standard are already significant at the 99 level this is fur ther pushed up by CNZ-SD with maximum values achieved by additive weights An interesting question to be verified with further experiments is whether the weighted versions of the CN2 algorithm improve the significance of the in duced subgroups also in the case when.CN2 rules are in duced without applying the significance test In summary CNZ-SD produces substantially smaller rule sets, where individual rules have higher coverage and sig nificance Table 3 Average size 9 coverage CVC and llkelihood ratio LHR of rules for different versions of the unordered algorithm induced from the entire data sets We also compared the sizes of the rule sets, average rule coverage, and the likelihd ratio of rules computed from the entire data sets not using cross-validation Table 3 compares CNZ-SD with CN2-standard and CN2-WRAcc in terms of the size of the rule set S is the number of rules in a rule set, including the default rule\average rule coverage CVG is computed as the averaged percentage of covered positive and negative examples per rule and likelihood ra tio\222 per rule The experimental results show that CN2-SD achieves im provements across the board In terms of AUC, the smallest improvement is achieved by additive weights and slightly better improvements of 34 are by multiplicative weights On the other hand additive weights result in ahout 2 times less rules on average than multiplicative weights and 6.5 times less rules than CNZ-standard Average rule coverage is also optimal for additive weights, improving on the av erage the coverage of CN2-standard rules with a factor of 222The likelihood ralio is used in CNZ for testing the significance of Ihs induced tule 141 For Iwo-class problems this stalislic is distributed ap prorimalely as xz wilh one degree of freedom Finally we illustrate our approach in the ROC space by means of the results on the Australian data set Fig ure 1 The solid lines in this graph indicate the ROC curves obtained by CN2-SD and CN2-standard evaluated with AUC-Method-2 i.e probabilistic classification with overlapping tules the top line \(squares for CN2-SD with additive weights, and the bottom line \(triangles for CNZ standard CN2-standard finds many more rules than CN2 SD which leads to overfitting as the ROC curve is mostly below the diagonal For illustrative purposes we also include positive and negative convex hulls constructed from individual sub groups using AUC-Method-l dotted lines The points on the X and Y-axes close to the origin are all small purely positive and negative subgroups found by CN2-standard that do not contribute to the convex hull \(presumably these are the rules that lead to poor performance using probabilis tic classification Using AUC-Method-l we can remove 271 


those overly specific subgroups leading to reasonable posi tive and negative convex hulls Notice, however that CNZ SD still improves on CN2-standard after removing redun dant subgroups Figure 1 Example ROC curves on the Aus tralian data set: solid curvesfor AUC-Method 2 and dotted positive and negative convex hulls for AUC-Method-1 squares for CNZ-SD with additive weights and triangles for CN2 standard 5 Related work Various rule evaluation measures and heuristics have been studied for subgroup discovery IO 241 aimed at bal ancing the size of a group referred to as factor g with its distributional unusualness \(referred to as factor p The properties of functions that combine these two factors have been extensively studied the so-called 221p-g-space\221 lo An alternative measure q   was proposed in 13 for expert-guided subgroup discovery in the TPIFP space aimed at minimizing the number of false positives FP and maximizing true positives TP guided by generalization pa rameter par Besides such 221objective\222 measures of interest ingness some 221subjective\222 measure of interestingness of a discovered pattern can be taken into the account such as ac tionability 221a pattern is interesting if the user can do some thing with it to his or her advantage\222 and unexpectedness 221a pattern is interesting to the user if it is surprising to the user\222 211 Instance weights play an important role in boosting I21 and alternating decision trees 20 Instance weights have been used also in variants of the covering algorithm imple mented in rule learning approaches such as SLIPPER 6 RL 1151 and DAIRY 191 A variant of the weighted cover ing algorithm has been used also in the context of subgroup discovery for rule subset selection 13 6 Conclusions We have presented a novel approach to adapting stan dard classification rule learning to subgroup discovery To this end we have appropriately adapted the covering algo rithm the search heuristics, the probabilistic classification and the performance measure Experimental results on 17 UCI data sets demonstrate that CNZ-SD produces substan tially smaller rule sets where individual rules have higher coverage and significance These three factors are important for subgroup discovery smaller size enables better under standing. higher coverage means larger support, and higher significance means that rules describe discovered subgroups that are significantly different from the entire population We have evaluated the results of CN2-SD also in terms of AUC-Method-2 and shown insignificant increase in terms of the area under the ROC curve In further work we will evaluate the results also by using AUC-Method-1 where each subgroup establishes a sepa rate point in the ROC space and compare the results with the MIDOS subgroup discovery algorithm We plan to in vestigate the behavior of CN2-SD also in multi-class prob lems An interesting question to he verified with further ex periments, is whether the weighted versions of the CN2 al gorithm improve the significance of the induced subgroups also in the case when CN2 rules are induced without ap plying the significance test Finally we plan to use the CNZ-SD subgroup discovery algorithm for solving practical problems in which expert evaluations of induced subgroup descriptions is of ultimate interest Acknowledgements Thanks to Dragan Gamberger for joint work on the weighted covering algorithm and JosC Hernbdez-Orallo and Cesar Ferr-Ramirez for joint work on AUC The work reported in this paper was supported by the Slovenian Min istry of Education Science and Sport the IST-1999-11495 project Data Mining and Decision Support for Business Competitiveness A European Virtual Enterprise and the British Council project Partnership in Science PSP-IS Refer en c e s l R Agrawal H Mannila R Srikant H Toivonen, and A.I Verkamo Fast discovery of association rules In U.M Fayyad G Piatetski-Shapiro P Smyth and R Uthurusamy editors, Advances in Knowledge Discov ery and Data Mining 307-328 AAA1 Press 1996 2 B Cestnik Estimating probabilities: A crucial task in machine learning In L Aiello editor Proc of rhe 9rh European Conference on Artificial Inrelligence 147 149. Pitman 1990 272 


3 P Clark and R. Boswell Rule induction with CNZ Some recent improvements. In Y Kodratoff editor Pmc of rhe 5rh European Working Session on Learn ing 151-163 Springer, 1991 141 P Clark and T Niblett The CNZ induction algorithm Machine Learning 3\(4 261-283 1989 5 W.W Cohen 1995 Fast effective rule induction. In Pmc of rhe 12th Intermrional Conference on Ma chine Learning 115-123 Morgan Kaufmann, 1995 161 W.W Cohen and Y Singer A simple fast and ef fective rule learner In Pmc of AAAI/lAAI 335 342. American Association for Artificial Intelligence 1999 171 S Dieroski B. Cestnik, and 1 Petrovski. \(1993 Us ing the m-estimate in rule induction Journal of Compuring andlnformarion Technology 1\(1 46 1993 8 U.M Fayyad and K.B Irani K.B Multi-interval dis cretisation of continuous-valued attributes for classi fication learning In R Bajcsy editor Pmc of the 13th Inrernarioml Joint Conference on Artificial In telligence 1022-1027 Morgan Kaufmann 1993 19 D Hsu 0 Etzioni and S Soderland A redundant cov ering algorithm applied to text classification In Pmc of the AAA1 Workshop on Learning from Tar Cate gorization American Association for Artificial Intel ligence, 1998 IO W Klosgen. Explora A multipattern and multistrat egy discovery assistant In U.M Fayyad G Piatetski Shapiro P Smyth and R Uthurusamy editors Ad vances in Knowledge Discovery and Data Mining 249-271 MITPress 1996 1111 C Fem-Ramirez P.A Flach and 1 Hemandez Orallo. Learning decision trees using the area under the ROC curve In Pmc of the 19th Internarional Conference on Machine Learning 139-146 Morgan Kaufmann 2002 I21 Y Freund and R.E Shapire. Experiments with a new boosting algorithm In Pmc of the 13th International Conference on Machine Learning 148-156 Morgan Kaufmann 1996 13 D Gamberger and N LavraE Descriptive induction through subgroup discovery A case study in a medi cal domain In Pmc of the 19th Internarional Confer ence on Machine Learning 163-170 Morgan Kauf mann 2002 I41 N Lavraf P Flach and B Zupan Rule evaluation measures A unifying view In Pmc of the 9th Inter national Workshop on Inducrive Logic Pmgramming 74-185 Springer 1999 I51 Y Lee B.G. Buchanan, and J.M. Aronis Knowledge based learning in exploratory science Learning rules to predict rodent carcinogenicity Machine Learning 30 217-240 1998 I61 R.S Michalski 1 Mozeti I Hong and N LavraE The multi-purpose incremental learning system AQl5 and its testing application on three medical domains In Pmc 5th National Conference on Artificial Inrelli gence 104-1045 Morgan Kaufmann, 1986 I71 P.M Murphy and D.W Aha UCI repos itory of chine learning databases http://www.ics.uci edurmleamiMLRepository.html Irvine CA University of California Department of Information and Computer Science 1994 I81 F Provost andT Fawcett Robust classification forim precise environments Machine Learning 42\(3 203 231,2001 I91 R.L Rivest Learning decision lists Machine Learn ing 2\(3 229-246 1987 201 R.E Schapire and Y Singer Improved boosting algo rithms using confidence-rated predictions In Pmc of the 11th Conference on Computational Learning The ory 80-91 ACM Press 1998  A Silbenchatz and A Tuehilin On subjective mea sures of interestingness in knowledge discovery In Pmc of the Isr Internarional Conference on Knowl edge Discovery and Data Mining 275-281 1995 1221 L Todorovski P Flach and N Lavraf Predictive performance of weighted relative accuracy In D.A Zighed 1 Komorowski, and J Zytkow, editors Pmc of the 4rh European Conference on Principles of Data Mining and Knowledge Discovery 255-264 Springer 2000 231 I.H. Witten and E Frank Data Mining: PracricalMa chine Learning Tools and Techniques wirh Java Imple mentations Morgan Kaufmann 1999 24 S Wrobel An algorithm for multi-relational discov ery of subgroups In Pmc of the 1st European Sym posium on Principles of Data Mining and Knowledge Discovery 78-87 Springer. 1997 273 


association-cube, base-cube and population-cube are derived from the volume cube; the confidence-cube is derived from the association cube and population cube and the support-cube is derived from the associationcube and base-cube. The slices of these cubes shown in Figure 2 correspond to the same list of values in dimension merchant, time, area and customer_group  Multidimensional and multilevel rules Representing association rules by cubes and underlying cubes by hierarchical dimensions, naturally supports multidimensional and multilevel rules. Also these rules are well organized and can be easily queried  First, the cells of an association cube with different dimension values are related to association rule instances in different scopes. In the association cube CrossSales cell CrossSales product \221A\222, product2 \221B\222  customer_group 221engineer\222, merchant \221Sears\222, area \221Los Angeles\222, time 221Jan98\222 represents the following multidimensional rule x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x,\221B\222  275 customer_group = \221engineer\222, merchant = \221Sears\222, area 221Los  Angeles\222, time =  \221Jan98\222 If this cell has value 4500, and the corresponding cell in the population cube has value 10000, then this rule has confidence 0.45 Next as the cubes representing rules can have hierarchical dimensions, they represent not only multidimensional but also multi-level association rules. For example, the following cells CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221Jan98\222 CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221 Year98 222 represent association rules at different area levels \(i.e the city level and the state level\d different time levels \(i.e., the month level, the year level x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221Jan98\222 x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221 Year98 222 The cell CrossSales\(product \221A\222, product2 \221B\222,  customer_group 221top\222, merchant \221top\222, area \221top\222,  time \221top\222 represents the customer-based cross-sale association rule for all customers, merchants, areas, and times in the given range of these dimensions, expressed as x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222 4.3  Generating Association Rule Related Cubes The basic task of our OLAP based association rule mining framework, either at the GDOS or at an LDOS is to convert a volume cube i.e. the cube representing the purchase volumes of customers dimensioned by product  area etc, into an association cube a base cube and a population cube These cubes are then used to derive the confidence cube and the support cube of multidimensional association rule instances. The following general steps are involved in cross-sale association rule mining 267  Roll up the volume cube SaleUnits by aggregating it along merchant, time, area dimensions 267  Derive cube NumOfBuyers from SaleUnits based on the antecedent condition SaleUnits 0 267  Populate cube NumOfShoppers by the counts of customers dimensioned by merchant, area  time not by product\at satisfy the antecedent conditions 267  Derive cube CrossSales from SaleUnits based on the association conditions SaleUnits  product  p 1  0 and SaleUnits  product2  p 2 0 267  Derive cube Confidence and cube Support using cell-wise operations 214  Confidence = CrossSales  NumOfBuyers 214  Support  CrossSales  NumOfShoppers  Cubes Confidence  Support  CrossSales are dimensioned by product  product2 customer_group,merchant  time, area NumOfBuyers is dimensioned by product  customer_group, merchant time, area  NumOfShoppers is dimensioned by customer_group, merchant  time, area Rules with confidence and support that exceed specified thresholds  may be considered interesting 4.4. Rules with Conjoint Items Cubes with conjoint dimensions can be used to represent refined multidimensional association rules For example, using OLAP, we can derive association rules across time  Time-variant or temporal association rules such as 


x 316 Customers buy_product\(x,\222 A\222, \221 Jan98\222  336 buy _product\(x, \221B\222, \221 Feb98\222   275 area = \221Los Angeles\222 can be used to answer such questions as \223 How are  the sales of B in Feb98  associated with the sales of A in Jan98 224 The items in this rule are value pairs of dimensions product and time In order to specify this kind of association rule we introduce a conjoint dimension product, time and mirror it with dimension product2, time2 This allows a cell in the association cube to cross two time values. Accordingly, the cubes related to association rule mining are defined as follows Association cube  CrossSales.2 \(<product, time>, <product2, time2 customer_group, merchant, area  Population cube  NumOfBuyers.2  \(<product, time>, customer_group merchant, area Base cube  NumOfShoppers.2  \( customer_group, merchant, area Confidence cube Confidence.2 \(<product, time>, <product2, time2 customer_group, merchant, area Support  cube  Support.2  product, time>, <product2, time2 customer_group, merchant, area  The steps for generating these cubes are similar to the ones described before. The major differences are that a cell is dimensioned by, besides others product, time and product2, time2 and the template of the association condition is  SaleUnit s  product p 1 time t 1  0 and  SaleUnits  product2 p 2 time2 t 2  0 where, in any instance of this condition, the time expressed by the value of time2 is not contained in the time expressed by the value of time The template of the antecedent condition is SaleUnits   product p 1 time t 1  0 In general, other dimensions such as area may be added to the conjoint dimensions to specify more refined rules 4.5. Functional Association Rules A multidimensional association rule is functional if its predicates include variables, and the variables in the consequent are functions of those in the antecedent.  For example, functional association rules can be used to answer the following questions, where a_month and a_year are variables q  What is the percentage of people in California who buy a printer in the next month after they bought a PC x 316 Customer buy_product\(x, \221PC\222, a_ month 336 buy_product\(x, \221printer\222, a_month+1  275 area = \221California\222 q  What is the percentage of people who buy a printer within the year when they bought a PC  x 316 Customer: buy_product\(x, \221PC\222, a_ year 336 buy_product\(x, \221printer\222, a_year 275 area = \221California\222 To be distinct, we call the association rules that are not functional as instance association rules; e.g x 316 Customer: buy_product\(x,\222 PC\222, \221Jan98\222 336 buy_product\(x,\222 printer\222, \221Feb98\222  275 area =  \221California\222 Time variant, functional association rules can be derived from time variant, instance association rules through cube restructuring. Let us introduce a new dimension time_delta that has values one_day, two_day 205, at the day level, and values one_month, two_month, \205, at the month level, etc. Then, let us consider the following functional association rule related cubes Association cube  CrossSales.3 \(product, product2, customer_group merchant, area, time_delta  Population cube  NumOfBuyers.3 \(product, customer_group, merchant area Base cube  NumOfShoppers.3 \( customer_group, merchant, area Confidence cube  Confidence.3 \(product, product2, customer_group merchant, area, time_delta Support cube  Support.3 \(product, product2, customer_group, merchant area, time_delta The association cube CrossSales.3  can be constructed from CrossSales.2   The cell values of CrossSales.2  in the selected time and time2 ranges are added to the corresponding cells of CrossSales.3 For example, the count value in cell  CrossSales.2\(<PC, Jan98>, <printer, Feb98>\205 is added to cell \(bin CrossSales.3\(PC, printer, one_month,\205 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


