An Epistemic Utility Based Set-Valued Multiple-Target Tracker W.C Stirling and J.B Thompson Elect  Comp Eng\222g Dept Brigham Young University Provo UT 84602 D.R Morrell Elect  Comp Eng\222g Dept Arizona State University Tempe AZ 85287-5706 Abstract The performance of a MTT multiple target tracking method should degrade gracefully as the conditions of the collection become less favorable to optimal oper ation By stressing the avoidance rather than the explicit minimization of error we obtain a 
decision rule for trajectory-data association that does not re quire the resolution of all conflicting hypotheses when the database does not contain suficient information to do so reliably 1 Introduction We develop a multiple target trajectory-data associ at.ion decision rule that is expressed in terms of two probabilities one governing the informational value of the association hypothesis and one governing the sub jective belief or credal, probability of the association hypothesis l We also define a criterion of serious possibility and all trajectory-data associations that are seriously possible are retained it 
is not necessary to resolve all conflicting hypotheses before processing more data We employ an set-valued estimator, rather than the conventional point-valued estimator to describe the evolution of the vehicle trajectory The set-valued es t.imator is based upon the set-valued Kalman filter 2 which computes a convex set of trajectories all with equal claims to validity, given the observations 2 Epistemic Utility Theory For an inquiry under investigation, suppose there are finitely many hypotheses that may be considered Let U denote this set of possible answers and assume that exactly one element of U is correct 
and that all ele ments of U are consistent U is said to be an ultimate partition and a potential answer is the collection of hypotheses remaining after we have rejected all mem bers of a subset of U Each element of a potential answer is said to be a serious possibility Epistemic utility is composed of a convex combina tion of two probability functions one measuring the importance of acquiring new information the other measuring the importance of avoiding error We define the utility of accepting g c U 
in the interest of avoid ing error as 221T\(g;t  1 if t  true and 221T\(g;t  0 if t  false In addition to the cost of error we also ap portion a unit of informational value to each hypothe sis hi E U by assigning to elements of U non-negative real values such that their sum is unity If we enumer ate U  hl,hz  in and let M\(hj 2 0 denote the value assigned to hj 
then Cy M\(hj  1 and for any set g c U we define m\(g  m\(hj as the informational value of rejecting g We may address the conflict that exists between the goals of avoiding error and acquiring information by defining an epistemic utility functzon for acquir ing error-free knowledge that is making a decision as the convex combination u\(g;t  ar\(g;t  1  a l  m\(g The quantity CY represents the relative importance attached to avoiding error versus 
acquir ing new information We must restrict 4 5 Y 5 1 to ensure that no erroneous answer is preferred to any correct answer Since all utility functions that are re lated by a positive linear transformation are equiva lent we may simplify this utility function by defining ua\(g;C  iu\(g;k\221  9 The resulting utility func tion for accepting g in the interest of both avoiding error and acquiring new knowledge is where b  e is the coeficient of boldness and lies in the unit interval The 
closer b is to unity the less caution is exercised that error will be introduced in creased boldness in accepting hypotheses the closer b is to zero the lower the risk of error decreased bold ness We must also establish a probabilistic measure of belief for the hypotheses that are available Credal probability is probability formed on the basis of sub jective judgment represents the likelihood that an op tion is true and is independent of any informational value or demand that might be associated with the option 207 10.586393191 01.000 1991 IEEE 


For a given ultimate partition U let q\(g denote the credal probability assignment to any element g c U For g c U the expected utility is where EQ is mathematical expectation This ex pected utility represents a tradeoff between the desire to acquire new knowledge and the desire to avoid er ror The choice of b establishes a threshold at which the demand for knowledge renders the risk of error worthwhile We may adopt any set of hypotheses in the Boolean algebra generated by the elements of the ultimate par tition U This expands our possibilities we are not constrained to select only the elementary hypotheses hi but may choose any subset of them. This decision philosophy may be summarized as follows Levi's Rule of Epistemic Utility 3 page 531 Gzven a finite ultimate partition U an information determining probability function m defined over the Boolean algebra of elements of U an expectation delermining probability funciion q defined over the same algebra and an index of boldness b the agent should reject all and only those elements of h E U salisfying q\(hi  bm\(hi 3 Multiple Target Tracking 3.1 Track Initialization We assume that each track is characterized by a linear st,ochastic dynamics model of the form Xjt  Fjt-lxjt-1  Gjt-1ujt-1 2 where j  1,2   and ujt-l  N\(0 Qjt-1 i.e uit-1 is Gaussian with mean 0 and covariance matrix Qjt-l there are  active tracks at time t rt is un laown The number of tracks is allowed to vary since tracks may be initiated or terminated at any time We assume that all tracks lie in the same state space In the interest of brevity we restrict attention to the outputs of a single sensor and assume that this output may be characterized by a linear stochastic model of the form where st is the number of observations vectors at time t and zit is an rit-dimensional random vector and vit  N\(0 R;t Each observation vector therefore lies in an rit dimensional space corresponding to the column space of Hit We assume that each such space is a subset of the state space it 5 n We do not however require all observations to lie in the same dimensional subspace of the state space and we per mit the dimensionality of these column spaces to be time-varying Before data are collected we characterize the target environment with one set-valued track defined by an initial credal matrix KO an initial centroid state go and a prior covariance matrix no The initial state vector set is where Polo  no is a positive-definite matrix and Solo  KoKT For sample times t  1,2   we observe st observations 2  z     gsrt Let us suppose at time t that we have z-1 sets of predicted from time t  1 random variables of the form where 4 where Silt-l  I<~,,_ f<ilt-l]T with dp-1  Klg-lp-l 5   Ft-lf~i-llt-l 7  p:lt-l  Ft-ip~-llt-lF~l  Gt-1Qt-1GF-1\(6 for j  1   z 1 We shall use the same Ft  Gt  and Qt matrices for each track, thus it will not be necessary to index these matrices with their track identifications 3.2 The Ultimate Partition At each time t  there exist st observation sample vec tors git}:Ll of the random variables zit and 3-1 predicted track sets lt-l We wish to make decisions regarding the association of each zit with each track set x:lt-l We must define an ultimate partition for each sam ple vector, resulting in a set of st ultimate partitions of the form Vit  hitl   ltit7,-1 hit7t i  1    s tl where where 0 signifies the hypothesis that none of the track sets associate with the observation zit We shall say that track set is associated with observation sample vector zit if we fail to reject the hypothesis hitj Each ultimate partition Vit has the property that exactly one element is true although each is logically possible 308 


3.3 Calculation of the m-Function The m-function measures the information-value of re jecting an association One such measure of rejecting the association of a predicted track set x:,t-l with an observation at is the distance between sample val ues of the observation and and the track set if the distance is small there is little value in rejecting the association \(in other words there is great value in ac cepting it For g E C R and zit E Fit we define the generalized distance between them as We restrict attention only to a seriously possible region for zit given by where R  diag P    p and B a given con st,ant Define for all c whose projection onto the space spanned by the columns of Hit lies in Xit We shall denote this space by Zit  g E 92  Pitg E Xit where Pit is the projection operator onto the space spanned by the columns of Hit The function mit is a normalized distance be tween zit and g E x:it-ll and is a measure of the information gained by rejecting the association of  z E x:lt-l and zit Let E  be a ball with center at g E x:lt-l The information value of rejecting the association of I with zit is 10 where Pit is the projection of the ball B onto the space spanned by the columns of Hit The vector Pit3 is the projection of g onto the same column space For g E X{ll-l and z E x:lt-l with 1  g let the diameter of the balls 200I and 200 become arbitrar ily small Then the condition mil  nljt indicates that the information value of accepting the association of 3 with lit is greater than the value of accepting the association of g and zit 3.4 Calculation of the g-Functions The credal probability or q-function is the probabil ity that a given track-data association is correct This probability is characterized by the conditional distri bution of the observation given the track For each j  1  7 1 the set of predicted random vectors is given by For each observation random vector zit we represent the corresponding set of filtered random vectors by where Wit is the Kalman gain matrix given via the Kalman filter The probability distributions of x E Xflt are characterized by the family of posterior distributions pytlt\(f;g E obtained via Bayes rule We shall restrict consideration to the case where x E X:lt-l and zit are jointly normal so the mean and variance associated with this conditional distribution are given by the Kalman filter For each observation zit  sit we calculate the filtered set-valued estimate according to forj  l;..,'Tt-l with Wijt the Kalmangain defined by wijt  P:lt-lHz HitP:,t-lHz nit  The posterior densities assume the form for g E 1 The posterior density is with x a function of 2 given by 16 g E xilt-l provides a measure of the truth-value of the associa tion of each The family of densities p~tlt E x:lt-l with the observation 4it 309 


Let  be a ball with center at g The credal prob ability that the true state lies in B conditioned on the observational value lit is For 3 E X<lt-l and 3 E with g  E the condition qi,t\(B,-;g  qijt\(&;g indicates that the association of g with st is more credible or believable than the association of g and zit 3.5 The Association Likelihood Ratio The decision rule may be formulated in terms of the information-determining probability density mit\(;c where g E Yilt-l and the family of subjective be lief or credal, probability density functions functions g;g E lt-l We desire to apply Levi's rule of expected utility to this problem let a be a ball with center at  z Using 10 and 18 Levi's rule of expected utility indicates we may not reject the association of  and zit if Now let the radius of B go to zero and define the function Test For 3 E qijt\(B,-;g L bmit\(B 19 20  def  qijt\(g  3 3 Xtlt Since the densities are continuous at g a necessary condition for 19 to hold for all balls L is that qijt 2 bmit\(g 21 If 21 holds for any g E we may not reject the association of c with zit consequently we may not reject the association of the track set xilt-l with Lit Since we have e-:\(H.fc  Zit  1f.f4  Z,t 2x IP$l 12   Given the information-value determining probabil ity density 9 and the credal probability density 22 we may form the Association Likelihood Ratio Test ALRT Let xi,t-l be a predicted track set and let zit be a sample value of the observation vector zit We shall say that K{,t-l and zit are associated with boldness b ifqijt\(3 2 bmit\(g for some g E Zit n&lt-l That is there ezists 3 E Zit n x{,t-l such that in which case X:t obtained via 11 is a filtered track set Ifx:,t-l n Zit  8 then we deem zit and to be dissociated and the set it has no meaning and is discarded If x:lt-l survives the ALRT for zit then the likeli hood that x{lt-l is associated with L is greater than the information value gained by rejecting the associa tion 4 Example To illustrate the concept of the ALRT methodology consider the case of tracking targets constrained to planar motion Let g  z y i IT denote the kine matic state of a target in some convenient coordinate system n  4 The dynamics equation is where At is the sample interval and we have set G 5 I Possible target maneuvers are assumed to be characterized by the process noise ut whose covari ance is Qt We assume that angle-of-arrival data are available further we assume that the sensor is suffi ciently far from the target that the linearized model is adequate For convenience we also assume that the co ordinate system is resolved along the azimuth and el evation angles Hit   1 that is rit 3 2 Let 3  z1,22,z3,z41T E l it ziti zitzIT and suppose Rit  diag{pf,p Then the seriously possible region is 1000 310 


and the information-value determining probability density is Each g E represents the mean value of a predicted conditional distribution there exists a filtered conditional density of the form For each such PYt 5 g  N 4twijt Zit Hi I wit Hit Pi1 i  1 1 9 24  2 E t-l and evaluating this expression at   c rlij*\(Z  27T  Pt I  The ALRT is Associate x{,t-l and zit  titlr zit2lT if for any g  21,22,~3,4 E Xtit--l qijt\(z 2 Figure a illustrates a family of three crossing tra jectories The tracks move generally from left to right at time increases the lines correspond to the z and y position components and the  symbols correspond to noise-corrupted observations Figure l\(b displays the filtered track sets corresponding to this simula tion with the ellipses representing the projections of the track sets onto position space The initial pre dicted track set Xol.-l includes the entire field of view and is not shown The three large elliptical regions correspond to the the filtered track sets after the first set of observations have been processed As time in creases the size of these track sets decreases rapidly for the first few observations corresponding to Tracks 1 arid 3 however there are multiple associations since tlhe tracks are fairly close and the track sets are still fairly large As more confidence is obtained in the as sociations these tracks become uniquely associated and the elliptical regions decrease rapidly in size and will asymptotically become point tracks At sample ten Tracks 1 and 2 nearly intersect and both tracks associate with the observations These multiple asso ciations persist for a few samples but as the tracks diverge the associations again become unique Track 2 is unambiguously associated and estimated as ev idenced by the radius of the track set converging to zero and the set-valued estimates asymptotically be come point-valued b??lit 5 References a D R Morrell and W C Stirling Set-Valued Fil tering and Smoothing IEEE Transactions on Sys tems Man and Cybernetics 21\(1 Jan uary/February 1991 3 I Levi The Enterprise of Knowledge MIT Press Cambridge Massachusetts 1980 208 Y Figure 1 Three Crossing Tracks a simulated tra jectories and observations; \(b filtered track sets l W C Stirling and D R Morrell Convex Bayes Decision Theory IEEE Transactions on Sys tmw Man and Cybernetics 21\(1 Jan uary/February 1991 311 


Figure 5 True and mean estimated backlog versus time,t   alternating between 1/9 and 00 223semi-genie\224 Figure 6 True and mean estimated backlog versus time t  I36  transients The normalized information throughput for this run was s  0558 Figure 6 shows a similar plot but where no genie in formation is provided The a priori estimEte of the arrival rate is taken to be the constant 6  163 A  14 We see that for this value of 6 the backlog is apparently over and under-estimated roughly equally The normalized through put that resulted from this run was 0388 or about 70 of that achieved with the semi-genie model Figure 7 shows the true and es2mated backlog pro cesses for the case when   1.86 A  65 Although the backlog is poorly estimated during the intervals of low arrival rates, the resulting normalized throughput obtained was 0415 which is somewhat better about 74 of semi genie than that achieved for the case of Figure 6 We ran simulations for the entire spectrum of values for 6 and generally found little sensitivity to the value selected We conclude that for the case considered some benefit may be possible by devising algorithms for tracking the gross arrival rate but this benefit is not major Recall the ex aggerated arrival rate extremes Given that a constant value is used it is not especially sensitive the value se lected We emphasize that these conclusions may or may not be extendable for all parameter settings and further analysis should be made for specific network designs 2231 0 Figure 7 Rue and mean estimated backlog versus time  1.86 V Conclusions In this paper we have developed implementable dynamic transmission control procedures for single-hop slotted ALOHA networks using frequency-hopped spread spec trum These control procedures are based on backlog es timates and a backlog estimation algorithm that operates in the presence of multiuser interference ambient noise and jamming was derived The backlog process is esti mated by sensing activity on the receiver-based code while the user is not transmitting half-duplex operation is as sumed 80 that minimal if any additional hardware is required Performance was derived by simulation and nu merical examples show that the feasible system operates at nearly the same level of performance as the 223genie\224 model These results demonstrate that considerable robustness is achievable VI References 1 2 3 4 5 L P Clare and A R K Sastry 223The effects of slot ting burstiness and jamming in frequency-hopped random access systems,\224 IEEE MILCOM\22289 Conf Rec Boston MA October 15-18 1989 pp 154-160 L P Clare J E Baker and A R K Sastry 223A performance comparison of control policies for slot ted ALOHA frequency-hopped multiple access sys tems,\222\222 IEEE MILCOM\222SO Conf Rec Monterey CA September 30  October 3 1990 pp 608-614 L P Clare and J E Baker 223The effects of jam ming on control policies for frequency-hopped slotted ALOHA,\224 Proc IEEE GLOBECOM\222SO San Diego CA December 2-5 1990 pp 1132-1138 B Hajek 223Recursive retransmission control  Ap plication to a frequency-hopped spread-spectrum sys tem,\224 Proc I$h Conf Infor Sciences and Systems Princeton University March 1982 pp 116-120 N Pronios 223On the stability of spread-spectrum networks, with decentralized recursive retransmission control under jamming,\224 Proc INFOCOM\222SO San Francisco CA, June 5-7 1990 pp 588-594 20.2.6 0401 


speci\002ed for each experiment and this value is then used to determine the number of itemsets rows or columns in a single block As itemsets are created each is added to an available block and new blocks are 223allocated\224 as needed If a new block is to be allocated and the number of blocks in the simulated memory also a parameter of the experiment is equal to the number of occupied blocks a victim block is chosen for replacement using one of a number of replacement strategies If the data in the victim block has been modi\002ed since last being written to disk it is 223written,\224 and a simulated data block write is counted If a data element is required from a block that has previously been written to disk e.g for a subset lookup that block is 223read\224 into the simulated memory and a data block read is counted When the counting algorithm is complete the total number of blocks written and read during the simulation is available for examination In addition to the number of data block reads and writes the simulation also counts the number of equal-cost CPU operations as described in Section 4 and the number of index block reads and writes This allows a comparison of the different algorithms based on the number of operations performed and the additional cost of index lookups The counting algorithms themselves are implemented in an I/O-ef\002cient manner using a block pinning and release strategy In all cases the join step is performed using a block nested loop 9 a l g o r i t h m w i t h each s e t o f b l o ck s i n the outer loop being pinned and released as necessary The number of blocks available for the inner and outer loops are both parameters of the experiment 5.2 Effects of Dataset Parameters The experiments were chosen to compare the effect of various distributions and relative 223dimensions\224 i.e number of rows compared to the number of distinct items or columns of the data For the RW approach subsets were identi\002ed in each row of the database and c ounters were accessed only as needed For the CW case the results of the conjoin were not kept For all experiments an LRU replacement strategy was used to determine victim blocks in memory Speci\002c parameters for each of the following 002gures are given at the end of this section in Figure 5 Figure 3 shows a comparison of the number of data block reads and writes for both the RW and CW approach as the level of minimum support is decreased leaving all other parameters 002xed Note that as the required level of minimum support is decreased the difference between the two approaches in the total number of block movements increases at an accelerating rate Figure 4 shows the results of an experiment intended to demonstrate the relative I/O cost of the RW and CW algorithms for datasets of 223equal height\224 i.e the number of  0 200,000 400,000 600,000 800,000 22 20 18 16 14 12 10 8 6 4 Minimum support RW CW Figure 3 Effect of decreasing support 100 1,000 10,000 100,000 0.10 0.20 0.40 0.60 0.80 1.00 1.20 1.40 1.60 Width ratio RW CW Figure 4 Effect of table width rows in the data held constant and varying 223width\224 i.e the number of columns The total number of blocks read or written is plotted in a logarithmic scale along the y axis including index blocks The x axis shows the relative ratio of the number of columns to the number of rows in the dataset For example a dataset with 1000 columns and 1000 rows would have a width ratio of 1 while a dataset with 100 columns and 1000 rows would have a width ratio of 0.1 That is the higher the width ratio the 223wider\224 the table is relative to its 223height\224 The number of rows i.e l as held constant at 500 while the number of columns i.e n  was varied from 50 to 1000 The other parameters of the dataset i.e B C max  B R max  were determined by the default values of the synthetic dataset generator As expected from the analysis of Section 4.3 the CW algorithm is far more ef\002cient than the RW algorithm for datasets with a relatively high width ratio Once the number of rows is suf\002ciently greater than the number of columns the advantage of the CW algorithm is no longer apparent Our experiments more results are presented in v e r ify that for datasets of the type we described accessing the database in a column-wise fashion leads to a reduction in the number of I/O operations required Again the key point 


Figure Rows  l  Items  n  M M D 1 M D 2 f I minsup 3 1000 1000 16 8 4 512 22-4 4 500 50-1000 32 10 8 512 0  2 l Figure 5 Experimental parameters per 002gure is that the counters which maintain the identity and frequency of occurrence for a particular set of items need not be repeatedly and expensively brought into the primary storage as is the case in the row-wise method 6 Discussion and Conclusions We discuss a few possible improvements not explored in our current work followed by a summary of our conclusions 017 We assumed that the RW algorithm strictly follows the strategy of identifying each subset present in a given row and then accesses the appropriate c ounter However an implementation may also read a number of blocks of counters and then 002nd all rows with an itemset to support that counter as per T h e o rde r ing o f the rows and the counters will likely have an effect on which of these two methods is more ef\002cient 017 An obvious improvement to the cost of the RW algorithm as presented is to perform the join for a pair of  k 000 1 itemsets or even a set of blocks thereof and then immediately perform the check of all its subsets Given the likelihood that many of the itemsets in ablockof L k 000 1 will share common items this could result in a signi\002cant savings since the cost of writing the join and then re-reading for the prune would be avoided probably without a signi\002cant increase in the number of counter blocks that need to be read 017 In the RW Apriori counting algorithm the primary purpose of the veri\002cation and pruning of all  k 000 1 subsets of any candidate before actually counting its support is to limit the number of times that a row or candidate need be brought in for support counting Since the CW algorithm already has all information necessary for counting at one pass it may be better to avoid the Apriori veri\002cation of subsets and simply to count the support of each candidate This is especially true if the candidates in a given block of itemsets all share a frequent common subset as the conjoin for that subset could be computed once and used directly in counting the support of candidates sharing that subset Also given the possibly larger processing cost of the CW algorithm when memory is unconstrained and the savings in I/O when memory is limited some combination of both the RW and CW algorithms may yield the best performance As organizations begin collecting their own electronic datasets and as growing sources of information become available through the Internet it is imperative that the data organization and access approaches be studied carefully to exploit ef\002cient and promising processing techniques In particular in this paper we have examined the I/O ef\002ciency considerations for association rules algorithms with respect to data organization We have shown that a column-wise strategy for mining tabular data may provide an improvement in the I/O ef\002ciency over a similar row-wise algorithm We used a simple analysis and experimental validation to support our approach This provides an indication that similar considerations may signi\002cantly bene\002t other high-cost computing problems associated with mining of datasets References 1 R  A gr a w al et al  T he Q u est D at a M i n i n g S yst e m  Technical report IBM Almaden Research Center 1996 http://www.almaden.ibm.com/cs/quest 2 R  A g r a w a l  T  I m i e lin s k i a n d A  S w a m i  M in in g A s s o c i ation Rules between Sets of Items in Large Databases In Proceedings of the 1993 ACM SIGMOD Int'l Conf on Management of Data  1993 3 R  A g r a w a l  H  M a n n illa  R  S rik a n t  H  T o i v o n e n  a n d A  I  Verkamo Fast Discovery of Association Rules In Advances in Knowledge Discovery and Data Mining  pages 307\226328 AAAI Press  MIT Press 1996 U.M Fayyad G PiatetskyShapiro P Smyth and R Uthurusamy editors 4 R  A gr a w al and R  S r i kant  F ast A l gor i t h m s f o r M i n i n g Association Rules In Proc 20th Int'l Conference on Very Large Databases  Santiago Chile 1994 5 R  J  B ayar do Ef 002ci e nt l y M i ni ng Long P a t t e r n s f r o m Databases In Proceedings of the 1998 ACM SIGMOD Conf on Management of Data  1998 6 V  C r e st ana D ec 1997 I n f o r m al cor r e spondence 7 B  D unkel a nd N  Sopar k ar  D at a O r g ani zat i o n a nd A ccess for Ef\002cient Data Mining Technical report The University of Michigan Ann Arbor 1999 8 B  D unkel  N  Sopar k ar  J  S zar o and R  U t hur usam y  Systems for KDD From concepts to practice Journal of Future Generation Computer Systems  October 1997 9 A  S i l b er schat z  H  K or t h  a nd S Sudar s han Database System Concepts  McGraw-Hill New York third edition 1997  M  J Zaki  S  P ar t h asar at hy  M  O gi har a  a nd W  Li  N e w Algorithms for Fast Discovery of Association Rules In 3rd Int'l Conf on Knowledge Discovery and Data Mining  pages 283\226286 Newport California Aug 1997 


local support count X.sup must be smaller than the local threshold s x D Following from the discus sion in Subsection 3.3 X.supq is bounded by the value min\(maxsupq X s x D  1 Hence an upper bound of X.sup can be computed by the sum x.supj  jEX.large-sites 2 min\(mazsupq\(X s x Dq  1 q=l q+?X.large-sites In FDM-LPP Si calls p-upper-bound to compute an upper bound for X.sup according to the above for mula This upper bound can be used to prune away X if it is smaller than the global support threshold 0 As discussed before both FDM-LUP and FDM LPP may have less candidate sets than FDM-LP How ever they require more storage and communication messages for the local support counts Their efficiency comparing with FDM-LP will depend largely on the data distribution 5 Performance Study of FDM An in-depth performance study has been performed to compare FDM with CD We have chosen to im plement the representative version of FDM FDM LP and compare it against CD Both algorithms are implemented on a distributed system by using PVM Parallel Virtual Machine 6 A series of three to six RS/6000 workstations running the AIX system are connected by a 10Mb LAN to perform the experi ment The databases in the experiment are composed of synthetic data In the experiment result the number of candidate sets found in FDM at each site is between 10  25 of that in CD The total message size in FDM is between 10  15 of that in CD The execution time of FDM is between 65  75 of that in CD The reduction in the number of candidate sets and message size in FDM is very significant The reduction in execution time is also substantial However it is not directly proportional to the reduction in candidate sets and message size This is mainly due to the overhead of running FDM and CD on PVM What we have ob served is that the overhead of PVM in FDM is very close to that in CD even though the amount of mes sage communication is significantly smaller in FDM From the results of our experiments it is also clear that the performance gain of FDM over CD will be higher in distributed systems in which the commu nication bandwidth is an important performance fac tor For example if the mining is being done on a distributed database over wide area or long haul net work The performance of FDM-LP against Apriori in a large database is also compared. In that case the response time of FDM-LP is only about 20 longer Interpretation transaction mean size mean size of maximal potentially large itemsets number of potentially large itemsets Number of items Clustering size Pool size Correlation level Multiplying factor Parameter ITI III ILI N sq Ps Mf Cr Value 10 4 2000 1000 5-6 50  70 0.5 1260  2400 Table 5 Parameter Table than 1/n of the response time of Apriori where n is the number of sites This is a very ideal speed-up In terms of total execution time FDM-LP is very close to Apriori The test bed that we use has six workstations Each one of them has its own local disk, and its partition is loaded on its local disk before the experiment starts The databases used in our experiment are synthetic data generated using the same techniques introduced in 2 lo The parameters used are similar to those in lo Table 5 is a list of the parameters and their values used in our synthetic databases Readers not familiar with these parameters can refer to 2  In the following we use the notation Tx.Iy.Dm to denote a database in which D  m in thousands IT1  x and 111  y T10.14.D200K s  3 4 5 6 Number of Nodes FDM CD Figure 1 Candidate Sets Reduction n  3 4 5 6 5.1 Candidate Sets and Message Size Re duction The sizes of the databases in our study range from 200K to 600K transactions and the minimumsupport threshold ranges from 3 to 3.75 Note that the number of candidate sets at each site are the same in CD and different in FDM In our experiment we witnessed a reduction of 75  90 of candidate sets on 39 


T10.14.D200K, n  3 T10.14.D200K, n  3 60  I S 8 3.00 3.25 3.510 3.75 YO  I YO Minimum support FDM kCD  gs 3.00 3.25 3.50 3.75 Minimum support FDM CD Figure 4 Message Size Reduction Figure 2 Candidate Sets Reduction average at each site when FDM-LP is compared with CD In Figure 1 the average number of candidate sets generated by FDM-LP and CD for a 200K transaction database are plotted against the number of partitions FDM-LP has a 75  90 reduction in the candidate sets The percentage of reduction increases when the number of partitions increases This shows that FDM becomes more effective when the system is scaled up In Figure 2 the same comparison between FDM-LP and CD is presented for the same database with three partitions on different thresholds In this case, FDM LP experienced a similar amount of reduction T10.14.D200K s  30/0 I 150 100 50 0 3 4 5 6 Number of Nodos FDM CB Figure 3 Message Size Reduction n  3 4 5 6 The reduction in candidate sets should have a pro portional impact on the reduction of messages in the comparison Moreover as discussed before the polling site technique guarantees that FDM only requires O\(n messages for each candidate set which is much smaller than the O\(n2 messages required in CD In our experiment FDM has about 90 reduction in the total message size in all cases when it is compared with CD In Figure 3 the total message size in FDM and CD for the same 200K database are plotted against the number of partitions In Figure 4 the same compari son on the same database of three partitions with dif ferent support thresholds are presented Both results confirm our analysis that FDM-LP is very effective in cutting down the number of messages required T10.14.D200K s  3 90 E3 28  U 70 cc Q 8 a c 50 c xs w  I 3 4 5 6 Number of Nodes FDM CD Figure 5 Execution Time n  3 4 5 6 T10.14.D200K n  3 3.00 3.25 3.50 3.75 Minimum Support E-FDM A-CD Figure 6 Execution Time 5.2 Execution Time Reduction We have also compared the execution time between FDM-LP and CD The execution time of FDM-LP and CD on a 200K database are plotted against the number of partitions in Figure 5 FDM-LP is about 40 


25  35 faster than CD in all cases In Figure 6 the comparison is plotted against different thresholds for the same database on three partitions Again FDM LP is shown to have similar amount of speed-up as in Figure 5 n  3 D  60011 s  2 I Apriori I FDM-LP response time sec I 1474 I 387 I total execution time sec I 844.7 I 842.9 I Table 6 Efficiency of FDM-LP We have also compared FDM-LP on three sites against Apriori with respect to a 600K transactions database in order to find out its efficiency in large database The result is shown in Table 6 The re sponse time of FDM-LP is only slightly 20 larger than 1/3 of that of Apriori In terms of the total ex ecution time FDM-LP is very close to Apriori For a large database FDM-LP may have a bigger portion of the database residing in the distributed memory than Apriori Therefore it will be much faster than running Apriori on the same database in a single ma chine This shows that FDM-LP on a scalable dis tributed system is an efficient and effective technique for mining association rules in large databases The performance study has demonstrated that FDM generates a much smaller set of candidate sets and requires a significantly smaller amount of mes sages when comparing with CD The improvement in execution time is also substantial even though the overhead incurred from PVM prevents FDM from achieving a speed-up proportional to the reduction in candidate sets and message size Even though we have only compared CD with FDM-LP there is enough evidence to show that FDM is more efficient than CD in a distributed environment In the follow ing sections we will discuss our future plan of imple menting the other versions of FDM 6 Discussions In this discussion we will first discuss the issue of possible extension of FDM for fast parallel mining of association rules Following that we will discuss two other related issues 1 the relationship between the effectiveness of FDM and the distribution of data and 2 support threshold relaxation for possible reduction of message overhead The CD and PDM algorithms are designed for share-nothing parallel environment. In particular CD has been implemented and tested on the IBM SP2 machine In designing algorithm for parallel mining of association rules not only the number and size of messages required should be minimized but also the number of synchronizations which is the number of rounds of message communication CD has a simple synchronization scheme It requires only one round of message communication in every iteration Besides the second iteration PDM also has the same synchro nization scheme as CD If FDM was used in the paral lel environment it has a shortcoming even though it requires much less message passings then CD it needs more synchronizations However FDM can be modi fied to overcome this problem In fact in each itera tion the candidate set reduction and global pruning techniques can be used to eliminate many candidates and then a broadcast can be used to exchange the local support counts of the remaining candidates This ap proach will generate less candidate sets than CD and has the same number of synchronization Therefore it will perform better than CD in all cases Performance studies has been carried out in a 32-nodes IBM SP2 to study several variations of this approach and the result is very promising Another interesting issue is the relationship be tween the performance of FDM and the distribution of the itemsets among the partitions From both The orem 1 and Example 1 it is clear that the number of candidate sets decreases dramatically if the distribu tion of itemsets is quite skewed among the partitions If most of the globally large itemsets were locally large at most of the sites the reduction of candidate sets in FDM would not have been as significant In the worst case if every globally large itemset is locally large at all the sites the candidate sets in FDM and CD will be the same Therefore data skewness may improve the performance of FDM in general Special partitioning technique can be used to increase the data skewness to optimize the performance of FDM Some further study is required to explore this issue The last issue that we want to discuss is the pos sible usage of the relaxation factor proposed in ll In FDM if a site sends not only those candidate sets which are locally large but also those that are almost locally large to the polling sites the polling sites may have local support counts from more sites to perform the global pruning of candidate sets For example if the support threshold is lo every site can send the candidate sets whose local support counts exceed 5 to their polling sites In this case for some candi date sets their polling sites may receive local sup port counts from more sites than the no relaxation case Hence the global pruning may be more effec tive However there is a trade-off between sending more candidate sets to the polling sites and the prun ing of candidate sets at the polling sites More study is necessary on the detailed relationship between the relaxation factor and the performance of the pruning 7 Conclusions In this paper we proposed and studied an efficient and effective distributed algorithm FDM for mining association rules Some interesting properties between 41 


locally and globally large itemsets are observed which leads to an effective technique for the reduction of can didate sets in the discovery of large itemsets Two powerful pruning techniques local and global prun ings are proposed Furthermore the optimization of the communications among the participating sites is performed in FDM using the polling sites Sev eral variations of FDM using different combination of pruning techniques are described A representative version FDM-LP is implemented and whose perfor mance is compared with the CD algorithm in a dis tributed system The result shows the high perfor mance of FDM at mining association rules Several issues related to the extensions of the method are also discussed The techniques of can didate set reduction and global pruning can be inte grated with CD to perform mining in a parallel envi ronment which will be better than CD when consider ing both message communication and synchronization Further improvement of the performance of the FDM algorithm using the skewness of data distribution and the relaxation of support thresholds is also discussed Recently there have been interesting studies on the mining of generalized association rules multiple level association rules quantitative association rules etc Extension of our method to the min ing of these kinds of rules in a distributed or parallel system are interesting issues for future research Also parallel and distributed data mining of other kinds of rules such as characteristic rules 7 classification rules, clustering 9 etc is an important direction for future studies For our performance studies an im plementation of the different versions of FDM on an IBM SP2 system with 32 nodes has been carried out and the result is very promising References l R Agrawal and J C Shafer Parallel mining of association rules Design implementation and experience In IBM Research Report 1996 2 R Agrawal and R Srikant Fast algorithms for mining association rules In Proc 1994 Int Conf Very Large Data Bases pages 487-499 Santiago Chile, September 1994 3 R Agrawal and R Srikant Mining sequential patterns In Proc 1995 Int Conf Data Engi neering pages 3-14 Taipei, Taiwan March 1995 4 D.W Cheung J Wan V Ng and C.Y Wong Maintenance of discovered association rules in large databases An incremental updating tech nique In Proc 1996 Int\222l Conf on Data Engi neering New Orleans, Louisiana Feb 1996 5 U M Fayyad 6 Piatetsky-Shapiro P Smyth and R Uthurusamy Advances zn Knowledge Dis covery and Data Mining AAAI/MIT Press 1996 6 A Geist A Beguelin J Dongarra W Jiang R Manchek and V Sunderam PVM Parallel Virtual Machine A Users\222 Guide and Tutorial for Networked Parallel Computing MIT Press 1994 7 J Han Y Cai and N Cercone Data driven discovery of quantitative rules in relational databases IEEE Trans Knowledge and Data En gineering 5:29-40 1993 Discovery of multiple-level association rules from large databases In Proc 1995 Int Conf Very Large Data Bases pages 420-431 Zurich Switzerland Sept 1995 8 J Han and Y Fu 9 R Ng and J Han Efficient and effective cluster ing method for spatial data mining In Proc 1994 Int Conf Very Large Data Bases pages 144-155 Santiago Chile, September 1994 lo J.S Park M.S Chen and P.S Yu An effec tive hash-based algorithm for mining association rules In Proc 1995 ACM-SIGMOD Int Conf Management of Data pages 175-186 San Jose CA May 1995 ll J.S Park M.S Chen, and P.S Yu Efficient par allel mining for association rules In Proc 4th Int Conf on Information and Knowledge Manage ment pages 31-36 Baltimore Maryland Nov 1995 12 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases In Proc 1995 Int Conf Very Large Data Bases pages 432-443 Zurich Switzerland Sept 1995 13 A Silberschatz M Stonebraker and J D U11 man Database research Achievements and op portunities into the 21st century In Report of an NSF Workshop on the Future of Database Sys tems Research May 1995 14 R Srikant and R Agrawal Mining general ized association rules In Proc 1995 Int Conf Very Large Data Bases pages 407-419 Zurich Switzerland Sept 1995 association rules in large relational tables In Proc 1996 ACM-SIGMOD Int Conf Manage ment of Data Montreal Canada June 1996 15 R Srikant and R Agrawal Mining quantitative 42 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


