Hebbian Learning and Self-Association in Nonlinear Neural Networks Francesco Palmieri Member IEEE Abatraci A self-organizing feature map based on a Hebbian paradigm is proposed as a universal adaptive memory The learn ing paradigm can be applied to arbitrary network topologies containing the standard sigmoidal nonlinearities at their nodes The system generalizes the linear principal com ponents by mapping the input space into a set of orthogonal nonlinear projections Only localized 
learning rules are necessary for the adaptation The size of the system is related to the desired accuracy and to the density of the examples I INTRODUCTION A self-organizing neural network has to provide a useful alternate or compressed representation of the input space feature map Previous contribu tions have shown how in artificial neural networks simple learning rules localized at the synaptic level can lead to interesting patterns of organization \(Lins ker 1986 These structures bear strong resem blance to 
patterns of activity observed in biological brains The emphasis of this work is on the search for suitable principles of self-organization that applied to loosely constrained architectures can lead to the emergence of representations useful for the appli cations For example in the linear framework the principal components provide compression for sources exhibiting marked linear correlations Our goal is to provide an analysis framework, solidly rooted in the signal processing and system theory that gener alizes the well-studied adaptive linear filter theory Haykin 1986 
We came to focus our attention in our previous papers Palmieri Zhu and Chang 1993 Palmieri and Zhu 1993 on two main criteria for the Heb bian \(excitatory and the anti-Hebbian inhibitory synapses respectively In particular an Hebbian The author is with the Dipartimento di Elettronica Univer rita\222 degli Studi di Napoli 223Federico 11,\224 via Claudio 21 80125 NAPOLI Italy Email pdmieriOnadis.dis.unina.it and with the Department of Electrical and System8 Eng The University of 
Connecticut Stom CT 06269-3157 USA synapse attempts to excite the node to which it is attached in a way that guarantees the preservation of the source information This is achieved by re quiring the input to be maximally reconstructible from the node value \(Principle of Maximum Inverse Linear Fieconstruction An anti-Hebbian synapse instead inhibits the node to which it is attached by removing the correlation between its input and the node value These criteria are analyzed in 
detail for the lin ear and the nonlinear neural networks made up of nodes containing the familiar sigmoidal nonlineari ties Palmieri 1993 The principle of self-association in linear networks has been proposed by Bourland and Kamp 1988 Baldi and Kornik \(1989\Hrycej \(1990 Xu 1993 Karhunen and Joutsensalo 1993 that did not con sider the hybrid architecture containing both Heb bian and anti-Hebbian synapses Also the constraints on the locality of the learning rules was not al ways imposed Hybrid structures for the computa tion of the principal components have 
been proposed by Foldiak 1988 and by Kung and Diamantaras 1990 that first postulated learning equations, and then showed how the principal components emerge from the self-organization We start from the objective functions and study the learning behavior of a linear and a nonlinear neural network that contains an arbitrary mixture of Hebbian and anti-Hebbian synapses The nonlinear projections that emerge from the self-organization provide a potentially universal multidimensional rep resentation of the input space The remarkable fea ture of this 
paradigm is in the full locality of the learning algorithms Even though the cost function backward reconstruction is formulated in global terms, the orthogonality that emerges from the anti Hebbian connections only need Hebbian learning equa tions completely localized to the synaptic level 11 THE LINEAR NEURAL NODE A The Anti-Hebbian Synapse Figure l\(a shows a linear neural node with an anti Hebbian input 2 and another generic input z The 0-7803-1901-2U94 4.00 01994 IEEE 1258 


X Figure 1 The neural node output value is y  coza  a where both zq and z are two arbitrary random variables As already dis cussed in Palmieri Zhu and Chang 1993 the anti Hebbian synapse inhibits the neuron by removing the correlation between xa and y The unique 80 lution to E[z,y  0 is clearly c  w Note that the linear node with the anti-Hebbian synapse can be seen as a linear adaptive filter of order one with the output y playing the role of the error in estimating z linearly from 2 In fact, the same solution can be found by minimizing E[g2 The classical adaptive linear filter theory Haykin 1986 is based on this formulation with the orthogonality pn'ncipIe ensuring that the input to and the error y are orthogonal in optimal conditions We prefer us ing the orthogonality condition as the starting point because as we will see in the following it becomes immediately applicable to the nonlinear scenario B The Hebbian Synapse Figure l\(b shows the same linear neural node of Figure 1\(a with one Hebbian input t  The Heb bian synapse excites the neuron by forcing the neural node value to be as related as possible to z We identified in our previous papers Palmieri and Zhu 1993 Palmieri 1993 the following strat egy for the Hebbbian synapse learn the value c that corresponds to the best linear backward reconstruc tion of t That is find the value of c that minimizes 200  E[\(x  cy 200  E[2]C  PE[tr]c3  E[4 1 The cat function can be rewritten as 2E[z2  E[zz]c  E[t2 2 By computing its derivatives it can be easily seen that for arbitrary z  it is not a purely convex func tion but it may exhibit one global and one local minimum \(Palmieri and Zhu 1993 Typical cases arise when both inputs are orthogonal E[zz  0 in which case there are two symmetric equivalent global minima and when a  0 which gives the two equivalent solutions c  fl C The Hybrid Node A hybrid node is shown in Figure l\(c where both excitatory and inhibitory synapses are present The node value is y  Cats  cz with the anti-Hebbian synapse trying to inhibit the node and the Hebbian synapse trying to excite it Since the anti-Hebbian synapse has a stationary point given by E[z,y  0 with c  cw by substituting this expression in  we have   E[x2  w c4  c2  E[z2 3 E[zal Since E[+z,I2 _ E[z$3[t2  has a maximum at c  0 and two equivalent global minima at c  fl The twostationary points are c c  f\(1  A typical special case arise when x and 2 are orthogonal E[z,t  0 The anti-Hebbian synapse does nor arise and the Hebbian synapse just copies the input to the output Also interesting is the case with ta  kz with k being any real constant The anti-Hebbian synapse becomes c  kc and g  0 The anti-Hebbian synapse shuts off the neuron completely, with the Hebbim synapse left undefined floating The inverse reconstruction of t is ill defined with 200  E[t2 V c The above analysis can be easily generalized to an hybrid node accepting M anti-Hebbian inputs 02  CaM  ca and N Hebbian inputs XI 22  ZN  x through N synapses c1  c2  CN  c as shown in Figure l\(d The anti-Hebbian synapses guarantee orthogonality with E[zaj  0 V i  1  M with values given by tal ta2  taM  Xa through M synapses col CO  E[x~x~]-'E[x~x~]c 4 The principle of linear backward reconstruction of x requires the simultaneous minimization of the cost function which becomes using 4 E  E[llx  CY1i21 5 8  E[xTx  2cTR  cTccTR 6 with and R  E[xxT and pa  E[xxz We have al ready proven Palmieri and Zhu 1993 and it is R  R-p,R,'pz 7 1259 


Figure 2 The architecture for orthogonalization easy to verify it by direct substitution that equa tion 6 has critical points at the eigenvectors of R The only two minima are at plus-or-minus the principal eigenvector All the other ones are saddle points It is useful to examine a special cases that will be important in the study of the architecture of the next section If the anti-Hebbian inputs are propor tional to the projections of x on a set of M eigen vectors of R M  N as za1 klqY1x;~a2  k*qT,x  za  kMqT,,x with kl k2  kM ar bitrary constants we have that from 7 c will con verge to plus-or-minus the principal eigenvector of the set qi,q2  qil,qia,...,QiN A more comprehensive analysis of the hybrid neural node would include a nonlearning input a as in Figures l\(a and l\(b This case may be of interest if we want to assume that each node in the network has an independent noise source or accepts nonlearning inputs or inputs that learn at a much lower speed to be considered stationary We leave the more gen eral analysis out for brevity It will be discussed somewhere else D A Linear Architectures The analysis of the general hybrid node allows us to establish a number of results for arbitrary ar chitectures made up of linear neurons and mixtures of Hebbian and anti-Hebbian synapses For exam ple the architecture of Figure 2 certainly gives the principal components of the input The first out put is the projection on the principal component The second one, using 6 and 7 is the projection on the second one and so on Note that the crite rion of maximum inverse reconstruction is applied in a decoupled fashion In other words, each neu ron maximally reconstruct its inputs from its out put independently from the others The solution is the same as that obtained from the more general cost function E  E[IIx  xbllq  E[llx CCxl12 which would require coupling of the various outputs for xb Palmieri and Zhu 1993\(b The equiva lence is obvious if we observe that at equilibrium the outputs are orthogonal and the minimization of E is the Bame as the independent minimization of 200i  E[IIx qyill V i  1  M 111 THE NONLINEAR NODE The great interest we attribute to the paradigm studied above for the linear scenario is in the fact that it can be immediately applied to neural nodes that have the common nonlinear structure with the sigmoidal nonlinearity The sigmoid can be any strictly increasing function such that a\(O  0 limh h  IC and lim,-m 4p\(h  kl with k and kl being any two real constants Even though most of results hold for these functions in our calcu lations and simulations we adopt the following func tion 8 1 ah 4a\(h  2tgh-2-l where the parameter Q is introduced to control the slope of the transition around aero In fact for a  00 da\(h  asgn\(h and for small a h N Zh According to the magnitude of the values at its input the sigmoid can behave either as a binary threshold or as a linear unit conveniently generalizing the lin ear case The derivative is easily computed to be 4h\(h  Q\(f  4 A The Anti-Hebbian Synapse Let us begin by considering the simple architecture of Figure l\(a with only one anti-Hebbian input and an extra input z The output is y  d\(cat  z Just as in the linear case the objective of the anti Hebbian synapse is that of removing the correlation E[tay The following theorem guarantees that this is possible under the most general conditions Theorem For any mndom variables xa and z and any sigmoidal function 4\(h with b\(0  0 there exist a unique value of c that satisfies the equation Proof Since V CO the function E[z,y is strictly increasing in Ca Therefore if a solution to the equation exists it is unique But ca can take arbitrarily large values to bring 4a\(caza  z to saturation This means that there are values of ca at which the quantity E[Za#a\(Caza  z is positive and values at which it is negative which implies that there exist a value of Ca where E[Zada\(Cata  z 1260 


Figure 3 A generic B The Hebbian Synapse hybrid architecture Refer to Figure l\(b The cost function for the Heb bian synapse on the nonlinear node becomes   E[\(z  c$b\(cz  421 11 The results on the linear case suggest that for arbi trary distribution of z and z the function  is not necessarily a convex function of c However we veri fied for special cases that the unicity of the solution two equivalent solutions remains in a mixed node just like in its linear counterpart C The Hybrid Node For the mixed architecture of Figure l\(c even though we cannot write down an explicit expression for the value of co we know from the above theorem that for any value of c a solution to the equation E[zada\(Caza  CZ  0 12 exists and it is unique The cost function for the Hebbian synapse becomes   E  c#\(coio  CZ where to co we should substitute the solution to 12 We have not yet been able to study rigor ously the convexity of  as in the linear case but we conjecture from simulations that a result simi lar to that of the linear case, should hold with one maximum and two symmetric minima The same criteria can be applied to the more general node of Figure l\(d The output is  I$\(c~x,+c*x For any c the anti-Hebbian synapse will have a unique weight configuration given by the solution to the set of equations E[Xa9\(C;fXo  cTx  0 13 and the cost function for the Hebbian part will be D The Nonlinear Neural Network An arbitrarily complex architecture with both Heb bian and anti-Hebbian synapses can be studied with the tools provided above To fix the ideas let us look a the network of Figure 3 The node values ii after the sigmoids are the result of arbitrary mixtures of excitatory and inhibitory inputs From the above diecueeion we know that if two nodes lsre connected though an anti-Hebbian synapse at equi librium their node values will be orthogonal Simul taneously, for the Hebbian synapses the criterion of inverse reconstruction requires minimization of i  E[\(z  bi  E[\(z  cji~j 15 jEBi applied to all the nodes in the network where Bi is the set of nodes excited by the ith node through Hebbian synapses This criterion does not appear to be properly 10 cal because it requires the computation of 16 which is the backpmpugation of the values zj E rti through the Hebbian connections However if every pair of zj E Si is orthogonal the minimization of  is equivalent to the separate simultaneous minimisa tion of jj  E[\(z  cji~j V j E B 16 Hence if each pair of nodes in Bi is connected through an anti-Hebbian synapse the cost function is com pletely local at the synaptic level A neural architecture composed by arbitrarily in terconnected nodes leads to the emergence of a rep resentation of the inputs that guarantees the exis tence of a linear transformation for \223retrieval\224 from the network This is the meaning we attribute to this system as an emerging physical memory Note that in a multi-layered feedforward topology a perfect backward reconstruction at each node im plies a perfect reconstruction at the inputs The network architecture that follows already in troduced in Palmieri 1993 demonstrates how to obtain a set of nonlinear projections of the input space that go from purely linear principal compo nents to binary networks E An Architecture for Nonlinear Principal The architecture shown in Figure 2 implements a generalization of the linear principal components If the sigmoids at each node are used in their lin ear region small net variance at the input of each node the network essentially behaves as a linear fil ter with the linear principal components emerging as the result of the self-organization If instead the input energy is increased or the sigmoidal functions are made increasingly sharp, the network begins to provide a progressively finer breakdown of the input space into orthogonal nonlinear projection Components 1261 


The following simulation performed with the ar chitecture of Figure 2 on a one-dimensional input should be sufficient to demonstrate the potential of this idea Figures 4 and 5 show the resulting mappings pro vided by the network of Figure 2 with N  1 after self-organization with input examples uniformly dis tributed in the interval 0.5,0.5 The learning rule for the anti-Hebbian synapsea is the simple one Acaij  pyiyj  j  1  M i  j  1  M 17 where c,jj is the anti-Hebbian connection from the output j to the output i A number of variations to the rule would work as well with slightly different convergence properties The Hebbian synapses are trained using to the rule Acjj  p\(xj  zbj j  1  N i  1  M 18 where Xbj  cjjyi is the output y backprop agated to the jth input As we pointed out above even though all the simulations that we report are performed using this rule a decoupled version of it 1  M which is completely local at the synaptic level would also work for the orthogonality of the outputs Oja's rule would just exhibit worse missad justment and generally slower convergence A de tailed analysis of the convergence behavior of these algorithms is beyond the purpose of this paper and it will be reported elsewhere Figure 4 shows the resulting orthogonal projec tions yl\(t  yd\(t for M  4 a  100 and 1000 examples uniformly distributed in 0.5,0.5 presented 100 times with p  0.003 Note how the sigmoid has still a linear region that is used by the various stages in the backward reconstruction of 2 The last graph in the figure shows the almost perfect reconstruc tion on the whole range Note that only 3 orthogo nal projections, with an oscillatory behavior emerge and are sufficient for an almost perfect reconstruc tion of t Figure 5 shows the results of the same simulation with M  6 Q  lo7 and 1000 examples presented 300 times with p  0.00003 The sigmoid is practically a step function and the orthogonal pro jections that emerge show the oscillatory behavior necessary to reconstruct the input The last graph shows the reconstruction of the input It is impor tant to note that the accuracy of the solution is due mainly to the density of the examples in the input space The aelf-organizing network zooms-in the de tails of the input space up to the resolution of the examples presented To demonstrate this effect more clearly we have performed two simulations with only 5 examples Oja's rule Acij  i\(xjy~-cijy j  1  N i  whose results are shown in Figure 6 The input space is now very sparse and the network converges to a configuration that guarantees almost perfect reconstruction of those points leaving the other re gions filled in by the interpolation provided by the final network configuration Figure S\(a is the result of 5 examples presented 20000 times to a network with A4  4 Q  100 and p  0.003 The result ing functions are orthogonal on the example set and are obviously less constrained providing good recon struction only on the input points The results of same simulation are repeated in Figure 5\(b for a very sharp sigmoid having Q  lo7 More simulations have been performed on multi dimensional examples confirming what we see from the one dimensional case The orthogonal projec tions that emerge guarantee an almost perfect re construction at the sample points More projections can be obtained at each stage by increasing the  degree of nonlinearity of the sigmoids IV CONCLUSIONS The framework provided by this paper provides a solid approach to the design and implementation of a self-organizing feature map The network struc ture based on standard linear combiners and sig moidal functions, derives its learning algorithms from a clear definition of the cost functions and uses only localized learning rules of the Hebbian type The network spans the whole spectrum ranging from linear principal components to nonlinear or thogonal projections The slope of the sigmoidal functions, or the net energy of the data at each node determines the degree of nonlinearity By increasing the size of the network the feature map zooms in the details of the input space up to density of the examples The feature map can be used as a pre-processing stage followed by a linear filter Palmieri 1993 for any adaptive nonlinear mapping and provides a promising alternative to backpropagation networks V REFERENCES Raldi P and K Kornik 1989 Neural Networks and Princi pal Component Analysis Learning from Examples Without Local Minima Neural Neiworks Vol 2 pp 53-58 Bourland H and Y Kamp 1988  Auto-Association by Multi-Layer Perceptronsand Singular Value Decomposition Biological Cybemctics Vol 59 pp 291-294 Foldiak P 1988 Adaptive Network for Optimal Linear Feature Extraction Neural Networks Vol 2 pp 459473 Haykin S 1986 Adaptive Filier Theory Pnntice Hall Hrycej T 1990 Self-Organization by Delta Rule Pro ceeding of the Intemaiional Joint Conference on Neural Neiworks San Diego CA pp 307-312 J Karhunen and J Joutsensalo 1993 Nonlinear General ieation of Principal Component Learning Algorithms Pmc 1262 


I 1 0.5 0 0.5 Figure 4 Resulting mappings for M  4 Q  100 and 1000 examples of the International Joint Conference on Neural Networka Nagoya Japan October Kung S Y and K I Dimantaras 1990 A Neural Net work Learning Algorithm for Adaptive Principal Component Extraction APEX Proceedings oj International Confer ence on Acourtics Speech and Signal Processing Vol 2 pp Lwker R l986 om Basic Network Principles to Neural Architectm Emergence of Spatial-Opponent Cells Proc National Acodemi of Science USA Neurobiologq Vol 83 pp 7508-7512 Palmieri F J Zhu and C Chang 1993 Anti-Hebbian Learning in Topologically Constrained Linear Networks A htorial IEEE Trans on Neural Networks Vol 4 N 5 Sept Palmieri F J Zhu 1993a The Behavior of a Single Self Associative Neuron Proc of the World Congress on Neural Networkr Portland OR July Palmieri F J Zhu 1993b Hebbian Learning in Linear Networks A Review Tech Rep 5-93 Dept of Electrid and Systems Eng The University of Connecticut Storm CT 062693157 Palmid F 1993 Linear Self-Association for Universal Memory and Approximation Proc of the World Congrerr on Neural Networkr Portland OR July Xu L 1993 Least Mean Square Error ReconstructionPrin ciple for Self-organization Neural Networks Vol 6 861-864 1 I 1.5 0 0.5 Figure 5 Resulting mappings for M  4 Q  lo7 and 1000 examples I        I 0.5 0 0 Figure 6 Resulting mappings for M  4 a  100 a and Q  lo7 b with 5 examples 1263 


Lines 3-9 loop selects an item for the next node and solve it More specifically it selects an item ifm for next node as show in line 3 If there is no node selected it goes to line 10 Otherwise it enters the loop body A new base is calculated at line 4 the infDoIfem method is called and the new-tail is set Then line 7 solves the sub node Upon returning from the sub node it adds the updated newsinfinto the infat line 8 and also saves the new-mf by method AddMf;/nf at line 9 It returns the mfi of the node at line IO I  Recursively find mfi  param base The tidSet for Current head  param tail The possible extension of the head  Bparam ginf The global information return The local maximal frequent itemsets I private Vector infMfi\(int[l base Shorts tails Vector ginfl 1  pep  voata.calSuglbase.tails 2 TInf inf  new TInfrainf oeo tails  3 while\(\(itm=inf.selectill~=Ol 4 int[l newbase  vOata.getBaSeiba5e.itml 5 Vector newginf=inf.DoItem\(itml 6 Shorts newtail=new Shorts\(inf.tail1 7 Vector newmfi=infMfiinewbase,newtail,newginfl 8 inf AddInfa\(newginfl  9 inf AddMfiInf newmfil  10 return inf.mfi Figure 10 The infMfi method For the node at the level 0 the local new-mf is actually maximal frequent itemsets and can output directly into a file Since its information for future searching is saved by the method inJAddMfilnf in line 9 there is no need to keep the new-mfi and the memory of new-mfi can he released 5 Experimental Results 10 1 0.1 0.01 Mnirmrn Support  Figure 11 Running time on Mushroom We compare SmartMiner with Mafia and GenMax All of them are implemented in Java JDK1.3 For fair comparison the three methods use the same vertical data model VData As we discussed before there are many ways to implement the vertical data model In this paper our purpose is to study the efficiency of different search strategies We choose VData because it takes less memory and is easy to implement The experiment was done on a lGhr Celeron with 512 MB of memory A detailed comparison of SmartMiner with Mafia and GenMax was conducted on two datasets Connect-4 and Mushroom Figure 11 shows the performance comparison of the three methods on Mushroom All three methods use the PEP pruning technique Our running time does not include the input time but does include the output time The horizontal axis shows minimum support in percentage The vertical axis is the running time in seconds In general SmartMiner is one order of magnitude faster than both Mafia and Genmax When minimal support is high Mafia is faster than Genmax Low minimal support increase the number of MFI, then Genmax performs better than Mafia 1000 10 1 0.1 0.01 Mnirmrn Support  Figure 12 Search tree size on Mushroom Figure 12 compares the sizes number of nodes in a tree of the search trees for the three methods From the figure we noticed that Genmax generates 10 times more nodes than SmartMiner and also much more than Mafia This indicates that the static ordering in GenMax is not as efficient as the dynamic reordering used by both SmartMiner and Mafia Moreover we noticed that SmartMiner generates less nodes than Mafia which is due to the heuristic select function used the SmarMiner 10000 I 1 5 1000   a 100 0 10 1 0.1 0.01 Minimum Support  Figure 13 the  of counting on Mushroom Figure 13 compares the number of support counti.ng which shows the number of times that the private method  base short item in VData is called As shown in Figure 13, Genmax calls the calSup methods significantly more than both SmartMiner and Mafia 576 


Further SmartMiner needs less number of support counting than Mafia Since GenMax introduces a fast superset checking algorithm the performance gain of dynamic reordering of Mafia is mitigated by the increasing time for superset checking when the set of MFI becomes large This is the reason we see in Figure 10 and Figure 13 that Mafia is better than Genmax when minimal support is high and the reverse when minimal support is low 10000 P I    smrtMinx 95 90 80 70 60 50 40 30 20 10 Mnirmm Support X Figure la Running time on Connect Figure 14 shows the performance comparison of the three methods for the Connect dataset Again we noticed the significant performance improvements of SmartMiner than Mafia and GenMax 6 Conclusion In this paper we propose the SmartMiner algorithm to find exact maximal frequent itemsets for large datasets The SmartMiner algorithm is able to take advantage of the information gathered from previous steps to search for MFI First it gathers global and local tail information and uses an heuristic select function to reduce the search tree Second the passing oftail information eliminates the need of known MFllfor supersel checking Smartminer does not require superset checking which can be very expensive Finally SmartMiner also reduces the number of support counting for determining the frequency of tail items and thus greatly saves counting time Our experiments reveal that the SmartMiner algorithm yields an order of magnitude improvement over Mafia and GenMax in generating the MFI on the two datasets Acknowledgements The authors wish to thank Professor Mohammed J Zaki for stimulating discussion in the performance study References I R Agrawal and R Srikant Fast algorithms for mining association des In Proceedings of the 20th VLDB Conference Santiago, Chile, 1994 2 R Agarwal C Agganval and V Prasad A tree projection algorithm for generation of frequent itemsets. Journal of Parallel and Distributed Computing 2001 3 Roberta Bayardo Efficiently mining long patterns from databases In ACM SIGMOD Conference 1998 4 D Burdick M. Calimlim, and J Gehrke MAFIA a maximal frequent itemset algorithm for transactional databases In Intl. Conf on Data Engineering Apr 2001 5 K Gouda and M J Zaki Efftciently Mining Maximal Frequent Itemsets Proc of the IEEE Int Conference on Data Mining, San Jose 2001 6 J Han I Pei and Y Yin Mining Frequent Patterns without Candidate Generation, Proc 2000 ACM-SIGMOD Int Conf on Management of Data SIGMODOO Dallas TX May 2000 7 Heikki Mannila Hannu Toivonen and A Inken Verkanw Eflicient algorithms for discovering association rules In KDD 94 AAAl Workshop on Knowledge Discovery in Databases pages 181-192, Seattle Washington July 1994 8 I S Park M Chen, and P S Yu An effective hash based algorithm for mining association rules In Proc ACM SIGMOD Intl Conf Management of Data May 1995 9 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules In 7th Intl Conf on Database Theory January 1999 IO 1 Pei, J Han, and R Mao Closet An efficient algorithm for mining frequent closed itemsets In SIGMOD Int'l Workshop on Data Mining and Knowledge Discovery May 2000 ll]Brin,S.;Mohvani,R.;Ullman,J.;andTsur,S 1997 Dynamic Itermet Counting and Implication Rules for Market Basket Data In Proc of the 1997 ACM-SIGMOD Conf On Management of Data 255-264  121 Ashok Sarasere Mward Omiecinsky, and Shamkant Navathe An eflicient algorithm for mining association rules in large databases In 21st Int'l Conf on Very Large Databases VLDB ZTrich, Switzerland, Sept. 1995  131 Hmu Toivonen. Sampling large databases for association rules In Proc of the VLDB Conference Bombay India September 1996 14]M I Zaki S Parthasarathy M Ogihara and W Li New algorithms for fast discovery of association rules In 3rd Intl Cod on Knowledge Discovery and Data Mining August 1997  151 M J Zaki and C. Hsiao Charm An efficient algorithm for closed association rule mining In Technical Report 99-1 0 Computer Science, Rensselaer Polytechnic Institute, 1999  161 Q Zou W Chu D Johnson and H Chiu A Panern Decomposition \(PD\Algorithm for Finding All Frequent Panems in Large Datasets Proc of the IEEE Int. Conference on Data Mining, San lose 2001 17]Q Zou W Chu D Johnson and H Chiu Panern Decomposition Algorithm for Data Mining of Frequent Patterns Joumal of Knowledge and Information System Volume 4 page 466482.2002 577 





BAT841 CAR881 I COD701 COD721 COL891 871 LAM89 J LEC88 MAC851 W861 Alashqur A.M Su S.Y.W and Lam H A Rule-based Language for Deductive Object Oriented Databases Proceedings of the 6th Inter national Conference on Data Engineering Los Angeles CA Feb 5-9 1990 Bancilhon F et al FAD a Powerful and Si ple Database Language Proceedings of the 13th VLDB Conference Brighton 1987 Banerjee J et al Queries in 8ject-oriented Databases Proceedings of the 4th Intl Confer ence on Data Encrineerine Los Angeles CA  97-105 I I  1988 pp 31-38 Batory D.S and Buchmann A.P Molecular Obiects Abstract Abstract Data Tvws and Data M6deIsi A Framework Proceedings Intl Confer ence on VLDB 1984, pp 172-184 Carey M J et al A Data Model and Query Language for EXODUS ACM-SIGMOD Confer ence 1988, pp. 413-423 Chuang H S Operational Role Processing in a Prototype OSAM KBMS Master's thesis University of Florida 1990 Codd E A Relational Model of Data for Large Shared Data Bank CACM Vol 13 No 6 1970 pp 377-387 Codd E R:lational Completeness of Database Sublanguages in Data Base Systems Rustin R ed Prentice-Hall Inc Englewook Cliffs NJ Colby L S A Recursive Algebra and Query Optimization for Nested Relations ACM SIGMOD Conference 1989 pp 273-283 Fishman D H et al Iris An Object-Oriented Database Management System ACM Transaction on Office Infomation Systems 51 1987 pp49 69 Goldberg A Introducing the Smalltalk-80 Sys tem Byte Aug 1981. pp.14-26 Hammer M and Mcleod D Database Descrip tion with SDM A Semantic Association Model Kim W et al Composite Object Support in an Object-oriented Database System Proceedings of King R Sembase A Semantic DBMS the Proceedings of the First Intemational Workshop on Expert Database Systems Oct 1984, pp.151 171 Lam H et al Prototype Implementation of an Object-oriented Knowledge Base Management System Proceedings of PROCIEM 89 Orlando Lecluse C Richard P and Velez F 02 an Object-Oriented Data Model ACM-SIGMOD Conference 1988, pp 425-433 MacGregor R ARTEL--A Semantic Front-End to Relational DBMSs Proceedings of VLDB 85 Atlanta GA April 1988 Maier D et al Development of an Object oriented DBMS Proc of OOPSLA 86 Confer ence Sept 29  Oct 2 Portland Oregon pp 472-482 1971 pp.65-98 ACM TODS Vol 6 NO 3 1981 pp 351-368 OOPSLA, Oct 4-8 1987 FL pp 118-125 FL NOV 13-15, 1989 SU86 SUSS SU89 ZD087 Manola F and Dayal U PDM An Object Oriented Model Int'l Workshop On Object Oriented Database Systems 1986, pp 18-25 Pant S An Intelligent Schema Design Tool for OsAM Master's thesis University of Florida 1990 Rowe L A and Stonebraker M R The POSTGRES Data Model Proceedings of the 13th VLDB Conference Brighton 1987 pp 83-96 Shaw G M and ZdoNc S B A Query Alge bra for Object-Oriented Databases IEEE Trans on Data Engineering 12\(3 pp 154-162 Feb 1990 Shipan D The Functional Data Model and the Data Language DAPLAX ACM Trans Database System ql March 1918 Singh M Transaction Oriented Rule Processing in an Object-Oriented Knowledge Base Manage ment System Master's thesis University of Florida 1990 Su S.Y.W Modeling Integrated Manufacturing Data With SAM IEEE Computer January Su S.Y.W et al An Object-oriented Comput ing Environment for Productivity Jmprovement in Automated Design and Manufacturing Project Summary PROCIEM 88,Orlandq E Nov 14-15 1988 Su S.Y.W Kxishnamurthy V and Lam H An Object-oriented Semantic Association Model OsAM AI in Industrial Engineering and Manufacturing Theoretical Issues and Applica tions Kumara S Soyster A.L and Kashyap R.L eds American Institute of Industrial Engineering 1989 Su S.Y.W Guo M and Lam H Association Algebra A Mathematical Foundation for Object Oriented Databases to be submitted to Trans on Knowledge and Data Engineering Tsurt S and Zaniolo C An Implementation of GEM  Supporting a Semantic Data Model on a Relational Back End Proceedings of the ACM SIGMOD Jntl Conference on the Management of Data 1984 pp 286-295 Frederick Ty The Design and Implementation of a Graphics Interface for an Object-oriented Language Master's thesis University of Florida 1988 Zaniolo C The Database language GEM Proceedings of the ACM SIGMOD Intl Confer ence on the Management of Data 1983 Zdonik S B Skarra A H and Reiss S P An Object Server for an Object-oriented Database System Intemational Workshop on Object oriented Database Systems Pacific Grove CA Sept 1986 Zdonik S.B The implementation of a Shared Clustered Memory System for an 0-0 Database System ACM Trans on Office Information Sys tems Apr 1987 1986 pp.34-49 32 I 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


