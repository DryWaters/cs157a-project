Text Document Categorization by Term Association Maria-Luiza Antonie University of Alberta Canada luiza@cs.ualberta.ca Abstract A good rexr classifer is a classifier that eflcienrly car egorizes large sers of rexr documenrs in a reasonable rime frame and with an acceptable accuracy and rhar provides classification rules rhar are human readable for possible fine-tuning If rhe rraining of the classifer is also quick rhis could become in some application domains a good as ser for the classiJer Many rechniques and algorithms 
for automatic rexr caregorizarion have been devised According to published lirerarure some are more accurare rhan others and some provide more inrerprerable classifcarion models rhan orhers However, none can combine all the beneficial properties enumerared above In rhis paper we present a novel approach for auromaric rexr caregorizarion that bor rows from marker basker analysis techniques using associ arion rule mining in rhe data-miningfield We focus on WO majorpmblems 1 the besr rem associarion rules in a rexrual darabase by generaring and pruning and 
2 using rhe rules IO build a text classifer Our rexr carego rization merhod proves ro be eflicienr and effecrive and er perimenrs on well-known collecrions show rhar rke classifer performs well In addition, training as well as classificarion are borh fasr and rhe generared rules are human readable 1 Introduction Automatic text categorization has always been an impor tant application and research topic since the inception of digital documents Today, text categorization is a necessity due to the very large amount of 
text documents that we have to deal with daily A text categorization system can he used in indexing documents to assist information retrieval tasks as well as in classifying e-mails, memos or web pages in a yahoo-like manner. Needless lo say automatic text catego rization is essential The text classification task can he defined as assigning category labels to new documents based on the knowledge gained in a classification system at the training stage. In the training phase we are given a set of documents with class Osrnar R Zai'ane 
University of Alberta, Canada zaiane@cs.ualberta.ca labels attached. and a classification system is built using a learning method. Classification is an important task in both data mining and machine learning communities, however most of the learning approaches in text categorization are coming from machine learning research Recent studies in the data mining community proposed new methods for classification employing association rule mining  12 131 These associative classifiers have proven to be powerful and achieve high accuracy. However they were only implemented and tested on small numerical datasets from the UCI archives 19 In this work 
we present a new classification method for text that takes advantage of association rule mining in the learning phase and makes the following contributions First a new rechnique for rexr caregorizarion rhar makes no as sumprion of rem independence is proposed This method proves ro perform as well as orher merhods in the lirerarure Second it is fasr during borh training and caregorizarion phases Third rhe classifierrhar is built using ourappmach can be read understood and modijied by humans 
Exper iments show that the effectiveness of the classifier can he improved by manually fine tuning the classification rules generated during the training phase The resulting classi fier is able to perform both single-class classification by which each document is assigned a unique class label, and multiple-class classification by which a document could be classified in many classes simultaneously Our experiments are performed on text databases however this doesn't limit the use of our classifier to text documents It can he ap plied in addition to images or any other database that can be 
modelled as a transactional database 2 The remainder of the paper is organized as follows Sec tion 2 gives an overview of related work in automatic text categorization and in association rule mining. In Section 3 we introduce our new text categorization approach Experi mental results are described in Section 4 along with the per formance of our system compared to known systems We summarize our research and discuss some future work di rections in Section 5 0-7695-1754-4102 17.00 0 2002 IEEE 19 


2 Related work Many text classifiers have been proposed in the litera ture using machine learning techniques. probabilistic mod els, etc They often differ in the approach adopted decision trees ndve-Bayes rule induction, neural networks. nearest neighbors and lately suppon vector machines Although many approaches have been proposed, automaled text cate gorization is still a major area of research primarily because the effectiveness of current automated text classifiers is not faultless and still needs improvement A classifier is built by applying a learning method to a training set of objects. This model is further used to predict thelabelstonewincomingobjects With all theeffon in this domain there is still place for improvement and a great deal of attention is paid to developing highly accurate classifiers The use of association rule mining for building classifi cation models is very new Recent studies have proposed the use of association rules in building effective classifiers for numerical data These classification systems discover the strongest association rules in the database and use them to build a categorizer In the following subsections a more detailed overview of the related work is presented from both domains that merge in our research text categorization and association rule mining 2.1 Text categorization The automatic text classification problem can be defined as building a classification model to assign one or more pre defined classes to new documents Text categorization re search has a long history starting in the early 1960s Nowa days with all the textual information on the Web or in com panies intranets, text categorization has revived and there is more demand for effective and efficient classification mod els Most of the research in text categorization comes from the machine learning and information retrieval communi ties Rocchio's algorithm 181 is the classical method in in formation retrieval being used in routing and filtering doc uments Researchers tackled the text categorization prob lem in many ways. Classifiers based on probabilistic mod els have been proposed starting with the first presented in literature by Maron in 1961 and continuing with n~ve Bayes IO that proved to perform well ID3 and C4.5 are well-known packages whose cores are making use of deci sion trees to build automatic classifiers 5 6 91 K-nearest neighbor k-NN is another technique used in text catego rization ZO Another method to construct a text categoriza tion system is by an inductive rule learning method This type of classifiers is represented by a set of rules in disjunc tive normal form that best cover the training set 14 11.31 As reported in  181 the use of bigrams improved text cate gorization accuracy as opposed to unigrams use In addi tion in the last decade neural networks and suppon vector machines SVM\were used in text categorization and they provedto be powerful tools 16.21.9 2.2 Association Rule Mining 2.2.1 Association Rules Generation Association rule mining is a data mining task that discov ers relationships among items in a transactional database Association rules have been extensively studied in the lit erature The efficient discovery of such rules has been a major focus in the data mining research community From the original apriori algorithm I there has been a remark able number of variants and improvements culminated by the publication the FP-Tree growth algorithm 71 How ever most popular algorithms designed for the discovery of all types of association rules, are apriori-based Formally, association rules are defined as follows Let I  il,iz  im be a set of items Let V be a set of transactions. where each transaction Tis a set of items such that T I Each transaction is associated with a unique identifier TID A transaction T is said to contain X a set of items in I if X C T An associarion ride is an impli cation of the form X  Y where X C I Y G Z and XnY TheruleX  Yhasasupporrsinthetransac tion set V ifs of the transactions in D contain X U Y In other words, the support of the rule is the probability that X and Y hold together among all the possible presented cases It is said that the rule X j Y holds in the transaction set 2 with confidence c if c of transactions in U that contain X also contain Y In other words the confidence of the rule is the conditional probability that the consequent Y is true under the condition of the antecedent X The problem of discovering all association rules from a set of transactions V consists of generating the rules that have a sirpporr and confidence greater than given thresholds These rules are called srmng rules The main idea behind apriori algorithm is to scan the transactional database searching for k-itemsers \(k items be longing to the set of items I As the name of the algorithm suggests it uses prior knowledge for discovering frequent itemsets in the database The algorithm employs an iter ative search and uses k-itemsets discovered to find k+l itemsets The frequent itemsets are those that have the sup pon higher than a minimum threshold 2.2.2 Associative classifiers Besides the classification methods described above re cently, and parallel to our work on associative text catego rization a new method that builds associative general clas 


Figure 1 Construction phases for an association-rule-based text categorizer sifiers has been proposed In this case the learning method is represented by the association rule mining The main idea behind this approach is to discover strong patterns that are associated with the class labels The next step is to take ad vantage of these patterns such that a classifier is built and new objects are categorized in the proper classes WO such models were presented in the literature CMAR 12 and CBA 13 Although both of them proved to be effective and achieve high accuracy on relatively small UCI datasets 19 they have some Limitations Both models perform only single-class classification and were not im plemented for text categorization In many applications however and in text categorization in particular multiple class classification is required In our paper we try to over come this limitation and construct an associative classifica tion model that allows single and multiple-class categoriza tions of text documents based on term co-frequency counts i.e a probabilistic technique that doesn\222t assume term in dependence 3 Building an Associative Text Classifier In this paper we present a method to build a categoriza tion system that merges association rule mining task with the classification problem This model is graphically pre sented in Figure 1 Given a data collection a number of steps are followed until the classification model is found Data preprocess ing represents the first step At this stage cleaning tech niques can be applied such as stopwords removal stem ming or term pruning according to the TFKDF values \(term frequencylinverse document frequency The next step in building the associative classifier is the generation of as sociation rules using an apriori-based algorithm Once the entire set of rules has been generated an important step is to apply some pruning techniques for reducing the set of association rules found in the text corpora The last stage in this process is represented by the use of the associa tion rules set in the prediction of classes for new docu ments The first three steps belong to the training pro cess while the last one represents the testing or classifi cation phase More details on the process are given in the subsections below If a document D is assigned to a set of categories G  cl,cz  c and after word pruning the set of terms T  tl,tz An is retained the following transaction is used to model the document D  cl,cp _ cm,tl,tp  t and the association rules are discovered from such transactions representing all docu ments in the collection The association rules are however constrained in that the antecedent has to he a conjunction of terms from T while the consequent of the rule has to a member of C 3.1 Data Collection Preprocessing In our approach we model text documents as transac tions where items are words or phrases from the document as well as the categories to which the document belongs as described above A data cleaning phase is required to weed out those words that are of no interest in building the asso ciative classifier We consider stopwording and term prun ing as well as the transformation of documents into trans actions as a pre-processing phase Stopword removal and term pruning is done according to the TFnDF values and a given list of stopwords We have opted to selectively turn on and off stopwording depending upon the data set to catego rize, It is only after the terms are selected from the cleansed documents that the transactions are formed The subsequent phase consists of discovering association rules from the set of cleansed transactions 3.2 Association Rule Generation In our algorithm as we shall see in this section we take advantage of the apriori algorithm to discover frequent tern-sets in documents Eventually these frequent item sets associated with text categories represent the discrimi nate features among the documents in the collection The association rules discovered in this stage of the process are further processed to build the associative classifier Using the apriori algorithm on our transactions repre senting the documents would generate a very large number of association rules most of them irrelevant for classifica tion We use an apriori-based algorithm that is guided by the constraints on the rules we want to discover Since we are building a classifier we are interested in rules that indicate a category label rules with a consequent being a category label In other words, given the document model described 21 


above we are interested in rules of the form T  c where T c T and c C C To discover these interesting rules ef ficiently we push the rule shape constraint in the candidate generation phase of the apriori algorithm in order to retain only the suitable candidate itemsets Moreover at the phase for rule generation from all the frequent k-itemsets we use the rule shape constraint again to prune those rules that are of no use in our classification There are two approaches that we have considered in building an associative text classifier The first one ARC AC Association Rule-based Classifier with All Categories 22 is to extract association rules from the entire training set following the constraints discussed above As a result of discrepancies among the categories in a text collection of a real-world application, we discovered that it is diffi cult to handle some categories that have different charac teristics small categories, overlapping categories or some categories having documents that are more correlated than others As a result we propose a second solution ARC-BC Associative Rule-based Classifier By Category that solves such problems In this approach we consider each set of documents belonging to one category as a separate text col lection to generate association rules from If a document belongs to more than one category this document will be present in each set associated with the categories that the document falls into The ARC-BC algorithm is described in more detail below Algorithm ARC-BC Find association rules on the training set ofthe text collection when the text corpora is divided in subsets by category Input A set of documents D of the form D  ci;t~,l2 Am where e is the category attached to the docu ment and 1 xe the selected terms for the document A minimum support threshold A minimum confidence threshold Output A set of association rules of the form t 1 A 12 A  A t 3 ct where c is the category and tj is a term Method I 2 3 4 C  F,-I w Ft-1 5 C  C  c 1 i  1 item-set of c 6 D  FilterTub/e\(D,-1 KI 7 fareach document d in D do  8 9 10  11  12 13 1 14 SetseUi{cEF,li>l 15 R=O 16 17 CI  Candidate I term-setsand their suppml FI  Frequent 1 term-sets and their support for i  2 R  0 i  i  1 do F+-I foreach c in Ci do  c.support  c.support  CounNc d F  c E C I c.support  U fareach itemset I in Sets do  R  R I  Cat 18  In ARC-BC algorithm step 2 generates the frequent 1 itemset In steps 3-13 all the k-frequent itemsets are gen erated and merged with the category in C1 Steps 16-18 generate the association rules The document space is re duced in each itereation by eliminating the transactions that do not contain any of the frequent itemsets This step is done by FilterTable\('D,-1 F,-I function Table 1 presents a set of rules that are discovered in the text collection Such rules are composing the classifier Al though the rules are human readable and understandable if the amount of rules generated is too large it is time consum ing to read the set of rules for further tuning of the system This problem leads us to the next subsection where prun ing methods are presented Although the rules are similar to those produced using a rule-based induced system, the approach is different In addition, the number of words be longing to the antecedent could he large in our experiments up to IO words\while in some studies with rule-based in duced systems the mles generated have only one or a pair of words as antecedent 3 3.3 Pruning the Set of Association Rules The number of rules that can he generated in the asso ciation rule mining phase could he very large There are two issues that must be addressed in this case One of them is that such a huge amount of rules could contain noisy in formation which would mislead the classification process Another is that a huge set of rules would make the classifi cation time longer This could be an imprtant problem in applications where fast responses are required The pruning methods that we study in this paper are the following: eliminate the specific rules and keep only those that are more general and with high confidence and prune unnecessary rules by database coverage Let us introduce the notions used in this subsection by the following defini tions Definition 1 Being given two rules TI  C and 2'2  C we say that the first rule is more general if TI The first step of this process is to order the set of rules This is done accordingly to the following ordering defini tion Definition 2 Being given two rules Rl and Rz RI is higher ranked than Rz if 1 RI has higher confidence than Rz 2 if the confidences are equal, supp\(R1 must exceed supp\(Rz 3 both confidences and support are equal but R1 has less attributes in left hand side than R2 With the set of association rules sorted the goal is to select a subset that will build an efficient and effective clas sifier In our approach we attempt to select a high quality subset of rules by selecting those rules that are general and Tz 22 


agriculture A depmment A grain  corn assistance A bank A england A market A money  inlmesl acute A coronary A function A left A ventricular arnbvlatoly A ischemia A myocardial myocardial-infarction coronary-disease Table 1 Examples of association rules composing the classifier have high confidence. The most significant subset of rules is finally selected by applying the database coverage The algorithm for building this set of rules is described below Algorithm Pruning the set of association rules Input The set of association rules that were found in the association rule mining phase S and the training text col lection D Output A set of rules used in the classification process Method sort the rules according to Definition 1 foreach rule in the set S find all those rules that are more specific according to Definition 2 prune those that have lower confidence a new set of rules S\222 is generated foreach rule R in the set S\222 go over D and find those transactions that are covered by the rule R if R classifies correctly at least one transaction remove those cases that were covered by R select R 3.4 Prediction of Classes Associated with New Documents The set of rules that were selected after the pruning phase represent the actual classifier This categorizer will be used to predict to which classes new documents are attached Given a new document, the classification process searches in this set of rules for finding those classes that are the clos est to be attached with the document presented for cate gorization This subsection discusses the approach for la belling new documents based on the set of association rules that forms the classifier A trivial solution would be to attach to the new document the class that has the most rules matching this new docu ment or the class associated with the first rule that apply to the new object. However in the text categorization domain multi-class categorization is an important and challenging problem that needs to be solved. In our approach we give a solution to this problem by introducing the dominance fac tor By employing this variable we allow our system to as sign more than one category The dominance factor 6 is the proportion of rules of the most dominant category in the applicable rules for a document to classify Given a docu ment to classify, the terms in the document would yield a list of applicable rules If the applicable rules are grouped by category in their consequent part and the groups are or dered by the sum of rules\222 confidences the ordered groups would indicate the most significant categories that should be attached to the document to be classified We call this order category dominance hence the dominance factor 6 The dominance factor allows us to select among the candi date categories only the most significant When 6 is set to a certain percentage a threshold is computed as the sum of rules\222 confidences for the most domina1 category times the value of the dominance facrnr Then only those categories that exceed this threshold are selected TakeKClasses\(S.6 function selects the most k significant classes in the classi fication algorithm The next algorithm describes the classification of a new document Algorithm Classification of a new object Input A new object to be classified 0 The associative classifier ARC The dominance factor 6 The confidence threshold r Output Categories attached to the new object Method 1 2 3 4 if count  1 5 6 S-Sur 7 9 elseexit 10 divide S in subsets by category 4 Sz  S 11 foreachsubset S1,S  S 12 sum the confidences of rules and divide by the number of rules in S 13 if it is single class classification 14 put the new document in the class that has the highest confidence sum 15 else multi-class classification 16 TakeKClasses\(S.6 17 s  0 set of rules that match o foreach rule r in ARC \(the sorted set of rules if T c o  count  fr.conf  r.conf keep the first rule confidence else if r.conf  fr.conf-r 8 scsur assign these k classes to the new document 23 


4 Experimental Results and Performance Study 4.1 Text Corpora In order to be able to objectively evaluate our algorithm vis-a-vis other approaches like other researchers in the field of automatic text categorization we used the Reuters-21578 text collection  151 as benchmarks. This text database is de scribed below Text collections for experiments are usually split into two parts one part for training or building the classifier and a second part for testing the effectiveness of the system There are many splits of the Reuters collection we chose to use the ModApte version This split leads to a corpus of 12,202 documents consisting of 9,603 training documents and 3,299 testing documents. There are 135 topics to which documents are assigned However, only 93 of them have more than one document in the training set and 82 of the categories have less than 100 documents 22 Obviously the performances in the categories with just a few docu ments would be very low especially for those that do not even have a document in the training set Among the doc uments there are some that have no topic assigned to them We chose to ignore such documents since no knowledge can be derived from them Finally we decided to test our clas sifiers on the ten most populated categories with the largest number of documents assigned to them in the training set Other researchers have used the same strategy  171 which constrained us to do the same for the sake of comparison By retaining only the ten most populated categories we have 6488 training documents and 2545 testing documents On these documents we performed stopword elimination but no stemming 4.2 Experimental Results On this data set we tested our classification system ARC BC on a Pentium 111 700MHz dual processor machine run ning Linux Several measurements have been used in previ ous studies for evaluation. Some measures as well as those used in our evaluation, can be defined in terms of precision and recall The formulae for precision and recall are given below R   and P  5 The terms used to express precision and recall are given in the contingency table Table 2 For evaluating the effectiveness of our system we used the breakeven points The breakeven point is the point at which precision equals recall and it is obtained as reported in 4 When dealing with multiple classes there are two pos sible ways of averaging these measures namely macro average and micro-average In the macro-averaging one Category col classifier Yes assignments No human assignments Yes I NO a I b c 1 d Table 2 Contingency table for category car HEY Xlthl pN"1"I rn m d*.L upli"i WW px I h=l,l is VI 111 U I O U I Xlh xn XI Xl I XI e Xlll i  70J hX1 so 7"s nl 781 VI I i I 111  72 in 114 U Table 4 the results for the other classification systems are reported as given in 9 shows a comparison between our ARC-BC classifier and other well-known methods The measures used are precisiodrecall-breakeven point, micro average and macro-average on ten most populated Reuters categories. Our system proves to perform well as compared to the other methods It outperforms most of the conven tional methods but it does not perform better than SVM In addition to these results our system has two more features First it is very fast in both training and testing phases see Table 6 The times reported are for all training and testing documents Second it produces readable and understand able rules that can be easily modified by humans \(see Table I Table 5 reports the improvements in the response of the system when human tuning was applied The support wds set to 20 which made corn and wheat categories to per form very poor By reading the rules we noticed that by adding 4 more rules for each of these categories the perfor 24 


most known classifiers ARC-BC suppc2036 6=90 rm-scdbcov BEP I initial set of NleS I manual tuned set of rules  I I I micm-avg 1 84.14 I M.62 mscmavg 1 63.55 74.41 Table 5 Micro-average PrecisionlRecall breakeven point for ten most populated Reuters categories  manual tuning of the classifier mances improved as presented in Table 5 A comparison between the pruning methods is given in Table 7 By applying the pruning methods the accuracy of the classifier is not improved However the reduction in number of rules represents a step further in manually or au tomatically tuning of the system 5 Conclusion and Future Work This paper introduced a new technique for text catego rization It employs the use of association rules Our study provides evidence that association rule mining can he used for the construction of fast and effective classifiers for auto matic text categorization We have presented an association rule-based algorithm for building the classifier ARC-BC that considers categories one at a time The algorithm as sume a transaction-based model for the mining documenl I suppon 11 training I testing 1 IOW 11 18 I 3 15 I1 9 I 2 Table 6 Training and testing time in sec onds with respect to the support threshold for Reuters-21570 dataset set The experimental results show that the association rule based classifier performs well and its effectiveness is com parable to most well-known text classifiers. One major ad vantage of the association rule-based classifier is its rela tively fast training time Moreover the rules generated are understandable and can easily be manually updated or ad justed if necessary The maintenance of the classifier is straight forward In the case of ARC-BC when new doc uments are presented for retraining only the concerned cat egories are adjusted and the rules could be incrementally updated The introduction of the dominance factor 6 allowed multi-class categorization However other feature selection techniques such as latent semantic analysis could improve the results by giving an insight on the discriminative fea ture among classes We are working on reducing the num ber of features thus better discrimination among classes is expected Currently the discovered rules consider the pres ence of terms in documents to categorize We are studying possibilities to take into account the absence of terms in the classification rules as well Table 4 PrecisionlRecall-breakeven point on ten most populated Reuters categories for ARC-BC and 25 


I BEP II ARC-BC with 6  50 and rupp=15 1 IO D Lewis Niive bayes at forty The independence assump tion in information retrieval In 10th European Conference on Machine Learning ECML-98 pages 4-15 1998 I I H Li and K Yamanishi Text classification using esc-based stochastic decision lists In Xrh ACM Inremarional Confer ence on Informarion and Knowledge Management\(C1KM 99 pages I22 130 Kansas City.USA 1999 I21 W Li I Han and 1 Pei CMAR Accurate and efficient c!assi!kation based on multiple class-association rules In IEEE Inrernarional Conference on Dara Mining ICDM'OIA San Jose California November 29-Dwmber 2 2001 I31 B Liu W Hsu and Y Ma Integrating classification and as sociation rule mining In ACMlnr Coni on Knowledge Dis covery and Dara Mining SIGKDD'9XA pages 8046 New York City NY August 1998 I41 1 Moulinier and J.4 Ganascia Applying an existing machine learning algorithm to text Categorization In S.Wemter E.Riloff and GScheler editors Connecfionisl srarisrica1,and symbolic approaches 10 learning for naru ral language processing Springer Verlag, Heidelberg Ger many 1996 Lecture Notes for Computer Science series number IMO I51 The reuters-21578 text categorization test collection http://www.research.att.co1nl~lewis/reuters2 1578.html I61 M Ruiz and P Srinivasan Neural networks for text catego rization In 22nd ACM SIGIR nlernariOMl Conference on Informarion Rerrieval pages 281-282 Berkeley CA USA August 1999 I71 F Sebastiani Machine learning in automated text cate gorization Technical Report IEI-B4-31-1999 Consiglio Nazionale delle Ricerche Pisa. Italy 1999 I81 C M Tan Y F Wang and C D Lee The use of bigrams to enhance text categorization Jour nal of Information Processing and Management 2002 http://www.cs.ucsb.edu yfwang/paperdig&m.pif I91 University of California irvine knowledge discovery in databases archive http://kdd.ics.uci.edd 20 Y Yang An evaluation of statistical approaches to text cat egorization Technical Report CMU-CS-97-127 Camegie mellon University, April 1997 211 Y.Yang and X.Liu A re-examination of text categorization methods In 22nd ACM Inremarional Conference on Re search and Developmenr in Informarion Retrieval SIGIR 99 pages 42-49 Berkeley.US, 1999 22 0 R Zaiane and M.-L. Antonie Classifying text documents by associating terms with text categories In Thirreenrh Aus tralasian Darabase Conference ADC'OZ pages 215-222 Melbourne Australia lanuan 2002  11 wlo pruning I rn-s I rn-s  db-cov 11 3072rules I 383rules I 127rules I IC II R99 I R42 I 76 6 I micro-avg 11 81.8 I 68.2 I 7 I 4 macro-avg 11 78.24 I 64.40 I 58.53 Table 7 PrecisionIRecall-breakeven point for ten most populated Reuters categories with different pruning methods References I R. Agrawal T Imielinski and A Swami Mining associa tion rules between sets of items in large databases In Proc 1993 ACM-SIGMOD Inr Conf Management of Dara pages 207-216 Washington, D.C., May 1993 Z M:L Antonie 0 R. Zaiane and A Coman Application of data mining techniques for medical image classification In Second lnrernarional ACM SIGKDD Workshop on Mul rimedia Dara Mining pages 94-101 San Francisco USA August 2001 131 C. Apte F Damerau and S Weiss Automated learning of decision rules for text categorization ACM Tronsnction on Informarion Sysrems 12\(3 1994 4 R Bekkerman R El-Yaniv N Tishby and Y Winter On feature distributional clustering for text categorization In Proceedings of SIGIR-01 24rh ACM lnrernarional Con ference on Research and Development in Informarion Re rrieval pages 146-153 New Orleans US 2001 5 W Cohen and H Hirsch Joins that generalize text clas sification using whirl In 4th lnrernnrional Conference on Knowledge Discovery and Data Mining SigKDD'98 pages 169-173 New York City,USA 1998 6 W Cohen and Y Singer. Context-sensitive learning methods for text categorization ACM Transactions on Informarion Sysrems 17\(2\-173 1999 7 J Han 1 Pei, and Y Yin Mining frequent patterns without candidate generation In ACM-SIGMOD Dallas. 2000 SI D A Hull Improving text retrieval for the routing problem using latent semantic indexing In 17rh ACM lnrernarional Conference on Research ad Development in Information Rerrievnl SIGIR-94 pages 282-289 1994 9 T Joachims Text categorization with support vector ma chines learning with many relevant features In 10th Euro pean Conference on Machine Learning ECML-98 pages 137-142 1998 26 


FIGURE 5 Execution time and rules returned versus minimum coverage for the various algorithms FIGURE 6 Execution time of dense_0002 as minconf is varied for both data-sets. Minimum coverage is fixed at 5% on pums and 1% on connect-4 FIGURE 7 Maximum confidence rule mined from each data-set for a given level of minimum coverage   1 10 100 1000 10000 100000 0 10 20 30 40 50 60 70 80 90 Execution time \(sec Minimum Coverage connect-4 apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 1e+06 0 10 20 30 40 50 60 70 80 90 Number of Rules Minimum Coverage connect-4 apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 0 10 20 30 40 50 60 70 80 90 Execution Time \(sec Minimum Coverage pums apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 1e+06 1e+07 0 10 20 30 40 50 60 70 80 90 Number of Rules Minimum Coverage pums apriori_c  dense_0002   dense_002   dense_02    0 500 1000 1500 2000 2500 3000 3500 20 25 30 35 40 45 50 55 60 65 Execution time \(sec minconf pums  connect-4  1 10 100 1000 10000 100000 1e+06 20 25 30 35 40 45 50 55 60 65 Number of Rules minconf pums  connect-4    0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100 Highest Rule Confidence Minimum Coverage pums  connect-4 


8.2  Effects of minimum confidence The next experiment \(Figure 6\ws the effect of varying minconf while fixing minimp and minsup to very low values. With connect-4, we used a minimum coverage of 1%, and with pums, a minimum coverage of 5%. Minimp was set to .0002 with both data-sets. As can be extrapolated from the previous figures, the number of rules meeting these weak minimp and minsup constraints would be enormous As a result, with these constraints alone, Dense-Miner exceeds the available memory of our machine The efficiency of Dense-Miner when minimum confidence is specified shows that it is effectively exploiting the confidence constraint to prune the set of rules explored. We were unable to use lower settings of minconf than those plotted because of the large number of rules. As minconf is increased beyond the point at which fewer than 100,000 rules are returned, the run-time of Dense-Miner rapidly falls to around 500 seconds on both data-sets 8.3  Summary of experimental findings These experiments demonstrate that Dense-Miner, in contrast to approaches based on finding frequent itemsets achieves good performance on highly dense data even when the input constraints are set conservatively. Minsup can be set low \(which is necessary to find high confidence rules as can minimp and minconf \(if it is set at all\This characteristic of our algorithm is important for the end-user who may not know how to set these parameters properly. Low default values can be automatically specified by the system so that all potentially useful rules are produced. Refinements of the default settings can then be made by the user to tailor this result. In general, the execution time required by Dense-Miner correlates strongly with the number of rules that satisfy all of the specified constraints 9.     Conclusions We have shown how Dense-Miner exploits rule constraints to efficiently mine consequent-constrained rules from large and dense data-sets, even at low supports. Unlike previous approaches, Dense-Miner exploits constraints such as minimum confidence \(or alternatively, minimum lift or conviction\ and a new constraint called minimum improvement during the mining phase. The minimum improvement constraint prunes any rule that does not offer a significant predictive advantage over its proper sub-rules. This increases efficiency of the algorithm, but more importantly it presents the user with a concise set of predictive rules that are easy to comprehend because every condition of each rule strongly contributes to its predictive ability The primary contribution of Dense-Miner with respect to its implementation is its search-space pruning strategy which consists of the three critical components: \(1\functions that allow the algorithm to flexibly compute bounds on confidence, improvement, and support of any rule derivable from a given node in the search tree; \(2\proaches for reusing support information gathered during previous database passes within these functions to allow pruning of nodes before they are processed; and \(3\ item-ordering heuristic that ensures there are plenty of pruning opportunities. In principle, these ideas can be retargeted to exploit other constraints in place of or in addition to those already described We lastly described a rule post-processor that DenseMiner uses to fully enforce the minimum improvement constraint. This post-processor is useful on its own for determining the improvement value of every rule in an arbitrary set of rules, as well as associating with each rule its proper sub-rule with the highest confidence. Improvement can then be used to rank the rules, and the sub-rules used to potentially simplify, generalize, and improve the predictive ability of the original rule set References 1 w a l  R.; Im ie lin ski  T   a n d S w a m i, A. 1 9 9 3   M i n i ng As so ciations between Sets of Items in Massive Databases. In Proc of the 1993 ACM-SIGMOD Int\222l Conf. on Management of Data 207-216 2 raw a l R.; M a n n ila, H Sri k an t  R T o i v o n en  H.; an d  Verkamo, A. I. 1996. Fast Discovery of Association Rules. In Advances in Knowledge Discovery and Data Mining AAAI Press, 307-328 3 K Ma ng a n a r is S a n d Sri k a n t, R 19 97  P a rtia l Cl a ssif i cation using Association Rules. In Proc. of the 3rd Int'l Conference on Knowledge Discovery in Databases and Data Mining 115-118 4 a rd o  R. J 1 9 9 8  Ef f i c i en tly Min i n g  Lo n g  P a ttern s fro m  Databases. In Proc. of the 1998 ACM-SIGMOD Int\222l Conf. on Management of Data 85-93 5  Mi c h ae l J. A a n d  Lin o f f G  S 1 9 9 7  Data Mining Techniques for Marketing, Sales and Customer Support John Wiley & Sons, Inc 6 Bri n, S  M o t w a n i, R.; Ullm a n J.; a n d  Tsu r S. 19 9 7 Dyn a m i c  Itemset Counting and Implication Rules for Market Basket Data. In Proc. of the 1997 ACM-SIGMOD Int\222l Conf. on the Management of Data 255-264 7 h e n  W   W   1 9 9 5 F a st Ef fecti v e Ru le In d u ctio n   In  Proc. of the 12th Int\222l Conf. on Machine Learning 115-123 8 In tern atio n a l Bu sin e s s Mac h in e s   1 9 9 6  IBM Intelligent Miner User\222s Guide Version 1, Release 1 9 m e t tin e n M   Ma nn ila  P  Ro nk a i ne n  P   a n d V e rk a m o  A  I. 1994. Finding Interesting Rules from Large Sets of Discovered Association Rules. In Proc. of the Third Int\222l Conf. on Information and Knowledge Management 401-407 10  Ng   R  T    L a k s hm ana n   V   S    Ha n  J   an d P a ng A  1 9 9 8   Exploratory Mining and Pruning Optimizations of Constrained Association Rules. In Proc of the 1998 ACM-SIGMOD Int\222l Conf. on the Management of Data 13-24 11 Ry mo n  R 1 9 9 2   Search  t h ro u g h Sy s t e m atic S e t En u m era tion. In Proc. of Third Int\222l Conf. on Principles of Knowledge Representation and Reasoning 539-550 1  Sha f e r  J  A g r a w a l R   an d Me ht a M 19 98  SPR I N T   A  Scalable Parallel Classifier for Data-Mining. In Proc. of the 22nd Conf. on Very Large Data-Bases 544-555 13  S m y t he P  and  Go od man   R  M 19 92 An I n f o r m at i o n Th eo retic Approach to Rule Induction from Databases IEEE Transactions on Knowledge and Data Engineering 4\(4\:301316 14  S r i k a n t   R    V u  Q an d Ag r a w a l  R  19 97 M i ni ng  A ssoc i a tion Rules with Item Constraints. In Proc. of the Third Int'l Conf. on Knowledge Discovery in Databases and Data Mining 67-73 15 W e bb, G. I 1 9 9 5 OP U S An Ef f i c i e n t Adm i ssible Algo rit h m for Unordered Search. In Journal of Artificial Intelligence Research 3:431-465 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


