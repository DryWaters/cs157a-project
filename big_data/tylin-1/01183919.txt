Mining Association Rules from Stars Eric Ka Ka Ng Ada Wai-Chee Fu Ke Wang Chinese University of Hong Kong Department of Computer Science and Engineering Sirnon Fraser University Department of Computer Science kkngl,adafu}@cse.cuhk.edu.hk Abstract Association rule mining is an important data mining problem It is found to be useful for conventional relational data Howevec previous work has mostly targeted on min ing a single table In real life a 
database is rypically made up ofmultiple tables andone imponant case is where some of the tables form a star schema The tables typically corre spond to entity sets and joining the tables in a star schema gives relationships among enriry sets which can be vev in teresting information Hence mining on the join result is an importantpmblem Based on characteristics of the star schema we propose an efficient algorithm for mining as sociation rules on the join 
result but without actually per forming the join operation We show that this approach can signifrrantly out-perform the join-then-mine approach even when the latter adopts a fastest known mining algorithm 1 Introduction Association rules mining AIS93, AS941 is identified as one of the important problems in data mining Let us first define the problem for a database D containing a set of transactions where each transaction contains a set of items An 
association rule has the form of X  Y where X and Y are sets of items In such a rule we require that the fre quency of the set of items X UY is above a certain threshold called the rninsup The frequency also known as support of a set of items Z is the number of occurrences of Z in D or the number of transactions in D that contain Z The con6dence 
of the rule should also be above a threshold By confidence we mean the probability of Y given X The mining can be divided into two steps first we find the sets of items that have frequencies above minsup which we call the frequent itemsets Second, from the sets of frequent itemsets we generate the association rules The first step is more difficult and is shown to he,NF-hard In our subsequent discussion we shall focus on the first step  wangk@cs.sfu.ca The techniques in association rule mining has been ex tended 
to work on numerical data and categorical data in more conventional databases SA96 RS981 some re searchers have noted the importance of association rule mining in relation to relational databases STAOO Tools for association rule mining are now found in major products such as IBMs Intelligent Miner, and SPSS's Clementine In real databases typically a number of tables will he defined In this paper we examine the problem of mining association rule from a set of relational tables In particu lar we are interested 
in the case where the tables form a star structure  see Figure 1 A star schema consists of a fact table FT in the center and multipledimension tables We aim to mine association rules on the join result of all the tables JSOO This is interesting because the join result typically tells us the relationship among different entities such as customers and products and to discover cross enti ties association can be of great value The 
star schema can be considered as the building block for a snowflake schema and hence our proposed technique can be extended to the snowflake structure in a straightforward manner Figure 1 Star with 3 Dimensional Tables At first glance it may seem easy to join the tables in a star schema and then do the mining process on the joined result However, when multiple tables are joined the resulting ta 0-7695-1754-4/02 17.00 Q 2002 IEEE 322 


ble will increase in size many folds There are two major problems Firstly in large applications often the join of all related tables cannot be realistically computed because of the many-to-many relationship blow up large dimension tables, and the distributed nature of data Secondly even if the join can be computed, the multi fold increase in bath size and dimensionality presents a bugh overhead to the already expensive frequent itemset mining step I The number of columns will be close to the sum of the number of columns in the individual tables As the performance of association rule mining is very sen sitive to the number of columns \(items\the mining on the resulting table can take much longer computation time com pared to mining on the individual tables 2 If the join re sult is stored on disk, the YO cost will increase significantly for mukipre scanning steps in data mining 3 For mining frequent itemsets of small sizes a large portion of the YO cost is wasted on reading the full records containing irrel evant dimensions 4 Each tuple in a dimension table will be read multiple times in one scan of the join result because of duplicates of the tuple in the join result We exploit the characteristics of tables in a star schema Instead of \224joining-then-mining\224 we can perform \224mining then-joining\224 in which the 224joining\224 part is much less costly Our strategy never produces the join result In the first phase we mine the frequent itemsets locally at each dimension table, using any existing algorithm Only rele vant information are scanned In the second phase, we mine global frequent itemsets across two or more tables based on local frequent itemsets Here we exploit the following pruning strategy if X U Y is a frequent itemset, where X is from table A and Y is from table B X must be a frequent itemset and Y must be a frequent itemset Thus, the first phase provides all local frequent itemsets we need for the second phase The difficulty lies in the second phase One major challenge in the second phase is how to keep track of the many-to-many relationship in the fact table without generating the join result We make use of the feature that a foreign key for a dimension table A can ap pear many times in the join result which allows us to intro duce some structure to record the key once, together with a counter for the number of duplications We also make use of the idea of semi-join in relational databases to facilitate the mining process. From these ideas we propose a set of al gorithm and data structures for mining association rules on a star schema which does not involve a join step of the ta bles involved. Experiments show that the proposed method is efficient in many scenarios 2 Problem Definition Consider a relational database with a star schema There are multiple dimension tables which we would de note as A E C  each of which contains only one pri mary key denoted by transaction id rid some other at tributes and no foreign keys. \(Sometimes we simply refer to A B C  as dimensions 0 bi ci denote the transaction id rid of dimension tables A B C respectively We as sume that the attributes in the dimension tables are unique If initially two tables have some common attributes re naming can make them different We assume that attributes take categorical values Numerical values can be parti tioned into ranges, and hence be transformed to categorical values RS98 The set of values for an attribute is called the domain of the attribute Conceptually we can view the dimension table in terms of a binary representation where we have one binary attribute or we call an 224item\224 corresponding to one 224attribute-value\222\222 pair in the original dimension table We also refer to each tuple in A or the binary representation as a transaction For example consider Figure 2 wl w2 w3 are attribute names for dimension table A and the value of attribute w1 for transaction 01 is Rz In the conceptual binary representation in Figure 2 we have attributes for 224wl  Ro\224 224vl  R1\224\224vl  R2\224  we call them zlr z2 23  respectively For transaction 01 in table A the value of attribute z3 is 1 81  R2 is TRUE and the values of 21 and 2\2212 both equal to 0 FALSE In our remaining dis cussions, binary items one item for each \223attribute-value\224 pair in the conceptual binary representation would be used xilzi2xi denotes the itemset that is composed of items zi xi z  We assume an ordering of items which is adopted in any transaction and iremser E.g z1 would al ways appear before x2 if they exist together in some uansac tion or itemset This ordering will facilitate our algorithm Figure 2 Dimension Table and its Binary Rep resentation There is one fact table which we denote as FT FT has attributes of tid\(A tid\(B tid\(C  where tid\(A is the tid of table A That is FT stores the rids from dimen sion tables A B C  as foreign keys. \(Later we shall dis cuss the more general case where FT also contains some other attributes In an ER model typically, each dimen sion table corresponds to an entity set and FT corresponds to the relationship among the entity sets The relationships among entity sets can be of any form many-to-many, many to-one one-to-many or one-to-one We are interested to mine association rules from the star structure. In particular we shall examine the sub-problem of 323 


finding all frequent itemsets in the table T resulting from a natural join of all the given tables FT W A W B W C    The join conditions are given by FT.Tid\(A  Adid FT.Tid\(B  B.tid FT.Tid\(C  C.tid  In the fol lowing discussions when we mention frequent itemset we always refer to the frequency of the itemset in the table T We assume that a frequency threshold of rninsup is given for the frequent itemsets A frequent item corresponds to a frequent itemset of size one In our mining process the dimension tables will he kept in the form of the VTV Vertical Tid-Vector representa tion SHSOO with counts Specifically suppose there are TA TB TC transactions in tables A B C respectively. For each frequent item x in table A we store a column of TA hits the it bit is 1 if item x is contained in transaction i and 0 otherwise We also keep an array of TA entries where the irA entry corresponds to the frequency of tid i in FT 3 The Proposed Method First we present a simple example to show the idea of discovering frequent itemsets across dimension tables with ant actually performing the join operation We shall use a data type called tiddist in our algorithm It is an ordered listof elementsoftheformtid\(count wheretidisatrans action ID and count is a non-negative integer Given two tidhts L1 Lz theunion L1 U Lz isthe listoftid\(count where tid appears in either L1 or L2 and the count is the sum of the counts of tid in L1 and L The intersection of two tidlists L1 Lz is denoted by L1 n L which is a listoftid\(count where tid appears in both L1 and Lz and the count is the smaller of the counts of tid in L1 and Lz Suppose we have 2 dimension tables A B and a fact table FT The following are some of the tiddists we shall use tid~\(xi  a tid-list for xi where x is an attribute  item  of table A In each element of tid\(count in the list lid is the id of a transaction in A that contains xi and count is the number of occurrences of the rid in FT If the tid of a transaction that contains xi does not appear in tidA\(xi the count of it is 0 in FT E.g tid~\(x3  a1\(5 Z means that the tids of transactions in A that contain x3 are al and a3 a1 appears 5 times in FT and a3 appears 2 times tidA X where X is an itemset with items from A it is similartotidA\(xi except xi isreplaced by X lidA\(r;x can beobtained bytidA\(xi ntidA\(xj B+ey\(a Given a tid a from A Bley\(a de notes a lid-list of tid\(count where tid is a tid from B and count is the number of Occurrences of tid together with a in FT E.g B-key\(al  b3\(4 2 means that a1b3 oc curs 4 times in FT and aibj occurs 2 times Blid\(xi Given an item xi in A Blid\(xi denotes a lid-list, of tid\(count where tid is a tid of E and count is the number of times tid appears together with any tid j of A such that transaction aj contains xi in A Blid\(X similar to Blid\(xi except item xi is re placed by an itemset X from A Example 3.1 Suppose we have a star schema for a num ber of dimension rabies related by a fact table FT The followingfigure shows 2 of the dimension tables A and B and the projection of FT on the two columns that contains transaction IDS for A B but without removing duplicate tuples B Some of the iids fmm the dimension tables may not ap pear in FT e.g a2 bl Suppose minsup isset 105 Wefirst mine frequent itemsets from each of A and B For example XI andx3appearfogetherinal a3 thetotalcountofal,a3 in FT is 6 hence XI23 is a frequent itemset Nen we check a frequent itemset from A can be combined with a fre quent itemset fmm B to form a frequent itemset of greater size yly6 is a frequent itemset fmm B with frequency  5 We want to see XIX can be combined with yly6 to form a frequent iternset the steps are outlined as follows 1 tida\(Z1  tal\(4 2 tid~\(2  01\(4 13\(2 tidn\(zrza  tidA\(z1 n tidA\(zs  01\(4 2 B\(YIYS   b\(2 b5\(3 2 Bkedal  4\(1 l 2 3 B-tid\(z1zs  Bkey\(a1 U BAey\(aa Bkey\(os  b\(l l  b2\(2 h\(l 3 4 B-tid\(Z123 ntidB\(ylYG  2 3 5 Tke combined frequency  total count in rhe list bz\(Z 3  5 Hence the iternset x1 x3yly6 is frequent 0 In general to examine the frequency for an itemset X that contains items from two dimension tables A and B we can do the following We examine table A to find the set of transactions TI in A that contain the A items in X Next we determine the transactions Tz in B that appear with such transactions in FT Note that this is similar to the derivation of an intermediate table in a semi-join strategy where the result of joining a first table with the key of a second table are placed the key of the second table is a 324 


foreign key in this intermediate table In the mean time the set of transactions T in B that contain the B items in X are identified. Finally TZ and T3 are intersected, and the resulting wunt is obtained The use of tidlist is a compressed form of recording the wciurences of lid's in the fact table Multiple Occurrences would be condensed as one single entry in a tidlist with the count associated Initial Step In order to do the above we need to have some initial informationabout tidA\(xi for each item xi in each dimension table A One scan of a dimension table can give us the list of transactions for all items In one scan of FT we can determine all the counts for all transactions in all the dimension tables. In the same scan we can also de termine B.key\(ai for each lid ai in each dimension table 3.1 Overall Steps For simplicity let us first assume that there are 3 dimen Step 1  Preprocessing sion tables A B C The overall steps of our method are Read thedimension tables, convert them intoVTV Vertical Tid-Vector\format with counts \(see Section 2 Perform local mining on each dimension table This can be done with any known single table algorithm with a slight modification of refering to the counts of transactions in FT which has been collected in the initial step described in Sec tion 2 The time taken for this step typcially is insignificant compared to the global mining steps Step 3  Global Mining Step 3.1  Scan the Fact Table Step 2  Local Mining Scan the Fact Table IT and record the information in some data structures We set an ordering for A E C First we handle tables A and B with the following 2 steps Step 3.2  Mining size-two iremsets This step examines all pairs of frequent items x and y which are from the two different dimension tables Repeat the following for k  3,4,5  Candidates are gen erated by the union of pairs of mined itemsets of size k  1 differingonly in the last item The technique of generation would be similar to FORC SHSOO Next count the fre quencies of the candidates and determine the frequent item sets After Steps 3.2 and 3.3 the results will be all frequent itemsets formed from items in tables A and/or B This can be seen as the frequent itemset mined from a single dimen sion table AB Similar steps as Steps 3.2 and 3.3 are then applied for the tables AB and C to obtain all frequent item sets from the star schema Step 3.3  Mining the rrstfor A and B 3.2 Binding multiple Dimension Tables We can easily generalize the overall steps above from 3 dimension tables to N dimension tables Suppose there are totally N dimension tables and a fact table FT in the star schema We start with two of the dimension tables say A and B We apply Steps 3.2 and 3.3 above to mine all frequent itemsets with items from A and/or B without joining the tables with FT This set of itemsets is called FAB We call Steps 3.2 and 3.3 a binding step of 4 and B After binding, we treat A and B as a single table AB and hegin the process of binding AB with another table this is repeated until all N dimensions are hound Some notations we shall use are  FA denotes the set of frequent itemsets with items from A FAB denotes thesetoffrequent itemsets withitems from tables A and/or B FAB denotes the set of frequent itemsets of the form XY where X is either empty set or an itemset from A and Y is either an empty set or an item set from B with size k E.g FAB  XI y~,z~y E.g FAk denotes the set of frequent itemsets of size k from A denotes the set of frequent itemsets in which the subset of items from A has size i and subset of items from B has size j E.g suppose xi's are items from table A and yj's are items from table B we may have FAaB1  After performing binding we can treat the items in the combined itemsets as coming from a single dimension table For example, after binding A and E we vinually combine A and B into a single dimension table AB and all items in FAB are from the new dimension table AB We always "bind 2 dimension tables at each step and iterate for N  1 times if there are totally N dimension tables. At the end all frequent itemsets will be discovered Figure 3 shows a possible ordering of the bind opera tions on four dimension tables A E C D FAB  YIYZ Y?Y3,~1Y1YZ,.lYZY3 Xlzzyl 23Z4Yz 10 12 I5 16 Figure 3 An example of binding order We need to do two things to combine two dimension ta bles 1 To assign each combination of tid from A and tid from B in FT a new tid and 2 to set the tid in the tiddists for items in AB to the corresponding new tid Consider an example in Figure 4 for a FT relating to 325 


A I 3 R Figure 4 Concatenating tids after "binding 4 dimensions A E C D after binding A and E the columns storing tid\(A and tid\(B would be concatenated Each combination of tid\(A and tid\(B\would be assigned a new tid A and E would be combined into AB For example before binding item z1 appears in transac tions a1,u3 and 14 tidA\(z1  a1\(2 1 aq\(l After binding since a1 corresponds to new tid tl and tz a3 corresponds to new tid t4 u4 corresponds to new tid 15 Therefore tidA\(zI is updated to tidAB\(z1  Similarly when FAB is then bound with FC AB is combined with C and FT would be updated again Note that in Figure 4 the tables with attribute newlid and the multiple fact tables are not really constructed as tables, but instead stored in a structure which is a prefix tree We always bind a given dimension table with the result of the previous binding because the tid of the dimension ta ble allows us to apply the technique of a foreign key as de scribed in the previous section The ordering can be based on the estimated result size of natural join of the tables in volved which can in turn be estimated by the dimension table sizes A heuristic is to order the tables by increasing table sizes for binding 33 Prefix'keeforFT tl\(l l l 1 In Step 3.1 of the overall steps the fact table FT is scanned once and the information is stored into a data struc ture which can facilitate the mining process The data struc ture is in the form of a prefu free Each node in the prefix tree has a label a rid and also a counter We need only scan FT once to insert each tuple into the prefix tree Suppose we have 3 dimensions A E C and a tuple is us bz cz we enter at the root node and go down a child with label as from a3 we go down to a child node with label b2 from bz we go to a child node labeled c2 Every time we visit a node we increment the counter there by 1 If any child node is not found it is created with the counter initialized to 1 Hence level n of the prefix tree corresponds to tid's of the nth dimension fable that would be bound When search ing for a foreign tidJist we can go down the path specified by the prefix In this way theforeign key and the global fre quency in the it iteration can be efficiently retrieved from the i  lth level of theprefur free Figure 5 shows how a fact table is converted to a prefix tree Figure 5 Prefix Tree structure representing FT 1 5 collapse  level 2  level 3 4'1 Figure 6 Collapsing the prefix tree Use of the preh tree  the foreign key The prefix tree is a concise structuring of FT which can facil itate our mining step When we want to bind FA with FE we have to check whether an itemset e.g zl in FA can be combined with an itemset in FB e.g yl We need to obtain the information of a for eign key in the form of lidlist e.g BAid\(z1 Let tida\(zl  u1\(2 We can find Bley\(a1 by searching the children of a which are labeled bl\(l b2\(1 similarly let B_key\(az  b2\(2 As a result B_tid\(zlyl  B.key\(hl UBley\(a2  bl\(l 3 Collapsingthe pdix tree Suppose A and B are bound AB is the derived dimension. If B is not the last dimension to be bound we can collapse the prefix tree by one level A new root node is built each node at the original second level becomes a child node of the new rootnode The subtree under such a node is kept intact in the new tree Figures 5 326 


and 6 illustrate the collapse of one level in a prefix tree To facilitate the above, we create a horizontal pointer for each node in the same level so that the nodes form a linked list A unique ABfid is given to each of the nodes in the second level which corresponds to the collapsed table AB These unique lids at all thc levels can he assigned when the prefix tree is first built Updating tid We need to do the following with the collapse of the prefix tree After binding two tables A and B a 223derived dimension\224 AB is formed We update the tidlists stored with the frequent itemsets and items that would be used in the following iteration so that all of them are referencing to the same \(derived\dimension tahle For example tidA\(X ortidB\(Y are updated totid~~\(X or tid~~\(Y D n rn s 3d Maintaining frequent itemsets in FI-trees number of dimensions number of transactions in each dimension tahle number of attributes in eachdimension table lareest size of freauent itemsets In both local mining and global mining \(Steps 2 and 3 we need to keep frequent itemsets as they are found from which we can generate candidate itemsets of greater sizes We keep all the discovercd frequent itemsets of the same size in a tree structure which we call an FI-tree FI stands for Frequent Itemset Hence itemsets of FAsB1 is mixed with itemsets of FAZBD the first one belongs to FABL the second belongs to FAB1 4 Related Work Some related work can he found in JSOO where the joined table T is computed hut without being materialized When each row of the joined table is formed it is processed and thereby storage cost for T is avoided. In the processing of each row in the table an array that contains the frequen cies for all candidate itemsets is updated As pointed out by the authors all itemsets are counted in one scan and there is no pruning from one pass to the next as in the apriori gen algorithm in AS94 Therefore there can be many can didate itemsets and the approach is expensive in memory costs and computation costs The empirical experiments in JSOO compare their approach with a base case of apply ing the apriori algorithm on a materialized table for T It is shown that the proposed method needs only 0.4 to 1 times the time compared to the base case However there are new algorithms in recent years such as HPYOO SHSOO which are shown by experiment to often run many times faster than the apriori algorithm Therefore the approach in JSOO may not be more efficient than such algorithms 5 Experiments We generate synthetic data sets in a similar way as HPDWOI First we generate each dimension tahle in dividually in which each record consists of a number of attribute values within the attribute domains and model the existence of frequent itemsets The parameters are listed in the following table I I I lage,i numhcrof inn,aciionr with acommon itemsci d 1 domm we of h alinbuie samc for 21 attibuica The domain SIX d of an attribute is the numher of dif ferent values for the attribute Nhish is the number of items derived from the attribute-value pars for the attribute The value of n is set as 1000 in our expenment After generating tmwtions for each dimension table we gener ate FT bad un parameters in the following table tup 1 large frequency of thc aisooiauon N~CS 1.1 I numberor maximal poieniially frequenl itcmscts I N I number of noise transactions I In constructing FT there can be correlations among two or more dimension tables so that some frequent itemsets contain items from multiple dimensions For the case of two dimensions we want the rids associated with the same group of transactions with common frequent itemsets from one dimension table to appear at least sup times together with another group of tid sharing common frequent item sets from another dimension table In doing so frequent itemsets across dimensions from these 2 groups would ap pear with a frequency count greater than or equal to sup after joining the two dimension tables and FT We repeat this process for 151 times so that IL maximal potentially frequent itemsets would be formed \(by maximal we mean that no supersel of the itemset is frequent In order to generate some random noise transactions which do not contain frequent itemset an generated N rows in FT are generated in which each tid from the di mension tables is picked randomly We compare our proposed method with the approach of applying FF-tree algorithm HPYOO on top of the joined tahle T We assume T is kept on disk and hence requires VO time for processing. FE-tree requires two scanning of T during the FP-tree mining The YO time required is up to 200 seconds in our experiments It tuns out that the table join time is not significant compared to the mining time All experiments are conducted on SUN Ultra-Enterprise Generic.106541-18 with SunOS 5.7 and 8192MB Main Memory The average disk U0 rate is 8MB per second Programs are written in C+t We calculate the total execu tion time of mining multiple tables as the sum of required 327 


5001 I 0 0 5 10 15 20 25 30 Figure 7 Running time for A related and A,B,C related datasets CPU and WO times and that of mining a large joined table as the CPU and WO times for joining and FP-tree mining For the local mining step in our approach, we use a verti cal mining technique as in FORC SHSOO frequent item sets of increasing sizes are discovered The tidlist of the itemsets are used to generate size k  1 frequent itemsets from size k frequent itemsets which is by intersecting the tidlists of pairs of size k frequent itemsets with only one differing item In the experiments we compare the running time of mas1 our proposed method implementing tidlist as a linked list structure masb our proposed method im plementing tidlist as a fixed-size bitmap and an array of count and fpt the join-beforsmine approach with FP tree\with different data setting in 3 dimension tables A B C and a fact table FT In most cases mosb runs slightly faster than mad but needs abut 10 times more memory In the first dataset we model the situation that items in A and B are strongly related such that frequent itemsets contain items across A and B while items in C are not involved In such cases transactions containing frequent itemsets from A and B can be related to Br transactions in C randomly Br is set to 100 in all of our experiments reported here We have varied the value of Br and discov ered little change in the performance The default values of other parameters used are  numkrof mnramun in the joined whlc number of atmbutes in each dimension whlc I 50K I 10 When we increase the number of items the running time of fpt increases steeply, while that of both masl and masb would increase almost linearly Running time of FP-tree numberda~hsschdrd0"Iatle b Figure 8 Running time for mixture datasets grows exponentially with the depth of the tree which is de termined by the maximum number of items in a transaction In this case performance of our proposed method outper foms FP-tree especially when the number of items in each transaction is large see Figure 7\(a When the number of transactions in the joined table increases running time of both methods would increase greatly masl and masb are about 10 times faster than Jpt see Figure 7\(b We also vary the percentage of random noise being included in the datasets see Figure 7\(c both masl and musb are faster than Jpt 328 


In the second dataset, we model the case that 4 B Care all strongly related so that maximal frequent itemsets al ways contain items from all of A B and C Compared with the previous dataset, performance of our approach does not vary too much, while the running time of fpi is faster in some cases \(Figures 7\(b and c With the strong correla tion there would he less different patterns to be considered and the FP-tree will be smaller However we believe that in real life situation such a strong correlation will be rare In real life application, there are often mixtures of rela tionships across different dimension tables in the database In the third group of dataset we present data with such mix ture In particular 10 of transactions contain frequent itemsets from only A B C respectively 15 contam fre quent itemsets from AB BC AC respectively 10 con tain frequent itemsets from ABC and 15 are random noise We investigate how the running times of mas1 and fpt vary against increasing number of items in each trans action and increasing number of transactions in T In Figure 8\(a we vary the number of transactions from 20K to 60K while kzeping the number of attributes in each dimension table to he 30 In Figure 8\(h we vary the num ber of attributes in each dimension table from 10 to 60 and keep the number of transactions to be 30K. In this case, run ning time vi fpt grows much fastertban our approach This demonstrates the advantage of applying our method, which would he more significant when we have more dimension tables so that the number of items in the T will be large 6 Conclusion We propose a new algorithm for mining association rules on a star schema without performing the natural join We show by experiments that it can greatly outperform a method based on joining all tables even when the naive approach is equipped with a state-of-the-art efficient algo rithm Our proposed method can he generalized to be applied to a snowflake structure where there is a star structure with a fact table FT but a dimension table can be replaced by an other fact table FT\222 which is connected to a set of smaller dimension tables We can consider mining across dimen sion tables related by FT\221 first We then consider the result as a single derived dimension and continue to process the star structure with FT This means that we always mine from the 224leaves\224 of the snowflake Our current experiments assume that the data structures we use can he kept in main memory It will be of interest to study the case where disk memory is required for the inter mediate steps. Since disk access is more expensive and our intermediate structure has been designed to he more com pact than the fact table we expect good performance to be found for the proposed approach in such cases We have not examined in our experiments the cases where the num her of dimension tables is large More study will be needed for these considerations In general FT can contain attributes other than rid from dimension tables In this case we group all these attributes put them in a separate new dimension table, and apply the same techniques to mine it as other dimension tables Acknowledgements This research is supported by the RGC \(the Hang Kong Research Grants Council grant UGC REECUHK 4179/01E We thank Yin Ling Cheung for her help in the experiments and performance analysis References AIS931 R Agrawal T Imilienski and A Swami Mining asso ciation rules between sets of items in large datasets Pro ceedingsof the ACM SIGMOD International Conferace on the Management of Data pp 207-2l6,1993 AS941 R Agrawal and R Srikant Fast algorithm for mining as sociation rules. Proceedings of the 20th VLDB Confer ence pp 487499,1994 CD97 S Chaudhuri and U Dayal An Overview of Data Ware housing and OLAP Technology ACM SIGMOD Record Vol 26 No pp 65-74.March 1997 HPDWOI I Han 1 Pei G Dong and K Wang Efficient com putation of iceberg cubes with complex measures Pro ceedings of the ACM SIGMOD International Conference on the Managementof Data pp 1-12,2001 HPYOO J Ha\224 and I Pei and Y Yin Mining Frequent Patterns without Candidate Generation Proceedings of the ACM SIGMOD International Conference on the Management of Data pp 1-12,2000 V.C Jensen and N Soparkar. Frequent Itemset Counr ing across Multiple Tables Proceedings of Pacific-Asia Conference on Knowledge Discovery and Data Mining PAKDD\pp 49-61,2000 RS98 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes h of International Conference on Data Engineering ICDE pp 503-512.1998 SHSW P Shenoy J.R Haritsa S Sudarshan Turbo-charging vertical mining of large databases. Proceedings of the ACM SIGMOD International Conference on Manage ment of Data pp 22-33.2000 SA961 Ramakrishnan Srikent Rakesh Agrawal Mining Quanti tative Association Rules in Large Relational Tables Pro ceedings of the ACM SIGMOD International Conference on Management of Data pp 1-12 1996 STAOO S.Sarawagi S Thomas R Agrawal Integrating assoEi ation rule mining with relational database systems Al ternatives and implications Data Mining and Knowledge Discovery 4\(2/3 2000 Also appeared in SIGMOD. pp 343-354.1998 IS001 329 


 Figure 5 Process Main Table into Transaction Table D Figure 6 Process Transaction Table into Concise Figure 7 I f bl 411 significant association rules for the target attribute television confidence=20 support=0.2 In Figure 7 we select television as our target and get the rules based on confidence level 20 and support 0.2 6 Conclusion This paper presents a solution to the problem by using a frame metadata model to invoke record count program in a metadata whenever relevant new record is created in the target database As a result web mining association rules can be accomplished online or incrementally without the need to re-compute the historical web transaction counts The metadata thus contributes saving of computing time and also the automation of data mining association rules non-stop The future research of this paper is web mining web pages association rules via Internet References I Chen Q Hsu M and Dayal U A Data Warehouse/OLAP Framework for Scalable Telecommunication Tandem Traffic Analysis a report of Software Technology Laboratory HP Labs 1501 Page Mill Road MS 1U4 Palo Alto CA 94303 USA Robert Cooley, Bamshad Mobasher and Jaideep Srivastava Data Preparation for Mining World Wide Web Browsing Patterns Journal of Knowledge and Information Systems Vol 1 No 1 1999 Robert Cooley, Bamshad Mobasher and Jaideep Srivastava Grouping Web Page References into Transactions for Mining World Wide Web Browsing Patterns Proceedings of the 1997 IEEE Knowledge 2 3 41 I51 t61 I71 and Data Engineering Exchange Workshop \(KDEX 97 November 1997 A.G Buchner M.D Mulvenna S.S Anand, J.G Hughes An Internet-enabled Knowledge Discovery Process the 9th International Database Conference Hong Kong pp 13-27, July 1999 Cookie A.G. Buchner, M. Baumgarten S.S Anand, M.D Mulvenna J.G. Hughes Navigation Pattern Discoven from Internet Data ACM Workshop on Web Usage Analysis and User Profiling WebKDD San Diego CA pp. 25-30 1999 Mena Jesus Data Mining Your Website Digital Press 1999 pp 3 13-3 14 Moore J Han E Boley D Gini M Gross R Hstings K Karypis G Kumar V and Mobasher B Web Page Categorization and Feature Selection Using Association rule and Principal Component Clustering University of Minnesota M Mohania S Madria and Y Kambayashi Seu Maintainable Aggregate Views Proceedings of the 91h International Database Conference City University of Hong Kong Press ISBN 962-937-046-8 p.306-3 17 T.Griffin and L Libkin Incremental mainteriance of views with duplicates Proceedings of the International Conference on Management of Data 1995 Sunita Sarawagi Shiby Thomas and Rakesh Agrawal Integrating Association Rule Mining With Relational Database Systems Alternatives and Implications Takeshi Fukuda Yasuhiko Morimoto Shinichi Morishita and Takeshi Tokuyama Data Mining Using Two-Dimensional Optimized Association Rules: Scheme Algorithms and Visualization ACM 199 pp13-23 Eui-Hong Sam Han George Karypis and Vipin Kumar Scalable Parallel Data Mining for Association Rules ACM 1997, pp277-288 Pieter Adriaans and Dolf Zantinge Data Mining Addision Wesley ISDN 0-201-40380-3 Fong J Huang S Architecture of a Universal Database A Frame Model Approach lntemational Journal of Cooperative Information Systems Volume 8 Number 1 March 1999, pp.47-82 Fong J and Pang F Schema Evolution for New Database Applications A Frame Metadata model Approach Proceedings of Systems Cybernetics and Informatics, Volume 5 1999 pp 104-1 1 1 Fong J Huang S Information Systems Reengineering Springer Verlag ISBN 98 1-3083-15 0 R Zaiane M Xin, J Han Discovering Web Access Patterns arid Trends by Applying OLAP and Data Mining Technology on Web Logs Proceedings Advances in Digital Libraries Conference ADL\22298 Santa Barbara CA April 1998 pp 19-29 A.G Buchner M.D Mulvenna Discovering Internet Marketing Intelligence through Online Analytical Web Usage Mining ACM SIGMOD Record ISSN ACM 1998 343-354 8 1997 179-212 0163-5808 Vol. 27 NO 4 pp 54-61 1998 129 


ofimages 50 100 150 200 1 feature 50292 80777 127038 185080 2 obj identif 210 338 547 856 3 aux image 3847 6911 10756 13732 4 assoc rules 6 3 6 4 Table 2 Measured times in seconds for each Image Mining step with different image set sizes colors and contrast When the L was close to another shape its colors were merged making it dissimilar to other L shaped objects This suggests that irregular shapes in general make image mining dif\256cult We worked with color images but it is also possible to use black and white images Color and texture were important in mining the geometric shapes we created However we ignored shape as mentioned above Shape may be more important for black and white images but more accurate shape descriptors are needed than those provided by the blobs 4.3 Performance evaluation We ran our experiments on a Sun Multiprocessor forge.cc.gatech.edu computer with 4 processors each running at 100 MHz and 128 MB of RAM The image mining program was written in Matlab and C The 256rst three steps are performed in Matlab The feature extraction process is done in Matlab by the software we obtained from UCB Object identi\256cation and record creation were also done in Matlab by a program developed by us An html page is created in Matlab to interpret results The association rules were obtained by a program written in C In this section we examine the performance of the various components of the image mining process as shown in Table 2 for several image set sizes These times were obtained by averaging the ellapsed times of executing the image mining program 256ve times 4.4 Running time analysis Feature extraction although linear in the number of images is slow and there are several reasons for this If image size increases performance should degrade considerably since feature extraction is quadratic in image size Nevertheless this step is done only once and does not have to be repeated to run the image mining algorithm several times Object identi\256cation is fast This is because the algorithm only compares unmatched objects and the number of objects per image is bounded For our experimental results time for this step scales up well Auxiliary image creation is relatively slow but its time grows linearly since it is done on a per image basis The time it takes to 256nd rules is the lowest among all steps If the image mining program is run several times over the same image set only the times for the second and the fourth step should be considered since image features already exist and auxiliary images have already been created 5 Application Image mining could have an application with real images The current implementation could be used with a set of images having the following characteristics 017 Homogeneous The images should have the same type of image content For instance the program can give useless results if some images are landscapes other images contain only people and the remaining images have only cars 017 Simple image content If the images are complex they will produce blobs dif\256cult to match Also the association rules obtained will be harder to interpret A high number of colors blurred boundaries between objects large number of objects signi\256cant difference in object size make the image mining process more prone to errors 017 A few objects per image If the number of objects per image is greater than 10 then our current implementation would not give accurate results since Blobworld in most cases generates at most 12 blobs per image 017 New information The image itself should should give information not already known If all the information about the image is contained in associated alphanumeric data then that data could be mined directly 6 Future Work Results obtained so far look promising but we need to improve several aspects in our research effort We are currently working on the following tasks We also need to analyze images with repeated geometric shapes If we want to obtain simple association rules this can make our program more general This can be done without further modi\256cation to what is working However if we want to mine for more speci\256c rules then we would need to modify our algorithm For instance we could try to 


produce rules like the following if there are two rectangles and one square then we are likely to 256nd three triangles The issues are the combinatorial growth of all the possibilities to mine and also a more complex type of condition We will also study more deeply the problem of mining images with more complex shapes such as the irregular one similar to the letter L We need a systematic approach to determine an optimal similarity threshold or at least a close one A very high threshold means only perfect matches are accepted On the other hand a very low similarity threshold may mean any object is similar to any other object Finding the right similarity threshold for each image type l ooks like an interesting problem Right now it is provided by the user but it can be changed to be tuned by the algorithm itself Also there are many ways to tune the eleven parameters to match blobs and the optimal tuning may be speci\256c to image type There also exists the possibility of using other segmentation algorithms that could perform faster or better feature extraction It is important to note that these algorithms should give a means to compare segmented regions and provide suitable parameters to perform object matching in order to be useful for image mining From our experimental results it is clear that this step is a bottleneck for the overall performance of image mining We can change the object identi\256cation algorithms to generate overlapping object associations using more features Our algorithm currently generates partititons of objects that is if one object is considered similar To another one the latter one will not be compared again By generating overlapping associations we can 256nd even more rules For instance a red rectangular object may be considered similar to another rectangular object and at the same time be similar to another red object Mining by position is also possible for instance two objects in a certain position may imply another object to be in some other position Since the software we are using for feature extraction produces eleven parameters to describe blobs we have 2 11 possibilites to match objects 7 Conclusions We presented a new algorithm to perform data mining on images and an initial experimental and performance study The positive points about our algorithm to 256nd association rules in images and its implementation include the following It does not use domain knowledge it is reasonably fast it does not produce meaningless or false rules it is automated for the most part The negative points include some valid rules are discarded because of low s upport there are repeated rules because of different object id's unwanted matches because of blobs representing several objects slow feature extraction step a careful tuning of several parameters is needed it does not work well with complex images We studied this problem in the context of data mining for databases Our image mining algorithm has 4 major steps feature extraction object identi\256cation auxiliary image creation and identi\256ed object mining The slowest part of image mining is the feature extraction step which is really a part of the process of storing images in a CBIR system and is done only once The next slowest operation is creating the auxiliary blob images which is also done once Object identi\256cation and association rule 256nding are fairly fast and scale up well with image set size We also presented several improvements to our initial approach of image mining Our experimental results are promising and show some potential for future study Rules referring to speci\256c objects are obtained regardless of object position object orientation and even object shape when one object is partially hidden Image mining is feasible to obtain simple rules from not complex images with a few simple objects Nevertheless it requires human intervention and some domain knowledge to obtain better results Images contain a great deal of information and thus the amount of knowledge that we can extract from them is enormous This work is an attempt to combine association rules with automatically identi\256ed objects obtained from a matching process on segmented images Although our experimental results are far from perfect we show that it is better to discover some reliable knowledge automatically than not discovering any new knowledge at all Acknowledgments We thank Chad Carson from the University of California at Berkeley for helping us setup the Blobworld system We also thank Sham Navathe and Norberto Ezquerra for their comments to improve the presentation of this paper References 1 R  A g r a w a l  T  I m i e lin s k i a n d A  S w a m i  M in in g a s s o ciation rules between sets of items in large databases In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data  pages 207\261 216 Washington DC May 26-28 1993  R  A gra w a l a n d R  S ri ka nt  F a s t a l gori t h m s for m i n i n g association rules in large databases In Proceedings of the 20th International Conference on Very Large Data Bases  Santiago Chile August 29-September 1 1994  S  B e l ongi e  C Ca rs on H  G r e e n s p a n  a nd J  Ma lik Recognition of images in large databases using a learning framework Technical Report TR 97-939 U.C Berkeley CS Division 1997 


 C  C a r s on S  Be l ongi e  H  G r e e n s p a n  a nd J  Ma l i k  Region-based image querying In IEEE Workshop on Content-Based Access of Image and Video Libraries  1997 5 G  D u n n a n d B  S  E v e r itt An Introduction to Mathematical Taxonomy  Cambridge University Press New York 1982  U  F a yya d  D  H a u s s l e r  a nd P  S t orol t z  M i n i n g s c i e n ti\256c data Communications of the ACM  39\(11\51\26157 November 1996  U  F a yya d G  P i a t e t s k y-S h a p i r o a n d P  S m y t h  T he kdd process for extracting useful knowledge from volumes of data Communications of the ACM  39\(11\:27\261 34 November 1996 8 D  F o r s y t h J M a l i k  M F l e c k H G r e e n s p a n  T L e ung S Belongie C Carson and C Bregler Finding pictures of objects in large collections of images Technical report U.C Berkeley CS Division 1997  W  J  F ra wl e y  G  P i a t e t s k y S ha pi ro a nd C J  Ma t h e u s  Knowledge Discovery in Databases  chapter Knowledge Discovery in Databases An Overview pages 1 261 27 MIT Press 1991  V  G udi v a da a n d V  R a gha v a n Cont e n t ba s e d i m age retrieval systems IEEE Computer  28\(9\18\26122 September 1995 11 R  H a n s o n  J  S t u t z an d P  C h ees eman  B ay es i a n c l a s si\256cation theory Technical Report FIA-90-12-7-01 Arti\256cial Intelligence Research Branch NASA Ames Research Center Moffet Field CA 94035 1990  M H o l s he i m e r a n d A  S i e be s  D a t a m i ni ng T h e search for knowledge in databases Technical Report CS-R9406 CWI Amsterdam The Netherlands 1993  M H out s m a a nd A  S w a m i  S e t ori e nt e d m i ni ng of association rules Technical Report RJ 9567 IBM October 1993  C O r done z a nd E  O m i e c i ns ki  I m a ge m i ni ng A new approach for data mining Technical Report GITCC-98-12 Georgia Institute of Technology College of Computing 1998  J  R Q u i n l a n Induc t i o n o f d e c i s i on t r e e s  Machine Learning  1\(1\81\261106 1986  A  S a v a s e re  E  O m i e c i ns ki  a nd S  N a v a t h e  A n e f 256 cient algorithm for mining association rules In Proceedings of the VLDB Conference  pages 432 261 444 Zurich Switzerland September 1995  O  R Z a i a ne  J  H a n  Z  N  L i  J  Y  Chi a ng a n d S Chee Multimedia-miner A system prototype for multimedia data mining In Proc 1998 ACM-SIGMOD Conf on Management of Data  June 1998 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


