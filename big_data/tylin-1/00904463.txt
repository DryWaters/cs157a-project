A Parameterised Algorithm for Mining Association Rules Nuansri Denwattana and Janusz R Getta School of Information Technology and Computer Science University of Wollongong Australia  nd22,jrg@cs.uow .edu.au Abstract A central part of many algorithms for mining asso ciation rules in large data sets is a procedure that finds so called frequent itemsets This paper proposes a new approach to finding frequent itemsets The approach reduces a number of passes 
through an input data set and generalises a number of strategies proposed so far The idea is to analyse a variable number n of itemset lattice levels in p scans through an input data set It is shown that for certain values of parameters n,p this method provides more flexible utilisation of fast access transient memory and faster elimination of itemsets with low support factor The paper presents the results of experiments conducted to find how performance 
of association rule mining algorithm depends on the val ues of parameters n,p Keywords Data Mining Association Rules Fre quent Itemsets Algorithms 1 Introduction The algorithms for mining association rules in large data sets attracted a lot of attention in the recent years The original problem l was to find the corre lations among the sales of different products from the analysis of large set of supermarket data Association rule is an implication that determines co-occurrence of the objects in 
a large set of so called transactions e.g customer baskets collections of measurements etc At present the research works on association rules are motivated by an extensive range of application ar eas such as banking, manufacturing health care and telecommunications Association rule discovery tech niques are used to detect suspicious credit card transac tions, money-laundering activities 9 in banking and fi nancial businesses The same techniques are applied in manufacturing controlling and scheduling of technical production processes 5 The other application areas include health 
care 7 and management of telecommu nication networks 6 The discovery of association rules is typically done in two steps l Analysis of experimental data performed in the first step provides a minimal set of objects item sets such that frequency of their co-occurrence is above a given threshold minimum support These itemsets are called as frequent itemsets The second step uses the frequent itemsets to construct the association rules It has been shown that computational complexity of 
the problem is buried in the searching for a minimal set of frequent itemsets in the first step Generation of association rules from frequent itemsets has a linear complexity and it has no impact on the overall perfor mance A number of algorithms finding frequent itemsets in large data sets have been already proposed Ma jority of them counts one category of itemsets e.g all IC element itemsets in one pass through an input data set For instance Apriori algorithm 2 counts n element itemsets in 
the n-th pass through a data set All frequent itemsets identified in the n-th pass are used to generate the hypothetically frequent item sets candidate itemsets for verification in the next pass Frequent itemsets obtained from the n-th pass and being the subsets of frequent itemsets identified in the next pass are pruned The process continues until no new frequent itemsets are found Sampling for frequent itemsets algorithm lo extracts a random sample from a data set and finds all frequent itemsets there Next it tries to verify 
the results on a complete data set A top-down approach ll applies the max imum clique generation algorithm to find a ceiling of the minimal set of frequent itemsets Next the sub sets of all frequent itemsets included in a ceiling are counted in each pass through a data set DIC algo rithm 3 stops counting itemsets as soon as there is no chance for an itemset to be frequent Each elim 45 1530-0919/01 10.00 0 2001 IEEE 


inated itemset is immediately replaced with another itemset A new technique recently proposed in 4 uses FP-tree to store compressed crucial information about frequent itemsets This technique needs a huge volume of transient memory if a number of frequent itemsets is too large Partition algorithm 8 transforms an input data set from a horizontal layout to a vertical layout and uses a list intersection technique to identify the frequent itemsets An approach presented in this paper considers a hy pothetical perfect algorithm capable of guessing and verifying all frequent itemsets in one scan through an input data set An input to the perfect algorithm is a set of frequent and non-frequent itemsets called as a perfect guess A perfect guess includes both frequent and non-frequent itemsets because for each frequent itemset found we have to show that none of its su persets is frequent For example if a set of all items is A B C and A B C A B are frequent itemsets then to verify that A B C is the mini mal set of frequent itemsets we have to check in a data set that A,B C are frequent and that A,C B C are not frequent As the result a perfect guess consists of the candidate itemsets from many levels of itemset lattice The quality of association rule min ing algorithms is determined by two factors The first one is a number of passes through an input data set The other one is a number of comparisons of candidate itemsets with input transactions in order to find which candidate itemsets should be counted The perfect al gorithm minimises both parameters It needs to read an input data set only once and it needs to perform the smallest number of comparisons to verify a perfect guess For instance elimination of any candidate item set in order to reduce a number of comparisons results with a different solution We are aware that implementation of the perfect al gorithm is unrealistic because probability of making a correct guess in a large data set is very low Our idea is to treat a concept of perfect algorithm in a way sim ilar to how a concept of absolute zero temperature is treated in physics It is going to be the ultimate goal i.e a point which cannot be achieved and in the same moment a point that can be used to measure the quality of the realistic algorithms One of the objectives is to construct an algorithm that makes a good guess i.e a guess that is not per fect and in the same moment it does not contain too many errors To make a good guess we need to get some information about the properties of an input data set It leads to a strategy where a data set is read once the statistics are collected and used to guess all 2,3   n  th element candidate itemsets Next a guessed set of candidate itemsets is minimised and ex tended by a minimal set of non-frequent itemsets that have to be tested to prove its correctness At the end an input data set is scanned for the second time to verify a guess Due to a fact that initial guess is not perfect some of the items that suppose to be frequent appear not to be frequent and vice versa A set of mis takes detected during verification is used to generate a new set of candidate itemsets that should be verified again The third scan through a data sets eliminates all mistakes and provides the final solution for a range of 2,3   n  th element itemsets Then the same procedure is repeated for the next range of itemsets To implement such an algorithm we need a procedure capable of guessing frequent itemsets from the statis tics collected in the first scan of input data set. To our best knowledge none of the algorithms proposed so far has such properties A problem with the approach sketched above is that we make more errors in guessing of itemsets that con tain more items. This is because the errors done at the lower levels of itemset lattice multiple themselves very fast at the higher levels A number of error has an im portant impact on performance because each of them requires the additional comparisons of candidate item sets with transactions from an input data set These observations lead to a parameterised version of the al gorithm In order to decrease a number of errors at the higher levels we parameterise a range of itemsets for which a guess is done On the other hand smaller guessing range increases a number of passes through an input data set The parameterised n,p algorithm finds all frequent itemsets from a range of n levels in itemset lattice in p passes n  p through an input data set A classical Apriori algorithm is a special case of n,p algorithm where n  p  1 i.e the candidate itemsets from one level of itemset lattice are verified in one pass An interesting question is what combina tions of n and p values provide the best performance Intuitions are such that as a ratio n/p increases we have to perform more unnecessary comparisons of candidate itemsets with transactions from an input data set On the other hand if ratio n/p decreases then we perform less unnecessary comparisons and in the same moment we read an input data set more frequently The rest of this paper is organised as follows A detail of n p algorithm including guessing verifying procedures and an example is given in Section 2 Ex periments of n,p algorithm is demonstrated in Sec tion 3 A summary and a discussion of future research are provided in Section 4 46 


2 Finding frequent itemsets This section presents a parameterised n p algo rithm for mining frequent itemsets It also contains the description of guessing and verification of candi date itemsets 2.1 Problem description Let I  il,i2   im be a set of literals called items Let D be a set of transactions where each transaction t E D consists of transaction identifier tid and set of items It I We assume that the items are kept ordered within each transaction We call an itemset that contains k items as k-itemset Association rule is an expression X j Y where X Y are itemsets and X Y C I and X fl Y  0 The support for an itemset is defined as a fraction of all transactions that includes X U Y The confidence of a rule X  Y is defined as X U Y We accept a rule X  Y as true if its confidence exceeds a given threshold value A candidate itemset is an itemset selected for veri fication of its support in a data set An itemset is a positive candidate itemset when it is assumed \(guessed to be frequent Otherwise it is called as a negative candidate itemset Both positive and negative candi date itemsets are verified in single pass through a data set A candidate itemset becomes a frequent itemset when verification shows its support level above a given threshold value A remaining candidate itemsets is can didates verified in another scan In the rest of the paper candidate k-itemsets are de noted by Ck positive negative\candidate k-itemsets are denoted Cz C remaining candidates are de noted by Ci and frequent k-itemsets are denoted by Lk 2.2 The algorithm The algorithm starts from an initial pass through an input data set in order to find all frequent 1-itemsets L1 and to collect the statistics of the total number of 1 2    n-element transactions that contain the elements from L1 The statistics are stored in table T e.g see Table 1 Then, the initial value of current level k is set to 2 and initial result is set to L1 If Lk-l is not empty a procedure guess-candidates is called to guess the candidate itemsets from the next n levels The procedure returns a set C of positive and negative candidate itemsets The elements of C are verified in an input data set by a procedure verify-candidates The procedure finds all errors done by guess-candidates in one pass through an input data set Then it corrects the errors and finds the solution for levels from IC to k  n  1 in the second pass through the data set A minimal set of frequent frequent itemsets found is added to the result set The value of k is then increased by n These steps are repeated until L\(kPl is empty A pseudo-code of the algorithm is given below n  number of lattice levels traversed at a time sup  minimum support Results  0 generate L1 generate statistics table T Result  Result UL1 k  2 while Lk-1  0 do guess-candidates\(T Lk-1 tf tt n k C verify-candidates\(C sup n k Result  Result U Lk Lk+i   Lk+n-l k  k  n end 2.3 Guessing candidate itemsets The procedure guess-candidates finds all candi date itemsets from a range of levels from k to k  n  1 that accordingly to our guessing method would ver ify as frequent itemsets The procedure takes on input statistics table T frequency thresholds tp m-element transaction threshold tt set Lk-1 of frequent item sets level k it starts from and number n of levels to be considered Guessing starts at level k The procedure uses apri ori-gen function proposed in 2 to generate a set Ck of candidate k-itemsets from Lk-1 Then it uses the statistics from table T to decide which candidate item sets in ck are positive cz and which one are negative CL A frequency threshold is applied to all transac tions that consists of k or more elements The output of this step is a set of single items whose frequencies satisfy the frequency threshold If any itemsets in Ck are subset of the output set then we put them into a set of positive candidate k-itemsets We repeat this step until it reaches transaction length m Finally if there are any k-itemsets in Ck which are not in a set of positive candidate k-itemsets then they are appended to a set of negative candidate k-itemsets In the next step apriori-gen is applied to 7 to form set of c\(k+1 This time we consider from k+l element to m-element transactions The sets of Cgtl and CG are then generated Next C is used to form C\(k+2 This procedure is repeated until we 47 


reach level k  n  1 Finally all subsets of itemsets in C&+n-l at lower levels are pruned For example assume that procedure guess-candidates is called with the following parameters item frequency threshold equals to SO m-element transaction threshold equals to five 5-element transaction number of levels to traverse equals to three starting level equals to two and table statistics table T as follows Item freq according to tr length 3 els I 4 els I 5 els 1 Total freq  B C 4 2 3 9 3 2 3 9 D E F 1 1 1 3 3 1 3 7 1 0 3 4 Suppose we are at level k and the apriori-gen function generated Cr  AB AC AD AE AF BC BD, BE BF,CD CE,CF DE DF EF As there is no k-element transactions in table 1 we consider k+l transaction 80 of the total number of k+l i.e five, is four The output set of single items whose frequencies satisfy the frequency threshold is B Consequently there is no itemsets in Ck which is subset of this set Next applying the frequency threshold to transaction length 4 and this time the output set is ABC As there are some itemsets in Ck are subset of ABC they are put into a set of positive candidates We repeat this step in the transaction length 5 Finally, set of Cz and Ci are as follows Ct  AB AC AE AF BC BE BF CE CF EF Ci  AD BD, CD DE DF no of m-els trs We use two thresholds to guess the candidate item sets item\222s frequency threshold and m-element trans action threshold The accuracy of candidate guessing is determined by both of them If a value of item\222s frequency threshold is high then accuracy of candidate guessing will be high as well However we will get less frequent itemsets from the first scan because we have less positive candidates Consequently the ex tra database passes may be needed to determine large number of remaining candidate itemsets On the other hand if the value of item frequency threshold is low we have too many errors In addition the higher value m-element transaction threshold is, more errors of can 5 2 3 10 didate itemsets are generated 2.4 Verification of candidate itemsets Verification of candidate itemsets includes verifica tion of candidates provided by guessxandidates pro cedure and elimination of errors done at the guessing stage The procedure verifyxandidates takes on in put a set C with positive and negative candidate item sets minimum support sup starting level IC and number of lattice levels traversed n In the first stage the procedure scans an input data set and finds all positive candidate itemsets which ap pear to be negative and vice versa Due to errors in guessing it has to construct a new set of candidate item sets and verify them once more If certain Cj\222 appears to be not frequent then all its subsets from levels k to IC  j  1 are generated 221Then they are trimmed by supersets which appear to be frequent Similarly if certain CjT appears to be not frequent then all its supersets from levels j  1 to IC  n  1 are generated and trimmed by the verified frequent itemsets In the next stage the confirmation procedure scans an input set for the second time and verifies the final solution Although the n p algorithm moves n levels at a time the total number of candidate itemsets is more or less the same as other algorithms moving level by level It is because itemsets in lower levels will not be subsets of any candidates in higher levels both in positive and negative candidate itemsets 2.5 Example This subsection describes a sample execution of n,p algorithm for n  3 p  2  and the statistics given in Table 1 Suppose that frequency threshold tf  SO m-element transaction threshold tt  5 and the sets of positive and negative candidates at level 2 are C  AB AC AE AF BC BE BF CE CF EF CT  AD BD CD DE DF Using only set of C and apply the thresholds to Table 1 set of C and C are as follows C  ABC ABE ABF ACE ACF AEF BCE BCF BEF CEF c   c  I The procedure is repeated a.t level 4 Cz  ABCE ABCF ABEF ACEF BCEF Set of final positive and negative candidate itemsets after pruning all subsets of positive superset are C  ABCE ABCF ABEF ACEF BCEF c  c   48 


I Parameters I no database scans I tl 10 CT  AD BD CD DE DF c  c   The database are scanned to verify these candidate itemsets With 20 of minimum support, partial fre quent 2 3-, 4-itemsets are as follows Lz  BD CD DE L4  ABCE ABCF ABEF ACEF BCEF L3  I Then set of remaining candidate itemsets are gener ated Cf  BCD BDE CDE Cf  BCDE BCDF Verifying sets of remaining candidate by scanning the database frequent 2 3 and 4-itemsets are gener ated c2\224  Lz  AB AC AE AF BC BD BE BF CD CE CF DE L3  ABC ABE ABF ACE ACF AEF BCD BCE L4  ABCE ABCF ABEF ACEF BCEF EF BCF BDE BEF CEF By using frequent 4-itemsets candidate itemsets of another three levels are formed As there is only one 5-itemset there is no need to form sets of candidate 6 7-itemsets Cs  ABCEF Scanning the database frequent 5-itemsets are fi nally determined L5  ABCEF n 2 3 5 7 8 10 12 np sup Apr 10 20 96443  3 Experimental Results L 12 10 20 117444 3  14 10 20 1376444 3 20100 10 8543   To assess the performance of n,p algorithm we conducted several experiments on different data sets The algorithm was implemented in C language and we tested it on Unix platform The experiments used the synthetic data sets generated by IBM\222s synthetic data generator from Quest project We considered the following parameters number of transactions in a database ntrans average transaction length tl number of patterns np and a minimum support We have tried a range of number of transactions average transaction length and a number of patterns As we expected the results show that for n  1 and p  1 our approach provides better results than Apriori algorithm n  1 and p  1 both in terms of execution time and the total number of database scans SUP between Apriori and n p algorithm Table 2 shows a number of database scans of n,p algorithm compared with Apriori in different distribu tions of data sets 1200 1000 1  800  E s 600 400 YI 200 0 100 1,000 10,000 50,000 100,000 150,000 200,000 ntrans Figure 1 Performance of Apriori and n,p with tk10 np=lO sup=20 Figure 1 presents the results for different numbers of transactions and fixed number of candidate item sets are We compared Apriori with n,p algorithm by moving several levels in two passes With small size of databases the performance of Apriori and n,p al gorithm is approximately the same When an input data set is larger the performance of n,p algorithm is much better than Apriori It is because a number of database scans of n,p algorithm is less than in Apri ori In addition three to four levels are the optimal movings which are the best performance of this data set We also conducted the other experiments with dif ferent data distributions as shown in Figures 2 and 3 When the data distribution is more scattered the exe cution time of n,p algorithm is not much different to Apriori It is because both algorithms have to deter mine many candidate itemsets which are not frequent Figure 4 shows the performance of n,p algorithm with the increasing of the ratio of nlp We parame 49 


1 14000 i  k 12000 g 10000 3 8000  6000 4000 2000 0 100 1,000 10,000 50,000 100,000 150,000 200,000 ntrans Figure 2 Performance of Apriori and \(n,p\al gorithm with tkl4 np=lO sup=20 terised the algorithm by moving one level in one pass of data set and moving more than one levels in three passes It showed that when a ratio increases the per formance decreases due to the itemset guessing with more elements which resulted in getting more errors Finally we illustrated performance of n,p algo rithm by varying number of database passes p and fixing number of levels moving a time n  8 as shown in Figure 5 to confirm that we should not move too many levels in a few database scans as well as should not move one level in one database pass 4 Summary and future works This work proposes a new approach to finding fre quent itemsets in mining association rules The im portant contribution of our method is the reduction of number of scans through a data set The main idea of our new algorithm are to guess candidate itemsets in each level of itemset lattice starting from level k up to IC  n  1 and to verify such candidate itemsets To have a good guess some statistical data from input data are corrected during the database is scanned By using such information the candidate itemsets are gen erated Next, these candidate itemsets are verified by scanning the database If there are some errors from the guessing another scan through a database will be needed to eliminate such errors and produce the final solution of frequent itemsets Experiments based on different data sets have been conducted to evaluate per formance of the algorithm r 400 I 2oo 0 I 2000 10,om 50,000 100,000 150,000 200,000 ntrans Figure 3 Performance of Apriori and n,p al gorithm with tk20 np=100 sup=lO As the central point of the algorithm is precise guess ing of candidate itemsets the future works include sig nificant improvements in collecting statistics and ac curacy of guessing It is necessary to measure what are the costs of getting more complex statistics in the first pass through an input data set and what benefits may be achieved from such statistics in the remaining part of the algorithm It is also necessary to improve the internal data structures of the algorithm in order to eliminate an impact of inefficient searching methods on the overall performance References R Agrawal T Imielinski and A Swami Mining as sociation rules between set of items in large databases In Proceedings of ACM SIGMOD pages 207-216 May 1993 R Agrawal and R Srijant Fast algorithms for mining association rules In In Proceedings of the 20th VLDB Conference Santiago Chile 1994 S Brin R Motwani J Ullman and S Tsur Dy namic itemset counting and implication rules for mar ket basket data In Proceedings of ACM SIGMOD pages 255-264 1997 J Han J Pei and Y Yin Mining frequent patterns without candidate generation In Proceedings of A CM SIGMOD International Conference on Management of Data SIGMOD'OO Dallas TX May 2000 W Kloesgen New techniques and Technologies for Statistics chapter Tasks methods, and applications of knowledte extraction pages 163-182 10s Press Amsterdam 1996 50 


 1,200 0  1,000 E 200  I 1f1 4f3 5f3 6f3 723 nf P t-nirani=lO,OOO ti44 np=lO tntrans=100.000 11.20 nu=100 Figure 4 A performance of n,3 with increas ing ratio of n/p 6 H Mannila H Toivonen and A Verkamo Efficient algorithms for discovery in databases In U M F Eds and Ramasamy editors AAAI Workshop on Knowl edge Discovery in Databases pages 181-192 Seattle Washington 1994 7 C Matheus G Piatetsky-Shapiro and D McNeill Advances in Knowledge Discovery and Data Mining chapter Selecting and reporting what is interesting The KEFIR application to healthcare data pages 495 516 AA1 Press/The MIT Press, Cambridge 1996  A Savasere E Omiecinski and S Navathe An ef ficient algorithm for mining association rules in large databases In Proceedings of the 2lst VLDB Confer ence Zurich Swizerland 1995 191 T Senator H Goldberg J Wooten M Cottini A Khan C Klinger W Llamas M Marrone  and R Wong The financial crimes enforcement network ai system fais Identifying PO tential money laundering from reports of large cash transactions 1995 lo H Toivonen Sampling large databases for association rules In Proceedings fo the 22nd VLDB Conference Mumbai Bombay 1996 11 M Zaki S Parthasarathy M Ogihara and W Li New algorithms for fast discovery of association rules In 3rd International Conference on Knowledge Dis covery and Data Mining KDD pages 283-286 New port California August 1997 1 600           VI n 8R 81'3 8/4 8J5 8/6 8/7 8B  ntrans=l 00,000 np=lO U 12 ntrans=l0000 nn=lotI  14 Figure 5 A performance of 8,p\with increas ing parameter p 51 


that may be of interest to the user in the future A priority assignment method was proposed which exploits the con\002dence and support values of the association rules The priorities were used to prune the candidate set of data items in order to develop a smaller set that will 002t the limited storage of mobile clients Experiments were also conducted to gauge the effectiveness of the proposed methods in terms of the client cache hit ratio We observed that the use of association rules considerably improves the hit ratio of the client cache especially for small cache size The results are promising at this stage and can be improved further with some extensions We also performed experiments to measure the processing overhead of rule extraction and inferencing These results support our claim that neither infrequent mining nor inferencing introduces a considerable overhead to the system We believe that the performance could be improved even further with a more ef\002cient incremental mining and inferencing process One of the more appealing aspects of this work is that we bring old problems of production rules into the arena with a new method of rule gathering and application We proposed the usage of the data mining concepts for different applications such as obtaining production rules Temporality in association rules is an important aspect which captures the temporal behavior of associations We are planning to investigate how temporal association rules could be used for automated hoarding Listed below are some more heuristics for limiting the hoard set that we are planning to test 017 Considering the size of data items for determining what to hoard Hoarding 10 small items inferred by a lower priority rule might be better than hoarding a huge 002le that may consume half of the client cache inferred by higher priority rules 017 Considering the time to load a data item on the cache In case the hoarding for weak connection we can hoard big data items and let smaller ones be loaded by the client during the weak connection time The 223size\224 and 223time to load\224 heuristics seem to contradict each other However for some items transmission time might be high because of their location in the network 017 Considering if available expiration times of data items for the hoarding process We may not want to hoard data that will expire in the near future User request patterns may change over time For that reason the history should be analyzed to eliminate those patterns that no longer re\003ect more recent user request patterns The frequency of mining the history depends on the particular system in concern Ef\002cient incremental mining algorithms are proposed in CHNW96 D ynami c n at ure o f the client request patterns will be considered in the future by using incremental association rule mining algorithms References AAB  96 R Ag ra w a l A Arn i n g  T  Bo llin g e r  M Mehta J Shafer and R Srikant The quest data mining system In Proc of the 2nd Int'l Conference on Knowledge Discovery in Databases and Data Mining  Portland Oregon August 1996 AS94 R Ag ra w a l a n d R Srik an t F a st alg o r ith ms fo r mining association rules In Proc of the 20th Int'l Conference on Very Large Databases  Santiago Chile September 1994 CHNW D a vi d W ai Lok C h eung J i a w ei H a n V i ncent Ng and C Y Wong Maintenance of discovered association rules in large databases An incremental updating technique In Proceedings of the 12th International Conference on Data Engineering ICDE  pages 106\226114 1996 JHE99 J Jin g  A Helal an d A  K  E lmag armid  Client server computing in mobile environments ACM Computing Surverys  June 1999 KP G  K u enni ng and G  P opek A u t o mat e d hoarding for mobile computers In Proceedings of the ACM Symposium on Operating Systems Principles  St Malo France 1997 KS J a mes J  K i s t l e r a nd Mahade v S at yanarayanan Disconnected operation in the coda 002le system ACM Transactions on Computer Systems  10\(1\3\22625 1992 MJHS B a ms had M obas h er  N ami t J a i n  E ui H ong Han and Jaideep Srivastana Web mining Pattern discovery from world wide web transactions Technical Report 96-050 Department of Computer Science University of Minnesota September 1996 PS Ev aggel i a P i t oura a nd G e or ge S a maras  Data Management for Mobile Computing  Kluwer Academic Publishers 1998 SU99 Y u cel Say g i n a n d Ozg u r Ulu so y  Ex p l o itin g data mining techniques for broadcasting data in mobile computing environments Submitted for Journal Publication  1999 TLAC C a rl T a i t  H u i L ei  S w a rup A charya a nd Henry Chang Intelligent 002le hoarding for mobile computers In Proceedings of Mobicom'95  Berkeley CA November 1995 8 


Figure 8 Visual interface for Moridou system Search EngineTest Page 0 UI 0 5 5 Keyword plealet Figure 9 Prototype system in hcterogeneous environment 283 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


