Uncertainty Abstract-The KDD Drocess aims handling in the data mining process Michalis Vazirgiannis, Maria Halkidi Athens University of Economics  Business Patision 76 10434, Athens, Greece \(Hellas Department of Informatics at searchine for interesting instances of patterm in data sets It is widely accepted that the patterns must be comprehensible One of the aspects that are under-addressed in the KDD process is the handling of uncertainty in the process of clustering classification and association rules extraction In this paper we present a classification framework for relational databases so as to support uncertainty in terms of natural language queries 
and assessments More specifically we present a classification scheme of non-categorical attributes into lexically defined categories based on fuzzy logic and provides decision support facilities based on related information measures I INTRODUCTION The purpose of Data Mining is the extraction of knowledge from large data repositories The knowledge may have various forms such as classifications association rules decision trees etc In the vast majority of KDD systems and approaches the data values are classified to one of a set of categories that have resulted from a clustering process Then we have two issues that may result in knowledge to be partially extracted or not to be extracted at all during the KDD process 
We address the following facts and their implications the clusters are not overlapping This means that each database value may be classified into at most one cluster, in some cases it falls out of the cluster limits so it is not classified at all Though, everyday life experience leads us to the fact that actually a value may be classified into more than one categories For instance a male person 182cm high in Central Europe is considered as of 223medium\224 height as well as 223tall\224 to some degree the data values are treated equally in the classification process In traditional data mining systems 
database values are classified in the available categories in a crisp manner i.e a value either belongs to a category or not The person of the above example is considered as tall and also another person 199cm high is also considered tall It is profound that the second person satisfies to a higher degree than the first the criterion 223tall\224 This piece of knowledge the difference of belief that A is tall and also B is tall cannot be acquired using the schemes As it is clear from the above brief analysis there is interesting knowledge that is not captured due to the fact that uncertainty is not considered in the KDD 
process The KDD process mainly aims at searching for interesting instances of patterns in data sets It is widely accepted that the patterns e.g classifications rules etc must be comprehensible i.e they should be understood by the analysts 8][5 Assume the transaction log of a computer sales store and that a subset of its scheme is R  client-salary client-age price Applying the techniques proposed in  121 we would have to come up with rules of the form cIient-salafy[8000,1 lOOO and client-age[25-40 rprice[l300,2000 with fuzzy logic Apparently the rule introduced above is not clearly comprehensible since it 
does not place the rule in the greater context of the involved attributes i.e what does that range client_salury[3000,4500 mean in the full range of salaries as well as in their population distribution features A managedanalyst as non-domain expert would not understand the meaning of such a rule since the underlying data semantics are not made clear in the rule context Thus a requirement for understandable patterns of knowledge as results of the data mining process arises This will be achieved by classifying the data into understandable categories represented by natural language values Another issue is the 223crispness\224 of the value domains imposed by this approach For 
instance see Table I the tuple with tid=ll is excluded from the supporting set although all its values support quite well the rule apart from the value of the attribute 223price\224 which is only 0.00615 out of the required range The result is that many 223interesting\224 tuples \(i.e. contributing to the semantics hidden behind such a rule are rejected due to the crisp limits that have been set. It is evident that the problem here is that the classification of the values in these domains is flat i.e all the values in the domain are treated equally as for the criterion of partitioning i.e the price The partition in domains reflects the classification of the attribute 
values in categories i.e 221.\221very cheap\224 223cheap\224 223moderate\224 223expensive\224 These natural language expressions should be mapped to the underlying database through a layer that maps the natural language terms to the underlying database schema and values TABLE 1 THE SALES TRANSACTION LOG TABLE I I I 10 14447 1\2229 1136 11 19765 136 11292 I I I 12 16822 137 11136 13 18763 179 I1444 I I I I 14 11643 I66 18 15 15387 173 1283 4584 18 6963 69 983 19 2323 80 742 0-7803-5877-5/00/$10.00 0 2000 IEEE 393 


Another requirement addressed is the usage and reveal of uncertainty in this context is an important issue  Another issue addressed in the bibliography is the relatively few efforts that have been devoted to classical data analysis techniques like clustering  classification in the area of data mining research In this paper we propose a methodology that represents uncertainty in the classification stages in KDD environment for large relational databases so as to support uncertainty in terms of belief measures More specifically we present A scheme that classifies non-categorical attribute values into categories maintaining the classification belief We use fuzzy logic in order to represent and manipulate this belief Information measures for the evaluation of the above defined classification scheme based on fuzzy logic concepts We can exploit classification belief based on these measures in order to support decision-making related to one or multiple data sets The paper is organized as follows In section two we present the classification scheme while in section three we propose information measures based on fuzzy logic in order to evaluate and exploit the information included in proposed scheme In section four we elaborate on the multi dimensional extension of this scheme while in section five we present reasoning based on proposed information measures We conclude in section six by summarizing and providing further work directions 11 CLASSIFICATION SCHEME The term classification implies the procedure according to which each of a set of values is decided to belong into one of a set of related categories As it is well known in order to classify a data set there has to be a set of clusters as a result of a preceding clustering process In this research effort we assume that there is a given set of clusters for each attribute As we mentioned in previous sections each value that belongs to a cluster category should not be treated equally but contribute according to its classification belief Thus we also assume a set of mapping functions assigned to the clusters as a result of an enhanced clustering process Then each database value is mapped to a category bearing a d.0.b degree of belief\for this classification as a result of using the corresponding mapping function The classification scheme is applied on a data set S under a certain relational schema R  Ai where Ai is an attribute The values of the non-categorical attributes Ai are classified into categories according to a set of categories L li where ii a category for instance 223tall\224 223short\224 etc and a set of classification functions based on fuzzy logic methodologies The result of this procedure is a set of degrees of belief d.0.b.s M pl,\(tk.Ai Each member of this set represents the confidence that the specific value tk.Ai where tk is the tuple identifier belongs to the set denoted by the category li A Classification space CS The term Classification Space CS implies the specifications for mapping data base values to the fuzzy domain. Each value of the database is classified in one of the above mentioned categories clusters with an attached d.0.b We assume the attribute 223client-salary\224 from our running example which in a data set ranges between the values 1500 and 10000 In real world people characterize the value of a salary as high low moderate How would one classify a specific salary value Min Max Function 1822263034384246&54586266707478 18 30 50 I 40 60 80 decr triangle increasing Fig 1 The transformation functions for the attribute 223client-age\224 Min Max Function into a category What are the values\222 ranges corresponding to these categories Are they overlapping As it is clear, there is inherent uncertainty in the classification of a value in a set of categories A fundamental issue is the acquisition of the related knowledge i.e the categories the corresponding value ranges and the mapping functions between the real values and the fuzzy domain Assuming the appropriate set of value domains for these categories for each attribute Ai we define the corresponding classification set LA  ct I ct is a classification tuple The classification tuples are of the form: \(li,[vl vz fi where 1 is a lexical category v vz is the corresponding value interval and fi the assigned transformation function The value domains may be overlapping This increases the expressive power of the classification mechanism since some values may be classified to more categories than one with different d.0.b.s Then the collection of all the classification tuples ct related to the relational schema R forms the Classification Space CS which defines the mapping of the data set to the fuzzy domain In Table I1 the CS appears based on the schema of our sales example see Table I For each attribute a set of lexical categories the corresponding domain limits and the related transformation functions are provided The transformation function selection is an important issue that can affect the results of classification In our system we have currently adopted linear functions decreasing triangle and increasing 7 In Figure 1 the mapping of the age values to the fuzzy domain appears based on the CS specifications see Table 11 I IO 35 70 15 50 80 150 decr triangle triangle triangle TABLE II THE CLASSIFICATION SPACE FOR THE SALES SCHEMA client-salary I Min 1 Max 5500 10000 Function decr trian le Increasin price I very cheap Cheap moderate expensive 394 


As it is depicted in the figure 1 the value domains may be overlapping so that an age value may be classified into two categories In a similar way the rest of the attributes are mapped to the fuzzy domain For each tuple t k in the data set S there is a value t k.Ai that corresponds to the attribute Ai Then the d.0.b that this value belongs to the sets denoted by the categories and the corresponding domains is where f is the transformation function that maps the value t,.Ai to the fuzzy domain The choice of functions is a fundamental issue and will have a great impact on the creditability of the d.0.b.s. Thus it is clear that for each value t k.A, a set of d.0.b.s pli\(S.tk.Ai is produced Assume n is the number of tuples in the relation and lAi is the number of categories corresponding to the attribute Ai then the overall number of d.0.b.s produced is li S.tk.L f tk.L 1 xnr 221Ai 2 Ai B Classification Value Space CVS The result of the transformation of the data set values to the fuzzy domains using the CS is a 3D structure \(see Figure 2 further called Classification Value Space CVS The front face of this structure stores the original data set included in Table I while each of the other cells C[Ai lj tk  where j,k 1 stores the d.0.b pli\(S.tk.Ai\Further we reference a cell in the CVS as CVS\(tk.Ai.lj The higher the d.0.b is the higher is our confidence that the specific value belongs to the specific set It is interesting to have an overall measure of classification information which is included in the values of the attribute with regard to each category The algorithm for computing the d.0.b.s for the data set values with reference to the CS follows for each category Cj of Ai for each attribute Ai in CS for each value tk.Ai in the data set end compute d.0. b.\(Ai Cj tk.Ai end end Thus the time complexity is O\(d*c*n where d is the number of attributes data set dimension c is the number of categories\(c1usters and n is the number of d.0.b values for a category n number of tuples in the data set Usually c d  n Thus the time complexity for computing the d.0.b.s for a data set will be O\(n 111 INFORMATION MEASURES IN THE CVS Classification Categories r 1 Tuples Data Set S Fig 2 The CVS holding the \223degrees of belief\222 d.0.b.s for the classification of the attributes\222 values A Category energy metric Let Ai be an attribute and li a related category Then the overall belief that the current data set S contains data that are successfully classified in the category li is given by the normalized information measure 3 where n is the number of tuples in the data set hence the number of values of the attribute and q is a positive integer The usual value of q is 2 Higher values suppress lower d.0.b making thus the contribution of the tuples with high close to 1 d.0.b.s more significant The exponent l/q is used to amortize the effect of the exponent q We can further compare the information measures of different categories of the same attribute So for a given data set S and a given attribute A with attached categories 11 12,..,ln the corresponding information measures E li S.A are ordered making thus feasible to decide which category has better support by the data set and also to compare. For instance the query 223Does the sales database contain mostly high of medium salaries?\224 is answered by comparing the values Emdium salary\E high salary as resulting from \(3 B Attribute energy metric The overall energy of an attribute Ai, is the normalized sum of the energy metric values for all the attribute categories This measure expresses the average overall information energy that is included in the values of the attribute i.e how strong is the belief for the classification assessment and also the amount of information regarding the considered categories Hence 4 where C is the number of categories for the attribute Ai Essentially EA,\(S is a measure of how successful is the classifications scheme EIi\(S.Ai XLl S\222tk\221Ai  r L EA s  xli Eli C The CVS conveys significant knowledge included in cumulative information measures One of the imoortant c cvs E  classification quality information measures that have been proposed in the bibliography is the Energy metric 7 which reflects the information auantitv that is included in a fuzzv set This in the data set is given bv the equation The overall information that is included in the CVS and represents the amount of classification information included 5 information quantit essentially is a measure of\222the overall Ecvs  CEAi  belief for a fuzzy set Ai where Ai are the attributes The result is the information content of the CVS This measure is used to compare different data sets as for their information content Data sets with higher Ecvs correspond to higher overall measure of 395 


information This energy shows how significant is the information contained in the values of this attribute and also how well the data set fits to the classification scheme. Also this measure is an indication of the quality of classification In principle an ideal classification scheme should maximize the Ecvs value Indeed when Ecvs is maximized the uncertainty is minimized and thus the confidence for classification is high IV MULTI-DIMENSIONAL CLASSIFICATIONS Another need that arises is the representation of the d.0.b related to composite classifications of tuples For instance we are interested to know to what degree a tuple in our sample data set satisfies more than one criteria e.g 223morning and cheap purchases\224 The term 223morning and cheap\224 defines a new category and we need to provide a mapping function for this. In this case we can introduce two alternatives Classijication based on multi-dimensional clusters In this case we define clusters initial categories\for our data set taking into account all the attributes referred to our criteria Then the clustering process produces multi dimensional clusters and we can define the membership functions for them based on the procedure used in the case of one-dimensional data sets Classification based on one-dimensional clusters We adopt the min measure for composition of fuzzy predicates from the bibliography 7 Thus for two attributes At A and I lj two corresponding categories  li referring to A and lj to A the d.0.b that a tuple tk belongs to the set characterized by the predicate 223AJi and AJj\224 is given by the equation pllandl tk.At tk.Ae  min\(pli tk.At plj tk.fb 6 The overall information measure related to the criterion 223A,.li and A,.l,\224 is given by the equation Liiandlj t k 221At 7 k 1 which represents the belief that tuple tk has both features belongs to both categories li Zj and therefore it is classified accordingly For instance, we may submit the query 223What is the overall belief that the database contains transactions for cheap purchases made in the morning?\224 A An experimental study of multi-dimensional classification approaches The objective of this study is to compare two approaches described in section I11 for the definition of multi dimensional classification More specifically we use a data set with data related to stock exchange transactions and we compute the overall energy produced by the adoption of the above-described alternatives The size of our data set was 1000 tuples and its schema is  R  closegrice highgrice volume where closegrice is the daily closing price of the stock highgrice is the highest price of the stock during a session and volume is the number of transactions for the specific stock In this point we also have to mention that the energies of our data set classification scheme computed based on a system we have implemented according to the above described classification framework TABLE 111 ENERGY METRIC FOR A GIVEN NUMBER OF CLUSTERS Close Price, Volume TABLE IV ENERGY METRIC FOR THE OPTIMAL CLUSTERING SCHEME Close Price Volume __ Ecl-vol p,284 I The overall result of this study is that the first approach based on multi-dimensional clusters produces better classification schemes Multi-dimensional clustering extracts clusters that are the best partitioning for a data set as it examines simultaneously all the attributes dimensions Also categories that are not supported by the data set ignored and thus the classification scheme could be adjusted better to the data set Assuming a two dimensional data set closegrice volume of stock exchange database we demonstrate the above with the following experiments 1 Classification scheme based on a given number of clusters In this case, the clustering procedure is applied for a given number of clusters so as to compare the results of the two approaches with respect to the definition of a data set partitioning that is as good as possible for the given number More specifically we apply clustering to each of the attributes closegrice volume so as to define three clusters categories for each of them Thus we defined nine new dusters for the category 224closegrice and 223volume\223 combining the extracted categories of each attribute Then we apply two-dimensional clustering in order to define a partitioning of the data set into nine clusters Table I11 presents the overall energy as in 4 as computed in each of the approaches As it is obvious the overall belief produced by two-dimensional clustering is higher It is also noteworthy that none of the nine categories produced by multi-dimensional clustering has zero 0 energy in contrast to the case of one-dimensional As a consequence the approach based on multi 396 


TABLE v SAMPLE QUERIES AND THE RELATED INFORMATION MEASURES salaries?\224 dimensional clustering searches for the best nine clusters that can be extracted by the data set and ignores the categories that are not supported 2 Classification based on optimal clustering schemes In this case we apply clustering procedure giving a range in which the number of clusters can take values and we ask for the optimum clustering scheme The selection of the clustering scheme is based on well-defined quality clustering criteria Table IV shows the overall energy in each of the multi-dimensional classification approaches The result of comparing two approaches is that the overall belief produced by two  dimensional clustering is higher and as a consequence the classification scheme defined is better The clustering procedure has defined the optimum partitioning of the data set taking into account both attributes and thus the outcome clusters are adjusted better to our data set than the clusters produced by the combination of clusters defined by separate attributes We carried out a similar study for three-dimensional data sets and we concluded into similar results to two-dimensional data i.e that multi-dimensional clustering could result in better initial categories for the multi-dimensional classification v REASONING WITH INFORMATION MEASURES The result of the KDD procedure is a set of assessments about the underlying data These assessments should be in an understandable form for the humans so that they will be useful and exploitable The scheme presented above contributes to this requirement since the results of the data set can be represented in the form of natural language statements The information measures mentioned above are exploited to support queries and decision support of the following categories A Single data set single attribute queries Here we have queries related to categories of an attribute in the same data set In Table V there is a list of indicative queries and the way they are handled by the classifications scheme B Multi-data set queries In this category we are concerned with queries that involve two or more data sets of the same relational schema and CS Assume two data sets including sales in two different supermarkets namely S1 S2 Then the queries appeared in Table VI can be processed using the information measures defined above VI RELATED WORK One of the three components~of a KDD system is the model whose functions among others include 5 the classification procedure 223Classification\224 aims at mapping an object to a predefined set of categories/classes unlikely to the 223clustering\224 procedure where the extraction of the classes from a set of data is achieved by finding grouping of values and similarity metrics The classification problem has been studied extensively in statistics pattern recognition and machine learning community as a possible solution to the knowledge acquisition or knowledge extraction problem  1 I TABLE VI QUERIES INVOLVING MULTIPLE DATA SETS Query I Value returned 223Which of the SI S2 1 If Lm,dS 1 time_ofb  contains more transactions made early morning?\224 223In which supermarket there are more cheap purchases made in the evening?\223 E,,,aning\(Titime-oC-pjj\222 retum Lmmg\(S1 time-of-p else retum S2.time-of-p If EdleapandeWing\(S1 price. S.time-of-p  heap and evening\(S2.pri~e S.time-of-p else retum Edcapandevening\(S 1 price S.time-of-p retum Ehrap andrvmlnu\(S2.pri~e S.time-of-p A number of classification techniques have been developed and are available in bibliography Among these the most popular are Bayesian classijication 3 Neural Networks  11 and Decision Trees  141 The above reference to some of the most widely known classical classification methods denotes the relatively few efforts that have been devoted to data analysis techniques \(i.e classification in order to handle uncertainty These approaches produce a crisp classification decision so an object either belongs to a class or not which means that all objects are considered to belong to a class equally Moreover most of the classification proposals and algorithms consider the classes as non-overlapping SI It is obvious that there is no notion of uncertainty representation in the proposed methods though usage and reveal of uncertainty is recognised as an important issue in research area of data mining 6 For this purpose the interest of research community has been concentrated on this context and new classification approaches have recently been proposed in bibliography so as to handle uncertainty The issue of classification involves the definition of categories that group the values of an attribute A in sets that have a specific feature. A recent approach in classification for data mining is presented in 4 Also an important issue in data clustering and classification is the extraction of appropriate value intervals that correspond to logical categories related to an attribute An interesting approach related to this issue is addressed in 9 An approach for pattern classification based on fuzzy logic is represented in lo The main idea is the extraction of fuzzy rules for identifying each class of data The rule extraction methods are based on estimating clusters in the data and each cluster obtained corresponds to a fuzzy rule that relates a region in the input space to an output class Thus for each class ci the cluster centre is defined that provides the rule If input is near x then class is e Then for a given input vector x the system defines the degree of fulfilment of each rule and the consequent of the rule with highest degree of fulfilment is selected to be the output of the fuzzy system As a consequence, the approach uses fuzzy logic to define the best class in which a data value can be classified but the final result is the classification of each data to one of the classes 397 


In 13 an approach based on fuzzy decision trees is presented and aims at uncertainty handing It combines symbolic decision trees with fuzzy logic concepts so as to enhance decision trees with additional flexibility offered by fuzzy representation More specifically they propose a procedure to build a fuzzy decision tree based on classical decision tree algorithm ID3 and adapting norms used in fuzzy logic to represent uncertainty  131 However, there is no evaluation of proposed inference procedures as regards the quality of new sample classification In general there are some approaches proposed in bibliography which aim at dealing with uncertainty representation e.g fuzzy decision trees According to these approaches each data value can be assign to more than one categories with an attached degree of belief However they don\222t propose ways to handle classification information and exploit it for decision-making In this paper we propose an approach that aims at uncertainty handling in the classification process based on fuzzy logic concepts We propose a classification framework that maps data to fuzzy domains and maintains uncertainty in terms of degrees of belief VII CONCLUSIONS One of the objectives of a KDD process is to produce understandable knowledge in terms of patterns detected in a large data set We feel there is a lot of potential in the area of mining patterns of knowledge as regards classification of quantitative attributes In this paper we presented 0 A scheme for classification of database values putting emphasis in uncertainty handling and classification quality measures The classification scheme maintains the uncertainty through the maintenance of a framework based on fuzzy logic Information measures for the classification scheme based on the energy metric function which reflect the information quantity that is included in a fuzzy set. Based on these measures we can compare different data sets as to the degree they fit to the classification scheme or compare different data sets under a specific criterion Also we extract 223useful\223 knowledge for reasoning and decision making based on the information measures Moreover we present how the proposed classification scheme can be used for multi-dimensional classification so as to support decision-making that combines more than one classification criteria For this purpose we proposed two approaches i classification based on multi-dimensional clusters ii classijication based on one-dimensional clusters while we described an experimental study we have carried out in order to evaluate these two approaches The overall result of this study is that the approach based on multi dimensional clusters produces better classification schemes Further work will be concentrated in usage of the proposed framework in order to adjust an initial classification model according to the feedback we get by classifying different data sets This adjustment will result in classification scheme that maximizes the energy metric functions related to the various related entities The overall objective in this case is the incremental production of optimal classification and association extraction models Also we aim at the study of different mapping functions and their effect to the proposed classification scheme as regards uncertainty representation Moreover in future more information measures for our classification scheme will be proposed based on various proposed in bibliography and they will be evaluated in order to select the optimal definition for the classification quality measures REFERENCES l M Befry G Linoff Data Mining Techniques For marketing Sales and Customer Support John Willey  Sons, Inc, 1996 2 S Chaudhuri 223Data Mining and Database Systems where is the intersection\224, bulleting of the IEEE CS TC on Data Engineering, 1997 3 P Cheesman J Stutz 223Bayesian Classification Autoclass Theory and Results\224 in Advances in Knowledge Discovery and Data Mining\224 Editors U Fayyad et al AAAI Press, 1996 4 M Dalkilic E Robertson D.V Gucht 223CE The classifier-Estimator for Data Mining\224 in the proceedings of IFIP-DS7 Conference on Database Semantics 1997  U Fayyad G Piatetsky-Shapiro, P. Smyth  223The KDD process for extracting Useful Knowledge from Volumes of Data\224 in CACM vo1.39 1 1 1996, pp 27-35  C Glymour D Madigan D Pregibon P Smyth 223Statistical Inference and Data Mining\224 in CACM 7 M.GUPTA and T YAMAKAWA eds Fuzzy Logic and Knowledge Based Systems Decision and Control North-Holland\198 8 8 W Kloegen 223Explora A1 Multipattern and Multistrategy Discovery Assistant\224 in the book 223 Advances in Knowledge Discovery and Data Mining\224 Editors U Fayad et al AAAI Press, 1996 9 D Rasmussen R Yager 223Induction of Fuzzy Characteristic Rules\224 in the proceedings of the First European Symposium PKDD Trondheim 1997  101 S Chiu. \223Extracting Fuzzy Rules from Data for Function Approximation and Pattern Classification\223 Fuuy Information Engineering A Guided Tour of Applications.\(Eds D Dubois, H. Prade R Yager\1997  Rastori K Shim 224PUBLIC A Decision Tree Classifier that Integrates Building and Pruning\224 Proceeding of the 241h VLDB Conference New York USA 1998  121 R Srikant R Agrawal 223Mining Quantitative Association Rules in Large Relational Tables\224 in the proceedings of ACM-SIGMOD \22296 Conference  1312 Cezary, Janikow 223Fuzzy Decision Trees Issues and Methods\223 IEEE Transactions on Systems Man and Cybernetics Vol 28 Issue 1, pp 1-14, 1998 01.39 1 1 1996, pp. 35-42 14 T Mitchell Machine Learning McGraw-Hill 1997 398 


has possibilities to be the platform for large scale web mining We have introduced some raw results of data mining on a portal site with focus on mobile users However we have not provided the evaluation of the quality of the results yet We believe that the quality of web mining techniques depends on the application The compari son with other techniques in certain applications such as prefetching and recommendation system will be one of our future work Acknowledgements We would like to thank people from NTT Software in particular Mr Katsumi Takahashi and Dr Atsuhiro Goto for providing the log file of MIS and helpful dis cussions References R Agrawal T Imielinski A Swami 224Mining Association Rules between Sets of Items in Large Databases\224 In Proc of the ACM SIGMOD Con ference on Management of Data 1993 R Agrawal, R Srikant 224Fast Algorithms for Min ing Association Rules\224 In Proc of the VLDB Con ference 1994 R Agrawal R Srikant 223Mining Sequential Pat terns\224 221In Proceedings of Int Conf on Data En gineering March 1995 R Srikant R Agrawal 223Mining Sequential Pat terns Generalizations and performance improve ments\224 221In Proceedings of 5th Int Conf on Ex tending Database Technology March 1996 G 0 Arocena A 0 Mandelzon G A Mihaila 223Applications of a Web Query Language\224 221In Pro ceedings of WWWG April 1997 S Brin L Page 223The Anatomy of a Large Scale Hypertextual Web Search Engine\224 In Proceedings of WWW7 May 1998 A Buchner M D Mulvenna 223Discovering inter net marketing intelligence through online analyti cal Web usage mining\224 In SIGMOD Record 4 1999 R Cooley B Mobasher J Srivistava 223Data preparation for mining World Wide Web browsing patterns\224 In Journal of Knowledge and Informa tion Systems 1 1999 9 E Spertus L A Stein 223Squel A Structured Query Language for the Web In Proceedings of WWW9 May 2000 lo M Houtsma A Swami 224Set-oriented Mining of Association Rules\224 In Proc of International Con ference on Data Engineering March 1995 ll J Kleinberg 223Authoritive sources in s hyper In Proceedings of ACM linked environment\224 SIAM Symposium in Discrete Algorithm 1998 12 S Lawrwence L Giles 224Accessibility of informa tion on the web\224 In Nature Vol 400 pp 107-109 1999 E131 M Perkowitz 0 Etzioni 223Towards Adaptive Web Sites Conceptual Framework and Case Study\224, In Proceedings of WWW8 May 1999 14 Katsumi Takahashi Seiji Yokoji Nobuyuki Miura 224Location Oriented Integration of Internet Infor mation  Mobile Info Search\224 In Designing the Digital City Springer-Verlag, March 2000 15 Takayuki Tamura Masato Oguchi and Masaru Kitsuregawa 223Parallel Database Processing on a 100 Node PC Cluster Cases for Decision Sup port Query Processing and Data Mining\224 In Pro ceedings of SC97 High Performance Networking and Computing\(SuperComputing 22297 November 1997 16 S Thomas S Sarawagi 223Mining Generalized Association Rules and Sequential Patterns Using SQL Queries\224 221In Proceedings of Int Conf on Knowledge Discovery and Data Mining March 1998 17 T Yan M Jacobsen H Garcia-Molina U Dayal 223From user access patterns to dynamic hypertext linking\224 In Proceedings of WWW5 May 1996 134 


267  Feature dimensions  on which the generated rules may be dimensioned such as merchant, time and area  A volume cube C v is sufficient for deriving the instances of rule X 336 Y if it has a base dimension that represents the base of the rule, and the association conditions for qualifying X 331 Y are definable on C v For deriving cross-sale association rules from cube SaleUnits an association condition can be  for each base and feature dimension C v product A\ > 0 331 C v product B\ > 0 If the association conditions used to compute multidimensional P X  307 P Y  are definable on C v then another kind of condition, called antecedent conditions that are used to compute multidimensional  P X    are also definable on C v such as  for each base and feature dimension C v product A\ > 0 Association cube The association cube C a  for rule X 336 Y gives a volume-based measure of multidimensional association relationships that are computed from the volume cube C v and is used to derive the confidence cube and the support cube of association rules. More exactly, it maintains dimensioned P X 307 P Y i.e the number of base elements that satisfy X 331 Y Usually C a is dimensioned differently from C v In the cross-sale association rule example, the association cube is defined as  CrossSales product, product2, customer_group, merchant time, area  A cell of this cube, CrossSales product 221A\222 product2 221B\222  customer_group \221 engineer\222 merchant 221Sears\222 time 221Jan98\222 area 221Los Angeles\222\eans that there are 4,500 customers who are engineers, who bought item A as well as item B, at a Sears store in Los Angeles in Jan98 For an association cube the item dimensions underlie the counts for deriving association rules, such as dimensions product and product2 for the above CrossSales cube. The dimension product2 has the same set of values as product and we call it the mirror dimension  of product We introduce a mirror dimension simply because the cross-sale association rule involves more than one element of the item dimension  The base dimension   such as the customer dimension  underlies the base of rules. Unlike the volume cube, the association cube does not necessarily have to be dimensioned by the base dimension However, we can dimension rules by a derived dimension, each value of which identifies a group of base dimension values at bottom levels. In the cube CrossSales shown above, we introduce the hierarchical dimension customer_group   which has levels customer_profession\222, \221customer_category\222 and 'top'. A relation is also defined for relating customers and customer groups. For example, a value of the derived customer_group dimension, say, \223engineer\224, is used to identify a group of individual customers who are engineers  An association can cube also have underlying feature dimensions  such as merchant, time and area Population cube and base cube The population cube C p and the base cube C b for rule X 336 Y are also derived from the volume cube C v  C p is used to measure dimensioned P X i.e the numbers of base elements satisfying X  C b is used to represent dimensioned B For the above cross-sale rules, the population cube is defined as NumOfBuyers \(product, customer_group, merchant, time area A cell of this cube NumOfBuyers product 221A\222 customer_group \221 engineer\222 merchant 221Sears\222 time 221Jan98\222 area 221Los Angeles\222  10000  means that there are 10,000 customers who are engineers, and who bought item A in Los Angeles in Jan98. The base cube is defined as  NumOfShoppers \(customer_group, merchant, time, area Note that NumOfShoppers is not aggregated from NumOfBuyers as a single customer may buy multiple products Confidence cube and support cube The confidence of rule X 336 Y defined as P X 307 P Y  P X  and the support, defined as P X 307 P Y B are represented as cubes C f  and C s  C f  is derived from C a and C p and C s is derived from C a  and C b They have the same dimensions as C a For the above cross-sale rules the confidence cube and support cube are defined as  Confidence \(product, product2, customer_group, merchant time, area   Support  product, product2, customer_group, merchant time, area Figure 2 shows the cubes related to cross-sale association rules, with one slice of each cube. The volume-cube is generated from transactions; the 


association-cube, base-cube and population-cube are derived from the volume cube; the confidence-cube is derived from the association cube and population cube and the support-cube is derived from the associationcube and base-cube. The slices of these cubes shown in Figure 2 correspond to the same list of values in dimension merchant, time, area and customer_group  Multidimensional and multilevel rules Representing association rules by cubes and underlying cubes by hierarchical dimensions, naturally supports multidimensional and multilevel rules. Also these rules are well organized and can be easily queried  First, the cells of an association cube with different dimension values are related to association rule instances in different scopes. In the association cube CrossSales cell CrossSales product \221A\222, product2 \221B\222  customer_group 221engineer\222, merchant \221Sears\222, area \221Los Angeles\222, time 221Jan98\222 represents the following multidimensional rule x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x,\221B\222  275 customer_group = \221engineer\222, merchant = \221Sears\222, area 221Los  Angeles\222, time =  \221Jan98\222 If this cell has value 4500, and the corresponding cell in the population cube has value 10000, then this rule has confidence 0.45 Next as the cubes representing rules can have hierarchical dimensions, they represent not only multidimensional but also multi-level association rules. For example, the following cells CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221Jan98\222 CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221 Year98 222 represent association rules at different area levels \(i.e the city level and the state level\d different time levels \(i.e., the month level, the year level x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221Jan98\222 x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221 Year98 222 The cell CrossSales\(product \221A\222, product2 \221B\222,  customer_group 221top\222, merchant \221top\222, area \221top\222,  time \221top\222 represents the customer-based cross-sale association rule for all customers, merchants, areas, and times in the given range of these dimensions, expressed as x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222 4.3  Generating Association Rule Related Cubes The basic task of our OLAP based association rule mining framework, either at the GDOS or at an LDOS is to convert a volume cube i.e. the cube representing the purchase volumes of customers dimensioned by product  area etc, into an association cube a base cube and a population cube These cubes are then used to derive the confidence cube and the support cube of multidimensional association rule instances. The following general steps are involved in cross-sale association rule mining 267  Roll up the volume cube SaleUnits by aggregating it along merchant, time, area dimensions 267  Derive cube NumOfBuyers from SaleUnits based on the antecedent condition SaleUnits 0 267  Populate cube NumOfShoppers by the counts of customers dimensioned by merchant, area  time not by product\at satisfy the antecedent conditions 267  Derive cube CrossSales from SaleUnits based on the association conditions SaleUnits  product  p 1  0 and SaleUnits  product2  p 2 0 267  Derive cube Confidence and cube Support using cell-wise operations 214  Confidence = CrossSales  NumOfBuyers 214  Support  CrossSales  NumOfShoppers  Cubes Confidence  Support  CrossSales are dimensioned by product  product2 customer_group,merchant  time, area NumOfBuyers is dimensioned by product  customer_group, merchant time, area  NumOfShoppers is dimensioned by customer_group, merchant  time, area Rules with confidence and support that exceed specified thresholds  may be considered interesting 4.4. Rules with Conjoint Items Cubes with conjoint dimensions can be used to represent refined multidimensional association rules For example, using OLAP, we can derive association rules across time  Time-variant or temporal association rules such as 


x 316 Customers buy_product\(x,\222 A\222, \221 Jan98\222  336 buy _product\(x, \221B\222, \221 Feb98\222   275 area = \221Los Angeles\222 can be used to answer such questions as \223 How are  the sales of B in Feb98  associated with the sales of A in Jan98 224 The items in this rule are value pairs of dimensions product and time In order to specify this kind of association rule we introduce a conjoint dimension product, time and mirror it with dimension product2, time2 This allows a cell in the association cube to cross two time values. Accordingly, the cubes related to association rule mining are defined as follows Association cube  CrossSales.2 \(<product, time>, <product2, time2 customer_group, merchant, area  Population cube  NumOfBuyers.2  \(<product, time>, customer_group merchant, area Base cube  NumOfShoppers.2  \( customer_group, merchant, area Confidence cube Confidence.2 \(<product, time>, <product2, time2 customer_group, merchant, area Support  cube  Support.2  product, time>, <product2, time2 customer_group, merchant, area  The steps for generating these cubes are similar to the ones described before. The major differences are that a cell is dimensioned by, besides others product, time and product2, time2 and the template of the association condition is  SaleUnit s  product p 1 time t 1  0 and  SaleUnits  product2 p 2 time2 t 2  0 where, in any instance of this condition, the time expressed by the value of time2 is not contained in the time expressed by the value of time The template of the antecedent condition is SaleUnits   product p 1 time t 1  0 In general, other dimensions such as area may be added to the conjoint dimensions to specify more refined rules 4.5. Functional Association Rules A multidimensional association rule is functional if its predicates include variables, and the variables in the consequent are functions of those in the antecedent.  For example, functional association rules can be used to answer the following questions, where a_month and a_year are variables q  What is the percentage of people in California who buy a printer in the next month after they bought a PC x 316 Customer buy_product\(x, \221PC\222, a_ month 336 buy_product\(x, \221printer\222, a_month+1  275 area = \221California\222 q  What is the percentage of people who buy a printer within the year when they bought a PC  x 316 Customer: buy_product\(x, \221PC\222, a_ year 336 buy_product\(x, \221printer\222, a_year 275 area = \221California\222 To be distinct, we call the association rules that are not functional as instance association rules; e.g x 316 Customer: buy_product\(x,\222 PC\222, \221Jan98\222 336 buy_product\(x,\222 printer\222, \221Feb98\222  275 area =  \221California\222 Time variant, functional association rules can be derived from time variant, instance association rules through cube restructuring. Let us introduce a new dimension time_delta that has values one_day, two_day 205, at the day level, and values one_month, two_month, \205, at the month level, etc. Then, let us consider the following functional association rule related cubes Association cube  CrossSales.3 \(product, product2, customer_group merchant, area, time_delta  Population cube  NumOfBuyers.3 \(product, customer_group, merchant area Base cube  NumOfShoppers.3 \( customer_group, merchant, area Confidence cube  Confidence.3 \(product, product2, customer_group merchant, area, time_delta Support cube  Support.3 \(product, product2, customer_group, merchant area, time_delta The association cube CrossSales.3  can be constructed from CrossSales.2   The cell values of CrossSales.2  in the selected time and time2 ranges are added to the corresponding cells of CrossSales.3 For example, the count value in cell  CrossSales.2\(<PC, Jan98>, <printer, Feb98>\205 is added to cell \(bin CrossSales.3\(PC, printer, one_month,\205 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


