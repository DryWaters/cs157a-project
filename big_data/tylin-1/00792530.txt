499 ARDx  A Fuzzy Expert System for ARD Site Remediation J.V Balcita J.A Meech M.M Ghomshei Department of Mining and Mineral Process Engineering University of British Columbia Vancouver B.C Canada ABSTRACT This paper details development of an expert system using fuzzy techniques to design remediation techniques for sites contaminated by 
Acid-Rock-Drainage. The fuzzy system is able to deal with missing, inaccurate, or heuristic data and still make useful design decisions Fuzzy sets are defined using a functional relationship between the degree of belief in a certain qualitative concept and one or more quantitative variables Rules were developed during interviews with a chosen expert in the field Using user input site data and characterization the association of the degree of belief in a concept with that of other concepts come 
together within these rules to a produce a decision or conclusion The development of a hzzy expert system for ARD is a benefit since it produces a standardized adaptable approach to the problem and provides quick advice to a user looking for a preliminary but detailed analysis The work done to this point includes a fuzzy controller separate control modules for treatment options cost analysis and interactive hypertext documents The controller and control modules work together 
in an attempt to follow the decision-making process as it chooses an appropriate treatment option for a site with possible or existing ARD The hypertext documents are set up as user help-resources to provide system output information and to use as a training tool on treatment options for ARD or as a diagnostic tool on the possibility of implementing a treatment system BACKGROUND Acid Rock Drainage \(ARD is contaminated acidic drainage 
from the spontaneous weathering and oxidation of pyrite and other sulfide minerals l Weathering conditions increase the solubility of heavy metals radionuclides sulfate and acidity and reduce the pH of the drainage ARD impacts on watershed characteristics and creates adverse effects in the surrounding ecosystem The problem exists in coal as well as metal mines Once exposed waste rock and/or tailings dams may continue to generate such acidity and pollution for decades and perhaps 
centuries It is imperative that prediction and prevention be used as a primary method to deal with and control ARD at virtually all mine sites However in active or abandoned mine sites where the problem already exists and as a supplement to preventative measures in new mines treatment of the contaminated drainage is necessary 2 This can add appreciably to the on-going operating 
costs of a mine Expertise in the field of ARD is often controversial as fundamental knowledge is lacking expertise are scarce and new knowledge is continually being sought and applied Prediction of weather conditions surface and ground-water flows chemistry of reactions in the waste piles and dissolution kinetics are all fraught with significant errors Data to assess and deal with ARD problems are often missing and so heuristic judgements play an important role 
in decision-making A fuzzy system is able to handle and manipulate missing, inaccurate, or heuristic data 3 A fuzzy expert system thrives on these conditions The development of a fuzzy expert system for ARD is of benefit since it produces a standardized adaptable approach to the problem, provides quick advice to a user and is equipped for training and teaching 0-7803-5489-3/99/$10.00 01999 IEEE 


500 ARDX COMPONENTS In its entirety, the ARDX system is designed to handle ARD problems ranging from prediction through to prevention and monitoring for treatment. The scope of the project to date deals with decision-making tactics for prevention and treatment at a mine site in one of three mine stages: planning, operating or closure. The components of the system come together through a knowledge base and inference engine by using rules and fuzzy concepts and with an explainer engine providing reasoning explanations and answers to user questions. Figure 1 shows the components of the ARDX system. The knowledge base is itself made up of a main system module ARDX main and numerous sub-modules ARDX main interacts with the sub-modules and drives the system It communicates with the user by asking for site specific data input moves through the appropriate sub-modules assesses whether an appropriate recommendation has been found and cycles through again or exits the system as required Forms pop up when called upon for data input and hypertext files are used for system output ARDX main" decides the final recommended treatment options for the site by assessing the cost of treatment with the probability of success for each treatment option  DEVELOPMENT PROCEDURE Development of an expert system requires 0 0 knowledge acquisition 0 system development \(programming steps testing and verification of the system a clear definition of the problem and domain of the system DEFINING THE PROBLEM AND DOMAIN The first step often poses the most obstacles In this case the domain of the system was designed to include all treatment possibilities However during knowledge acquisition, it became clear that the chosen domain was extremely large. Rather than change the chosen domain of the system, it was decided to focus on two aspects an overall structure to the decision-making process \(developed through ARDX main and a more detailed evaluation of separate treatment options sub-modules In this way a working system could be developed while separate modules containing further treatment options were added, revised or discarded as seen fit. ARDX "main" has become the seed for development of a larger and more complete system Figure 2 shows the basic ARDX flow-chart that has become instrumental in organizing and developing the system Essentially, the system examines all appropriate methods in terms of their ability to deal with the potential or existing problem Once a particular method or methods have been evaluated, the system looks for combinations of methods that may improve the solution further When these have been assessed the cost of each option is calculated and the recommendations of the least-costly, most-effective options are presented to the user   


50 1 b WATER COVERS FLOODlNG ACTIVE METHODS LA ACTIVE METHODS  Covers Sulphide Reduction Collect  Treat Bactericides Collect  Treat  z PASSIVE METHODS Collect  Treat DESIGN I I Fig 2 Basic ARDX Flowsheet KNOWLEDGE ACQUISITION Initially the knowledge acquisition phase included choosing and interviewing the expert as well as extensive literature searches on the topic of ARD treatment. Through interviews with the expert, the framework of the system was established Expertise was taken from numerous and, sometimes, contradictive published papers on the subject The challenge of building an expert system for ARD would appear to be in defining rules in a field where many decisions are presently being made by trial and error. Case studies were used to attempt to mimic the actual decision-making process followed by the expert Acquiring expertise is an ongoing process as the system is developed and expanded DEVELOPMENT OF THE SYSTEM ARDX operates within the COMDALEN environment. Information is represented using keyword triplets a method which assigns an attribute and value to each object Data in a keyword triplet can be stored as strings floating point numbers dates, logical fuzzy variables, etc 4 The user interface consists of pop up FORMS text boxes and hypertext documents. Through "forms", data consisting of drainage characteristics and site specifics are input by the user and stored as keyword triplets The system will communicate with the user in the event of any inconsistency in the input data via text boxes. Once a conclusion has been reached the output is displayed in a hypertext document The first step in successfully developing ARDX main" was to write a set of preliminary rules to call upon the sub-modules Output from the sub-modules are based on the concept of high", "medium", and/or "low assigned to the probability of successful mitigation or prevention and the capital cost according to the amount of money available for the project of the treatment option. Information from each sub-module can be used as inputs to other sub-modules as the system moves through the modules again to review the option of using combinations of the different treatment systems 


502 PRECIPITATION AVAILABLE FOR INFILTRATION 0 Precipitation snow Precipitation Rain Development of the sub-modules is ongoing as new ones are continually being added The ACTIVE METHODS module is itself like ARDX main a smaller driving module that calls upon the various active treatment sub-modules This secondary smaller driving module was necessary because of the large number of options available Outputs to each module probability of siiccess and the cost calculated separately\are compared through Fuzzy Associative Memory \(FAM maips and given a Degree of Belief DoB in the treatment option WATER INFILTRATION  Waste Dump Permeability Waste Surface Area Water Available for Infiltration The COVERS sub-module was developed first This module is part of the extended ACTIVE METHODS" sub-module and decides on the probability of a cover to be used as an active treatment option Inputs of site details and characteristics are placed into fuzzy sets Through these fuzzy sets inputs are assigned a membership value in a set and a Degree of Belief \(DoB in the concept low medium and/or high 4 5 Figure 3 shows the FAM map elements that comprise the COVERS" module PONDING SUBSURFACE INFLUX 0 Water table Topographical Relief Potential Underflux rain 0 Underlying Permeability Waste Dump Permeability  0 ponding T 0 Ratio:Ponding Surface Area Local Climate EFFLUENT MOBILITY Water table Water gradient Direction Water Bodies in Area 0 Scavenger Material I I SOCIO-ENVIRONMENTAL IMPACT b Distance to Populated area Population Size 0 Protected Regions Fig 3 COVERS" Module Flowchart A FAM map is a means to depict rules that combine to determine a degree of belief in a concept from a number of variables 5 The FAM maps are created through interviews with the expert They are used within the COVERS module to assess input information and decide upon an appropriate cover choice The FAM map used to acquire a degree of belief in environmental sensitivity from two variables that are themselves determined through other FAM maps is shown in Figure 4 Certainty Factors CF of the concepts sensitive slightly sensitive", and resistant need not add up to 100 as there may be an overlap in the belief in each concept and can be assigned as indicated within the FAM map 


503 ENVIRONMENTAL SENSITIVITY FAM Socio-Environmental Impact Fig 4 FAM map for Environmental Sensitivity of a site ss  slightly sensitive s  sensitive r  resistant Effluent Mobility As the number of variables necessary to decide on a concept increases the size and complexity of the FAM maps also increase resulting in large multi-dimensional maps of the decision-making process However by using a two-dimensional FAM map approach as shown above this complexity can be separated into unique modules which are easy to understand and develop in consultation with the expert L M H s=30 s  70 s 100 H ss 70 ss  40 ss  0 r 10 r=O r=O s=o s=20 s  60 M ss 50 ss 90 ss  50 r=60 r 10 r=O s=o s=o s  30 L ss  0 ss  40 ss  70 r IO0 r=70 r 10 A separate cost module has been developed and is accessible by all modules as necessary Calculated costs for an option can be used as inputs to modules. Costs for each remediation option are calculated using unit prices 6 and site-specific information To account for future cost variability the module updates all information according to the Marshall  Swift M&S index values 7 The module is able to store input M&S values for future reference and calculations The economic evaluation of an treatment option is broken down into capital cost maintenance and inspection costs and operating costs due to continued effluent treatment and sludge disposal The net present value of all on-going maintenance and operating costs are calculated from a user-defined rate of return value defaulted to 3.5 if unavailable Defuzzification is performed using a weighted-average approach for the concepts ho an unacceptable treatment option no unless an acceptable option at a high cost use if no other is available okl acceptable option good1' \(acceptable and low-cost and very good most cost effective for each treatment option This becomes the final degree of belief DoB in each treatment option recommended The output hypertext display provides a list of recommended treatment options the probability of success and the cost demanded by each option The user is able to click" through the document for justification of each recommended treatment option and for information on the decision-making process and has access to justification of the decision making process within the individual sub-modules TESTING Testing the ARDX system is currently incomplete Actual mining cases that have used or are using treatment options similar to those investigated by ARDX will be adopted to test the system. It is intended to apply both successful and unsuccessfd cases for a comparison of chosen treatment options their success and treatment options decided upon by ARDX The COVERS sub-module has undergone preliminary testing using Samatosum mine data SI The data was input by the expert. The resulting output for probable cover treatment options corresponded to one of the options being considered for the mine 


504 CONCLUSION Development of a Fuzzy Expert System on the design of ARD remediation plans has been successful. The system has the following benefits     Future expansion of this system will include ARD predictions based on expertise derived from case studies of existing sites. These predictions will be used as inputs to the existing system a comprehensive, logical organization of the design methodologies has been developed a consistent design philosophy can be generated by use of this system a training tool has been created to assist in the transfer of ARD technology to the industry economic and effective procedures to use for a wide variety of site problems are available ACKOWLEDGEMENT The authors acknowledge financial support from the National Research Council through IRAP Grant No 304695 We are also gratehl for travel support from the Faculty of Graduate Studies and Research at UBC REFERENCES  1 2 3 4  5 6 7 8 R.W. Lawrence A MacG. Robertson 1994 Acid rock drainage Understanding the problems  finding solutions CIM District 6 Annual General Meeting, Workshop No 1 L Filipek, A. Kirk, W. Schafer 1996. Control Technologies for ARD Mining Environment Management, Dec, 4-8 A. Bowen 1995 Expert systems Truth and Rumors Canadian Mining J Mining Sourcebook, 8-12 J.A Meech C.A Harris 1992 Expert Systems for Gold Processing Plants Randol Gold Forum Vancouver B.C 3 1-39 J.A Meech 1995 AI Applications in the Mining Industry into the 21 Century APCOM XXV Conference 93 10 1 MEND 1995 Economic Evaluation of Acid Mine Drainage Technologies. MEND Report 5.8.1 Marshall, Swift, 1999. Marshall  Swift Equipment Cost Index. Chemical Engineering 106\(3 170 M Ghomshei A Holmes E. Denholm R Lawrence T Carriou 1997. Acid Rock Drainage from the Samatosum Waste Dump, B.C Canada Proc 4'h Inter Conf on Acid Rock Drainage 1 35 1-366 


Any Course introduction lntmductian Programming Information Artificial Programming Unix Data Design and Analysis User Neural Compilers tu Computers to Unix in rascal Systems Intelligence in C Platform Communications of Algon\222thms Interfaces Nehvorkr Figure 4 Course hierarchy It should be noted that sometimes considerable amounts of rules remain even when the user has found the desired focus with the described methods Automatic pruning sorting and structuring methods should at this point be available for invocation by the user especially for removal of redundancy see e.g 15 for rule covers statistical methods and cluster ing 5 Application Experience TASA has been in prototype iise in foiir telecom munication companies since the beginning of 1995 and experiences are encouraging A telecommunica tion network manager can use episode rules as correl ation patterns after possibly editing them and assign ing them appropriate correlation actions Discovered patterns can be used for filtering uninformative alarms for combining alarms to construct hypothesis of faults or for prediction of faults Episode and association rules are also useful for providing different views to the analyzed alarms as in Example 3 Although the discovery algorithms are not directly applicable for on-line analysis TASA has turned out to be useful also in network surveillance The analysis can be rerun or augmented e.g every day or every hour Recent patterns may point to yet unnoticed problems in the network The fault management experts in the telecommunic ation companies have found the approach and TASA useful in, e.g 0 finding long-term, rather frequently occurring de 0 creating an overview of a short-term alarm se 0 evaluating the alarm data base consistency and pendencies quence and correctness On the other hand many of the rules discovered by TASA are deemed trivial by the network managers Luckily much of the trivial knowledge can be ex pressed and removed with templates Example 4 Consider an alarm correlation system which operates in real time and is also able to handle delayed alarms and slightly inaccurate time stamps In the correlation patterns delays are handled with a spe cial wait function Episode and assocaataon rules can be applaed an this system in a rather straightforward way The episode rule IF THEB high fault rate WITH SI SO conf\(0.70 freq\(700/1000 link alarm  link failure discovered by TASA can be coded in the system as fol lows  if 223alarm type  link alarm\224 then start time wait until \223alarm type  link failure\224 or LLtime  5 sec\224 f 223alarm type  link failure\224 then display warning \223high fault rate with 70 probability in 60 sec\224 else forward the original alarm That is if a link alarm occurs and a link failure follows within 5 seconds the rule right-hand side information is sent and the original alarms are suppressed If a link failure does not follow 221wzthin 5 seconds the 07.2 ginal link ularm is forwarded The correlation procedure can be enhanced using as sociation rules For example assume that we have de tccted that if the alarm type is link alarm the time of the day is ofice hours and alarm severity is I then with probability of 95 the alarming element type is BS After expert evaluation of the rule we know that such alarms from network elements of type BS can be ignored However if an element of any other type sends that alarm it is an interesting one The following correlation function removes such alarms  if 223alarm type  link alarm\224 and 223office hours\224 and 223severity  1\224 and 221\221element type  BS\224 then exit forward the original alarm 0 Unexpected but useful dependencies have been found e.g between network elements which are not immediately connected in the network topology Be ginning from the first tests discovered rules have been applied in alarm correlation systems 676 


6 Conclusion We have described a knowledge discovery method ology that can be used to automate knowledge acquis ition The methodology has the following two distinct ive characteristics 1 a large collection of potentially relevant rules is discovered at once and 2 different views can be formed on the discovered rules iteratively and interactively The motivation is that after the dis covery of all potentially interesting rules iteration is very efficient As an example application area we descibed tele communication networks alarm data The flow of alarms should be correlated automatically to a more in telligible form in order to facilitate identification and correction of faults Unfortunately the construction of an alarm correlation system requires a lot of both expertise and time and is a process that never is com plete Our experience in this field with both association and episode rules supports the claim that the meth odology is useful in practice For formalisms stronger than association and episode rules strong focus in the pattern discovery phase may be essential for keeping the computation tractable For well-defined problems this is probably fine but for exploring regularities in large data sets the interaction may be cumbersome References 1 R Agrawal T Imielinski and A Swami Mining as sociation rules between sets of items in large data bases In Proceedings of AGM SIGMOD Conference on Management of Data SIGMOD\22293 pp 207  216 May 1993 a R Agrawal H Mannila R Srikant H Toivonen and I Verkamo Fast discovery of association rules. In Advances in Knowledge Discovery and Data Mining pp 307  328 AAAI Press Menlo Park CA 1996 31 R Brachman and T Anand The process of know ledge discovery in databases A first sketch. In Know ledge Discovery in Databases, Papers from the 1994 AAA I Workshop IcDD\22294 July 1994 4 U Fayyad G Piatetsky-Shapiro and P Smyth The KDD process for extracting useful knowledge from volumes of data Communications of the ACM 39\(11  34 November 1996 5 U Fayyad G Piatetsky-Shapiro and P Smyth From data mining to knowledge discovery An overview In Advances in Knowledge Discovery and Data Mining pp 1-34 AAA1 Press Menlo Park CA 1996 6 R Goodman B Ambrose H Latin and C Ulmer Noaa  an expert system managing the telephone network In Integrated Network Management IV pp 316  327 Chapman  Hall London 1995 7 K Hatonen M Klemettinen H Mannila P Ronkainen and H Toivonen Knowledge discov ery from telecommunication network alarm data bases In 12th Int\222l Conference on Data Engineering ICDE\22296 pp 115  122 New Orleans Louisiana February 1996 SI G Jakobson and M. Weissman Alarm correlation. In IEEE Network 7\(6  59 November 1993 9 W Kloesgen Efficient discovery of interesting state ments in databases Journal of Intelligent Informa tion Systems 4\( 1  69 1995 lo M Klemettinen H Mannila P Ronkainen H Toivonen and I Verkamo Finding interesting rules from large sets of discovered association rules In Pro ceedings of the Third Int\222l Conference on Information and Knowledge Management CIKM\22294 pp 401  407 Gaithersburg MD November 1994 ACM 111 C Matheus G Piatetsky-Shapiro and D McNeill Selecting and reporting what is interesting In Ad vances in Knowledge Discovery and Data Mining pp 495  515 AAA1 Press Menlo Park CA 1996 121 H Mannila and H Toivonen Discovering generalized episodes using minimal occurrences In Proceedings of the Second Int\222l Conference on Knowledge Discovery and Data Mining \(KDD\22296 pp 146  151 Portland Oregon August 1996 AAAI Press 13 H Mannila and H Toivonen On an algorithm for finding all interesting sentences In Cybernetics and Systems Volume II The Thirteenth European Meet ing on Cybernetics and Systems Research pages 973  978 Vienna Austria April 1996 Austrian Society for Cybernetic Studies 14 H Mannila H Toivonen and I Verkamo Discov ering frequent episodes in sequences In Proceedings of the First Int\222l Conference on Knowledge Discovery and Data Mining I<DD\22295 pp 210  215 Montreal Canada August 1995 AAA1 Press 15 H Toivonen M Klemettinen P Ronkainen K Hatonen and H Mannila Pruning and grouping of discovered association rules In Workshop Notes of the ECML-95 Workshop on Statistics Machine Learning and Knowledge Discovery in Databases pp 47  52 Heraklion Greece April 1995 MLnet 16 R Zembowicz and J Zytkow From contingency tables to various forms of knowledge in databases In Advances in Knowledge Discovery and Data Mining pp 329  349 AAAI Press Menlo Park CA 1996 677 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


