Efficiently Mining Maximal Frequent Itemsets T,D Frequent iternsets Frequent emsets Mio-Sup E 3 trans Min-Sup  2 trans Karam Gouda and Mohammed J. Zakit Computer Science  Communication Engg. Dept Kyushu University, Japan Computer Science Dept., Rensselaer Polytechnic Institute USA Email kgouda@csce.kyushu-u.ac.jp zaki @cs.rpi.edu Itemset Maxlmal lemsets Maximal ltemsels Sire Min.Sup-3 trans MinrSup  2 trans Abstract We present GenMax a backtrack search based algorithm for mining maximal frequent itemsets GenMax uses a num ber of optimizations to prune the search space It uses a 
novel technique called progressive focusing to perform maximality checking and diffset propagation to perfarm fast frequency computation Systematic experimental com parison with previous work indicates that different methods have varying strengths and weaknesses based on dataset characteristics We found GenMax to be a highly efJicient method to mine the exact set of maximal patterns 1 Introduction Mining frequent itemsets is a fundamental and essential problem in many data mining applications such as the dis covery of association rules strong rules, correlations, multi dimensional patterns and many other important discovery tasks The problem is formulated as 
follows Given a large data base of set of items transactions find all frequent item sets, where a frequent itemset is one that occurs in at least a user-specified percentage of the data base Many of the proposed itemset mining algorithms are a variant of Apriori 2 which employs a bottom-up, breadth first search, that enumerates every single frequent itemset In many applications especially in dense data with long frequent patterns enumerating all possible 2m  2 subsets of a m length pattern m can easily be 30 or 40 or longer is 
computationally unfeasible Thus, there has been recent interest in mining maximal frequent patterns in these \224hard\224 dense databases Another recent promising direction is to mine only closed sets 9,11 a set is closed if it has no superset with the same frequency Nevertheless for some of the dense datasets we consider in this paper, even the set of all closed patterns would grow to be too large The only recourse is to mine the maximal patterns in such domains In this paper we introduce GenMax a new algorithm that utilizes a backtracking search for efficiently enumerating all maximal patterns GenMax uses a number 
of optimizations to quickly prune away a large portion of the subset search space It uses a novel progressive focusing technique to eliminate non-maximal itemsets and uses diffset propaga tion for fast frequency checking We conduct an extensive experimental characterization of GenMax against state-of-the-art maximal pattern min ing methods like MaxMiner 3 and Mafia 141 We found that the three methods have varying performance depend ing on the database characteristics mainly the distribution of the maximal frequent patterns by length We present a  ACTW ACDW.ACTW systematic and realistic set of experiments showing under 
which conditions a method is likely to perform well and un der what conditions it does not perform well We conclude that while Mafia is the best method for mining a superset of all maximal patterns GenMax is the current best method for enumerating the exact set of maximal patterns We fur ther observe that there is a type of data where MaxMiner delivers the best performance 2 Preliminaries and Related Work The problem of mining maximal frequent patterns can be formally stated as follows Let 1  
il 22    im be a set of m distinct items Let 23 denote a database of trans actions, where each transaction has a unique identifier tid and contains a set of items The set of all tids is denoted 2217  tl t2  tn A set X C Z is also called an item set An itemset with k items is called a k-itemset The set t\(X C 7 consisting 
of all .the transaction tids which contain X as a subset is called the tidset of l For con venience we write an itemset A C W as ACW and its tidset  1,3,4,5 as t\(X  1345 4 ACTW ACDW,ACTW 1 ACTW A.C.D.1.W A.C,D,T,W CDW AC,AT AW AC AD AT AW ACTW CD,CT,CW CD.CT.CW,DT 4 ACDW ADW ATW CDT 6 CDT CDW COW CTW 0-7695-1 119-8/01 17.00 0 2001 IEEE 163 


Backtracking Search GenMax uses backtracking search to enumerate the MFI We first describe the backtracking paradigm in the context of enumerating all frequent pat terns We will subsequently modify this procedure to enu merate the MFI Backtracking algorithms are useful for many combina torial problems where the solution can be represented as a set I  io,Zl  where each ij is chosen from a fi nite possible set Pj Initially I is empty it is extended one item at a time as the search space is traversed The length of I is the same as the depth of the corresponding node in the search tree Given a partial solution of length 1 I  io il  il-l the possible values for the next item il comes from a subset Cl C_ Pl called the combine set If y E Pl  Cl then nodes in the subtree with root node I1  io il  il-1 y will not be considered by the back tracking algorithm Since such subtrees have been pruned away from the original search space the determination of Cl is also called pruning Invoke as FI-backtrack FI 0 FI-backtrack\(Il CI 1 1 2 11+1  I U z also add 1i+1 to FI 3 Fi+1  y  y E Ci and y  z 4 CI  FI-combine ZI+~,PI 5 FI-backtrack\(li+l CL 2  1 Can 11+1 combine with other items in Cl F1-combine\(Il+l Pi+l 1 C=0 2 3 4 5 return C for each z E CI for each y E P if ZI U y is frequent C  C U y Figure 2 Backtrack Algorithm for Mining FI Consider the backtracking algorithm for mining all fre quent patterns shown in Figure 2 The main loop tries ex tending I with every item z in the current combine set CL The first step is to compute I~+I which is simply I1 ex tended with z The second step is to extract the new possi ble set of extensions PI which consists only of items y in Cl that follow z The third step is to create a new com bine set for the next pass, consisting of valid extensions. An extension is valid if the resulting itemset is frequent The combine set Cl+1 thus consists of those items in the possi ble set that produce a frequent itemset when used to extend 4+1 Any item not in the combine set refers to a pruned sub tree The final step is to recursively call the backtrack rou tine for each extension As presented the backtrack method performs a depth-first traversal of the search space Example 2 Consider the full subset search space shown in Fig ure 3 The backtrack search space can be considerably smaller than the full space For example we start with IO  8 and CO  Fl  A C D T W At level I each item in CO is added to IO in turn For example A is added to obtain I1  A The possi ble set for A Pl  C D T W consists of all items that follow A in CO However from Figure 1 we find that only AC AT and AW are frequent at min_sup=3 giving C1  C T W Thus the subtree corresponding to the node AD has been pruned Related Work Methods for finding the maximal elements include All-MFS 5 which works by iteratively attempt ing to extend a working pattern until failure A random ized version of the algorithm that uses vertical bit-vectors was studied but it does not guarantee every maximal pat tern will be returned The Pincer-Search 7 algorithm uses horizontal data format It not only constructs the candidates in a bottom-up manner like Apriori but also starts a top down search at the same time maintaining a candidate set of maximal patterns This can help in reducing the number of database scans by eliminating non-maximal sets early The maximal candidate set is a superset of the maximal pat terns, and in general, the overhead of maintaining it can be very high In contrast GenMax maintains only the current known maximal patterns for pruning MaxMiner 3 is another algorithm for finding the max imal elements It uses efficient pruning techniques to quickly narrow the search MaxMiner employs a breadth first traversal of the search space it reduces database scan ning by employing a lookahead pruning strategy i.e if a node with all its extensions can determined to be frequent there is no need to further process that node It also em ploys item re heuristic to increase the effective ness of superset-frequency pruning Since MaxMiner uses the original horizontal database format, it can perform the same number of passes over a database as Apriori does Depthproject I finds long itemsets using a depth first search of a lexicographic tree of itemsets and uses a counting method based on transaction projections along its branches This projection is equivalent to a horizontal ver sion of the tidsets at a given node in the search tree. Depth Project also uses the look-ahead pruning method with item reordering It returns a superset of the MFI and would re quire post-pruning to eliminate non-maximal patterns FP growth 6 uses the novel frequent pattern tree FP-tree structure which is a compressed representation of all the transactions in the database It uses a recursive divide-and conquer and database projection approach to mine long pat terns. Nevertheless since it enumerates all frequent patterns it is impractical when pattern length is long Mafia  is the most recent method for mining the MFI Mafia uses three pruning strategies to remove non-maximal sets The first is the look-ahead pruning first used in MaxMiner The second is to check if a new set is subsumed by an existing maximal set The last technique checks if t\(X C t\(Y If so X is considered together with Y for extension Mafia uses vertical bit-vector data format, and compression and projection of bitmaps to improve perfor mance Mafia mines a superset of the MFI and requires a post-pruning step to eliminate non-maximal patterns In contrast GenMax integrates pruning with mining and re turns the exact MFI 3 GenMax for efficient MFI Mining There are two main ingredients to develop an efficient MFI algorithm The first is the set of techniques used to remove entire branches of the search space, and the second is the representation used to perform fast frequency compu tations We will describe below how GenMax extends the basic backtracking routine for FI and then the progressive focusing and diffset propagation techniques it uses for fast maximality and frequency checking The basic MFI enumeration code used in GenMax is a straightforward extension of FI-backtrack The main ad dition is the superset checking to eliminate non-maximal itemsets as shown in Figure 4 In addition to the main steps 164 


Level O\(A.C.D.T.W 1 2 0 4 AqTW 5 Figure 3 SubseVBacktrack Search Tree nzinsup 3 Circles indicate maximal sets and the infrequent sets have been crossed out Due to the downward closure property of support i.e all subsets of a frequent itemset must be frequent the frequent itemsets form a border shown with the bold line such that all frequent itemsets lie above the border while all infrequent itemsets lie below it Since MFI determine the border it is straightforward to obtain FI in a single database scan of MFI is known Invocation: MFI-backtrack FI 0 MFI-backtrack\(li Ci 1 1 for each z E Ci 3 4 S 6 7 if C~+I is empty 8 10 2 Il+l  IU x A+1  y  y E Cl and y  z if 1~+1 U Pi+1 has a superset in MFI CL  FI-combine 4+1 Pi+1 return Nall subsequent branches pruned if 1/+1 has no superset in MFI 9  MFI= MFI U li+1 else MFI-backtrack\(ll+l ci+1 1  1 Figure 4 Backtrack Algorithm for Mining MFI indicates a new line not in FI-backtrack in FI enumeration the new code adds a step line 4 after the construction of the possible set to check if Zl+1 U Pl+1 is subsumed by an existing maximal set If so the current and all subsequent items in Cl can be pruned away After creating the new combine set if it is empty and I/+l is not a subset of any maximal pattern it is added to the MFI If the combine set is non-empty a recursive call is made to check further extensions Superset Checking Techniques Checking to see if the given itemset combined with the possible set P/+l is subsumed by another maximal set was also proposed in Mafia 4 under the name HUTMFI. Further pruning is pos sible if one can determine based just on support of the com bine sets if Il+1 U4+1 will be guaranteed to be frequent In this case also one can avoid processing any more branches This method was first introduced in MaxMiner 3 and was also used in Mafia under the name FHUT Reordering the Combine Set Two general principles for efficient searching using backtracking are that 1 It is more efficient to make the next choice of a subtree \(branch to explore to be the one whose combine set has the fewest items This usually results in good performance since it minimizes the number of frequency computations in FI combine 2 If we are able to remove a node as early as possible from the backtracking search tree we effectively prune many branches from consideration Reordering the elements in the current combine set to achieve the two goals is a very effective means of cutting A1D.T.W.C DIT.W.CI ADTiW kl I I  ADWlC1 ADC ATWIC ATC a'WC DTWic DTC DWC  1 ADWC ATWC TWC  Figure 5 Backtracking Trees of Example 2 down the search space The first heuristic is to reorder the combine set in increasing order of support This is likely to produce small combine sets in the next level since the items with lower frequency are less likely to produce fre quent itemsets at the next level This heuristic was first used in MaxMiner and has been used in other methods since then 1,4 1 I In addition to sorting the initial combine set at level 0 in increasing order of support, GenMax uses another novel reordering heuristic based on a simple lemma Lemma 1 Ler IF\(x  y  y E PI xy is notfrequent  denote the set of infrequent 2-iternsets that contain an item x E F1 and let M x be the longest maximal pattern con taining z Then M   IF1 I  IIF\(x Assuming Fz has been computed reordering CO in de creasing order of IF\(z with x E CO ensures that the smallest combine sets will be processed at the initial lev els of the tree which result in smaller backtracking search trees GenMax thus initially sorts the items in decreasing order of IF and in increasing order of support Example 3 For our database in Figure 1 with min-sup  2 IF\(x is the same of all items x E 4 and the sorted order on support\is A D T W C Figure 5 shows the backtracking search trees for maximal itemsets containing prefix items A and D Un der the search tree for A Figure 5 a we try to extend the partial solution AD by adding to it item T from its combine set We try another item W after itemset ADT turns out to be infrequent 165 


and so on Since GenMax uses itemsets which are found earlier in the search to prune the combine sets of later branches after find ing the maximal set ADWC GenMax skips ADC After finding ATWC all the remaining nodes with prefix A are pruned, and so on The pruned branches are shown with dashed arrows, indicating that a large part of the search tree is pruned away Theorem 1 Correctness MFI-backtrack returns all and only the maximal frequent itemsets in the given database 3.1 Optimizing GenMax Superset Checking Optimization The main efficiency of GenMax stems from the fact that it eliminates branches that are subsumed by an already mined maximal pattern Were it not for this pruning Gen Max would essentially default to a depth-first exploration of the search tree Before creating the combine set for the next pass in line 4 in Figure 4 GenMax check if Il+1 U Pl+1 is contained within a previously found maximal set If yes then the entire subtree rooted at Il+l and including the el ements of the possible set are pruned If no then a new extension is required Another superset check is required at line 8 when Il+1 has no frequent extension i.e., when the combine set Cl+1 is empty Even though Il+l is a leaf node with no extensions it may be subsumed by some maximal set and this case is not caught by the check in line 4 above The major challenge in the design of GenMax is how to perform this subset checking in the current set of maximal patterns in an efficient manner If we were to naively imple ment and perform this search two times on an ever expand ing set of maximal patterns MFI and during each recursive call of backtracking we would be spending a prohibitive amount of time just performing subset checks. Each search would take O IMFII time in the worst case where MFI is the current growing set of maximal patterns Note that some of the best algorithms for dynamic subset testing run in amortized time O  log s per operation in a sequence of s operations 8 for us s  O\(MF1 In dense do main we have thousands to millions of maximal frequent itemsets and the number of subset checking operations per formed would be at least that much Can we do better The answer is yes Firstly we observe that the two sub set checks \(one on line 4 and the other on line 8 can be easily reduced to only one check Since Zl+l U PL is a su perset of Zl+1 in our implementation we do superset check only for 11+1 U E\221 While testing this set we store the maximum position say p at which an item in Il+1 U Pl+1 is not found in a maximal set M E MFI In other words all items before p are subsumed by some maximal set For the superset test for I~+I we check if 11~+1 I  p If yes ZL is non-maximal If no we add it to MFI The second observation is that performing superset checking during each recursive call can be redundant For example suppose that the cardinality of the possible set Pl+l is m Then potentially MFI-backtrack makes rn re dundant subset checks if the current MFI has not changed during these m consecutive calls To avoid such redun dancy, a simple checkstatus flag is used If the flag is false no superset check is performed Before each recursive call the flag is false it becomes true whenever Cl+1 is empty which indicates that we have reached a leaf and have to backtrack  Invocation LMFLbacktrack\(0 Fl 0,O  LMF4 is an output parameter LMFI-backtrack\(4 Cl LMFIl E 1 2 3 Pl+1  y  y E Ci and y  z 4 if Il+1 U Pl+1 has a superset in LMFIi 5 return subsequent branches pruned 7 Cl+1  FI-combine Il+l Pl+l 8 if Cl+1 is empty 9 if has no superset in LMFIl 10 1 I 12 LMFI-backtrack\(Il+l Cl+1 LMFI1+1,1 1 for each z E Cl Il+l  I U z 6  LMFIl+1  8 LMF4  LMF4 U I~+I else LMFIl+1  M E LMFIl  z E M 13 LMFIi  LMFIl U LMFI1+1 Figure 6 Mining MFI with Progressive Focus ing  indicates a new line not in MFI-backtrack The O log s time bounds reported in 8 for dy namic subset testing do not assume anything about the se quence of operations performed In contrast we have full knowledge of how GenMax generates maximal sets we use this observation to substantially speed up the subset checking process The main idea is to progressively nar row down the maximal itemsets of interest as recursive calls are made In other words we construct for each invocation of MFI-backtrack a list of local rnaxirnal frequent itemsets LMFIl This list contains the maximal sets that can po tentially be supersets of candidates that are to be generated from the itemset Il The only such maximal sets are those that contain all items in Il This way instead of check ing if 11+1 U P,+l is contained in the full current MFI we check only in LM FI1  the local set of relevant maximal itemsets This technique that we call progressive focusing is extremely powerful in narrowing the search to only the most relevant maximal itemsets, making superset checking practical on dense datasets Figure 6 shows the pseudo-code for GenMax that incor porates this optimization the code for the first two opti mizations is not show to avoid clutter Before each in vocation of LMFI-backtrack a new LMFIl+1 is created consisting of those maximal sets in the current LDIFIl that contain the item z see line 10 Any new maximal itemsets from a recursive call are incorporated in the current LMFIl at line 12 Frequency Testing Optimization So far GenMax as described is independent of the data format used The techniques can be integrated into any of the existing methods for mining maximal patterns We now present some data format specific optimizations for fast fre quency computations GenMax uses a vertical database format, where we have available for each item its tidset the set of all transaction tids where it occurs The vertical representation has the fol lowing major advantages over the horizontal layout Firstly computing the support of itemsets is simpler and faster with the vertical layout since it involves only the intersections of tidsets or compressed bit-vectors if the vertical format is stored as bitmaps 4 Secondly with the vertical lay out, there is an automatic 223reduction\224 of the database be fore each scan in that only those itemsets that are relevant 


to the following scan ofcthe mining process are accessed from disk Thirdly the vertical format is more versatile in supporting various search strategies, including breadth-first depth-first or some other, hybrid search Can 11+1 combine with other items in Ci FI-tidset-combine\(li 1 Pi 1  1 c=0 2 for each y E R+I 3 y\221  y\222 4 5 if It\(y\222 2 minsup 6 7 returnC t\(y\222  t\(h+d n t\(y c  c U y\222 Figure 7 FI-combine Using Tidset Intersec tions  indicates a new line not in FI-combine Let\222s consider how the FI-combine \(see Figure 2 routine works, where the frequency of an extension is tested Each item z in Cl actually represents the itemset 11 U z and stores the associated tidset for the itemset I1 U z For the initial invocation since 11 is empty, the tidset for each item z in Cl is identical to the tidset t\(z of item z Before line 3 is called in FI-combine we intersect the tidset of the ele ment 11+1 i.e t\(1lU{x with the tidset of element y i.e t\(1l U 9 If the cardinality of the resulting intersection is above minimum support, the extension with y is frequent and y\222 the new intersection result is added to the combine set for the next level Figure 7 shows the pseudo-code for FI-tidset-combine using this tidset intersection based sup port counting In Charm  1 I we first introduced two new properties of itemset-tidset pairs which can be used to further increase the performance Consider the items z and y in C1 If during intersection in line 4 in Figure 7 we discover that t\(z  or equivalently t\(Ii+l  is a subset of or equal to t\(y then we do not add y\222 to the combine set, since in this case z always occurs along with y Instead of adding y\221 to the combine set we ada it to Il+l This optimization was also used in Mafia 4 under the name PEP Diffsets Propagation Despite the many advantages of the vertical format, when the tidset cardinality gets very large e.g for very frequent items the intersection time starts to become inordinately large Furthermore the size of in termediate tidsets generated for frequent patterns can also become very large to fit into main memory GenMax uses a new format called diffsets  101 for fast frequency testing The main idea of diffsets is to avoid storing the entire tidset of each element in the combine set Instead we keep track of only the differences between the tidset of itemset 11 and the tidset of an element z in the combine set which actually denotes 1l U z These differences in tids are stored in what we call the diffset which is a difference of two tidsets at the root level or a difference of two diffsets at later levels Furthermore these differences are propagated all the way from a node to its children starting from the root In an extensive study  101 we showed that diffsets are very short compared to their tidsets counterparts and are highly effective in improving the running time of vertical methods We describe next how they are used in GenMax with the help of an example. At level 0 we have available the tidsets for each item in F1 When we invoke FI-combine at this level we compute the diffset of y\222 denoted as d\(y\222 instead  Can ll+1 combine with other items in Cl FI-diffset-combine\(l+l A+1 1 c=0 2 for each  E Fj+l 3 yf  y 4 if level  0 then d\(y\222  t\(ll+l  t\(y 5 6 if u\(y\222 2 minsup 7 8 return C else d\(y\222  d\(y  d\(Ii+1 c  c U y\222 Figure 8 FI-combine Diffset Propagation of computing the tidset of y as shown in line 4 in Figure 7 That is d\(y\222  t\(z  t\(y The support of y\222 is now given as y\222  a  ld\(y\222 At subsequent levels we have available the diffsets for each element in the combine list. In this case d\(y\222  d\(y  d\(z but the support is still given as cr\(y\222  a\(z  ld\(y\221 Figure 8 shows the pseudo-code for computing the combine sets using diffsets GenMax 1 Compute FI and F2 3 4 5 MFI  6 7 returnMF1 Compute IF\(z for each item z E F1 Sort F1 decreasing in IF\(z increasing in a LMFI-backtrack Fl MFI 0 he diffsets Figure 9 The GenMax Algorithm Final GenMax. Algorithm The complete GenMax algo rithm is shown in Figure 9 which ties in all the optimiza tions mentioned above GenMax assumes that the input dataset is in the vertical tidset format First GenMax com putes the set of frequent items and the frequent 2-itemsets using a vertical-to-horizontal recovery method lo This information is used to reorder the items in the initial com bine list to minimize the search tree size that is generated GenMax uses the progressive focusing technique of LMFI backtrack combined with diffset propagation of FI-diffset combine to produce the exact set of all maximal frequent itemsets MFI 4 Experimental Results Past work has demonstrated that Depthproject  11 is faster than MaxMiner  and the latest paper shows that Mafia 4 consistently beats Depthproject In out experi mental study below, we retain MaxMiner for baseline com parison At the same time, MaxMiner shows good perfor mance on some datasets which were not used in previous studies We use Mafia as the current state-of-the-art method and show how GenMax compares against it All our experiments were performed on a 400MHz Pen tium PC with 256MB of memory running RedHat Linux 6.0 For comparison we used the original source or ob ject code for MaxMiner and MAFIA 4 provided to us by their authors. Timings in the figures are based on total wall-clock time and include all preprocessing costs such as horizontal-to-vertical conversion in GenMax and Mafia The times reported also include the program output We believe our setup reflects realistic testing conditions as op posed to some previous studies which report only the CPU time or may not include output cost Benchmark Datasets We chose several real and syn thetic datasets for testing the performance of the the al 167 


Database 1 I AL R chess I 76 I 37 I 3,196 connect mushroom pumsb 71 I7 pumsb 71 I7 67,557 8,124 49,046 49,046 100,000 100,000 MPL 23 20 31 2.5 22 0.025 43 2.5 27 40 13 0.01  25 0.1 Figure 10 Database Characteristics I denotes the number of items AL the average length of a record R the number of records and MPL the maximum pattern length at the given min-sup gorithms shown in Table IO The real datasets have been used previously in the evaluation of maximal patterns  1,3 41 Typically these real datasets are very dense i.e they produce many long frequent itemsets even for high val ues of support The table shows the length of the longest maximal pattern at the lowest minimum support used in our experiments for the different datasets For exam ple on pumsb*, the longest pattern was of length 43 any method that mines all frequent patterns will be impracti cal for such long patterns We also chose two synthetic datasets which have been used as benchmarks for testing methods that mine all frequent patterns Previous maxi mal set mining algorithms have not been tested on these datasets which are sparser compared to the real sets All these datasets are publicly available from IBM Almaden www.almaden.ibm.com/cs/quest/demos.html While conducting experiments comparing the 3 different algorithms we observed that the performance can vary sig nificantly depending on the dataset characteristics We were able to classify our benchmark datasets into four classes based on the distribution of the maximal frequent patterns Type I Datasets Chess and Pumsb Figure 11 shows the performance of the three algorithms on chess and pumsb These Type I datasets are character ized by a symmetric distribution of the maximal frequent patterns \(leftmost graph Looking at the mean of the curve we can observe that for these datasets most of the maximal patterns are relatively short average length 1 1 for chess and 10 for pumsb The MFI cardinality figures on top center and right show that for the support values shown, the MFI is 2 orders of magnitude smaller than all frequent itemsets Compare the total execution time for the different algo rithms on these datasets \(center and rightmost graphs We use two different variants of Mafia The first one, labeled Mafia does not return the exact maximal frequent set, rather it returns a superset of all maximal patterns The second variant labeled MafiaPP uses an option to eliminate non maximal sets in a post-processing PP step. Both GenMax and MaxMiner return the exact MFI On chess we find that Mafia without PP is the fastest if one is willing to live with a superset of the MFI Mafia is about IO times faster than MaxMiner However notice how the running time of MafiaPP grows if one tries to find the exact MFI in a post-pruning step GenMax though slower than Mafia is significantly faster than MafiaPP and is about 5 times better than MaxMiner All methods, except MafiaPP show an exponential growth in running time since the y-axis is in log-scale this appears linear\faithfully fol lowing the growth of MFI with lowering minimum sup port as shown in the top center and right figures MafiaPP shows super-exponential growth and suffers from an ap proximately O IMF11  overhead in pruning non-maximal sets and thus becomes impractical when MFI becomes too large, i.e at low supports On pumsb we find that GenMax is the fastest, having a slight edge over Mafia It is about 2 times faster than Mafi aPP. We observed that the post-pruning routine in MafiaPP works well till around 0\(104 maximal itemsets Since at 60 minsup we had around that many sets, the overhead of post-processing was not significant With lower support the post-pruning cost becomes significant so much so that we could not run MafiaPP beyond 50 minimum support MaxMiner is significantly slower on pumsb a factor of 10 times slower then both GenMax and Mafia Type I results substantiate the claim that GenMax is an highly efficient method to mine the exact MFI It is as fast as Mafia on pumsb and within a factor of 2 on chess. Mafia on the other hand is very effective in mining a superset of the MFI Post-pruning in general is not a good idea, and GenMax beats MafiaPP with a wide margin over 100 times better in some cases, e.g chess at 20 On Type I data MaxMiner is noncompetitive Type I1 Datasets Connect and Pumsb Type I1 datasets as shown in Figure 12 are characterized by a left-skewed distribution of the maximal frequent pat terns i.e there is a relatively gradual increase with a sharp drop in the number of maximal patterns The mean pattern length is also longer than in Type I datasets it is around 16 or 17 The MFI cardinality is also drastically smaller than FI cardinality by a factor of lo4 or more in contrast for Type I data, the reduction was only lo2 The main performance trend for both Type I1 datasets is that Mafia is the best till the support is very low at which point there is a cross-over and GenMax outperforms Mafia MafiaPP continues to be favorable for higher sup ports but once again beyond a point post-pruning costs start to dominate MafiaPP could not be run beyond the plotted points MaxMiner remains noncompetitive about 10 times slower The initial start-up time for Mafia for creating the bit-vectors is responsible for the high offset at 50 support on pumsb GenMax appears to exhibit a more graceful increase in running time than Mafia Type I11 Datasets T1014 and T40110 As depicted in Figure 13 Type I11 datasets  the two synthetic ones  are characterized by an exponentially de caying distribution of the maximal frequent patterns Ex cept for a few maximal sets of size one the vast majority of maximal patterns are of length two A.fter that the number of longer patterns drops exponentially The mean pattern length is very short compared to Type I or Type I1 datasets it is around 4-6 MFI cardinality is not much smaller than the cardinality of all frequent patterns The difference is only a factor of 10 compared to a factor of 100 for Type I and a factor of 10,000 for Type 11 Comparing the running times we observe that MaxMiner is the best method for this type of data The breadth-first or level-wise search strategy used in MaxMiner is ideal for 168 


maximal itemset distribution chess Dumsb  MaxMiner  MafiaPP  GenMax 0 a  0  Mafia  7000 6000 5000 p 4000 m 3000 e IL 2000 1000 0 0 5 10 15 20 25  70 65 60 55 50 45 40 35 30 25 20 MaxMiner     t 100000  Mafia   1000  100  10  t 1 1 100 90 80 70 60 50 40 Length Minimum Support  Minimum Supporl Figure 11 Type I Datasets chess and pumsb maximal iternset distribution connect pumsb 4000 3500 3000 2500  2000  1000 1500 500 0 500 1        1 loo0 0 5 10 15 20 25 30 35 40 Length Figure  0 E F   I 12 100 90 80 70 60 50 40 30 20 10 0 Minimum Support  Type II Datasets \(connect and 50 45 40 35 30 25 20 15 10 5 0 Minimum Support  pu msb maximal itemset distribution T1014D100K T40llODlOOK TlO\(0 025 f 35000  7 I I I  1 30000 T40\(07125 0  25000 000~0 15000 10000 5000 0 2 4 6 8 10 12 14 16 18 Length 30000 25000  20000 t 10000 U 2 15000 U 5000 0 1 0.16 0.14 0.12 0.1 0.08 0.06 0.04 0.02 0 10000 0 1000 E F d 100  10 Minimum Support  Minimum Support  Figure 13 Type 111 Datasets T10 and T40 maximal iternset distribution mushroom mushroom\(9.1 c mushroom\(0.075 e p 100 0 g 10 E I F 21  MaxMiner e MafiaPP  GenMax o GenMax   Mafia 4 1 0 5 10 15 20 25 10 1 0.1 0.01 Length Minimum Support  Figure 14 Type IV Dataset \(mushroom vcly Uuarly SGalLll LIGGS UllU WllGll LllC aVclagG llI~Allllal pal LU gG1 LIIGII IlG;qUGIILY U11 LIIG ULllt;l llUllU Vt;lllLill IllGLll tern length is small Horizontal methods are better equipped to cope with the quadratic blowup in the number of fre quent 2-itemsets since one can use array based counting ods spend much time in performing intersections on long item tidsets or bit-vectors GenMax gets around this prob lem by using the horizontal format for computing frequent 169 


2-itemsets denoted Fz but it still has to spend time per forming O F2 I pairwise tidset intersections Mafia on the other hand performs O IF1 12 intersections where FI is the set of frequent items The overhead cost is enough to render Mafia noncompetitive on Type I11 data On TI0 Mafia can be 20 or more times slower than MaxMiner GenMax exhibits relatively good performance and it is about 10 times better than Mafia and 2 to 3 times worse than MaxMiner On T40 the gap between GenMax/Mafia and MaxMiner is smaller since there are longer maximal patterns MaxMiner is 2 times better than GenMax and 5 times better than Mafia Since the MFI cardinality is not too large MafiaPP has almost the time as Mafia for high supports Once again MafiaPP could not be run for lower support values It is clear that in general post-pruning is not a good idea the overhead is too much to cope with Type IV Dataset Mushroom Mushroom exhibits a very unique MFI distribution Plotting MFI cardinality by length we observe in Figure 14 that the number of maximal patterns remains small until length 19 Then there is a sudden explosion of maximal pat terns at length 20 followed by another sharp drop at length 21 The vast majority of maximal itemsets are of length 20 The average transaction length for mushroom is 23 see Ta ble IO thus a maximal pattern spans almost a full transac tion The total MFI cardinality is about IO00 times smaller than all frequent itemsets On Type IV data Mafia performs the st MafiaPP and MaxMiner are comparable at lower supports This data is the worst for GenMax which is 2 times slower than MaxMiner and 4 times slower than Mafia In Type IV data a smaller itemset is part of many maximal itemsets of length 20 in case of mushroom this renders our pro gressive focusing technique less effective To perform max imality checking one has to test against a large set of maxi mal itemsets we found that GenMax spends half its time in maximality checking Recognizing this helped us improve the progressive focusing using an optimized intersection based method as opposed to the original list based ap proach This variant labeled GenMax\222 was able to cut down the execution time by half GenMax\222 runs in the same time as MaxMiner and MafiaPP 5 Conclusions This is one of the first papers to comprehensively com pare recent maximal pattern mining algorithms under realis tic assumptions Our timings are based on wall-clock time we included all pre-processing costs and also cost of out putting all the maximal itemsets \(written to a file We were able to distinguish four different types of MFI distributions in our benchmark testbed We believe these distributions to be fairly representative of what one might see in prac tice since they span both real and synthetic datasets Type I is a normal MFI distribution with not too long maximal patterns, Type I1 is a left-skewed distributions with longer maximal patterns Type I11 is an exponential decay distri bution with extremely short maximal patterns and finally Type IV is an extreme left-skewed distribution with very large average maximal pattern length We noted that different algorithms perform well under different distributions We conclude that among the current methods MaxMiner is the best for mining Type I11 distri butions On the remaining types, Mafia is the best method if one is satisfied with a superset of the MFI For very low supports on Type I1 data Mafia loses its edge. Post-pruning non-maximal patterns typically has high overhead It works only for high support values, and MafiaPP cannot be run be yond a certain minimum support value GenMax integrates pruning of non-maximal itemsets in the process of mining using the novel progressive focusing technique along with other optimizations for superset checking GenMax is the best method for mining the exact MFI Our work opens up some important avenues of future work The IBM synthetic dataset generator appears to be too restrictive It produces Type I11 MFI distributions We plan to develop a new generator that the users can use to produce various kinds of MFI distributions This will help provide a common testbed against which new algorithms can be benchmarked Knowing the conditions under which a method works well or does not work well is an impor tant step in developing new solutions In contrast to pre vious studies we were able to isolate these conditions for the different algorithms For example we were able to im prove the performance of GenMax\222 to match MaxMiner on mushroom dataset Another obvious avenue of improving GenMax and Mafia is to efficiently handle Type I11 data It seems possible to combine the strengths of the three meth ods into a single hybrid algorithm that uses the horizontal format when required and uses bit-vectors/diffsets or per haps bit-vectors of diffsets in other cases or in combination We plan to investigate this in the future Acknowledgments We would like to thank Roberto Bayardo for providing us the MaxMiner algorithm and Johannes Gehrke for the MAFIA algorithm References R Agrawal C Aggarwal and V Prasad Depth First Gener ation of Long Patterns In ACM SIGKDD Con Aug 2000 R Agrawal et al Fast discovery of association rules In Advances in Knowledge Discovev and Data Mining AAA1 Press, 1996 R J Bayardo Efficiently mining long patterns from databases In ACM SIGMOD Con June 1998 D Burdick M Calimlim and J Gehrke MAFIA a maximal frequent itemset algorithm for transactional databases In Intl Con on Data Engineering Apr 2001 D Gunopulos H Mannila and S Saluja Discovering all the most specific sentences by randomized algorithms In Intl Conf on Database Theon\222 Jan 1997 J Han J Pei and Y Yin Mining frequent patterns without candidate generation In ACM SIGMOD Con May 2000 D.4 Lin and Z M Kedem Pincer-search A new algorithm for discovering the maximum frequent set In Intl Con Ex reridin Database Technology Mar 1998 D Yelfin An algorithm for dynamic subset and intersection testin  Theoretical Cornputer Science 129:397-406 1994 M J.!Zaki Generating non-redundant association rules In ACM SIGKDD Con Au 2000 M J Zaki and K Gouda.%ast vertical mining using Diffsets TR01-1,CSDe t,RPI,Mar.2001 M J Zaki and 2-J Hsiao CHARM An efficient algorithm for closed association rule mining TR 99-10 CS Dept RPI Oct. 1999 170 


n I31 41 51 61 71 8 I 9 Graph diameter q5\(SCCn Average number of lateral links E Average number of MI local links MI\(loc Average number of MB local links MB\(Zoc I Graph size n  1 e n I 12 I 72 I 480 I 3,600 I 30,240 I 282,240 I 2,903,040 6 8 16 19 1.500 2.583 3.683 4.783 0.667 1.500 3.200 5.000 0.833 1.222 1.925 2.337 1.500 3.000 Average number of local links L Average distance d\(SCC 2.722 5.125 7.337 5.306 8.808 12.121 Table 1 Average distance of SCC graphs under minimal routing 300000 250000 g 200000 3 E L 150000   100000 50000 0 erage number of MB local links is concerned. Also observe that for 3 5 n 5 4 the greedy routing algorithm performs as well as the minimal routing algorithm Besides, our re sults indicate that the performance of these algorithms is quite similar for 5 5 n 5 9 which makes the less complex greedy routing algorithm particularly attractive Average costs of paths produced by the three routing al gorithms are summarized in Table 2 The random routing algorithm has a complexity of O\(n and performs reason ably well on the average Utilization of such an algorithm may however result in variations in the average cost of routes up to the worst-case values shown in Table 2  Minimal routing  andom rout. \(worst case     n 3 Minimal Greedy Random routing rout rout Theor Simul Worst-case 3.000 3.000 3.000 3.084 3.167 I1 I I I I I 4 5 I I I I I 5.306 5.305 5.500 5.514 5.694 8.808 8.812 9.261 9.264 9.775 Table 2 Average costs vs routing algorithms Figure 6 shows distribution curves comparing the three routing algorithms in the case of an SCC graph A point 01 NI in one of these curves indicates that the corre sponding routing algorithm will compute a route of cost DI to the identity for NI nodes in the SCC graph The aver age distribution for the random routing algorithm is shown but the results for that algorithm may actually vary from the minimal to the worst-case distributioncurves due to the non deterministic nature of the algorithm It is also interesting to observe that the greedy routing algorithm provides a dis tribution curve which is close to that of the minimal routing algorithm presenting however a smaller complexity 6 Considerations on wormhole routing 3 In this section we briefly describe how the algorithms presented in the paper cam be combined with wormhole routing 6 which is a popular switching technique used in parallel computers All three algorithms can be used with wormhole routing when implemented as source-based routing algorithms  111 In source-based routing tlhe source node selects the entire path before sending the packet Because the processing delay for the routing algorithm is incurred only at the source node it adds only once to the communication latency and can be viewed as part of the start-up latency Source-based routing however has two disadvantages 1 each packet must carry complete information about its path in the header which increases the packet length and 2 the path cannot be changed while the packet is being routed which precludes incorporating adaptivity into the routing algorithm Distributed routing eliminates the disadvantages of source-based routing by invoking the routing algorithm in each node to which the packet is forwarded ll Thus the decision on whether a packet should be delivered to the local processor or forwarded on an outgoing link is done 451 


locally by the routing circuit of a node Because the routing algorithm is invoked multiple times while a packet is being routed the routing decision must be taken as fast as pos sible From this viewpoint it is important that the routing algorithm can be easily and efficiently rendered in hardware which favors the random routing algorithm over the greedy and minimal routing algorithms Besides being the most complex algorithm discussed in this paper the minimal routing algorithm includes a feature which precludes its distributed implementation in associa tion with wormhole routing namely its backtracking mech anism Distributed versions of the random and greedy al gorithms, however, can be used in combination with worm hole routing A near-minimal distributed routing algorithm which supports wormhole routing can be obtained by re moving the backtracking mechanism from Alg 3 Such an algorithm is likely to have computational complexity and average cost that lie between those of the greedy and the minimal routing algorithm Due to its non-deterministic nature the random routing algorithm also seems to be a good candidate for SCC net works employing distributed adaptive routing  1 I Adap tivity is desirable for example if the routing algorithm must dynamically respond to network conditions such as conges tion and faults Some degree of adaptivity is also possible in the greedy and minimal routing algorithms which in some cases can decide between paths of equal cost 7 Conclusion This paper compared the average cost and the complex ity of three different routing algorithms for the SCC graph We divided routes into three components \(lateral links MI local links and MB local links and showed that only the number of MB local links may be affected by the routing algorithm being considered Exact expressions for the aver age number of lateral links and the average number of MI local links were presented Also an upper bound for the average number of MZ local links was derived considering a random routing algorithm As a result a tight upper bound on the average distance of the SCC graph was obtained Simulation results for a random a greedy and a minimal routing algorithm were presented and compared with theo retical values The complexity of the proposed algorithms is respectively O\(n O\(n2 and O\(n3 where n is the dimensionality of the SCC grap.h The results under mini mal routing produce exact numerical values for the average distance of SCC for 3 5 n 5 9 Results for the greedy algorithm match those of the min imal algorithm for 3 2 n 5 4 The greedy algorithm also performs close to minimality for 5 5 n 5 9 and is an in teresting choice due to its O\(n2 complexity The random routing algorithm has an O\(n complexity and performs fairly well on the average but may introduce additional MB local links in the route under worst-case conditions Finally we discussed how each of the routing algorithms can be used in association with the wormhole routing switch ing technique Directions for future research in this area in clude an evaluation of requirements for deadlock avoidance e.g number of virtual channels References l S B Akers,D. HarelandB Krishnamurthy,\223TheStarGraph An Attractive Altemative to the n-Cube,\224 Proc Int\222l Con Pal Proc 1987 pp 393-400 2 M M Azevedo N Bagherzadeh and S Latifi 223Broadcasting Algorithms for the Star-Connected Cycles Interconnection Network,\224 J Pal Dist Comp 25,209-222 1995 3 M M Azevedo N Bagherzadeh and S Latifi 223Embed ding Meshes in the Star-Connected Cycles Interconnection Network,\224 to appear in Math Mod. and Sci Comp 4 M M Azevedo N Bagherzadeh and S Latifi 223Fault Diameter of the Star-Connected Cycles Interconnection Net work,\224 Proc 28th Annual Hawaii Int\222l Con5 Sys Sci Vol 11 Jan. 3-6 1995 pp 469-478 SI W.-K Chen M F M Stallmann andE E Gehringer 223Hy percube Embedding Heuristics An Evaluation,\224 Int\222l J Pal Prog Vol 18 No 6 1989 pp 505-549 6 W J Dally and C I Seitz 223The Torus Routing Chip,\224 Dist Comp Vol 1 No 4 1986 pp 187-196 7 K Day and A Tripathi,\223A Comparative Study ofTopologica1 Properties of Hypercubes and Star Graphs,\224 IEEE Trans. Pal Dist Sys Vol 5 No 1 Jan. 1994 pp 31-38 8 D E Knuth The Art of Computer Programming Vol I Addison-Wesley 1968 pp 73 pp 176-177 9 S Latifi 223Parallel Dimension Permutations on Star Graph,\224 IFIP Trans A Comp Sei Tech 1993 A23 pp 191-201 lo S Latifi M M Azevedo and N Bagherzadeh 223The Star Connected Cycles A Fixed-Degree Interconnection Net work for Parallel Processing,\224 Proc Int\222l Con5 Pal Proc  1 11 L M Ni and P K McKinley 223A Survey of Wormhole Rout ing Techniques in Direct Routing Techniques,\224 Computer Feb 1993 pp 62-76  121 E P Preparata and J Vuillemin 223The Cube-Connected Cy cles A Versatile Network for Parallel Computation,\224 Comm ACM Vol 24 No 5 May 1981 pp 300-309  131 Y Saad and M H Schultz 223Topological Properties of Hy percubes,\224IEEE Trans Comp Vol 37 No 7 July 1988 pp 14 S Shoari and N Baghenadeh 223computation of the Fast Fourier Transform on the Star-Connected Cycle Network,\224 to appear in Comp  Elec. Engl 1996 15 P Vadapalli and P K Srimani 221\223ho Different Families of Fixed Degree Regular Cayley Networks,\224 Proc Int\222l Phoenix Con Comp Comm Mar 28-31,1995 pp 263-269 1993 Vol 1 pp 91-95 867-872 452 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


