Using Multi-A ttri bute Predicates for Mining Classi\014cation Rules Ming-Sy an Chen Electrical Engineering Departmen t National T aiw an Univ ersit y T aip ei T aiw an R OC email msc hen@cc.e e.n tu.edu t w Abstract In or der to impr ove the e\016ciency of deriving classi\014c ation rules fr om a lar ge tr aining dataset we develop in this p ap er a two-phase metho d for multi-attribute extr action A fe atur e that is useful in inferring the gr oup identity of a data tuple is said to have a go o d inferenc ep ower to that gr oup identity Given a lar ge tr aining set of data tuples the 014rst phase r eferr e dto as fe atur e extr action phase is applie d to a subset of the tr aining datab ase with the purp ose of identifying useful fe atur es which have go o d infer enc ep owers to gr oup identities In the se c ond phase r eferr e dtoasfe atur e c ombination phase these extr acte dfe atur es ar e evaluate dto gether and multi-attribute pr e dic ates with str ong infer enc ep owers ar e identi\014e d A te chnique on using match index of attributes is devise dtor e duc e the pr oc essing c ost 1 In tro duction V arious data mining capabilities ha v e b een explored in the literature Mining asso ciation rules has attracted a signi\014can t amoun t of researc h atten tion 3  9 11 15  F or example giv en a database of sales transactions it is desirable to disco v er all asso ciations among items suc h that the presence of some items in a transaction will imply the presence of other items in the same transaction Another t yp e of data mining is on ordered data suc h as sto c k mark et and p oin t of sales data In teresting asp ects to explore from these ordered data include searc hing for similar sequences 1 e.g sto c ks with similar mo v emen t in sto c k prices and sequen tial patterns e.g gro cery items b ough to v er a set of visits in sequence Mining on W eb path tra v ersal patterns w as studied in 6  In addition one imp ortan t application of data mining is the abilit y to p erform classi\014cation in a h uge amoun t of data This is referred to as mining classi\014cation rules Explicitly  mining classi\014cation rules is an approac h of trying to dev elop rules to group data tuples together based on certain common features F or an example of commercial applications it is desirable for a car dealer to kno w what are the common features of its most customers so that its sales p ersons will kno w whom to approac h and its catalogs of new mo dels can b e mailed directly to those customers with iden ti\014ed features The business opp ortunit y can th us b e maxim i zed It is noted that due to the increasing use of computing for v arious applications the imp ortance of mining classi\014cation rules is gro wing at a rapid pace The fast gro wth in the amoun t of data in those applications has furthermore made the e\016cien t mining for classi\014cation rules a v ery c hallenging issue Classi\014cation rule mining has b een explored b oth in the AI domain 12 14  and in the con text of databases 2 5,7,8  In mac hine learning a decision-tree classi\014cation metho d dev elop ed b y Quinlan 13 14  is one of the most imp ortan t results and has b een v ery in\015uen tial to later studies It is a sup ervised learning metho d that constructs decision trees from a set of examples The qualit y of a tree dep ends on b oth the classi\014cation accuracy and the size of the tree Other approac hes on data classi\014cation include statistical approac hes 12  rough sets approac h 17  etc In the con text of databases an in terv al classi\014er has b een prop osed in 2  t o reduce the cost of decision tree generation An attribute-orien ted induction metho d has b een dev elop ed for mining classi\014cation rules in relational databases 8  The w ork in 10  explores rule extraction in a database based on neural net w orks It is noted that in mining classi\014cation rules for a giv en database one w ould naturally lik eto ha v ea training dataset large enough so as to ha v e a su\016cien t 


con\014dence on the rules deriv ed Ho w ev er with a large training set the execution time required for rule deriv ation could b e prohibitiv e in particular when forming m ulti-attribute predicates is needed When a sophisticated predicate is constructed from a com bination of features the execution time required gro ws exp onen tially with the size of a training database whic h is highly undesirable in man y applications Consequen tly w e presen t in this pap er a t w o-phase metho d for m ulti-attribute extraction and impro v e the e\016ciency of deriving classi\014cation rules in a large training dataset A feature that is useful in inferring the group iden tit y of a data tuple is said to ha v e a good infer enc e p ower to that group iden tit y  Giv en a large training set of data tuples the 014rst phase referred to as fe atur e extr action phase  is applied to a subset of the training database with the purp ose of iden tifying useful features whic hha v e go o d inference p o w ers to group iden tities Note ho w ev er that in some cases the group iden tit y is not so dep enden t on the v alue of a single attribute Rather the group iden tit y dep ends on the com bined v alues of a set of attributes This is particularly true in a database where attributes ha v e strong dep endencies among themselv es Com bining sev eral individual features is th us required for constructing m ulti-attribute predicates with b etter inference p o w ers In the second phase referred to as fe atur ec ombination phase  those features extracted from the 014rst phase are ev aluated together and m ulti-attribute predicates with strong inference p o w ers are iden ti\014ed A tec hnique on using match index of attributes is devised to reduce the processing cost In essence a matc h index is a heuristic indication on the com bined inference p o w er of m ultiple attributes and can b e used to iden tify unin teresting com bined attributes and remo v e them from later processing Note that b eing p erformed only on a subset of the training set the feature extraction phase can b e executed e\016cien tly  On the other hand since the features extracted are used to the whole training set in the feature com bination phase the con\014dence of the 014nal classi\014cation rules deriv ed can hence b e ensured This pap er is organized as follo ws A problem description is giv en in Section 2 The t w o-phase metho d for mining classi\014cation rules is describ ed in Section 3 Section 4 con tains the summary  2 Problem Description In general the problem on mining classi\014cation rules can b e stated as follo ws W e are giv en a large database W in whic h eac h tuple consists of a set of n attributes features f A 1  A 2    A n g  The terms attribute Lab el Gender Age Bev erage State Group id 1 M 3 w ater CA I 2 F 4 juice NY I 3 M 4 w ater TX I 4 F 4 milk TX I 5 M 5 w ater NY I 6 M 3 juice CA I 7 M 3 w ater CA II 8 F 5 juice TX II 9 F 5 juice NY II 10 F 6 milk TX III 11 M 4 milk NY III 12 M 5 milk CA III 13 F 4 milk TX III 14 F 6 w ater NY III 15 F 6 w ater CA III T able 1 A sample pro\014le for classifying 15 c hildren and feature are used in terc hangeably in this pap er F or example attributes could b e age salary range gender zip co de etc Our purp ose is to classify all data tuples in this database in to di\013eren t groups according to their attributes In order to learn prop er kno wledge on suc h classi\014cation w e are giv en a small training database E in whic h eac h tuple consists of the same attributes as tuples in W and additionally has a kno wn group iden tit y asso ciated with it An example group iden tit y is the t yp e of car o wned sa y plain go o d or luxury W ew an t to 1 learn the relationship b et w een attributes and group iden tit y from the training database E and then 2 apply the learned kno wledge to classify data in the large database W in to di\013eren t groups Note that once the relationship b et w een attributes and group iden tit y is learned in 1 the pro cess in 2 can b e p erformed in a straigh tforw ard manner Hence w e shall fo cus our discussion on metho ds for 1 in this pap er i.e to iden tify attributes from f A 1  A 2    A n g that ha v e strong inference to the group iden tit y  Consider a sample pro\014le for 15 c hildren in T able 1 as an example In T able 1 eac h tuple corresp onding to eac hc hild con tains attributes his/her gender age b ev erage preferred and state liv ed and additionally his/her group iden tit y  F or ease of exp osition eac h tuple is giv en a lab el in it 014rst column whic h is ho w ev er not part of the attributes W eno ww ould lik e to explore the relationship b et w een the attributes i.e gender age b ev erage and state in this case and the group iden tit y  As stated b efore an attribute that is useful in inferring the group iden tit y of a data tuple 


is said to ha v e a go o d inference p o w er to that group iden tit y A pr e dic ate in this study means a resulting classi\014cation rule from step 1 men tioned ab o v e and will b e used in step 2 to classify data tuples in the database W In our discussion the qualit y of a predicate refers to the com bined inference p o w er of the attributes this predicate is comp osed of The prop osed metho d consists of t w o phases fe atur e extr action phase and fe atur ec ombination phase  Giv en a large training set of data tuples the 014rst phase feature extraction phase is to learn useful features whic hha v e good inference p o w ers to group iden tities from a subset D of the training database E As men tioned earlier in some cases the group identit y is not so dep enden t on the v alue of a single attribute but instead dep ends up on the com bined v alues of a set of attributes This is particularly true in the presence of those attributes that ha v e strong inference among themselv es Consider the pro\014le in T able 2 as an example In T able 2 it is found that a male with lo w income and a female with high income usually driv e cars whereas a male with high income and a female with lo w income ride bik es In this case exploring the relationship b et w een v ehicle corresp onding to the group id in T able 1 and either gender or income attribute will lead to little results since neither gender nor income has a go o d inference p o w er to the v ehicle Ho w ev er a com bination of gender and income e.g a male and lo w income indeed has a go o d inference p o w er to the v ehicle It can b e seen that the t yp e of v ehicle in eac h tuple can in fact b e determined from the com bined v alue of gender and income In view of this to exploit the b ene\014t of m ulti-attribute predicates w e devise the second phase feature com bination phase whic hev aluates individual features extracted in the 014rst phase and pro duces m ulti-attribute predicates with strong inference p o w ers The t w o-phase metho d for mining classi\014cation rules can b e summarized as follo ws Algorithm M  Mining classi\014cation rules F eature extraction phase T o learn useful features whic hha v e go o d inference p o w ers to group identities from a subset of the training database F eature com bination phase T oev aluate extracted features based on the en tire training database and form m ulti-attribute predicates with go o d inference p o w ers Lab el Gender Income V ehicle 1 male lo w car 2 male lo w car 3 female high car 4 female high car 5 male high bik e 6 male high bik e 7 female lo w bik e 8 female lo w bik e T able 2 A sample pro\014le for preferred v ehicles 3 Mining Classi\014cation Rules W e describ e in this section a t w o phase metho d for mining classi\014cation rules The 014rst phase feature extraction phase is presen ted in Section 3.1 and the second phase feature com bination phase is presen ted in Section 3.2 As illustrated in Figure 1 the feature extraction phase is applied to a subset of the training database In this phase attributes that ha v e go o d inference p o wers to group iden tities are iden ti\014ed The op erations of this phase are explained b elo w First tuples in the database D are divided in to sev eral groups according to their group id's The inference p o w er of eac h attribute is then in v estigated one b y one Supp ose A is an attribute and f a 1  a 2    a m g are m p ossible v alues of attribute A  Also the domain of the group iden tit y g is represen ted b y domain g  f v 1  v 2    v j domain  g  j g  The primary gr oup for a v alue a i of attribute A  denoted b y v a i  is the group that has the most tuples with their attribute A  a i  Explicitly  use n A  a i v k  to denote the n um b er of tuples whic h are in group v k and ha v eav alue of a i in their attribute A  Then w e ha v e n A  a i v a i  max v k 2 domain  g  f n A  a i v k  g  1 The primary group for eac hv alue of attribute A can hence b e obtained F or the example pro\014le in T able 1 if A is gender then domain A  f Male F emale g  and n A Male,I n A Male,I I and n A Male,I I I Group I is therefore the primary group for the v alue Male of the attribute gender 


The hit r atio of attribute A  denoted b y h  A  is de\014ned as the p ercen tage of tuples whic h according to their corresp onding attribute v alues fall in to their primary groups Let N denote the total n um ber of tuples Then h  A  P 1 024 i 024 m n A  a i v a i  N  2 It can b e seen that the stronger the relationship b et w een an attribute and the group iden tit y  the larger the hit ratio of this attribute will b e A hit ratio of an attribute w ould b ecome one if that attribute could uniquely determine the group iden tit y  The hit ratio is a quan titativ e measuremen t for the inference p o w er of an attribute According to the primary groups of v arious v alues of an attribute the hit ratio of that attribute can b e determined Note that in essence w e w an ttoin v estigate the inference p o w er of some combined features Ho w ev er to reduce the pro cessing cost w ew ould lik e to restrict our atten tion to those attributes whose individual hit ratios meet a predetermined threshold Sp eci\014cally w e include attribute A in to a set S A for future pro cessing only if the hit ratio of A is larger than or equal to a predetermined threshold Note that this is a greedy approac h and do es not guaran tee pro viding the optimal solutions As a consequence those features with p o or inference p o w ers will b e remo v ed from later pro cessing and the pro cessing cost can th us b e reduced The most distinguishing attribute refers to the attribute with the largest hit ratio F ormally  the 015o w of the feature extraction phase is summarized as follo ws F eature extraction phase Step 1 Divide tuples in the database D in to sev eral groups according to their group id's Step 2 Let A denote the next attribute to pro cess Step 3 Determine the primary group for eac hv alue of attribute A  Step 4 According to the primary groups of v arious v alues of attribute A  obtain the hit ratio of A  Step 5 Include attribute A in to set S A for future processing if the hit ratio of A is larger than or equal to a predetermined threshold Gender I II III max group Male 4 1 2 4 I F emale 2 2 4 4 I I I hit ratio 8 15 T able 3 Distribution when the pro\014le is classi\014ed b y gender Step 6 If there is an y more attribute to pro cess then go to Step 2 Otherwise stop F or illustrativ e purp oses consider the example pro\014le in T able 1 whic h can b e view ed as a subset of the training database to b e used in the feature extraction phase First w e classify this pro\014le according to gender and obtain the results in T able 3 As explained earlier Group I is the primary group for the v alue Male of attribute gender Also it can b e obtained that Group I I I is the primary group for the v alue F emale of attribute gender As a result there are 8 tuples out of 15 tuples fall in to their primary groups The hit ratio of attribute gender is th us 8 15  Similarly  it can b e v eri\014ed that the hit ratios of age bev erage and state are resp ectiv ely  10 15  9 15 and 6 15  Finally ha ving the largest hit ratio among the four attributes age is the most distinguishing attribute in this example Supp ose the predetermined threshold for the inclusion in to S A is 8 15  Then attributes gender age and b ev erage are included in to S A whereas attribute state is not The attributes collected in S A will b e ev aluated together in the feature com bination phase to form m ulti-attribute predicates Moreo v er w eha v e the follo wing lemma whic h sp eci\014es the lo w er b ound of the hit ratio of an attribute Lemma 1 Let A b e an attribute and g b e a group iden tit y  Then h  A  025 1 j domain  g  j  whic h is a tigh tlo w er b ound Note that the feature extraction phase explores the relationship b et w een the group iden tit y and individual 


attributes Ho w ev er as explained b y using the pro\014le in T able 2 earlier in some cases the group iden tit yis not so dep enden t on the v alue of a single attribute but instead dep ends up on the com bined v alues of a set of attributes This is the v ery reason that the feature com bination phase is called for The feature com bination phase is applied to the en tire training database with the purp ose of ev aluating the inference p o w er resulting from com bining attributes As stated b efore w e shall only examine those attributes in S A so as to decrease the pro cessing cost In addition a tec hnique on using matc h index of attributes is devised to further reduce the pro cessing cost Sp eci\014cally  instead of ev aluating ev ery pair of attributes w e shall only deal with those attribute pairs whose match indexes meet another threshold where a matc h index is a heuristic indication on ho w lik ely a pair of attributes will lead to a strong inference po w er as a whole Supp ose that A and B are t w o attributes to b e used to form a 2-attribute predicate Let domain A  f a 1  a 2    a m g and domain B  f b 1  b 2    b p g  Recall that n A  a i v k  is the n um beroftuples whic h are in group v k and ha v e their attribute A  a i  n B  b j v k  is de\014ned similarly  Then the match index of t w o attributes A and B  denoted b y M I  A  B  is de\014ned as M I  A B  X 1 024 i 024 m X 1 024 j 024 p max v k 2 domain  g  f min  n A  a i v k  n B  b j v k  g  Essen tially  M I  A  B  is a heuristic indication on the com bined inference p o w er of A and B  It can b e observ ed that M I  A  B  is in fact an upp er b ound for the n um b er of tuples falling in to the primary groups of v arious v alues of attribute pair  A  B  Explicitly  M I  A;B  N is an upp er b ound for the hit ratio of attribute pair  A  B  The use of matc h index will signi\014can tly reduce the pro cessing cost while causing little compromise on the qualit y of the resulting predicates The o v erall 015o w of the feature com bination phase can b e summarized as follo ws F eature com bination phase Step 1 Let attribute pair  A  B  b e the next attribute pair from S A to pro cess Step 2 If the matc h index of  A  B  do es not reac h the predetermined threshold go to Step 6 Step 3 Determine the primary group for eac hv alue of attribute pair  A  B  Step 4 According to the primary groups of v arious v alues of attribute pair  A  B  obtain the hit ratio of pair  A  B  Step 5 Include attribute pair  A  B n to a set S C for future pro cessing if the hit ratio of  A  B  is larger than or equal to a predetermined threshold Step 6 If there is an y more attribute pair to pro cess then go to Step 1 Otherwise stop 4 Conclusion W eha v e dev elop ed in this pap er a t w o-phase metho d for m ulti-attribute extraction and impro v ed the e\016ciency of classi\014cation rule deriv ation in a large training dataset Giv en a large training set of data tuples the feature extraction phase w as applied to a subset of the training database with the purp ose of iden tifying useful features whic hha v e go o d inference po w ers to group iden tities In the feature com bination phase these extracted features w ere ev aluated systematically and m ulti-attribute predicates with strong inference p o w ers w ere iden ti\014ed A tec hnique on using matc h index of attributes w as utilized to reduce the pro cessing cost Ac kno wledgemen ts M.-S Chen is in part supp orted b y National Science Council Pro ject No NSC 87-2213-E-002-009 and Pro ject No 87-2213-E-002-101 T aiw an R OC References  R Agra w al C F aloutsos and A Sw ami E\016cien t Similarit y Searc h in Sequence Databases Pr o c e e dings of the 4th Intl c onf on F oundations of Data Or ganization and A lgorithms  Octob er 1993 


 R Agra w al S Ghosh T Imielinski B Iy er and A Sw ami An In terv al Classi\014er for Database Mining Applications Pr o c e e dings of the 18th International Confer enc eonV ery L ar ge Data Bases  pages 560{573 August 1992  R Agra w al and R Srik an t F ast Algorithms for Mining Asso ciation Rules in Large Databases Pr o c e e dings of the 20th International Conferenc eonV ery L ar ge Data Bases  pages 478{499 Septem b er 1994  R Agra w al and R Srik an t Mining Sequential P atterns Pr o c e e dings of the 11th International Confer enc e on Data Engine ering  pages 3 14 Marc h 1995  T.M An w ar H.W Bec k and S.B Na v athe Kno wledge Mining b y Imprecise Querying A Classi\014cation-Based Approac h Pr o c e e dings of the 8th International Confer enc e on Data Engine ering  pages 622{630 F ebruary 1992  M.-S Chen J.-S P ark and P S Y u E\016cien t Data Mining for P ath T ra v ersal P atterns IEEE T r ansactions on Know le dge and Data Engine ering  10\(2 April 1998  J Han Y Cai  and N Cercone Kno wledge Disco v ery in Databases An A ttribute-Orien ted Approac h Pr o c e e dings of the 18th International Confer enc eonV ery L ar ge Data Bases  pages 547{559 August 1992  J Han Y Cai and N Cercone Data Driv en Disco v ery of Quan titativ e Rules in Relational Databases IEEE T r ansactions on Know le dge and Data Engine ering  pages 29{40 F ebruary 1993  J Han and Y F u Disco v ery of Multiple-Lev el Asso ciation Rules from Large Databases Pr oc e e dings of the 21th International Confer enc eon V ery L ar ge Data Bases  pages 420{431 Septem ber 1995  H Lu R Setiono and H Liu NeuroRule A Connectionist Approac h to Data Mining Pr oc e e dings of the 21th International Confer enc eon V ery L ar ge Data Bases  pages 478{489 Septem ber 1995  J.-S P ark M.-S Chen and P S Y u Using a Hash-Based Metho d with T ransaction T rimmi ng for Mining Asso ciation Rules IEEE T r ansactions on Know le dge and Data Engine ering  9\(5 825 Octob er 1997  G Piatetsky-Shapiro Disco v ery  analysis and presen tation of strong rules In G PiatetskyShapiro and W J F ra wley  editors Know le dge Disc overy in Datab ases  pages 229{238 AAAI/MIT Press 1991  J R Quinlan C4.5 Pr o gr ams for Machine L e arning  Morgan Kaufmann 1993  J.R Quinlan Induction of Decision T rees Machine L e arning  1:81{106 1986  R Srik an t and R Agra w al Mining Generalized Asso ciation Rules Pr o c e e dings of the 21th International Confer enc eon V ery L ar ge Data Bases  pages 407{419 Septem b er 1995  J T.-L W ang G.-W Chirn T.G Marr B Shapiro D Shasha and K Zhang Combinatorial P attern Disco v ery for Scien ti\014c Data Some Preliminary Results Pr o c e e dings of A CM SIGMOD Minne ap olis MN  pages 115{125 Ma y  1994  W Ziark o The disco v ery  analysis and represen tation of data dep endancies in databases In G Piatetsky-Shapiro and W J F ra wley  editors Know le dge Disc overy in Datab ases  pages 195 209 AAAI/MIT Press 1991 


GLU66 ALA1 GLY71 ILE96 THR168 GLY72 THR164 TRP173 TYR184 ASN24 T1 1GLU66 2ALA1 3GLY71 4ILE96 T2 1ALA1 2GLY71 3ILE96 4THR168 T3 1GLY71 2ILE96 3THR168 4GLY72 T4 1ILE96 2THR168 3GLY72 4THR164 T5 1THR168 2GLY72 3THR164 4TRP173 T6 1GLY72 2THR164 3TRP173 4TYR184 T7 1THR164 2TRP173 3TYR184 4ASN24  Figure 4 Example r the generation of transactions from a sequence is true with a certain probability To restrict the set of possible association rules a minimum con\336dence conf min is required Since the relative positions of the residues are directly encoded in each item only subsequences with the exactly same order of residues are matched This approach also allows gaps in the sequences However in contrast to general frequent sequences the lengths of the gaps in the matching subsequences all have the same size which is important in this application context Details regarding the process and experimental results without the use of an underlying database system can be found in 4.2 Generating Frequent Item Sets inside the Database The necessary prerequisite to ef\336ciently produce frequent item sets inside the database consists in the existence of adequate operators which n the one hand allow the user to easily specify application speci\336c tasks and n the other hand enable the system to apply internal optimization strategies In this section we show that the pure grouping functionality extended by the cube operator family is not suf 336cient to support the application of data mining algorithms The relational mapping of our sample protein conformation scenario is rather straightforward The generated transactions are stored within a single table with four columns denoting the position of the amino acid residues inside the sliding window de\336ning the substructure and a running number as the transaction ID TID A1 A2 A3 A4 T1 1GLU66 2ALA1 3GLY71 4ILE96 T2 1ALA1 2GLY71 3ILE96 4THR168 T3 1GLY71 2ILE96 3THR168 4GLY72 T4 1ILE96 2THR168 3GLY72 4THR164 T5 1THR168 2GLY72 3THR164 4TRP173 T6 1GLY72 2THR164 3TRP173 4TYR184 T7 1THR164 2TRP173 3TYR184 4ASN24  Computing item sets and weighting them with the necessary support and con\336dence is the main crucial operation in computing association rules r the n th generation an item set re\337ects all n combinations for a iven set of grouping attributes With the number of attributes reasonably low i.e short subsequences e may use the grouping sets clause to specify the necessary combinations within a single query For example with four attributes A 1  A 2  A 3  and A 4  e may issue the following grouping condition GROUP BY GROUPING SETS A1,A3 A1,A4 A2,A3 A2,A4 A3,A4 r larger subsequences i.e columns contributing to the computation of item sets the speci\336cation of all permutation combinations can be a tedious task Another alternative is to issue a cube operator r the n set of parameters and eliminate the unwanted grouping combinations in a having clause referring to the grouping columns For the ongoing example with four attributes we yield the following expression GROUP BY CUBE\(A1,A2,A3,A4 HAVING NOT exclude 0-cardinality combinations GROUPING\(A1  1 AND GROUPING\(a2 ND GROUPING\(a3   1 AND GROUPING\(a4  1 exclude 1-cardinality combinations OR GROUPING A1  1 AND GROUPING\(A2  1 AND GROUPING\(A3   OR GROUPING A1  1 AND GROUPING\(A2  1 AND GROUPING\(A4   OR GROUPING A1  1 AND GROUPING\(A3  1 AND GROUPING\(A4   OR GROUPING A2  1 AND GROUPING\(A3  1 AND GROUPING\(A4  1 exclude 3and 4-cardinality combinations  Although this would return the required 2-itemset combinations the speci\336cation becomes tedious with an increasing number of columns r the computation of all possible combinations is certainly not the most ef\336cient y of computing the required item sets Proceedings of the 15th International Conference on Sci entific and Statistical Database Management \(SSDBM\22203 1099-3371/03 $17.00 \251 2003 IEEE  


With this motivation as a background we propose the GROUPING COMBINATIONS operator generating all k item sets for a given set of n grouping columns The syntax integrates seamlessly into the SQL grouping extensions of cube  rollup  and grouping sets  GROUP BY GROUPING COMBINATIONS\(\(A1,A2  An k This expression may be seen as a shortcut for the grouping sets expression with k over n item sets each with a cardinality of k  i.e GROUP BY GROUPING SETS\(\(A1,A2,...,Ak A1,A2,...,Ak-1,Ak+1  The basic idea of the grouping combinations operator is generating all possible grouping combinations with a given cardinality of k  Special cases must be considered regarding the value of k and the number of attributes 200 For k  n  the grouping combinations operator returns a single set with all grouping columns n s a 336rst parameter e.g GROUP BY GROUPING COMBINATIONS\(\(A1,A2  An n is equal to GROUP BY GROUPING SETS\(\(A1,A2  An is equal to a egular GROUP BY A1,A2  An 200 For k 1  the operator corresponds semantically to a grouping sets with n single sets each consisting of exactly a single grouping column More formally GROUP BY GROUPING COMBINATIONS\(\(A1,A2 An 1 is equal to GROUP BY GROUPING SETS\(\(A1 A2  An 200 For k 0  the proposed grouping combinations operator produces a single grouping combination encompassing all tuples of the underlying table e.g GROUP BY GROUPING COMBINATIONS\(\(A1,A2 An 0 is equal to a query without any explicit group by clause but an aggregation function in the select clause The grouping combinations operator tends the set of group by operators proposed within the OLAP context The generation of frequent item sets for a iven generation i.e cardinality of the grouping combinations r is not supported by these operators 5 Cost Estimations This section analyses the different methods to compute the cluster and to 336nd the frequent item sets from a performance point of view In a 336rst step our underlying cost model is outlined followed by costing the different alters to assign the nearest prototype to each amino acid residue 5.1 General Cost Model To estimate the cost we rely on the linear cost model taking the size of the data stream between relational plan operators to compare the costs of different query plans The size of the data stream is the result of the number of attributes multiplied with the cardinality of the table To evalutate the selection predicates we consider a full table scan with the exception of a single tuple access ploiting the existence f an index structure r the join operator we may choose between a nested loop and a hash join The 336rst alternative implies a multiplication of the data stream cardinalities and is omitted in favour of a hash join r justifying this assumption we rely on the fact that a machine has enough capacity to keep the prototype table in main memory A hash join would read the smaller table during the build phase and the larger table during the probe phase so that we end up in adding the cardinalities of the data streams to compute the cost of a join operator Due to our 1:N-relationship of prototypes and conformations the cardinality of the resulting data stream after a join equals the cardinality of the larger table At last we consider the size of the input stream as the cost of a group by operator because every tuple has to be taken into account and assigned to the corresponding group With this assumption we again consider a hash-based implementation so that we may neglect the sorting cost in case of a sort-based group by implementation The return operator 336nally simulates the cost delivering the result to the client program or in detail storing the result set temporarly within the SQLDA of a database system In our example the PConformations table has 1 d attributes with d as the number of maximum angles and a cardinality of pc  Similarly the Prototypes table has the same number of attributes with a cardinality of pt  Based on this assumption the cost computing the following query would be computed s follows SELECT pc.phi pc.psi pc.omega pc.chi1 pc.chi2 pt.omega pt.chi1 pt.chi2 FROM PConformations pc prototypes pt WHERE pc.phi  pt.phi AND pc.psi  pt.psi The scan and the join of both tables would yield an rhead of pc 004 6 pt 004 6 with d 5  The return operator Proceedings of the 15th International Conference on Sci entific and Statistical Database Management \(SSDBM\22203 1099-3371/03 $17.00 \251 2003 IEEE  


a Computing view b query graph t f ivot method Figure 5 pivot method query graphs re\337ecting the produced columns of the select clause contributes with a value of pc 004 8 to the overall cost resulting in 14 004 pc 6 004 pt  5.2 Comparing different Methods The cost of computing the nearest neighbour using the t method may be split into the computation of the cost for the view de\336nition to perform the t of the prototype information and the core select statement using the t view As already mentioned the pt 212 1 ary join of the prototype table is easily optimized by the existence of an index of the prototype ID In this case each join partner has the size of 1 004 1 d  yielding pt 004 1 d  as the overall cost computing the view without any return operator The query itself joins the view with the PConformations table Reading the result of the view corresponds to one single tuple with pt 004 1  d  columns The access of the PConformations table has the size of pc 004 1  d   The outer query adds to the overall costs with reading the result of the inner query  pc 004 1  d  004  pt 1  and accessing the Prototypes table  pt 004 1  d   The return operator 336nally consumes a data stream of cardinality pc with d 3 attributes The cost of discretization using the self-join method can be computed in three steps The 336rst step considers the computation of the view P rototypeAssignment which requires the access to the two tables P roteinConf ormation  pc 004 1  d   and P rototypes  pt 004 1  d   The resulting data stream which has to be read for further processing yields due to the semantics of the Cartesian product to  pc 004 pt  004 1  d 2  The second step addresses the inner query consuming exactly the size of the view and producing a data stream of pc 004 2 for only two columns The outer query reads the result of the inner query and the result of the view   pc 004 pt  004 3  d   At last the return operator has costs the size of the join operator yielding pc 004  d 3  In a similar y the cost for the minpos  method can be computed Instead of the join with the P rototypeAssigment view in the outer query the P roteinConf ormation table with a cost of pc 004  d 1 a Computing view b query graph PrototypeAssignment of self join method Figure 6 self-join method query graphs Figure 7 cost reduction scenario is referenced Additionally the view must be executed only once further reducing the cution costs Table 1 summarizes the partial and total cost for all different methods computing the prototype for each amino acid residue Since this tabular and formular-based representation does not give any hint about the best strategy 336gure 7 gives a scenario with four different dimensions i.e d 1  4  7  10  and 5000 amino acid residues The scenario shows the resulting performance gain of the pivot method compared to minpos  and selfjoin method with 25 100 1000 and 2500 prototypes Within the proposed cost model the t method yields a slightly lower total cost than the minpos  method because the pt 212 1 ary selfjoin is considered extremely cheap r due to the dependency of the total cost from the number of prototypes the t method can not be considered a feasible solution for real applications with a reasonable high number of prototypes Compared to the self-join method the minpos  method yields a substantial cost reduction 6 Summary and Conclusion This paper introduces the problem of 336nding frequent substructures in protein data sets The analysis process is split into two parts The 336rst step consists in 336nding the Proceedings of the 15th International Conference on Sci entific and Statistical Database Management \(SSDBM\22203 1099-3371/03 $17.00 \251 2003 IEEE  


 t method view execution pt 001 1  d   f database operations inner query pt 001 1  d  pc 001 1  d   pt 1 joins outer query pc 001 1  d  001  pt 1 pt 001 1  d  pc 001  d 3 total cost pt 001 3  5 d  pc 001 5  3 d  pc 001 pt 001 1  d  self-join method view execution 2 001  pc 001 1  d  pt 001 1  d   f database operations inner query  pc 001 pt  001 3  d  1 join outer query pc 001 2 pc 001 pt  001 3  d  pc 001 3  d  2 cross product total cost pt 001 2  2 d  pc 001 7  3 d  pc 001 pt 001 6  2 d  1 group by minpos method view execution pc 001 1  d  pt 001 1  d   f database operations inner query  pc 001 pt  001 3  d  1 join outer query pc 001 2 pc 001 1  d  pc 001 3  d  1 cross product total cost pt 001 1  d  pc 001 7  3 d   pc 001 pt  001 3  d  1 group by Table 1 Cost comparison of the different methods nearest prototype in the multi-dimensional dihedral angle space To accomplish this task the data sets are brought into a relational schema and a method is proposed to compute the minimal distance considering the wrap-around effect in the angle space Three different methods to 336nd an associated prototype inside the database systems are compared A minimal SQL extension  minpos  maxpos  function results in much more ef\336cient query execution plans The second step of generating frequent item sets to detect frequent substructures within the amino acid sequences requires substantial SQL extension A ew operator as a new member of the OLAP grouping function operators is introduced This operator is a generic tool and may be ploited by a huge set of data mining applications To summarize a database system used to ef\336ciently analyse huge data volumes requires additional support from the technology  The required extension range from minimal UDFs like our proposed minpos  maxpos functions to more complex operators like our proposed grouping combinations operator f and only if the database community provides this kind of functionality the acceptance of database systems in the biotechnology community will increase in the near future References  R Agra w al T  Imielinski and A N Sw ami Mining association rules between sets of items in large databases In Proceedings of the International Conference on Management of Data  pages 207\320216 ACM Press 1993  S Bohl M Dink elack er  J  Griese and S Schrader  Highly adaptable amino acid side chain rotamer library in pdb coordinates In Workshop in Computational Biology at the Plant Biochemistry Department of the Albert-Ludwigs-Universitt Freiburg Germany  2002  M Bo wer  F  Cohen and R Dunbrack Homology modeling with a backbone-dependent rotamer library J Mol Biol  267:1268\3201282 1997  R Chandrasekaran and G Ramachandran Studies on the conformation of amino acids xi analysis of the observed side group conformations in proteins Int J Pept Prot Res  2:223\320233 1970  R Dunbrack and F  Cohen Bayesian statistical analysis of protein side-chain rotamer preferences Protein Sci  6:1661\320 1681 1997  J Gray  A  Bosw orth A Layman and H Pirahesh Data cube A relational aggregation operator generalizing groupby cross-tab and sub-total In Proceedings of the Twelfth International Conference on Data Engineering  pages 152\320 159 IEEE Computer Society 1996  A Hinneb ur g M Fischer  and F  Bahner  Finding frequent substructures in 3d-protein databases In Workshop on Bioinformatics at the 19th International Conference on Data Engineering  IEEE Computer Society 2003  M James and A Sielecki Structure and re\336nement of penicillo-pepsin at 1.8 a resolution J Mol Biol  125:299\320 361 1983  J K usze wski A Gronenborn and G Clore Impro ving the quality of nmr and crystallographic protein structures by means of conformational database potential derived from structure databases Protein Sci  5:1067\3201080 1996  S C Lo v ell J M W ord J S Richardson and D C Richardson The penultimate rotamer library Proteins Struct Funct Genet  40:389\320408 2000  M MCGre gor  S  Islam and M Sternber g Analysis of the relationship between side-chain conformation and secondary structure in globular proteins J Mol Biol  198:295\320310 1987  R Srikant and R Agra w al Mining generalized association rules In VLDB\32595 Proceedings of 21th International Conference on Very Large Data Bases Switzerland  pages 407\320 419 Morgan Kaufmann 1995  M J Zaki Ef 336cient enumeration of frequent sequences In Proceedings of the 1998 ACM CIKM International Conference on Information and Knowledge Management Bethesda Maryland USA November 3-7 1998  pages 68\32075 ACM 1998  M Zhang B Kao C L Y ip and D  W L Cheung Ffs an i/o-ef\336cient algorithm for mining frequent sequences In Knowledge Discovery and Data Mining PAKDD 2001 5th Paci\336c-Asia Conference Hong Kong China April 16-18 2001 Proceedings  volume 2035 of Lecture Notes in Computer Science  pages 294\320305 Springer 2001 Proceedings of the 15th International Conference on Sci entific and Statistical Database Management \(SSDBM\22203 1099-3371/03 $17.00 \251 2003 IEEE  


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


