Similarity Search in Sets and Categorical Data Using the Signature Tree Nikos Mamoulis David W Cheung and Wang Lian Department of Computer Science and Information Systems University of Hong Kong Pokfulam Road Hong Kong 000 nikos,dcheung,wlian 001 csis.hku.hk Abstract Data mining applications analyze large collections of set data and high dimensional categorical data Search on these data types is not restricted to the classic problems of mining association rules and classi\336cation but similarity search is also a frequently applied operation Access methods for multidimensional numerical data are inappropriate 
for this problem and specialized indexes are needed We propose a method that represents set data as bitmaps signatures and organizes them into a hierarchical index suitable for similarity search and other related query types In contrast to a previous technique the signature tree is dynamic and does not rely on hardwired constants Experiments with synthetic and real datasets show that it is robust to different data characteristics scalable to the database size and ef\336cient for various queries 1 Introduction Similarity search is a core operation of many data 
analysis tasks in data mining multimedia and time-series databases biological and scien ti\336c databases Database research has primarily focuse d on the special case where the data and queries are points in a multidimensional space and the domains of the dimensions are numerical However in many applications multivariate analysis is applied on complex data domains which do not have a natural order Consider for example a database 002 that contains consumer transactions Given a transaction 003  corresponding to a customer a search problem is 336nding the most similar 
transactions in the database in order to provide recommendations about items the customer would be interested in If 004 is the total number of available items this problem can be thought of as nearest neighbor search in an 004 dimensional space where the discrete domain of each dimension is 005 007 t 013 r  A related problem is similarity search in a multidimensional space where the dimensions ha ve categorical domains It is not hard to see that it is a special case of the search problem in transac tional data described above the items correspond to values of categorical attributes and 
they are divided into 016 groups 017 021 t 017 023 t 025 025 025 t 017 027  which correspond to the natural dimensions i.e the attributes The data are 016 tuples 030 032 021 t 032 023 t 025 025 025 t 032 027 037 where 032  is an element of group 017   In this case the data tuples have 336xed size and no two items i.e values of the same group co-exist in a tuple essentially an attribute takes exactly one value in each 
tuple Although these problems are fundamental in data analysis tasks they have not received much attention from the database literature as opposed to the extensive work e.g 20 18 23 21 6 fo r s imilarity search in lo w a n d h i g h d i mensional spaces of ordered domains On the other hand categorical and set data types are ubiquitous For example in most high-dimensional datasets of the UCI-KDD Archive 22  c ol l ect ed by real appl i cat i o n domai ns  t he maj o ri t y of the attributes are categorical In addition set data types e.g market basket transactions are frequently used to describe complex data in object-oriented/object-relational sys 
tems 11  In this paper we show how a hierarchical index can be used to process ef\336ciently sim ilarity search and other related query types on sets and categorical data In contrast to a previous method t h e signature tree SG\320tree is suitable for a dynamic environment with frequent updates and does not rely on hardwired constants which are hard to de\336ne a-priori The SG\320tree is a natural extension of the B  320tree and the R\320tree 13 found i n man y commerci a l DBMSs Thus the index carries many advantages of these structures it is i easy to implement sharing most of its 
modules with them and ii appropriate for various query types The remainder of the paper is organized as follows Section 2 de\336nes the problem of similarity search in sets and reviews related work In Section 3 we describe the hierarchical indexing method Section 4 shows how similarity search and other related queries can be evaluated using the SG\320tree Section 5 includes an experimental study    75  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


 76 Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


methods are considered for nearest neighbor search Some e.g 18  a re b a sed o n d imen sio n a lity red u c tio n  o t h e rs e.g 23 21 on compres s i on and o thers  e.g  6 o n d ata or query skew However extending these methods to operate on set and categorical data is not straightforward To our knowledge the only method previously proposed for similarity search in set and categorical data spaces is Due t o its high relevance to our approach we describe it in detail in the following paragraph The similarity search problem for sets has also been studied in 11  w here has h bas ed i nde x e s which provide approximate results are proposed In this paper we deal with the problem of 336nding the exact answers to queries thus our method is not directly comparable to these indexes Finally a similar hierarchical index to the SG\320tree was proposed in 7 Ne v e rt hel e s s opt i m i zat i o n o f insertions and splits has not been studied and the method is tuned/tested only for exact retrieval of signatures Moreover as shown in 14 s i gnat u re t r ees are not appropri a t e for set equality or subset queries which are best processed by inverted indexes and hash-based indexes In this paper we demonstrate that the SG\320tree is conversely suitable for similarity search The related problem of clustering categorical data has been studied during the past few years and several algorithms have been proposed e.g 10 12 9 Thes e m et hods apply on a static set of categorical data and consider the number of common neighbors as a metric of similarity between two transactions Using the same techniques for nearest neighbor search requires preprocessing information which may not be available in a dynamic environment 2.2.1 The signature table The signature table SG\320table is a hash-based index built from a static set of market-basket data 000 Itisusedtohash the transactions into a set of buckets based on their similarity to 001 frequent itemsets called signatures in 1  I n our paper we will refer to them as vertical signatures  to distinguish them from the signature de\336nition that we use The SG\320table 1 is co n s tru c ted i n t w o step s First a minimum spanning tree algorithm is run to cluster the set of items 002 into 001 groups each containing frequently correlated items The grouping process starts by considering each item a separate cluster and progressively re\336nes the clusters by merging item pairs with the maximum cooccurrence frequency In order to achieve clusters whose contents appear with approximately the same frequency in some transaction groups for which the total support in the database of their conten ts exceeds a certain threshold called critical mass  are removed before they grow larger The itemsets of the resulting clusters formulate the set of 001 vertical signatures which are used to construct the SG\320 A = {a,e B = {c,d C = {b,f,g T1 = {c,d T2 = {a,b,c T3 = {a,b,e T4 = {b,d,f,g T5 = {a,b,c,d,e T6 = {b,e,f S = {a,b,c,d,e,f,g  A B C 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 signature table in memory transactions in disk  T2 T1  T5 T3  T4,T6  a dictionary vertical signatures and transactions b the signature table Figure 1 Example of signature table table and hash the transactions Let 003 be a small constant called activation threshold  If a transaction 004 has at least 003 common items with a vertical signature 005  006 004 b 005 006 f 003  004 is said to activate 005  Based on which vertical signatures a transaction activates it is hashed into one of the r 017 entries of the SG\320table Figure 1 shows an example of a signature table and a set of transactions 020 004 022 024 026 026 026 024 004 032 034 hashed into it The items in the dictionary 002 are split into three groups 035 024 037 024   and the activation threshold is set to 2 For example transaction 004  activates only the vertical signature 035  006 004  b 035 006 f r  and is hashed to the partition with binary code     The index is used to answer similarity queries as follows The query transaction is compared to each signature 005 and a lower bound for the distance between  and the transactions indexed by the table entries depending on whether their 005 th bit is  or   is computed These lower bounds are accumulated for each table entry in an optimistic estimation of the distance between  and the transactions indexed by that entry The table entries are sorted in increasing order of their lower-bound distance and the hash buckets are read in this order to be compared with   If after reading a partition the distance between  and the  nearest neighbor found so far is smaller than the optimistic bound in the next table entry in the sorted order the search stops since none of the remaining entries may point to a closer transaction in the worst case see 1 for m ore d etails Although the signature table can be fast for nearest neighbor search queries it suffers from certain drawbacks First its performance is sensitive to various parameters number of vertical signature s critical mass activation threshold which are hard to determine a-priori and have to be tuned to achieve good performance Second it is appropriate for static data on which a clustering algorithm has to be applied in order to determine the vertical signatures The preprocessing cost is rather high and the index is sensitive to data updates which may change the correlations between the items and their optimal grouping Thus expensive periodic re-organization of the index is required in a dynamic environment Finally the SG\320table is not ef\336cient when the  77  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


memory resources are limited The experiments in 1 indicate that its performance drops fast as the space allocated for the memory-resident table decreases Moreover since the size of the table is hardwired at construction time it does not adapt to dynamic changes in memory resources In the next section we show how a hierarchical index can alleviate these problems 3 The Signature Tree SG\226tree A nice property of the signatures is that we can use the same representation i.e a bitmap for transactions and groups of transactions In other words assuming that 000 is a group of transactions we can characterize it by a signature which has 1 in a position iff the corresponding item exists in at least one transaction in 000  Formally De\336nition 5 Let 000 be a set of transactions The signature of 000 is de\223ned by 001 003 005 007 000 t 013 001 003 005 007 017 020 022 024 026 t 013 030 020 022 024 001 003 005 007 026 t 1 This property is employed by a simple yet ef\336cient hierarchical index for signa tures The SG\320tree or signature tree is a dynamic balanced tree similar to R\320tree 13 f or signature bitmaps Each node of the tree corresponds to a disk page using multipage nodes is a potential implementation and contains entries of the form 031 001 003 005 033 035 037    In a leaf node entry 001 003 005 is the signature of the transaction and 035 037  is a transaction-id  2 The signature of a directory node entry is the logical OR of all signatures in the node pointed by it and 035 037  is a pointer to this node In other words the signature of each entry is the signature of all transactions in the subtree pointed by it All nodes contain between  and  entries where  is the maximum capacity and      except from the root which may contain fewer entries Figure 2 shows an example of a signature tree The leaf entries contain the signatures and ids of nine transactions In this graphical example the maximum node capacity  is three and the signatures are six bits long In practice  is in the order of several tens and the length of the signatures in the order of several hundreds The SG\320tree is not useful only for restricted types of queries but can serve as a general-purpose index for set data In Section 4 we will describe how it can be used to evaluate similarity search queries Here we will discuss brie\337y how it can be used for simple queries like itemset containment queries  e.g 336nd all transactions containing items  and   Assuming that 0 013 2 4 033  033  033 6 033 9 033   this 2 The transaction-id although not necessary during nearest neighbor search may be useful when search on the tree is combined with other operations e.g there may be additional f eatures related to a transaction like customer class   100000 100010 T1 T2 001010 001100 T 3 T4 001100 T 5 110000 011000 T 8 T 9 100001 010001 T 6 T 7 100010 001110 110001 111000 101110 111001 level 0 level 1 level 2 Figure 2 Example of a signature tree query can be transformed to a signature 001 003 005 007  t 013  B    B and the tree is traversed in a depth-\336rst fashion to evaluate it The search algorithm follows entries whose signature contains 001 003 005 007  t  if the signature of an entry does not contain 001 003 005 007  t  no transaction indexed in the subtree below it can participate in the result Consider for example the tree of Figure 2 Since the 336rst entry of the root has 0 in the sixth position we know that no transaction indexed in the subtree under it can participate in the query result On the other hand the second entry should be ollowed and the ightmost node of the next level i.e level 1 is visited Only the 336rst entry of this node contains the query signature and it is followed Finally the query result is found in the third leaf node The qualifying entries are highlighted in the 336gure Observe that the number of visited pages in this case is optimal On the other hand assuming that we are looking for transactions containing item   multiple paths are traversed and a signi\336cant part of the tree is accessed Therefore the ef\336ciency of the tree increases if transactions with similar signatures are clustered together in the leaf nodes This observation holds for all query types including similarity search 3.1 Construction and updates The insertion algorithms of hierarchical access methods aim at a common goal to bring together indexed units which have small distance be tween them and separate well ones with large distance The B C 320tree uses the natural order of the indexed domain to solve the problem optimally Multidimensional access methods like the R\320tree employ heuristics to achieve this goal since there is no total ordering of objects in space that preserves spatial proximity 8 F i gure 3 s h o w s t he generi c i ns ert i o n a l gori t h m u s e d for these hierarchical access methods When a new entry 9 needs to be inserted the algorithm is called with parameters the root node and 9 and recursively traverses the tree in order to 336nd the most appropriate leaf node to accommodate 9  If the leaf node over\337ows a split algorithm divides the entries into two groups and oves one group to a newly created node A pointer to the new node is returned to the parent directory node and a new entry is created for it Splits are recursively propagated upwards The core components of the 003 E 001 9  037 function are  F G G 001 9 001 I  037  9 9 and 001 035 J 003 037  The 336rst chooses the most ap 78  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


function 000 002 004 006 b n Node 002 Entry 006  Node 004 f 016 000 n 002 022 023 006 026 if 002 is a leaf node then 026 insert 006 into 002  if 002 over\337ows then 002 006 027 002 022 023 006  004 f 016 000 n 034 002 036  return 002 006 027 002 022 023 006   else 026  002 is a directory node  002 006  n    022 022 004 006 004   n b 006 006 034 002  006 036  004 f 016 000 n 002 022 023 006  000 002 004 006 b n 034 002 006  n  006 036  if 004 f 016 000 n 002 022 023 006 1 2 002  016 016 then insert new entry pointing to 004 f 016 000 n 002 022 023 006 in 002  if 002 over\337ows then 002 006 027 002 022 023 006  004 f 016 000 n 034 002 036  return 002 006 027 002 022 023 006   return 002  016 016   Figure 3 Insertion in balanced tree indexes propriate entry of the current node 4 in order to insert 5 under it The second divides the entries of an over\337owed node into two groups Both functions should be tuned to maximize the ef\336ciency of the tree For the SG\320tree we need to de\336ne quality criteria based on which these functions operate The directory node entries of a good SG\320tree should have i a small area 3  which intuitively decreases the distance between the transactions in the subtrees indexed by them and ii small overlap between them if they are at the same level which intuitively discriminates as much as possible the branches of the sear ch process and maximizes the data that are pruned during search The 6 7 8 8 9 5 9     5 5 algorithm we used in our SG\320tree implementation can be described as follows When a entry 5 is to be inserted in the subtree under node 4 three cases are considered In the 336rst case only one entry 5 B D 4 contains the new entry 5 and it is directly chosen In the second case multiple entries contain 5  The algorithm chooses the one with the minimum area since this re\336nes the structure in analogy to choosing the smaller MBR that contains the new entry in R\320trees Finally the third case applies when no 5 B D 4 contains 5  The algorithm in this case picks the entry which requires the smallest area enlargement to index 5 under it or more formally the entry for which G I K K M 5 N 5 B O is the um Ties are broken by choosing the entry with the minimum area We also implemented another version of 6 7 8 8 9 5 9     5 5 that picks the entry which after extended has the minimum overlap incr ease with the rest of the entries in the same node Nevertheless through experimentation we found that the minimum area enlargement heuristic creates trees of the same quality at a much lower insertion cost 3 For simplicity we extend the function de\336nitions of Section 2.1 to apply on SG\320tree node entries e.g S b 006 T 034 006 036 U S b 006 T 034 006 W 004 000 X 036  For the split algorithm of the SG\320tree we consider several alternatives The 336rst one is based on the quadratic-split method of the R\320tree 13 W e 336 r s t pi ck t h e p ai r o f e nt ri es in the over\337owed node with the maximum distance We call these two entries seeds and assign them to two groups with initial signatures same as the seeds The rest of the entries are assigned to the group that requires the smallest signature area enlargement to include them Ties are broken by choosing the group with the minimum area In case of a new tie the group with the minimum number of entries is selected If at some point the cardinality of a group plus the number of remaining entries equals 6  the remaining entries are assigned to the group to avoid under\337ow in the new node We also consider two more approaches for splitting a node The 336rst is based on hierarchical clustering with group average 16  I n itially  a ll en tries a re co n s id ered as clusters Then clusters are hierarchically merged until only two remain these will form the new nodes after the split The next pair of clusters Y Z  N Z  _ to be merged is the one for which the average distance G I 9 M 5  b 9 I d N 5  b 9 I d O between pairs Y 5  N 5  _ of entries 5  D Z  N 5  D Z  in them is the smallest In order to avoid under-utilization of a node when a cluster grows above a threshold according to 6  the other clusters are immediately merged and the algorithm terminates We denote this method by d i 9 j k I   The last split policy is based on hierarchical clustering according to the minimum spanning tree  the next pair of clusters Y Z  N Z  _ to be merged is the one containing the closest pair of entries Y 5  N 5  _ N 5  D Z  N 5  D Z   We denote this method by n 9  9 j k I   In Section 5 we compare  9 j k I   d i 9 j k I  and n 9  9 j k I   Finally deletions in the SG\320tree are handled as in the R\320 tree if a leaf node under\337ows it is deleted the entries are put in a temporary buffer and reinserted to the tree This increases space utilization and the quality of the tree 3.2 Compression In many cases the signatures are very sparse i.e a single transaction contains only a small percentage of the possible items Only few bits are then set in the signatures and saving them as bitmaps would waste a lot of space In order to alleviate this problem we use a compression technique if a bitmap is too sparse we choose to encode the signature as a list of positions where the bits are set or else as a list of item-ids For example a 256-bit signature having only 10 1\325s would be encoded by a sequence of 10 characters indicating the positions of the 1\325s which occupy 10 bytes as opposed to 32 bytes needed to store the bitmap We also store an extra 337ag-byte which stores the number of 1\325s and also indicates that the next bytes contain the positions of 1\325s Other compression schemes can also be employed but it is  79  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


out of the scope of this paper to study their effectiveness 4 Query Processing In this section we describe how the branch-and-bound techniques for similarity sear ch on R\226tree-like structures can be adapted to perform search on the SG\226tree ef\036ciently We discuss 036rst the most common and simple types of similarity search and then some more complex queries 4.1 Similarity search Given a query transaction 000  we can identify two types of similarity search queries on a database 001 of transactions which constitute components of various data analysis tasks The 036rst is the similarity range query askingforall 002 004 001 within some distance 006 from 000  The second is the nearest neighbor search query  asking for the 007 closest 002 004 001 to 000  given a small constant 007  Both queries can be evaluated ef\036ciently if the database is indexed by an SG\226tree The search algorithms are adaptations of the equivalent ones that apply on an R\226tree and they take advantage of the coverage property of the entries in the directory nodes to derive distance bounds for the transactions indexed by them We 036rst con\036ne our discussion on the evaluation of nearest neighbor queries where 007 1 i.e simple nearest neighbor queries and then show how the same techniques can be extended for the other cases Figure 4 shows a depth-\036rst search algorithm for nearest neighbor queries on R\226trees 20  a dapt ed for t he S G 226t ree Two variables t t and t t 013 r 017 021 are initialized to 022 024 026 026 and 030  corresponding to the nearest neighbor found so far and its distance from 000  The branch-and-bound 031 033 022 022 017 036     algorithm is initially called for the root of the SG\226tree It recursively traverses the tree following the entries that are most likely to contain the query result When visiting a directory node the entries 036 are sorted according to 031 r    000  036  017 r  0  which provides a lower optimistic bound for the nearest neighbor of 000 in the subtree indexed by 036  Intuitively by visiting the subtrees in this order the chances of 036nding early the result are maximized Ties between entries having the same lower bound are broken by picking 036rst the one with the minimum area This secondary sorting key is due to the fact that among several subtrees with the same number of common items with 000 the one with the smallest area is more likely to index an entry with exactly these common items i.e the optimistic nearest neighbor In other words given two groups of transactions 1 3  1 5  where 6 1 3 6 9 6 1 5 6 and   036   017 r   1 3 0 0    036   017 r   1 5 0 0  and an itemset B that could be included in both 1 3 and 1 5 i.e both 017 r   1 30 and 017 r   1 5 0 cover 017 r   B 0  probabilistically the group with the smallest area i.e 1 3 smore likely to contain B  4 function C 001 002 002 005 007 t 013 r 017 Node 002  020  021 021 int 021 021 024 025 005 027  030 if 002 is a directory node then 030 sort entries 007 in 002 in ascending order of C 025 033 033 036 020  007  005 025    break ties by placing 336rst the entries with the smallest area for each entry 007 in this order do  recursive call  1 if C 025 033 033 036 020  007  005 025    021 021 024 025 005 027 then C 001 002 002 005 007 t 013 r 017  007  1 027 013  020  021 021  021 021 024 025 005 027  else break for loop  no need to visit other subtrees  3 else  002 is a leaf node  for each entry 007 in 002 do 2 if C 025 005 027 036 020  007  005 025    021 021 024 025 005 027 then  new NN found  021 021  007  021 021 024 025 005 027  C 025 005 027 036 020  007  005 025    3 Figure 4 A depth-\336 rst search algorithm for NN queries The nodes under the entries are visited in this order If the optimistic bound for some subtree is greater than the distance of the nearest neighbor found so far search is not required for this subtree and the emaining ones in the order since they may not contain a closer neighbor to the one already found When a leaf node is visited during the search process the distances between 000 and all its entries are computed and the bounds t t and t t 013 r 017 021 are updated if a closer neighbor is found The search algorithm of Figure 4 is appropriate for 036nding one nearest neighbor of 000  It can be easily adapted for 036nding all nearest neighbors with the same minimum distance from 000  by maintaining a set of current nearest neighbors instead of a single variable t t  and changing the predicates in lines 1 and 2 of the algorithm to 221 8 222 In the general problem where the 007 nearest neighbors are required  007 NN search the parameter t t is replaced by a priority queue of size 007  organizing the 007 nearest neighbors found so far and t t 013 r 017 021 bound corresponds to the 036rst element of the queue i.e the one with the largest distance among the 007 NN found so far The algorithm of Figure 4 can also be used to evaluate similarity range queries In this case the t t 013 r 017 021 bound is replaced by the 036xed query parameter 006 and ll transactions within this distance from 000 are retrieved The directory entries with 031 r    000  036  017 r  0  006 are pruned as before 036ltering out large parts of the data early Finally we need to mention that the 031 033 022 022 017 036     algorithm of Figure 4 is in fact sub-optimal for nearest neighbor search on the SG\226tree An optimal  033 022 022 017 036     algorithm in terms of node accesses follows a best-\336rst search paradigm 15 and e mpl o ys a p ri ori t y queue T hi s queue organizes  036  031 r    000  036  017 r  0  tuples for directory 4   has higher density than  B and thus higher probability to include C   80  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


 81 Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


large itemset I and the cardinality D For example a dataset with 200,000 transactions of mean size 10 and large itemsets of mean size 6 is denoted by T10.I6.D200K By tuning these parameters we were abl e to generate a wide range of datasets with various characteristics We also experimented with a real categorical dataset from the UCI KDD Archive 22 The d ataset contains census data extracted from the 1994 and 1995 current population surveys conducted by the U.S Census Bureau Each tuple corresponds to an individual and includes demographic and employment related info rmation After removing some numerical attributes and clean ing the data e.g missing values were replaced by an extra special value for each attribute we ended up with 36 categorical attributes the domain sizes of which vary from 2 to 53 the total number of values is 525 he data are split into two datasets with 200K and 100K tuples respectively We indexed the 336rst dataset which we denote as CENSUS and we used random samples from the second for querying it 5.2 Comparison between split policies In the 336rst experiment we compare the three SG\320tree split policies described in 3.1 We generated three uncompressed SG\320trees for the CENSUS dataset using 000 001 003 005 007 t  n f 001 003 005 007 t and 017 001\t 001 003 005 007 t  respectively Table 1 compares the characteristics of the resulting trees and shows their relative performance averaged on 100 nearest-neighbor queries Table 1 Comparison of the three split policies comparison metric 021 022 023 025 027 031 032 034 022 023 025 027 031 037 022 031 022 023 025 027 031 average  021  034    at level 1 90 73 74 average  021  034    at level 2 210 158 154 average  021  034    at level 3 458 325 348 insertion cost msec 0.331 0.655 0.645  of data accessed 15.79 4.78 5.72 CPU time msec 119 34.6 41.8 I/O s 862 266 323 All three trees have 4 levels The entries in level 0 leaf level have 336xed area 36 since all data tuples have 36 values The 336rst three rows of Table 1 show the average area of the entries at levels 1,2 and 3 root This can be considered as a quality metric for the three split policies the smaller the average area of the entries at the intermediate levels the better the quality of the clustering The n f 001 003 005 007 t and 017 001 t 001 003 005 007 t policies construct much better trees than 000 001 003 005 007 t  and this can be validated from the last three rows of the table which show the average pruning in terms of data accessed the average CPU cost and average number of node accesses at nearest neighbor search queries On the other hand 000 001 003 005 007 t has the lowest average insertion cost and tree construction time Experimentation with other datasets shows similar results In the sequel we use n f 001 003 005 007 t as the standard split policy for the SG\320tree since it achieves the best quality of the three at an acceptable cost 5.3 NN search on synthetic data We compared the performance of SG\320table and SG\320tree on nearest neighbor search by generating a series of synthetic datasets and using the same itemsets and parameters to also generate a number of queries for each dataset Figures 5 through 12 show the relative performance of the methods for various paramete r settings For each experimental instance the results we re averaged over 100 queries Figures 5 7 9 11 and 12 show in combined diagrams the pruning ef\336ciency bars and computational cost lines of the two methods The pruning ef\336ciency is measured in terms of the transactions accessed and compared with the query transaction percentage Figures 6 8 and 10 compare the number of random I/Os on the two indexes for three of the 336ve experimental instances Figures 5 and 6 show the performance of the indexes when the size of itemsets is 336xed I=6 the size of the dataset is 200K and the size of the transactions T varies When T is small both indexes have similar performance but as T increases the SG\320tree starts to slightly outperform the SG\320table managing to prune more transactions Especially the I/O cost differ ence is high for large values of T since in that case the distance of the nearest neighbor usually increases and the contents of many entries of the SG\320table need to be visited Figures 7 and 8 show the relative costs for T=30 as the size of the large itemsets I in creases This increase generates datasets where the tran sactions are better lustered having smaller average distance between them and favors both structures Observe that the relative performance between them increases and th e SG\320tree becomes signi\336cantly faster than the SG\320table when both T and I are large In the third experimental instance Figures 9 and 10 we 336x the ratio I/T to 0.6 and incr ease the transaction size The rationale is to test the robustness of the indexing methods to the dimensionality of the problem when the data skew remains constant Clearly the SG\320tree is obust to the transaction size whereas the SG\320table fails to index well large transactions even if they cont ain well-clustered data This observation is also validated at the comparison of the structures for real categorical datasets of high dimensionality see Section 5.4 We also tested the robustness of the two structures to the database size by 336xing T 10 and I=6 two parameter values for which the SG\320tabl e performs well and increasing the dataset cardinality D Figure 11 shows that the relative pruning ef\336ciency of the SG\320tree increases with the database size The I/O cost diagram is omitted since it  82  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


 0 5 10 15 20 25 30 35 40 10 15 20 25 30 average number of items in transactions T I=6 D=200K  o f d ata p rocessed 0 50 100 150 200 250 300 350 400 450 500 ti m e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 5 Pruning and CPU time varying T 0 2000 4000 6000 8000 10000 12000 14000 10 15 20 25 30 average number of items in transactions T I=6 D=200K number o f r andom I  Os SG-table SG-tree Figure 6 Random I/Os varying T 0 5 10 15 20 25 30 35 40 6121824 average length of large itemsets I T=30 D=200K  o f d ata p rocessed 0 50 100 150 200 250 300 350 400 450 500 ti m e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 7 Pruning and CPU time varying I 0 2000 4000 6000 8000 10000 12000 14000 6121824 average length of large itemsets I T=30 D=200K numbe r of ra ndom I  O s SG-table SG-tree Figure 8 Random I/Os varying I 0 2 4 6 8 10 12 14 16 T=10,I=6 T=20,I=12 T=30,I=18 T=40,I=24 T=50,I=30 Varying T and I I/T=0.6 D=200K  o f d ata p rocessed 0 20 40 60 80 100 120 140 160 tim e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 9 Pruning and CPU time 336xed I/T 0 200 400 600 800 1000 1200 1400 1600 1800 2000 T=10,I=6 I=12 T=30,I=18 I=24 T=50,I=30 Varying T and I I/T=0.6 D=200K numbe r of ra ndom I  Os SG-table SG-tree Figure 10 Random I/Os 336xed I/T 0 1 2 3 4 5 6 7 100 00 300 400 500 Data set cardinality T=10 I=6  o f d ata p rocessed 0 10 20 30 40 50 60 70 80 90 ti m e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 11 Pruning and CPU time varying D 0 10 20 30 40 50 60 0 1 to 3 4 to 10 11 to 20 20 distance of nearest neighbor T30.I18.D200K  o f d at a p rocessed 0 100 200 300 400 500 600 700 800 time m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 12 Pruning and CPU time var 000 000 003 005 007 t  83  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


shows a pattern similar to the CPU cost as in the previous experiments During the experiments we observed that queries having a close nearest neighbor were processed fast using both structures whereas for cases with distant neighbors the SG\226tree was signi\036cantly faster than the SG\226table We validated this observation by running 1000 queries on the T30.I18.D200K dataset and averaging the query costs for various distance ranges of the nearest neighbor Figure 12 shows the average pruning an d CPU cost for 036ve distance ranges When the distance is small search is fast for both methods actually for distances in the range 1\2263 the SG\226 table outperforms the SG\226tree However the distant cases are handled much faster by the SG\226tree showing that this access method is more robust to 221outlier\222 queries As a general conclusion from this set of experiments the SG\226tree is a more ef\036cient and robust access method than the SG\226table in addition to its other inherent advantages dynamic data handling independence to hard-wired constants In the next subsection we compare the indexes for other query types on both synthetic and real data 5.4 Real data nd other queries Figures 13 and 14 show the performance of the indexes for 000 NN queries on the T30.I18.D200K synthetic dataset and the CENSUS dataset respectively for various values of 000  The results for each experimental instance were averaged over 100 queries In both 036gures for small to medium values of 000 the SG\226tree is signi\036cantly faster than the SG\226 table When 000 is large  001 003 005 005 005  the fraction of the data that need to be visited becomes too large for the indexes to be useful This is due to the fact that the search space becomes less appropriate for search For example when 000 t 003 005 005 005 005 we observed that the average distance of the 000 th neighbor is very large 31.81 for T30.I18.D200K and 18.06 for CENSUS and very close to the average distance of all transactions from f  This is due to the 221dimensionality curse\222 effect 3 o ften o b s erv e d i n h ig h d i men s io n a l search problems Observe that the SG\226tree is less sensitive to this effect since its performance degenerates at a smaller pace especially for the real dataset We also compared the indexes for similarity range queries Figures 15 and 16 The same datasets and queries as before are used and the distance threshold from the query varies from 2 to 10 For r t 020  the SG\226table outperforms the SG\226tree on the synthetic dataset In all other cases the tree is much faster Observe that on the real dataset in particular for both 000 NN queries and range queries the performance difference quite large in favor of the tree This indicates that the structure can perform very well in real life cases  0 10 20 30 40 50 60 70 80 90 100 1 10 100 1000 10000 k-nn search varying k T30.I18.D200K  o f d ata p rocessed 0 200 400 600 800 1000 1200 1400 time\(msec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 13 021 NN queries T30.I18.D200K 0 10 20 30 40 50 60 70 80 90 100 1 10 100 1000 10000 k-nn search varying k CENSUS  o f d ata p rocessed 0 100 200 300 400 500 600 tim e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 14 022 NN queries CENSUS 5.5 Dynamic data changes In this experiment we compare the structures simulating a case where the nature of the data changes dynamically We generated a synthetic dataset T10.I6.D100K and built an SG\226table and SG\226tree for it We then gradually updated the structures by inserting batches of 100K transactions each with the same characteristics i.e T=10 I=6 but putting different seeds to the random generator i.e the large itemsets used were different for each batch We ran nearest neighbor queries on the two structures after each insertion phase The queries for phase 023 after batch 023 has been inserted 024 026 023 026 032  are generated as follows For each query i a random number 033 from 1 to 023 is chosen and ii the generator parameters i.e large itemsets for batch 033 are used to produce the query For example a query for the phase where the dataset contains 300K data is generated using randomly one of the generators of batches 1 2 or 3 Figure 17 shows the average pruning ef\036ciency and CPU time of the two structures Initially both have similar performance but as more data with different characteristics are inserted into the structures the performance of the SG\226table degenerates since it is optimized for the 036rst 100K data  84  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


 0 5 10 15 20 25 30 35 40 246810 similarity range queries varying epsilon T30.I18.D200K  o f d at a p r o cessed 0 50 100 150 200 250 300 350 400 tim e m s e c  SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 15 Range queries T30.I18.D200K 0 10 20 30 40 50 60 70 80 246810 similarity range queries varying epsilon CENSUS  o f d ata p r o cessed 0 50 100 150 200 250 300 350 400 ti me\(msec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 16 Range queries CENSUS On the other hand the SG\226tree is robust to updates and exhibits very good query performance since each batch contains skewed data generated from a different collection of large itemsets 6 Conclusions and Future Work We presented a hierarchical indexing method for similarity search in sets and categorical data The SG\226tree is a disk-based height-balanced tr ee that organizes 036xed-length bitmaps and is appropriate for various query types We have shown how several branch-and-bound methods which apply on R\226tree-like structures can be adapted for ef\036cient similarity search on the SG\226tree Extensive experimental evaluation has shown that the SG\226tree is in most cases much faster than the SG\226table a previous hash-based index The advantages of the SG\226tree can be summarized as follows 000 It is ef\036cient and robust to various data types both categorical and set data and characteristics cardinality density dimensionality It is a versatile structure that can be used for several query types 000 The tree is dynamically adapted to updates and re0 2 4 6 8 10 12 100 200 300 400 500 Dataset cardinality T=10 I=6  o f d ata p rocessed 0 20 40 60 80 100 120 140 160 180 200 tim e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 17 NN search after dynamic updates quires no preprocessing of the data Thus it can be useful for analyzing data which change dynamically over time 001 It relies on no hardwired constants and requires no tuning using a-priori de\036ned parameters 001 It is a disk-based paginated data structure so it can operate with limited memory resources and dynamically changing memory resources Caching policies previously used for the B 002 226tree and the R\226tree can be seamlessly applied on this structure There are several directions for extending the current work In our study we used hamming distance as the similarity metric However the SG\226tree can also be de\036ned tuned and searched for other set theoretic similarity metrics For example if the Jaccar d coef\036cient is used the lower distance bound in fact the upper similarity bound for nearest neighbor search can be de\036ned by 003 005 007 b n f 016 007 020 021 023 024 026 030 003 005 007 b n f 026  We plan to test the effectiveness of the structure using alternative metrics Another direction or future work is to study methods for bulk-loading SG\226trees instead of inserting the data oneby-one We can adapt categor ical clustering algorithms 12 for t hi s purpos e Anot her a pproach i s t o s o rt t h e transactions using gray codes as key in analogy to using space-\036lling curves for bulk-loading multidimensional data to an R\226tree 17  A lternati v ely  hashing t echniques can be used to group similar signatures together The resulting 221globally-optimized\222 tree could have much better quality characteristics while being built faster In a reverse direction we can investigate whether the SG\226tree can be used for clustering large dynamic collections of set and categorical data The cost of existing methods is at least 035 n   026 and the tree could be used to derive good clusters much faster e.g by merging the leaf nodes using their signatures as guides Finally we plan to empirically test the ef\036ciency of the tree to the query types discussed in Section 4.2 In  85  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


addition for some data types search can be further optimized For example if the indexed categorical data have 223xed-dimensionality 000 we know that the area of each indexed signature is 223xed to 000  We can use this property to derive stricter lower bounds for the directory node entries 001  instead of the rather relaxed 002 004 006 006 t 013 r 001 020 022 004 025 027 For this example a better bound is 002 004 006 006 t 013 r 001 020 022 004 025 027 033 t 000 037    t 001 020 022 004 025 r 013 027 027  We plan to study such search optimizations using domain properties or statistics from the indexed data References  C  C  A ggarw al  J  L  W ol f and P  S  Y u A N e w Method for Similarity Indexing of Market Basket Data SIGMOD Conference  pages 407\205418 1999  R  A gra w al and R  S ri kant  F as t A l gori t h ms for M i n ing Association Rules in Large Databases VLDB Conference  pages 487\205499 1994  K  S  B e y er  J  G ol ds t e i n  R  R amakri s hnan and U Shaft When Is 215Nearest Neighbor\216 Meaningful International Conference on Database Theory  pages 217\205235 1999  T  B ri nkhof f H.-P  K ri e g el  a nd B  S e e g er  E f 223 ci ent Processing of Spatial Joins Using R-Trees SIGMOD Conference  pages 237\205246 1993  A  C orral  Y  Manol opoul os  Y  T heodori d i s  a nd M Vassilakopoulos Closest Pair Queries in Spatial Databases SIGMOD Conference  pages 189\205200 2000  A  P  d e V ries N  M amoulis N  N es a nd M K e r sten Ef\223cient k-NN Search on Vertically Decomposed Data SIGMOD Conference  pages 322\205333 2002  U  D eppisch S-T r ee A D ynamic B alanced Signature Index for Of\223ce Retrieval ACM SIGIR Conference  pages 77\20587 1986  V  G aede a nd O G 250 unther Multidimensional Access Methods ACM Computing Surveys  30\(2\170\205231 1998  V  G ant i  J  Gehrk e  a nd R  R a makri s hnan C A C T US 205 clustering categorical data using summaries ACM SIGKDD Conference on Knowledge Discovery and Data mining  pages 73\20583 1999  D Gi bs on J  M Kl ei nber g  a nd P  R a gha v a n C l us tering Categorical Data An Approach Based on Dynamical Systems VLDB Conference  pages 311\205322 1998  A Gi oni s  D Gunopul os  a nd N K oudas  Ef 223 c i e nt and Tunable Similar Set Retrieval SIGMOD Conference  2001  S  Guha R  R as t ogi  a nd K S h i m  R OC K A R obust Clustering Algorithm for Categorical Attributes International Conference on Data Engineering  pages 512\205521 1999  A Gut t m an R T rees  A Dynami c I nde x S t r uct u re for Spatial Searching SIGMOD Conference  pages 47\205 57 1984  S  Hel m er and G  M oerk ot t e  A S t udy of F our Inde x Structures for Set-Valued Attributes of Low Cardinality Technical Report University of Mannheim  number 2/99 1999  G R Hjaltason a nd H Samet Distance Bro w sing in Spatial Databases TODS  24\(2\265\205318 1999  A K J a i n and R  C  D ubes  Algorithms for Clustering Data  Prentice-Hall 1988  I Kamel a nd C  F a louts o s  Hilbert R tree An Improved R-tree using Fractals VLDB Conference  pages 500\205509 1994  F  K o rn N  S i d i r opoul os  C  F al out s o s  E S i e g el  a nd Z Protopapas Fast Nearest Neighbor Search in Medical Image Databases VLDB Conference  pages 215\205 226 1996  N K oudas a nd K C  S e vci k  H i g h D i m ens i onal S i m i larity Joins Algorithms and Performance Evaluation International Conference on Data Engineering  pages 466\205475 1998  N R ous s opoul os  S  K el l e y  and F  V i n cent  Neares t Neighbor Queries SIGMOD Conference  pages 71\205 79 1995  Y  S a kurai  M  Y os hi ka w a  S  U emura and H  K oj i m a The A-tree An Index Structure for High-Dimensional Spaces Using Relative Approximation VLDB Conference  pages 516\205526 2000  The U C I KDD Archi v e ht t p    kdd.i c s  uci  edu 23 R W e b e r  H.-J S ch ek  a n d S Blo tt A Q u a n titative Analysis and e Study for SimilaritySearch Methods in High-Dimensional Spaces VLDB Conference  pages 194\205205 1998  86  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


