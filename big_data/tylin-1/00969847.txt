Quantification And Granulation Lawrence J Mazlack Computer Science University of Cincinnati Cincinnati OH 45221-0030 mazlack@uc.edu Abstract Data mining holds the promise of extracting un suspected information from very large databases Methods have been developed to build association rules from categorical data However often many fme grained rules are generated. Additionally much real data is not only categorical; it is quantitative. In forming association rules quantitative values are often r&d to categorical values; this may overly simplify results The concern 
of this work is considering how fme grained rules might be aggregated and the role that non categorical data might have It appears that soft com puting techmques may be useful Keywords Association rules quantitative, granulation, soft com puting, conceptual hierarchies 1 Introduction Data mining is an advanced tool for the manage ment of large masses of data. Data mining is the proc ess of secondury analysis of large databases Data mn ing is secondary analysis because the data were not collected to answer questions now posed The 
data is examined in an attempt to discouver if there are pat terns beyond those that were hypothesized before the data was collected. For example perhaps we are at looking at long distance telephone call records The records were originally collected for billing Secondar ily, they can be examined to recognize patterns involv ing such things as call length time-of-day, customer calling plan from where-to-where etc Broadly speaking, data mining is part of the general data exploration activity The consideration of repeti tively presented data such 
as in a database is called knowledge discouvery in databases KDD KDD in cludes traditional data exploration techniques such as classical statistical analysis and statistical learning such as Baysian learning as well as a variety of other methods Some methods are too computationally complex to apply to an entire large data set Data mining methods often include elements of ar tificial intelligence such as learning andor methods to help handle impreciseness The most common data mining result representations are rules and 
ussocia tions For example Rules are the older representation. In data mining they are often formed from a graph built by some form of inductive learning Associations are often called association ndes While this terminology may be un fortunately confused with the different \223rules\224 result it is what we have to work with Associations are built by directly examining tabular data 2 Association Rules The problem of finding association rules was intro duced by Agrawal  11 Association rules meet a neces sardata mining subgoal of having efficient data struc 
tures and algorithms Their algorithms also have the advantage of being linearly upwardly scaleable Once processing begins discouvering association rules often follows methods based on Apriori 2 A number of other authors have considered efficiently de veloping association rules 21 29 34 Finding association rules is somewhat of an unsu pervised discouvery process It may not be 223pure\224 un supervised learning as there is often a predefined hierar chical concept hierarchy structure that is applied to pre cluster the data this would be an aspect of supervised 
discouvery. For example, all brands of beer might be grouped into a single class called 221\221Beer.\224 Predefmed concept hierarchies may not serve the purposes of data analysis well. Perhaps, implicit in the data, there may be more useful concept hierarchies that go undiscouvered because of pre-processing assump tions As with much real world data the clusters that concept hierarchies represent can possibly be best formed by fuzzy sets; or alternatively, rough sets It is a speculation of this work that it may be pos sible to increase the granularity of discouvered associa tion rules either through \(a\using discouvered concept hierarchies b soft clustering the rules themselves or 
c\increasing the granularity of the data before forming association rules Association Rules represent positive associations between attributes. Most commonly the rules are de veloped from data that is expressed as coIumns in a 011 Boolean categorical matrix For example consider the transactions shown in Figure I Parts of this work were performed while the author was a visiting at BISC, Computer Science Division EECS Department University of California Berkeley 0-7803-7087-2/01/$10.00 0 2001 412 


transaction t tl t2 t2 t2 t2 t2 t3 t3 item quantity chips 1 Miller 1 chips 2 mustard 1 sausage 3 Coke 5 Tuberg 1 Miller 1 Tuberg 2 413 t t2 t3 Chips ard age Drink Beer 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 tl t2 t3 t4 t5 t7 to t9 tl0 t6 Chips ard age Drink Beer 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 


This is possibly an undesirable oversimplification In contrast to Boolean attributes, the values of quanti tative attributes corresponding to other records can be distinct i.e one quantitative value for a quantitative attribute may closely associate with another attribute while a different quantitative value may not There are several kinds of information that might be obscured a differences of behavior due to diffixing quantities and b trends that might be recognizable if data magnitudes were considered Several strategies have been pursued to deal with scalar, non-categorical data when mining data and form ing association rules The results are usually called quantitative association rules Some methods reduce data magnitudes others try to cluster by finding opti mized ranges in a decision tree Dequantification Discretization Reducing Data Magnitudes If the quantitative rule problem can be usefully mapped into the Boolean categorical rules problem any algorithm for finding categorical association rules can be used to find association rules Srikant discussed mining association rules over quantitative and categorical attributes. They dealt with quantitative attributes by fine-partitioning the values of the attribute and then combining adjacent partitions as necessary For example a rule might contain a quantified age range Age  30,39 the re sulting rule might be Age 30,39  and 44arried Yes  Their method first determines the number of parti tions for each quantitative attribute and then maps all possible values for each attribute. They partition val ues into equal intervals Their method is illustrated by Figures 3 and 4 Fipre 3 shows an example table with three non-key attributes X and Z are quantitative attributes while Y is a categorical attribute with two enumerated values Yes No The table in Fipre 4 contains as many Boolean categorical fields as the number of attribute values/i@ervals for each quantitative or categorical attribute To reduce the count of values to consider values have been partitioned into intervals Then the value of any Boolean categorical field that corresponds to a attribute,v would be 2231\224 if the attribute had value v in the original record and 2230\224 otherwise A rule that is present in the table in Figure 4 is NumCars 2 X 20..29 and fl=Yes  Z:2 Figure 3 Exam Figure 4 Example table recast into categorical data Generally this type of method can be summarized Determine the number of partitions for each quantita tive attribute Here lies a decision for each attribute partition or not And if partitioning is decided what should be the number of partitions Map values to partitions After this point only the mapped values are used Find support for each value of all attributes. To avoid minimum support problems, combine adjacent values as long as their support is less than the minimum support threshold Find all sets of items whose support is greater than the minimum support These are the fiepent itemsets Generate association rules from the frequent item sets This simple mapping approach leads to two problems Support If the number of valuedintervals for an at tribute is large the support for any particular value can be low That means a large number of intervals can cause some rules involving this attribute may not satisfy minimum support Confidence As the size of the intervals becomes larger information loss can be encountered These two concerns are adversely complementary if the intervals are too large some rules may not have minimum confidence if they are too small some rules may not have minimum support. To break this chain when processing each particular valuehterval all pos sible ranges over that value/interval need to be con sidered Unfortunately this approach is overly computa tionally complex. Additionally while it may be a way of dequantifyins values it will generate many fine grained rules Starting with Srikant a number of workers have addrased the question of optimizing interval design Often but not always the results have been called optimizedassociation rules I31 141  26 331 71 A satisfactory resolution has not yet been reached It may be that dequantification into intervals is not a viable approach to quantitative association rules Some workers have focused on trying to discouver the more interesting rules from the numerous rules generated from dequantified association rules 3 1  32  39 I71  20 30 35 27 Some of the interest approaches work better than others They all struggle with knowledge acquisition issues None of them have been outstandingly successful  de table 414 


Directly Dealing With Magnitudes In Quantified Association Rules Cai 8 considers transactions with quantities as supporting \223weighted\224 rules. Cai wanted to balance be tween weight and support Wang  also considered weighted association rules Wang first finds association rules by converting values to Boolean categorical val ues, then builds the weights onto the association rules Aumann 6 was concerned with the distribution of quantitative attributes They looked for events that differ significantly from a statistical norm and were then considered to be interesting. Their approach is to look for a subset and its mean or median variance andcompare it to the mean of the whole. This might identify an interesting population subset Yao 42 presented another statistical approach Soft Methods There is a considerable history applying either fuzzy or rough set techniques to databases. To name a few approaches, there are databases of fuzzy values fuzzy query rules and rough set partitioning Both fuzzy and rough sets have been used in various data mining efforts Our focus is on the relatively unex plored area of rule aggregation the lightly explored clustering antecedents andor consequents and the role of quantification There have been a number of fuzzy learning algo rithms for inducing rules from sets Considerable work has also been done on decision trees. Both concerns are reviewed in Mazlack 24 Chan 191 presents an algorithm for increasing data granularity that eliminates the need for user-supplied thresholds for support and confidence and to find nega tive as well as positive association rules. Using fimy set theory, linguistic terms are used to find the degree to which they characterize records in the database Zhang 43 extended the equi-depth \(equal sized\par tition algorithm to mine fuzzy quantitative association rules. They considered Y  r where an item z,v represents either a crisp value, an interval if numeri cal or a fuzzy term and a is an attribute with value v If v is fuzzy the item is fuzzy. They used a minimum support for each attribute Kuok  181 mines fuzzy association rules to avoid either ignoring or overemphasizing the elements near the boundaries in the mining process A user-supplied threshold is used to test each side of the rule as to be ing \223satisfied.\224 Other efforts at association rules with fuzzy antece dents and consequents include Au  5 Fu  1 11 Lee  191 Au 4 and Chan 9 are working on the same problem Fu\222s work is closely related to 18 Du lo has a somewhat different approach to hitied ranges Somewhat related is work on incomplete data  3 The difficulty with these hzzy methods is that they are overly complex in the face of very large data sets However the potential of soft computing is just be ginning to be applied to data mining 4 Reducing Association Rules Generated Value Aggregation Another approach to quantification that might prove to be useful would be to granulate values, then apply quantified association rule techniques For exarn ple if academic grades are recorded from 100 to 0 they often are granulized into A B C   This approach would have the substantial advantage that it would reduce the count of rules generated How ever the problem is identifying what the extents of the appropriate granules Possibly the mountain method 41 J might be applied to form the granules. The moun tain method may also me useful in clustering associa tion rules together Granularity, Concept Hierarchies An unanswered research question is Granularity It is unclear whether concept \(hierarchy\trees should be part of the initial formation of the transaction matrix categorical or non-categorical or dealt with later. For example, should all beer brands be grouped together as a single class under all cases Or should they be pre sented separately Consider Figure I shown earlier. Should Miller and Tuberg be grouped together into a single class called beer Should all beverages be grouped together i.e from example I should Miller Tuberg and Coke be grouped together Can items be grouped together de pending on context i.e perhaps sometimes beers should be in one group and Coke in another and sometimes they should all be grouped together \(e.g picnic supplies Is there some way of computationally deciding what should be grouped together Extending the example further Consider the case where we have more beer brands say Budwiser Miller Sam Adams and Tuberg Most regular beer drinkers would agree that there is a substantial taste difference between the four of them There may also be price differentiation Should Budwiser, Miller Sam Adams and Tuberg be grouped into a single category; or Should each be grouped separately or Should Budwiser  Miller form one group while Adams  Tuberg form another group Currently concept hierarchy descriptions are done by a human expert This is not entirely satisfactory because \(a potentially useful groups may go unrecog nized and b\the expert may make an error Granules 41 5 


Overlapping Concept Hierarchies Concept hierarchies group similar attributes to gether For example, various types of wine might be all be placed into to concept of 223wine.\224 For example all of the following are wines If the transactions were preinium wines E Carminet, Chalone, Caneros white wines E Chardonnay red wine E Burgundy Pinot Noir wine type E alcohol alcohol-jiree basic wines E Meier, Gallo Meier Carminet store Jean alcohol alcohol Chalone green brand D\222Arc cottage free free Gallo Pinot Almadin Carneros grapes cheddar brie cheese Burgundy Chardonnay Burgundy Noir Chardonnay Chardonnay 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 Figure 5 Wine and cheese transactions for items of diflering perceived quality A human observer might not be able to easily re ognize that the premium, alcohol wines can be grouped together 22 They are all associated with both green grapes and brie \(a premium cheese\Similarly the basic alcohol wines can be grouped together as they are all associated with basic cheddar cheese However if a predefined hierarchy was used to group all wines together, the association between type of wine and type of cheese would not be discouvered and, the Figure 6 would result This would result in loosing the information that premium wines are related to the purchase of brie cheese and green grapes Why might this be important to a store Well if the store stopped carrying green grapes or brie you may also lose your premium wine customers store Jean brand D\222Arc cottage i~~~~~s cheddar brie cheese wine 1 0 1 0 1 1 0 1 0 1 I1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 lo 1 0 1 1 10 1 0 0 1 Figure 6 All wine grouped together green grapes cheese wine 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Figure 7 All cheese and wine grouped together It gets even worse if all of the cheese varieties are grouped together as well as in Figure 7 Now the analysis would tell us that any cheese and any wine are associated. These results are clearly over-generalized Now, the analysis would tell us that any cheese and any wine are associated. These results are clearly over generalized Ahierarchy such as items E food E wine can be many levels deep. There are two ways of establishing hierarchies a have a human predefme them or b learn them There are a number of different ways that they can be learned As part of a rough-set based approach Han 15 used crisp concept hierarchies provided by the user before the initiation of data mining Alternatively Mazlack 22 developed and tested a methodology to learn useful concept hierarchies. Potentially, the same methodology could be applied here 5 Link Analysis Link analysis is an extension of simple association rules It recognizes sequential patterns. The goal is to detect when one set of items is followed by another set of items over a specified time period. Each set of items is distinct from the other In a shopping example each set or transaction\represents a separate visit to a store Some of the same issues of granularity and quanti fication that occurs in simple association rules also appear in sequence rules For example How are con cept hierarchies formed How are quantitative values handled A comparative question is Are the dferences in handling quant@cation and granulation when form ing Link Rules as opposed to simple association rules 6 Summary Data mining holds the promise of extracting new in formation from very large databases One difficulty of 416 


data mining is that traditional ways of discouvening new information from data are largely drawn from clas sic machine learning methods The work involved in these methods increases geometrically with the amount of data considered Consequentially, the use of these methods is problematic in very large databases Association rules are a linearly complex, user under standable way of doing data mining The results are particularly useful when it is important that both the rules and the methodology behind the rules\222 develop ment are understandable There are several open questions across all types of association rules One of the most immediately en gaging is how to identify interesting rules The heuris tic most commonly used is to identify rules that OCCUT more often than others or rules that occur above a specific threshold However it might be that there are high value rules that would not be recognized when using a threshold A more fundamental question is ds termining whether or not an association is casual Bibliography I R Agrawal, T hielinski A Swami 223Mining Association Rules Between Sets Of Items In Large Databases,\224 Proceedings of ACM SIGMOD Conference on Management of Data SIGMOD 93 P Buneman S Jajodia eds Washington DC May pp 207 216, 1993 2 R Agnwal R Srikant, \223Fast Algorithms for Mining Asso ciation Rules VLDB Proceedings pp 487-499, 1994 3 K Ali S Manganaris R Srikant 221\221Partial Classification Us ing Association Rules,\224 Knowledge Discovey In Databases And Data Mining Proceedings Newport Beach Ca August 1997 4 W.H Au K.C.C Chan 223An Effective Algorithm For Dis covering Fuzzy Rules In Relational Databases,\224 PAKDD-98 Pro ceedings Melbourne, pp 1314-1319, 1998 5 W.H Au K.C.C Chan 223Farm A Data Mining System For Discovering Fuzzy Association Rules,\224 IEEE International Fuzzy Systems Conference Proceedings Seoul, August pp 1217-1222 1999 6 Y Aumann Y Lindell 223A Statistical Theory for Quantita tive Association Rules,\224 ACM SIGKDD Proceedings San Diego 7 S Brin R Rastogi K Shim 223Mining Optimized Gain Rules For Numeric Attributes,\224 ACM SIGKDD Proceedings San Diego pp 135-144, 1999 8 C.H Cai A W.C Fu C.H Cheng WW Kwong 223Mining Association Rules With Weighted Items,\224 Proceedings of IEEE Database And Engineering Applications, July pp 68-77, 1999 9 K.C.C Chan, W Au 223Mining Fuzzy Association Rules,\224 Proceedings of the Sixth International Conference on Information and Knowledge Management pp 209-21 5,1997 IO X.Du Z Liu N Ishii, \223Mining Association Rules On Re lated Numeric Attributes,\224 PAKDD-99 Third Pacific-Asia Confer ence Beijing pp 44-53, 1999 Ill A.W Fu,M.H Wong S.C Sze W.C Wong W.L Wong W.K Yu, \223Finding Fuzzy Sets For The Mining Of Fuzzy Associa tion Rules For Numerical Attributes,\224 IDEAL 98 1st Intem\222onal Symposium On Intelligent Engineering and Learning October pp 263-268, 1998 I21 T Fukuda Y Morimoto S Morishta T Tokuyama 223Constructing Effcicnt Decision Trees By Using Optimized Nu meric Association Rules,\224 Proceedings VLDB Bombay 1996 13 T Fukuda Y Morimoto S Morishita T Tokuyama 223Mining Optimized Association Rules for Numeric Attributes,\224 Proceedings ACM SIGACT-SIGMOD-SIGART Symposium on Principles ofDatabase Systems pp 182-1 91 1996 I41 T Fukuda Y Morimoto S Morishita T Tokuyama 223Data Mining Using Two-Dimensional Optimized Association Rules Schemes Algorithms And Visualization,\224 Proceedings Of the ACM SZGMOD Conference on Management of Data June pp 13 23,1996 pp 261-270, 1999 15 J Han Y Cai N Cercone, \223Knowledge Discovery in Da tabases An Attribute-Oriented Approach,\224 VLDB Proceedings Vancouver B.C Canada, pp 547-559, 1992  161 J Han Y Fu 223Mining Optimized Association Rules For Numeric Attributes,\224 VZDB Proceedings Zurich 1995 I71 M Hemetinen H Mannila P Ronkainen H Toivonen A.I Verkamo, \223Finding Interesting Rules from Large Sets Of Dis covered Association Rules,\224 CIKM-1994 1994 18 C.M Kuok A Fu M.H Wong, \223Mining Fuzzy Association Rules In Databases,\224 Proceedings Of the ACMSIGMOD Confer ence on Management ofData pp 4146 1998 I91 D.H Lee M.H Kim 223Database Summarization Using Fuzzy ISA Hierarchies,\224 IEEE Transactions On Systems Man and Cybernetics Part B Vo 27 No 1 pp 68-77, 1997 20 B Liu W Hsu Y Ma 223Pruning and Summarizing the Dis covered Associations,\224 Proceedings of the fifth ACM SIGKDD XDD-99\Proceedings pp 125-134, 1999 21 H Mannila H Toivonen, A.I Verkamo 223Efficient Algo rithms For Discovering Association Rules,\224 AAAI Workshop on Knowledge Discovery and Databases U.M Fayyad R Utharusamy eds\Seattle, Washington, July, pp 181 1 92 1994 22 L. Mazlack, \224Approximate Reasoning Applied To Unsuper vised Database Mining,\224 International Journal of Intelligent Sys tems Vol 12 No 5 May pp 391-414,1997 23 L Mazlack 223Clustering Mined Association Rules,\224 NMIPS 2000 Proceedings July 2000 24 L Mazlack White Paper Granulation Of Quantitative Association Rules U Cincinnati 30 pages 2001 25 RJ Miller Y Yang 221\221Ahxiation Rules Over Interval Data,\224 ACM SIGMOD Vol 26 No 2 May pp 452461,1997 26 R.T. Ng J Han, \223Efficient And Effective Clustering Meth ods For Spatial Data Mining,\224 VLDB Proceedings pp 144-155 1994 27 V Ng J Lee 223Quantitative Association Rules Over In complete Data,\224 IEEE Conference On Systems Man And Cyber netics San Diego pp 2821-2826, 1998 28 R.T Ng, L. Lakshmanan J Han. \223Exploratory Mining And F\221nming Optimizations Of Constrained Association Rules,\224 SIGMOD-98 Proceedings 1998 29 J.S Park M Chen P.S Yu 223An Effective Hash Based Al gorithm For Mining Association Rules,\224 ACM SIGMOD Proceed ings San Jose, May 1996 30 B Padmanabhan A Tuzhilin 223A Belief-Driven Method For Discovering Unexpected Patterns,\224 KDD-98 Proceedings 1998 31 G Piatetsky-Shapiro. \223Discovery, Analysis, and Presenta tion Of Strong Rules,\224 in G piatetsky-Shapiro W.J Frawly eds Knowledge Discovery In Databases AAI Press/MIT Press Cam bridge MA pp 229-248 1991 32 L Rendell H Ragavan, \223Improving The Design Of Induc tion Methods By Analyzing Algorithm Functionality And Data Based Concept Complexity,\224 IJCAI-93 Proceedings Chambrey August pp 952-958,1993 33 R Rastogi K Shim 223Mining Optimized Gain Rules For Nu meric Attributes,\224 Technical Report Bell Laboratories Murray Hill 1998 34 A Sawsere E Omiecinski S Navathe 223An Efficient Al gorithm For Mining Association Rules In Large Databases,\224 VLDB Proceedings Zurich pp 144-1555,1995 35 A Silbershantz A Tuzhilin 223What Makes Patterns Inter esting In Knowledge Discovery Systems,\224 IEEE Transaztions On Knowledge And Data Engineering 222\222 Vol 8 No 6,1996 38 R Srikant R Agrawal 223Mining Quantitative Association Rules in Large Relational Tables.\224 ACM SIGMOD Proceedinm  pp 1-12 199 With Item Constraints,\224 KDD-97 Proceedings 1997 39 R Srikant Q Vu R Agrawal 223Mining Association Rules 40 W Wang J Yang P.S Yu 223Efficient Mining Of Weighted Association Rules,\224 KDD-2000 Proceedings, pp 270-274,2000 41 J RR Yager D.P Filev 223Approximate Clustering Via The Mountain Method,\224 IEEE Transactionr On Systems Man And Cybernetics Vol 24 No 8 pp 1279-1284, 1994 42 Y.Y Yao N Zhong 223An Analysis Of Quantitative Meas ures Associated With Rules,\224 PAKDD-99 Third Pacific-Asia Con ference Beijing pp 479487 1999 43 W Zhang, \223Mining Quantitative Association Rules,\224 IEEE International Conference on Tools Eth Artijkial Intelligence Chicago, November, pp 99-102,1999 417 


and 2 since it is in the second mask meaning the cluster extends two rows This cluster is indicated by the dashed circle Finally mask identifies no clusters since the mask contains no set bits, signifying there are no clusters that be gin at I-ow l and extend through row 3 We now repeat this process beginning with the second row of the bitmap, producing the two clusters as shown Bitmap Masks starting at row 2 row3 1 0 0 mask row2 mask row 1 0 1 1 The process ends when the mask for the last row is com puted Our algorithm can be implemented efficiently since it only uses arithmetic registers, bitwise AND and bit-shift machine instructions We assume that the size of the bitmap is such that it fits in memory which is easily the case even for a 1000x1000 bitmap 3.4 Grid Smoothing As a preprocessing step to clustering we apply a 2D smoothing function to the bitmap grid In practice, we have often found that the grids contain jagged edges or small 223holes\224 of missing values where no association rule was found A typical example is shown in Figure 7\(a These features inhibit our ability to find large, complete clusters To reduce the effects caused by such anomalies we use an image processing technique known as a low-pass jilter to smooth out the grid prior to processing 8 Essentially a low-pass filter used on a two-dimensional grid replaces a value with the average value of its adjoining neighbors thereby 223smoothing\224 out large variations or inconsistencies in the grid The use of smoothing filters to reduce noise is well known in other domains such as communications and computer graphics Details of our filtering algorithm are omitted for brevity, but Figure 7\(b nicely illustrates the res ults Experiments using the association rule support values instead of binary values were also performed yielding prom ising results See Section 5 3.5 Cluster Pruning Clusters that are found by the BitOp algorithm but that do not meet certain criteria are dynamically pruned from the set of final candidate clusters Typically we have found that clusters smaller than 1 of the overall graph are not use ful in creating a generalized segmentation Pruning these smaller clusters also aids in reducing \223outliers\224 and effects from noise not eliminated by the smoothing step In the case where the BitOp algorithm finds all of the clusters to be suf ficiently large no pruning is performed. Likewise if the al Input 11 R the number of bins for attributex 12 C the number of bins for attribute Y 13 BM, the bitmap representation of the grid  is the ith row of bits in the bitmap output 01  clusters of association rules row  1 while row i R do begin RowMask  set all bits to 222 1\222 PriorMask  BM[row height=O for r=row riR r do begin RowMask  RowMask  BM[row if RowMask  0 do begin I Locate clusters in PriorMask I process_row\(PriorMask,height  break end-if if RowMask  PriorMask do begin processxo w\(PriorMask,height  PriorMask  RowMask end-if height  height+l I extend height of possible clusters I end-for processJow\(PriorMask,height end-while Figure 6 The BitOp algorithm  8 B 1 e m attribute X 4 Figure 7 A typical grid \(a\prior to smoothing b after smoothing 226 


computed cluster Function 2 false-positives 1 age  40 A 50K L salary 5 1OOK j GroupA 2 40 5 age  60 A 7511\221 5 salary 5 125K GroupA 3 age 2 60 A 25K 5 salary 5 751q 3 Group A LI B Y else  Group other 0 a E false-negatives   Y a 0 Figure 8 The function used to generate syn thetic data gorithm cannot locate a sufficiently large cluster it termin ates The idea of pruning to reduce error and to reduce the size of the result has been used in the AI community espe cially for decision trees  171 3.6 Cluster Accuracy Analysis To determine the quality of a segmentation by a set of clustered association rules we measure two quantities i the number of rules computed and ii\the summed error rate of the rules based on a sample of the data. The two quantities are combined using the minimum description length MDL principle  181 to arrive at a quantitative measure for determ ining the quality compared to an 223optimal\224 segmentation We first describe our experiments then we describe our error measure for a given rule then we describe our application of the MDL principle In 2 a set of six quantitativeattributes salary, commis sion age, hvulue hyears loan and three categorical attrib utes educationdevel cur zip code for a test database are defined. Using these attributes 10 functions of various com plexity were listed We used Function 2 shown in Figure 8 to generate the synthetic data used in our experiments. The optimal segmentation for this data would be three clustered association rules each of which represents one of the three disjuncts of the function. The clustering process is made dif ficult when we introduce noise, random perturbations of at tribute values and error due to binningof the input attributes age and salary An intuitive measure of the accuracy of the resulting clustered rules would be to see how well the rectangular clusters overlap the three precise disjuncts of the function in Figure 8 We define the notion of false-positives and fulse-negatives graphically as shown in Figure 9 and seek to minimize both sources of error In Figure 9 the light grey rectangle represents an actual cluster according to the function and the dark-grey rectangle represents a computed cluster. Note that in general an optimal cluster need not be rectangular The false-positive results are when the com puted cluster incorrectly identifies tuples outside of the op timal cluster as belonging to the specified group whereas the false-negatives are tuples that should belong to the group but are not identified as such by the computed cluster. The total summed error for a particular cluster is the total false Figure 9 Error between overlapping rc-g\222  ions positives  false-negatives However unless the optimal clustering is known beforehand such as here where a func tion is used to generate the data, this exact measure of error is not possible. Because we are interested in real-world data where the optimal clustering is not known we instead se lect a random sample of tuples from the database and use these samples to determine the relative error of the com puted clusters The relative error is only an approximation to the exact error since we are counting the number of false negatives and false-positives based only on a sample of the original database In order to get a good approximation to the actual error we use repeated k out of n sampling a stronger statistical technique The strategy we use to measure the quality of a segmenta tion given a set of clustered association rules is b<asecf on the MDL principle We are using a simplified model of MDL that has worked in practice The MDL principle states that the best model for encoding data is the one that minimizes the sum of the cost of describing the model and the cost of describing the data using that model The goal is to find a model that results in the lowest overall cost with cost typ ically measured in bits In the context of clustering, the models are the descrip tions of the clusters and the data is the sampled data de scribed above The greater the number of clusters used for segmentation, the higher the cost necessary to describe those clusters The cost of encoding the sampled data using a given set of clusters \(the model\is defined to be the sum of all errors for the clusters The intuition is that if a ruple is not an error, then it is identified by a particular cluster and hence its cost is included with the description of the cluster Otherwise if the tuple is an error, then we must specifically identify it as such and this incurs a cost We use the follow ing equation to determine the cost of a given set of clustered association rules cost  w log,\(ICI  we log,\(errors where IC is the number of clusters and errors is the sum of false-positives  false-negatives\for the clusters C The logarithmic factor is used because having more clusters re quires a logarithmically increasing number of bits to enu 227 


merate and the logarithmic factor provides a favorable non linear separation between close and near-optimal solutions Based on empirical evidence we made the simplifying as sumption the clusters themselves have a uniform encoding cost. The constants wc and we allow the user to impart a bias towards 223optimal\224 cluster selection, providing greater flex ibility in finding a representation of the segmentation that is the most usable If w is large, segmentations that have many clusters will be penalized more heavily since they will have a higher associated cost and segmentations of the data that have fewer clusters will have a greater probability of be ing 223optimal\224. Likewise if we is large, the system will favor segmentations where the error rate is lowest If both con stants are equal wc  we  1 as in thedefault case, neither term will bias the cost Our heuristic optimizer recall Figure 2 by means de scribed in the following section, seeks to minimize the MDL cost 3.7 Parameter Heuristics In this section we describe the algorithm our overall sys tem uses to adjust the minimum support and confidence thresholds based upon the accuracy analysis from the previ ous section. \(currently the number of bins for each attribute is preset at 50 We discuss this issue more in the following section We desire values for support and confidence that will result in a segmentation of the data that optimizes our MDL cost function. The search process involves successive iterations through the feedback loop shown in Figure 2 We identify the actual support and confidence values that appear in the binned data and use only these values when adjusting the ARCS parameters We begin by enumerating all unique support thresholds from the binned data with one pass and then all of the unique confidence thresholds for each of these support thresholds with a second pass A data structure sim ilar to that shown in Figure 10 is used to maintain these val ues. Note that as support increases, there become fewer and fewer cells that can support an association rule and we have found a similar decrease in the variability of the confidence values of such cells Given a choice to either begin with a low support threshold and search upwards or begin with a high sup port threshold and search downwards we chose the former since we found most 223optimal\224 segmentations were derived from grids with lower support thresholds If we were using a previous association rule mining algorithm, for efficiency it might be preferable to start at a high support threshold and work downwards, but our efficient mining algorithm al lows us to discover segmentations by starting at a low sup port threshold and working upwards. Our search starts with a low minimum support threshold so that we consider a lar ger number of association rules initially, allowing the dy namic pruning performed by the clustering algorithm to re Confidence List  I I I I I I 5 rl 12 I 25 I 40 I 56%I 90 Sumort I List\222 Wra*luoal Figure 10 Ordered support thresholds    lists of confidence and move unnecessary rules The support is gradually increased to remove background noise and outliers until there is no im provement of the clustered association rules \(within some E 4 Experimental Results The ARCS system and the BitOp algorithm have been implemented in C comprising approximately 6,300 lines of code To assess the performance and results of the al gorithms in the system we performed several experiments on an Intel Pentium workstation with a CPU clock rate of 120MHz and 32MB of main memory running Linux 1.3.48 We first describe the synthetic rules used in generating data and present our initial results We then briefly compare our results with those using a well-known classifier C4.5 to perform the segmentation task Finally we show perform ance results as the sizes of the databases scale 4.1 Generation of Synthetic Data We generated synthetic tuples using the rules of Func tion 2 from Figure 8 Several parameters affect the distri bution of the synthetic data These include the fraction of the overall number of tuples that are assigned to each value of the criterion attribute aperturbation factor to model fuzzy boundaries between disjuncts and an outlier percentage that defines how many tuples will be assigned to a given group label but do not match any of the defining rules for that group. These parameters are shown in Table 1 4.2 Accuracy and Performance Results We generated one set of data for Function 2 with ID1  50,000 and 5 perturbation and a second set of data for the same function but with 10 outliers, i.e 10 of the data are outliers that do not obey the generating rules In every ex perimental run we performed ARCS always produced three clustered association rules each very similar to the gener ating rules and effectively removed all noise and outliers 228 


Attribute 1 Value salary age 1 uniformly distributed from 20,000 to 150k uniformly distributed from 20 to 80 PI fracA fracother P U Number of tuples 20,000 to 10 million Fraction of tuples for \223Group A\224 40 Fraction of tuples for \223Group other\224 60 Perturbation factor 5 Outlier percentage 0 and 10 Table 1 Synthetic data parameters from the database. The following clustered association rules were generated for Group A clusters from the set of data containing outliers and with a minimum support threshold of 0.01 and a minimum confidence threshold of 39.0 20 5 Age 5 39 A 48601 5 Salary 5 100600 j Grp A 40 5 Age 5 59 A 74601 5 Salary 5 124000  Grp A 60 5 Age 5 80 A 25201 5 Salary 5 74600  Grp A The reader can compare the similarity of these rules with those used to generate the synthetic data in Figure 8 We measured the error rate of ARCS on these databases and compared it to rules from C4.5 C4.5 is well known for building highly accurate decision trees that are used for classifying new data and from these trees a routine called C4.5RULES constructs generalized rules  171 These rules have a form similar to our clustered association rules and we use them for comparison both in accuracy and in speed of generation. Figures 11 and 12 graph the error of both sys tems as the number of tuples scale using the two sets of gen erated data The missing bars for C4.5 on larger database sizes are due to the depletion of virtual memory for those ex periments, resulting in our inability to obtain results \(clearly C4.5 is not suited to large-scale data sets From Figure 11 we see that C4.5 rules generally have a slightly lower error rate than ARCS clustered association rules when there are no outliers in the data However, with 10 of the data appearing as outliers the error rate of C4.5 is slightly higher than ARCS as shown in Figure 12 C4.5 also comes at a cost of producing significantly more rules as shown in Figures 13 and 14 As mentioned earlier we are targeting environments where the rules will be processed by end users so keeping the number of rules small is very im portant The primary cause of error in the ARCS rules is due to the granularity of binning The coarser the granularity, the less likely it will be that the computed rules will have the same boundary as the generating rules To test this hypo thesis we performed a separate set of identical experiments using between 10 to 50 bins for each attribute We found a general trend towards more 223optimal\224 clusters as the number of bins increases 100 10002000 4000 8000 10000 Number of Tuples in 222000s Figure 15 Scalability of ARCS Table 2 Comparative execution timers sec 4.3 Scaleup Experiments To test scalability we ran ARCS on several databases with increasing numbers of tuples. Figure 15 shows that ex ecution time increases at most linearly with the size of the database Because ARCS maintains only the ElinArray and the bitmap grid ARCS requires only a constant amount of main memory regardless of the size of the database \(assum ing the same number of bins\This actually gives our system significantly better than linear performance as can be seen by close inspection of Figure 15 since some overhead ex ists initially but the data is streamed in faster from the U0 device with larger requests For example when the num ber of tuples scales from 100,000 to 10 million a factor of loo the execution time increases from 42 seconds to 420 seconds a factor of lo In comparison C4.5 requires the entire database, times some factor, to fit entirely in main memory This results in paging and eventual depletion of virtual memory which prevented us from obtaining execu tion times or accuracy results for C4.5 rules from databases greater than 100,000 tuples\Both C4.5 alone and C4.5 to gether with C4.5RULES take exponentially higher execu tion times than ARCS as shown in Table 2 229 


10 r  ______ 20 18 7 20 50 100 200 500 1000 Number of Tuples in 221000s Figure 11 Error rate with U  0 20 50 100 200 500 1000 Number of Tuples in 221000s Figure 12 Error rate with U  10 35 4 30 II 6 25  v   L 3 20  5 15  z 10  5 OT 12 16 31 3 20 50 100 200 500 1000 Number of Tuples in 221000s Figure 13 Number of rules produced U  0 5 Conclusions and Future Work In this paper we have investigated clustering two attribute association rules to identify generalized segments in large databases The contributions of this paper are sum marized here 0 We have presented an automated system to compute a clustering of the two-attribute space in large databases 0 We have demonstrated how association rule mining technology can be applied to the clustering problem We also have proposed a specialized mining algorithm that only makes one pass through the data for a given partitioning of the input attributes and allows the sup port or confidence thresholds to change without requir ing a new pass through the data e A new geometric algorithm for locating clusters in a two-dimensional grid was introduced Our approach has been shown to run in linear time with the size of 16  14  z 12 O 10 kl 5 z6 4 2 0 Lc 20 50 100 200 500 1000 Number of Tuples in 221000s Figure 14 Number of rules produced U  10 the clusters Further, parallel implementations of the algorithm would be straightforward 0 We apply the Minimum Description Length MDL principle as a means of evaluating clusters and use this metric in describing an 223optimal\224 clustering of associ ation rules 0 Experimental results show the usefulness of the clustered association rules and demonstrates how the proposed system scales in better than linear time with the amount of data The algorithm and system presented in the paper have been implemented on several platforms including Intel DEC and SGI So far we have performed tests only us ing synthetic data but intend to examine real-world demo graphic data We also plan on extending this work in the fol lowing areas 0 It may be desirable to find clusters with more than two attributes One way in which we can extend our pro posed system is by iteratively combining overlapping 230 


sets of two-attribute clustered association rules to pro duce clusters that have an arbitrary number of attrib utes Handle both categorical and quantitative attributes on the LHS of rules To obtain the best clustering we will need to consider all feasible orderings of categorical at tributes Our clustering algorithm has been extended to handle the case where one attribute is categorical and the other quantitative and we achieved good results By using the ordering of the quantitative attribute we con sider only those subsets of the categorical attribute that yield the densest clusters Preliminary experiments show that segmentation can be improved if the association rule support values rather than binary values are considered in the smooth ing filter and more advanced filters could be used for purposes of detecting edges and corners of clusters It may be beneficial to apply measures of information gain 16 such as entropy when determining which two attributes to select for segmentation or for the op timal threshold values for support and confidence The technique of factorial design by Fisher 6 41 can greatly reduce the number of experiments necessary when searching for 223optimal\224 solutions This tech nique can be applied in the heuristic optimizer to re duce the number of runs required to find good values for minimum support and minimum confidence Other search techniques such as simulated annealing can be also be used in the optimization step Acknowledgements We are grateful to Dan Liu for his work in extending functionality in the synthetic data generator and for the ex periments using C4.5 References I R Agrawal S Ghosh T Imielinski, B. Iyer and A. Swami An interval classifier for database mining applications In Proceedings of the 18th International Conference on Very Large Data Bases Vancouver Canada, 1992 2 R Agrawal T Imielinski, and A. Swami. Database mining A performance perspective In ZEEE Transactions on Know ledge and Data Engineering volume 5\(6 pages 914-925 Dec 1993 3 R Agrawal T Imielinski and A Swami Mining associ ation rules between sets of items in large databases In Pro ceedings of the I993 ACM SIGMOD International Confer ence on Management of Data Washington D.C 1993 4 G E Box W Hunter and J S Hunter Statistics for Ex perimenters An Introduction to Design, Data Analysis and Model Building John Wiley and Sons 1978 5 T Cormen, C. Lieserson and R Rivest Introduction to Al gorithms The MIT Press 1990 6 R A Fisher The Design of Experiments Hafner Publishing Company 1960 71 T Fukuda Y Morimoto S Morishita and lr Tokuyama Data mining using two-dimensional optimized association rules Scheme algorithms, and visualization In Proceed ings of the 1996ACM SIGMOD International Conference on Management of Data Montreal Canada June 1996 8 R C Gonzalez and R Woods Digital Image Processing Addison-Wesley 1992 9 M Klemettinen H Mannila P Ronkainen and H Toivonen Finding interesting rules from large sets of discovered association rules In 3rd International Confer ence on Information and Knowledge Management CIKM Nov 1994 IO J B Kruskal. Factor analysis and principle components Bi linear methods. In H Kruskal and J. Tanur editors Interna tional Encyclopedia of Statistics Free Press 1978  111 D N Lawley Factor Analysis as a Statistical Method American Elsevier Publishing second edition 1971 I21 D J Lubinsky Discovery from databases A review of ai and statistical techniques In IJCAI-89 Workshop on Know ledge Discovery in Databases pages 204218,1989 I31 M Mehta, R. Agrawal and J Rissanen Sliq A fast scal able classifier for data mining In Proceedings of the 5th In ternational Conference on Extending Databaste Technology EDBT Avignon France Mar 1996 I41 M Muralikrishna and D DeWitt Statistical pirofile estima tion in database systems ACM Computing Suweys 20\(3 Sept. 1988  151 G. Piatetsky-Shapiro Discovery, analysis and presentation of strong rules Knowledge Discovery in Databases 1991 16 J Quinlan. Induction of decision trees In MachineLearning volume 1 pages 81-106,1986  171 J Quinlan C4.5 Programs for Machine Learning Morgan Kaufmann San Mateo California 1993  J Rissanen Stochastic Complexity in Statistical Inquiry World Scientific Publishing Company, 1989 I91 H C Romesburg Cluster Analysis for Researchers Life time Learning Publications-Wadsworth Inc 1984 20 J Shafer R Agrawal and M Mehta Fast serial and paral lel classification of very large data bases In Proceedings of the 22nd International Conferenceon Very Luge Databases Bombay, India, 1996 21 H Spath Cluster AnalysisAlgorithms for data reductionand classijication ofobjects Ellis Horwood Publishers, 1980 22 R Srikant and R Agrawal Mining quantitative association rules in large relational tables In Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data Montreal Canada June 1996 23 K Whang S Kim and G Wiederhold Dynamic mainten ance of data distribution for selectivity estimation VLDB Journal 3\(1\Jan 1994 231 


