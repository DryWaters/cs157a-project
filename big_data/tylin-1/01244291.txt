Fuzzy Rule Extraction Based on the Mining Generalized Association Rules Tnshihikn Watanabe Faculty of Engineering Osaka Electro-Communication University Osaka Japan t-wata@isc.osakac.ac.jp Abstract  In data mining approach the quantitative attributes should be appropriately dealt with as well os the Boolean anributes This paper describes a fuzzy rule extraction method based on the mining generalized association rules from database The objectives of the method are 
to improve the computational time of mining and the accuracy of the extracted rules for the actual application In our approach, we construct a hierarchical taxonomic fuq sets structure in each anribute Two algorithms are shown based on the structure and the Apriori algorithm We propose a multistage fuq rule extraction algorithm and a multiscan algorithm From the results of numerical experiments our methods are found to be effective in terms of computational 
time Keywords Data Mining fuzzy rule association rule database 1 Introduction Recently, Data Mining to extract knowledge or rules from massive data sets stored in the database or dataware house is studied for utilizing in various business scenes As the promising applications of Data Mining Association have been applied to various marketing problems As for the other needs like in manufacturing area there still exist many problems to cope with the stored valuable data such as 
business decision, process improvement, and so on In such domain it seems that conventional approach such as computer assisted data analysis of stored data has been applied depending upon the human skill Several reasons can be stated why the approach based on Data Mining methods is not applicable for such problems one of the main reasons is that the mining system should deal with quantitative attributes, appropriately  In order to deal with the quantitative attributes in mining association rules algorithms based on the generalized association rules that 
handle the continuous attributes as the Boolean vector by partitioning into several intervals are proposed 1,2 Though several methods were also proposed to improve the computational Makishi Nakayama Electronics Research Laboratory Kobe Steel Ltd Kobe, Japan m-nakayama@rd.kcrl .kobelco.co.jp time the results through the algorithms were still time consuming and are complicated to the user Fuuy association rules are proposed to overcome such disadvantages based on the fuuy set These approaches were based on the fuzzy extensions to the classical association rules mining 
by defining support and confidence of the fuuy rule Though the mining results are easy to understand by human 221operator, two drawbacks are still remained for applying such fuzzy approaches to the actual problems One is the computational time for mining from database and the other is accuracy of extracted rules The accuracy of extracted rules depends upon the fuuy set defined and the fineness of its partition In existing approaches, it is assumed that the input space was divided by grid-type fuzzy partitions in advance[4,5 
However this assumption leads to deterioration of accuracy when the hue fuzzy set exists between the grids as well as to waste of computational time In this paper we describe a fuzzy rule extraction method based on the mining generalized association rules The objectives of the method are to improve the computational time of mining and the accuracy of the extracted rules for the actual application Results of numerical experiments are also shown 2 Hierarchical Taxonomic 
Fuzzy Sets Structure for Data Mining The main problem of 223Mining Association Rules\224 is to heighten computational efficiency in extraction process from huge database For mining rule under the grid type fuzzy partition increase of the partition number directly affects the computational time i.e query response time to the database and the memory consuming Our approach is to generate a 223fuzzy set taxonomy\224 on each attribute  0-7803-7952-7/03/$17.00 0 2003 IEEE 2690 


Figure 1 Hierarchical Fuzzy Set Taxonomy from the pre-defined grid type fuzzy partitions which are expressed as the 223information granularity.\224 The altemative fuzzy itemsets are generated dynamically through the mining procedure based on the 223fuzzy set taxonomy.\224 The 223fuzzy set taxonomy\224 is created from the base fiuq sets i.e the defined grid type fuzzy partitions by composite operations and the linguistic hedge as shown in Fig 1 There are three base fuzzy sets shown as A B C The fuzzy set 223X AND Y\222 is characterized by product operation as PXANDY 221PY 1 where U denotes the membership function. The fuzzy set 223X+Y is characterized as PX+Y  midl9 Px  Pr 1 2 It should be noted that the above definition is different from the well-known fuzzy 223OR\224 operation as the composite fuzzy set should be valid in the sense of information retrieval A new fuuy set is generated by combining adjacent base fuzzy sets applying the 223+\224 operation defined in Eq.\(2 The linguistic hedge represented as 223Very\224 is characterized as follows 3 2 P,,X  Px By using these operations the 223fuzzy set taxonomy\224 is automatically formulated from user specified base fuzzy sets These generated fuuy sets are within partial order relation in terms of containment expressed as xc y 3 PX\(U U 221dUEU 4 where U denotes the universal set. The relation plays an important role in mining procedure 3 Fuzzy Data Mining Algorithm 3.1 Fuzzy Association Rules fuvy sets in different attribute as follows Let F denote the fuvy itemset which consists of F  P?Q,T 5 where P Q and T denote the fuzzy sets Support of the itemset F is defined as where xp denotes the transaction of the database and pF\(xP denotes the membership value calculated by the product operation of each item\(Fuzzy Set in F and m denotes the total number of transactions From the support value confidence of the fuzzy association rule G  H is calculated by c\(G 3 H  s\(G U H G 7 where G and Hare fuzzy itemsets Fuuy association rule is extracted when these values of the rule are more than pre-defined minimal support and pre-defined minimal confidence Generally the 223combinatorial explosion\224 is occurred in fuzzy rule approach When we apply the mining algorithm to the actual problems especially using fuuy sets we should avoid such problems i.e numerous query and memory consumption. In OUT approach the following equation holds from Eq.\(2 s\(X  Y  s\(X  s\(Y 8 This contributes the slight saving in computation There exists more important computational phase in the association rule mining When we apply the mining algorithm to the actual huge problems the support calculation is critical calculation conceming the number of query to the database The itemset which has the value greater than predefined threshold is called 223frequent itemset.\224 The main problem of mining association rules is how efficiently fmd the 223frequent itemsets\224 from the database 2691 


3.2 Apriori Algorithm The Apriori algorithm is essential and effective method for mining association rules. The basic idea is that the frequent itemset should contain the subsets of frequent itemsets Owing to this characteristic frequent itemsets can be compounded from the smaller frequent itemsets one after another. Let k-itemset denote an itemset having k items. Let Lk represent the set of frequent k-itemsets and C the set of candidate k-itemsets The algorithm to generate the frequent itemsets is as follows Al Ck is generated by joining the itemsets in Lk.i A2 The itemsets in Ck which have some  that is not in Lk are deleted Figure 2 shows the system flow outline of mining fuq association rule based on the Apriori algorithm In our fuuy mining problem the Apriori algorithm can be applicable like in 223Boolean\224 mining problem as the item in Apriori algorithm corresponds to the fuuy set label It should be noted that the fuzzy sets defined in the same attribute cannot be joined in the above joining phase Figure 2 Association Rule Mining Flow Chart 4 Mining Algorithm Based on the Fuzzy Set Taxonomy Structure In our approach faster algorithm can be formulated by utilizing its hierarchical taxonomic structure Let us assume that fuur set A contains fuzzy set B on attribute G as the following relation where ai denotes the attribute value of i-th transaction Let us assume that the fuzzy set C is also defined on attribute H and the itemsets composed of fuzzy set B and C don\222t satisfy the constraint of support as follows From Eq.\(9 and Eq.\(lO the following relationship holds We define the relationship \223fuzzy item subset\224 that fuur itemset A C is the subset of E C in terms of the fuzzy support definition This characteristic of taxonomic structure contributes the decrease of iteration to calculate the frequent itemsets From these relationships above Apriori algorithm can be enforced as the following algorithms 4.1 Multiscan Algorithm We first define the Ck which contains only the fuzzy sets in predefined higher level of the taxonomic hierarchy Let Ck denote the candidate set of L\222 by joining L\222k The procedure is as follows 1 L\222k.I is selected from based on the hierarchy level 2 Ck is generated by joining L\222k 3 L\222r is decided through the inappropriate itemsets in 4 Ck is generated by joining Lk 5 A2 procedure is employed Furthermore the itemsets in C are deleted on condition that the itemset in Ck which is not the 223fuzzy item subset\224 of any itemset in Ck are deleted in the same manner of A2 L\222k 6 Lk is decided In the algorithm we should do the multiple scan of the database As this leads to increase of the computational time we apply the sampling method in which we use the partial transactions of the database for calculation of the 2692 


item support When we use the whole transactions in the algorithm the result is same as one by standard Apriori algorithm 4.2 Multistage Fuuy Rule Extraction Algorithm The higher level fuzzy sets in hierarchical taxonomy represent more rough information of the attribute In a multistage fuuy rule extraction algorithm fuzzy association rules are decided by using only the higher level fuzzy sets first Let L\224 denote the fuzzy itemsets corresponding to the derived fuzzy association rules The second mining process is employed as follows 1 Ck is generated by joining 2  itemsets in C which have some lj-suhset that is not in Lk are deleted. Furthermore the itemsets in ck are deleted on condition that the itemset in C which is not the 223fuzzy item subset\224 of any itemset in L\224 By this algorithm required computational time can be reduced in each mining stage As the multiple scan of the database is also needed in the algorithm we can use the database sampling method Table 1 Abalone Data 41J71 0 10.555 10.195 11.9485 10.9455 10.3765 0.495 I 12 Table 2 Statistical Information and Fuuy Sets Settings a\Range Information of Abalone Data b Fuzzy Sets Settings 5 Numerical Experiments We develop the fuzzy mining system and evaluate the proposed algorithms through numerical experiments We apply the system to 223abalone data\224 available from the UCI Machine Learning Repository hap www ics.uci.edu/-mleam/MLRepository.html The abalone data set consists of 4177 measured data with 1 nominal attribute and 8 continuous attributes as shown in Table.1 In the experiments the nominal attribute is transformed to the continuous attribute Table Z\(a shows the statistics and the fuzzy partition information of attributes is summarized in Table 2\(b\The range of fuzzy partition is set wider than the actual data distribution for performance evaluation The fuzzy partition example is shown in Fig.3 We use 2-level hierarchical fizzy set taxonomy for each attribute Although it is better to use the factor such as 223interesting\224 for pruning the derived rules we apply the native association rule extraction for algorithm evaluation The results by standard Apriori algorithm are shown in Fig.4 In 4\222 itemset calculation, waste computation is employed We should reduce the computation as possible as we can Figure 5 shows the number of calculated itemsets by the multiscan algorithm without the sampling method The reduction rates of the waste support calculation compared with the Apriori algorithm are shown in Fig.6 Though the required time for database scan is increased the support calculation is reduced Membmhip Value I 1 0 0.0 2.0 Figure 3 An Example of Fuuy Partition 2693 


  CaknhrcQ Sa      0 I23456189 Itemset Sm Figure 4 Mining Results by Apriori Algorithm   touMculodcs.e-l   s 123456789 lv-t ser Figure 7 Results by the 2-stage Fuuy Rule Extractio Algoritbmn I     i       _  100 80  E a 840 1 P 60 E   m 20 0 123456789 1lClDIet see Figure 5 Mining Results by Multiscan Algorithm 12 I 1W     2 3 4'5 6 7 8 9 I,eme*sk Figure 6 Reduction Rate of Waste Itemsets by Multiscan Algorithm   23156789 IltmrC smc Figure 8 Reduction Rate of Waste Itemsets by 2-Stage Fuzzy Rule Extraction     J 0 0 0.2 0.4 0.6 0.8 I sa*ph8 She Rate Figure 9 Rule Extraction by the Sampling Method in First Mining Stage 2694 


1w  0 0.2 0.4 0.6 0.8 1 Samphg She Parameter Figurelo Rule Extraction by the Sampling Method in Both Mining Stage The results by using 2 stage fuzzy rule extraction algorithm without the sampling method are shown in Fig.7 In the first stage only the higher level fuzzy sets are used for mining association rule The reduction rates of the waste support calculation compared with the Apnori algorithm are very large as shown in Fig.8 The extracted rules are the same as the standard mining rules in this case. Though the required time for database scan is increased the method is promising from the results Furthermore the extracted rules by the first stage mining can also be utilized for users In order to reduce the cost of database scan we apply the sampling method in the first mining stage We should confirm the quality of rule extraction in such approximate method The results are shown in Fig.9 The sampling size rate denotes the used transaction rate in database The quality of rule extraction is maintained over the sampling size rate 0.5 i.e the half of all transactions Next simulation is application of the sampling methods to the both mining stages The sampling size parameter a denote the sampling size rate in the first stage In the second stage the sampling size rate is set to I-a In this situation the cost of database scan may be considered to be equivalent to the scan cost in conventional approach The results are shown in Fig.10 The quality of the extracted rules is adequately high around the rate 0.5 As the required computational time is reduced and the consumed memory is restrained from the results in Fig.8 the proposed method is effective for fuzzy rule mining Furthermore the method could also contribute the realization of 223Interactive mining architecture\224 which provides us the useful computational environment for mining by the multistage calculation 6 Conclusions In this paper fuuy rule extraction method based on the generalized association rules from the database composed of quantitative attributes was proposed The mining calculation efficiency is improved owing to the fuzzy sets expression as hierarchical taxonomic structure Based on the structure the Multi Scan algorithm which can calculate the support value iteratively according to the hierarchy level of the fuzzy sets was discussed We proposed the Multistage fuzzy rule extraction algorithm to improve the computational efficiency of the mining In the first stage of the algorithm the fuzzy rule extraction is employed using the fuzzy sets of high hierarchy level In the next stage mining procedure is employed utilizing the results in first stage. We developed the mining system and applied to the benchmark database Simulation results show the effectiveness of our methods In future we will apply the system to more complicated and large actual databases References l R Srikant and R Agrawal 224Mining Generalized Association Rules,\224 Proc. of the 21\221\221 VLDB Conz pp.407 419 1995  R Srikant and R Agrawal 223Mining Quantitative Association Rules in Large Relational Tables,\224 Proc of the ACM ConJ on Management of the Data pp.1-12 1996 3 G Chen and Q Wei 223Fuuy Association Rules and the Extended Mining Algorithms Information Sciences Vo1.147 pp.201-228 2002 4 H Ishibuchi and T Yamamoto 223Fuzzy Rule Selection by Data Mining Criteria and Genetic Algorithms,\224 Proc of Genetic and Evolutionary Computation Conference pp 399406,2002 5 Y Hu R. Chen and G Tzeng 223Discovering Fuuy Association Rules Using Fuzzy Partition Methods,\224 Knowledge-Based Systems Vol 16, pp.137-147,2003 2695 


acquires a lock on this leaf node for mutual exclusion while inserting the itemset r if we exceed the threshold of the leaf we convert the leaf into an internal node with the lock still set This implies that we also have to provide a lock for all the internal nodes and the processors will have to check if any node is acquired along its downward path from the root This complication only arises at the interface of the s and internal nodes With this locking mechanism each process can insert the itemsets in different parts of the hash tree in parallel r since we start with a hash tree with the root as a leaf there can be a lot of initial contention to acquire the lock at the root r we did not 336nd this to be a signi\336cant factor on 12 processors 3.2 Support Counting r this phase we could either split the database logically among the processors with a common hash tree or split the hash tree with each processor traversing the entire database We will look at each case below 3.2.1 Partitioned vs Common Candidate Hash Tree One approach in parallelizing the support counting step is to split the hash tree among the processors The decisions for computation balancing directly in\337uence the effectiveness of this approach since each processor should ideally have the same number of itemsets in its local portion of the hash tree Another approach is to keep a single common hash tree among all the processors There are several ways of incrementing the count of itemsets in the common candidate hash tree Counter per Itemset Let us assume that each itemset in the candidate hash tree has a single count 336eld associated with it Since the counts are common more than one processor may try to access the count 336eld and increment it We thus need a locking mechanism to provide mutual exclusion among the processors while incrementing the count This approach may cause contention and degrade the performance r since we are using only 12 processors and the sharing is very 336ne-grained at the itemset level we found this approach to be the better than using private or separate counters 1  3.2.2 Partitioned vs Common Database We could either choose to logically partition the database among the processors or each processor can choose to traverse the entire database for incrementing the candidate support counts Balanced Database Partitioning In our implementation we partition the database in a blocked fashion among all the processors r this strategy may not result in balanced work per processor This is because the work load is a function of the length of the transactions If l t is the length of the transaction t  then during iteration k of the algorithm we have to test whether all the 1 r all the databases we looked at on our system the overhead of contention was within 4 which leads us to conclude that contention is not a big problem Other mechanisms like separate counters to eliminate locking and local counters to eliminate false sharing were studied t not shown to be bene\336cial 14 7 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


000 l t k 001 subsets of the transaction are contained in C k  Clearly the complexity of the work load for a transaction is n s O 000 min 000 l t k 000l t l t 000 k 001\001  i.e it is polynomial in the transaction length This also implies that a static partitioning won\325t work r we could devise static heuristics to approximate a balanced partition r example one static heuristic is to estimate the maximum number of iterations we expect say T  We could then partition the database based on the mean estimated work load for each transaction r all iterations n s 000 P T k 000 1 000 l i k 001 001 001T  Another approach is to re-partition the database in each iteration In this case it is important to respect the locality of the partition by moving transactions only when it is absolutely necessary We plan to investigate different partitioning schemes as part of future work 3.3 Parallel Data Mining Algorithms Based on the discussion in the previous section we consider the following algorithms for mining association rules in parallel 000 Common Candidate Partitioned Database CCPD This algorithm uses a common candidate hash tree across all processors while the database is logically split among them The hash tree is built in parallel see section 3.1.4 Each processor then traverses its local database and counts the support see section 3.2.1 for each itemset Finally the master process selects the large itemsets 000 Partitioned Candidate Common Database PCCD This has a partitioned candidate hash tree t a common database In this approach we construct a local candidate hash tree per processor Each processor then traverses the entire database and counts support for itemsets only in its local tree Finally the master process performs the reduction and selects the large itemsets for the next iteration Note that the common candidate common database\(CCCD approach results in duplicated work while the partitioned candidate partitioned database PCPD approach is more or less equivalent to CCPD r this reason we did not implement these parallelizations 4 Optimizations In this section we present some optimizations to the association rule algorithm These optimizations are bene\336cial for both sequential and parallel implementation 4.1 Hash Tree Balancing Although the computation balancing approach results in balanced work load it does not guarantee that the resulting hash tree is balanced Balancing C 2 No Pruning  We\325ll begin by a discussion of tree balancing for C 2  since there is no pruning step in this case We can balance the hash tree by using the bitonic partitioning scheme described  We simply replace P  the number of processors with the fan-out F for 8 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


the hash table We label the n large 1-itemsets from 0 o n 000 1 n exicographical order and use P 000 F to derive the assignments A 0 000 001\001\001 000 A F\000 1 for each processor Each A i is treated as an equivalence class The hash function is based on these equivalence classes which is simply n as h 001 i 002\000 A i  for i 000 0 000 001\001\001 000 F  The equivalence classes are implemented via an indirection vector of length n  For example let L 1 000 f A\000 D 000 E 000 G\000 K 000 M 000 N 000 S\000 T 000 Z g  We 336rst label these as f 0 000 1 000 2 000 3 000 4 000 5 000 6 000 7 000 8 000 9 g  Assume that the fan-out F 000 3 We thus obtain the 3 equivalence classes A 0 000 f 0 000 5 000 6 g  A 1 000 f 1 000 4 000 7 g  and A 2 000 f 2 000 3 000 8 000 9 g  and the indirection vector is shown in table 1 Furthermore this hash function is applied at all levels of the hash tree Clearly this scheme results in a balanced hash tree as compared to the simple g 001 i 002\000 i mod F hash function which corresponds to the d partitioning scheme from section 3.1.1 Label 0 1 2 3 4 5 6 7 8 9 Hash Value 0 1 2 2 1 0 0 1 2 2 Table 1 Indirection Vector Balancing C k 001 k\001 2 002  Although items can be pruned for iteration k 002 3 we use the same bitonic partitioning scheme for C 3 and beyond Below e show that n n this general case bitonic hash function is very good as compared to the d scheme Theorem 1 below establishes an upper and lower bound on the number of itemsets per leaf for the bitonic scheme Theorem 1 Let k 002 1 denote the iteration number I 000 f 0 000 002\002\002\000 d 000 1 g the set of items F the fan-out of the hash table T 000 f 0 000 002\002\002\000 F\000 1 g the set of equivalence classes modulo F  T 000 T k the total number of leaves in C k  and G the family of all size k ordered subsets of I  i.e the set of all k itemsets that can be constructed from items in I  Suppose d 2 F is an integer and d 2 F 000 F\002 k  De\336ne the bitonic hash function h  I\003 T by h 001 i 002\000 i mod F if 0 004 001 i mod 2 F 002 003 F and 2 F\000 1 000 001 i mod 2 F 002 otherwise and the mapping H  G 003 T from k itemsets to the leaves of C k by H 001 a 1 002\002\002\000 a k 002 000 001 h 001 a 1 002 000\002\002\002\000h 001 a k 002\002  Then for every leaf B 000\001 b 1 000\002\002\002\000b k 002 005T  the ratio of the number of k itemsets in the leaf  k H 000 1 001 B 002 k  o the average number of itemsets per leaf  kG k 004 kT k  s bounded above and below by the expression e 000 k 2 d\000 F 004 k H 000 1 001 B 002 k kG k 004 kT k 004 e k 2 d\000 F 002 A proof of the above theorem can be found in W e also obtain the same lo wer and upper bound for the d hash function also r the two functions behave differently Note that the average number of k itemsets per leaf kG k 004 kT k is 000 2 w F k 001 004 F k 006 000 2 w 001 k k   Let 005 001 w 002 denote this polynomial We say that a leaf has a capacity close to the average if its capacity which is a polynomial in w of degree at most k  s f the form 000 2 w 001 k k  003 006 001 w 002  with 006 001 w 002 being a polynomial of degree at most k 000 2 9 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


r the bitonic hash function a leaf speci\336ed by the hash values 000 a 1 000\000\000\001 a k 001 has capacity close to 002 000 w 001 if and only if a i 000 002 a i 000 1 for all i 1 001 i 001 k 002 1 Thus there are F 000 F\002 1 001 k 000 1 such leaves and so 000 1 002F 000 1 001 k 000 1 fraction of the s ave capacity close to 002 000 w 001  Note also that clearly 000 1 002F 000 1 001 k 000 1 approaches 1 On the other hand for the d hash function a leaf speci\336ed by 000 a 1 000\000\000\001 a k 001 has capacity close to 002 000 w 001 if and only if a i 000 002 a i 000 1 for all i  and the number of i such that a i 003a i 000 1 is equal to 000 k 002 1 001 004 2 So there is no such leaf if k is even r odd k 003 3 the ratio of the 322good\323 s decreases as k increases achieving a maximum of 2 004 3 when k 002 3 Thus at most 2 004 3 f the s achieve the average From the above discussion it is clear that while both the simple and bitonic hash function have the same maximum and minimum bounds the distribution of the number of itemsets per leaf is quite different While a signi\336cant portion of the s are close to the average for the bitonic case only a ew are close in the simple hash function case 4.2 Short-circuited Subset Checking  Candidate Hash Tree \(C 3 Hash Function: h\(i DEPTH 0 DEPTH 1 DEPTH 2 01 35 7101113 12986 24 LEAVES ABE ADE CDE A,C,EB,D B,D B,D B,D B,D B,D B,DA,C,E A,C,E A,C,E A,C,E A,C,E A,C,E ABD ACD ACEBCEBCDBDE ABC Figure 2 Candidate Hash Tree  C 3  Recall that while counting the support once we reach a leaf node we check whether all the itemsets in the leaf are contained in the transaction This node is then marked as VISITED to avoid processing it more than once for the same transaction A further optimization is to associate a VISITED 337ag with each node in the hash tree We mark an internal node as VISITED the 336rst time we touch it This enables us to preempt the search as soon as possible We would 10 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


expect this optimization to be of greatest bene\336t when the transaction sizes are large r example if our transaction is T 000 f A\000 B 000 C\000 D\000 E g  k 000 3 fan-out 000 2 then all the 3-subsets of T are f ABC,ABD,ABE,ACD,ACE,ADE,BCD,BCE,BDE,CDE g  Figure 2 shows the candidate hash tree C 3  We ave to increment the support of every subset of T contained in C 3  We egin with the subset AB C  and hash to node 11 and process all the itemsets In this downward path from the root we mark nodes 1 4 and 11 as visited We then process subset AD B  and mark node 10 Now consider the subset CDE  We see in this case that node 1 has already been marked and we can preempt the processing at this very stage This approach can r consume a lot of memory r a n fan-out F  for iteration k  e need additional memory of size F k to store the 337ags In the parallel implementation we have to keep a VISITED 336eld for each processor bringing the memory requirement to P\000F k  This can still get very large especially with increasing number of processors In we sho w a mechanism by which further reduces the memory requirement to only k 000F  The approach in the parallel setting yields a total requirement of k 000F 000P  5 Experimental Evaluation Database T I D Total Size T5.I2.D100K 5 2 100,000 2.6MB T10.I4.D100K 10 4 100,000 4.3MB T15.I4.D100K 15 4 100,000 6.2MB T20.I6.D100K 20 6 100,000 7.9MB T10.I6.D400K 10 6 400,000 17.1MB T10.I6.D800K 10 6 800,000 34.6MB T10.I6.D1600K 10 6 1,600,000 69.8MB Table 2 Database properties 5.1 Experimental Setup All the experiments were performed on a 12-node SGI Power Challenge shared-memory multiprocessor Each node is a MIPS processor running at 100MHz There\325s a total of 256MB of main memory The primary cache size is 16 KB 64 bytes cache line size with different instruction and data caches while the secondary cache is 1 B 128 bytes cache line size The databases are stored on an attached 2GB disk All processors run IRIX 5.3 and data is obtained from the disk via an NFS 336le server We used different synthetic databases with size ranging form 3MB to 70MB 2  and are generated using the procedure described in These databases mimic the transactions in a retailing en vironment Each transaction has a unique ID followed by a list of items bought in that transaction The 2 While results in this section are only shown for memory resident databases the concepts and optimization are equally applicable for non memory resident databases In non memory resident programs I/O becomes an important problem Solutions to the I/O problem can be applied in combination with the schemes presented in this paper These solutions are part of future research 11 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


  0 500 1000 1500 2000 2500 0 2 4 6 8 10 12 Number of Large Itemsets Iterations Large Itemset at Support = 0.5 222T5.I2.D100K\222  222T10.I4.D100K\222   222T15.I4.D100K\222   222T20.I6.D100K\222   222T10.I6.D400K\222   222T10.I6.D800K\222   222T10.I6.D1600K\222  Figure 3 Large Itemsets per Iteration data-mining provides information about the set of items generally bought together Table 2 shows the databases used and their properties The number of transactions is denoted as jD j  average transaction size as j T j  and the average maximal potentially large itemset size as j I j  The number of maximal potentially large itemsets j L j 000 2000 and the number of items N 000 1000 We refer the reader to for more detail on the database generation All the e xperiments were performed with a minimum support value of 0.5 and a leaf threshold of 2 i.e max of 2 itemsets per leaf We note that the  improvements shown in all the experiments except where indicated do not take into account initial database reading time since we speci\336cally wanted to measure the effects of the optimizations on the computation Figure 3 shows the number of iterations and the number of large itemsets found for different databases In the following sections all the results are reported for the CCPD parallelization We do not present any results for the PCCD approach since it performs very poorly and results in a speed-down on more than one processor 3  5.2 Aggregate Parallel Performance Table 3 s actual running times for the unoptimized sequential and a naive parallelization of the base algorithm Apriori for 2,4 and 8 processors without any f the techniques descibed in sections 3 and 4 In this section all the graphs showing  improvements are with respect to the data for one processor in table 3 Figure 4 presents the speedups obtained on different databases and different processors for the CCPD parallelization The results presented on CCPD use all the optimization discussed 3 Recall that in the PCCD approach every processor has to read the entire database during each iteration The resulting I/O costs on our system were too prohibitive for this method to be  12 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


