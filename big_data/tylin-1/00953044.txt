Towards XML Metamodel Patterns for XML Data Modeling Zaijun Hu e-mail: zaijun.hu@de.abb.com Speyerer Strasse 4 D 691 I5 Heidelberg ABB Corporate Research Center Germany Abstract XML as an emerging industrial standard markup language is very suitable for representing the meta model that contains data structure data element rela tionship, rules etc In this paper we will present a pat tern-oriented 
approach for building metamodel and defining the basic elements of an XML metamodel pat tern We will also introduce some pattern examples such as the embedding container the self-embedding container, the link, and the Association Finally we will define some rules on how to use those pattern exam ples The use of pattern-oriented approach will simplifi and speed up metamodeling KEY WORDS XML Metarnodel 
Pattern 1 Introduction Metamodeling is a process and an activity to find the most suitable and economical representation of real data on the abstraction level 2 223Metamodeling can result in great savings in development effort and in much better reuse potential.\224[l But it needs much effort for abstraction synthesis and classification To reduce effort experience and knowledge are required The pattern-based approach helps people to collect and abstract the experience, thus to reduce the development 
cost and speedup the metamodeling process According to Alexander 3 the reusable recurrence e.g experience and rules is the essential part of pat terns. Buschmann grouped patterns into three categories of patterns architectural patterns design patterns and idioms 6 Riehle and Zullighoven 7 also define three catego ries of patterns: conceptual patterns design patterns and programming pattern The design pattern and program ming patterns are similar to or as same as design pattern and idioms described by Buschmann 
A conceptual pattern is a pattern whose form is described by means of Gerhard Vollmar e-mail: gerhard.vollmar@de abb.com Speyerer Strasse 4 D 691 I5 Heidelberg ABB Corporate Research Center Germany terms and concepts from an application domain The conceptual pattern covers reusable recurrence on the application domain level The patterns defined by Gamma Buschmann Rihle and Zullighoven can therefore be classified to four layers  conceptual 
patterns architecture patterns de Architectural Pattern Conceptual Patterns Design Pattern Application Patterns Programming Pattern Data Model Patterns Data Instance Model Pattern Data Metarnodel Pattern I Figure 1 Pattern Structure sign patterns and programming patterns In this paper we introduce another layer of pattern  data model pattern to collect experience and knowledge and thus define rules for data modeling It deals with reusability issue in the data modeling area 
The data model pattern contains the data instance model pattern and the data metamodel pattern Data instance models are the in stances of the data mttamodels The data instance model pattern is the model pattern on the instance level while the data metamodel pattern on the metamodel level To better describe the pattern structure we put the three pattern groups  the architecture pattern, the de sign pattern and programming pattern  introduced by Buschmann into one group 
called application pattern because they reflect internal different abstraction levels of a software application and are realized in it. Figure 1 shows the pattern structure 2 XML metamodel pattern XML I21 data modeling is a processing of using basic XML elements to model data. The result of XML data modeling is XML data models that consist of XML 71 1529-4188/01 10.00 0 2001 IEEE 


data instance models and XML data metamodels or in short term, XML instance model and XML metamodel The focus of this paper is the XML metamodel An XML metamodel pattern refers to a rule or a group of rules that describes the way of creating using and com posing the XML elements to establish XML metamod els 3 Elements of XML metamodel pattern We use the basic element defined in 4][5][7][8 and adapt them to the XML metamodel pattern 3.1 Name Each XML metamodel pattern must have a mean ingful name which can be used to refer to the pattern The name must be explainable and understandable A good name will simplify the abstraction activity or modeling activity, improve the communication among modelers between the modeler and application devel oper that uses models because the modelers or devel opers can use the pattern name in their discussion A good name is also a good basis for pattern classification or categorization 3.2 Problem and context The problem describes what shall be modeled or the purpose of the pattern It concentrates on the problem world that shall be described The problem also covers the modeling problems that are encountered by model ers The context describes all issues such as bounding conditions which are related to the problem A clear description of the problem and the context will benefit to the decision 3.3 Additional Use Cases The additional use case describes other typical cases where the pattern can be used A use case can be a structure, a special relationship a special object, a spe cial scenario etc 3.4 Structure The structure of XML metamodel pattern describes the elements that compose the pattern and their relation ships The XML metamodel pattern is built by means of XML elements that can be the basic XML elements or XML elements created by modelers The structure of the pattern tells which elements are used and how the relationships among them look like 3.5 Rules The rules describe the conditions of using the pat tern It concentrates on the technical aspect rather than problem aspect It might contain some modeling con straints that have to be taken into account in the model ing It could also be some considerations coming from the usage and presentation of models or some con straints from modelers or developers The rules help modelers to choose the suitable pattern 3.6 Implementation The implementation describes which XML language is used to realize the pattern and how the pattern is implemented. For example there are two XML schemas defined by W3C 9 and Microsoft 13 Both schemas define the basic elements that can be used to build the metamodel The implementation only shows an example on how the pattern with pre-selected modeling language realized Maybeit shall be adapted for the concrete case 3.7 Consequence The consequences are result of and tradeoffs of applying the pattern 4 They describe the benefits and drawbacks of the pattern They are used to evaluate the pattern in the modeling decision 3.8 Related patterns The related patterns refer to the patterns that are similar to this pattern or can be used for similar pur pose, context or under similar conditions The relation patterns help modelers to evaluate different alternatives so that the most suitable pattern can be selected and used 4 Some XML Metamodel Patterns 4.1 Embedding Container w Embedding Container Problem and context the embedding container is used to model an information element whose instance con tains the instances of other information elements There are two modeling problems here First of all is to decide the direction of the relationship namely which element contains which one The second modeling issue is to decide if it is really necessary to embed an instance of an element into one of another element Additional Use Cases the embedding container can Figure 2 Embedding Container also be used to model information hierarchy or a list that contains a list of information element of the same type Structure the embedding container involves at least two elements Fig 21 the embedding element A and the embedded element B The relationship between A and B can be 1:n Rules the embedding container is or not used under the following conditions 72 


Mandatory constraint IO If instance of element B cannot exist without an instance of element A em bedding B in A is a possible expression of the rela Schema name="EmbeddingContainerSchema xmlns="urn:schemas-microsoft-com:xmldata xmlns:dt="um:schemas-microsoftcom:datatyps CAttributeType name="color dt:type="string CElernentType name="B" content="eltOnlf model="closed ElementType ElementType name="A content="eltOnlf model="closed ElementType cattribute type="colof required="yes element type minOccurs="O maxOccurs Schema Figure 3 Example for the Embedding Container tionship between B and A The part-whole relation ship is an example of such a case Uniqueness If an instance of element B is associ ated with only one instance of element A then em bedding B in A is a possible modeling way If ele ment B is referenced by other elements beside ele ment A or an instance of element B can be refer enced by more than one instance of element A, em bedding B in A could result in redundancy on the instance model level Dependence If instances of element B have to be organized together with the instance of element A the embedding container can be used The embedding container can be used for building a tree structure Each node in the tree structure could be different But it is not suitable for model ing graph-like structure where a child could have more than one parent In this case we have multiple reference mentioned above Possibility Sometimes it is impossible to embed B in A if B has a binary format such as binary file image file etc Imulementation In this paper we use MS XML schema for the implementation of XML metamodel \(Fig 3 Conseauence The embedding container is a simple modeling way But the embedding container could result in redundancy on the level of the data instance model if the embedded element can be associated with other elements beside the embedding element ReIated Pattern Self-Embedding Container Link Container, Association Container 4.2 Self-Embedding Container  Self-Embedding Container Problem and context In modeling we often face tree structure with each node containing an information element of the same type The modeling problem is how to describe the homogenous hierarchical structure Us ing the embedding container could result in a compli cated model system The self-embedding container could be the choice Additional Use Cases The self-embedding container Fiaure 4 Self-Embeddina Container can also be used for modeling linked list Structure The self-embedding container is a kind of the embedding container with an exception that the embed ded element has the same type as the embedding ele ment. Figure 4 shows the structure U the self-embedding container is or not used under the following conditions Mandatory constraint If an instance of element A lives only in its parent or container of the same element type A then the self-embedding container can be used Uniqueness If an instance of element A is associ ated with only one parent then the self-embedding container could be used Otherwise the use of the          Schema name="SeMmbeddingContainerSchema xmlns="urn:schemas-micrasoft-corn:xrnldata xmlns:dt='um:schemas-microsofiam:datatypes CAttributeType name="colof dt:type="string 4lementType name="A content="eltOnlf rnodel="closed ElernentType Cattribute type="wlof required="yes element type="A" minOccurs="l maxOccurs Schema Figure 5 Example for the self embedding container self-embedding container could result in redun dancy on the data instance model level Dependence If instances of embedded element have to be organized together with the parent ele ment, the self-embedding container can be used The self-embedding container is suitable for build ing a tree structure where each node has the same type Imdementation In this paper we use MS XML schema for the implementation of XML metamodel The figure 5 shows an example of implementation Consequence It has the same consequence as the em bedding container but could reduce the complexity of models Related Pattern the embedding container 4.3 Link M Link 73 


Problem and context In modeling there are often cases where an instance is referenced by more than one in stance In order to avoid redundancy and to realize easy management we could use link A modeling decision issue may be which one shall be used to express a refer ence a link element or an attribute in the element that references to another element Another modeling issue Figure 6 Link is to decide what kind of link shall be used, general link or special link that can only be used for referencing special elements Additional Use Cases The link can also be used to introduce a new abstraction layer or a new category in the information hierarchy Structure A link is a reference to an instance of infor mation element That means it has direction A link needs an instance identification to identify the instance it references to Figure 6 illustrates that the element A contains the link element Blink that references the ele ment B U The link is or not used under the following conditions Multiplicity A link should be used if multiple instances of information elements reference to an instance Separate organization if instances of element B shall be organized separately a link element shall be used Extensibility in comparison to attribute the link element is more extensible and stable Specialization there are two kinds of links, general Schema name="LinkSchema xmlns="urn:schemas-microsofl-com:xml-data xmlns:dt="um:schemas-microsoft-com:datatypes CAttributeType name="ElementlD dt:type="string"P CElementType name="B content="eltOnly model="closed ElementType CElementType name="0Link content="eflOnly rncdel="closed ElementType CElementType name="A content="eltOnly' model="closed ElementType eattribute type="ElernentlD" required="yes cattribute type="ElementlD" required="yes element type="BLink minOccurs="O maxOccurs Schema Figure 7 Example for Link link and special link The general link can be used to reference all instances of information elements The special link can be used only for special ele ment If the specialization for the use of the refer enced information element is important then the special link shall be used Navigation search query and sorting: The condi tion is about how to use the information elements It covers the usage aspect of the information mod eling The purpose of information modeling is the use of information Good modeling enables effec tive use of information In the Link shown in fig 7 it is relatively easy to navigate from the element A to the element B But the navigation from B to A will be difficult That needs other mechanism and would increase the modeling effort Imulementation we use MS XML for the implementa tion Fig 7 shows an example for the Link Consequence Using link element could reduce the information redundancy improve the extensibility of models and simplify the use of information But on another hand it could make the model structure complex and complicated and enhance the modeling effort An other weakness of link is that because a link only refer ences an instance it is necessary to get the instance if the instance is required This could require some program ming logic to get the instance and enhance burden on runtime processing Related Pattern association embedding container 4.4 Association  Association Problem and context Sometimes we have difficulty in determining the direction of a relationship For example the husband-wife relationship is such one Does hus band have wife Or does wife have husband Both questions can be answered with yes How to model such a relationship? The embedding container and the link are both used for expressing a directional relationship If they are used to model the husband-wife relationship a redundancy is not avoidable To describe the relation ship we could use the association Additional Use Cases The association is also suitable to describe a group of information elements where each element shall be handle equally It must not be limited to the pair relationship such as husband-wife Structure The association contains at least two ele ments. Each element has the same position in the struc ture We have here different structure alternatives by combining embedding and linking The figure 8 shows the structure Figure 8 Association Rules The association is or not used under the fol lowing conditions 74 


Equality The elements associated through the association have the same position in the modeling process or shall be treated equally Cardinality the relationship among the associated elements shall be 1.1 Schema name="AssociationSchema xmlns="um:schemas-micr~oR-com:xrnl-data xmlns:dt="um:schemas-microsoflcom:datatypes CAttributeType name="color dt:type="string CAttributeType name="position" dt:type="string"l CElementType name="B" content="enOny mcdel="closed IElementType CElernentType name="A content="enOnly model="clased ElementType CElementType name="Association" content="eitOnl element type="A minOccurs="l" maxOccurs="l element type minOccurs="l" maxOccurs="l cattribute type="coloV required="yes cattribute type="position required="yes model="closed ElementType Schema Figure 9 Example for Association Embedding or linking whether the instances of element A and B shall be embedded or linked in the association instance depends on whether rules we have listed in the embedding container and link are true or false Imvlementation The implementation is performed by means of MS XML schema We have implemented an association for the embedding case Figure 9 shows an implementation example for Association Conseauence The association simplifies some model ing cases could reduce unnecessary redundancy and thus the modeling effort Related Pattern Link 5 Conclusions and further work In this work we make the following contributions First we introduce an additional pattern layer data model pattern to address the reusability on the data modeling level Then we define the XML metamodel pattern and its basic elements The most elements can also be found in the publications But we adapt the basic elements of pattern to the special modeling con text and put special semantics related to the data mod eling into the elements so that the XML metamodel pattern can be collected and abstracted specially for the modeling purpose Finally we present some pattern examples These pattern examples have been used in our ongoing.project The purpose of XML metamodel pattern is to col lect to abstract and thus to share the modeling experi ence and knowledge The key for the success of the data model pattern is to create and to publish good patterns that are proven in the practice 6 Reference  11 Hafedh Mili Francois Pachet Ilham Benyahia Fred Eddy Metamodeling in 00 OOPSLA 95 Workshop on Metamodeling in 00 2 D Riehle Position Paper for OOPSLA 99 Workshop 21 on Metadata and Active Object Models 3 C Alexander S Ishikawa MSiverstein M.Jacobson 1 Fiksdahl-King S Angel A Pattern Language Towns Buildings Construction Oxford University Press, New York 1977  Erich Gamma Richard Helm Ralph Johnson John Vlissides Design Patterns Elements of Resuable Ob ject-Oriented Software Addison-Wesley Publshing Company 1995 SI E Gamma R Helm R Johnson J Vlissides De sign Patterns: Abstraction and Reuse of Object-Oriented Design ECOOP'93 LNCS-707 Conference Proceed ings pp 406-43 1 Berlin Heidelberg Springer Ver lag 6 F Bushmann R Meunier H Rohnert P Sommer lad M Stahl Siemens AG Deutschland Pattern oriented Software Architecture A System of Patterns Jon Wley  Sons, Ltd 1996  Riehle Zullighoven Understanding and Using Patterns in Software Development Theory and Prac tice of Object System 2 1 1996 Page 3-13 8 J Vlissides Pattern Hatching  Perspectives from the gang of four C++ Report March-Arpril 1995  XMLSchema http://www.w3 org/XML/Schema lo L,Bird A Goodchild T Halpin Object Role Modeling and XML Schema 19 International Confer ence on Conceptual Modeling, Salt Lake City, Utah I I Teny Halpin Modeling Collection in UML and ORM Proc. EMMSAD 00 5'h IFlP WG8.1 Int. Work shop on Evaluation of Modeling Method in System Analysis and Design ed K Siau Kista Schweden June  XML 1 O http://www.w3.org/XML  131 XML Schema http://msdn.microsofi.com/xml 75 


Generalisation The attribute value has heen replaced by the higher level concept that retains the same set of characteristics hut possibly with larger support Contradiction The attribute value is in contradiction with another potentially pre-defined as a belief For example auser may he allocated a parlicularcomputer but the profile indicates the use of a different one The recognition of the occurrences of these differences may be automated Some may be combined repetition or discarded as unimportant generalisation or trivial beliefs Others may require inspection by investigators to decide if they are worth following up. Algorithm 2 Irem$er itemxet fame describes the calculation of a metric that indicates the closeness or similarity of two k-itemsets by comparing their elements It employs the attr function defined prior to presenting Algorithm 1 and assumes that the itemsets to be compared are represented by bits from the hit-vector U format defined there Because of the consecutiveness re quirement it follows that the hits at position o in a k-itemset he in three distinct relationships 1 They may belong to different attnbutes A  A 2 They may belong to the same attribute A and he the same attribute value or concept or have a child-parent relationship 3 They may belong to the same attribute A but he dif ferent valueslconcepts with no relationship Algorithm 2 IS2IS-dist Inputs K-itemsets I  b     bi and Ij  g  q an m x m concept relationship bit-matrix C attribute function attr\(b Outputs Distance d E 0     k  11 1 Initialised  0 2 Foro  1 to k 2.1 If attr\(bp  by set d  k  1 and stop 2.2 If bp  b A b not in child-parent relationship with by increment d It can be seen that distance d of Algorithm 2 can he less than the length of the itemset k only if the same attribute valuelconcept is found duplicated \(i.e. equals or is in a con cept relationship with at least once in the two itemsets he ing compared It also follows that the total number of such duplicates found and d equals k with d  0 only if the re spective elements of the two itemsets are the same or are in a concept relationship Thus the metric is a non-negative in Figure 2 Example time slice of past and cur rent user login information as obtained by ex ecuting the UNIX last command hold nil values for attributes not originally in the itemset then comparing this itemset with the data record the same way as comparing two I-itemsets The difference between Algorithm 2 and this modified version is that d is not in cremented for attributes where the itemset holds a nil value This limits d to a maximum value of k 5 Data Experiments and Results To evaluate the profiling methodology proposed in this paper, a number of experiments have been performed Both Algorithms 1 and 2 have been implemented as well as IS2DAT-dist As input log files captured by executing the UNIX last command were used which searches the wtmp system log file and lists past and current user login informa tion for a computer An example output from executing the last command is shown in Figure 2 Note that the data used in our experiments are actual log data recorded by a UNIX-based computer set up as a server with remote login access However in order to preserve anonymity the data attribute name instances have been modified Furthermore there was no implication of inappropriate behaviour in the data set Of several columns of information generated six at tributes were copied or composed into a table containing formatted input. Some filtering was performed at this stage to remove incomplete current and non-user e.g shut down logins The table, using additional higher level con cepts from attribute hierarchies was then mined to produce a profile containing association rules. Intra-profile and data to-profile contrasting was then performed The distance metric of IS2IS-dist and IS2DAT-dist was employed to produce reports for both contrasting methods 5.1 Intra-Profile Experiments Intra-profile contrasts were calculated only for itemsets teger 2 E 0    k 11 from which only values 0 d  k    of the same length For example in one test from about 2000 original data records approximately 2200 itemsets are or interest A similar algorithm can be devised to calculate the distance hetween a k-itemset and a data record IS2DAT with than element were produced Intra-profile con dist This can he achieved by expanding the itemset trasting produced roughly 43000 distances that were less This algorithm is not presented due to its similarity to AlgonUlm 2 than the lengths of the itemsets being compared Although 16 


this is a far smaller number than what it potentially could have been it is still more than what can be perused manu ally To rcduce this set further, additional strategies need to be devised One option is to prioritise attributes That ih if difference is measured only in a particular attribute that may not be carrying imponant information \(such as day of the week then pairs exhibiting distance only in such at tributes may be dropped Similarly a strategy may be em played to drop contrasts that are too high That is the distance metric for a particular itemset length may be re garded as high even though it satisfies the initial constraint of being less than the length This may for example render all distances produced for 2-itemsets unnecessary Finally focusing techniques may be provided to filter the distances for certain attributes or attribute values One of the more interesting contrasts produced by IS2IS-dist during testing was the I-distance pair io  User  pedru A Origin  viiunli 11  User  pedro A Origin  adeluide which indicates that the same user has been logging in from two very different geographic locations Further inspection of this contrast revealed that the user in question left his place of work in Adelaide for another in Miami while still regularly accessing his old Adelaide account 5.2 Data-to-Profile Experiments The filtering requirement to reduce the set of distances to manageable proportions becomes even more evident with data-to-profile contrasts Without pre-processing each itemset needs to be compared to every data record po tentially producing a much larger result set than for intra profile contrasts This is partly due to the fact that a number of records are not included in the profile due to unsatisfac tory support Each of these records could produce small dis tances to itemsets similar to it that made it into the profile As in the case of intra-profile contrasts measures can be taken to reduce the final result set In addition to the strate gies outlined in Section 5.1 duplicate records may be re moved by post-processing the results Also data-to-protile distances may be zero if a particular data record was one of those used to generate the itemset it is being compared to These distances should also be pruned from the results Figure 3 shows some of the distances from a test calcu lated for a particular itemset of length 5 top row Non zero distances up to a maximum value of 2 were allowed in order to list contrasts where difference is present in not too many attributes Duplicates were removed and as men tioned some attributes were sanitised to remove contiden tial information from the data The itemset contains gen eralised concepts for both the User and Origin attributes while Durarion is represented by concepts categorising a Figure 3 Example data-to-profile distances from ISZDAT-dist for a sample profile element and a collection of data records, ordered by User for readability potentially large number of discrete values From the def inition of the metric valueslconcepts in the same hierar chy have a distance of zero, which explains the diversity of rows of \(non-generalised values in the data having similar distances For readability we give here some of the concept relationships from the otherwise rather large hierarchies that exist for User and Origin mar,milo,pedro stuort c lecturer cs.x?/u.edu.au 188.191.47 c cs.xyu.edu.au C zyu.edu.au c adelaide  tnt2.tow.net.au 198.twun0103.twn.net.ou C ISP.adelaide c adelaide Using this information the first data row with d  1 shows that user Clyde is not a lecturer whilst for lecturers viuz and pedru who log onto university computers we can ob serve that the same wtmp login information is valid for sev eral weekdays other than Monday Figure 3 as is contains superfluous information De pending on the support used in mining the profile some or most of the data records contribute to itemsets gener ated by the algorithm Comparing a k-itemset to data that contributes to another k-itemset is a repetition of compar ing an itemset with another Crosschecking a data record against every other k-itemset prior to calculating a distance would, however, be even less cost-effective Instead a strat egy of producing distances in a matrix form for k-itemsets k  2   1 then discarding rows with at least one zero in it would be a better solution Alternatively a separate algorithm may parse the data set to locate individual occur rences of records that do not contribute to any itemset of a given length and then run the contrasting algorithm against this filtered data set only This is indeed the requirement proposed in Section 4.2 for data-to-profile contrasting 17 


6 Conclusions and Future Directions The initial implementation of the profiling analysis pro cess described in this paper has resulted in promising results capable of identifying irregularities in computer logs that can serve as useful evidence in computer crime investiga tions Protile analysis, however forms only a pan of the in vestigative process and relies heavily on expert knowledge It is therefore best perceived as a component in a larger col lection of tools designed to aid the forensic investigator The profiling tool presented in this paper presents further opportunities for enhancement One such area is the han dling of multiple log information in a single process Multi dimensional mining may offer a solution for this problem with some interesting work already found in the literature 16 201 Alternatively it may be possible to 223flatten\224 sev eral logs into a sequence of 223events\224 for which more tradi tional sequential mining techniques can be applied Further improvements may be achieved by replacing the mining 81 gorithm used in protiling One obvious candidate is the attribute-oriented induction technique  141 This technique compacts a collection of records into a generalised relation or a conjunction of generalised records where individual at tribute values are replaced by higher level concepts by as cending concept hierarchies One of the advantages of this technique is that the final rule set incorporates information about every record in the original data set Further work is also to be carried out in the intelligent presentation of results notably in the provision of appropriate visual inter pretation of the profiles and its potential contrasts. Contrast measures currently used are itemset-specific Deriving dis tance measures for rules such as the value distance metric VDM 221 may yield better results in identifying discrep ancies Some of the better known data mining interesting ness measures 12 or variations of may also be adopted for this purpose References I G Adomavicius and A Tuzhilin Expert-driven valida tion of rule-based user models in personalization applica tions Data Mining and Knowledge Discovery 5\(1/2 58,2001 121 G Adomavicius and A Tuzhilin Using data mining meth ads to build customer profiles Computer 34\(2 2001 3 C Aggamal Z Sun and P Yu Online algorithms for find ing profile association rules In Proceedings of the ACM Internatinno1 Conference on Informorion and nowledge Management CIKM-98 Bethesda MD USA 1998  R Agrawal T Imielinski and A Swami Mining associ ations between sets of items in massive databases In Pro ceedings ofthe ACM SIGMOD hi Conference on Manage ment ofDara Washington, DC USA May 1993 SI R Agrawal H Mannila R Srikant H Toivonen and A Verkamo Advances in Knowledge Discovery and Data Mining chanter Fast discoverv of association rules AAA1 224 Press 1996 161 E Casev Dipifal Evidence and Computer Crime Academic  Press 2w0.\221 171 P K Chan A non-invasive learninx amroach to building   224 web user profiles In Proceedings of the Workshop on Web Usage Analysis and User Profiling WEBKDD\22299 1999 8 0 de Vel A Anderson M Corney and G Mohay Mining e-mail content for author identification forensics SIGMOD Record 30\(4 2001 191 M Ester H.-P Krieeel J Sander and X Xu A densitv  based algorithm for discovering clustc~s in large spatial databases with noise In Proceedings of the Second Ini Con ference on Knowledge Discovery ond Dam Mining 1996 IO T Fawcett and F Provost Adaptive fraud detection Data Mining and Knowledge Discovery 1\(3 1997 1111 J Han and Y Fu Discovery of multiple-level assmiation rules from large databases In Proceedings of2lst VLDB Conference September 1995 I21 R 1 Hilderman and H I Hamilton Knowledge discovery and interestingness measures A survey Technical Repon CS-99-04 Dept of Computer Science University of Regina 1999 I31 M Hirsh, C Basu. and B Davidson Learning to personal ize Communicarions ofthe ACM 43\(8 2ooO I41 H 1 Y Cai and N Cercone Knowledge discovery in databases an attribute-oriented approach In Proceedings ofl8th hr Conference on Very Large Databases 1992 I51 1 Konstan B Miller D Malte J Herlocker L Gordon and 1 Riedl Grouplens Applying collaborative filtering to usenet news Communications ofthe ACM 40\(3 1997 Beyond intra-transaction assmiation analysis mining multi-dimensional inter transaction rules ACM Transactions on Information Sw I61 H Lu L Feng and J Han terns 18\(4 2000 1171 B Mobasher H Dai T Luo Y Sun and J Wiltshire  Discovery of aggregate usage profiles for web personaliz tion In Proceedings ofthe Workhop on Web Mining for E-Commerce WEBKDD\222OO August 2000 IS A Nanopoulos D Katsaros and Y Manolapoulos Ef fective prediction of web-user accesses a data mining ap proach In Proceedings of the Workshop on Mining Logdata Accross All Customer Touchpoints WEBKDD\222OI 2001 I91 S Nesbitt and 0 de Vel A collaborative filtering agent system for dynamic virtual communities on the web In Proceedings of the Conference on Learninp and Discoven CONALD98 June 1998 1201 T Oates and P R Cohen Searchine for structure in multiole I streams of data In Proceedings of the Thirteenth Interno rional Conference on Machine Learning 1996 ZI R Srikant and R Agrawal Mining generalized assmiation rules In Proceedings ofZlsr VLDB Conference 1995 221 C Stanfill and D Waltz Toward memory-based reasoning Communications of the ACM 29\(12 1986 23 P N Tan and V Kumar Mining indirect assmiations in web data In Proceedings oflhe Workshop on Mining Logdata Accross All Customer Touchpoints WEBKDD\222OI 2001 18 


Category Manual Automatic No of associations 63 30 No of rules 330 44 Max association size 6 4 Avg support 0.45 0.43 Avg rule con\256dence 0.80 0.82 Table 1 Manual versus automatic image content mining 4.2 Quality of results We should mention that there were no false association rules It did not happen that an object was incorrectly identi\256ed and then a rule was generated with the incorrect identi\256er In general when we found a match between two objects they were the same shape All the incorrect matches are 256ltered out by the support parameter and then the association rules are generated for objects correctly ideinti\256ed Also some redundant matches happened b ecause of the blobs that represented several shapes but these matches are 256ltered out by the rule support In Table 1 we present a summary of our experimental results with 100 hundred images We compare the results obtained by manually identifying objects in each image and then generating association rules from such identi\256ers Manual Column against the results obtained by our current implementation Automatic Column Ideally our image mining algorithm should produce the same results as the manual process So the table gives a standpoint to assess the quality of our experimental results For these 100 images unwanted matches either incorrect or involving many objects happened in at most 4 images and therefore their support was well below the minimum support frequency which was at 30 These experiments were run using the same parameters for object identi\256cation as in our small example with 10 images The parameters for object identi\256cation had the following values We set color standard deviation to 0.5 contrast standard deviation to 0.5 and anisotropy also to 0.5 The similarity threshold as needed by the similarity function was set to 0.6 We tuned these parameters after several experiments These parameters maximized the number of associations and decreased the errors in unwanted matches The association rule program was set to look for rules with a 30 support and 70 con\256dence The background represents an object itself Since association rules with the background were not interesting for our purposes it was eliminated from consideration by the object identi\256cation step It is important to note that this is done after objects have been identi\256ed We tuned the object identi\256cation step to 256nd similar objects changing values for several parameters in the following manner The most important features used from each object were color and contrast We allowed some variance for color 0.5 and the maximum allowed variance for contrast 0.5 The anisotropy helped eliminate matches involving several geometric shapes We ignored shape b ecause objects could be partially hidden and rotated Position was considered unimportant because objects could be anywhere in each image Anisotropy and polarity were i gnored because almost all our shapes had uniform texture Area was given no weight because objects could be overlapping and thus their area diminished this can be useful to make perfect matches when objects are apart from each other A few rules had high support One problem that arose during our experiments was that the same shape could have two different blob descriptors and these blob descriptors could not be matched with two other descriptors for the same shape in another image This caused two problems First a rule could be repeated because it related the same shapes Second a rule did not have enough support and/or con\256dence and therefore was discarded So the rules found were correct and in many cases had an actual higher support and also higher con\256dence To our surprise in some cases there were no object matches because an object was very close to another one or was located in a corner of the image When two or more objects were overlapping or very close they were identi\256ed as a single object This changed the features stored in the blob The problem was due to the ellipsoidal shape of the blobs and the fact that when a geometric shape was located in a corner thta changed its anysotropy and polarity descriptors Given a blob for an object very close to one corner means determining an adequate radius for the blob i.e ellipse Regular shapes such as the triangle square and hexagon were easily matched across images This is a direct consequence of the circular blob representation produced when the image is segmented In this case neither position nor rotation affect the mining process at all It was surprising that in some cases there were no matches for the circle in these cases it was in a corner or some other shape was very close or overlapping Another important aspect about shape is that we do not use it as a parameter to mine images but shape plays an important role during the segmentation step So shape does affect the image mining results quality The rectangle and the ellipse are the next shapes that are easily matched even though we did not use the shape feature The most complicated shape was the L In this case a number of factors affected matches When this shape was overlapped with other shapes a few matches were found b ecause a big blob was generated Also orientation changed dominant 


ofimages 50 100 150 200 1 feature 50292 80777 127038 185080 2 obj identif 210 338 547 856 3 aux image 3847 6911 10756 13732 4 assoc rules 6 3 6 4 Table 2 Measured times in seconds for each Image Mining step with different image set sizes colors and contrast When the L was close to another shape its colors were merged making it dissimilar to other L shaped objects This suggests that irregular shapes in general make image mining dif\256cult We worked with color images but it is also possible to use black and white images Color and texture were important in mining the geometric shapes we created However we ignored shape as mentioned above Shape may be more important for black and white images but more accurate shape descriptors are needed than those provided by the blobs 4.3 Performance evaluation We ran our experiments on a Sun Multiprocessor forge.cc.gatech.edu computer with 4 processors each running at 100 MHz and 128 MB of RAM The image mining program was written in Matlab and C The 256rst three steps are performed in Matlab The feature extraction process is done in Matlab by the software we obtained from UCB Object identi\256cation and record creation were also done in Matlab by a program developed by us An html page is created in Matlab to interpret results The association rules were obtained by a program written in C In this section we examine the performance of the various components of the image mining process as shown in Table 2 for several image set sizes These times were obtained by averaging the ellapsed times of executing the image mining program 256ve times 4.4 Running time analysis Feature extraction although linear in the number of images is slow and there are several reasons for this If image size increases performance should degrade considerably since feature extraction is quadratic in image size Nevertheless this step is done only once and does not have to be repeated to run the image mining algorithm several times Object identi\256cation is fast This is because the algorithm only compares unmatched objects and the number of objects per image is bounded For our experimental results time for this step scales up well Auxiliary image creation is relatively slow but its time grows linearly since it is done on a per image basis The time it takes to 256nd rules is the lowest among all steps If the image mining program is run several times over the same image set only the times for the second and the fourth step should be considered since image features already exist and auxiliary images have already been created 5 Application Image mining could have an application with real images The current implementation could be used with a set of images having the following characteristics 017 Homogeneous The images should have the same type of image content For instance the program can give useless results if some images are landscapes other images contain only people and the remaining images have only cars 017 Simple image content If the images are complex they will produce blobs dif\256cult to match Also the association rules obtained will be harder to interpret A high number of colors blurred boundaries between objects large number of objects signi\256cant difference in object size make the image mining process more prone to errors 017 A few objects per image If the number of objects per image is greater than 10 then our current implementation would not give accurate results since Blobworld in most cases generates at most 12 blobs per image 017 New information The image itself should should give information not already known If all the information about the image is contained in associated alphanumeric data then that data could be mined directly 6 Future Work Results obtained so far look promising but we need to improve several aspects in our research effort We are currently working on the following tasks We also need to analyze images with repeated geometric shapes If we want to obtain simple association rules this can make our program more general This can be done without further modi\256cation to what is working However if we want to mine for more speci\256c rules then we would need to modify our algorithm For instance we could try to 


produce rules like the following if there are two rectangles and one square then we are likely to 256nd three triangles The issues are the combinatorial growth of all the possibilities to mine and also a more complex type of condition We will also study more deeply the problem of mining images with more complex shapes such as the irregular one similar to the letter L We need a systematic approach to determine an optimal similarity threshold or at least a close one A very high threshold means only perfect matches are accepted On the other hand a very low similarity threshold may mean any object is similar to any other object Finding the right similarity threshold for each image type l ooks like an interesting problem Right now it is provided by the user but it can be changed to be tuned by the algorithm itself Also there are many ways to tune the eleven parameters to match blobs and the optimal tuning may be speci\256c to image type There also exists the possibility of using other segmentation algorithms that could perform faster or better feature extraction It is important to note that these algorithms should give a means to compare segmented regions and provide suitable parameters to perform object matching in order to be useful for image mining From our experimental results it is clear that this step is a bottleneck for the overall performance of image mining We can change the object identi\256cation algorithms to generate overlapping object associations using more features Our algorithm currently generates partititons of objects that is if one object is considered similar To another one the latter one will not be compared again By generating overlapping associations we can 256nd even more rules For instance a red rectangular object may be considered similar to another rectangular object and at the same time be similar to another red object Mining by position is also possible for instance two objects in a certain position may imply another object to be in some other position Since the software we are using for feature extraction produces eleven parameters to describe blobs we have 2 11 possibilites to match objects 7 Conclusions We presented a new algorithm to perform data mining on images and an initial experimental and performance study The positive points about our algorithm to 256nd association rules in images and its implementation include the following It does not use domain knowledge it is reasonably fast it does not produce meaningless or false rules it is automated for the most part The negative points include some valid rules are discarded because of low s upport there are repeated rules because of different object id's unwanted matches because of blobs representing several objects slow feature extraction step a careful tuning of several parameters is needed it does not work well with complex images We studied this problem in the context of data mining for databases Our image mining algorithm has 4 major steps feature extraction object identi\256cation auxiliary image creation and identi\256ed object mining The slowest part of image mining is the feature extraction step which is really a part of the process of storing images in a CBIR system and is done only once The next slowest operation is creating the auxiliary blob images which is also done once Object identi\256cation and association rule 256nding are fairly fast and scale up well with image set size We also presented several improvements to our initial approach of image mining Our experimental results are promising and show some potential for future study Rules referring to speci\256c objects are obtained regardless of object position object orientation and even object shape when one object is partially hidden Image mining is feasible to obtain simple rules from not complex images with a few simple objects Nevertheless it requires human intervention and some domain knowledge to obtain better results Images contain a great deal of information and thus the amount of knowledge that we can extract from them is enormous This work is an attempt to combine association rules with automatically identi\256ed objects obtained from a matching process on segmented images Although our experimental results are far from perfect we show that it is better to discover some reliable knowledge automatically than not discovering any new knowledge at all Acknowledgments We thank Chad Carson from the University of California at Berkeley for helping us setup the Blobworld system We also thank Sham Navathe and Norberto Ezquerra for their comments to improve the presentation of this paper References 1 R  A g r a w a l  T  I m i e lin s k i a n d A  S w a m i  M in in g a s s o ciation rules between sets of items in large databases In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data  pages 207\261 216 Washington DC May 26-28 1993  R  A gra w a l a n d R  S ri ka nt  F a s t a l gori t h m s for m i n i n g association rules in large databases In Proceedings of the 20th International Conference on Very Large Data Bases  Santiago Chile August 29-September 1 1994  S  B e l ongi e  C Ca rs on H  G r e e n s p a n  a nd J  Ma lik Recognition of images in large databases using a learning framework Technical Report TR 97-939 U.C Berkeley CS Division 1997 


 C  C a r s on S  Be l ongi e  H  G r e e n s p a n  a nd J  Ma l i k  Region-based image querying In IEEE Workshop on Content-Based Access of Image and Video Libraries  1997 5 G  D u n n a n d B  S  E v e r itt An Introduction to Mathematical Taxonomy  Cambridge University Press New York 1982  U  F a yya d  D  H a u s s l e r  a nd P  S t orol t z  M i n i n g s c i e n ti\256c data Communications of the ACM  39\(11\51\26157 November 1996  U  F a yya d G  P i a t e t s k y-S h a p i r o a n d P  S m y t h  T he kdd process for extracting useful knowledge from volumes of data Communications of the ACM  39\(11\:27\261 34 November 1996 8 D  F o r s y t h J M a l i k  M F l e c k H G r e e n s p a n  T L e ung S Belongie C Carson and C Bregler Finding pictures of objects in large collections of images Technical report U.C Berkeley CS Division 1997  W  J  F ra wl e y  G  P i a t e t s k y S ha pi ro a nd C J  Ma t h e u s  Knowledge Discovery in Databases  chapter Knowledge Discovery in Databases An Overview pages 1 261 27 MIT Press 1991  V  G udi v a da a n d V  R a gha v a n Cont e n t ba s e d i m age retrieval systems IEEE Computer  28\(9\18\26122 September 1995 11 R  H a n s o n  J  S t u t z an d P  C h ees eman  B ay es i a n c l a s si\256cation theory Technical Report FIA-90-12-7-01 Arti\256cial Intelligence Research Branch NASA Ames Research Center Moffet Field CA 94035 1990  M H o l s he i m e r a n d A  S i e be s  D a t a m i ni ng T h e search for knowledge in databases Technical Report CS-R9406 CWI Amsterdam The Netherlands 1993  M H out s m a a nd A  S w a m i  S e t ori e nt e d m i ni ng of association rules Technical Report RJ 9567 IBM October 1993  C O r done z a nd E  O m i e c i ns ki  I m a ge m i ni ng A new approach for data mining Technical Report GITCC-98-12 Georgia Institute of Technology College of Computing 1998  J  R Q u i n l a n Induc t i o n o f d e c i s i on t r e e s  Machine Learning  1\(1\81\261106 1986  A  S a v a s e re  E  O m i e c i ns ki  a nd S  N a v a t h e  A n e f 256 cient algorithm for mining association rules In Proceedings of the VLDB Conference  pages 432 261 444 Zurich Switzerland September 1995  O  R Z a i a ne  J  H a n  Z  N  L i  J  Y  Chi a ng a n d S Chee Multimedia-miner A system prototype for multimedia data mining In Proc 1998 ACM-SIGMOD Conf on Management of Data  June 1998 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


