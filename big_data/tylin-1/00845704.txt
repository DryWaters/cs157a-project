The Use of an Association Rules Matrix for Economic Modelling Raouf Velicv Alex Rubinov and Andrcw Stranicri School of Information Technology and Mathematical Scienccs University of Ballarat Victoria Australia Donald Rerman Laboratory for Information Technology and Law Department of Computer Science and Computer Engineering La Trobe University Bundoora Victoria Australia E-mail eI~ev~kra~s~,~~~~ar~t edu air amr@ballarat edu au stranier@lnlcsl cs lutrobe edu.auj Abstract Knowledge 
discovery techniques have not becn widely used with macroeconomic data despite the social and political importance inherent in accuratc economic forecasting Rather economic forecasting is currently performed with the use of models that rely heavily on theoretical assumptions Economic assumptions are invariably contentious and model predictions are often rejected or accepted based on subjective perceptions about assumptions We present an application of KDD that generates a forecasting model that avoids economic assumptions by 
focussing cntirely on existing data Although association rules are typically used for finding interesting patterns in data this is not a strategy wc employed Our approach differs in that all possible association rules between variables representing the current state of an economy in a quarter and the state in the next quarter are generated to form a matrix A metric based on the support confidence nnd expected probability of each rule 
is then derived The system has been used to perform economic analyses of existing and future government policies The system has bcen developed using data from the US economy over the last thirty years 1 Introduction A national economy is such a complex object of study that direct analysis is not possible Therefore, economists havc limited research to the study of models of economies However the development of a macroeconomic 
model is heavily dependent on a researcher's own subjective view of thc world and theorctical background and beliefs For example hypotheses generated by researchers who accept Keyncsiari assumptions are quite different from hypotheses from Classical theorists Hypotheses are not only dependent upon the subjective beliefs of their creators but can easily become obsolete Completely different economic systems can emerge in different times in different countries and be described by different models Thus 
if making assumptions and deriving hypotheses about an economy leads to subjective models and successful theories do not last long then the following questions arise Is it possible to eliminate niodel dependence on the subjectivc researcher's assumptions about featurcs and properties of thc object of study Can there exist an approach that auitornatically generatcs B hypotlietical basis for constructing a model Can this approach be applied in different times to different types of economic 
systems In this paper we suggest an approach that uses domain data only Our approach is based on the technique of deriving association rules Association rules AR wcre introduced by 2 and still attract the attcntion of many computer science researchers 7 lo but to date 110 attempt has been made to apply AR to inacroeconomic dfita Association rules are typically used to find interesting rules between attributes in a 
largc database Here we use AR to generate a data structure we call an Associution Mufrix AM The Association tnatrix is a simple model that is automatically generated from macroeconomic data without the need for any background assumptions We have trialed the Association Matrix using macroeconomic data from the US cconomy Rcsults indicate the approach is suitable for economic policy analysis 0-7803-58714/99/$10.00 0 1999 JEEE 836 


The Association Matrix is described in the next section Following that we describe the application of our approach with US data and then offer concluding remarks and future research  2 Association Matrix Our approach is based on only one general assumption that the current state of ai1 economy determines its state in the next period of time This assumption seems reasonable if we take into account two arguments weights of a system's impact on each macroeconomic variable However to estimate the system of equations wc must assume thc functional form of this dependence because the exact form is unknown The adoption of one fuiictional form over another is a very strong assumption about the nature of the dependence and is best avoided 6 Furthermore low quality data and the presence of noise can also significantly distort the picture of dependence This especially applics to rnacrocconomic data which reflects a high level of data aggregation To avoid with these difficulties we fonnalise the mapping 1 into a set of Association rules  A macroeconomic system typically IF variuble\(i cquals value\(k changes slowly If thc intervals of time when observations are made is small enough THEN vuriabb cquals vcrluefl 3 quarter or month then it is unlikely that huge changes in the state of an cconamy will bc that noticed Therefore we expect macroeconomic variables will predict values in the next period of time The degree of certainty of this rule be defined by confirmed cases in the past using the rnetrics support confidence and expected predictability Each state of the cconomy represents the result of numerous activities of many different economic agents participating in economic life Most agents form cxpectatioris about the future that influcncc their behaviour This assumption is broadly accepted in cconornic modelling lhesc types of expectations are called adaptive expectations 3 4 SI  The problem of forecasting the value of variables in the next timc period converges to determiiiiiig L mapping F Y\(l  Y\(r-I-1 1 where Y is a set of imcroeconomic variablcs in period of time t It is necessary to find the contribution of each variable in this mapping The question arises Why not use The interestingness of milied rules is typically defined by a ininirnuin support and confidence of mined rules However we do not set a minimum value for support and confidence in generating association rules but instead generate all rules possible These rules are used to construct an association matrix The rows of AM represent the IF parts of association rules The columns rcfIect the THEN parts Each variable of the system and each value from a set of values for the variable define a ccll I-Icncc the intersection of row vlrriah le 4 vulue\(4 and colnrnn variable 0 value reflccts thc rule 3 This ccll stores a coefficient representing the degree of certainty of the ruIe 3 The coefficients for each rule consequently for each cell in the matrix are obtaincd in the same way statistical techniques to estimate this mapping such as those employed in 5 We might have built a system ofequations The degree of certainty of a rule can be calculated in various ways For our purposes a coefficient that represcnts a degree of certainty 837 


probability of appearance of this dependence in the data 2 It must be proportional to the confidence of a rule because the confidence value determines the trueness ofa nile 3 It must be inversely proportional to the expected predictability of a rule Thc expectcd predictability is the frequency of occurrence of the THEN items of the rules The higher the expected predictability the less significance the rule has the morc likely the THEN item happens anyway and less likely it depends on the IF item The assaciation matrix represents tlie hypotheses for our future model and can be automatically generated for any macroeconomic system in any country It docs not depend on the modeller's personal assumptions about thc objcct's natturc or its behaviour arid it can be fairly easily calculated in the next section a description of a simple modcl which we can build on thc basis of this matrix is given 3 Example of a modcl In this section we develop a siniplc macroeconomic model which is based on the association matrix dcscribed above The suggcsted model can be written in the following form where A is the association matrix X represents a vector of valucs which are degrees of certainty for each variable of the system to hold each value from the set of possible values for this vwiable These values represents the trueness of the IF part of the association rules as illustrated in Figure I For example we see from this Figure that thc degree of certainty that variable I will have value I at time T-tl if it had value 1 at time t is 35.5 Thus, knowing the degree of certainty for each value iu the current period of time for each variable in the system we inilltiply them by coefficients of association matrix Then we summarise them by columns to obtain the accumulated dcgrce of certainty of each value in the iicxt period of time for each variablc representing the THEN parts of AR in columns These values of degree of certainty form a vector X\(t+l and we start tlie process again Provided the period of time for measuring the variables is relatively small say a month or a qiiarter for macroeconomic indicators we can assume the linearity of the model reflected in the matrix transition II I I I I t I I Figure 1 Association matrix The described model is able to generate thc next state of the ccoiiomy on the basis of its current state Thus having determined the initial point we can recursively cvahatc thc ncxt point and construct a chain-trajectory of economic system devalopment The forecasting model consists of the following blocks 1 The input which roprosents the vector of the degree of certainty that variables will hold particular vaIucs in the current period of time 2 The association matrix which represents the parameters of the model based on the derived association rules between current and the next states 3 The Output which reprcsents a vector of the degree of certainty that variables will hold particular values in the next period of time 838 


Variables that reflect a government\222s economic policies cannot be predicted It is not appropriate to try to predict them because they can be changed at any time by government decree The set of these variablcs is represented in the Control lock of the system and can be manipulated by the user for scenario testing This block allows the user to test different ecorioinic policy decisions and perform 223what if\221 scenarios According to different values of the control variables of the Control Block different economic devcloprnent trajectories call be generated and the conseqiiences of these decisions can be compared and analysed The next section describes an implementation of this model to the economic system of the USA 4 Implcrnentation of the model In this section we described the data used for the implementation of the model how the association rules have been obtaincd and thc structure of thc forccastiiig model applied to the economic system of thc USA 4.1 Data The data chosen for our experiment was obtained from the Federal Rescrvc Economic Datn which is supported by The Federal Reserve Bank of Saint Louise The total number of variables is 412 Thc records are made on an annually quarterly or monthly basis depcnding on a particular variable The records report economic indicators between 1900 and 1998 However the values for some economic indicators were not collected for the entire period The goal of data pre-processing was to prepare a data sample with variables recorded for the same period of time and with the same frequency The largest possible samplc we could obtain contained 100 variables recordcd from 1960 until 1997 quarterly The most common form of trajectory for the development of most macroeconomic variables in thc short-run is similar to a monotonically increasing or dccrcasing function and can be fairly well approximated by a linear function In order to capture more complex underlying dependencies in an economy we substituted values of the variables far the change from one quarter to the next For each variable we fourid thc minimum and maxiinurn value of change in percentage and divided thc intervals into four sub-intervals Tho final stage of data transformation was to classify the value of change of each variable into the appropriate sub-interval category This allowerl us to reduce the set of possible values of the variables to the sot of one two three four which was done in order to map a 100 dimensionnl Quantitativc Rules problem to a Boolean Rules problem Any algorithm for finding Boolean Association Rules could then be applied lo In our application 80 variables werc chosen to represent the Input block while the remaining 20 were allocated for the Control black of the systcm 4.2 Deriving association rules To derive association rules we used the 223Mineset\223 software package on a UNIX platform The standard Apriori algorithm  13 has been used to gcncratc rubs Wc prepared two tables the second table was the same as the first one except that the first record was missing It allowed us to generate rules which predict variables horn the next HI statc of the system on the basis of the currcnt I state In order to reflect all possible dependencies in the data the threshold far support and confidcnco has been sct to the minimum value  1.00  It allowcd us to obtain almost all possible rules The total number of rules exceeded 57,000 Thc significancc of each rule has been presented by thrcc indicators confidence support and expected prcdictability 4.3 Forecasting Model Once the association rules were obtained the support confidence and cxpcctcd predictability of rules need to be combined in order to provide a value for each association matrix cell The three metrics are combined in this study by simply taking their average Thus the 839 


association matrix cell value represents the association between a value on a a variable and a Value on another variabel find is Tlie association matrix derived model was tested calculated from the association rule A as with data from the first two quarters of 1998 5 Results Only those rules which have confidencc or support value less than I have null vaIiic in the corresponding cell For example the variable v51 has the value of one" only once in the data set therefore at Ieast tlie support valuc of all rules which havc v5 I in their IF or THEN parts will be less than 1 Thus they do not satisfy the threshold value and Mincsct will not show those rules The output values of the system are evaluated by multiplying the input vector by tlie association matrix Hence for each vahic of each variable in the next quarter wc obtain the value which represents the accumulatcd influence of the entire system on this variable Although this number is not the cxact probability of this value it rcprcsents a degree of ccrtainty that this variable will have this valuo in the next period of time The degree of certainty varies from zero to 400*200/3 when the support and confidcncc equals 100 and expected predictability equals 0 for each rule for this variable Zero indicates that it is impossible for the variable to hold the value Thc value 400*200/3 indicate that this variable will definitely have this value We normalise this numbers to the interval 0,l to kecp a balance between input and output This vector is transfcrred to the spreadsheet, which reprcscnts thc next quarter forecasting system Before proccssing the next quarter we pick up the value of each variable with the greatest value of the degree of certainty These values generate the current point in the trajectory of development of the economic systcrn Then the process starts from the beginning SO iiidicntors were correctly predicted The remaining 25 indicators though not corrcctly predicted were still close to tlie real data Most of these variablcs wcrc from the financial group and price indexes Many changes have recently occurred within sector due to the influence of changing global conditions Thcreforc possible inaccuracies in these predictions resulted because we didn't include the world's economic description in our data set Fuhire research aims to test our approach with an expanded data set that capture global indicators in addition to national indicators Simulations of the system under different control regimes have been performed Results wcrc largely consistent with some existing macroecoiiotnic models For example the impact of changes in taxation levcls on gross domestic product and employment is consistent with the main propositions of Kcyncsian General Theory The impact of interest rates on consumption and gross domestic product was consistcnt with Classical Tlicory On the other hand some new dcpcndcncies in the US economic system have been discovered which are not covered by cxisting theories For example nn increase in real national defense investment lcads to a dccrcasc in cmploymeiit in transportation and a decrease in non-financial corporate business profit after tax An incrcasc in einployment in the government sector causes an increase in total automobile credit outstanding 6 Conclusions In this paper we proposed an approach to enhance the economic modelling process with the use of association rules A simple macroeconomic model on the basis of this approach has been built and implemeiited for the economic system of the USA The simulation X40 


of the model showed that it could be used in are stable with regards to the Control block and economic policy analysis to test different can hardly be influenced arid monitoring by government economic decisions and analyse goveriiment as well as those which are different scenarios of economic deveIopment It also allows us to explorc which indicators especially sensitive to such a control 7 Rcfcrcnces I Agrawal R Srikaiit R Fast Adgorilhm for Mining Association Rules in Large Databases VLDB 2 Agrawal R Imielinski T Swami A Mining Association Rules between Seis of laems in Large Databases SIGMOD Conferencc 1993 207-21 6 3 Brayton F Mauskopf E Reifschiieider P Williams J The Role of Expectations in the FRB/US Macroeconomic Model Federal Reserve Bulletin April 1997 227-245 4 Brayton F Mauskopf E The federal reserve Board MPS Quwlerly Ecanomelric Model of the USEcunomy Economic Modelling vol 3 July 170-292 1985 5 Fair C Testing Mucroeconomelric Models Cambridge Mass Harvard University Press 1994 6 Glymour C Madigan D Pregibon D Smyth P Statistical Themes and Lclssons for Dafu Mining Data Mining and Knowledge Discovery vol.1 No.1 1997 7 Meo R Psaila G Cori S A New SQL-like Operatorfor Mining Association Rules VLDB 1996 SI Murphy C An Overview afthe Murphy Model Australian Economic papers Supplement 1988 9 Rubinov A Nagiyev A Elemenb ofEcunomic Themy Baku Bilik 1992 in Russian lo Srikant R Agrawal R Mining Quanlitative Association Rubes in Large Relalional Tables ACM SIGMOD Conference on Management of Data 1996 1994 487-499 122 133 175-1 99 84 I 


1232.119s 90,146,85 FHUT 185.933 s 20,142,05 HUTMFI PEP 103.492 s 21.582 s 9,150,058 1,331,158 2.904 s Mushroom at 1 support Scaled 1 Ox vertically With Reordering Figure 8 FH+HM 103.323 s 9,150,030 Figures 8 and 9 show the effects of each component of the MAFIA algorithm on the mushroom dataset at 1 minimum support The number of transactions was increased by repeating all transactions in the database by a certain scaling factor We call this form of scaling vertical scaling In this case mushroom was scaled ten times vertically Note that vertical scaling will not change the search space and will only affect the time taken for counting the support of itemsets The components of the algorithm are represented in a cube format where the running times and number of lattice nodes visited during the MAFIA search for all possible combinations are shown The top of the cube shows the time for a simple traversal where the full search space is explored while the bottom of the cube corresponds to all three pruning methods being used Two separate cubes with and without dynamic reordering\rather than one giant cube are presented for readability Note that all pruning components yield some savings in running time but that certain components are more effective than others In particular HUTMFI and FHUT yield very similar results since they use the same type of superset pruning but with different methods of implementation The efficient MFI lookups that HUTMFI uses to check for frequency explain why HUTMFI outperforms FHUT see Section 3 It is also FH+PEP HM+PEP 16.925 s 8.943 s 1,134,863 535,813 Mushroom at 1 support Scaled lox vertically Without Reordering Figure 9 interesting to see that adding FHUT when HUTMFI is already performed yields very little savings i.e from HM to HM+FH or from HM+PEP to ALL the running times do not significantly change HUTMFI checks for the frequency of a node\222s HUT by looking for a frequent superset in the MFI while FHUT will explore the leftmost branch of the subtree rooted at that node Apparently, there are very few cases where a superset of a node\222s HUT is not in the MFI but the HUT is frequent PEP has the biggest effect of the three pruning methods All of the running time of the algorithm occurs at the lower levels of the tree where the border between frequent and infrequent itemsets exists and since PEP is most likely to trim out large sections at the lower levels this pruning yields the greatest results  Dynamically reordering the tail also has dramatic savings cf Figure 8 with Figure 9 It is interesting to note that without PEP dynamic reordering runs nearly an order of magnitude faster than the static ordering while with PEP it is 223only\224 3-5 times faster Since both PEP and reordering remove elements from a node\222s tail it is not surprising that they overlap in their efficacy 5.2 Comparison With Depthproject We tested the algorithm on 223real\224 datasets containing long patterns that have been used in earlier work  1,7 449 


These datasets are publicly available from the UCI Machine Learning Repository http://www.ics.uci.edu/-mlearn/MLRepository html At the lowest supports tested the longest patterns in these databases have over 20 items, making any algorithm that examines all possible subsets of these patterns or a significant portion thereof infeasible This makes the task of finding the patterns computationally intensive despite the small size of the databases For some of the experiments the databases were scaled vertically by Time Comparison on Connect4.data 1000 MAFIA  Depthproject 100 h  U 10  I 1 I 0.1 90 80 70 60 50 40 30 20 10 Min Support  Figure 10 I IlllW wJrlllJ"llsull UII c.lless.ulla 100 MAFIA DP I 60 55 50 45 40 35 30 25 20 Min Support  Figure 12 concatenating copies of the database together This only affects the time counting takes since the bitmaps compressed or not are longer and the search space examined remains constant Figures 10  12 illustrate the results of comparing MAFIA to our implementation of the Depthproject method the state-of-the-art method for finding maximal patterns I The x-axis is the user-specified minimum support, while the y-axis uses a logarithmic scale to show the running time Time Comparison on Mushroom.data 10 MAFIA  Depthproject I 1 h Y cn  i I o 1 0.01 10 8 6 4 2 0 Min Support  Figure 11 scaleup OT r;ness.aata 45 40 35 MAFIA DP 30 rr 25 20 E 15 10 5 0 0 5 10 15 20 25 Scaleup factor Figure 13 Table 1  Reduction Factor of Nodes Considered Due to PEP Pruning 450 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


