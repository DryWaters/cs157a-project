TREATING WEIGHTS AS DYNAMICAL VARIABLES  A NEW APPROACH TO NEURODYNAMICS  U RAMACHER M WESSELING SIEMENS AG Corp R&D Otto-Hahn-Ring 6,8000 Munich 83 F.R GERMANY ABSTRACT The recall and learning dynamics of Artificial Neural Networks are described by means of a partial differential equation PDE that may incorporate weights either as parameters 
or variables In the case that weights are interpreted as variables a new type of neurodynamics is discovered weights have to obey second order differential equations called learning laws Experiments on the association of time-varying patterns indicate the superiority of the learning law over the known types of learning rules It is also shown that a single first-order Hamilton-Jacobi parametric PDE suffices to derive 
the various neurodynamical paradigms used today l-51 INTRODUCTION A popular approach to the description of the learning dynamics of neural networks looks at the weights as being parameters that have to be fitted according to some learning rule 1-6 Moreover weights are assumed to be time-invariant after learning Since neural networks with their thousands or millions of weights run the danger of firstly over-fitting the experiments 
and secondly the use of parameters tends to darken the actual dynamics of the system we suggest to treat weights in the same manner as neurons ie as dynamical variables of the system Hence dynamical weights have to obey an ODE like the neurons THE HAMILTONIAN CONCEPT AND ITS GEOMETRICAL INTERPRETATION The basic variables for the dynamical description of a neural network are considered to be the 
time t the neural states y and the weight states W The change of the states is influenced by two sources external inputs and physical implementation In this paper dynamical effects caused by the physical implementation are not considered  see 7 for example  The external inputs may comprise reference signals for learning as well as actual input signals Both 
types of inputs form the generally time-varying\boundary conditions which control any change in the state space of the neural net Besides the action of the system invokes a set of observables J\(t each of which can be considered as the collective result of the states and the boundary conditions JW  J\(t,y\(t t Figure 1 depicts a trajectory J\(t,y\(t t Changing the boundary conditions for instance by means of a new 
pattern input or the initial states of the neural network one obtains a new trajectory. Imagining that all possible boundary conditions were experienced by the network, one obtains a surface J  J\(t,y,W that represents the complete information about the dynamics of the neural network Since beside t y W and J the partial derivatives of J with respect to all its variables carry the knowledge 
of how the surface J\(t,y,W is formed it is natural to request that the surface J must obey an equation D\(t y W J  U Jat TJ?y aJ Jaw  0 where D is some operator function We restrict ourselves to a particularly simple operator D 0-7803-0559-0 92 3.00 Q 1992 EJlE III-497 


Figure 1 Trajectory of an observable over the state space aJ aJ aJ H\(t,y,A,W,M  with A   M  al ayi aw  11 In the theory of partial differential equations it is shown 8,9 that Equation l which is called Hamilton-Jacobi gives rise to a fundamental set of ordinary differential equations the so-called characteristic equations 2a dYi a dAi a dWij a dMij a   _   _     aw  11 ayi  dt aM  dt 11 dt ahi  dt dJaJ aH a   i 11 dt at Aj\(t  M..\(t t  aAj ij aM Once the solutions for the characteristic equations 2a are known the trajectory J\(t,y\(t t can be determined by direct integration of the Equation 2b The corresponding surface J  J\(t,y,W is generated by the manifold of all solutions of 2a obtained by varying the initial states of the system and the boundary conditions ie by confronting the neural network with sufficiently many input as well as reference patterns Integration of 2b\yields Then searching for an extremal trajectory between two points of the state space yields lo  6Wij+\(;W d aH 6Mu 11 aM  v dt awu 1J The most general set of necessary conditions for an extremal trajectory J\(tf therefore reads W i,J:O  A.\(t   0  M..\(t   0  H\(t  34 If 11 f 1 111-498 


Obviously any choice of a Hamiltonian leads to an extrema1 trajectory J\(t This underlines clearly the importance of the construction of the Hamiltonian that allows to learn It is noted also that recall and learning have to happen simultaneously Remark If weights are not conceived as variables but as parameters the type of neurodynamics envisaged by Optimal control 5 can be derived Then the conjugate variable M associated with W would not exist and therefore the characteristic equations be reduced to equations for y A and 1 Consequently the derivative dHldW would remain unbalanced by a dynamic term but give rise to an extra term in 3a aH f  awu Wt.ltlt  t Any method producing weights that satisfy 3b constitutes a learning rule for details see 1 11  EXAMPLE OF A HAMILTONIAN THAT GENERATES A LEARNING LAW We assume a recursive direct decomposition of the weight functions according to what has been and what remains to be learned during an epoch of width T W\(t  WT\(t  w\(t  with WT\(t t-T  44 Consequently we arrive at a similar decomposition for the conjugate variables of the weights J aw m J M\(t MT\(t m\(t  with MT  awT Let us consider the Hamiltonian with The Hamiltonian 5 yields the characteristic Equations dm a dm Y      ZJ dM a aE  _  Ai t  y, \(t  t  aw dt 222 dt awi aws dt The Equations \(17c-d can be combined to yield second order ODES for the weights  V v I t    A{t t y,W  t  aE 2WT 2WT 2w 1 dt2 dt2 222 dt2 0 awi 111-499 


It follows that the ODES for the variables y A w and MT accumulate new knowledge whereas the remaning ones for W and m contain the knowledge already learned ie the latter ones need not be computed A qualitative understanding of the Equations 6a-c is obtained by assuming a constant reference output r and any feedfoward network For simplicity the error function E\(t   112  y\(t r is chosen If the reference r is set to 0 the weights w are expected to decrease in time Now the error dUdy   y is never positive since f is the sigmoidal function Hence by integrating the A-equation from A\(T  0 A must be negative for the output layer This implies MT to be negative too It follows that w has to be positive in order that the weights w decrease In the next time epoch of length T the new' weights w are added to the old weights W in accordance with the decomposition 4a of the weights Repeating the integration of the Equations 6a-c one arrives at another set of new' weights which are smaller in magnitude than the first set because of the error correction induced by the first update. Similarly the variables A and MT of the hidden layer are forced by the A of the output layer to change in such a manner that the error E is diminuished In order to test the Equations 6a-c and compare their performance to the ones obtained with learning rules a pattern association task was performed Three time-varying inputs were to be associated to three time-varying reference patterns see Figure 3 The network chosen is composed of 1 input 15 hidden and 1 output neuron, and denoted as R1-15-1 The hidden layer is fully connected The error function for the output of the network was E\(t  112  y\(t r\(t I2  Output  Reference Weights 10 0 5 kd 10 J I 0 50 100 150 200 Output  Reference Error 25 15 Figure2 Resultsfor R1-15-1 and the Equations6a-c a O.l,stepsize=O.l Upper figures output of the net after 4000 updates Lower figures Weight functions of the output neuron after 4000 updates horizontal units   of samples Accumulated total and partial errors horizontal unit   of updates Figure 2 shows the results obtained for R1-15-1 with a 0.1 and constant step size of 0.1  The equations were found to converge to zero update changes more precisely n  OD a A\(n.T XI 0 for all 0Sr;ST Note that in the limit one obtains the relation W\(l  W\(t-T telling that the weights functions are either constant or time-varying In any case they are III-500 


Inputs Reference Figure 3 Input and reference patterns repeated periodically unless a pattern is presented that restarts the learning process Figure 4 presents the results obtained for R1-15-1 when using the following equations d dt yi\(t,s  yi\(t,s  fi  1 wii 4s  yj\(t J dA aE ai t,s  Ai\(t,s ZAj\(t,s fj\(t,s W t,s t,s  Ai\(T,d=0  J i dt Output  Reference Output  Reference Weights 81 Error 401 0 Figure4 Resultsfor R1-15-1 incaseofthe Equations7a-c a  lO,stepsize=O.l Upper figures Output of the net after 4000 updates Lower figures Accumulated weight functions of the output neuron after 4000 updates horizontal units   of samples Accumulated total  partial errors horizontal unit   of updates III-501 


Equations 7a-c are the gradient version of the characteristic equations of the Hamiltonian h\(t Ai t Fj YO W\(t   E\(t,y\(t WO  7d i which treats weights as time-varying parameters see relation 3b or 11,51 In the case that weights are restricted to constant parameter functions the condition 3b as well as Equation 7c have to be integrated over time l 11 The combined result reads 2 1 0 1 2  T dWli ah a s    t dt   Io  Ai\(t,s 6 0,s yj\(t,s  T 0 aW T  dt  8 aw I v ds 50 100 150 200 A relation similar to 10 was obtained by Pearlmutter for a special cost function 41  Figure 5 depicts corresponding results It is to be noted here that the variable 5 has no interpretation as a dynamical process run by the network This is in contrast to the fully dynamical case which has a single time t for recall as well as learning Hence we may differentiate between learning rules fulfilling 3b and learning laws similar to 6d Output  Reference Output  Reference  out 2 Error 30   Ob 1000 2000 3000 4000 5000 6000 Figure 5 Resultsfor R1-15-1 and the Equations7a-b,8 apT  O.l,stepsize=O.l Upper figures output of the net after 6000 updates Lower figures Accumulated weight functions of the output neuron after 6000 updates horizontal units   of samples Accumulated total  partial errors horizontal unit   of updates COMPARING THE FIGURES 2,4,5 It is clearly seen that Equations 6a-c give the fastest decline of the error functions in terms of updates as well as computation time Further fitting with constant weights lasts considerably longer in terms of equal error In passing we note that the recurrent network R1-15-1 was found always to work better than the feedforward version of R1-15-1 111-502 


CONCLUSIONS We have described a neural network as being a dynamical system Its observables are introduced as solutions of a partial differential equation of Hamilton-Jacobi type The observables are functions of time the neural states and the weight states The dynamics of an observable is interpreted as a surface over the state space of the neural net which is generated by all its admissible trajectories If the weights are treated as parameters the only dynamical process that actually will be run by the network is the recall process It is shown that the known types of learning dynamics can be reproduced in this framework and that weights are generally time-varying during recall This feature is shown to account for much faster learning as compared to weights being constant for each learning epoch If on the other hand weights are conceived as variables a fully dynamical new picture of recall and learning results. Neurons as well as weights obey differential equations ie recall and learning are dynamical processes of the neural net It is important to note that the differential equations to be obeyed by neurons and weights respectively constitute half of the characteristic equations associated with the Hamiltomlacobi equation The remaining equations are obeyed by the conjugate variables of the neurons and weights respectively These latter are mediating the interaction between neurons and weights Comparing the fully dynamical concept with the optimization approach that exploits techniques from Optimization Theory Control Theory or Dynamic Programming 12  the dynamical concept was found to learn much faster than the optimization concept ACKNOWLEDGEMENT We are particular grateful to B Schurmann for discussing with US the Hamiltonian concept and its interpretation with respect to neural networks REFERENCES D Rumelhart G Hinton and R Williams Learning Internal Representations by Error Propagation In PARALLEL DISTRIBUTED PROCESSING EXPLORATIONS IN THE MICRO STRUCTURE OF COGNITION Vol I MIT Press 1986 L.B Almeida: "Backpropagation In Non-Feedforward Networks IEEE INT CONF ON NEURAL F.J PINEDA Generalization Of Backpropagation To Recurrent Neural Networks PHYS REV LETTERS 59 pp 2229,1987 B.A Pearlmutter Learning State Space Trajectories In Recurrent Neural Networks NEURAL COMPUTATION 1 pp 263,1989 0 Farotimi A Dembo and T Kailath A General Weight Matrix Formulation Using Optimal Control IEEE TRANSACTION ON NEURAL NETWORKS May 1991 Vol2 No 3 pp 378 U Ramacher B.Schurmann Unified Description of Neural Algorithms For Time-Independent Pattern Recognition in VLSl DESIGN OF NEURAL NETWORKS Kluwer Acad. Publishers 1991 L.O Chua L Yang Cellular Neural Networks Theory and Applications IEEE E Kamke PARTIAL DIFFERENTIAL EQUATIONS B.G Teubner Stuttgart 1977 F Verhulst NONLINEAR DIFFERENTIAL EQUATIONS AND DYNAMICAL SYSTEMS Springer 1985 NETWORKS SAN DIEGO PP 11-609,1987 TRANSACTIONS ON CIRCUITSANDSYSTEMS Vol 35 NO 10 pp 1257-1290 Oct 1988 lo A.P Sage and C.C White OPTIMUM SYSTEMS CONTROL Prentice-Hall 1977  1 11 U Ramacher M Wesseling Hamiltonian Approach to Neural Network Dynamics Proc IJCNN-91 Singapore, Vol 3 pp.1930-1936 Nov 1991 12 L.S Pontrijagin et al THE MATHEMATICAL THEORY OF OPTIMAL PROCESSES Wiky 1962 III-503 


Typically the number of complex relationships found was greater than but correlated with the number of other relationship types found As Figure 9 shows the number of complex relationships at a given con\223dence threshold was sensitive to the variance in the number of the other relationship types Self-exclusion and self-colocation were modeled together in Figure 9 emphasize the complementary relationship between the two as described in section 5.2 This is revealed in the corresponding steepness f gradient for self-exclusion/colocation at con\223dence 000 000 001 001 and con\223dence 002 000 001 002  7.5 Limitations/Strengths of the representation While there are representational issues with any type of data appropiate representation is particularly important in the spatial domain 9 Limitations In one-to-many relationships this model doesn\220t capture interesting ranges or distributions in the 217many\220 which is a task better suited for mixture modelling or the techniques described in As pointed out in 10 t h e cos t o f ful l y t r ans cri b i n g s pat i a l data into a transactional representation can in some cases be more expensive than the mining of the colocations but as a full representation is necessary to accurately add the features representing absent and multiple items a solution to this in the current representation may be problematic Strengths The most obvious strength of this representation is that currently it is the only model that allows the mining of complex relationships in spatial data A major strength of a transactional representation of spatial data not explored here is that it may be combined with non-spatial data and so the addition of nonspatial data to the representation described here would be uncomplicated 8 Conclusions  Future Work We have de\223ned the concept of complex relationships in spatial data We have described how even in transactional representations spatial data is undamentally erent from other forms of data making the need to mine complex relationships of inherent interest We have demonstrated that even when simple relationships are the goal of mining spatial data the mining of complex relationships is necessary for determining the signi\223cance of those relationships We have implemented and demonstrated a transactional representation of spatial data that allows the ef\223cient mining of complex relationships and discussed its limitations and strengths 8.1 Future Work Apart from investigating improvements to the representation to address the limitations mentioned in 7.5 there are several future directions evident such as the application to other types of data with a spatial component such as spatiotemporal data and to a lesser extent natural language and biological systems One important step would be the combination of spatial coordinate features with spa tial volume features this is especially important in Geographic Information Systems where a volume may represent the area of a lake valley etc As we have demonstrated that with a purely coordinate system 003 in 004 000 003 must be treated as a volume the inclusion of features that explicitly represent volumes should prove interesting References  R  A gra w al and R  S r i kant  F ast al gori t h ms for m i n i n g a ssociation rules In J B Bocca M Jarke and C Zaniolo editors Proc 20th Int Conf Very Large Data Bases VLDB  pages 487\205499 Morgan Kaufmann 12\20515 1994 2 T  C  B aile y a n d A T  Gatrell Interactive spatial data analysis  Longman Scienti\223c  Technical 1995 3 S  B rin  R  R asto g i  a n d K Sh im M i n i n g o p timized g a in rules for numeric attributes IEEE transactions on knowledge and data engineering  15 2003 4 G  P iatetsk y Sh ap iro  Discovery analysis and presentation of strong rules AAAI/MIT Press 1991 5 J  H an J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In W Chen J Naughton and P A Bernstein editors 2000 M SIGMOD Intl Conference on Management of Data  pages 1\20512 ACM Press 05 2000 6 Y  H uang H Xi ong and S  S hekhar  M i n i n g con\223 dent colocation rules without a support threshold In Proc 18th M Symposium on Applied Computing ACM SAC  2003  K K operski and J Han Di sco v e ry of spat i a l a ssoci at i o n rules in geographic information databases In M J Egenhofer and J R Herring editors Proc 4th Int Symp Advances in Spatial Databases SSD  volume 951 pages 47\205 66 Springer-Verlag 6\2059 1995 8 R  M unr o S  C h a w l a  a nd P  S un C o mpl e x spat i a l r el at i onships University of Sydney School of Information chnologies chnical Report 539  2003 9 D  J  P euquet  Representations of space and time  Guilford Press 2002  S  S h ekhar and S  C ha wl a Spatial Databases A Tour  2002  S  S h ekhar and Y  H uang Di sco v e r i ng spat i a l c ol o cat i o n patterns A summary of results Lecture Notes in Computer Science  2121 2001  X  W u  C  Z hang and S  Z hang Mi ni ng bot h posi t i v e and negative association rules In 19th International Conference on Machine Learning ICML-2002  2002 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


 A A A A A A A A B B B B B B B A B A B A B A B AB A B A A A A B B B A B A B A A B B B B A B A B A B A B A B A B A disjoint B A inside B A contains B A equals B A meets B A covered by B A covers B A overlaps B A B A B A B A B A B AB Figure 4 Topology and resolution increase with minimum bounding circles 64Mb of main memory Since the Apriori algorithm uses the number of transactions as support and we wanted to compare our algorithm with Apriori we have implemented MaxOccur and the na\250 021ve with transaction based support MaxOccur1 The second version of MaxOccur MaxOccur2 used the object-based support as presented in Algorithm 3.1 Table 9 shows the average execution times for the four algorithms with different image set sizes and 033 0 0  05 for Apriori 223Na\250 021ve\224 and MaxOccur1 and 0  0035 for MaxOccur2 The results are graphically illustrated in Figure 5 Clearly MaxOccur scales well with both versions treating one thousand images in 1.3 seconds on average regardless of the size of the data set The running time for 002ltering the frequent item-sets with 033 0  the maximum support threshold line 16 of Algorithm 3.1 is negligible since it is done in main memory once the frequent item-sets are determined Moreover the calculation of the total number of items line 4 of Algorithm 3.1 is done during the 002rst scan of the data set and has limited repercussion on the algorithms execution time The major difference between Apriori and MaxOccur is in ascertaining the candidate item-sets and counting their repeated occurrences in the images Obviously MaxOccur discovers more frequent item-sets The na\250 021ve algorithm also 002nds the same frequent item-sets but is visibly capable of less performance in execution time The left graphic in Figure 6 shows the average number of frequent item-sets discovered with the three algorithms Apriori found on average 109 different frequent k-item-sets while MaxOccur1 and Na\250 021ve found 148 on the same data sets and MaxOccur2 found 145 on average The discrepancy between MaxOccur1 and MaxOccur2 is basically due to the different de\002nition of support The price we pay in performance loss with MaxOccur is gained by more frequent item-sets and thus more potentially useful association rules with recurrent items discovered ofimages Apriori Na\250 021ve MaxOccur1 MaxOccur2 10K 6.43 70.91 13.62 13.68 25K 15.66 176.69 32.35 34.11 50K 30.54 359.38 66.07 67.44 75K 44.93 514.33 97.27 101.23 100K 60.75 716.01 130.12 137.81 Table 9 Average execution times in seconds with different number of images 0 100 200 300 400 500 600 700 800 10K 25K 50K 75K 100K Apriori MaxOccur1 MaxOccur2 Na\357ve time images Figure 5 Scale up of the algorithms 6 Discussion and conclusion We have introduced in this paper multimedia association rules based on image content and spatial relationships between visual features in images using coarse to 002ne resolution approach and we have demonstrated the preservation and changes in topological features during resolution re\002nement We have put forth a Progressive Resolution Re\002nement approach for mining visual media at different resolution levels and have presented two algorithms for the discovery of content-based multimedia association rules These rules would be meaningful only in a homogeneous image collection a collection of semantically similar images or received from the same source channel Many improvements could still be added to the multimedia mining process to speed up the discovery or to re\002ne or generalize the discovered results 017 One major enhancement in the performance of the multimedia association rule discovery algorithms is the addition of some restrictions on the rules to be discovered Such restrictions could be given in a metarule form Meta-rule guided mining consists of dis#ofimages 033 0 0  25 0  20 0  15 0  10 0  05 10K 1.43 2.20 2.70 5.06 13.51 25K 2.80 4.78 6.31 11.20 32.35 50K 6.27 9.28 11.59 22.74 66.07 75K 8.24 13.57 17.69 33.94 97.27 100K 11.32 17.63 23.13 46.74 130.12 Table 10 Average execution time in seconds of MaxOccur with different thresholds 


 0 20 40 60 80 100 120 140 160 MaxOccur2 MaxOccur1 Na\357ve Apriori Apriori MaxOccur1 MaxOccur2 Na\357ve F k  Figure 6 Frequent item\255sets found by the dif\255 ferent algorithms covering rules that not only are frequent and con\002dent but also comply with the meta-rule template For example with a meta-rule such as 223 H-Next-to X Y   Colour x red  Overlap Y Z   P  Y Z  224 one need only to 002nd frequent 3-item-sets of the form f HNext-to\(red Y  Overlap Y 003  P  Y 003  g where Y is an attribute value and P a visual descriptor or spatial relationship predicate Obviously such a 002lter would greatly reduce the complexity of the search problem A method for exploiting meta-rules for mining multilevel association rules is given in  017 We have approximated an object in an image to a locale which is an area with a consistent visual feature such as colour Objects in images and videos are obviously more complex In a recent paper 9 re gions and their signatures are used as objects in a similarity retrieval system A computationally ef\002cient way to identify distinct objects in images is however still to be proposed Automatically identifying real objects and using spatial relationships between real objects would reduce the number of rules discovered and make them more signi\002cant for some multimedia applications 017 Object recognition or identi\002cation in image processing and computer vision is a very active research 002eld Accurately identifying an object in a video for example as being an object in itself is a very dif\002cult task We believe that data mining techniques can help in this perspective Multimedia association rules with spatial relationships using the motion vector of locales as a conditional 002lter can be used to discover whether locales moving together in a video sequence are part of the same object with a high con\002dence 017 There are many application domains where multimedia association rules could be applied and should be tested such as global weather analysis and weather forecast medical imaging solar surface activity understanding etc We are investigating the application with Magnetic Resonance Imaging MRI to discover associations between lesioned structures in the brain or between lesions and pathological characteristics Further development and experiments with mining multimedia data will be reported in the future References 1 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules In Proc VLDB  pages 487\226499 1994 2 M  J  E genhof er  Spatial Query Languages  PhD thesis University of Maine 1989 3 M  J  E genhof er and J  S har ma T opol ogi cal r e l a t i ons between regions in r 2 and z 2 In Advances in Spatial Databases SSD'93  Singapore 1993 4 U  M  F ayyad S  G  D j or go vski  a nd N  W e i r  A ut omat i n g the analysis and cataloging of sky surveys In U Fayyad G Piatetsky-Shapiro P Smyth and R Uthurusamy editors Advances in Knowledge Discovery and Data Mining  pages 471\226493 AAAI/MIT Press 1996 5 Y  F u a n d J Han  M e ta-ru le-g u i d e d m in in g o f a sso ciatio n rules in relational databases In Proc 1st Int Workshop Integration of Knowledge Discovery with Deductive and ObjectOriented Databases  pages 39\22646 Singapore Dec 1995 6 J  H an an d Y  F u  Disco v e ry o f mu ltip le-le v el asso ciatio n r u l es from large databases In Proc VLDB  pages 420\226431 1995 7 Z  N  L i  O R Z a 250 021ane and Z Tauber Illumination invariance and object model in content-based image and video retrieval Journal of Visual Communication and Image Representation  10\(3\:219\226244 September 1999 8 R  M iller a n d Y  Y a n g  Asso ciatio n r u l es o v e r i n t erv a l d ata In Proc ACM-SIGMOD  pages 452\226461 Tucson 1997 9 A  N atse v  R Rasto g i  a n d K Sh im W ALR U S A s imilar ity retrieval algorithm for image databases In Proc ACMSIGMOD  pages 395\226406 Philadelphia 1999  R Ng L  V  S  L akshmanan J  H an a nd A Pang E x ploratory mining and pruning optimizations of constrained associations rules In Proc ACM-SIGMOD  Seattle 1998 11 R Srik an t a n d R Ag ra w a l M i n i n g q u a n titati v e asso ciatio n rules in large relational tables In Proc ACM-SIGMOD  pages 1\22612 Montreal 1996  P  S t ol or z H  N a kamur a  E  M esr obi an R  M unt z E  S h ek J Santos J Yi K Ng S Chien C Mechoso and J Farrara Fast spatio-temporal data mining of large geophysical datasets In Proc Int Conf on KDD  pages 300\226305 1995  O  R  Z a 250 021ane Resource and Knowledge Discovery from the Internet and Multimedia Repositories  PhD thesis School of Computing Science Simon Fraser University March 1999  O  R  Z a 250 021ane,J.Han,Z.-N.Li,J.Y.Chiang,andS.Chee MultiMediaMiner A system prototype for multimedia data mining In Proc ACM-SIGMOD  Seattle 1998  O  R  Z a 250 021ane J Han Z.-N Li and J Hou Mining multimedia data In CASCON'98 Meeting of Minds  Toronto 1998 


18001  balancing mechanism which requires further investi gation 4.5 Speedup Figure 12 shows the speedup ratio for pass 2 vary ing the number of processors used, 16 32 48 and 64 where the curve is normalized with the 16 processor execution time The minimum support value was set to 0.4 4.5 0.5 1 1 0 I 10 20 30 40 50 60 70 number of mxessors Figure 12 Speedup curve NPA HPA and HPA-ELD attain much higher lin earity than SPA HPA-ELD an extension of HPA for extremely large itemset decomposition further in creases the linearity HPA-ELD attains satisfactory speed up ratio This algorithm just focuses on the item distribution of the transaction file and picks up the extremely frequently occurring items Transferring such items could result in network hot spots HPA-ELD tries not to send such items but to process them locally. Such a small mod ification to the original HPA algorithm could improve the linearity substantially 4.6 Effect of increasing transaction Figure 13 shows the effect of increasing transac tion database sue as the number of transactions is increased from 256,000 to 2 million transactions We used the data set t15.14 The behavior of the results does not change with increased database size The minimum support value was set to 0.4 The num ber of processors is kept at 16 As shown each of the parallel algorithms attains linearity 5 Summary and related work In this paper we proposed four parallel algorithms for mining association rules A summary of the four database size Sizeup 0 I 0 500 loo0 1500 uxw amount of transaction thousands Figure 13 Sizeup curve algorithms is shown in Table 5 In NPA the candi date itemsets are just copied amongst all the proces sors Each processor works on the entire candidate itemsets NPA requires no data transfer when the supports are counted However in the case where the entire candidate itemsets do not fit within the mem ory of a single processor the candidate itemsets are divided and the supports are counted by scanning the transaction database repeatedly Thus Disk 1/0 cost of NPA is high PDM, proposed in 6 is the same as NPA which copies the candidate itemsets among all the processors Disk 1/0 for PDM should be also high The remaining three algorithms SPA HPA and HPA-ELD partition the candidate itemsets over the memory space of all the processors Because it better exploits the total system's memory, disk 1/0 cost is low SPA arbitrarily partitions the candidate itemsets equally among the processors Since each processor broadcasts its local transaction data to all other pro cessors the communication cost is high HPA and HPA-ELD partition the candidate itemsets using a hash function which eliminates the need for transac tion data broadcasting and can reduce the comparison workload significantly HPA-ELD detects frequently occurring itemsets and handles them separately which can reduce the influence of the workload skew 6 Conclusions Since mining association rules requires several scans of the transaction file its computational requirements are too large for a single processor to have a reasonable response time This motivates our research In this paper we proposed four different parallel algorithms for mining association rules on a shared nothing parallel machine and examined their viabil 29 


Table 5 characteristics of algorithms ity through implementation on a 64 node parallel ma chine the Fujitsu AP1000DDV If a single processor can hold all the candidate item sets parallelization is straightforward It is just suf ficient to partition the transaction over the proces sors and for each processor to process the allocated transaction data in parallel We named this algo rithm NPA However when we try to do large scale data mining against a very large transaction file the candidate itemsets become too large to fit within the main memory of a single processor In addition to the size of a transaction file a small minimum support also increases the size of the candidate itemsets As we decrease the minimum support computation time grows rapidly but in many cases we can discover more interesting association rules SPA HPA and HPA-ELD not only partition the transaction file but partition the candidate itemsets among all the processors We implemented these al gorithms on a shard-nothing parallel machine Per formance evaluations show that the best algorithm HPA-ELD attains good linearity on speedup by fully utilizing all the available memory space which is also effective for skew handling At present we are doing the parallelization of mining generalized association rules described in 9 which includes the taxonomy is-a hierarchy Each item belongs to its own class hierarchy In such mining associations between the higher class and the lower class are also examined Thus the candidate itemset space becomes much larger and its computation time also takes even longer than the naive single level association mining Parallel pro cessing is essential for such heavy mining processing Acknowledgments This research is partially supported as a priority research program by ministry of education We would like to thank the F\221ujitsu Parallel Computing Research Center for allowing us to use their APlOOODDV sys tems References l R.Agrawal T.Imielinski and ASwami 223Min ing Association Rules between Sets of Items in Large Databases\224 In Proc of the 1993 ACM SIGMOD International Conference on Manage ment of Data pp207-216 May 1993 2 R.Agrawal and RSrikant 223Fast Algorithms for Mining Association Rules\224 In Proc of the 20th International Conference on Very Large Data Bases pp.487-499 September 1994 3 J.S.Park M.-S.Chen and P.S.Yu 223An Effec tive Hash-Based Algorithm for Mining Associ ation Rules\224 In Proc of the 1995 ACM SIG MOD International Conference on the Manage ment of Data SIGMOD Record Vo1.24 pp.175 186 June 1995 4 H.Mannila H.Toivonen and A.I.Verkamo 223Ef ficient Algorithms for Discovering Association Rules\224 In KDD-94:AAAI Workshop on Knowl edge Discovery in Databases pp.181-192 July 1994 5 A.Savasere, E.Omiecinski and S.Navathe 223An Effective Algorithm for Mining Association Rules in Large Databases\224 In Proc of the 21th International Conference on Very Large Data Bases pp.432-444 September 1995 6 J.S.Park M.-S.Chen and P.S.Yu 223Efficient Parallel Data Mining for Association Rules\224 In Proc of the 4th International Conference on In formation and Knowledge Management pp.31 36 November 1995 7 T.Shintani and M.Kitsuregawa 223Considera tion on Parallelization of Database Mining\224 In Institute of Electronics Information and Com munication Engineering Japan SIG CPS Y95 88 Technical Report Vo1.95 No.47 pp.57-62 December 1995 8 T.Shimizu T.Horie and H.Ishihata 223Perfor mance Evaluation of the APlOOO Effects of message handling broadcast and barrier syn chronization on benchmark performance-\224  In S WO PP 22292 9.2 ARC 95 Information Processing Society of Japan Vo1.92 No.64 1992 9 R.Srikant and R.Agrawal 223Mining Generalized Association Rules\224 In Proc of the 21th Inter national Conference on Very Large Data Bases pp.407-419 September 1995 30 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


