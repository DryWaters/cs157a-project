Learning Capability in Fuzzy Petri Nets\222 Eric C.C Tsang Daniel S Yeung and John W.T Lee Dept of Computing The Hong Kong Polytechnic University Abstract Petri nets \(PNs have been used widely in modeling and analyzing many real applications such as computer automatic control and management information system etc The power of PNs comes from its ability to model and analyze the behaviors and states of systems events concurrently. Neural networks 224s on the other hand were developed to handle and solve many linear and nonlinear complex problems by forming an association relationship 
between its input and output training patterns It will be advantageous if a learning capability is incorporated into a fuzzy Petri net FPN which has the capability of both systems In this paper a FPN model which has learning capability is proposed The purpose of including a learning facility in FPNs is that many parameters of a fuzzy expert system included in fuzzy production rules FPRs once when it has been modeled by a FPN could be tuned These parameters including membership values weights local and global and certainty factors etc play important roles in capturing and representing complex domain expert knowledge By comparing the artificial neural 
networks ANN with FPNs having learning capability we have the advantages such as a FPNs provide a transparent modeling and analyzing capability whereas ANN provides a black-box learning and no-analysis capability b FPNs representing a fuzzy expert system could be used to analyze the different inference states step-by-step c FPNs could tune parameters in a fuzzy expert system so that the overall system performance is improved To achieve the above goals we need to solve many problems such as how to model a FPR with a FPN so that parameters to be tuned are mapped onto this FPN, what modifications need to be done 
in order to make a FPN having a learning capability This paper will try to address these problems and issues 1 Introduction FPN is a variation of traditional PN which have the ability to model represent and capture vague imprecise uncertain and fuzzy concepts Many researchers have proposed different FPNs In I the fuzzy neural network modeling the FPN is defined as having 7 tuple while in  a high level FPN is defined as having 8-tuple In these models the PNs have been modified to act like a neural network model which allows the PNs to have learning capability The problem with 
these models is that many complicated fuzzy production rules could not be modeled by FPNs NN incorporated in these FPNs could not have a general learning capability as used in traditional Backpropagation In this paper a FPN model which has learning capability is proposed The purpose of including a learning facility in FPNs is that many parameters of a fuzzy expert system included in FPRs once when it has been modeled by a FPN could be tuned The paper is arranged into different sections In Section 2 different types of fuzzy production rules having parameters such as local weights and certainty factors which could enhance its representation power are 
presented Section 3 will concentrate on the FPNs which could model these production rules while in Section 4 a neural network learning algorithm which could be incorporated in the FPNs is proposed Section 5 provides an experiment to demonstrate that the NN could be used to tune the parameters in FPNs Finally a conclusion is given 2 Fuzzy Production Rule and Its Reasoning Method FPRs widely used by many researchers can be summarized to have three different forms: a simple FPR whose antecedent part has only one proposition a conjunctive FPR whose antecedent part has two or more propositions connected by 223AND\222 and a disjunctive FPR whose antecedent part has 
two or more propositions connected by 221LOR\222 The consequent of these FPRs are assumed to have only one proposition In the following paragraphs we will present the last two FPRs with local weights and certainty factors included A conjunctive FPR has the form oj R IF VI is A AND V is A  AND V is A THEN U is B CF hAiA,u,-JAn LWi LW2,--.9 LW Given Facts  V is A\222 V is A\222 V is A\222 Conclusion U is B with a degree of 
truth T where V,,V  V and U are variables A,,A  A and B are the fuzzy values of the variables VI V  Vn and U respectively LW LW  LW are the local weights of each proposition in the antecedent part of the FPR, and h is the threshold value for the relative degree of similarity between Ai and its given or observed fact Ati 222 This work is supported by the Hong Kong Polytechnic University Central Research Grant G-S724 0-7803-5731-0/9!2!$10.00 01999 IEEE In-355 


Similarly,a disjunctive FPR has the form of R IF VI is A OR V is A  ORV is A THEN U is B CFR AA Lw Lw2 Lwn Given Facts  VI is A V is A  V is A Conclusion U is B with a degree of truth T When n=l each of these FPRs will be reduced to a simple FPR In order to draw a reasonable conclusion, first of all we have to determine the degree of similarities of a set of given facts with their corresponding propositions in the antecedent A table which tabulates the relative degrees of similarity of all possible given facts with the hzzy set in the antecedent could be used An example is given in the next paragraph to demonstrate such an approach Furthermore we will show how the degree of truth of the consequent is computed Written English Results from a Public Examination Excellent very good Good Satisfactory Fair Pass Fail Sexcellem 1 o 0.9 0.7 0.5 0.3 0.1 0.0 Fluent almost fluent quite fluent fairly good quite bad Bad Spoken English Results from a Public Examination 1 O 0.9 0.7 0.4 0.2 0.1 RSnuenl Fail I 0.0 able 2 relative degree of similaril tY of given facts with respect to a fuzzy term fluent e.g If X's written English is excellent and X's spoken English is fluent then X's English is good If we are given the fact that Peter's written English is good and his spoken English is almost fluent it is reasonable to draw the conclusion that Peter's English is good to a degree of truth 0.7 assumed that LW,=l LW,=1 CF=l in this case It is because by the traditional method the antecedent will have a degree of truth 0.7 which is equal to Min\(0.7,0.9 and the conclusion will also have a degree of truth 0.7 which is equal to CF=0.7 h,=0.6,h2=0.5 LW,=0.9 LW,=O.8 Min\(0.7,0.9 The two values 0.7 and 0.9 in Min\(O.7 0.9 are the relative degrees of similarity of the fuzzy term excellent with its given fact good and the hzzy termfluent with its given fact afmostfluent respectively which are obtained from the relative degrees of similarity as shown in tables 1 and 2 The following issues must be addressed when local weight and certainty factor are considered 1 2 How can we make use of the local weight and the certainty factor of a conjunctive and a disjunctive FPR so that a reasonable conclusion can be drawn  Is there a general method to compute the degree of truth of the consequent of a FPR by taking the local weight and certainty factor into consideration One criteria for such a general method is that the traditional method is a limiting case of it Let us assume that RSA is the relative degree of similarity of a hzzy term Ai in the antecedent of a FPR with its given fact A'i and RSA 2 kAi for all Ai For a conjunctive fuzzy rule the overall degree of truth of the antecedent is given by following equation 1  The degree of truth T of the consequent is given by T=RS  CF For a disjunctive fuzzy rule the overall degree of truth of the antecedent is given by equation 2 The degree of truth T of the consequent for this type of rule is also given by T=RS,*CF From the equations 1 and 2 one may notice that the local weights in the antecedent are used to adjust the relative degree of similarity so that they will compensate for any differences among themselves. When LW  LW  LW in either a conjunctive or disjunctive FPR our method will reduce to the traditional one Thus our method generalises the traditional one 3 Mapping Fuzzy Production Rules into Fuzzy Petri Nets 


Fuzzy Petri Nets Many modelling methodologies have been devised for different systems Some of them are appropriate to model FPRs In this paper a generalized fuzzy Petri net structure based on 4 is defined as a 14-tuple where P  pI,p2  pn} denotes a set of places T  tl,t  t denotes a set of transitions P n T  0 Th  AI  As denotes a set of threshold values I\(0 T+P is the input output\function a mapping from transitions to bags of places F   fl f2    fr denotes a set of fuzzy sets W={LW LW  LW denotes a set of local weights of FPRs C={CFRl CFw   CF  denotes a set of certainty factor of the rules Mc\(PxT is a finite set of arcs which represents the local weights Nc\(TxP is a finite set of arcs which represents the certainty factor f N+[0,1 is an association function which assigns a certainty factor to an arc a P+F is an association function which assigns a fuzzy set to each place and is an association function which assigns a local weight to an arc y P+Th is an association function a mapping from places to threshold values FPN=\(P T Th I 0 F W C M N,f a P y Mapping Fuq Production Rules into Fuzzy Petri Nets Let's take a type 2 rule as an example for illustration R IF VI is A AND V is A2 THEN U is B CFR Rule R is changed to the following representation h,l,A LW LW Y\(P Y\(Pz P\(Pl tl P\(pztJ=LW2 a\(pl  a\(p  tokens representing fuzzy sets of given facts The mappings for types 1 and 3 rules could be defined similarly The Mapping FPRs into FPNs is shown in Figure 1 ti pd.=CFR 4 My Approach In this section we present a learning algorithm in a FPN which is divided into initialization presenting training examples, the feed forward computation and backward arc adjustment method The feed forward computation is based on equations 1 and 2 presented in section 2 while the backward weight adjustment involves changing the traditional backpropagation learning algorithm Initialization Start with a FPN which represents a set of FPRs in fuzzy expert system, the arcs in FPNs represent the local weight and certainty factors as shown in Figure 1 Presenting Training Examples Present the network with training examples For each example presented to FPN the following sequence of forward and backward computations are performed Feed Forward Pass In the FPN the feed forward pass is same as firing a transition when each of the input places has a token in it The firing of a OR transition is expressed as and for an  AND transition we have where LW  max[LWV I 1  j  J means the maximum value of local weights LWs among all J places which pass into a transition ti and Out  max[Outj 1 1  j  J means the maximum firing output values Out among all J places which pass into a transition ti Backward Weight Adjustment In a FPN incorporated with a NN backpropagation learning method the error calculated for the local weights LWs is not the same as for the certainty factors CFs The adjustments to the arcs in FPN CFs and LWs of the network are presented as follows A CF  q 6 Ou else A CF  0 and where q  Sj and 6 are the learning-rate parameter the error rates in the layers j and k respectively 5 An Experiment In this experiment a set of FPRs representing the knowledge base of a fuzzy expert system for choosing a computer m 357 


professional is presented 11 rules are extracted from the system Two of them are presented as follows R1  If X excels in written English LW=0.9 and fluent in spoken English LW=0.8 then X's English is excellent CF=0.7 If X attends a lot of extra-curricular activities FC LW=0.7 or X has excellent personal relationship LW  1 O then X's social skill is good CF=0.5 These 11 rules when mapped into a FPN is shown in Figure 2 The different notations used in the Figure is listed as follows Notation of the 14 input nodes is presented as follows PL--Knowledge on Programming Language  Coding Skill  SA--Knowledge on Software Application OS Knowledge on Operating System MT--Knowledge on Machine Type ECA--Extra-curricular Activities PR Capability of Personal Relationship  EW--Written English  EO--Spoken English CW--Written Chinese CO Spoken Chinese WE--Working Experience PE--Public Education INT--Interview AT--Aptitude Test wi the synaptic weights between layers LW the i local weight of a FPR CF--the certainty factor of a FPR; Ri the i FPR The FPN is trained with a set of 15 records and the training data is shown in Table 4 The expected and the actual results after training the FPN is shown in Table 5 The final weights \(arcs in FPN are presented in Table 3 The results in Tables 3 and 5 demonstrate the FPN with NN learning capability incorporated could be used to tune the local weights and certainty factors The results are promising as shown in the actual output columns in Table 5 I 6 Conclusion In this paper a FPN model is used to model FPRs and a NN learning algorithm is incorporated The FPN with a learning capability is more advantageous than the traditional ANN as a FPNs provide a transparent modeling and analyzing capability whereas ANN provides a black-box learning and no-analysis capability; b FPNs representing a fuzzy expert system could be used to analyze the different inference states step-by-step c FPNs could tune parameters in a fuzzy expert system so that the overall system performance is improved The local weights and certainty factors of FPRs have been used to demonstrate the tuning capability of FPNs 7 Reference l S.I Ahson Petri net models of fuzzy neural networks IEEE Trans On SMC vo1.25 no.6 pp 926-932, June 1995 2 H Scarpelli F Gomide and R.R Yager A reasoning algorithm for high level fuzzy Petri nets IEEE Trans On Fuzzy Systems vo1.4 No.3 pp 3 H Scarpelli F Gomide A high level fuzzy Petri net approach for discovering potential inconsistencies in fuzzy knowledge bases  Fuzzy Sets System vol 64 pp 175-193, 1994 4 D.S Yeung and E.C.C Tsang A multilevel wegithed fuzzy reasoning algorithm for expert systems IEEE Trans On SMC Part A System and Human vo1.28 no.2 pp149-158 Mar 1998 282-294 August 1996 a b Fig 1  Different types c  b FPRs Mapping into FPN a A simple FPR b A FPR with an AND connective c A FPR with an OR connective II-358 


 wz 0.6815 0.8008 W4 0.9766 0.6456 0.6 0.9 W5 w6 w7 0.6799 0.6061 1 0.7032 0.8121 0.656 0.9 1v9 1 0.8001 0.9 0.6372 wzo wzz wz2 0.6124 0.6992 0.7126 0.7302 0.8881 10.9779 10.8289 I I I Table 3  Updated Weights for d W8 1  0.7799 0.6012 ferent L Layer 0 N TECH AP SA Fig 2  Mapping from FPRs into FPNs In-359 


Zec No 1 Table 4 Training Data Input Value of being of being 0.5376 0.5208 0.7122 0.6124 0.823 0.8 0.3732 0.3784 0.9213 0.8913 0.8873 0.634 7 11 0.7263 I 0.6376 8 11 0.3584 I 0.3392 1 0.8877 1 0.824 0.8636 0.7622 0.9205 0.721 1 12 0.5522 0.406 13 11 0.3689 I 0.3376 14 11 0.3942 I 0.324 LL I 15 0.8664 0.9463 1 11 0.8643 0.7312 0.6983 0.6103 0.9756 0.9132 0.5908 0.5372 0.8724 0.7724 0.6977 0.5196 0.4205 0.3908 0.6632 I 0.4936 0.9619 0.9312 0.3102 0.234 0.3972 0.362 0.7812 0.733 0.6123 0.5908 0.7732 I 0.642 0.7123 0.6412 Table 5  Actual and Expected Outp 0.3278 0.2916 0.3742 0.3638 0.5766 0.4826 0.6906 0.5428 0.6864 0.6132 0.1906 0.1728 0.7976 0.7808 0.9502 0.8736 0.521 1 0.4916 0.5934 0.5439 0.6168 0.5175 0.9753 0.9320 0.8753 0.7808 0.9664 0.932 0.3903 0.3729 It III--360 


                          


                               


I Plenary Panel Session J Future Directions in Database Research  456 Chair Surajit Chaudhuri Microsoft Corporation Panelists Hector Garcia-Molina Stanford University Hank Korth, Bell Laboratories Guy Lohman IBM Almaden Research Center David Lomet Microsoft Research David Maier Oregon Graduate Institute I Session 14 Query Processing in Spatial Databases I Chair Sharma Chakravarthy University of Florida Processing Incremental Multidimensional Range Queries in a Direct Manipulation Visual Query Environment  458 High Dimensional Similarity Joins Algorithms and Performance Evaluation  466 S Hibino and E Rundensteiner N Koudas and K.C Sevcik Y Theodoridis E Stefanakis and T Sellis Cost Models for Join Queries in Spatial Databases  476 Mining Association Rules Anti-Skew Algorithms  486 J.-L Lin and M.H Dunham Mining for Strong Negative Associations in a Large Database of Customer Transactions  494 A Savasere E Omiecinski and S Navathe Mining Optimized Association Rules with Categorical and Numeric Attributes  503 R Rastogi and K Shim Chair: Anoop Singhal AT&T Laboratories S Venkataraman J.F Naughton and M Livny Remote Load-Sensitive Caching for Multi-Server Database Systems  514 DB-MAN A Distributed Database System Based on Database Migration in ATM Networks  522 T Hara K Harumoto M Tsukamoto and S Nishio S Banerjee and P.K Chrysanthis Network Latency Optimizations in Distributed Database Systems  532 I Session 17 Visualization of Multimedia Data I Chair Tiziana Catarci, Universita di Roma 223La Sapienza\224 W Chang D Murthy A Zhang and T.F Syeda-Mahmood Global Integration of Visual Databases  542 X 


The Alps at Your Fingertips Virtual Reality and Geoinformation Systeps  550 R Pajarola l Ohler P Stucki K Szabo and P Widmayer C Baral G. Gonzalez and T.C Son Design and Implementation of Display Specifications for Multimedia Answers  558 1 Session 18 Management of Objects I Chair: Arbee Chen National Tsing Hua University P Boncz A.N Wilschut, and M.L. Kersten C Zou B Salzberg, and R Ladin 0 Wolfson S Chamberlain S Dao L Jiang, and G. Mendei Flattening an Object Algebra to Provide Performance  568 Back to the Future Dynamic Hierarchical Clustering  578 Cost and Imprecision in Modeling the Position of Moving Objects  588 ROL A Prototype for Deductive and Object-Oriented Databases  598 A Graphical Editor for the Conceptual Design of Business Rules  599 The Active HYpermedia Delivery System AHYDS using the M Liu W Yu M Guo and R Shan P Lang W Obermair W Kraus and T Thalhammer PHASME Application-Oriented DBMS  600 F Andres and K. Ono S Chakravarthy and R Le S Mudumbai K Shah A Sheth K Parasuraman and C Bertram ECA Rule Support for Distributed Heterogeneous Environments  601 ZEBRA Image Access System  602 Author Index  603 xi 


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


