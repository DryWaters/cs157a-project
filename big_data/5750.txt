HFSP Size-based Scheduling for Hadoop Mario Pastorelli   Antonio Barbuzzi   Damiano Carra   Matteo DellêAmico  and Pietro Michiardi   EURECOM  Campus SophiaTech France Email pastorel@eurecom.fr,barbuzzi@eurecom.fr,della@linux.it,michiard@eurecom.fr  University of Verona Italy Email damiano.carra@univr.it Abstract Size-based scheduling with aging has for long been recognized as an effective approach to guarantee fairness and near-optimal system response times We present HFSP a scheduler introducing this technique to a real multi-server complex and widely used system such as Hadoop Size-based scheduling requires a priori job size information which is not available in Hadoop HFSP builds such knowledge by estimating it on-line during job execution Our experiments which are based on realistic workloads generated via a standard benchmarking suite pinpoint at a signiìcant decrease in system response times with respect to the widely used Hadoop Fair scheduler and show that HFSP is largely tolerant to job size estimation errors I I NTRODUCTION The advent of large-scale data analytics fostered by parallel processing frameworks such as MapReduce has created the need to manage the resources of compute clusters that operate in a shared multi-tenant environment Within the same company many users share the same cluster because this avoids redundancy both in physical deployments and in data storage and may represent enormous cost savings Initially designed for few and very large batch processing jobs data-intensive scalable computing frameworks such as MapReduce are nowadays used by many companies for production recurrent and even experimental data analysis jobs This heterogeneity is substantiated by recent studies 3 that analyze a variety of production-level workloads An important fact that emerges from previous works is that there exists a stringent need for short system response times Data exploration preliminary analyses and algorithm tuning on small datasets often involve interactivity inthe sense that there is a human in the loop seeking answers with a trial-and-error process In addition workîow schedulers such as Oozie contrib ute to w orkload heterogeneity by generating a number of small orchestration jobs Interactive and orchestration jobs should not wait too long before being served even if larger production jobs are in execution Commonly the task of a cluster administrator involves the manual setup of a number of pools to dedicate resources to different job categories and the ne-tuning of the parameters governing resource allocation This process is tedious error prone and cannot adapt easily to changes in the workload composition In addition it is often the case for clusters to be over-dimensioned this simpliìes resource allocation with abundance managing resources is less critical but has the downside of costly deployments that are left unused for long We address the problem of job scheduling  that is how to allocate the resources of a cluster to a number of concurrent jobs and focus on Hadoop the most widely adopted opensource implementation of MapReduce We proceed with the design of a new scheduling protocol that caters both to a fair and efìcient utilization of cluster resources while striving to achieve short response times Our approach satisìes both the interactivity requirements of small jobs and the performance requirements of large jobs which can thus coexist in a cluster without requiring manual setups and complex tuning Our solution implements a size-based  preemptive scheduling discipline The scheduler allocates cluster resources such that job size information  which is not available a-priori  is inferred while the job makes progress toward its completion Scheduling decisions use the concept of virtual time  in which jobs make progress according to an aging function cluster resources are focused on jobs according to their priority computed through aging This ensures that neither small nor large jobs suffer from starvation The outcome of our work materializes as a full-îedged scheduler implementation that integrates seamlessly in Hadoop we called our scheduler HFSP to acknowledge an inîuential theoretic work in the size-based scheduling literature The contribution of our work can be summarized as follows  We design and implement the system architecture of HFSP Section III including a pluggable component to estimate job sizes and a dynamic resource allocation mechanism that strives at efìcient cluster utilization HFSP is available as an open-source project 1  Our scheduling discipline is based on the concepts of virtual time and job aging These techniques are conceived to operate in a multi-server system with tolerance to failures scale-out upgrades and multi-phase jobs  a peculiarity of MapReduce  We reason about the implications of job sizes not being available a-priori both from an abstract Section II and from an experimental Section IV point of view Our results indicate that size-based scheduling is a realistic option for Hadoop clusters because HFSP sustains even rough approximations of job sizes  We perform an extensive experiment campaign where we compare the HFSP scheduler to a prominent scheduler used in production-level Hadoop deployments the 1 https://bitbucket.org/bigfootproject/hfsp 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 51 


Hadoop Fair Scheduler For the experiments we use PigMix a standard benchmarking suite that performs real data analysis jobs Our results show that HFSP represents a sensible choice for a variety of workloads catering both to interactivity and efìciency requirements II S IZE B ASED S CHEDULING FOR D ATA INTENSIVE S YSTEMS Next we outline the principles of size-based scheduling in view of our goal to bring its beneìts to a system like Hadoop MapReduce In addition we discuss on the feasibility of such an approach to job scheduling when job sizes are not known a priori but can only be evaluated approximately A Scheduling First Come First Serve FCFS and Processor Sharing PS are arguably the two most simple and ubiquitous scheduling disciplines in use in many systems for instance the FIFO and FAIR schedulers in Hadoop are inspired by these two approaches In FCFS jobs are scheduled in the order of their submission while in PS resources are divided evenly so that each active jobs keeps progressing In loaded systems these disciplines have severe shortcomings in FCFS large running jobs can delay very signiìcantly small ones that are waiting to be executed in PS each additional job delays the completion of all the others Essentially size-based scheduling adopts the idea of giving priority to small jobs as such they will not be slowed down by large ones The Shortest Remaining Processing Time SRPT policy which prioritizes jobs that need the least amount of work to complete is the one that minimizes the mean sojourn time or response time  that is the time that passes between a job submission and its completion Policies lik e SRPT may however incur in starvation  if smaller jobs are continuously submitted larger ones may never get scheduled In order to avoid starvation a common solution is to perform job aging  virtually decreasing the size of jobs waiting in the queue in order to make sure that they will be eventually scheduled Figure 1 compares PS with the SRPT scheduling discipline with an illustrative example in this case two small jobs  j 2 and j 3  are submitted while a large job j 1 is running While in PS the three jobs run slowly in parallel in a size-based discipline j 1 is preempted  the result is that j 2 and j 3 complete earlier It is worth noting that in this case the completion time of j 1 does not suffer from preemption somewhat counter to intuition this is often the case for SRPT-based scheduling B Impact of Size Estimation Errors In MapReduce job size distribution is very skewed ranging from few seconds to several hours 3 These sizes are difìcult to obtain a priori even though various recent works tackle the task of estimating MapReduce job sizes  we discuss them in more detail in Section V in addition Lu et al evaluate the impact of estimation errors on sizebased scheduling for synthetic traces Unfortunately  the combination of these works is not sufìcient to understand  100 usage cluster 50 10 15 37.5 42.5 50 time s 100 usage cluster 10 50 20 30 50 time s job 1 job 2 job 3 job 1 job 3 job 2 job 1 Fig 1 Comparison between PS top and SRPT bottom which level of estimation errors would be acceptable for sizebased scheduling in our context of extremely diverse job sizes To evaluate at an abstract level the interplay between estimation errors and performance we rst use a simulation-based approach The simulator which is available as free software 2 abstracts from the details of a full-îedged MapReduce system it assumes that jobs can utilize the full cluster capacity and that they can be preempted istantaneously Thus a job is only characterized by its submission time and the amount of time it would need to complete if utilizing the full cluster capacity we will lift these assumptions by evaluating our real system implementation in Section IV In our simulation we replay a 24-hour trace made available with the SWIM tool which comprises 24,442 production jobs at Facebook in 2010 and has been used to validate other works 17 We consider estimation errors to be log-normally distributed in our simulator a job having size of s will be estimated as  s  sX  where X is a random variable with distribution LogN 0  2   the choice of the log-normal distribution reîects the intuition that an under-estimation  s  s/t  t 1  is as likely as an over-estimation  s  ts in Section IV-C we show experimentally that our estimation errors are indeed well approximated by such distribution We report results for the default simulator settings which reîect a heavily-loaded system where aggregated disk bandwidth is larger than network bandwidth Details on the simulator and its parameter space are available in a technical report Figure 2\(b shows box-plots that is the most important percentiles of a cumulative distribution function for the mean sojourn times achieved by SRPT with 100 simulation runs for each value of   comparing it with mean sojourn times using PS and SRPT without estimation errors Notice that because of the large variance in sojourn times we plot them on a logarithmic scale Since the trace is xed what changes between simulation runs are only estimation errors for reference log-normal distributions with different values of  are shown in Figure 2\(a Clearly in situations of high load a FCFS policy performs poorly results are outside of the plotted graphs with a mean sojourn time of 1934 s Instead 2 https://bitbucket.org/bigfootproject/schedsim 52 


        0  00  51  01  52  02  53  0 x 0  0 0  5 1  0 1  5 2  0 2  5 3  0 3  5 PDF   0  125   0  25   0  5   1 a Log-normal distributions LogN 0  2                0 0.125 0.25 0.5 1  10 1 10 2 mean sojourn time s PS SRPT no error b SRPT               0 0.125 0.25 0.5 1  10 1 10 2 mean sojourn time s PS SRPT no error c SRVT Fig 2 Impact of job size estimation errors on the performance achieved by size-based scheduling disciplines Given a reference error distribution  i size-based scheduling is robust to approximate job sizes and ii job aging further mitigates the impact of such errors the performance of SRPT is strongly dependent on the error distribution When  0  25  where more than half of the estimations are wrong by a factor of 20 or more SRPT largely outperforms PS For higher values of   however estimation errors impact scheduling choices and the sojourn time dramatically increases We nd that aging policies effectively counter these shortcomings even a bad scheduling choice  caused for example by overestimating the size of a job  is in the long run corrected by decreasing job size through aging We verify our claim by implementing a common aging policy where the remaining processing time is decreased by the amount of work performed in a virtual PS scheduler 19 20 This technique which we label Shortest Remaining Virtual Time SRVT results in the absence of estimation errors in scheduling jobs in series following the order in which they would complete with the virtual PS As shown in Figure 2\(c the mean sojourn time of SRVT is slightly worse than that of SRPT in the absence of errors However when errors are present aging is vastly preferable We can conclude that size-based scheduling appears largely beneìcial to systems like Hadoop MapReduce even in the presence of large estimation errors we attribute this to the fact that job sizes vary by several orders of magnitude in characteristic workloads and that this eases the task of a scheduler that only has to distinguish between jobs having very different sizes In addition job aging mitigates estimation errors These ndings inspire us in the design of our system which is described next III S YSTEM I MPLEMENTATION Implementing a size-based scheduling protocol in Hadoop raises a number of challenges A few of them come from the fact that MapReduce jobs are scheduled at the lower granularity of tasks  and that they consist of two separate phases  M AP and R EDUCE  which are scheduled independently Sec III-A In addition job size is in general unknown a priori to evaluate it we develop an estimation module Sec III-D that provides at rst a coarse estimation of job size upon submission and then reìnes it after the rst few sample tasks have been run Estimations are used by an aging module Sec III-C which outputs job priorities nally the scheduler Sec III-B uses such priorities to allocate resources while ensuring that sample tasks are allocated quickly to converge rapidly to a more accurate job size estimation Next we introduce existing Hadoop schedulers we then describe the components of our system A Hadoop MapReduce MapReduce popularized by Google and by Hadoop 5 is both a programming model and an execution framework In MapReduce a job consists of three phases and accepts as input a dataset appropriately partitioned and stored in a distributed le system In the rst phase called M AP a user-deìned function is applied in parallel to input partitions to produce intermediate data stored on the local le system of each machine of the cluster intermediate data is sorted and partitioned when written to disk Then a R EDUCE phase begins It comprises a S HUFFLE sub-phase where intermediate data is pulled by the reducers  data from multiple mappers is sorted and aggregated to produce output data Hadoop Scheduling In Hadoop the J OB T RACKER coordinates the worker machines called T ASK T RACKER s The scheduler resides in the J OB T RACKER and allocates T ASK T RACKER resources to running tasks M AP and R EDUCE tasks are granted independent slots on each machine 3 The scheduler is called whenever one or more task slots become free and it decides which tasks to allocate on those slots When a single job is submitted to the cluster the scheduler assigns a number of M AP tasks equal to the number of partitions of the input data The scheduler tries to assign M AP tasks to slots available on machines in which the underlying storage layer holds the input intended to be processed a concept called data locality  Also the scheduler may need to wait for a portion of M AP tasks to nish before scheduling subsequent mappers that is the M AP phase may execute in 3 HFSP is currently implemented for Hadoop version 1 which is currently the most widely used in production settings Hadoop v ersion 2 Y ARN uses a different architecture As we further detail in Section V we believe that porting HFSP to YARN would mostly be an implementation effort rather than a research accomplishment 53 


multiple waves especially when processing very large data Similarly R EDUCE tasks are scheduled once intermediate data output from mappers is available 4 When multiple jobs are submitted to the cluster the scheduler allocates available task slots across jobs In this work we consider the Hadoop Fair Scheduler which we call FAIR FAIR groups jobs into pools generally corresponding to users or groups of users and assigns each pool a guaranteed minimum share of cluster resources which are split up among the jobs in each pool In case of excess capacity because the cluster is over dimensioned with respect to its workload or because the workload is lightweight FAIR splits it evenly between jobs When a slot becomes free and needs to be assigned a task FAIR proceeds as follows if there is any job below its minimum share it schedules a task of that particular job Otherwise FAIR schedules a task that belongs to the job that has received less resources B The Job Scheduler In our architecture the scheduler operates on a set of job priorities that are output by the aging module Sec III-C which uses job size information provided by the estimation module Sec III-D Next we highlight the main issues that we encountered while implementing our scheduler and we motivate our design choices Job Preemption Unlike the abstract protocols shown in Section II which schedule full jobs here scheduling is performed at the task granularity From an abstract point of view when the priority of a running job is lower than the one of a waiting task the running job should be preempted to free resources for the other In Hadoop this can be implemented either by killing the running tasks of the preempted job or by simply waiting for those tasks to complete Note that scheduling choices are more critical in situations of high load and that the choice of killing running tasks may result in increasing load even more because the work done by killed tasks should be performed again As such in this work we opt for wait-based preemption Job Phases In MapReduce a job is composed by a M AP phase followed optionally by a R EDUCE phase We estimate job size by observing the time needed to compute the rst few tasks of each phase for this reason we cannot estimate the length of the R EDUCE phase when scheduling M AP tasks For the purpose of scheduling choices we consider M AP and R EDUCE phases as two separate jobs For ease of exposition we thus refer to both M AP and R EDUCE phases as jobs in the remainder of this section As we experimentally show in Section IV the good properties of size-based scheduling ensure shorter mean response time for both the M AP and the R EDUCE phase resulting in better response times overall Priority to Training Initially the estimation module provides a rough estimate for the size of new jobs This estimate is then updated after the rst s sample tasks of a job are executed To guarantee that job size estimates quickly 4 More precisely a slowstart setting indicates the fraction of mappers that are required to nish before reducers are awarded execution slots converge to more accurate values the scheduler gives priority to sample tasks across jobs  up to a threshold of t  of the total number of slots Such threshold avoids starvation of regular jobs in case of a bursty job arrival pattern Data locality For performance reasons it is important to make sure that M AP tasks work on local data For this reason we use the delay scheduling strategy which postpones scheduling tasks operating on non-local data for a xed amount of attempts in those cases tasks of jobs with lower priority are scheduled instead Scheduling Policy As a result of all the choices described above our scheduling policy  which is called whenever a task slot frees up  behaves as follows 1 Select eligible jobs those with tasks waiting to be scheduled that conform to the delay scheduling constraints 2 Sort them according to the priorities obtained from the aging module 3 Check if sample tasks are running on less than t  of the slots and if one or more eligible jobs need to execute sample tasks a If so schedule a sample task from the highest priority of such jobs b Otherwise schedule a task from the highest priority eligible job C Aging Module The aging module takes as input job size estimates produced by the estimation module and outputs a priority for each active job which is used by the scheduling module described above To do that we adopt the notion of virtual time  a technique used in many practical implementations of well-known schedulers 19 20 Essentially  w e k eep track of the amount of remaining work for each job in a virtual fair system and update it every time the scheduler is called job priorities are then output sorted by amount of remaining work While the remaining work does not necessarily reîect accurately the completion time for queued jobs the order in which those jobs complete in virtual time is all that matters for size-based scheduling to work As shown at the abstract level in Section II and experimentally in Section IV this technique is robust against inaccurate job size Job aging avoids starvation achieves fairness and it requires minimal computational load since the virtual time does not incur in costly updates for jobs already in queue 19 Max-Min Fairness The estimation module outputs job sizes in a serialized form that is the sum of runtimes of each task As such the physical conìguration of the cluster does not inîuence estimated size In the virtual time instead this becomes a factor for example a job requiring only a few tasks cannot occupy the whole virtual cluster which has the same number of compute slots of the real one We simulate a Max-Min Fairness criterion to take into account jobs that request less compute slots than their fair share  i.e  1 n th of the slots if there are n active jobs a round-robin mechanism allocates virtual cluster slots starting from small jobs in terms of the number of tasks As such small jobs are implicitly 54 


given priority which reinforces the idea of scheduling small jobs as soon as possible Job Aging Each job arrival or task completion triggers a call to the job aging function which decreases the remaining amount of work for each job according to the virtual allocation described above and to the time that has passed from the last invocation of the aging function The priorities output by the module correspond to the remaining amount of work per job so that jobs with the least remaining work in the virtual time will be scheduled rst Failures The aging module is robust with respect to failures and supports cluster size upgrades the max-min fairness allocation uses the information about the number of slots in the system which is provided by the Hadoop framework once Hadoop detects a failure job aging will be slower Conversely adding nodes will result in faster job aging Job Priority and QoS Our scheduler does not currently implement a concept of job priority however the aging function can be easily modiìed to simulate a Generalized Processor Sharing discipline leading to a scheduling policy analogous to Weighted Fair Queuing D Job Size Estimation Size-based scheduling requires knowledge of job size In Hadoop such information is unavailable until a job completes however a rst rough estimate of job size can use job characteristics known a priori such as the number of tasks after the rst sample tasks have executed the estimation can be updated based on their running time The estimation component has been designed to result in minimized response time rather than coming up with perfectly accurate estimates of job length this is the reason why sample tasks should not be too many our default is s 5  and they are scheduled quickly We stress that the computation performed by the sample tasks is not thrown away the results computed by sample tasks are used to complete a job exactly as those of regular tasks 1 Initial Estimation In Hadoop the number of M AP and R EDUCE tasks each job needs is known a priori In turn each M AP task processes an input split  data essentially residing on a single xed-size HDFS block Our rst job size approximation is therefore directly proportional to the number of tasks per job The size of a M AP resp R EDUCE  job with k tasks is at rst estimated as   k  l  where l is the average size of past M AP resp R EDUCE  tasks and   1    is a tunable parameter that represents the propensity the system has to schedule jobs of unknown size At the extreme  1  new jobs are scheduled quite aggressively based on the initial estimate with the possible drawback of scheduling particularly large jobs too early More conservative choices of  1 avoid this problem but might result in increased response times by scheduling jobs later We note that particularly small jobs with s or less tasks are scheduled immediately and nish in the training phase 2 M AP Phase Size It has been observed across a variety of jobs that M AP task execution times are generally stable and short 17 It is thus reasonable to perform job size estimation using only s sample tasks albeit runtime skew may induce inaccurate size estimation We recall here that the aging module described above does not require perfect accuracy Our estimation uses a measure of the execution time  i,j for each sample task j of job i  For each job we obtain an estimate of the M AP phase size by multiplying the average sample task runtime by k  which is the number of M AP tasks for the estimated job Data Locality AM AP sample task could perform worse than normal due to network latencies if operating on non-local data However since the sample tasks are between the rst to be scheduled there is a larger choice of blocks to process making the need of operating on remote data less likely In combination with the delay scheduling strategy described in Section III-B we found that data locality issues on sample tasks as a result are negligible 3 R EDUCE Phase Size The R EDUCE phase can be broken down in two parts S HUFFLE time  needed to move and merge data from mappers to reducers  and the execution time of the R EDUCE function which can only start when the S HUFFLE phase has completed Size of S HUFFLE  As soon as a R EDUCE task is scheduled it starts pulling data from the mappers once data from all mappers is available a global sort is performed by merging all the mappers output Since each mapper output is already locally sorted a linear-time merge step is sufìcient Thus an approximate duration of the S HUFFLE phase can be computed as follows For each of the s sample R EDUCE tasks of a job we measure the time required for their S HUFFLE phase to complete This is given by the difference between the moment a task executes the R EDUCE function and the moment the same task was scheduled in the training module The estimated S HUFFLE time of the entire R EDUCE phase is then the weighted average of the individual S HUFFLE times of the sample tasks multiplied by the total number of R EDUCE tasks of the job where the weights are the normalized input data size to each sample task Execution Time The execution time of the R EDUCE phase is evaluated analogously to the M AP phase described before However R EDUCE tasks can be orders of magnitude longer than M AP tasks therefore we aim at providing an estimate of the duration of the sample tasks before their completion In particular we set a timeout   If a sample task j of job i is not completed by the timeout its estimated execution time will be   i,j    p i,j  where p i,j is the progress done during the execution stage The progress of a task is computed as the fraction of data processed by a R EDUCE task over the total amount of its input data Once we obtain the size or an estimation of it for each sample task we compute the total execution time using the same procedure described in Section III-D2 The nal estimate of the whole R EDUCE phase is obtained by adding the estimated S HUFFLE time to this estimated execution time 55 


IV E XPERIMENTS This Section focuses on a comparative analysis between the FAIR and HFSP schedulers After evaluating the global performance of the two schedulers we focus on the estimation error as output by our size estimation module A Experimental Setup We used a cluster composed by 36 T ASK T RACKER machines with 4 CPUs and 8 GB of RAM each We conìgured Hadoop according to advised best practises 24 the HDFS block size is 128 MB with replication factor of 3 each T ASK T RACKER has 2 map slots with 1 GB of RAM dedicated to each and 1 reduce slots with 2 GB of RAM In total our cluster has 72 M AP slots and 36 R EDUCE slots The slowstart factor is conìgured to start the R EDUCE phase for a job when 95 of its M AP tasks are completed HFSP operates with the following parameters the sample set size s for both M AP and R EDUCE tasks is set to 5 the  timeout to estimate R EDUCE task size is set to 10 seconds we schedule aggressively jobs that are in the training phase setting  1 and t  100  The FAIR scheduler has been conìgured with a single job pool Workloads We generate workloads using PigMix a benchmarking suite used to test the performance of Apache Pig releases PigMix is appealing to us because much like its standard counterparts for traditional DB systems such as TPC it generates realistic datasets with properties such as data skew and deìnes queries inspired by real-world data analysis tasks We generated four datasets of sizes respectively 1 GB 10 GB 40 GB and 100 GB Job arrival follows a Poisson process and jobs are generated by choosing uniformly at random a query between the 17 deìned in PigMix and applying it to one of the datasets according to a workload-deìned probability distribution We evaluate two workloads  SMALL this workload is inspired by the Facebook 2009 trace observed by Chen et al  where a majority of jobs are very small The mean interval between job arrivals is  30 s   LARGE this workload is predominantly composed of relatively heavy-duty jobs In this case the mean interval between jobs is   120 s  In Table I we report the probability distribution for choosing a particular dataset size we remark that PigMix queries operate on different subsets of the generated datasets resulting in a variable number of M AP R EDUCE tasks Each workload is composed of 100 jobs and both HFSP and FAIR have been evaluated using the same jobs the same inputs and the same submission schedule We have additional results  not included here for lack of space  that conìrm our results on different platforms Amazon EC2 and the Hadoop Mumak emulator and with different workloads synthetic traces generated by SWIM The y are available in a technical report TABLE I J OB SIZES IN OUR EXPERIMENTAL WORKLOADS   Dataset size Map tasks Workload SMALL LARGE 1GB  5 65 0 10 GB 10  50 20 10 40 GB 50  150 10 60 100 GB  150 5 30 TABLE II M EAN SOJOURN TIME MST AND MEAN LOAD  Workload MST s Mean Load FAIR HFSP FAIR HFSP SMALL 63 53 2.26 1.99 LARGE 2,291 544 16.80 4.60 B Macro Benchmarks In order to evaluate the overall performance of our system we compare FAIR with HFSP on sojourn time  the interval between a jobês submission and its completion  and load in terms of number of pending jobs i.e those that have been submitted and not yet completed Table II shows mean sojourn time across all jobs and mean load over the duration of the experiment for our two workloads In the SMALL workload HFSP decreases the mean sojourn time by around 16 By observing the empirical cumulative distribution function ECDF of sojourn times in Figure 3\(a we notice larger differences between FAIR and HFSP for jobs with longer sojourn times note the logarithmic scale on the x axis In this workload the system is on average loaded with around 2 pending jobs see Table II since these jobs are often small the system is generally able to allocate all tasks of pending jobs resulting in analogous scheduling choices and therefore sojourn time for both FAIR and HFSP However when system load is higher HFSP outperforms FAIR Our results are strikingly different for the LARGE workload Figure 3\(b where the mean sojourn time with HFSP is less than a quarter of the one with FAIR In this workload most jobs require several task slots and complete more quickly since HFSP awards them the entire cluster if needed when they are scheduled Instead the sharing strategy of FAIR has the drawback of increasing the sojourn time of all jobs M AP phases of most jobs complete earlier in HFSP making it possible to schedule R EDUCE phases sooner than with FAIR As a result with HFSP 30 of jobs complete within 100 seconds from their submission while in the same time window FAIR only completes 2 of them after 1,000 seconds from submission 90 of jobs are completed with HFSP while only 15 are completed with FAIR Scheduling choices are more critical when the cluster is loaded by jobs that require many resources and the difference between the SMALL and LARGE workloads exempliìes this clearly Figure 3\(c shows the evolution of load run on the LARGE workload even if the job submission schedule for HFSP and FAIR is the same load is promptly decreased in HFSP by focusing resources on single jobs The fact that 56 


             10 1 10 2 10 3 Sojourn Time s 0  0 0  2 0  4 0  6 0  8 1  0 ECDF HFSP FAIR a Sojourn time for the SMALL workload                10 1 10 2 10 3 10 4 Sojourn Time s 0  0 0  2 0  4 0  6 0  8 1  0 ECDF HFSP FAIR b Sojourn time for the LARGE workload                              0  00  51  01  52  02  53  03  54  04  5 Time h 0 5 10 15 20 25 30 35 Load pending jobs HFSP FAIR c Cluster load for the LARGE workload Fig 3 Macro benchmark results scheduling becomes more critical in situations of high load is indeed conìrmed by our simulation results These results allow us to conclude that HFSP performs better than FAIR in two very different workloads the advantage is more pronounced when the job and workload size is large with respect to the cluster size In that case scheduling decisions become critical and the inefìciencies of simple fair sharing become apparent C Estimation Errors and Sojourn Times We have shown that HFSP outperforms FAIR in particular when applied to clusters with high load and heavy jobs Next we characterize estimation errors we measured in our experiments and discuss their impact on job sojourn times in light of our initial analysis of scheduling performance discussed in Section II-B In our experiments task times are clearly skewed Figure 4 shows the distribution of task times measured for all our experiments most tasks complete within few tens of seconds but around 10 of R EDUCE tasks and a non-negligible number of M AP tasks need orders of magnitude more time to complete As such we now characterize the job size estimation errors induced by our sampling-based technique As done in Section II-B when s i is the real size of the job and  s k i the estimated size obtained using k sample tasks we deìne the estimation error as  k i  s k i s i   k j  1 means that job size is under-estimated whereas  k j  1 in case of over-estimation Figure 5 shows the ECDF of estimation errors across all our experiments for our setting of k 5 sample tasks The empirically observed error distribution maps well to the log-normal distribution we use in Section II-B Using a maximum likelihood tting method we approximate the error distribution as LogN    2   for M AP   0  0976 and  0  411  for R EDUCE   0  0878 and  0  228  The Kolmogorov-Smirnov goodness of t test does not reject the tting at a signiìcance level of 0.05 It is impossible to evaluate HFSP in a real deployment and in the complete absence of estimation errors since execution time of a given job in Hadoop varies at each run according to complex and rather unpredictable system properties  T o isolate the impact of errors on scheduling and sojourn                  10 0 10 1 10 2 10 3 10 4 Task Time 0  0 0  2 0  4 0  6 0  8 1  0 ECDF M AP R EDUCE Fig 4 Task time distribution                  0.25 0.5 124 Error 0  0 0  2 0  4 0  6 0  8 1  0 ECDF M AP R EDUCE Fig 5 Estimation error  k i for k 5  time we thus turn to our simulation results on SRVT which is a size-based scheduler with aging induced by fair sharing in virtual time It thus can be seen as a model for HFSP which abstracts from the intricacies of a real system deployment In our observed errors the estimation module tends to slightly over-estimate job sizes i.e  0 and error distributions are not exactly centered on 1 this results in marginally slower job aging with respect to the  0 case Moreover the  values suggest that the impact of size estimation errors on sojourn time are small as shown in Figure 2\(c on page 3 for log-normal error distributions with  0  5  SRVT achieves a mean sojourn time which is close to the one obtained with no estimation errors We believe that HFSP is likely to be similarly tolerant to such errors Indeed the main difference between SRVT and HFSP is that the latter operates in an environment that 57 


has to deal with the constraints imposed by Hadoop such as data locality task granularity and dependencies between M AP and R EDUCE phases see Section III-B These constraints limit the degrees of freedom available to the scheduler and result in cases where HFSP will take the same scheduling choices regardless of estimation errors With analogous error distributions and more possibility to deviate from optimal behavior SRVT achieves near-optimal mean sojourn time this suggests that while HFSP could certainly beneìt from more sophisticated and accurate size estimation methods further improvements in sojourn times are likely to be marginal V R ELATED W ORK MapReduce in general and Hadoop in particular have received considerable attention recently both from the industry and from academia Since we focus on job scheduling we consider here the literature pertaining to this domain Theoretical Approaches Several theoretical works tackle scheduling in multi-server system s  a recent example is the work by Moseley and Fox These w orks which are elegant and important contributions to the domain provide performance bounds and optimality results based on simplifying assumptions on the execution system e.g jobs with a single phase Some works provide interesting approximability results applied to simpliìed models of MapReduce 31 In contrast we focus on the design and implementation of a scheduling mechanism taking into account all the details and intricacies of a real system Fairness and QoS Several works take a system-based approach to scheduling on MapReduce For instance the FAIR scheduler and its enhancement with a delay scheduler is a prominent example to which we compare our results Several other works  focus on resource allocation and strive at achieving fairness across jobs but do not aim at optimizing sojourn times Sandholm and Lai study the resource assignment problem through the lenses of a bidding system to achieve a dynamic priority system and implement quality of service for jobs Kc and Anyanwu address the problem of scheduling jobs to meet user-provided deadlines but assume job runtime to be an input to the scheduler Flex is a size-based scheduler for Hadoop which is available as a proprietary commercial solution In Flex fairness is deìned as avoiding job starvation and guaranteed by allocating a part of the cluster according to Hadoopês FAIR scheduler size-based scheduling without aging is then performed only on the remaining set of nodes In contrast by using aging our approach can guarantee fairness while allocating all cluster resources to the highest priority job thus completing it as soon as possible Job Size Estimation Various recent approaches  propose techniques to estimate query sizes in recurring jobs Agarwal et al  report that recurring jobs are around 40 of all those running in Bingês production servers Our estimation module on the other hand works on-line with any job submitted to a Hadoop cluster but it has been designed so that the estimator module can be easily plugged with other mechanisms beneìtting from advanced and tailored solutions Complementary approaches Task size skew is a problem in general for MapReduce applications since larger tasks delay the completion of a whole job skew also makes job size estimation more difìcult The approach of SkewTune greatly mitigates the issue of skew in task processing times with a plug-in module that seamlessly integrates in Hadoop which can be used in conjunction with HFSP Tian et al 13 propose a mechanism where IO-bound and CPU-bound jobs run concurrently beneìtting from the absence of conîicts on resources between them We remark that also in this case it is possible to beneìt from size-based scheduling as it can be applied separately on the IOand CPU-bound queues Tan et al  41 propose strate gies to adapti v ely start the R EDUCE phase in order to avoid starving jobs also this technique is orthogonal to the rest of scheduling choices and can be integrated in our approach Hadoop offers a Capacity Scheduler which is designed to be operated in multitenant clusters where different organizations submit jobs to the same clusters in separate queues obtaining a guaranteed amount of resources We remark that also this idea is complementary to our proposal since jobs in each queue could be scheduled according to a size-based policy such as HFSP and reap according beneìts Framework Schedulers Recent works have pushed the idea of sharing cluster resources at the framework level for example to enable MapReduce and Spark applications to run concurrently Monolithic schedulers such as YARN and Omega use a single component to allocate resources to each framework while two-level schedulers 47 ha v e a single manager that negotiates resources with independent framework-speciìc schedulers We believe that such framework schedulers impose no conceptual barriers for size-based scheduling but the implementation would require very careful engineering In particular size-based scheduling should only be limited to batch applications rather than streaming or interactive ones that require continuous progress VI C ONCLUSION Our work was motivated by the realization that MapReduce has evolved to the point where shared clusters are used for a wide range of workloads which include a non-negligible fraction of interactive data processing tasks As a consequence we have witnessed the raise of deployment best practices in which long sojourn times  due to a fair sharing of resources among competing jobs  were compensated by overdimensioned Hadoop clusters In addition we remarked that an efìcient cluster utilization could be approximated through a tedious manual exercise involving the creation of static resource pools to accommodate workload diversity and an important tuning effort To overcome such limitations in this work we set off to study the beneìts of a new scheduling discipline that targets at the same time short sojourn times and fairness among jobs We thus proposed a size-based approach to scheduling jobs 58 


in Hadoop which we called HFSP Our work brought up several challenges evaluating job size on-line without wasting resources avoiding job starvation both on small and large jobs and guaranteeing short sojourn time despite estimation errors were the most noteworthy We solved these problems in the context of a multi-server system using virtual time and aging that is built to be tolerant to failures scale-out upgrades and supports the composite job structure of MapReduce We showed that a size-based discipline such as HFSP performs very well and that a precise job size information is not essential for the scheduler to function properly Our experimental results in which we compared HFSP to the widely used FAIR scheduler indicate that both interactivity and efìciency requirements were largely met both small and large jobs do not starve and the job sojourn time distribution is consistently in favor of HFSP Our work has practical consequences as well HFSP is simple to conìgure and allows resource pools to be consolidated because workload diversity is intrinsically accounted for by the size-based discipline Our future work is related to job preemption We are currently investigating a novel technique to ll the gap between killing running tasks and waiting for tasks to nish Indeed killing a task too late is a huge waste of work and waiting for a task to complete when it just started is detrimental as well Our next goal is thus to provide a new set of primitives to suspend and resume tasks to achieve better preemption A CKNOWLEDGEMENTS This work has been partially supported by the EU projects BigFoot FP7-ICT-223850 and mPlane FP7-ICT-318627 R EFERENCES  J Dean and S Ghema w at MapReduce Simpliìed data processing on large clusters in Proc of USENIX OSDI  2004  Y  Chen S Alspaugh and R Katz Interacti v e query processing in big data systems A cross-industry study of MapReduce workloads in Proc of VLDB  2012  K Ren et al  Hadoopês adolescence An analysis of Hadoop usage in scientiìc workloads in Proc of VLDB  2013  Apache Oozie W orkîo w Scheduler  http://oozie.apache.or g   Hadoop Open source implementation of MapReduce  http hadoop.apache.org  E Friedman and S Henderson F airness and ef cienc y i n web serv er protocols in Proc of ACM SIGMETRICS  2003  L E Schrage and L W  Miller  The queue m/g/1 with the shortest remaining processing time discipline Operations Research  vol 14 no 4 1966  M Harchol-Balter et al  Size-based scheduling to improve web performance ACM TOCS  vol 21 no 2 2003  A V erma L Cherkaso v a  and R H Campbell  Aria automatic resource inference and allocation for MapReduce environments in Proc of ICAC  2011   T w o sides of a coin Optimizing the schedule of MapReduce jobs to minimize their makespan and improve cluster performance in Proc of IEEE MASCOTS  2012  S Agarw al et al  Re-optimizing Data-Parallel Computing in Proc of USENIX NSDI  2012  A D Popescu et al  Same queries different data Can we predict query performance in Proc of SMDB  2012  C T ian et al  A dynamic MapReduce scheduler for heterogeneous workloads in Proc of IEEE GCC  2009  D Lu H Sheng and P  Dinda Size-based scheduling policies with inaccurate scheduling information in Proc of IEEE MASCOTS  2004  Y  Chen et al  Statistical workload injector for MapReduce https github.com/SWIMProjectUCB/SWIM  M Zaharia et al  Delay scheduling A simple technique for achieving locality and fairness in cluster scheduling in Proc of ACM EuroSys  2010  Y  Chen A Ganapathi R.Grif th and R Katz The case for e v aluating MapReduce performance using workload suites in Proc of IEEE MASCOTS  2011  M DellêAmico  A simulator for data-intensi v e job scheduling  EURECOM Tech Rep RR-13-282 2013  J Nagle On pack et switches with inìnite storage  Communications IEEE Transactions on  vol 35 no 4 1987  S Gorinsk y and C Jechlitschek F air ef cienc y  or lo w a v erage delay without starvation in Proc of IEEE ICCCN  2007  Apache Hadoop wiki po wered by   http://wiki.apache.or g/hadoop PoweredBy  D Stiliadis and A V arma Latenc y-rate serv ers a general model for analysis of trafìc scheduling algorithms IEEE/ACM TON  vol 6 no 5 1998  Apache Hadoop f air scheduler   http://hadoop.apache.or g/docs/stable fair  scheduler.html   Hadoop MapReduce JIRA 1184  https://issues.apache.or g/jira browse/MAPREDUCE-1184   PigMix  https://cwiki.apache.or g/PIG/pigmix.html  TPC Tpc benchmarks  http://www tpc.or g/information/benchmarks asp  M P astorelli et al  Practical size-based scheduling for MapReduce workloads CoRR  vol abs/1302.2749 2013  G Ananthanarayanan et al  Reining in the outliers in map-reduce clusters using mantri in Proc of USENIX OSDI  2010  K F o x and B Mosele y  Online scheduling on identical machines using SRPT in In Proc of ACM-SIAM SODA  2011  H Chang et al  Scheduling in MapReduce-like systems for fast completion time in Proc of IEEE INFOCOM  2011  B Mosele y et al  On scheduling in map-reduce and ow-shops in In Proc of ACM SPAA  2011  T  Sandholm and K  Lai MapReduce optimization using re gulated dynamic prioritization in Proc of ACM SIGMETRICS  2009  M Isard et al  Quincy fair scheduling for distributed computing clusters in Proc of ACM SOSP  2009  A Ghodsi et al  Dominant resource fairness Fair allocation of multiple resources types in Proc of USENIX NSDI  2011  B Hindman et al  Mesos A platform for ne-grained resource sharing in the data center in Proc of USENIX NSDI  2011  T  Sandholm and K Lai Dynamic proportional share scheduling in Hadoop in Proc of JSSPP  2010  K Kc and K An yanwu Scheduling Hadoop jobs to meet deadlines  in Proc of CloudCom  2010  J W olf et al  FLEX A slot allocation scheduling optimizer for MapReduce workloads in Proc of ACM MIDDLEWARE  2010  Y  Kw on et al  Skewtune mitigating skew in MapReduce applications in Proc of ACM SIGMOD  2012  J T an X Meng and L Zhang Delay tails in MapReduce scheduling  in Proc of ACM SIGMETRICS  2012   Performance analysis of coupling scheduler for MapReduce/Hadoop in Proc of IEEE INFOCOM  2012  Apache Hadoop capacity scheduler   http://hadoop.apache.or g/docs stable/capacity scheduler.html  M Zaharia et al  Resilient distributed datasets a fault-tolerant abstraction for in-memory cluster computing in Proc of USENIX NSDI  2012  Apache Hadoop ne xtgen MapReduce yarn  http://hadoop.apache org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html  M Schw arzk opf et al  Omega exible scalable schedulers for large compute clusters in Proc of EuroSys  2013  B Hindman et al  Mesos a platform for ne-grained resource sharing in the data center in Proc of USENIX NSDI  2011  Apache Hadoop on demand  http://hadoop.apache.or g/docs/stable hod scheduler.html 59 


608 


  11 that it will be able to meet all of the Van Allen Probes communications goals with its intended ground segments A CKNOWLEDGEMENTS  This work was performed with the support of the Radiation Belt Storm Probes mission under NASA\222s Living with a Star program. The authors would like to thank Rick Fitzgerald and Kim Cooper, Van Allen Probes project managers at JHU/APL for supporting this work. There are many at JHU/APL who contributed to the development and verification of the RF system. Significant technical contributions were made by: Christopher Haskins, Bob Wallis, Matthew Angert, Laurel Funk, Joe Sheehi, Wesley Millard, Norman Adams, Lloyd Ellis, Sheng Cheng, John Daniels, Phillip Huang, Avi Sharma, Carl Herrmann, David Jones, Brian Bubnash, Melanie Bell, Horace Malcom Michael Pavlick, Mark Bernacik, Christopher Deboy, Bob Bokulic, Sharon Ling, Albert Hong, Erik Hohlfeld, Judy Bitman, William Dove and Tony Garcia. Significant contributions were also made by the USN and TDRSS compatibility test teams  R EFERENCES  1 eck D. G.; Mau k  B  H.; Greb o w sk y  J  M.; Fo x  N J, \223The Living With a Star Radiation Belt Storm Probes Mission and Related Missions of Opportunity 224 American Geophysical Union, Fall Meeting 2006   h o rs k i y  A Y., Mauk B. H., Fox N. J Sibeck D G., Grebowsky, J. M., \223Radiation belt storm probes Resolving fundamental physics with practical consequences,\224 Journal of Atmospheric and SolarTerrestrial Physics Vol. 73, Issues 11-12, July 2011 Pages 1417-1424   S. Bu s h m a n M. Bu tler, R C o n d e, K. Fretz, C  Herrmann, A. Hill, R. Maurer, R. Nichols, G. Ottman M. Reid, G. Rogers, D. Srinivasan, J. Troll, B. Williams 223Radiation Belt Storm Probe Spacecraft and Impact of Environment on Spacecraft Design,\224 Proceedings of the 2012 IEEE Aerospace Conference, Big Sky Montana USA, March 3-10, 2012   opelan d D.J DeB o y C  C R o y s ter, D.W., Dov e  W.C., Srinivasan, D.K,. Bruzzi, J.R., Garcia, A., "The APL 18.3m station upgrade and its application to lunar missions," Aerospace Conference, 2010 IEEE , vol., no pp.1-10, 6-13 March 2010    Figure 10. FER/BER performance for all downlink modes for RF GSE, SCF, USN, and TDRSS 


  12  iv as a n D. K., A r ti s  D  A Bak er, R  B., Stil w e ll, R   K., Wallis, R. E., \223RF Communications Subsystem for the Radiation Belt Storm Probes,\224  Acta Astronautica vol 65, issue 11-12, December 2009, Pages 1639-1649   k i n s  C B., Mi llard, W P 223 M u l t i Ban d  So f t w a re Defined Radio for Spaceborne Communications Navigation, Radio Science, and Sensors,\224 2010 IEEE Aerospace Conference, March 2010  k i n s  C B., Mi llard, W P A d a m s  N. H Sri n i v a s a n  D. K., Angert, M. P., \223The Frontier Software-Defined Radio: Mission-Enabling, Multi-Band, Low-Power Performance,\224 61st  International Astronautical Congress IAC-10.B2.5.11, October 2011 8  Crowne, M.J.,  Haskins, C. B., Wallis, R. E.,  Royster D.W, \223Demonstrating TRL-6 on the JHU/APL Frontier Radio for the Radiation Belt Storm Probe mission,\224 2011 IEEE Aerospace Conference, March 2011  o ckw ood, M. K K i n n i s o n  J., F o x  N C o n d e, R  Driesman, A., \223Solar Probe Plus Mission Definition,\224 63rd  International Astronautical Congress, IAC 12.A3.5.2, October 2012   i t m a n J  223An I n D ept h  L o o k at t h e R a dio Freq u e n c y    Ground Support Equipment for the Radiation Belt Storm  Probes Mission,\223 IEEE Autotestcon, 2011, September 2011  d a m s  N.H., Bi t m a n J C opela n d D. J Sri n ivas a n  D  K.,  Garcia. A., \223RF Interference at Ground Stations Located in Populated Areas,\224 2013 IEEE Aerospace Conference, March 2013  B IOGRAPHY  Matthew J. Crowne is a member of the Senior Professional Staff of the RF Engineering group in JHU/APL\222s Space Department. He received his B.S from Johns Hopkins University in 2000 and his M.S. from the same university in 2009, both in electrical engineering Matthew joined JHU/APL in 2007 where he has been working on the development of radios for spaceflight communications systems. Prior to joining JHU/APL, he worked for Integrated Defense Systems Inc., where he developed solid state power amplifiers for electronic warfare and communication systems. Matthew was the integration and test lead for the Van Allen Probes RF communication system and is currently working on the Solar Probe Plus mission   Dipak K. Srinivasan is the supervisor of the RF Systems Engineering Section in the JHU/APL Space Department. He received his B.S. and M.Eng. in electrical engineering in 1999 and 2000 in electrical engineering from Cornell University, and an M.S. in applied physics from The Johns Hopkins University in 2003. Dipak joined the APL Space Department in 2000, where he has served as the lead RF Integration and Test Engineer for the CONTOUR and MESSENGER spacecraft and lead mission system verification engineer for the New Horizons project. He is currently the Lead RF Telecommunications Systems Engineer for the MESSENGER and Van Allen Probes missions and chairs technical sessions at the annual International Astronautical Congress  Darryl W. Royster is a member of the Senior Professional Staff in the RF Engineering Group at JHU/APL.  He led compatibility testing for the Van Allen Probes, STEREO, and MESSENGER missions.  Previously he was the System Engineer for the Satellite Communications Facility at JHU/ APL and the lead RF Integration and Test Engineer for the STEREO spacecraft.  Prior to joining the JHI/APL Space Department in 2001, Mr. Royster designed cellular and land mobile radio products for Ericsson, GE and Motorola.  He received his B.S. and M.S. in electrical engineering from Virginia Polytechnic Institute and State University in 1982 and 1984, respectively  Gregory L. Weaver joined the Senior Professional Staff of JHU/APL in 2003 and works within the RF Engineering Group of the Space Department.  He is a technologist with extensive background in the technical and business aspects of the frequency control industry and has held positions as a senior design engineer, technical manager and marketing strategist over a 25 year career history, including vice president positions with Bliley Technologies Inc. and the former Piezo Crystal Company. He received his M.S in Technology Management from the University of Pennsylvania in 1993 and his B.S. in Physics from Dickinson College in 1982.  He is a licensed professional engineer in the state of Pennsylvania, member of the IEEE and the UFFC Societ y.  He has contributed to the technical proceedings of the IEEE International Frequency Control Symposium, Precise Time and Time Interval Systems and Application Meeting and the European Frequency and Time Forum   


  13 Daniel Matlin is an Associate Professional Staff at JHU/APL and a member of the RF engineering group in the Space department.  He went through a dual Bachelors/Masters program at Johns Hopkins University graduating with his Bachelor of Science in Electrical Engineering in 2008 and his Masters of Science in Engineering from the Electrical Engineering department in 2009.  As a student he specialized in RF systems design.  Mr. Matlin started at the JHU/APL in February of 2010 and in his short time with the lab has been privileged to work on various tasks supporting the RBSP program, including supporting a successful launch and early operations.  Mr. Matlin assisted in the qualification testing for the flight DSP slices as well as the integrated flight transceivers.  He also carried out electrical testing and flight qualification of the newly designed Hypertronics stacking connectors as well as components and cables used for the RF subsystem  Nelli Mosavi is an EMC and RF Engineer in the JHU/APL Space Department, RF Systems Engineering section. She received a B.S. degree in Electrical Engineering from Oakland University Michigan in 2004 and an M.S. in Electrical Engineering from The Johns Hopkins University in 2010. She is currently working toward her Ph.D. at the University of Maryland Baltimore County. She joined APL in 2009 and has since been working on RF and EME issues on the Van Allen Probes mission. Nelli previously worked for SENTEL Corporation, General Motors, DENSO International, and Molex Automotive   


APPENDIX 3ñ RESULTS \(SEM I-PROFESSIONAL DSLRS     Run by TFDEA add-in ver 2.1 Frontier Type Orientation 2nd Goal Return to Scale Avg RoC Frontier Year MAD Dynamic OO Max CRS 1.124802 2008 1.394531 Input\(s Output\(s SOA products at Release SOA products on Frontier RoC contributors Release before forecast Release after forecast 22166527 DMU Name Date Efficiency_R Efficiency_F Effective Date Rate of Change Forecasted Date 1 Nikon D100 2002 1 1.66666667 2007.000000 1.107566 2 Olympus E1 2003 1 1.666666667 2007.000000 1.136219 3 Pentax *ist D 2003 1 1.358024691 2007.000000 1.079511 4 Nikon D20 0 2005 1 1.2 2007.000000 1.095445 5 Canon EOS 5D 2005 1 1.664796311 2007.730028 1.205269 6 Pentax K10D 2006 1 1 2006.000000  7Nikon D30 0 2007 1 1 2007.000000  8 Olympus E3 2007 1 1 2007.000000  9 Sony Alpha DSLR A70 0 2007 1 1 2007.000000  1 0 Nikon D70 0 2008 1.46 1.46 2007.000000  11 Canon EOS 5D Mark II 2008 1.065464119 1.065464119 2008.000000  12 Sony Alpha DSLR A90 0 2008 1 1 2008.000000  13 Olympus E3 0 2008 1.02 1.02 2007.000000  1 4 Pentax K20D 2008 1 1 2008.000000  15 Nikon D300s 2009 1.142857143 0.874450785 2007.000000  2008.140742 16 Canon EOS 7D 2009 1 0.754166667 2007.000000  2009.399022 17 Sony Alpha DSLR A85 0 2009 1 0.774820627 2008.000000  2010.169290 18 Pentax K-7 2009 1 0.772738276 2006.503130  2008.695302 19 Olympus E5 201 0 1.466133763 1.173333333 2007.000000   2 0 Pentax K-5 201 0 1.009024674 0.776190476 2007.000000  2009.15427 0 21 Nikon D80 0 2012 1 0.686950618 2008.000000  2011.192776 22 Canon EOS 5D Mark III 2012 1.115010291 0.930769231 2007.502755  2008.112786 23 Pentax K-5 II 2012 1 0.632075669 2006.839705  2010.740375 24 Sony Alpha SLT A99 2012 1.009662059 0.854117647 2007.640496  2008.981286 Results 2129 2013 Proceedings of PICMET '13: Technology Management for Emerging Technologies 


