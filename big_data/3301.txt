Privacy Preserving Data Classiìcation with Rotation Perturbation Keke Chen Ling Liu College of Computing Georgia Institute of Technology  kekechen lingliu  cc.gatech.edu 1 Introduction Data perturbation techniques are one of the most popular models for privacy preserving data mining 3 It is especially convenient for applications where the data owners need to export/publish the privacy-sensitive data A data perturbation procedure can be simply described as follows Before the data owner publishes the data they 
randomly change the data in certain way to disguise the sensitive information while preserving the particular data property that is critical for building the data models Several perturbation techniques have been proposed recently among which the most typical ones are randomization approach and condensation approach Loss of Privacy vs Loss of Information Perturbation techniques are often evaluated with two basic metrics loss of privacy and loss of information resulting in loss of accuracy for data classiìcation An ideal data perturbation algorithm should aim at minimizing both privacy loss and information loss However the two metrics 
are not well-balanced in many existing perturbation techniques 3 2 5 Loss of privacy can be intuitively described as the difìculty level in estimating the original value from the perturbed data In the v ariance of the added random noise is used as the level of difìculty for estimating the original values However later research 5 re v eals that v ariance is not an effective indicator for randomization approach since the original data distribution has to be known In addition paper sho ws that the loss of pri v a c y is also subject to the special attacks that can reconstruct the original data 
from the perturbed data The loss of information typically refers to the amount of critical information preserved about the datasets after the perturbation Different data mining tasks such as classiìcation and association mining typically utilize different set of properties of a dataset Thus the information that is considered critical to data classiìcation may differ from those critical to association rule mining We argue that the exact information that need to be preserved after perturbation should be task-speciìc Since most classiìcation models typically concern multi-dimensional properties perturbation techniques for data classiìcation should perturb multiple columns together To our knowledge very few 
perturbation-based privacy protection proposals so far have considered multi-dimensional perturbation techniques  Contribution and Scope of the paper  Bearing these issues in mind we have developed a random rotation perturbation approach to privacy preserving data classiìcation In contrast to other existing privacy preserving classiìcation methods 1 3 our approach e xploits the task-speciìc information about the datasets to be classiìed aiming at producing a robust data perturbation that exhibits a better balance between loss of privacy and loss of information without performance penalty Concretely we observe that the multi-dimensional geo 
metric properties of datasets are the critical task-speciìc information for many classiìcation algorithms One intuitive way to preserve the multi-dimensional geometric properties is to perturb the original dataset through rotation transformation We have identiìed and proved that kernel methods SVM classiìers with the three popular kernels and the hyperplane-based classiìers are the three categories of classiìers that are rotation-invariant Another important challenge for the rotation perturbation approach is the privacy loss measurement the level of uncertainty and privacy assurance the resilience of the rotation transformation against unauthorized disclosure 
Given that a random rotation based perturbation is a multidimensional perturbation the privacy guarantee of the multiple dimensions attributes should be evaluated collectively to ensure the privacy of all columns involved and the privacy of the multi-column correlations We design a uniìed privacy model to tackle the problem of privacy evaluation for multi-dimensional perturbation which addresses three types of possible attacks direct estimation approximate reconstruction and distribution-based inference attacks With the uniìed privacy metric we present the privacy assurance of the random rotation perturbation as an op 
timization problem given that all rotation transformations result in zero-loss of accuracy for the discussed classiìers we want to pick one rotation matrix that provides higher privacy guarantee and stronger resilience against the three types of inference attacks 1 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


2 Rotation and Classiìers In this section we rst describe rotation transformation and the set of geometric properties of the datasets signiìcant to most classiìers and then we deìne rotation-invariant classiìers Notations for Datasets Training dataset is the part of data that has to be exported/published in privacy-preserving data classiìcation A classiìer learns the classiìcation model from the training data and then is applied to classify the unclassiìed data Suppose that X is a training dataset consisting of N data rows records and d columns attributes For the convenience of mathematical manipulation we use X d  N to notate the dataset i.e X  x 1  x N   where x i is a data tuple representing a vector in the real space R d  Each data tuple belongs to a predeìned class which is determined by its class label attribute y i  The class labels can be nominal or continuous for regression The class label attribute of the data tuple is public i.e privacy-insensitive All other attributes containing private information needs to be protected Properties of Geometric Rotation Let R d  d represent the rotation matrix Geometric rotation of the data X is generally notated as a function g  X   g  X  RX  Note that the transformation will not change the class label of data tuples i.e R x i still has the label y i  A rotation matrix R d  d is deìned as a matrix having the follows properties Let R T represent the transpose of the matrix R  r ij represent the  i j  element of R  and I be the identity matrix Both the rows and the columns of R are orthonormal  i.e for any column j   d i 1 r 2 ij 1  and for any two columns j and k   d i 1 r ij r ik 0  The similar property is held for rows The deìnition infers that R T R  RR T  I  It also implies that by changing the order of the rows or columns of rotation matrix the resulting matrix is still a rotation matrix A key feature of rotation transformation is preserving length It follows that rotation also preserves inner product and Euclidean distance between any pair of points In general rotation preserves the geometric shapes such as hyperplane and hyper curved surface in the multidimensional space Rotation-invariant Classiìers We can treat the classiìcation problem as function approximation problem  the classiìers are the functions learned from the training data Therefore we can use functions to represent the classiìers Let  f X represent a classiìer  f trained with dataset X and  f X  Y  be the classiìcation result on dataset Y  Let T  X  be any transformation function which transforms the dataset X to another dataset X   We use Err   f X  Y  to notate the error rate of classiìer  f X on testing data Y and let  be some small real number     1  Deìnition 1 A classiìer  f is invariant to some transformation T if and only if Err   f X  Y   Err   f T  X   T  Y    for any training dataset X and testing dataset Y  It follows that the strict condition  f X  Y    f T  X   T  Y  trivially guarantees the invariance property If a classiìer  f is invariant to rotation transformation we speciìcally name it as a rotation-invariant classiìer  The initial result shows several popular classiìers dealing with numerical data are rotation-invariant Due to the space limitation we will ignore the concrete proofs and summarize that KNN  general Kernel methods SVM classiìers using polynomial radial basis and neural network kernels and Hyperplane-based classiìers are invariant to rotation 3 Evaluating Privacy Quality for Random Rotation Perturbation The goals of rotation based data perturbation are twofold preserving the accuracy of classiìers and preserving the privacy of data The discussion about the rotationinvariant classiìers has proven that the rotation transformation theoretically guarantees zero-loss of accuracy for three popular types of classiìers We dedicate this section to discuss how good the rotation perturbation approach is in terms of preserving privacy The critical step to identify the good rotation perturbation is to deìne a multi-column privacy measure for evaluating the privacy quality of any rotation perturbation to a given dataset With this privacy measure we can employ some optimization methods to nd good rotation perturbations for a given dataset For data perturbation approach the quality of preserved privacy can be understood as the difìculty level of estimating the original data from the perturbed data Basically the attacks to the data perturbation techniques can be summarized in three categories 1 the original data directly from the perturbed data 3 without an y other knowledge about the data naive inference 2 approximately reconstructing the data from the perturbed data and then estimating the original data from the reconstructed data 8 approximation-based inference and 3 if the distributions of the original columns are known the values or the properties of the values in the particular part of the distribution can be estimated 2 distrib ution-based inference A multi-colum metric should be applicable to all three types of inference attacks to determine the robustness of the perturbation technique We will focus on the rst two attacks in this paper 3.1 Privacy Model for Multi-column Perturbation Unlike the existing value randomization methods where multiple columns are perturbed separately the random rotation perturbation needs to perturb all columns together 2 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


where the privacy quality of all columns is correlated under one single transformation Since in practice different columns\(attributes may have different privacy concern we consider that a generalpurpose privacy metric  for entire dataset is based on column privacy metric  An abstract privacy model is deìned as follows Let p be the column privacy metric vector p  p 1 p 2 p d  for d columns and there are privacy weights associated to the columns respectively notated as w  w 1 w 2 w d    p  w  deìnes the overall privacy guarantee Basically the design of privacy model should consider determining the three factors p  w  and function   We summarize our design of privacy metric as follows Uniìed Column Privacy Metrics Below we extend the variance-based privacy metric to the multi-column uniìed metric Let Y be a random variable representing a column of the dataset Y  be the perturbed/reconstructed result of Y  and D be the difference between Y and Y   Let E  D  and Var  D  denote the mean and the variance of difference VoD respectively E  D  is not effective in protecting privacy thus VoD becomes the primary measure in terms of the rst level of inferences Unfortunately this single-column privacy metric does not work across different columns since it ignores the effect of value range and the mean of the original data column It is easy to understand that the same amount of VoD is not equally effective for different value ranges One effective way to unify the different value ranges is via normalization  Let s i 1   max  Y i   min  Y i   t i  min  Y i    max  Y i   min  Y i  denote the constants determined by the value range of the column Y i  The column Y i is scaled to range 0 generating Y si  with the transformation Y si  s i  Y i  t i   This allows all columns to be evaluated on the same base eliminating the effect of diverse value ranges The normalized data Y si is then perturbed to Y  si  Let D  i  Y  si  Y si  We use Var  D  i   instead of Var  D i   as the uniìed column privacy metric Composing the Column Metrics Having the uniìed column metrics p  we can compose the multiple metrics into one metric for optimization Let w denote the importance of columns in terms of preserving privacy Intuitively the more important the column is the higher level of privacy guarantee will be required for the perturbed data column Therefore we let  d i 1 w i 1 and use p i w i to represent the weighted column privacy  The rst composition function is the minimum privacy guarantee among all columns Concretely when we measure the privacy quality of a multi-column perturbation we need to pay special attention to the column having the lowest weighted column privacy because such columns could become the breaking point of privacy Hence we design the minimum privacy guarantee  1 min d i 1  p i w i   Similarly the average privacy guarantee of the multi-column perturbation  2  1  d  d i 1 p i w i  is another interesting measure With the deìnition of privacy guarantee we can evaluate and optimize the privacy quality of a give perturbation Multi-column Privacy Analysis for Random Rotation Perturbation With the variance metric over the normalized data we can formally analyze the privacy quality of random rotation perturbation Let X be the normalized dataset X  be the rotation of X  and I d be the d dimensional identity matrix Thus VoD can be evaluated based on the difference matrix X   X  and the VoD for column i is the element i,i in the covariance matrix of X   X  which is represented as Cov  X   X   i,i   Cov  RX  X   i,i   R  I d  Cov  X   R  I d  T   i,i  1 Let r ij represent the element  i j  in the matrix R  and c ij be the element  i j  in the covariance matrix of X  The VoD for i th column is computed as follows Cov  X   X   i,i   d  j 1 d  k 1 r ij r ik c kj  2 d  j 1 r ij c ij  c ii 2 We develop a simple method to implement a fast local optimization As shown in Equation 2 the privacy metric of column i is only related to the row vectors of rotation Therefore swapping the rows of rotation matrix could provide a better rotation that provides higher privacy guarantee This method can signiìcantly reduce the search space and thus provides better efìciency as we observed in experiments ICA-based Attack to Rotation Perturbation Independent Component Analysis ICA is the most potential method to breaching the privacy protected by rotation perturbation However we argue that ICA is in general not effective in breaking the rotation perturbation in practice ICA can be brieîy describes as follows Let matrix X composed by the source signals where each row vector is a signal and the observed mixed signals X  be X   AX  ICA model can be applied to estimate the independent components the row vectors of the original signals X  from the mixed signals X   if the four conditions are satisìed Three factors make the ICA attacks are often quite ineffective for rotation perturbation First two of the four conditions although reasonable for signal processing are not common for data classiìcation 1 The source signals are independent and 2 All the source signals must be nonGaussian with possible exception of one signal In addition the ordering of the reconstructed signals can not be determined In practice we can evaluate the effectiveness of ICA attacks with difference between the constructed data and the original data Since the ordering of the reconstructed row 3 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


 Privacy Quality for Breast-w Data 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 11213141 of iterations Privacy Quality \(Standard Deviation LocalOpt Non-Opt ICA reconstruction Figure 1 ICA reconstruction has no effect on privacy guarantee Privacy Quality for Votes Data 0.4 0.45 0.5 0.55 0.6 0.65 0.7 111213141 of iterations Privacy Quality Standard Deviation LocalOpt Non-Opt ICA Reconstruction Combined Figure 2 Example that ICA undermines the privacy guarantee Minimum Privacy Guarantee 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 B reast-w Credi ta C redit-g D i abet es E C ol i Heart Hepa tit is Ionoshpere I r i s T ict ac-toe Vo te s Wi ne Privacy Quality \(stdev  Condensaton-Min Rotation-Min Figure 3 Comparison on minimum privacy level with condensation approach vectors is not certain we estimate the VoDs with the best effort of the d  possible row orderings Let  X k be the ICA reconstructed data with one of the orderings and P k ica be the minimum privacy guarantee for  X k  k 1 d   The ordering that gives lowest minimum privacy quality is selected as the most likely ordering and the corresponding privacy quality is the undermined privacy quality Algorithm Combining the local optimization and the test for ICA attacks we develop a random iterative algorithm to nd a better rotation in terms of privacy quality The algorithm runs in a given number of iterations In each iteration it randomly generates a rotation matrix Local optimization through row-swapping rows is applied to nd a better rotation matrix which is then tested by the ICA reconstruction We take the combination P  min  P ica P opt  as the nal privacy guarantee The rotation matrix is accepted as the best perturbation yet if it provides highest P among the previous perturbations 4 Experimental Result We design three sets of experiments The rst set is used to show that the discussed classiìers are invariant to rotations The second set shows privacy quality of the good rotation perturbation The third one compares the privacy quality between the condensation approach and the random rotation approach Due to the space limitation we report some results of the later two sets of experiments The datasets are all from UCI machine learning database Three results are selected to show the effectiveness of the rotation perturbation approach Figure 1 represents a typical scenario that ICA attacks are totally ineffective while Figure 2 shows when ICA attacks are substantial the algorithm can also nd a rotation that has the highest combined privacy guarantee in the random rotation matrices Figure 3 demostrates the rotation approach can provide much higher privacy quality than the condensation approach 5 Conclusion Loss of privacy and loss of information/accuracy are treated as two conîict factors in privacy preserving data classiìcation In this paper we propose a rotation based perturbation technique that guarantees zero loss of accuracy for many classiìers Meanwhile we can adjust the rotation to nd a locally optimal rotation in terms of basic privacy guarantees and possible attacks where the optimality is measured by a new multi-column privacy metric Experiments show that the rotation perturbation can greatly improve the privacy quality without sacriìcing accuracy References  C C Aggarw al and P  S Y u  A condensation approach to privacy preserving data mining Proc of Intl Conf on Extending Database Technology EDBT  2004  D Agra w a l and C C Aggarw al On the design and quantiìcation of privacy preserving data mining algorithms Proc of ACM PODS Conference  2002  R Agra w a l and R Srikant Pri v a c y-preserving data mining Proc of ACM SIGMOD Conference  2000  K Chen and L Liu A random rotation perturbation approach to privacy preserving data classiìcation Technical Report  2005  A Evìmie vski J Gehrk e and R Srikant Limiting pri v a c y breaches in privacy preserving data mining Proc of ACM PODS Conference  2003  Z Huang W  Du and B Chen Deri ving pri v ate information from randomized data Proc of ACM SIGMOD Conference  2005  A Hyv arinen J Karhunen and E Oja Independent Component Analysis  Wiley-Interscience 2001  H Kar gupta S Datta Q W ang and K Si v akumar  O n the privacy preserving properties of random data perturbation techniques Proc of Intl Conf on Data Mining ICDM  2003  Y  Lindell and B Pinkas Pri v a c y preserving data mining Journal of Cryptology  15\(3 2000 4 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


natural dataset that was used as the experimental dataset for the KDD Cup 2003 1  The HEP-Th dataset contains a total of 29016 papers with 1.7Gbytes of associated data. Each paper in the dataset is described by a un ique ID, its authors, their email addresses, paper title, the journal it appeared in publication date, abstract and a set of other papers cited by it. The source text of each paper is also available which we ignore To model the data we used five different types of nodes and ten different types of links. Nodes represent paper IDs 29016\thor names \(12755\journal names \(267 organization names \(753\ and publication times encoded as year/season pairs \(60\bers in parentheses indicate the number of different entities for each type in the dataset. Organizations are not given directly but inferred from author\220s e-mail addre sses. Different spellings of author names were not consolidated and resulted in multiple nodes We defined the following types of links to connect nodes author_of  a  p connects author a to his/her paper p  date_published  p  d onnects paper p to its publication date d  affiliation  a  o connects person p to an organization o he/she belongs to published_in  p  j paper p to journal j it is in cites  p  r onnects paper p to a paper r it cites All of these links are viewed to be directional with an implicit inverse link, thus there are a total of 5*2 link types In sum there are 42871 different nodes and 461932 links in the graph representing the data. We then applied our rarity measures to identify interesting paths, loops and significant nodes in this graph 4.1 Significant node discovery In our first experiment we attempted to evaluate our significant node discovery me thod. That is, given some source node S we wanted to find other nodes of various types that were significan tly connected to S Since the nodes represent real-world entities such as people, we can then manually \215verify\216 the computed results by investigating whether they reflected real-world significant connections visibl e on the World-Wide Web For the experiment we picked C.N. Pope as the source node S since in this dataset he is the one with the highest number of publications \(130 in total\h provides us with a rich number of connections through this node Table 1 lists the top three interesting nodes connected to C.N. Pope for various different node types with their significance scores relative to Pope  1 http://www.cs.cornell.edu/projects/kddcup/datasets.html Table 1: Nodes significantly connected to C.N. Pope The results show that among the 12755 people in this dataset, the one that is the mo st significantly connected to Pope is H. Lu, while M. Cvetic is the second and K.S Stelle is the third. To get some further insight why these people were picked as the most significant ones, we can look at what path types cont ributed the most to the overall significance value. The most significant path for person entities connected to C.N. Pope is that of co-authorship This type of path emanates from Pope a total of 332 times and ends up at H. Lu 117 times, i.e., Lu contributes 35 of them while the runners up Cvetic and Stelle contribute 42 times \(12.6%\21 times \(6.3%\respectively. The second-most significant pat h represents a chain of coauthorship \(i.e., Person1 writes with Person2 and Person2 writes with Person3 on different papers\This path is not really rare from Pope\220s point of view \(it occurs 34473 times\etic was involved in it 5376 times thus, for her this type of relatio n still contributes 15.6% to the overall score. It shows that a significant path is not necessarily a rare path; it could be a non-rare one but occurs frequently for a specific target. The third-most significant path represents a citation relationship. Pope cites Lu\220s papers much more often than those of others Looking for organizations that are interestingly connected with Mr. Pope, we found that U. Texas A&M is the most important surpassing the second U. Michigan and third U Pennsylvania significantly Next we tried to verify whether the discovered relationships actually repre sent important real-world relationships visible th rough other means. After investigating through the World-Wide Web, we found that Dr. Pope is a professor at U. Texas A&M and he was Dr. Lu\220s thesis advisor \(1988-1994\ and that Dr. Lu is currently a post-doc at U. Michigan. Dr. Cvetic is a professor at U. Pennsylvania, has similar research interests to Pope and works clos ely with him. Dr. Stelle is a professor of Imperial College London who has ties with Pope not only academically but also personally. For example, Dr. Pope\220s homepage shows a picture showing him, Dr. Stelle, and others traveling together in Afghanistan Node Type Top-Three Scoring Nodes sum of path rarity  Person H. Lu 4.1 M. Cvetic 2.60 K.S. Stelle 0.98 Organization UTexas 3.42 UMich 1.80 UPenn 1.18 Journal Nucl.Phys 1.33 Phys.Lett 0.30 Phys.Rev 0.27 Time Spring 2000 0.40 Summer 2002 0.37 Winter1995 0.37 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


While this \215verification\216 is anecdotal, it does indicate that our unsupervised method, which did not know any semantics of the entities and links in this domain, is capable of returning signi ficant relationships that are relevant in the real world The rest of Table 1 describes journals and time periods significantly connected to Pope. The results show that the journal Nucl.Phys. has the highest score followed by Phys.Lett. and Phys.Rev. We checked the three types of paths that contribute the most to each of these rarity values. The most important relationship discovered and taken into account by ou r program is frequency of publication, which intuitiv ely makes sense. Pope published a total of 110 journal papers and 52 of them are in Nucl.Phys. He did not publish that many papers in Phys.Lett., but a significant portion of his colleagues\220 papers are published there. For his connection with Phys.Rev. the program discovered that the papers cited by Mr. Pope\220s papers are also frequently cited by papers published in Phys.Rev. As to the time periods, Spring 2000 followed by Summer 2002 and Winter 1995 connect significantly to Mr. Pope, because various types of paths such as, for example, the publica tion time for his papers and the publication time for his colleagues\220 papers contribute relatively highly fro m these nodes to Pope 4.2 Novel path discovery We also experimented with novel path discovery questions such as, for example, which path is the most interesting \(or rarest\two people. To determine rare paths between two known nodes, we applied 1/N1 as our rarity measure where N1 is the spindle fan-out described in Section 3.1. Looking at all paths between Pope and Lu we find the path \215Pope belongs to organization O that has another member P who writes a paper together with Lu\216 to be the rarest according to this measure. This indicates that not many of Pope\220s colleagues at his university write pa pers with Lu, which is consistent with Lu\220s role as Pope 220s student. However, this type of path is not the rarest between Pope and Cvetic instead \215Pope co-authors a paper with Cvetic\216 is rarer since Cvetic seems to write more with Pope\220s colleagues than with him. The examples show that our novel path discovery method can take poi nt-of-view into account since the computed interestin gness of paths changes when the view shifts \(e.g. from Lu to Cvetic in this case In this domain rarity of indi vidual paths does not convey such strong semantic relati onships as node significance and is harder to evaluate. In this sense the relationship between path rarity and node significance resembles the relationship between a probability density function and its corresponding probab ility distribution function, since the integrated probability usually carries more real-world meaning than the density function itself 4.3 Novel loop discovery For experiments on novel l oop discovery, we calculated loop rarity via 1/N4 004 where N4 004 is a variation of global fan-out \(see Section 3.1\th e additional requirement that source and target have to be the same node. Said differently, for each possible path type leading from a node to itself we count how often that path occurs in the dataset. The rarest, least frequent loops are listed in the top portion of Table 2, the most common loops are listed at the bottom Table 2: The rarest and the most common loops The rarest loops are papers citing themselves directly which only occurs 28 time s in the whole dataset. We do not have a real world explan ation for this and can only attribute it to errors in the dataset. The second third and fourth loops are also citation loops of different length The explanation behind this finding is that for a paper to cite another, the cited pape r needs to be published earlier In this sense a citation loop such as \215P1 cites P2 cites P3 cites P1\216 is really a temporal contradiction and should not occur at all. One explanation for such \215contradictions\216 is that sometimes an author \(or close colleague\ight cite one of his/her own submitted but not yet published papers P2 \(which has already cited P1\ in a paper P1. The other explanation is that one journal might have a very long revising period and during th at period other people can access the previous version. For both explanations we have found supporting instances from the dataset. The fifth path shows a similar concept where it is rare for a paper to cite another paper that was published during the same time period. This type of loop could also be an indicator for authors that wo rk closely with each other Finally, the last path shows that people seldom publish multiple papers at the same time Top 6 loops with highest rarity value PaperX cites PaperX PaperX cites Paper1 000\306 Paper1 cites PaperX PaperX cites Paper1 000\306 Paper1 cites Paper2 000\306 Paper2 cites PaperX PaperX cites Paper1 000\306 Paper1 cites Paper2 000\306 Paper2 cites Paper3 000\306 Paper3 cites PaperX PaperX cites \(or cited by\ Paper1 000\306 Paper 1 published at Time1 000\306 At Time1, PaperX also published PaperX is written by Person1 000\306 Person 1 has another Paper1 000\306 as PaperX Bottom 3 loops with lowest rarity value PaperX cites PaperY 000\306 PaperY is being cited by PaperZ 000\306 PaperZ is being cited by PaperX PaperX cites PaperY 000\306 PaperY published in the same journal as PaperZ 000\306 PaperZ cites PaperX PaperX is cited by Paper Y 000\306 Paper Y published in same season as PaperZ 000\306 PaperZ cites Paper X Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


The bottom portion of th e table shows the most frequently occurring loops as a contrast to the rare loops described above. For example, the most frequent loops are two papers published at the same time period that both cite X. They are loops that intuitively should occur very frequently. Note that \215A cites B cites C\216 is a very common path, thus, we did not expect it to be interesting as a loop and were surprised by the results The experiments demonstrate that our approach is capable of uncovering in teresting instances masked inside thousands of uninteresting facts. Furthermore, the instances found by novel loop discovery lead us to the discovery of interesting hypotheses or patterns, e.g., that citation loops are an indicator for authors who work closely with each other or for journals that have a long revision cycle 4.4 Discussion The experiments show that our program can find interesting connections in a network without having to learn the patterns of interestingn ess. For the bibliography dataset, which does not have too many different types of be able to write a rule-based system or supervised learning program to answer similar queries as we did. However, it is time consuming to do this, since different rules or training data are required for different queries \(e.g. the rules to id entify the people that are interestingly connect to a keyword are different from the ones required to determine the organizations that are interestingly connected to a person\he advantage of our method is that it does everything in an unsupervised manner and eliminates the necessity to regenerate new rules or new training data for different queries or even when the whole domain is changed. It also eliminates the risk of being biased by th e apparent meaning of link types Another advantage of our approach is that it can focus the user\220s attention on events that are otherwise hard to be noticed. The inspirations triggered by such evidences can sometimes lead to the discovery of pattern/knowledge For example, without being made aware of those rare loops, we might not ever look into the issue of citation loops at all, since there are thousands of different loops in the dataset that mask this phenomenon. They also prompt us to discover other related knowledge when we try to explain them, for example, that citation loops can be an indicator of authors adding additional citations during a revision of a journal submission 000 5 Related work To our knowledge there is no other work that addresses the NLD problem in multi-relational data via an unsupervised approach. One focus of current link discovery research is on learning patterns from complex multi-relational data. For example, inductive logic programming has been applied to learn relational patterns  p h-bas ed m e thods s u ch as 6 have  been used to learn subgraph ca tegories and isomorphisms These approaches either require training examples or learn things at the structure/schema level, while for the NLD problem it is necessary to perform discovery at the instance level by using unsupervised methods Kovalerchuk and V ityaev\220s hybrid evidence correlation technique [1 id en tifies co mm o n p a ttern s v i a standard data mining techniques and then hypothesizes interesting or unusual patterns by negating some of the statistically significant patterns fou nd. It is conceptually similar to our approach but requires the occurrence of very common patterns in the data Other analysis algorithms such as PageRank compute the importance of links through the connections between nodes in an unsupervised manner [12  In t h at  framework, however, all relations are treated to be identical \(that is, \215A kills B\216 is not different from \215A writes to B\215\therefore, this approach is not suitable for the multi-relational NLD problem The area of outlier detection in data mining and statistics aims at detecting points that are considerably dissimilar or inconsistent with th e remainder of the data 2, 3, 7, 14 c e ptually related to our use of rarity analysis to solve th e NLD problem. Current research on outlier detection, however, analyzes primarily numerical entity-attribute data instead of multi-relational social network data. In threat detection each individual event is usually not an ou tlier; nevertheless, combinations of seemingly harmless events can suddenly become threatening when they occur in a particular context Outlier computations that do not take such combinations into account will fail to detect such threats. Our path rarity analysis is designed to search for these kinds of unusual connections in a multi-relational dataset The area of social network analysis has investigated multi-relational social behavior using graph and matrixtheoretic representations [16  Th e con cep t  o f 215cen trality\216  is applied widely to determin e important nodes in a network from a global point of view, while our significant node discovery tries to tackle the problem locally by answering \215which node is im portant to a chosen node\216 Moreover, centrality analysis uses only the connectivity the number of paths\to judge the significance while our algorithm considers no t only the quan tity but also the quality \(rarity\of the paths Valdes-Perez h a r acteriz es disc overy i n science as the generation of novel, in teresting, plausible and intelligible knowledge about the objects of study. In this sense the novel link discovery problem is similar to literature-based discovery introduced by Swanson [18   since t h e y bot h inte nd to find i n teres ting fa cts and connections in large amounts of data. Since 1986 Swanson has triggered interesting discoveries in Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


biomedicine strictly by looking for mediators that connect otherwise unconnected corpora of scientific literature Literature-based discovering systems are primarily aimed at finding one-step connections between independent corpora instead of ranking the interestingness of the multi-step paths in a multi-relational network, and are therefore different from our approach 6 Conclusion We presented an unsupervised link discovery method aimed at detecting interesting paths or interestingly connected nodes in multi-relational datasets Interestingness is modeled via different measures of rarity that are based on computing how often similar paths occur in the data. The method does not rely on any preexisting or learnable pattern information and can detect novel, interesting connections that do not need to be conceived prior to the analysis. Our approach is a generalpurpose method and can be applied to arbitrary multirelational datasets. Potential applications are in law enforcement, threat detection, data  and scientific discovery. The experiment shows that our approach can capture interesting connections that are representative of meaningful real-world relationships Future work will include more extensive evaluation with different data sets, handling of temporal information negation and better handling of noise and corruption 7 Acknowledgements This research was supported by the Defense Advance Research Projects Agency under Air Force Research Laboratory contract F30602-01-2-0583 8 References  B. Kovalerchuk E. Vity aev  Correlation of complex evidences and link discovery  The Fifth International Conference on Forensic Statistics 2002. Venice, Italy  C. Aggarwal, P.Yu Outlier detection for high dimensional data  ACM SIGMOD Conference 2001  E.M. Knorr, R  T. Ng Algorithms for Mining DistanceBased Outliers in Large Datasets  Proc. of VLDB Conf 1998  A. A. Freitas  On rule interestingness measures  Knowledge-Based Systems 1999 5] R.Kimball Dealing with Dirty Data  DBMS Magazine  1996  L. B  Hold er, D J Cook  IEEE Intelligent Systems 15, 2000  M.M. Breun ig H.P. Krieg el R T. Ng and J. Sa nder Optics-of: Identifying local outliers  Proc. of PKDD '99   P.N. Tan, V. Kumar Interestingness measures for association patterns: A perspective  KDD 2000  R Hild erm an , H Ham ilton  Knowledge discovery and interestingness measures: A survey 1999, Technical Report University of Regina  R. J. Mooney P Melville L. P Rupert Tang J Shavlik I.d. Dutra, D. Page, V. S. Costa Relational Data Mining with Inductive Logic Programming for Link Discovery  Proceedings of the National Science Foundation Workshop on Next Generation Data Mining 2002  R.J. Mooney  P.Melville, L P. R upert Tang J. Shavlik I  d Dutra, D. Page, V. S. Costa Relational Data Mining with Inductive Logic Programming for Link Discovery  Proceedings of the National Science Foundation Workshop on Next Generation Data Mining 2002  S Brin, L Page The anatomy of a large-scale hypertextual Web search engine  Proceedings of the 7th International World Wide Web Conference 1998  S. Candan W.S. Li Reasoning for Web Document Associations and Its Applications in Site Map Construction International Journal of Data and Knowledge Engineering 2002: p.121-150  S. Ramaswam y   R. Rastog i, K. S h im Efficient algorithms for mining outliers from large data sets  Proceedings of the ACM SIGMOD Conference 2000  S. Shekhar C  T. Lu, P Zhang  Detecting Graph-based Spatial Outliers: Algorithms and Applications  The Seventh ACM SIGKDD 2001  S. Wasserman K. Faust Social Network Analysis: Methods Applications 1994: Cambridge, UK: Cambridge University Press  Sen ator  Evidence Extraction and Link Discovery Program 2002, DARPATech 2002 http://www.darpa.mil/DARPATech2002/presentations/iao_ pdf/speeches/SENATOR.pdf  D. R. Swanson Fish Oil, Raynaud's syndrome and undiscovered public knowledge Perspectives in Biology and Medicine, 1986  D. R. Swanson Somatomedin C and arginine: Implicit connections between mutually isolated literatures Perspectives in Biology and Medicine, 1990  U Fayy ad G. Piatetsky Shapiro P Smy th The KDD Process for Extracting Useful Knowledge from Volumes of Data Communications of the ACM, 1996. p.27-34  R. E  Va ldes-Per ez Principles of human-computer collaboration for knowledge discovery in science Artificial Intelligence, 1999 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


21  and denote wjk as the set of weights for Yj where    k j k jw 1 1.  A classifier H is defined as YD ? such that it assigns a weight of the correct class label to an instance as  iWdH where deD, and k j i WW ? . For a set of single-class instances I = &lt; \(x1 y1 x2, y2  xn, yn Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Table 4. Classification accuracy of PART RIPPER, CBA and MMAC Dataset PART RIPPER CBA MMAC Tic-Tac 92.58  97.54  98.60  99.29 Contactlenses 83.33  75.00  66.67  79.69 Led7 73.56 69.34  72.39  73.20 Breastcancer 71.32  70.97  68.18  72.10 Weather 57.14  64.28  85.00  71.66 Heart-c 81.18  79.53  78.54  81.51 Heart-s 78.57  78.23  71.20  82.45 Lymph 76.35  77.70  74.43  82.20 Mushroom 99.81 99.90  98.92  99.78 primarytumor 39.52  36.28  36.49  43.92 Vote 87.81  87.35  87.39  89.21 CRX 84.92  84.92  86.75  86.47 Sick 93.90  93.84  93.88  93.78 Balancescale 77.28  71.68  74.58  86.10 Autos 61.64  56.09  35.79  67.47 Breast-w 93.84  95.42  94.68  97.26 Hypothyroid 92.28 92.28  92.29  92.23 zoo 91.08  85.14  83.18  96.15 kr-vs-kp 71.93 70.24  42.95  68.75 is  m k ii i ydHw m 1  1 ?  , where          yxif yxif yx 0 1  For example, if an item \(A ,a labels  c1    c2  and  c3  7, 5  and 3 times 


labels  c1    c2  and  c3  7, 5  and 3 times respectively, in the training data. Each class label will be assigned a weight, i.e. 7/15, 5/15, and 3/15, respectively for labels  c1    c2  and  c3  This technique assigns the predicted class label weight to the case if the predicted class label matches the case class label. For instance if label  c2  of item \(A, a test data that has  c2  as its class, then the case will be considered a hit, and 5/15 will be assigned to the case 5. Experimental Results We investigated our approach against 19 different datasets from [20] as well as a different datasets for forecasting the behaviour of an optimisation heuristic within a hyperheuristic framework [5, 16]. Stratified tenfold cross-validation was used to derive the classifiers and error rates in the experiments. Cross-validation is a standard evaluation measure for calculating error rate on data in machine learning. Three popular classification techniques a decision tree rule \(PART CBA have been compared to MMAC in terms of classification accuracy, in order to evaluate the predictive power of the proposed method The choice of such learning methods is based on the different strategies they use to generate the rules. Since the chosen techniques are only suitable for traditional classification problems where there is only one class assigned to each training instance, we therefore used classification accuracy derived by only the top-label evaluation measure for fair comparison All experiments were conducted on a Pentium IV 1.6 GH PC.  The experiments of PART and RIPPER were conducted using the Weka software system [20]. Weka stands for Waikato Environment for Knowledge Analysis. It is an open java source code for the machine teaching community that includes implementations of different methods for several different data mining tasks such as classification, clustering, association rule and regression. CBA experiments were conducted using a VC++ implementation version provided by [19]. Finally MMAC was implemented using Java We have evaluated 19 selected datasets from Weka data collection [20], in which, a few of them \(6 reduced by ignoring their integer and/or real attributes Several tests using ten-fold cross-validation have been performed to ensure that the removal of any real/integer attributes from some of the datasets does not significantly affect the classification accuracy. To do so we only considered datasets where the error rate was not more than 6% worse than the error rate obtained on the same dataset before the removal of any real/integer attributes.  Thus, the ignored attributes do not impact on the error rate too significantly Many studies have shown that the support threshold plays a major role in the overall classification accuracy of the set of rules produced by existing associative classification techniques [9, 12]. Moreover, the support value has a larger impact on the number of rules produced in the classifier and the processing time and storage needed during the algorithm rules discovery and generation. From our experiments, we noticed that the support rates that ranged between 2% to 5% usually achieve the best balance between accuracy rates and the size of the resulted classifiers. Moreover, the classifiers derived when the support was set to 2% and 3 achieved high accuracy, and most often better than that of decision trees rule \(PART the MinSupp was set to 3% in the experiments. The confidence threshold, on the other hand, is less complex and does not have a large effect on the behaviour of any associative classification method as support value, and thus it has been set to 30 


Table 4 represents the classification rate of the classifiers generated by PART, RIPPER, CBA and MMAC against 19 benchmark problems from Weka data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 3 5.00 2 5.00 15.00 5.00 5.00 15.00 2 5.00 3 5.00 4 5.00 55.00 6 5.00 75.00 8 5.00 9 5.00 1 2 3 4 5 6 7 8 9 Nine  Scheduling Runs D iff er en ce  in  A cc u ra cy  CB A To p-label A ll-label A ny-label Figure 3a. Difference of accuracy between MMAC evaluation measures and CBA algorithm 35.00 25.00 15.00 5.00 5.00 15.00 25.00 35.00 45.00 55.00 65.00 75.00 85.00 95.00 1 2 3 4 5 6 7 8 9 Nine  Scheduling Runs D iff er en ce in  A cc u ra cy  P A RT To p-label A ll-label A ny-label Figure 3b. Difference of accuracy between MMAC   evaluation measures and PART 


MMAC   evaluation measures and PART algorithm 0 2 4 6 8 10 12 14 16 18 20 22 24 26 Run1 Run2 Run3 Run4 Run5 Run6 Run7 Run8 Run9 Ten Runs  Scheduling Data N um be r o f R ul es To p Label P A RT CB A Figure 4. Classifier sizes of MMAC \(toplabel the scheduling   data collection. The accuracy of MMAC has been derived using the top-label evaluation measure. Our algorithm outperforms the rule learning methods in terms of accuracy rate, and the won-loss-tied records of MMAC against PART, RIPPER and CBA 13-6-0, 15-4-0 and 154-0, respectively The evaluation measures of MMAC have been compared on 9 solution runs produced by the Peckish hyperheuristic [5] with regard to accuracy, and number of rules produced. Figures 3a and 3b represent the relative prediction accuracy that indicates the difference of the classification accuracy of MMAC evaluation measures with respect to those derived by CBA and PART, respectively. In other words, how much better or worse MMAC measures perform with respect to CBA and PART learning methods. The relative prediction accuracy numbers shown in Figures 3a and 3b are conducted using the formula PART PARTMMAC Accuracy AccuracyAccuracy  and CBA CBAMMAC Accuracy AccuracyAccuracy  respectively. After analysing the charts, we found out that there is consistency between the top-label and label-weight measures, since both of them consider only one class in the prediction. The top-label takes into account the topranked class, and the label-weight considers only the weight for the predicted class that matches the test case Thus, both of these evaluation measures are applicable to traditional single-class classification problems. On the other hand, the any-label measure considers any class in the set of the predicted classes as a hit whenever it matches the predicted class regardless of its weight or rank. Is should be noted that, the relative accuracy of MMAC evaluation methods against dataset number 8 in Figure 3a and 3b, is negative since CBA and PART 


Figure 3a and 3b, is negative since CBA and PART achieved a higher classification rate against this particular dataset A comparison of the knowledge representation produced by our method, PART and CBA has been conducted to evaluate the effectiveness of the set of rules derived. Figure 4 represents the classifiers generated form the hyperheuristic datasets. Analysis of the rules sets indicated that MMAC derives a few more rules than PART and CBA for the majority of the datasets. In particular, the proposed method produced more rules than PART and CBA on 8 and 7 datasets, respectively. A possible reason for extracting more rules is based on the recursive learning phase that MMAC employs to discover more hidden information that most of the associative classification techniques discard, since they only extract the highest confidence rule for each frequent item that survives MinConf Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 6. Conclusions A new approach for multi-class, and multi-label classification has been proposed that has many distinguishing features over traditional and associative classification methods in that it \(1 that contain rules with multiple labels, \(2 evaluation measures for evaluating accuracy rate, \(3 employs a new method of discovering the rules that require only one scan over the training data, \(4 introduces a ranking technique which prunes redundant rules, and ensures only high effective ones are used for classification, and \(5 discovery and rules generation in one phase to conserve less storage and runtime. Performance studies on 19 datasets from Weka data collection and 9 hyperheuristic scheduling runs indicated that our proposed approach is effective, consistent and has a higher classification rate than the-state-of-the-art decision tree rule \(PART and RIPPER algorithms. In further work, we anticipate extending the method to treat continuous data and creating a hyperheuristic approach to learn  on the fly   which low-level heuristic method is the most effective References 1] R. Agrawal, T. Amielinski and A. Swami. Mining association rule between sets of items in large databases In Proceeding of the 1993 ACM SIGMOD International Conference on Management of Data, Washington, DC May 26-28 1993, pp. 207-216 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rule. In Proceeding of the 20th International Conference on Very Large Data Bases, 1994, pp. 487   499 3] M. Boutell, X. Shen, J. Luo and C. Brown. Multi-label semantic scene classification. Technical report 813 Department of Computer Science, University of Rochester Rochester , NY 14627 &amp; Electronic Imaging Products R &amp D, Eastern Kodak Company, September 2003 4] A. Clare and R.D. King. Knowledge discovery in multilabel phenotype data. In L. De Raedt and A. Siebes editors, PKDD01, volume 2168 of Lecture Notes in Artificial Intelligence, Springer - Verlag, 2001,  pp. 42-53 5] P. Cowling and K. Chakhlevitch. Hyperheuristics for Managing a Large Collection of Low Level Heuristics to Schedule Personnel. In Proceeding of 2003 IEEE conference on Evolutionary Computation, Canberra Australia, 8-12 Dec 2003 6] R. Duda, P. Hart, and D. Strok. Pattern classification Wiley, 2001 7] E. Frank and I. Witten. Generating accurate rule sets without global optimisation. In Shavlik, J., ed., Machine Learning: In Proceedings of the Fifteenth International 


Learning: In Proceedings of the Fifteenth International Conference, Madison, Wisconsin. Morgan Kaufmann Publishers, San Francisco, CA, pp. 144-151 8] J. Furnkranz. Separate-and-conquer rule learning Technical Report TR-96-25, Austrian Research Institute for Artificial Intelligence, Vienna, 1996 9] W. Li, J. Han and J. Pei. CMAR: Accurate and efficient classification based on multiple class association rule. In ICDM  01, San Jose, CA, Nov. 2001, pp. 369-376 10 ] T. Joachims. Text categorisation with Support Vector Machines: Learning with many relevant features. In Proceeding Tenth European Conference on Machine Learning, 1998,  pp. 137-142 11] T. S. Lim, W. Y. Loh and Y. S. Shih. A comparison of prediction accuracy, complexity and training time of thirtythree old and new classification algorithms. Machine Learning, 39, 2000 12] B. Liu, W. Hsu and Y. Ma. Integrating Classification and association rule mining. In KDD  98,  New York, NY, Aug 1998 13] J.R. Quinlan. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann, San Francisco, 1993 14] J.R. Quinlan. Generating production rules from decision trees. In Proceeding of the 10th International Joint Conferences on Artificial Intelligence,  Morgan Kaufmann San Francisco, 1987, pp. 304-307 15] R. Schapire and Y. Singer, "BoosTexter: A boosting-based system for text categorization," Machine Learning, vol. 39 no. 2/3, 2000, pp. 135-168 16] F. Thabtah, P. Cowling and Y. Peng. Comparison of Classification techniques for a personnel scheduling problem. In Proceeding of the 2004 International Business Information Management Conference, Amman, July 2004 17]Y. Yang. An evaluation of statistical approaches to text categorisation. Technical Report CMU-CS-97-127 Carnegie Mellon University, April 1997 18] X. Yin and J. Han. CPAR: Classification based on predictive association rule. In  SDM  2003, San Francisco CA, May 2003 19]CBA:http://www.comp.nus.edu.sg/~dm2/ p_download.html 20] Weka: Data Mining Software in Java http://www.cs.waikato.ac.nz/ml/weka 21] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New algorithms for fast discovery of association rules. In Proceedings of the 3rd KDD Conference, Aug. 1997 pp.283-286 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207ñ216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intíl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intíl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





