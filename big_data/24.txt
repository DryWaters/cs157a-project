V4.17 Computationally efficient tracking of multiple targets by probabilistic data association using neural networks DEBASIS SENGUPTA AND RONALD A ILTIS Department of Electrical and Computer Engineering University of California Santa Barbara Santa Barbara CA 93106 Abstract The joint probabilistic data association JPDA algorithm has been previously reported to be suitable for the problem of tracking multiple targets in the presence of clutter Although it make few assumptions and can handle many targets the com plexity of this algorithm increases rapidly with the number of targets and returns An approximation of the JPDA has been suggested in this paper The proposed algorithm uses an analog 
computational network to solve the data association problem The problem is viewed as that of optimizing a suitably chosen objective function Simple neural network structures for the ap proximate minimization of such functions have been proposed by other researchers The analog network used here offers a signifi cant degree of parallelism and thus can compute the association probabilities more rapidly Computer simulations indicate the ability of the algorithm to track many targets simultaneously in the presence of moderate density clutter 1 Introduction The problem of tracking multiple targets in the presence of clutter has derived considerable attention in recent years The probabilistic data association PDA\approach reduces the com plexity of more sophisticated algorithms by focusing on the few most likely hypotheses [Bar-Shalom 19781 
This is done by form ing a validation gate around the predicted position of each tar get and selecting only the returns inside the gate for association with the target The algorithm assigns a probability called the association probability to every hypothesis associating a return to a target A weighted average of the state estimates under all the hypotheses associating different returns to a particular target serves as the PDA estimate of the state of that target The associ ation probabilities are used as weights Assuming the true return from a given target to be a Gaussian random vector with mean and covariance given by the predicted state vector and covari ance matrix the likelihood function can be evaluated under every hypothesis associating different returns to that 
target The as sociation probabilities are computed from these likelihoods The simplest of the PDA methods computes the association probabil ities taking one target at a time regardless of how close the other targets may be For this reason the ordinary PDA tracker per forms poorly while tracking crossing targets or when the targets are close to each other This difficulty is alleviated by the joint probabilistic data association JPDA algorithm which computes the association probabilities from the joint likelihood functions corresponding to the joint hypotheses associating all the returns  This work was sponsored in part by the SDIO/IST program managed by the Office of Naval Research under contract no N00014-85-K-0551 to different permutations of targets and clutter points [Fortmann et a1 
19831 However the increase of complexity in the computa tion of the association probabilities may prove significant for a number of targets in moderate density clutter There have been efforts to approach the performance of the JPDA by imitating its properties via ad hoc association rules Fitzgerald 19861 How ever the effectiveness of these approximations in tracking many targets in the presence of clutter is not guaranteed This paper suggests the use of an analog computational net work for computing the association probabilities The task of finding these probabilities is viewed as a constrained optimiza tion problem The constraints are obtained by a careful evalu ation of the properties of the JPDA association rule Some of these constraints are 
analogous to those of the classical 223Trav eling salesman problem\224 TSP and hence the neural network solution of this problem suggested by Hopfield and Tank 1985 is used as a reference 2 Problem description and the PDA solution sidered T dynamic systems of the following familiar form are con zt\(k  1  Ft\(k k  wt\(k t 5 T 1 where z is the plant or target state vector F is a known matrix describing the dynamics of the target and w is a vector of zero mean Gaussian noise uncorrelated to any such noise vector at a different instant of time The superscript t corresponds to 
the tth target and k is the index of time The plant noise vectors wt\(k for different targets are assumed to be uncorrelated with each other at all instants of time k The measurement model is z\(k  H\(k k  u\(k 2 where z is the measurement vector H is a known matrix and U is a zerc-mean Gaussian noise vector independent of wt The vectors U at different time instants are assumed to be independent of each other The covariance matrices Qt\(k and R\(k of wt\(k and u\(k respectively are known The initial state of the target t 
t  1,2,...,T is assumed to be Gaussian with known mean vector kt\(O1O and known covariance matrix Pt\(O1O The PDA approach considers only a subset of measurements which are validated by an appropriate gate The Zth return zr\(k is validated for the tth target if its Mahalanobis distance from the predicted location kt\(klk  1 of the tth target is less than a threshold i.e r;\(k  q\(k k klk  l k zl\(k  H\(k klk  l 5 7\222 3 where Wyk  H\(k k/k  l k R\(k 4 2152 CH2561-9/88/0000-2152 1.00 0 1988 IEEE 


Pt\(klk  1 is the predicted error covariance for the tth tar get and the threshold 7 is selected to produce a predetermined probability of erroneous rejection of the correct return It is as sumed for simplicity that the true return is a Gaussian random vector with mean H\(k klk  1 and covariance Pt\(klk  1 A validation matrix may be formed by using 3 in the following way 1 0 otherwise if the Zth return belongs to Gt\(k W4ll,t   5 where G'\(k is the validation region of the tth target n\(k has dimension m\(k x T where m\(k is the total number of validated returns at the kth instant and Z indexes the set of these returns The PDA state estimate is given by a weighted combination of all the estimates under different hypotheses m\(k et\(klk  Bf\(k klk 6 I=O where f\(klk is the state estimate under the hypothesis that the Zth validated return came from the tth target The weight k corresponds to the probability that the lth return came from the tth target The overall covariance update is m\(k Pt\(klk  P,t\(W,t\(klk 7 1=0 where P:\(klk is the covariance matrix corresponding to f\(klk The various PDA techniques differ only in the ways they compute the association probabilities aj\(k which must satisfy m\(k1 Bj\(k  1 t 1,2,...,T l=O The ordinary PDA focuses only on the returns in one validation region at a time If the probability of detecting the correct re turn is PD and the probability of validating a detected return is Pa then it can be shown that Fortmann et aZ 19831 the PDA association probabilities are where otherwise 001 I Computation of the JPDA association probabilities is more com plicated Define a feasible hypothesis of joint events x\(n k as a permutation of targets and clutter points which allows at most one target to be associated with a given return and at most one return to be associated with a given target It was shown by Fortmann et a1 1983 that the JPDA association probabilities are t=l 1  r  T 0 5 I m\(k 11 where jr is the return to which the rth target is associated under a given hypothesis and the sum is over all feasible hypotheses which allow the Zth return to be associated with the tth target In the above equation pf is as described in 10 except for a minor difference in the case of p which is given by X\(l  PD c is a normalizing constant 11\reflects the fact that the JPDA uses the joint likelihoods of all the returns while \(9 shows that the ordinary PDA effectively uses a normalized version of the simple likelihood functions as the association probabilities It is useful to examine the properties of k which are ex clusive to the JPDA It is unlikely that k will be large if B[\(k for some r  t is also large Furthermore a close exami nation of 11 reveals that k is large if in addition to pf\(k being large there is a highly likely combination of the remain ing targets r  1  r  T r  t and the remaining returns j  1 5 j  m\(k j  I A highly likely combination is indi cated by a large value of the product of the corresponding pf\(k This property of the JPDA association probabilities was not con sidered in the following ad hoc formula for the 3:\(k proposed by Fitzgerald 1986 The additional term Et p;\(k in the denominator compare \(9 is introduced in order to reduce the chances of k and P[\(k r  t being large at the same time The arbitrary constant po is expected to improve the performance in the presence of clutter Another PDA scheme, called the nearest neighbor PDA NNPDA picks the joint hypothesis which best explains the ordinary PDA association probabilities described by 9 The resulting matrix of B\(k consists only of 1s and Os The task of choosing the nearest hypothesis may require an exhaustive search over all feasible hypotheses All of the above PDA techniques can track two targets satisfactorily However their performance in tracking more targets in the presence of moderate density clutter may be inadequate The complexity of the JPDA is overwhelming for relatively large T and m\(k and yet a computationally efficient substitute based on a careful understanding of its properties is lacking The next section develops a method which is expected to serve as an effective alternative to the JPDA tracking scheme 3 The traveling salesman problem and the JPDA The traveling salesman problem TSP is a well-known con strained multidimensional optimization problem Given a set of n cities and the distance between each pair of them the task is to design a closed tour for a traveling salesmhn The length of the tour should be minimized subject to the constraints that no city should be excluded or visited twice For an n-city TSP there are n!/2n distinct valid tours out of which the shortest has to be chosen Hopfield and Tank 1985 proposed a neural network solution to the TSP by using n2 interconnected neurons They formulated the problem as that of minimizing an objective or energy function of the form over all V They also designed an analog circuit which approxi mately minimizes the above energy function T represents the strength of connection from the output of the jth neuron to the 2153 


input of the ith neuron while 1 represents the externally supplied input current to the ith neuron The evolution of the circuit is described by the differential equation where s is the index of time so is a time constant and U is the input to the ith neuron The output V of the neuron is related to its input by a monotonically nondecreasing function g with domain 00 CO and range 0,1 Although the circuit operates over the N-dimensional hypercube defined by 0 5 V 5 1 the minima of 13 occur only at the corners of the hypercube pro vided the connections are symmetric T,J  T3 In the case of TSP a propel: choice of a few design parameters causes the net work to converge to a stable state or minimum which corresponds to one of the best few solutions The problem of finding the association probabilities B:\(k from the likelihoods p:\(k described in the previous section has features similar to the TSP If the output voltages of an m\(k  1 x T array of neurons are defined to be the association proba bilities then the columns would represent targets and the rows would indicate returns The zeroth row would correspond to no return from a given target The constraints of the data associ ation problem DAP\are as follows Each column sum of the voltages must be unity so that 8 is satisfied At most one large entry may be favored in every row and column in accordance with the assumptions that no two returns can come from the same tar get and no return can come from two targets k should be large if p:\(k is large and there is a combination of the remaining targets and returns which result in a large product of the cor responding likelihoods All these requirements are accomplished by minimizing the energy function where pf is a normalized version of the p:\(k pf is roughly the PDA association probability for the lth return and tth target compare 9 The Bi\(k are obtained as the output 1  O,l  m\(k t  1,2  T The index k is dropped for convenience The first term achieves its minimum value of 0 when there is at most one large entry in each row Similarly the second term inhibits more than one return to be strongly associated with a given target The third term biases the final solution towards a normalized set of numbers The fourth term favors associations which have a high likelihood and the fifth term favors a highly likely combination of the remaining targets and returns Minimizing 15 can be shown to be equivalent to minimizing the energy function where and I  C  D  E  E\(T  1  CpI 19 Here 6,j is the usual Kronecker delta function Equation 17 is equivalent to 13 except that each neuron is identified by a superscript as well as a'subscript It may be observed that the strengths of connection T given by 18 do not depend on p Thus a new network need not be designed every time a new set of likelihood functions are available Instead the input currents 1 as described by 19 can be easily controlled by these functions The equations of motion for this circuit are r Kt  g\(4 20a  D  E\(T  l  D  E The coefficients A B C D and E can be adjusted to control the emphasis on different constraints and properties A large value of D will produce close to pf which are approximately the ordinary PDA association probabilities A larger emphasis on A B and C will produce the nearest neighbor solution as in the TSP This special case offers an attractive method of deter mining the nearest hypothesis without explicitly examining all the hypotheses Finally a balanced combination of all five terms will lead to the most complete emulation of all the properties of the JPDA A larger number of targets will only require a larger array of interconnected neurons instead of an increased load on any sequential software to compute the association probabilities Figure 1 shows the block diagram of a tracker which uses the pre posed analog network to compute the association probabilities 4 Simulation of performance The performance of the neural network PDA NPDA was simulated along with those of the Fitzgerald's simplified PDA SPDA and JPDA Four targets were chosen with nearly con stant velocities in a two dimensional plane The discretized state equation for each target is given by 1 where 2\(k  z i y y 21 22 1AOO P\(k    Vt Vk 0001 using Cartesian coordinates in two dimensions The quantities in the right hand side of 21 indicate the position and velocity at time k and target t The sampling interval A was assumed to be 0.5 second The plant noise wt\(k is assumed to be of the form Fortmann 19831 2154 


where wL\(k wi\(k is a vector of independent Gaussian ve locity noises with associated variances k  i\(k  0.0009 km\222s-\222 V t V k which correspond to a standard devia tion of at least 04 times the mean velocity These uncertainties allow for small changes of the target velocity It was assumed that only position measurements are available so that 1000 H\(k o 0 1 0 for all k The measurement noise covariance matrix is R\(k  diag\(.0225, .09 assuming all the measurement noise to be uncorrelated The probability of validation Pc and the probability of detection Po were both assumed to be 0.95 The corresponding threshold of the Mahalanobis distance as obtained from the table of xi dis tribution is 7  m This determines the size of the validation gate A uniform clutter density\222 of about 0.05 km-2 was chosen This produced on the average 0.35 clutter point per validation gate Equations 20a and 20b represent the evolution of the states of the analog circuit for the neural network solution of the DAP The function g\(u in 20a was defined to be Hopfield and Tank 19851 Yt  g\(u  1 1  tanh 2 2 23 In order to avoid bias to any particular set of stable states the initial states were chosen to be such that they approximately produce cE_bk Yt  1 V t initially It is known Hopfield and Tank 19851 that the addition of a random noise to the initial values enhances convergence Accordingly the initial states were chosen to be where Suf is a random variable having uniform distribution over O.lu O.lu The constant U was chosen to be 0.02 to pro duce a moderate rate of convergence The differential equation 20b was approximated by a difference equation with stepsize 0.0005 s so was selected to be 1 s An experimentally deter mined limit of 100 iterations was imposed on the recursive dif ference equation The 100th iterates xt\(l0O 15 m\(k t 5 T are expected to be close to the steady state solutions of 20b These values were normalized for each t to ensure strict compli ance with the constraint 8 The constants A B C D and E were also chosen experimentally to produce a stable operating point The values A  1 B  40 C  1000 D  30 and E  5 appeared to be suitable for two to four targets Figure 2 shows the ability of the NPDA and the failure of the SPDA to track four targets in the presence of moderate density clutter The SPDA association probabilities were computed from 12 with po  0 No non-zero value of po appeared to improve its performance The performance of the NPDA is equivalent to that of the JPDA in terms of rms position and velocity errors Sengupta and Iltis 19871 5 Conclusion Computation of the association probabilities by an analog network appears to be a viable alternative to the JPDA It out performs the SPDA by accurately emulating all the properties of the JPDA The coefficients of the function EDA have to be chosen suitably for a given number of targets and returns Com puter simulations indicate that a suitable operating point may be obtained for a range of complexity of the problem The ana log network is structurally simple and there is a high degree of parallelism inherent in it A DAP of higher complexity will only require a larger array of neurons instead of increased burden on a sequential algorithm such as the JPDA. In spite of the necessity of D/A and A/D conversions these attractive features may ren der the NPDA useful for tracking a reasonable number of targets in the presence of moderate density clutter References 1 Y Bar-Shalom 221\(Tracking methods in a multitarget environ ment,\224 Survey paper IEEE Transactions on Automatic Control 2 R.J Fitzgerald 223Development of practical PDA logic for mul titarget tracking by microprocessor,\224 Proc American Controls Conference Seattle WA 1986 pp 889-898 3 T.E Fortmann Y Bar-Shalom and M. Scheffe 223Multitarget tracking using joint probabilistic data association,\224 IEEE Journal of Oceanic Eng., July 1983 4 J.J Hopfield and D.W Tank 223\223Neural\224 Computation of de cisions in optimizing problems,\224 Biological Cybernetics Vol 52 5 D Sengupta and R.A. Iltis 221\(Computationally efficient track ing of multiple targets by probabilistic data association using neural networks,\224 Submitted to IEEE Trans on Aerospace and Electronic Systems 1987 Vol AC-23 NO 4 Aug 1978 pp 618-626 1985 pp 141-152  Figure 1 Schematic diagram of NPDA tracker NPDA estimate1 i r I  SPDA estimate true track t  L i I 10  5 0 5 10 15 Figure 2 Tracking four targets by NPDA and SPDA 


ing technology in an additional system component that offers performance advantages for a huge range of appli cations and that is independent of the concrete applica tion and the DWDBS Currently none of the existing systems follows this approach Optimization Strategy 4.1 Rewrite Strategies for Query Sequences In formati on Information Request A Request C Strategy I Single-Query SQ Typical query sequences that are produced by deci sion support applications can be rewritten as a single and in most cases more complex query A straightforward method to achieve this single-query is as follows Start ing with the second query in the sequence, replace the occurrence of each temporary table in the FROM clause by the complete SELECT statement that generates data for this temporary table If we use this simple syntactic replacement algorithm for the query sequence in Figure 4 we first have to look at Q2 As this one does not use any temporary table we have to examine query 43 It needs data from table A1 as well as from A2 Therefore the FROM clause is changed and AI is replaced by the SELECT statement of Q1 The same replacement is nec essary for A2 The new query for A3 does not include any references to temporary tables Hence we can proceed with the last query in this sequence It is only based on the content of A3 Its purpose is to filter out the relevant data according to the ranking criterion We replace A3 in the FROM clause of query 44 by the new version of the query for A3 The resulting single-query for information request A is shown in Figure 6 a4.orderyear a5.partkey a5.partname. al.sumquantity a2 smquantity a1 sumquantity  a2 sumquantity AS incrquantity 1al.sumquantity  a2.sumquantity 1 from Q3 INSERT INTO A4 ordermonthkey, ardermodthname, orderyearkey, orderyear partkey. partname. swnquantity, Imsumquantity incrquantity incrquantity2 SELECT al.ordermonthkey al.ordermonthname, a1,orderyearkey a1.orderyear al.partkey al.partname, al.swnquantity a1 Imsumquantity. al.incrquantity. al.incrquantity2 FROM ISELECT a2.ordervearkev a2 ordermonthkev  al.partkey, SUM\(al.quantity  1 200ROM lineitem-orders al orderday a2 WHERE a2.orderdate  al.arderdate 1 AND a2.ordermonthkev IN 1199401 199402 I GROUP BY a2.orderyearkey. a2,ardermonthkey. al.partkey  AS a1 orderyearkey, ordermonthkey, partkey. sumguantity FROM lineitem-orders al orderday a2 WHERE a2.lastmonthdate i al.orderdate GROUP BY a2,ordermonthkey. al.partkey a2.ordermonthkey IN 199401 199602  AS a2 ordelmonthkey. partkey. sumquantity ordermonth a3 orderyear a4 part a5 WHERE al.ordermonthkey  a2.ordermonthkey AND al.partkey  a2.partkey AND al.ordermanthkey  a3.ardermonthkey AND al.orderyearkey  a4.orderyearkey AND al.partkey  a5.partkey  AS a1  ordermonthkey ordermonthname, orderyearkey orderyear partkey partname su-antity lmsumguantity incrquantity, incrquantityl  WHERE al.inc-antity2  98 from Q3 Figure 6 Single-Query for Request A This type of query rewrite that is similar to view ex pansion is an appropriate strategy for the DSS optimizer because it does not require any knowledge about the ap plication that generated the query sequence or about the DWDBS The rewrite strategy is only based on the list of queries that constitute the given query sequence I I 1 JOINS I SORTS I JOINS 1 SORTS f QS  Mergeselect SQ  Mergeselect I I I SQ  WhereToGroup I 16 1 16 Given these advantages of a single-query it seems to be a good idea to combine queries of a sequence into a single one in general, Nevertheless, the task is not that easy for the DSS optimizer. We have further analyzed the queries in our application scenario based on the query plans the optimizer of DB2 generated for our experi ments. Table 1 shows how they differ in the number of joins and sort operations. The values given here espe cially show that the straightforward combination of the original query sequence into one SQL query can result in a large number of joins and sorts For information request 181 


C the single-query SQ needs 33 joins and 37 sort opera tions The corresponding query plan has more than 200 nodes This complexity is not easy to handle for the optimizer of the DWDBS Its output might be a rather bad query plan As one example the number of join orders increases exponentially with the number of joins With more than 10 joins it is likely that the optimizer is not able to consider all relevant orders. Additionally it might not detect all common sub-expressions that are part of the query and that could be combined The appropriate and manageable complexity of queries depends on the characteristics of the optimizer used in the DWDBS So far our discussion has shown that in some cases a sequence of queries might run faster than the combined single-query and that in other situations this might not be true Our experiments show examples for both cases Hence, the DSS optimizer has to decide for each query sequence whether it should be combined or not Strategy IZ Partial Combination One alternative is not to combine the whole sequence into a single-query but to merge it into a couple of que ries As we have argued, one single-query could be too complex for the optimizer of a DWDBS to find a good execution plan If the DSS optimizer generates a small number of semi-complex queries this new sequence could be more efficient The optimization task is then to find the proper points where to cut off the original se quence. This could be done based on dependency graphs as given in Figure 8 For some query sequences it is not possible to gener ate an equivalent single-query  because there are depend encies within the sequence This appears in the case of ranking In the query sequence shown in Figure 4 the last query Q4 selects data according to a filter criterion For the sake of simplicity we assumed that the filter value is known in advance. Depending on the way the user de fined the information request and the query generation process of the application this filter predicate might be determined based on data in A3 In this case the last query of the sequence could include a variable instead of the filter predicate. Hence, the partial combination strat egy is applicable The DSS optimizer could combine QI Q2 and 43 as described for Strategy I and leave Q4 as is This strategy is not appropriate for the query se quences we present in this paper Nevertheless it is a suitable strategy for the DSS optimizer because it is only based on the list of queries that constitute the given query sequence Strategy ZIZ Semantic Rewrite Another deciding factor for the DSS optimizer could be based on supplementary analysis of the original query sequence. One method to minimize the runtime of query execution is to minimize the number of temporary tables in the whole sequence, thus removing the overhead for writing to and reading from temporary tables It is also important to avoid repeated references to temporary ta bles when composing the single query Most temporary tables imply joins between underlying tables Repeated referencing of temporary tables in different queries of a sequence implies a redundant computation of these tem porary tables via nested SQL statements when composing the single query Queries which restrict one single previous temporary table by additional projections and/or selections cat1 be propagated to the query creating this preceding tempo rary table i.e the projections and/or selections of the consuming query are moved into the foregoing query by concatenating both projections ConcutPruject  and/or selections ConcarSelect Queries with identical FROM, WHERE, GROUP BY HAVING and ORDER BY clause can be merged into one query by concatenating the SELECT clauses. We refer to this transformation as MergeSelect Queries with identical SELECT FROM GROUP BY HAVING and ORDER BY clauses can sometimes be merged into one query by concatenating the WHEW clauses by AND MergeWhere If the queries differ only in a selection on the same column they can be combined by replacing parts of the WHERE clause by an additional grouping on this column Where Tocroup Queries with identical SELECT FROM WHERE GROUP BY and ORDER BY clause can be merged into one query by concatenating the HAVING clauses by AND MergeHaving Simple transformations are the following QS QS  MergeSelect INSERT INK C5 Icustkey. stddeviation INSERT INlW C5 SELECT al.cuietkey sTwBVlal.urrtpric Icustkey stddevl stdde'r2 FROM lineitem-orders al orderday a2 CL 8TDDBV\(al..ndpric a1.custkey STDDBY\(Il..ndprice I AVC\(a1 ndpris IN 11992,1993,19941 FROM lineitemorders al AND al.cucfkey  C4.custkey orderday a2 GROUP BY al.custkey WHERE a2.orderdate i 11992,1993.1994 GROUP BY al.curtkey SELECT a1.custkey AND a2.ordaryearkey IN l1992,1993;1994 AND a1.custkey  C4.custkey GROUP BY al.aurtkey INSERT 1\260K Cl Icustkey cuscn stddevl, stddev2. turnoverl992 turnoverl993, turnoverl994 CS.stddevlation Cfi.stddeviation Cl.turnoverl992, C2.turnover1993 c3.turnovar1994 SELECT al.custkey. a4.custname FROM cl. c2, c3. c5 cs WHERE c5.cusckey  Cl.custkey AND C5,custkey  C2.custkey AND C5.custkey  C3.custkey AND C5.custkey  C6.custkey AND C5.custkey  a4.cu~tkey customer a4 INSERT 1\260K C7 Icustkey custnme stddevl stddev2, turnoverl992 turnove~1993 rurnoverl994l CS.stddev1. C5.stddev2 C4.turnoverl992 c4.turnover1993 C4.turnoverl994 SELECT C4.custkey 13.~~1tname FROM El C5 Customer a3 WHERE C4.custkey  C5.custkey AND C4.custkey  a3.custkey i Figure 7 Modified Sequences for Request C 182 


We generated two modified query sequences for in formation request C QS+MergeSelect is built by using the Mergeselect transformation for the tables C.5 and C6 of QS and by using a more complex transformation which additionally reduces the number of joins in the new tables C.5 and C7 The transformation process is shown in Figure 7 The other modification QS WhereToGroup uses the WhereToGroup transfor mation to further reduce the number of queries in the sequence 25 Obviously, these rewrite strategies can also be com bined with the single-query strategy and the partial com bination strategy We refer to the queries resulting from these hybrid strategies as SQ+MergeSelect and SQ WhereToGroup where each respective query se quence is combined into one query As a consequence the number of join and sort operations in the rewritten queries drop drastically more than 50 as can be seen in Table 1 4.2 Parallelism In this section we discuss how parallel execution can support the query sequences generated by decision sup port applications. Our focus is not'on intra-query parallel ism We assume that the DWDBS is able to generate par allel query plans for all queries in a sequence, where suit able Apart from this perspective on parallelism it is also interesting to see whether the DSS optimizer can figure out a group of queries in the sequence that could be run in parallel Therefore, our focal point is inter-query paral lelism For each sequence of queries we can determine how the steps of the sequence depend on each other and describe these dependencies for example by means of a graph The dependency graphs for the query sequences A and C of our application scenario are shown in Figure 8 For sequence A the graph shows that AI and A2 could be generated in parallel For sequence C the tables CI C2 and C3 could first be generated in parallel and later in the sequence the same holds for tables C.5 and C6 In general a group of queries can be executed in parallel if none of the queries within the group depends on each other Figure 8 Dependency Graphs The dependency graph for a given sequence of que ries can easily be determined by observing the FROM clauses of all queries The graph consists of the tempo rary tables Temp as nodes An edge from node Temp to Temp exists if the FROM clause of query Q contains Temp In particular no knowledge about the application is required. Hence this optimization strategy turns out to be applicable for the DSS optimizer The implementation of this strategy can be based on methods that generate parallel execution plans for SQL queries as they are used in standard database systems These techniques produce an internal representation of the queries which is extended by data and pipeline paral lelism The same methods could be used to exploit de pendency graphs for query sequences like those in Figure 8 That is why available technology is very well suited for the DSS optimizer 7 21 But how about the interface of the DSS optimizer to the database system The standard interface of a DWDBS allows the application to send one query after the other within one database transaction For each of the SQL statements to be run in parallel a separate connec tion and a separate transaction is necessary. Since those queries that are independent of each other should be started in parallel only their access to the data warehouse base tables could inhibit the use of parallel transactions If we assume that decision support applications only read base tables in the data warehouse the standard SQL in terface of database systems turns out to be sufficient for the optimization strategy we have discussed here 4.3 Statistics Query sequences generated by decision support appli cations usually produce the result data set that is relevant for the end user as the last step of the sequence As soon as this last query terminates the final result is available and all other temporary tables could be dropped For query sequence A in our example table A4 contains all relevant data for the end user whereas tables AI A2 and A3 may be deleted as soon as A4 is complete Each query in a sequence is processed by the opti mizer of the DWDBS One important factor that deter mines the efficiency of query execution is whether all necessary statistical information is available for the opti mizer or not Relevant statistical information is the num ber of rows the number and type of all columns and the distribution of values in all columns of each table that is read by a query. Most database systems do not automati cally update statistical information For example a sepa rate statement is available for Oracle8i that starts the up date of statistical information for a single table 22 The following statement would be necessary to generate sta TISTICS Additional parameters may be used in order tistics for AI ANALYZE TABLE A1 COMPUTE STA 183 


to make available more detailed information on the table and its indexes  We assume that statistical information is up to date for all base tables of the data warehouse. However this is not true for temporary tables generated by a sequence of queries If the execution of a query sequence should be based on current statistics the statements that provide this information have to be added to the sequence. The sequence for query C as given in Figure 8 could be augmented by calling ANALYZE TABLE for the tempo rary tables Cl through C7 The statistical informatioo has to be available for each of these tables before the tables are read by another query in the sequence for the first time The statistical information for table C1 does not have to be present until the query for C4 starts execution This leads to an additional strategy for the DSS opti mizer that is applicable without any knowledge about the application that generated a sequence of queries A se quence could be extended by statements that provide ad ditional statistical information for the DWDBS optimizer For this task the DSS optimizer has to know, how this is done for different database systems and which parameters influence the available statistics It also has to take into account the trade-off between the time that is needed for the update of statistical information and the query execu tion time that is saved with execution plans based on this information. Further enhancements could be achieved by running statistic updates in parallel to other statements in the sequence. This is always possible if the queries do not depend on the temporary table for which the statistic up date is in progress 4.4 Partitioning and Indexing All optimization strategies we have discussed so far are based on rewriting a sequence of SQL statements or enhancing the way it is executed In this section we will briefly discuss some approaches to support OLAP appli cations by changing the schema of the data warehouse Additional indexes on base tables in the data ware house enable the optimizer of the DWDBS to find query plans that are more efficient No rewriting of queries is required It is necessary to find the proper combination of indexes that offer maximum support for all applications on the data warehouse and take into account given con straints on disk space and time for index maintenance If typical query sequences on the data warehouse are known, administration tools of current database systems can be used to establish the suitable set of indexes 5 Partition tables support typical OLAP queries because they reduce the amount of base data that has to be scanned for one sequence of queries. One large fact table could include data for several years If the typical query retrieves data for exactly one year it might be more effi cient to store the facts in partition tables for each year 31 184 The scan of one partition table instead of scanning the complete large fact table might then be sufficient for many queries A smaller volume of data is likely to result in a more efficient retrieval of results This kind of parti tioning is supported by some OLAP tools 16 Applications on a data warehouse can also be sup ported by aggregate tables Especially OLAP applicai ions require a lot of grouping and aggregation If the agge gated data is already present in the data warehouse, que ries run more efficient because less data has to be proc essed and less aggregations have to be computed. Some algorithms to find the proper set of aggregate tables are described in 12  describes a couple of approaches how a given query can be processed based on existing aggregate tables Our experimental results indicate that indexes, aggre gate and partition tables are appropriate means to en hance query sequences. Nevertheless, the DSS optimizer is not the suitable component to create indexes, aggregate tables or partition tables The successful exploitation of these three strategies is based on knowledge about the schema of the data warehouse and the characteristics of all applications that access data in the warehouse. There is another important difference to the optimization ap proaches described in Section 4.1 through 4.3 All strate gies we described there offer a local optimization for ex actly one sequence of queries, whereas additional indexes and additional tables also influence the performance of other queries Therefore they should be part of the DWDBS administration 5 Experimental Results For our experiments we have created the modified schema as described in Section 2.1 in a database system We populated the tables with data based on the originai TPC-H data generated with three different scale factors according to the benchmark specifications Our largest data set for which the results are presented here was pro duced with scale factor 10 The raw data summarizes to 10 GB the main fact table lineitem-orders has about 60 million rows The environment used for the experiments consists of a Sun Enterprise E4500 with 12 processors 12 GB main memory, the object-relational database sys tem IBM DB2AJDB V7.1 and a benchmark tool that is provided with DB2 From a technological point of view we could have used any other market-strength DBMS since we only used techniques that are commonly available e.g paral lelism indexes partitioning as well as statistics The decision for DB2 was twofold First we wanted to collect detailed information on the generated query plans This information is provided by the EXPLAIN utility of DB2 13 Second DB2 is known for its good optimization technology and thus can play a good reference point 


The results given in Table 2 are a subset of more than 400 experiments where one experiment is the execution of one query sequence for one information request on one test data set Each value presented here is the average of at least three experiments Some cells of Table 2 are empty because not all optimization strategies were appli cable to all of the three information requests Most of them are applicable to information request A and request C Business questions that are similar to these two are very likely to build the majority of questions in a real environment Table 2 Experimental Results in seconds Looking at the results in more details one can see that in almost all cases the optimization strategies were suc cessful in reducing the runtime of the information re quests. Sometimes there was only a slight enhancement whereas other strategies led to an execution time that was an order of a magnitude shorter than for the original query The execution time for the original sequence of que ries QS is given in line 1 The modified versions of the query sequences and their combination into a single query SQ as described in Section 4.1 are shown in lines 2 through 6 These results show that combining the statements of a sequence is likely to improve the per formance. However this is not true for information re quest C The sequence QS+WhereToGroup runs much faster than the corresponding single-query Hence the DSS optimizer has to decide in which situation it is ap propriate to combine the queries of a sequence and in which the eventually modified version of the sequence is the best choice Line 7 of Table 2 shows the results for the query se quence that was enhanced by generating additional statis tical information for temporary tables In our experi ments this version of the queries runs always faster than the original sequence. For information request C it was also faster than the very complex single-query SQ whereas for request A it was slower than the single query Therefore, leaving the query sequence as is and generating additional statistical information for tempo 185 rary tables is a supplementary alternative to the other strategies which can be used by the DSS optimizer The third basic alternative for the DSS optimizer is to run the sequence of queries as it was generated but with some queries of the sequence in parallel as described in Section 4.2 The experimental results for this strategy are given in line 8 They show only a slight improvement compared to the original sequence. Especially for infor mation request C the enhancement is less than 10 percent This is because the runtime of the query sequence for this question is dominated by only one query that joins sev eral temporary tables Some results for the strategies that turned out not to be appropriate for the DSS optimizer are given in lines 9 and 10 With additional indexes and partitions we can see a remarkable performance enhancement for informa tion request B Additional partition tables reduced the runtime for information requests A and C where the re duction is between 8 and 25 percent For the information requests A and C we achieved further improvements by other strategies that are useable by the DSS optimizer Hence we do not loose too much optimization potential if the DSS optimizer has to ignore additional indexes and partitions as optimization strategies Nevertheless in a real environment these strategies are mandatory for the administrator of the data warehouse 6 Conclusion In this work we investigated that an optimizer in be tween decision support applications and the DWDBS is essential for the efficient processing of some classes of typical information requests generated by DSS tools Hence we suggested an optimized system architecture for decision support applications In this architecture generated query sequences are rewritten by an additional system component, called DSS optimizer The transfor mation is based on optimization strategies that are inde pendent of the application and use little knowledge about the underlying database system. This DSS optimizer has two main advantages First it is able to support several applications without any need to change their query gen eration algorithms. Second it is capable of applying op timization strategies that are not supported by state-of the-art database systems We set up a realistic application scenario, selected a set of relevant information requests and ran a large series of experiments based on TPC-H data Our experiments have shown that all three strategies we proposed for the DSS optimizer were successful. They reduced the run time of the given query sequences significantly None of the three strategies turned out to be the best in all situa tions Hence it is the task of the DSS optimizer to decide upon the strategy to use This decision should be based on information gained from the query sequence itself and on meta data gained from the underlying database sys 


tem Therefore further work will focus on heuristics that could be used by the DSS optimizer in order to decide which optimization strategy or which combination of strategies should be used for a given sequence of queries Another important issue of future work is the design and implementation of the DSS optimizer The basic technology for a DSS optimizer is in place because all three strategies are based on matured technology Gener ating single-queries is similar to view expansion Deter mining queries within a sequence that could run in paral lel is based on parallel database technology 7 21 Us ing statistical information for query optimization is a ba sic optimization technology Furthermore we want to apply extensible optimization technology as is available with systems like CASCADES 8 In doing so one has to observe that the DSS optimizer is not a general purpose optimizer but only a specific component that supports only a predefined set of optimization strategies Consequently we want to reconsider the main design decisions of an optimizer e.g search space, rule set and cost-based decision making As a result we want to come up with a tailored and efficient DSS optimization compo nent Acknowledgement We would like to thank Leonard Shapiro and Ralf Rantzau for helpful discussions and their comments on an early version of the paper References  11 AberdeenGroup: Bringing Analytical Reporting to Enter prise Business Intelligence Aberdeen Group Boston 1999 R Agrawal J C Shafer Parallel Mining of Association Rules. In TKDE 8\(6 1996 S Chaudhuri R Krishnamurthy S Potamianos K Shim Optimizing Queries with Materialized Views In ICDE March 1995 S Chaudhuri U Dayal An Overview of Data Warehous ing and OLAP Technology In SIGMOD Record Vol 26 No 1 1997 S Chaudhuri V Narasayya AutoAdmin 223What-if\222 Index Analysis Utility In SIGMOD Record Vol 27 No 2 1998 C J Date H Darwen A guide to the SQL standard 4th ed Addison-Wesley, Reading 1997 D DeWitt J Gray Parallel Database Systems The Fu ture of High Performance Database Systems In CACM G Graefe The Cascades Framework for Query Optimiza tion In: DE Bulletin Vol 18 No 3 1995 J Gray S Chaudhuri A Bosworth A Layman D Reichart M Venkatrao F Pellow H Pirahesh Data Cube A Relational Aggregation Operator Generalizing Group-By Cross-Tab and Sub-Totals In Data Mining and Knowledge Discovery, Vol 1 No 1 1997  101 P Gulutzan Trudy Pelzer SQL-99 Complete Really R&D Books, Lawrence, 1999 2 3 4 5 6 7 Vol 35 NO 6 pp 85-92 1992 8 9 ll J Han M Kamber Data Mining Concepts and Tech niques Morgan Kaufmann 2000 1121 V Harinarayan A Rajaraman, J D. Ullman Impleinent ing Data Cubes Efficiently. In SIGMOD Record Vol. 25 No 2 1996 13 IBM: DB2 Command Reference, 1999  141 Informix Data Warehousing for the Retail Industry White Paper http://www.informix.com 1998 15 R Kimball The Data Warehouse Toolkit John Wiley  Sons New York 1996 16 MicroStrategy The Case for Relational OLAP White Paper MicroStrategy, 1995 17 B Mitschang Query Processing in Database Systems in German\Vieweg-Verlag, 1995 IS P O\222Neil G Graefe Multi-Table Joins Through Bit mapped Join Indices In SIGMOD Record Vol 24 No 3 1995 19 P ONeil D Quantas: Improved Query Performance with Variant Indexes In SIGMOD Record Vol 26 No 2 1997 20 C Nippl Providing Efficient Extensible and Adaptive Intra-query Parallelism for Advanced Applications Tech nische Universitat Munchen Dissertation 2000 21 C Nippl B Mitschang TOPAZ A Cost-Based Rule Driven, Multi-Phase Parallelizer. Proc VLDB Conf New York, 1998 22 Oracle Corporation Oracle8i SQL Reference Release 3 8.1.7 Oracle September 2000 23 D Peppers M Rogers Data Warehousing and Retailing DM Reviews October 1998 24 SAS Institute Finding the Solution to Data Mining. White Paper SAS Institute 1998 25 H Schwarz R Wagner B Mitschang Improving the Processing of Decision Support Queries Strategies for a DSS Optimizer University of Stuttgart Faculty of Com puter Science Technical Report TR-2001-02,2001 126 A Sen V S Jacob Industrial-Strength Data Warehous ing In CACM Vol. 41 No 9 1998 27 D Srivastava S Dar S Jagadish A Levy In VLDB September 1996 28 S N Subramanian S Venkataraman: Cost-Based Opti mization of Decision Support Queries using Transient Views. In SIGMOD Record Vol. 27 No 2 29 Thinking Maschines Corporation Darwin Reference Release 3.0.1 Thinking Maschine Corporation 1998 30 D S Tkach: Information Mining with the IBM Intelligent Miner Family. White Paper IBM, 1998 3 11 Transaction Processing Performance Council TPC Benchmark H Decision Support Standard Specification Revision 1.1 O June 1999 32 R Wagner Optimization of an OLAP Application for Retailers in German University of Stuttgart Faculty of\221 Computer Science Project Paper Nr 1770,2000 33 M Zaharioudakis, R Cochrane G Lapis H Pirahesh M Urata Answering Complex SQL queries Using Auto matic Summary Tables In SIGMOD Record Vol 29 No 2 2000 34 Y Zhao P M Deshpande J F Naughton A Shukla Simultanous Optimization and Evaluation of Multiple Dimensional Queries In SIGMOD Record, Vol 27 No 2 1998 186 


International Conference on Very Large Databases 1994  R  J  B ayardo J r  R  A gra w al and D  G unopul os  Constraint-Based Rule Mining in Large Dense Databases In Proceedings of the 15th International Conference on Data Engineering 1999 pp 188\226197 4 S  B rin  R Mo tw an i J.D Ullman  a n d S Tsu r  Dynamic itemset counting and implication rules for market basket data In Proceedings of the ACM SIGMOD Conference on Management of Data 1997 pp 255\226264  A  B roder  On the r esemblance and c ontainment of documents In Compression and Complexity of Sequences SEQUENCES'97  1998 pp 21\22629  D  W  C heung J  H a n V  T  N g  e t a l  A Fast Distributed Algorithm for Mining Association Rules In Proceedings of Conference on Parallel and Distributed Information Systems 1996 pp 31\22642 7 E  C o h e n  Size-Estimatio n F rame w o rk with Applications to Transitive Closure and Reachability Journal of Computer and System Sciences 55 1997 441\226453 8 E  C o h e n  M Datar  S Fu jiw ara A Gio n i s et al Finding Interesting Associations without Support Pruning In Proceedings of the 16th International Conference on Data Engineering  2000  R O D uda and P E H art Pattern Classi\002cation and Scene Analysis  A Wiley-Interscience Publication New York 1973  A  G i oni s  P  Indyk and R  M ot w a ni  S i m i l a ri t y Search in High Dimensions via Hashing In Proceedings of the 25th International Conference on Very Large Databases 1999 11 D Go ld b e r g  D  Nich o l s B.M Ok i an d D  T erry  Using collaborative 002ltering to weave an information tapestry Communications of the ACM 55 1991 1\226 19  S  G uha R  R as t ogi  a nd K  S h i m  C U R E A n Ef\002cient Clustering Algorithm for Large Databases In Proceedings of the ACM-SIGMOD International Conference on Management of Data 1998 pp 73\22684  R  Mot w ani a nd P  R a gha v a n Randomized Algorithms Cambridge University Press 1995  J  S  P a rk M S  C hen and P S  Y u A n ef fect i v e hash-based algorithm for mining association rules In Proceedings of the ACM SIGMOD Conference on Management of Data 1995 pp 175\226186  P r oj ect G u t e nber g  http:..www.gutenberg.net  1999  N Shi v akumar and H  G arcia-Molina B u ilding a Scalable and Accurate Copy Detection Mechanism In Proceedings of the 3rd International Conference on the Theory and Practice of Digital Libraries  1996  H.R  V a rian and P  R esnick E ds C A C M S pecial Issue on Recommender Systems Communications of the ACM 40 1997 


User Anomaly Description programmer2 logs in from beta secretary logs in at night sysadm logs in from jupiter programmer1 becomes a secretary secretary becomes a manager programmer1 logs in at night sysadm becomes a programmer manager1 becomes a sysadm manager2 logs in from pluto Table 12 User Anomaly Description User Normal Anomaly programmer2 0.58 0.79 0.00 secretary  1  1  0.00 sysadm 0.84 0.95 0.00 programmer1 0.31 1.00 0.04 secretary 0.41 0.98 0.17 programmer1  1  1  0.00 sysadm 0.64 0.95 0.00 manager1 0.57 1.00 0.00 manager2 1.00 1.00 0.00 Table 13 Similarity with User's Own Pro\002le tivities of each time segment am pm and nt We treat the 5th week as the training period during which we compare the patterns from each session to the pro\002le of the time segment We record the normal range of the similarity scores during this week The data in the 6th week has some user anomalies as described in Table 12 For each of the anomalous sessions we compare its patterns against the original user's pro\002le and then compare the resulting similarity score against the recorded normal range of the same time segment In Table 13 the column labeled 223Normal\224 is the range of similarity of each user against his or her own pro\002le as recorded during the 5th week A 1 here means that the user did not login during the time segment in the 5th week The column 223Anomaly\224 is the similarity measure of the anomalous session described Table 12 We see that all anomalous sessions can be clearly detected since their similarity scores are much smaller than the normal range For example when the sysadm becomes programmer see Table 12 his/her patterns have zero matches with the sysadm's pro\002le while for the whole 5th week the pm similarity scores are in the range of 0.64 to 0.95 Unfortunately formal evaluation statistics are not available to determine the error rates of this approach However this initial test indicates a path worthy of future study 6 Related Work Network intrusion detection has been an on-going research area 17  M ore r ecent s ystems e g B ro 18   NFR 6  a n d EMERALD  1 9  a ll mad e e x ten s ib ility th eir primary design goals Our research focuses on automatic methods for constructing intrusion detection models The meta-learning mechanism is designed to automate the extention process of IDSs We share the same views discussed in 20 t h at an ID S s houl d b e b ui l t us i n g s t a ndard components We believe that the operating system and networking community should be responsible for building a robust 223Event\224 box In 10  a l gori t h ms for a nal y zi ng us er s h el l c ommands and detecting anomalies were discussed The basic idea is to 002rst collapse the multi-column shell commands into a single stream of strings and then string matching techniques and consideration of 223concept drift\224 are used to build and update user pro\002les We believe that our extended frequent episodes algorithm is a superior approach because it considers both the association among commands and arguments and the frequent sequential patterns of such associations 7 Conclusions and Future Directions In this paper we outline a data mining framework for constructing intrusion detection models The key idea is to apply data mining programs to audit data to compute misuse and anomaly detection models according to the observed behavior in the data To facilitate adaptability and extensibility we propose the use of meta-learning as a means to construct a combined model that incorporate evidence from multiple lightweight base models This mechanism makes it feasible to introduce new ID components in an existing IDS possibly without signi\002cant re-engineering We extend the basic association rules and frequent episodes algorithms to accommodate the special requirements in analyzing audit data Our experiments show that the frequent patterns mined from audit data can be used as reliable user anomaly detection models and as guidelines for selecting temporal statistical features to build effective classi\002cation models Results from the 1998 DARPA Intrusion Detection Evaluation Program showed our detection models performed as well as the best systems built using the manual knowledge engineering approaches Our future work includes developing network anomaly detection strategies and devising a mechanical procedure to translate our automatically learned detection rules into modules for real-time IDSs A preliminary project in collaboration with NFR has just started 12 


8 Acknowledgments We wish to thank our colleagues at Columbia University Chris Park Wei Fan and Andreas Prodromidis for their help and encouragement References 1 R  A g r a w a l  T  I m i e lin sk i a n d A  S w a m i  M in in g a sso c i a tion rules between sets of items in large databases In Proceedings of the ACM SIGMOD Conference on Management of Data  pages 207\226216 1993 2 P  K  C han a nd S  J S t ol f o  T o w ar d p ar al l e l a nd di st r i b u t e d learning by meta-learning In AAAI Workshop in Knowledge Discovery in Databases  pages 227\226240 1993 3 W  W  C ohen Fast ef f ect i v e r ul e i nduct i on I n Machine Learning the 12th International Conference  Lake Taho CA 1995 Morgan Kaufmann 4 U  F ayyad G P i at et sk yS h api r o and P  S myt h  T he KDD process of extracting useful knowledge from volumes of data Communications of the ACM  39\(11\:27\22634 November 1996 5 K  I l gun R  A K e mmer e r  and P  A  P or r a s S t at e t r a nsition analysis A rule-based intrusion detection approach IEEE Transactions on Software Engineering  21\(3\:181\226 199 March 1995 6 N  F  R  I n c  N etw o rk 003ig h t reco rd er  h ttp www n fr co m  1997 7 V  J acobson C  L e r e s and S  M cC anne t cpdump a v ai l a bl e via anonymous ftp to ftp.ee.lbl.gov June 1989 8 C  K o  G Fin k  a n d K  L e v itt Au to m a te d d e t e c tio n o f v u l nerabilities in privileged programs by execution monitoring In Proceedings of the 10th Annual Computer Security Applications Conference  pages 134\226144 December 1994 9 S  K umar and E  H  S paf f or d A s of t w ar e a r c hi t ect ur e t o support misuse intrusion detection In Proceedings of the 18th National Information Security Conference  pages 194\226 204 1995  T  L a ne and C  E  B r odl e y  S equence m at chi n g a nd l ear ni ng in anomaly detection for computer security In AAAI Workshop AI Approaches to Fraud Detection and Risk Management  pages 43\22649 AAAI Press July 1997  W  L e e a nd S  J S t ol f o  D at a m i n i n g a ppr oaches f o r i nt r u sion detection In Proceedings of the 7th USENIX Security Symposium  San Antonio TX January 1998  W  L ee S  J S t ol f o  a nd K W  Mok Mi ni ng i n a d at a\003 o w environment Experience in intrusion detection submitted for publication March 1999  T  L unt  D et ect i n g i nt r uder s i n comput er syst ems I n Proceedings of the 1993 Conference on Auditing and Computer Technology  1993  T  L unt  A  T amar u F  Gi l h am R  J agannat h an P  N eumann H Javitz A Valdes and T Garvey A real-time intrusion detection expert system IDES 002nal technical report Technical report Computer Science Laboratory SRI International Menlo Park California February 1992  H Manni l a and H  T oi v onen Di sco v e r i ng gener a l i zed episodes using minimal occurrences In Proceedings of the 2nd International Conference on Knowledge Discovery in Databases and Data Mining  Portland Oregon August 1996  H Manni l a  H  T oi v onen and A  I  V er kamo D i s co vering frequent episodes in sequences In Proceedings of the 1st International Conference on Knowledge Discovery in Databases and Data Mining  Montreal Canada August 1995  B M ukherjee L T  Heberlein and K  N  L e v itt Netw ork intrusion detection IEEE Network  May/June 1994  V  Paxon B r o  A syst em f o r d et ect i n g n et w o r k i n t r uder s in real-time In Proceedings of the 7th USENIX Security Symposium  San Antonio TX 1998  P  A P o r r a s a nd P  G Neumann E m er al d E v ent m oni t o r i ng enabling responses to anomalous live disturbances In National Information Systems Security Conference  Baltimore MD October 1997  S  S t ai nf or dC h en C ommon i nt r u si on det ect i o n f r a me w o r k  http://seclab.cs.ucdavis.edu/cidf  S  J S t ol f o  A  L  P r odr omi d i s  S  T sel e pi s W  L ee D W  Fan and P K Chan JAM Java agents for meta-learning over distributed databases In Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining  pages 74\22681 Newport Beach CA August 1997 AAAI Press  S unS of t  Mount ai n V i e w  C A  SunSHIELD Basic Security Module Guide  13 


 30 40 50 60 70 80 90 100 110 120 30 40 50 60 70 80 90 10 0 Execution Time [s Number of Nodes Figure 13 Execution time of HPA program pass 2 on PC cluster 2 Scan the transaction database and count the support count Each processing node reads the transaction database from its local disk 000 itemsets are generated from that transaction and the same hash function used in phase 1 s applied to each of them Each of the 000 itemsets is sent to certain processing node according the hash value For the itemsets received from the other nodes and those locally generated whose ID equals the node\220s ID the hash table is searched If hit its support count value is incremented 3 Determine the large itemset After reading all the transaction data each processing node can individually determine whether each candidate 000 itemset satisfy user-specified minimum support or not Each processing node sends large 000 itemsets to the coordinator where all the large 000 itemsets are gathered 4 Check the terminal condition If the large 000 itemsets are empty the algorithm terminates Otherwise the coordinator broadcasts large 000 itemsets to all the processing nodes and the algorithm enters the next iteration 4.2 Performance evaluation of HPA algorithm The HPA program explained above is implemented on our PC cluster Each node of the cluster has a transaction data file on its own hard disk Transaction data is produced using data generation program developed by Agrawal designating some parameters such as the number of transaction the number of different items and so on The produced data is divided by the number of nodes and copied to each node\220s hard disk The parameters used in the evaluation is as follows The number of transaction is 5,000,000 the number of different items is 5000 and minimum support is 0.7 The size of the data is about 400MBytes in total The message block size is set to be 16KBytes according to the results of communication characteristics of PC clusters discussed in previous section The disk I/O block size is 64KBytes which seems to be most suitable value for the system Note that the number of candidate itemset in pass 2 s substantially larger than for the other passes which relatively frequently occurs in association rules mining Therefore we have been careful to parallelize the program effectively especially in pass 2 so that unnecessary itemsets to count should not be generated 14 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


The execution time of the HPA program pass 2 is shown in figure 13 as the number of PCs is changed The maximum number of PCs used in this evaluation is 100 Reasonably good speedup is achieved in this application as the number of PCs is increased 5 Conclusion In this paper we presented performance evaluation of parallel database processing on an ATM connected 100 node PC cluster system The latest PCs enabled us to obtain over 110Mbps throughput in point-to-point communication on a 155Mbps ATM network even with the so-called 217\217heavy\220\220 TCP/IP This greatly helped in developing the system in a short period since we were absorbed in fixing many other problems Massively parallel computers now tend to be used in business applications as well as the conventional scientific computation Two major business applications decision support query processing and data mining were picked up and executed on the PC cluster The query processing environment was built using the results of our previous research the super database computer SDC project Performace evaluation results with a query of the standard TPC-D benchmark showed that our system achieved superior performance especially when transposed file organization was employed As for data mining we developed a parallel algorithm for mining association rules and implemented it on the PC cluster By utilizing aggregate memory of the system efficiently the system showed good speedup characteristics as the number of nodes increased The good price/performance ratio makes PC clusters very attractive and promising for parallel database processing applications All these facts support the effectiveness of the commodity PC based massively parallel database servers Acknowledgment This project is supported by NEDO New Energy and Industrial Technology Development Organization in Japan Hitachi Ltd technically helped us extensively for ATM related issues References  R Agrawal T Imielinski and A Swami Mining association rules between sets of items in large databases In Proceedings of ACM SIGMOD International Conference on Management of Data  pages 207--216 1993  R Agrawal and R Srikant Fast algorithms for mining association rules In Proceedings of International Conference on Very Large Data Bases  1994  A C Arpaci-Dusseau R H Arpaci-Dusseau D E Culler J M Hellerstein and D A Patterson High-performance sorting on Networks of Workstations In Proceedings of International Conference on Management of Data  pages 243--254 1997  D.S Batory On searching transposed files ACM TODS  4\(4 1979  P.A Boncz W Quak and M.L Kersten Monet and its geographical extensions A novel approach to high performance GIS processing In Proceedings of International Conference on Extending Database Technology  pages 147--166 1996 15 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


 R Carter and J Laroco Commodity clusters Performance comparison between PC\220s and workstations In Proceedings of IEEE International Symposium on High Performance Distributed Computing  pages 292--304 1995  D.J DeWitt and J Gray Parallel database systems  The future of high performance database systems Communications of the ACM  35\(6 1992  J Gray editor The Benchmark Handbook for Database and Transaction Processing Systems  Morgan Kaufmann Publishers 2nd edition 1993  J Heinanen Multiprotocol encapsulation over ATM adaptation layer 5 Technical Report RFC1483 1993  M Kitsuregawa M Nakano and M Takagi Query execution for large relations on Functional Disk System In Proceedings of International Conference on Data Engineering  5th pages 159--167 IEEE 1989  M Kitsuregawa and Y Ogawa Bucket Spreading Parallel Hash:A new parallel hash join method with robustness for data skew in Super Database Computer SDC In Proceedings of International Conference on Very Large Data Bases  16th pages 210--221 1990  M Laubach Classical IP and ARP over ATM Technical Report RFC1577 1994  D.A Schneider and D.J DeWitt Tradeoffs in processing complex join queries via hashing in multiprocessor database machines In Proceedings of International Conference on Very Large Data Bases  16th pages 469--480 1990  T Shintani and M Kitsuregawa Hash based parallel algorithms for mining association rules In Proceedings of IEEE International Conference on Parallel and Distributed Information Systems  pages 19--30 1996  T Sterling D Saverese D.J Becker B Fryxell and K Olson Communication overhead for space science applications on the Beowulf parallel workstaion In Proceedings of International Symposium on High Performance Distributed Computing  pages 23--30 1995  T Tamura M Nakamura M Kitsuregawa and Y Ogawa Implementation and performance evaluation of the parallel relational database server SDC-II In Proceedings of International Conference on Parallel Processing  25th pages I--212--I--221 1996  TPC TPC Benchmark 000\001 D Decision Support Standard Specification Revision 1.1 Transaction Processing Performance Council 1995 16 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


In accordance with 1910.97 and 1910.209 warning signs are required in microwave areas For work involving power line carrier systems this work is to be conducted according to requirements for work on energized lines Comments s APPA objects to the absolute requirement implied by the word ensure regarding exposure to microwave radiation and recommends revision of s l iii to read when an employee works in an area where electromagnetic radiation levels could exceed the levels specified in the radiation protection guide the employer shall institute measures designed to protect employees from accidental exposure to radiation levels greater than those permitted by that guide  I1 an employee must be stationed at the remote end of the rodding operation Before moving an energized cable it must be inspected for defects which might lead to a fault To prevent accidents from working on the wrong cable would require identification of the correct cable when multiple cables are present Would prohibit an employee from working in a manhole with an energized cable with a defect that could lead to a fault However if the cable cannot be deenergized while another cable is out employees may enter the manhole but must protect against failure by some means for example using a ballistics blanket wrapped around cable Requires bonding around opening in metal sheath while working on cable Underaround EIectrical Installations t Comments t This paragraph addresses safety for underground vaults and manholes The following requirements are contained in this section Ladders must be used in manholes and vaults greater than four feet deep and climbing on cables and hangers in these vaults is prohibited Equipment used to lower materials and tools in manholes must be capable of supporting the weight and should be checked for defects before use An employee in a manhole must have an attendant in the immediate vicinity with facilities greater than 250 volts energized An employee working alone is permitted to enter briefly for inspection housekeeping taking readings or similar assuming work could be done safely Duct rods must be inserted in the direction presenting the least hazard to employees and APPA recommends that OSHA rewrite section 7\regarding working with defective cables This rewrite would include the words shall be given a thorough inspection and a determination made as to whether they represent a hazard to personnel or representative of an impending fault As in Subsection \(e EEI proposes the addition of wording to cover training of employees in emergency rescue procedures and for providing and maintaining rescue equipment Substations U This paragraph covers work performed in substations and contains the following requirements Requires that enough space be provided around electrical equipment to allow ready and safe access for operation and maintenance of equipment OSHA's position A2-16 


is that this requirement is sufficiently performance oriented to meet the requirements for old installations according to the 1987 NEW Requires draw-out circuit breakers to be inserted and removed while in the open position and that if the design permits the control circuits be rendered inoperative while breakers are being inserted and removed stated in the Rules and requests that existing installations not be required to be modified to meet NESC APPA recommends that Section u 4 i which includes requirements for enclosing electric conductors and equipment to minimize unauthorized access to such equipment be modified to refer to only those areas which are accessible to the public Requires conductive fences around substations to be grounded Power Generation v Addresses guarding of energized parts  Fences screens, partitions or walls This section provides additional requirements and related work practices for power generating plants  Entrances locked or attended Special Conditions w  Warning signs posted  Live parts greater than 150 volts to be guarded or isolated by location or be insulated  Enclosures are to be according to the 1987 NESC Sections llOA and 124A1 and in 1993 NESC  Requires guarding of live parts except during an operation and maintenance function when guards are removed barriers must be installed to prevent employees in the area from contacting exposed live parts Requires employees who do not work regularly at the substation to report their presence Requires information to be communicated to employees during job briefings in accordance with Section \(c of the Rules Comments U APPA and EEI provide comments as follows Both believe that some older substations \(and power plants would not meet NESC as This paragraph proposes special conditions that are encountered during electric power generation, transmission and distribution work including the following Capacitors  Requires individual units in a rack to be short circuited and the rack grounded  Require lines with capacitors connected to be short circuited before being considered deenergized Current transformer secondaries may not be opened while energized and must be bridged if the CT circuit is opened Series street lighting circuits with open circuit voltages greater than 600 volts must be worked in accordance with Section q\or t and the series loop may be opened only after the source transformer is deenergized and isolated or after the loop is bridged to avoid open circuit condition Sufficient artificial light must be provided where insufficient naturals illumination is present to enable employee to work safely A2-17 


US Coast Guard approved personal floatation devices must be supplied and inspected where employees are engaged in work where there is danger of drowning Required employee protection in public work areas to include the following  Warning signs or flags and other traffic control devices  Barricades for additional protection to employees  Barricades around excavated areas  Warning lights at night prominently displayed Lines or equipment which may be sub to backfeed from cogeneration or other sources are to be worked as energized in accordance with the applicable paragraphs of the Rules Comments w APPA submits the following comments regarding this Special Conditions section Recommends that the wording regarding capacitors be modified to include a waiting period for five minutes prior to short circuiting and grounding in accordance with industry standards for discharging of capacitors For series street light circuits, recommends that language be added for bridging to either install a bypass conductor or by placement of grounds so that work occurs between the grounds Recommends modification of the section regarding personal floatation devices to not apply to work sites near fountains decorative ponds swimming pools or other bodies of water on residential and commercial property Definitions x This section of the proposed Rules includes definitions of terms Definitions particularly pertinent to understanding the proposal and which have not previously been included are listed as follows Authorized Employee  an employee to whom the authority and responsibility to perform a specific assignment has been given by the employer who can demonstrate by experience or training the ability to recognize potentially hazardous energy and its potential impact on the work place conditions and who has the knowledge to implement adequate methods and means for the control and isolation of such energy CZearance for Work  Authorization to perform specified work or permission to enter a restricted area Clearance from Hazard  Separation from energized lines or equipment Comments x The following summarizes the changes in some of the definitions which APPA recommends Add to the definition for authorized employee It the authorized employee may be an employee assigned to perform the work or assigned to provide the energy control and isolation function  Recommends that OSHA modify the definition for a line clearance tree trimmer to add the word qualified resulting in the complete designation as a qualified line clearance tree trimmer Recommends that OSHA modify the definition of qualified employee" to remove the word construction from the definition since it is felt that knowledge of construction procedures is beyond the scope of the proposed rule resulting in APPA's new A2-18 I 


wording as follows more knowledgeable in operation and hazards associated with electric power generation transmission and/or distribution equipment Recommends that OSHA add a definition for the word practicable and replace the word feasible with practicable wherever it appears in the proposed regulations and that practicable be further defined as capable of being accomplished by reasonably available and economic means OTHER ISSUES Clothing OSHA requested comments on the advisability of adopting requirements regarding the clothing worn by electric utility industry employees EEI has presented comments which indicates research is underway prior to establishing a standard for clothing to be worn by electric utility employees However EEI's position is that this standard has not developed to the extent that it could be included in the OSHA Rules Both APPA and EEI state that they would support a requirement that employers train employees regarding the proper type of clothing to wear to minimize hazards when working in the vicinity of exposed energized facilities Grandfathering Due to the anticipated cost impact on the utility industry of the proposed Rules requiring that existing installations be brought to the requirements of the proposed Rules both APPA and EEI propose that the final Rules include an omnibus grandfather provision This provision would exempt those selected types of facilities from modification to meet the new rules EEI states that if the grandfathering concept is incorporated that electric utility employees will not be deprived of proper protection They propose that employers be required to provide employees with a level of protection equivalent to that which the standard would require in those instances in which the utility does not choose to modify existing facilities to comply with the final standard Rubber Sleeves OSHA requests comments from the industry on whether it would be advisable to require rubber insulating sleeves when gloves are used on lines or equipment energized at more than a given voltage EEI states its position that utilities should continue to have the option of choosing rubber gloves or gloves and sleeves to protect employees when it is necessary to work closer to energized lines than the distances specified in the clearance tables Preemuting State Laws EEI requests that the final Rules be clear in their preempting state rules applicable to the operation and maintenance work rules for electric power systems. This is especially critical since some states now have existing laws which are more stringent than the proposed OSHA Rules Examples are 1 in California and Pennsylvania where electric utility linemen are prohibited from using rubber gloves to work on lines and equipment energized at more than certain voltages and 2 in California and Connecticut where the live line bare hand method of working on high voltage transmission systems is prohibited One utility Pacific Gas  Electric has obtained a variance from the California OSHA to perform live line bare-hand transmission maintenance work on an experimental basis Coiiflicts Between the Rilles and Part 1926 Subpart V Since many of the work procedures in construction work and operation and maintenance work are similar and difficult to distinguish between EEI requests that the final order be clear in establishing which rule has jurisdiction over such similar work areas A2-19 v 


IMPACTS ON COSTS AND ASSOCIATED BENEFITS In its introduction to the proposed rules OSHA has provided an estimate of the annual cost impact on the electric utility industry for the proposed des of approximately 20.7 million OSHA estimates that compliance with this proposed standard would annually prevent between 24 and 28 fatalities and 2,175 injuries per year The utilities which have responded to this proposed standard through their respective associations have questioned the claims both of the magnitude of the cost involved and the benefit to the industry in preventing fatalities and lost-time injuries Both EEI and APPA feel that the annual cost which OSHA estimates are significantly lower than would be realized in practice Factors which APPA and EEI feel were not properly addressed include the following OSHA has not accurately accounted for cost of potential retroactive impacts including retrofitting and modifying existing installations and equipment OSHA has not consistently implemented performance based provisions in proposed rules  many portions require specific approaches which would require utilities to replace procedures already in place with new procedures Estimates were based on an average size investor-owned utility of 2,800 employees and an average rural cooperative of 56 employees, which are not applicable to many smaller systems such as municipal systems OSHA has not adequately addressed the retraining which would be necessary with modifying long-established industry practices to be in accordance with the OSHA rules EEI claims that OSHA's proposed clearance requirements would not allow the use of established maintenance techniques for maintaining high voltage transmission systems and thus would require new techniques For an example of the cost which is estimated to be experienced as a result of the new Rules one of the EEI member companies has estimated that approximately 20,000 transmission towers would need to be modified to accommodate the required step bolts in the Rules at an estimated cost of 6,200,000 Additionally this same company estimates that the annual cost of retesting live line tools for its estimated 1,000 tools would be 265,000 Additionally, both EEI and APPA question the additional benefits which OSHA claims would result from implementation of the new Rules APPA questions the estimates of preventing an additional 24 to 28 fatalities annually and 2,175 injuries per year in that it fails to account for the fact that the industry has already implemented in large part safety measures which are incorporated in the Rules EEI and APPA also point out that many preventable injuries cannot be eliminated despite work rules enforcement and safety awareness campaigns since many such accidents which result in fatalities are due to employee being trained but not following the employer's training and policies PRESENT STATUS OF RULES According to information received from the OSHA office in February 1993 the final Rules are to be published no later than July 1993 and possibly as soon as March 1993 OSHA closed their receipt of comments in March 1991 and no further changes in the rules are thought possible A2-20 


CONCLUSION The OSHA 1910.269 which proposes to cover electric utility operation and maintenance work rules affects a multitude of working procedures as are summarized in this paper It is not possible at the present time to assess the final structure of the Rules as may be proposed in 1993 or subsequent years Since the comments from the utility associations APPA and EEI were made following the initial release of the proposed OSHA Rules in 1989 a significant amount of time has elapsed where other events have occurred which may affect the form of the final Rules The 1993 NESC went into effect in August 1992 and includes some of the requirements to which the commenters objected For example a significant requirement in the Part 4 of the 1993 NESC requires that rubber gloves be utilized on exposed energized parts of facilities operating at 50 to 300 volts This requirement is in conflict with EEl\222s proposed change to the OSHA Rules which would still allow working such secondary facilities without the use of rubber gloves Electric utilities are advised to review the January 31 1989 proposed operation and maintenance Rules as summarized in this paper and to review their procedures which would be affected by application of the Rules Many of the procedures proposed in the Rules provide valuable guidance in electric utilities\222 operation and maintenance activities Where the cost impact is not significant, it is recommended that utilities consider implementing such procedures in expectation of the Rules being published in the next few months Also it would be appropriate for electric utilities to review the 1993 edition of the NESC since there are portions of the Rules which have resulted in changes in the NESC These changes mainly occur in Part 4 Rules for the Operation of Electric Supply and Communications Lines and Equipment The concerns which the commenters have addressed regarding the cost impact and the resulting benefits experienced as a result of the promulgation of the Rules are real ones and must be addressed in the final Rules As a result this paper cannot present a conclusion regarding the full impact of the Rules The development of such Rules continue to be an ongoing matter and will undoubtedly require later analysis when the final rules are published A2-21 


