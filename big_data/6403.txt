A Similarity Measure for Time Frequency and Dependencies in Large-Scale Workloads Mario Lassnig Thomas Fahringer Institute of Computer Science University of Innsbruck 6020 Innsbruck Austria mario,tf}@dps.uibk.ac.at Vincent Garonne Angelos Molfetas Martin Barisits European Organisation for Nuclear Research 1211 Geneva Switzerland rstname.lastname}@cern.ch ABSTRACT Performance evaluations of large-scale systems require the use of representative workloads with certiìable similar or dissimilar characteristics To quantify the similarity of the characteristics we describe a novel measure comprising two ecient methods that are suitable for large-scale workloads One method uses the discrete wavelet transform to assess the periodic time and frequency characteristics in the workload The second method evaluates dependencies in descriptive attributes via association rule learning Both methods are evaluated to nd the limits of their similarity spaces Additionally the wavelet method is evaluated against existing similarity methods and tested for noise robustness and random bias An empirical study using workloads from seven operational large-scale systems evaluates the measureês accuracy The results show that our measure is highly resistant to noise well-suited for large-scale workloads covers 87 of the possible similarity space and improves accuracy by 24.5 and standard deviation by 10.8 when compared to existing work 1 INTRODUCTION Performance evaluation studies of computing systems are strongly dependent on the input workload 24 T his i s especially important for large-scale systems with subtle dependencies or emergent behaviour in the workload e.g different computations that use the same data in parallel the same computations that use dierent data or even automatically triggered resource provisioning 27 This usually requires studies at the full scale of the system for correct performance evaluation and consequently presents a hard problem for large-scale systems 8 W e a re no w f orced t o test in smaller environments as large-scale systems continue to grow Therefore we would like to certify and guarantee that the workloads used in these smaller simulations capture the essentials of the workload that a real system would take Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proìt or commercial advantage and that copies bear this notice and the full citation on the rst page To copy otherwise to republish to post on servers or to redistribute to lists requires prior speciìc permission and/or a fee SC11 November 12-18 2011 Seattle Washington USA Copyright 2011 ACM 978-1-4503-0771-0/11/11 10.00 as input Thus the principal question we want to answer is Are our workload models correct Additionally we might be interested to test only certain parts of the largescale system for example to evaluate power consumption This requires that we nd certiìable dissimilar workloads that exercise only isolated parts of the system To solve this problem we propose an approach to quantify the similarity between two workloads by focusing on two key aspects the periodic time and frequency characteristics of event arrivals and the attribute dependencies of these events We combine both methods to yield a comparison factor 0  m  1 describing the similarity of two workloads The upper limit of m  1 denotes equivalent behaviour whereas the lower limit of m  0 denotes dierent behaviour The main contributions of this work can be summarised as follows In section 3 we introduce a novel quantitative method based on the discrete wavelet transform We compare the method against commonly employed quantitative methods and test its robustness against noise and random bias In section 4 we introduce a novel second method based on association rule learning and evaluate it with synthetic workloads to nd its upper and lower bounds of the similarity space In section 5 we combine both methods and demonstrate that the combination of both introduced methods improves upon existing work as it increases accuracy and reduces standard deviation The evaluation is performed against two prominent scheduling optimisation metrics mean waiting time and mean execution time and complemented by the Anderson-Darling statistical hypothesis test Finally we conclude the paper in sections 6 and 7 with a summary of the results and an outlook on future work 2 BACKGROUND Our notion of workload is a time-dependent vector of events Each event stores many attributes like submission time  user  resource or queue  The workload needs to have at least one time-dependent attribute that ensures a total ordering of the events e.g the submission time All other attributes are called descriptive attributes and give more details of the event e.g requested processors or memory requirements The workload is then used as input to real or simulated systems which execute the given events The primary class of target systems for the similarity measure are large-scale distributed systems for data-intensive science i.e we compare scientiìc workloads More precisely the particular motivation stems from simulation studies of the large-scale distributed data management system Don Quijote 2 DQ2 DQ2 m anages t ens o f p etab ytes 


of data for the high-energy physics experiment ATLAS at the Large Hadron Collider LHC with almost 1000 storage endpoints distributed globally and wildly varying usage patterns 4 N ew data managemen t strategies need to be evaluated continuously through smaller scale simulations because the operational instance cannot be suciently modiìed and used for experimental purposes Examples of such strategies include intelligent placement of data in storage centres or dynamic resource-aware scheduling of transfers To test these strategies the evaluation of many different scenarios is required each with thousands of dierent parametrised workloads to account for possible emergent behaviour For example what happens when the number of users with a particular behaviour is increased but at the same time the amount of available storage required for that particular behaviour is changed Therefore it is important to ensure that the workloads generated for the simulation are similar in time frequency and dependencies to the real DQ2 workload otherwise emergent behaviour like data transfer/deletion cycles or throughput degradation are not triggered Furthermore in the course of the simulation monthly workloads with more than 5 million data accesses per day need to be evaluated resulting in more than 150 million events that need to be assessed for similarity in each evaluation of the simulation This requires a fast similarity measure that is able to compare workloads within seconds 3 WAVELET METHOD First we analyse the time-dependent attribute in the workload for periodic time and frequency characteristics 3.1 Wavelets Wavelets are sets of functions under recursive relationships that are used for the analysis of periodic time and frequency characteristics of a signal T he discrete wavelet transform DWT is a recursive algorithm to remodel the original signal into components at dierent scales of resolution while preserving the locality in time for the signal Thus the DWT provides a way to examine the periodic characteristics of a signal at a higher level of detail when compared to frequency-only analysis like Fourier transforms The wavelet itself is deìned via two functions the mother wavelet function  and its corresponding scaling function   Both functions can be dilated or translated  inìnitely if needed into daughter wavelets  The original signal can then be reconstructed from the scale and dilation coecients of  and  using the inverse discrete wavelet transform IDWT These coecients store all information about the periodic time and frequency characteristics of the original signal A formal description of wavelets and their usage is provided by Graps and Daubechies 11 3.2 Method Our method is based on an idea by Mohror and Karavanic who introduce a method that uses wavelets for direct similarity checking using distance metrics  H o w ever we generalise the idea under the assumption that comparing time and frequency coecients between the complementary wavelet components of the two signals provides a more realistic comparison than comparing their distance directly Additionally previous work has shown that direct distance or mean error methods 12 a re not s ucien t f or environments with large amounts of noise and burst behaviour 21 as is the c ase w ith t he scien t iìc w orkloads w e are investigating Therefore we use the multi-scale discrete wavelet transform in one dimension with the Haar wavelet as previous work in performance modelling has shown that the Haar wavelet is a good choice for many workload characterisation cases because it adapts well to sudden changes in the signal 9 We use the complementary coecients of each scale of resolution to compare the similarity of two signals The novelty of this approach is that it takes advantage of the fact that the set of all coecients from all components in combination with the mother wavelet oers a detailed description of the structure of the original signal In other words because the IDWT guarantees the correct reconstruction of the original signal it is possible to compare the coecients of the process that is used to re the signal rather than having to compare the signals themselves This enhancement increases the realism of the comparison because now we are comparing the parameters of one possible constructive process of the original signal at a high resolution This process would otherwise have to be approximated via stochastic methods but the IDWT serves as a exible proxy process for the exact reconstruction of the signal Figure 1 shows an example of the DWT on ve scales of resolution using the DQ2 workload from the evaluation in section 5 The top left graph shows the original signal derived as a time series signal from the submission time attribute in the workload Every row beneath shows the decomposition of the original signal into multiple components using one-dimensional Haar wavelets with an increasing number of scales The higher the scale the more information about local structure is extracted from the signal with the DWT These components thus represent the internal structure of the signal from multiple scaled viewpoints The DWT recursively remodels the innermost component of each structural scale i.e the rightmost graph per row into two smaller structural components The previously computed scaling components stay constant The top right graph shows the result of the IWDT where the original signal is reconstructed from the constituent components of scale 5 The reconstruction produces the same signal  and shows how each component stores essential information about the structure of the original signal We now derive our wavelet similarity measure m w based on these observations on wavelet components First we nd the maximum decomposition scales l 1 l 2 for the two original time series signals X 1 X 2  The result is the maximum number of usable recursions for the DWT  Let l 1   log  X 1     w   1  log\(2  l 2   log  X 2     w   1  log\(2  1 with w as the mother wavelet In the case of the Haar wavelet its length  w   2 Next we calculate the discrete wavelet transform results x 1 x 2 from X 1 X 2 to the selected scales l 1 l 2  The results x 1 x 2 are the sets of coecients of the daughter wavelets x 1 DWT X 1 w min l 1 l 2  x 2 DWT X 2 w min l 1 l 2  2 The intermediary result c w is the inverse root mean squared error from the coecients of each corresponding components 


              Original signal Reconstructed signal Inverse discrete wavelet transform Components per scale Discrete wavelet transform \(on 5 scales of resolution   Figure 1 Discrete wavelet transform of a signal through 5 scales of resolution and the reconstruction of the signal between the two transformed signals c w 1    n 1 i 1  x 1 i  x 2 i  2   n 1 3 Finally we normalise the intermediary result by the range of the DWT transformed signals m 1  max\(max x 1   max x 2  m 2  min\(min x 1   min x 2  m w  c w   m 1  m 2  4 The nal result m w of the method is then the normalised mean of all inverse root mean squared errors from each corresponding component between the two transformed signals The similarity is thus provided relative on each scale of resolution and does not need a threshold for evaluation 3.3 Evaluation The method is evaluated in 4 studies it should coherently identify similar and dissimilar behaviour of two signals it should be resistant against increasing amounts of noise in the signals the bias of random number generation must be assessed and the execution time needs to be examined 3.3.1 Stud y 1  Similarity and dissimilarity First we need to make a sanity check of the method We construct two types of signals one to identify similarity and onetoidentifydissimilarity Oncewehaveestablishedsuch signals where we have an understanding what their similarity and dissimilarity values should be we can assess how accurate the wavelet method is Additionally we also compare its accuracy to the ancestor method by Mohror and Karavanic  MK-wavelet  as well as commonly employed methods in the time-series similarity domain the normalised root mean squared error  moving average  exponential smoothing  Kalman lter and discrete Fourier transform  The following conìgurations for the methods are used in the evaluation For all methods the time dependent attributes are transformed into time series signals using cell count aggregation We use the number of cells as suggested by the Freedman-Diaconis rule  with a suggested cell size  2  IQR x  n  1  3  with IQR as the inter-quartile range and n thenumberofobservationsinthesample x  The rule ensures that the mean squared dierence between aggregated and original values is minimal and still preserves the characteristics of the original distribution For the ltering methods we use a 5 moving average lter and a 5 exponential smoothing lter For the discrete Fourier transform the resulting outputs are remodelled down from complex space into real numbers We create eight signals with the same behaviour but add 1 noise each to make the similarity search nontrivial The sample size is 8  8  2  8  2  28 possible comparisons for the data set each representing the time-dependent attribute of a workload Assume independent and dierent sinusoid signals f 1  8  x  L  a  sin  r  x  c  with amplitude a 1 sine rate r  1 and a free variable c  over a discrete linear space L  The standard error for this data set is estimated at   f 1    28  0  10 thus yielding a 90 conìdence for each result from a signal comparison We substitute the free vari 


able c with 1 random variates sampled from a continuous uniform distribution These two signals exhibit similar almost overlapping behaviour even though they are masked by uniform random noise For the dissimilarity evaluation we create a data set with 8 random signals We use repeated Bernoulli trials to produce one-dimensional random walk signals r 1  8  x  L  The standard error for this data set is estimated at   r 1     28  0  02 yielding a conìdence of 98 for each comparison result These random walks will expose a practical lower limit of the method We now evaluate the sinusoid data set and the random walk data set using all methods There are two basic objectives The rst one is that a given method should score as close as possible to 0.99 on the sinusoid data set because all signals come from the same sinusoid function and only dier by 1 noise This directly evaluates the ability of the method to nd similar behaviour The second objective is to score as low as possible with minimal variance on the random walk data set The reason for this is that the method must consistently nd dissimilar behaviour which is in direct relation to the variance of the random walk A low value alone is therefore not enough Figure 2 shows the results for both evaluations as two box-and-whisker plots The high similarity values of the discrete Fourier transform in both gures 2a and 2b are immediately striking and show that the Fourier method is unable to identify similarity and dissimilarity coherently The direct comparison using the normalised root mean squared error provides good results at almost 0.90 though variance is still at 1  4 th of the signalês cardinality for the dissimilarity evaluation The three ltering methods i.e moving average exponential smoothing and Kalman lter expectedly produce lower similarity values on the random walk data set because they remove too much local structure from the signal Our wavelet method produces a high score properly identifying the similar structures of the sinusoid data set Interestingly the MK-wavelet method behaves the same like the lter methods for similarity though with less variance We attribute this to the approximation property of the used Minkowski distance metric For dissimilarity the wavelets behave the same properly trying to remove the inîuence of randomness Our method does not score lower than the other methods on the random walk data set though However it does produce the most consistent results as shown by its small variance in gure 2b If one considers the inîuence of the random number generation we can see that this is actually the more important aspect 3.3.2 Stud y 2  The inîuence of randomness Figure 2b shows the diculty when working with random data sets Through the law of large numbers the results from a uniform random process can only converge towards the expected value i.e 0.5 in our case It is therefore highly unlikely to reach a similarity value lower than 0.5 The superposition of uniform random processes halves the expected value i.e to 0.25 in our case This convergence shows that even though 0 is the theoretical lower limit we can only approach it through superposition of many random processes For the evaluation in gure 2b it is thus sucient to assume 0.5 as the target for plausible dissimilarity One basic problem however remains i.e the random source can shift the expected value We use the Mersenne Twister random number generator to generate the Bernoulli trials  H o w ev er M ersenne Twister w as sho w n t o b e predictable after just 624 iterations even though it passes existing statistical tests on randomness  T herefore e v e n t he random walks can exhibit behaviour that can be found by similarity methods thus increasing the limit of dissimilarity In fact 0.5 as plausible dissimilarity might be unattainable because of underlying structures in the random signal We are thus evaluating the wavelet method as a test of randomness on 69 dierent random number generators from the dieharder suite  w here one o f t hem i s a random d ata generator with high entropy from the Linux dev/random device We generate 8 data sets with 2 16 uniform random numbers each and construct their random walks based on Bernoulli trials Again this yields 28 comparison per data set Figure 3 shows the result per random number generator sorted by median It is striking that those random number generators that approach 0.5 more closely also exhibit larger amounts of variance which eectively means that the random signal is not random at all Even with dev/random the only almost true random source in the set of generators it is dicult to provide a suciently random signal Also the Mersenne Twister only achieves a mean of 0.63 We have thus shown that even in a practical setting it is highly unlikely to approach the required value of 0.5 for plausible dissimilarity even with a true random source As a conclusion the higher the limit of plausible dissimilarity the more important smaller dierences in similarity values become 3.3.3 Stud y 3  Robustness against noise All methods scored high i.e more than 75 on the sinusoid data set but this is not enough to be applicable in an operational setting Especially scientiìc workloads usually exhibit large amounts of noise a nd previous w ork has shown the detrimental eects noise can have on a system  T herefore w e need to test the r obustness o f t he dierent methods under noisy conditions to make sure they work in such environments We evaluate the methods again using linearly increasing amounts of uniformly distributed random noise We add local noise in the range of 0   o the sinusoid data set as well as local noise in the range of 0  sup  data set    to the r andom w alk d ata s et A dding random noise to an already random data set might seem counter-intuitive but this only disperses the random walk locally and not its overall structure Figure 4a shows the results of this evaluation for the sinusoid data set The direct comparison using the normalised root mean squared error as well as Kalman lter show almost monotonic decreases in accuracy with increased local noise This behaviour is expected for the direct comparison However Kalman lter falls prey to its accurate reconstruction of the original noisy signal and therefore degenerates like the direct comparison Moving average and exponential smoothing decrease quickly but then stabilise at 0.78 regardless of noise as expected by a lter The Fourier transform scores high with a sharp drop at about 80 of noise Until that point the higher values are due to the law of large numbers because the periodicity spikes in the output function of the Fourier transform are sparse and therefore most of its values are similar at 0 With increasing amounts of noise though the output function of the Fourier transform produces more periodicity spikes and degenerates like an averaging lter due to the law of large numbers The output thus converges with the same results as the moving average and exponential smoothing lters Our wavelet method however is stable at 0.91 


               NRMSE Moving Average Exponential Smoothing Kalman Filter Fourier Transform Wavelet MK-Wavelet 0.0 0.2 0.4 0.6 0.8 1.0 normalised similarity value a Similarity All methods evaluated on all sinusoids               NRMSE Moving Average Exponential Smoothing Kalman Filter Fourier Transform Wavelet MK-Wavelet 0.0 0.2 0.4 0.6 0.8 1.0 normalised similarity value b Dissimilarity All methods evaluated on all random walks Figure 2 Method evaluation The similarity should be 0.99 but the dissimilarity as low as possible with minimal variance                                                                                                                                           Mersenne Twister MT19937 dev/random Figure 3 Using the wavelet method as a test of randomness Closer to 0.5 with less variance is better     0 20 40 60 80 100 noise 0.0 0.2 0.4 0.6 0.8 1.0 normalised similarity value NRMSE Moving Average  Exponential Smoothing  Kalman Filter  Fourier Transform  Wavelet MK-Wavelet a All methods evaluated on all sinusoids     0 20 40 60 80 100 noise 0.0 0.2 0.4 0.6 0.8 1.0 normalised similarity value b All methods evaluated on all random walks Figure 4 Stability evaluation of the methods under increasing amounts of noise The stability curve should stay constant regardless of the amount of noise This is due to the decomposition of the signal into its describing features where the noise is eectively stripped away until its inîuence on the scaling components is negligible 11 I n t erestingly  t he Minkowski distance seems to have the same inîuence as in study 1 and the MK-wavelet behaves as the lter methods Figure 4b shows the same approach on the random walk data set to evaluate if dissimilarity is found coherently under increasing amounts of noise The comparisons using normalised root mean squared error exponential smoothing and Kalman lter do not stabilise but instead degrade quickly They cannot cope with increasing amounts of noise 


and converge towards the expected value of 0.25 The output function of the Fourier transform stays at near zero for all data sets thus yielding a high similarity value again for the same properties as mentioned before Therefore the Fourier transform is unable to identify the random walks in the data sets correctly because it misinterprets the noise Only our wavelet method stays stable at 0.64 and is only mildly inîuenced by increasing amounts of local noise The result for the wavelet is almost equal to the one in gure 2b and also correlates with the Mersenne Twister result thus strengthening conìdence in its proper behaviour 3.3.4 Stud y 4  Execution time Finally another important issue is the large scale of the systems under study which requires a fast method Our baseline was that results should be attainable in under 5 seconds per comparison We experimented with other methods namely seasonal auto-regressive integrated moving average SARIMA models  E lman-st y le recurren t artiìcial neural networks  a nd longest c ommon s ubsequence  Unfortunately  S ARIMA mo del  tting a nd neural n etwork training were too time-consuming to pursue further Whereas the wavelet method evaluates 1 million events in under 2 seconds we were not able to t or learn sample sizes as small as 10000 events within several minutes using these methods The longest common subsequence method was worst with an execution time of more than 10 minutes for only 1000 events It was because of these timing results in early trials that we only focused on the methods described Note that we only used straightforward implementations for all the models and methods even though some higher performance variants exist in literature The full runtime evaluation is omitted due to lack of space but since all presented methods were considerably faster than our baseline a more detailed examination of runtime is not as interesting We will now move on to the second method that is used by our similarity measure 4 ASSOCIATION RULE METHOD In this section we explain association rules how we use them and evaluate their suitability to measure similarity 4.1 Association rules Agrawal et al d eìne asso ciation rule l earning as follows Let I   i 1 i n  be a set of n binary attributes called items Let D   t 1 t n  be a set of transactions called the database  Each transaction in D must be uniquely identiìable and contains a subset of the items in I An association rule is then deìned as an implication X  Y  X Y  I    X 012 Y    Such association rules therefore map the structural dependency between attributes X and dierent attributes Y  As a result the set of all association rules for a given database D corresponds to the internal structure of all transactions The most frequent and signiìcant association rules are selected via two indicators called the support and the conìdence  using the apriori algorithm The s upp ort i s t he minimum required percentage of existence for a given item set in the database For example if the item set  a b c  exists 5 times in a database of 8 item sets then its support is 5  8=0  625 The analyst speciìes the support at the beginning of the evaluation eectively selecting the minimum threshold for the inclusion of a rule In the second step the conìdence then states the probability threshold that multiple Y from dierent association rules have the same X  eectively selecting only item sets that are common in pairwise comparison Again the analyst speciìes the conìdence at the beginning of the evaluation The apriori algorithm then generates item sets breadth-ìrst where parts of the search tree are pruned via support and conìdence We chose to use the apriori algorithm with support and conìdence because it is the basic algorithm for generating frequent item sets There are also other association rule learning algorithms available for example the Eclat algorithm F P-gro w th algorithm 15 o r O PUS s earc h  E v e n t hough d ieren t a lgorithms m igh t yield d ieren t rules the choice of which rule learning algorithm to use is actually not important The method only works on rules and does not need to know how the rules were created so every analyst can choose a rule learning algorithm of their liking Via selection of dierent learning algorithms one can tailor the method to a speciìc use case but care has to be taken that comparative evaluations of multiple workloads always use the same learning algorithm 4.2 Method As a preprocessing step the apriori algorithm has to run on the descriptive attributes of the given workload events We suggest using support=0.01  i.e the apriori algorithm considers every rule that contributes to at least 1 of the original transaction database regardless of its size That should yield a granularity that is small enough for detailed studies while eliminating most of the one-time rules that occur at the lower values of support  0  01 We also suggest using conìdence=0.80  i.e the top 20 of common rules are considered the most important This threshold is suggested based on the Pareto principle i.e the 80Ö20 rule Once the appropriate rules have been generated the similarity measure m a can be calculated The workîow of the method is shown in algorithm 1 First the corresponding type of attribute has to be evaluated for each value in X and Y of the rule We call this attributes 1  attributes 2 collection a link  The reason to store the attributes only and not the value as well is that in dierent workloads from the same system the values can change e.g user U 12 in one workload sample corresponds to user U 37 in another workload sample However we are not interested in a particular value at this stage but the dependencies of their attributes By eliminating such possible value inconsistencies we focus only on the dependency link between the attributes These links between attributes are then sorted and stored in lists The second step uses the results hash table to count the occurrences of each link By using the sorted link as the key in the hash table every occurrence of dependency links can be accounted for This means that association rules attributes such as user 015 protocol and protocol 015 user are considered equal because both correspond to the same link  protocol user   The third step evaluates these counts of occurrences Evaluation starts with one workloadês association rules r 1  The maximum and minimum values for each link are evaluated against the corresponding link from the dependency list of the comparison rules r 2  The relative dierence in the number of occurrences is then stored e.g link  datatype project user   22 from r 1 and  datatype project user   12 from r 2 result in a re 


 Algorithm 1 Association rule similarity method Require association rules r 1  2 1 dependencies 1  2  list    2 for event  r 1  2 do 3 link 1  2  list    4 for value  event do 5 link 1  2   attributes 1  attributes 2   link 1  2   6 end for 7 dependencies 1  2  sort  link 1  2   dependencies 1  2 8 end for 9 result 1  2   10 for link 1  2  dependencies 1  2 do 11 if link 1  2  result 1  2 then 12 result 1  2  link 1  2  link 1  2 1 13 else 14 result 1  2  link 1  2  1 15 end if 16 end for 17 final  list    18 count  0 19 for link 1  results 1 do 20 if link 1  results 2 then 21 m 1  min results 1  link 1  results 2  link 1  22 m 1  max results 1  link 1  results 2  link 1  23 final  final  min m 1   max m 2  24 delete result 2  link 1 25 else 26 final  final  0 27 end if 28 count  count 1 29 end for 30 for link 1  sequence   results 1   count  do 31 final  final  0 32 end for 33 for link 2  results 2 do 34 final  final  0 35 end for lative dierence of 12  22  0  54 Thatspeciìclinkisthen removed from the dependency list of the comparison workload If the link is not found then a penalty of 0 is applied These 0s directly reduce the similarity value During this loop the number of evaluated links is counted The nal step penalises leftover rules that were not used in the previous step From the starting workload r 1 all leftover links are penalised by adding 0s to the result set as counted from the number of previously evaluated links Any leftover links from workload r 2 are also directly penalised by adding 0s to the result set The method nishes with a list of relative dierences and penalties The algorithm returns with the similarity measure m a  r 1 r 2   mean  final ycalculating the arithmetic mean over all elements in the multi-set 4.3 Evaluation We evaluate the association rule method with two synthetic data sets each having 5 descriptive attributes 4.3.1 Methodology Again each data set is split into 8 distinct segments that are compared to each other yielding a sample size of 28 comparisons For one data set the workloads are created with round-robin selection of 1024 distinct values for each attribute per segment repeated 2 16 times This creation guarantees that dependencies stay consistent and should result in m a  1 by the association rule method The other data set selects uniformly distributed random values between 0 and 1024 for each attribute repeated 2 16 times Random data provides the practical lower limit of the method Again the evaluation of the method is subject to random number generation as mentioned in section 3 4.3.2 Results As expected the association rules found in the roundrobin workload scored a similarity value of m a 1 The method thus is able to nd consistent repeatable behaviour perfectly On the random uniform workload though the method scored a similarity value of m a 0 Avalueof0 was slightly unexpected because we assumed uniform random data to produce at least some usable rules However the apriori algorithm did not consider any of the random attribute dependencies as frequent enough Once we reduced the number of distinct values from 1024 to only 16 the apriori algorithm was able to nd 3 to 5 rules This reduction resulted in a low similarity value of m a 0  13 However note that xed values for support and conìdence are responsible for this result As we see later this problem does not exist in large-scale workloads as the possible permutations of attributes and events produce a much larger sample space As we are not aware of any other method that automatically quantiìes the dependencies of association rules we have no ground truth or competitors against which we can compare our methodês eectiveness With a practical result of 0  13  m a  1 we are quite conìdent that the method is applicable to many workloads because it covers the possible range of similarity values to a high degree Nevertheless while the association rule measure on its own is not a good choice for a similarity measure it does play the key role in improving the results attainable by our wavelet method 5 COMPOUND MEASURE We combine the wavelet measure and the association rule measure into a compound measure and evaluate its accuracy on operational workloads and with a hypothesis test 5.1 Combining the two methods The combination is based on the concept of transfer functions  where outputs of one measure are weighted according to the outputs of another measure This was shown to be superior to simple linear combinations of measures  I n our case we deìne our transfer function to weight the coefìcients of the wavelet transform before the intermediary result is calculated The idea is that coecients that involve events with attributes from the association rule measure should be weighted according to the inîuence of the attribute Finding these events is trivial because the length of the wavelet is given and the involved attributes were already found via the association rule measure A simple match on event and attribute can thus trigger the weighting and use the weighting value of an attribute f attribute  final  m a  t 1 DWT X 1 w min l 1 l 2   f attribute t 2 DWT X 2 w min l 1 l 2   f attribute 5 These t 1 t 2 arethenusedinsteadof x 1 x 2 to nish the wavelet method calculation However we call this result then our compound similarity measure 0 m 1 


 Data set Events Attr Intvl Intvl AuverGrid 404176 7 12 months DAS-2 1124772 7 22 months Gridê5000 1020195 7 31 months LCG 188041 4 11 days NorduGrid 781370 5 38 months SHARCNET 1195242 4 13 months DQ2 965519 7 24 hours Table 1 Overview of the GWA and DQ2 workloads 5.2 Methodology We evaluate the accuracies of all methods against two scheduling metrics mean waiting time and mean execution time  The premise of this evaluation is as follows if input workloads are similar then the performance metrics that evaluate the output of the system that is executing the workloads should be similar as well An elegant way of doing that is to use scheduling metrics b ecause they can b e principal targets for performance optimisation in a system Two workloads that produce the same waiting times or the same execution times can be considered similar in performance evaluation Many more dierent scheduling metrics can be used if necessary e.g mean resource utilisation or link throughput but for this evaluation we narrow ourselves to mean waiting time  W andmeanexecutiontime  E  The objective is to minimise the error between a similarity measure and the output metrics For example two workloads that dier by 0.3 in their relative waiting times should also yield a similarity value of 0.3 The normalised errors between two workloads w 1 w 2 are thus error  W   similarity w 1 w 2      W w 1   W w 2    error  E   similarity w 1 w 2      E w 1   E w 2    6 We use all the available workloads provided by the Grid Workloads Archive GWA  as w e ll as a w orkload from DQ2  T he GW A w orkloads a re pro vided i n a common trace format for the following large-scale distributed systems AuverGrid Distributed ASCI Supercomputer 2 DAS2 Gridê5000 LHC Computing Grid LCG NorduGrid and SHARCNET These workloads all stem from scientiìc computational traces DQ2 workload stems from scientiìc distributed data management traces N ote t hat D Q2 uses LCG resources but its traces are sampled independently and separately from the LCG traces in the GWA Table 1 gives an overview of the contents of the data sets that store the workload It shows the number of events the number of attributes and the segments per data set DQ2 generates as many events in one third of a single day as the other systems in multiple years It is because of this imbalance that we decided to keep the number of events roughly equivalent and not the actual time spent in the workload Note that the actual number of events is not relevant because the association rule method will only consider the most important constituent events and attributes Additionally the original workload traces contained many dierent attributes but we are only using attributes with a cardinality greater than one i.e multiple dierent values of the attribute are used throughout the workload The inclusion of attributes that stay constant during the workload would yield no additional information to the attribute analysis i.e trivial association rules and such information could be modelled statically anyway The apriori algorithm thus takes care of selecting proper attributes and events and the analyst does not need to select sets of attributes manually Lastly we only use attributes that are part of the workload deìnition e.g submission time or user identiìcation  We are not using output attributes like completion time because they are not part of the workload they are the result of the workload being executed by the system We split each data set into distinct segments that we compare against each other thus yielding sample sizes between 55 and 703 comparisons depending on the number of intervals in each workload In total there are 1874 possible permutations Each workload segment is then separated into two distinct parts a time series signal for the submission time attribute and a list of attribute tuples from descriptive attributes The time series signal is created by aggregating the count of submission times in the workload into cells appropriate for the workload We chose hourly daily and monthly cells as indicated in table 1 with the corresponding number of segments We obtain all possible error  W and error  E by calculating all similarity measures as well as  W  E for all segments in the workloads from the GWA that have this information This includes AuverGrid DAS-2 Gridê5000 SHARCNET In DQ2 workload the  W  E are staging and transfer times respectively and are calculated in exactly the same way 5.3 Results Figure 5 shows the error results over all evaluated workloads Figure 5a focuses on the errors of all methods against  W  There the compound measure is more accurate than all competing methods The interesting observation is that both constituent methods have a higher error but their combination as a compound measure reduces error drastically Especially the association rule measure cannot be used on its own Against the best case i.e the moving average the improvement in accuracy is 12.2 Against the worst case i.e the discrete Fourier transform the improvement is 24.5 This shows how the inclusion of a method for descriptive attributes is highly relevant in addition to time-dependent attributes and should not be ignored The wavelet method however is responsible for reducing standard deviation between 4.6 and 10.4 showing the advantage of a multi-resolution approximation free approach Figure 5b focuses on the errors of all methods against  E  Here the improvement in accuracy for the compound method is only 3.8 against the best case i.e again the moving average and 20.8 against the worst case i.e the Kalman lter Also the improvement in standard deviation is slightly less with 0.3 to 7.6 It is interesting though that the ltering methods also have a slight advantage in this case We have the following explanation for these results The waiting time is largely dependent on the number of already existing events in a system Therefore the speciìcs of a new event are important for its proper scheduling e.g which resource to use These speciìcs are stored in the descriptive attributes and can be found by the association rule method Like a scheduler uses this additional information to make an informed choice where and when to execute the event so does the association rule method evaluate the importance of the attributes This is less obvious for the execution time 


                    NRMSE Moving Average Exp Smoothing Kalman Filter Fourier Transform Wavelet MK-Wavelet Association Rules C ompound Measure 0.0 0.2 0.4 0.6 0.8 1.0 normalised error a Similarity error against Mean Waiting Time                    NRMSE Moving Average Exp Smoothing Kalman Filter Fourier Transform Wavelet MK-Wavelet Association Rules Compound Measure 0.0 0.2 0.4 0.6 0.8 1.0 normalised error b Similarity error against Mean Execution Time  Figure 5 Normalised errors of all methods against mean waiting time and mean execution time Lower is better Method  p value  p   0  50  25 0  125 NRMSE 0.38 510 224 3 Mov Avg 0.22 1120 504 22 Exp Smooth 0.36 488 252 1 Kalman 0.30 693 360 12 Fourier 0.36 589 252 7 Wavelet 0.21 1412 522 65 MK-Wavelet 0.22 1344 504 68 Ass Rules 0.25 640 450 5 Compound 0.20 1510 558 182 Table 2 Two-sample Anderson-Darling test per method for all 1874 pairwise permutations of segments Results are the number of times the null hypothesis could not be rejected where scheduling already took place and the descriptive attributes are less useful However the timing of the event is more important which is properly modelled through the wavelet methods Consequently the wavelet methods have less error and less variance than all other methods in this case A wavelet-based method biased by a transfer function is therefore a good choice in both the waiting and execution time cases In the case of our scientiìc workloads the association rule measure only has a signiìcant impact when the descriptive attributes inîuence the system We can conìrm this for DQ2 workload from our own operational experience 5.4 Hypothesis test We complement the accuracy evaluation with a statistical hypothesis test The two-sample Anderson-Darling test can validate if the empirical distribution functions of two independent samples follow from the same distribution It is a non-parametric k-sample test that does not make assumptions about the distribution function and is therefore an ideal candidate for cases without ground truth Our null hypothesis is that both samples come from the same distribution i.e the similarity error is minimal We prepare all 1874 workload segments by normalising their time-dependent attribute to a relative start of zero Additionally we replace all values in the descriptive attributes into unique numerical representations for easy calculation of an attributeês empirical distribution Then we evaluate all segments with each available method and present the results in table 2 For each method we give the mean P-value of the test and the number of times we could not reject the null hypothesis for a given statistical signiìcance   We want this number to be as close as possible to the maximum of 1874 The higher the number the more often the particular method identiìed two segments as similar which corresponds to a lower error for the similarity measure The results conìrm our previous performance metrics evaluation At a signiìcance of  0  5 the results show four good candidates our compound measure the two wavelet methods and the moving average These results are also true for error  W and error  E in the metrics evaluation The same observation still holds at  0  25 but only at  0  125 we can see the clear advantage of the compound measure Whereas all other methods mostly reject the null hypothesis the compound measure still provides an acceptable result at about 11 with 182 non-rejects Again as in the metrics evaluation the association rule method by itself is seemingly useless but in combination with the wavelet method provides a signiìcant boost to error reduction 6 CONCLUSIONS The performance evaluation of a system strongly depends on the input workload If fundamentally dierent workloads are used for evaluations then conclusions drawn from the results are likely to be non-representative Therefore we propose a compound measure to quantify the similarity between two workloads This compound measure comprises two independent methods the rst one to analyse the timedependent attribute in the workload and the second one to analyse the descriptive attributes in the workload The rst method uses the discrete wavelet transform to derive and compare components that describe the periodic time and frequency behaviour of the time-dependent attribute The novel idea of this approach is that we take advantage of the property of the inverse discrete wavelet transform that guarantees that the original signal can be reconstructed from the scaled coecients This idea improves upon exist 


ing work because the approach is free from any assumptions on the structure of the attribute and does not have to rely on statistical approximations We evaluate the approach using two synthetic data sets to establish the upper and lower bounds of the covered similarity space We nd that we can cover the whole similarity space and that we are only constrained by the random number generation To validate the method against this constraint we investigate the speciìc inîuence of the random number generation by using the method as a test of randomness Additionally operational systems usually exhibit large amounts of noise in their workload therefore we validate the method against noise as well Our ndings show that it is highly resistant to noise whereas other commonly employed methods yield to the law of large numbers or fail completely Additionally the method can consistently identify dissimilar behaviour as well even though it is not as good at that task which is only partially true for other commonly employed methods The second method uses association rule analysis to identify and quantify the relationship between descriptive attributes of the workload The novel idea of this approach is to eliminate the use of values and instead focus on the attributes themselves and therefore rank the relative usage of the attribute That way the most important building blocks of the workload can be compared directly The algorithm is described and then evaluated using synthetic data sets to establish the upper and lower bounds of the covered similarity space This time we nd that we are only constrained by the amount of learnt rules which follows from the used rule learning algorithm At least 16 rules need to be available to cover the whole similarity space when the apriori algorithm is used The method itself however does not require a speciìc rule learning algorithm We then present our compound measure that can address problematic workload characteristics We use the transfer function concept to weight speciìc events in the waveletmethod based on relative attribute dependencies Then we conduct an empirical study to evaluate all methods on operational workload from seven large-scale distributed systems Two important characteristics stand out First the time and frequency behaviour is surprisingly well-modelled with a wavelet method Second the association rule by itself is seemingly useless However the inclusion of descriptive attributes improves accuracy when determining similarity of workloads We show that the compound measure improves upon existing work by evaluating it against two important scheduling metrics mean waiting time and mean execution time The analysis of descriptive attributes for the similarity in addition to the analysis of the time-dependent attribute yields a compound similarity measure that can improve accuracy by 24.5 At the same time the standard deviation can be reduced by 10.4 We conìrm these results with an independent statistical hypothesis test a twosample Anderson-Darling statistic and show the advantage of the compound measure even at higher signiìcance levels of  0  125 with only 11 dierence Our results strongly reassure our initial expectations that the compound measure is a good choice for large-scale and data-intensive systems that have to deal with enormous amounts of events and previously unknown dependencies in their workload Furthermore even though we only used scientiìc workloads in our evaluations we did not observe any constraint that would limit the use to scientiìc workloads 7 FUTURE WORK As a result of this work we can continue to evolve our simulation eort for DQ2 We will use the similarity measure to evaluate the results of our future workload models to speciìcally address problems like data transfer cycles distributed le caching or popularity-based deletion However the measure can still be improved If it is suspected that dependencies change over time then the rate of change needs to be investigated Extending the association rule measure with time-evolving graphs may prove to be appropriate for such cases Additionally we only used the Haar wavelet as suggested by previous work We are interested in investigating the inîuence of dierent types of wavelets on the analysis of non-periodic burst behaviour An implementation of the similarity measure including already pre-processed versions of the used workloads from the evaluation can be downloaded from our website  8 ACKNOWLEDGEMENTS We are grateful to the following teams for providing traces via the Grid Workloads Archive The AuverGrid team the DAS-2 team the Gridê5000 and OAR teams the HEP eScience Group at Imperial College London for the LCG traces the NorduGrid team and J Morton and C Chrush for the SHARCNET traces 9 REFERENCES 1 R A g r a w a l T I m i e l i  nski and A Swami Mining association rules between sets of items in large databases ACM SIGMOD Record  22\(2 June 1993  L  B ergroth H Hak o nen and T  R aita A surv ey of longest common subsequence algorithms In 7th International Symposium on String Processing Information Retrieval  page 39 A Coru na ES Sept 2000 IEEE Computer Society  C  Borgelt and R  K ruse Induction of asso ciation rules apriori implementation In 14th Conference on Computational Statistics  pages 395Ö400 Berlin DE 2002 Physica  M  B ranco Distributed data management for large scale applications  PhD thesis School of Electronics and Computer Science University of Southampton UK Nov 2009  M  B ranco E Zalusk a D de Roure M  L assnig a nd V Garonne Managing very large distributed data sets on a data grid Concurrency and Computation Practice and Experience  22\(11 Aug 2010  R  G  B ro wn D  E ddelbuettel a nd D Bauer Dieharder A random number test suite http://www.phy.duke.edu rgb/General/dieharder.php Oct 2009  L  C  C arrington M Laurenzano A Sna v e ly  R  L  Campbell Jr and L P Davis How well can simple metrics represent the performance of hpc applications In ACM/IEEE International Conference for High Performance Computing Networking Storage and Analysis  Seattle WA USA 2005 ACM  G  C asale N Mi a nd E Smirni C WS a mo del-driv e n scheduling policy for correlated workloads In ACM SIGMETRICS International conference on 


measurement and modeling of computer systems  pages 251Ö262 New York NY USA June 2010 ACM  K  P  C han a nd A W.-C F u Ecien t t ime s eries matching by wavelets In 15th International Conference on Data Engineering  pages 126Ö133 IEEE 1999  C Chatìeld The analysis of time series an introduction  CRC Press 1997  I Daub ec hies Ten lectures on wavelets  Society for Industrial and Applied Mathematics 1992  P  A Dinda a nd D R OêHallaron H ost l oad prediction using linear models Cluster Computing  3\(4 2000  D F r eedman and P  D iaconis On the h istogram as a density estimator L2 theory Probability Theory and Related Fields  57\(4 1981  A Graps An in tro duction to w a v e lets IEEE Computational Science  Engineering  2\(2 1995  J Han J P e i Y Yin and R  M ao Mining frequen t patterns without candidate generation A frequent-pattern tree approach Data Mining and Knowledge Discovery  8\(1 2004  T Ho eîer T  S c hneider a nd A Lumsdaine Characterizing the inîuence of system noise on large-scale applications by simulation In ACM/IEEE International Conference for High Performance Computing Networking Storage and Analysis New Orleans LA USA 2010 IEEE Computer Society  A Iosup H Li M  J an S  A no ep C  Dumitrescu L Wolters and D H J Epema The grid workloads archive Future Generation Computer Systems  24\(7 July 2008  A Kramp e  J  L epping a nd W Sieb en A h y brid markov chain model for workload on parallel computers In ACM International Symposium on High Performance Distributed Computing  pages 589Ö596 Chicago IL USA June 2010 ACM  M Lassnig C ERN P H-ADP DDMLAB public website http://cern.ch/ddmlab-public April 2011  M Lassnig T  F ahringer V  G aronne A  M olfetas and M Branco Stream monitoring in large-scale distributed concealed environments In 5th IEEE International Conference on e-Science  pages 156Ö163 Oxford UK Dec 2009 IEEE Computer Society  M Lassnig T  F ahringer V  G aronne A  M olfetas and M Branco Identiìcation modelling and prediction of non-periodic bursts in workloads In 10th IEEE/ACM International Symposium on Cluster Cloud and Grid Computing  pages 485Ö494 Melbourne AU May 2010 IEEE Computer Society  H Li W orkload dynamics on clusters and g rids The Journal of Supercomputing  47\(1 2009  H Li and M  Muskulus Analysis and m o d eling o f j ob arrivals in a production grid ACM SIGMETRICS Performance Evaluation Review  34\(4 Mar 2007  U Lublin and D  G  F eitelson T he w o rkload on parallel supercomputers modeling the characteristics of rigid jobs Journal of Parallel and Distributed Computing  63\(11 Nov 2003  D P  Mandic a nd J A Cham b e rs Recurrent Neural Networks for Prediction Learning Algorithms Architectures and Stability  Wiley 2001  M Matsumoto a nd T Nishim ura M ersenne Twister a 623-dimensionally equidistributed uniform pseudo-random number generator ACM Transactions on Modeling and Computer Simulation  8\(1 Jan 1998  T N Minh L W o lters and D  E p e ma A realistic integrated model of parallel system workloads In 10th IEEE/ACM International Conference on Cluster Cloud and Grid Computing  pages 464Ö473 Melbourne AU May 2010 IEEE Computer Society  J C Mogul Emergen t mis eha v ior v s complex software systems ACM SIGOPS Operating Systems Review  40\(4 Oct 2006  K Mohror a nd K L Kara v a nic Ev aluating similarity-based trace reduction techniques for scalable performance analysis In 22nd Annual International Conference on Supercomputing  page 55 ACM 2009  Q P a n L Zhang G Dai and H  Z hang T w o denoising methods by wavelet transform IEEE Transactions on Signal Processing  47\(12 Dec 1999  P  Ratn F  Mueller B  R  d e Supinski a nd M Sc h u lz Preserving time in large-scale communication traces In 22nd Annual International Conference on Supercomputing  pages 46Ö55 ACM 2008  F W Sc holz and M  A  S tephens K sample anderson-darling tests Journal of the American Statistical Association  82\(399 1978  Z R Struzik and A  S ieb e s Principles of Data Mining and Knowledge Discovery  volume 1704 of Lecture Notes in Computer Science  chapter The Haar Wavelet Transform in the Time Series Similarity Paradigm pages 12Ö22 Springer 1999  E Theresk a and G  R  G anger IR ONMo del Robust performance models in the wild In ACM SIGMETRICS International conference on measurement and modeling of computer systems  pages 253Ö264 Annapolis MD USA June 2008 ACM  F W a silewski P yW a v elets Discrete W a v e let Transform in Python http://www.pybytes.com/pywavelets May 2010  G I W e bb OPUS A n e cien t admissible algorithm for unordered search Journal of Artiìcial Intelligence Research  3:431Ö465 1995  M J Zaki S calable a lgorithms f or asso ciation m ining IEEE Transactions on Knowledge and Data Engineering  12\(3 May 2000 


association rules and decision trees on analysis of diabetes data from the DiabCare program in France stud health technol inform 2002;90:557-61 6] J.Mondelle Simeon and Rober, Hilderman Exploratory Quantitative Contrast Set Mining:A Discretization Approach, 19th IEEE International Conference on Tools with Artificial Intelligence - Vol.2 ICTAI 2007 7] D.Newman, J. S.Hettich, C.L.S. Blake, and C.J. Merz, UCI Repository of machine learning databases,Irvine, CA: University of California, Department of Information and Computer Science.1998 last accessed: 1/10/2009 8] J.Han, and M.Kamber, Data mining: Concepts and techniques, San Francisco: Morgan Kaufmann Publisher, pp.47- 94, 2006 9] Glenn J. Myatt  Making sense of data: A Practical Guide to Exploratory Data Analysisand   Data Mining:Wiley\(2007 10] G. Chen, AND T.Astebro,  How to deal with missing categorical data: Test of a simple Bayesian method, Organ. Res. Methods 6, 3 2003 11] R.Agrawal, T. Imielinski, & A. Swami, Database mining aperformance perspective, IEEE Transactions on Knowledge and Data Engineering, 5\(6 1993 Special issue on Learning and Discovery in Knowledge-Based Databases 12] R.Agrawal, T.Imielinski, & A.Swami,Mining association rules between sets of items in large databases, In Proc. ACM-SIGMOD int. conf. management of data \(SIGMOD93 USA \(pp. 207216 13] Ian H.Witten and Elbe Frank, Datamining Practical Machine Learning Tools and Techniques, Second Edition, Morgan Kaufmann, .San Fransisco, 2005 14] S.Brin, R. Motwani, J.D. Ullman,  & S.Tsur, Dynamic itemset counting andimplication rules for market basket data, Proceedings of the ACM SIGMODInternational Conference on Management of Data pp. 255-264, Tucson, AZ, May 1997,ACM Press 15] M.J.Zaki, S. Parthasarathy, M. Ogihara, & W.Li, W. New algorithms for fast discovery of association rules, Proceedings of the 3rd International Conference on KnowledgeDiscovery and Data Mining \(KDD 1997,AAAI Press 16] B.Liu, W. Hsu, & Y.Ma, Pruning and summarizing the discovered association, Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 125-134 San Diego, CA, August 1999 


17] J.Han, J.Pei, & Y.Yin, Y,  Mining frequent patterns without candidate generation, Proceedings of the ACM SIGMOD International Conference on Management of Data,  Dallas, TX, May 2000 18] Y.Li, & L.Sweeney, Adding semantics and rigor to association rule learning: the GenTree approach, Technical Report CMU ISRI 05-101 2005 19] M.Rangsipan, Structure-Based Rule Selection Framework for Association Rule Mining of Traffic Accident Data, CIS 2006: 231239                                 


           334 


21] S. Baker and S.K. Nayar, A Theory of Catadioptric Image Formation, IEEE International Conference on Computer Vision \(ICCV pp.35-42, Jan, 1998 22] S.K. Nayar, Catadioptric Omnidirectional Cameras, IEEE Conference on Computer Vision and Pattern Recognition \(CVPR 488, Jun, 1997 23] A.Victorino, La commande referencee capteur: une approche robuste au proble`me de navigation, localisation et cartographie simultanees pour un robot dinterieur. PhD thesis, LUniversite de Nice-Sophia Antipolis, Inria Sophia Antipolis, 2002 3524 


ec  d Fig. 5: Computation Performance Comparison Tab. 4: Computation Savings by TOP-MATA K Connect K Retail K Wap La12 50 58.35% 100 0.01% 200 0.83% 23.04 150 55.91% 400 2.65% 400 30.12% 45.38 250 53.61% 700 1.84% 800 20.03% 25.95 350 48.28% 1100 3.95% 1600 13.06% 27.89 450 43.12% 1400 1.48% 3200 6.14% 12.70 550 39.36% 1700 4.00% 6400 5.63% 7.11 Second, Fig. 5 shows the results of four data sets computed by TOP-MATA and TOP-DATA, respectively. As can be seen, in general, TOP-MATA shows a better performance than TOP-DATA. And as the increase of the ? value, the advantage tends to be even more impressive for these four data sets 4.3. The Computation Saving of TOP-MATA As can be seen in the Tab. 4, four data sets, enjoy signi?cant computation savings brought by TOP-MATA. We can conclude that the computation saving is a major factor for the performance of TOP-MATA. That is, compared with TOP-DATA, a higher computation saving implies a much better performance of TOP-MATA. Since this saving is more signi?cant as the increase of the items, TOP-MATA works better for large scale data sets with a large number of items 5. Conclusion In this paper, we studied the problem of searching for top? item pairs with the highest cosine values among all item pairs. Speci?cally, we provided a novel algorithm TOPMATA which employ a Max-First traversal strategy for ef?ciently performing top-? cosine similarity search. Extensive experimental results veri?ed the effectiveness of the algorithms, And TOP-MATA algorithm is superior to TOPDATA for large-scale data sets with multiple items Acknowledgment This research was partially supported by the National Natural Science Foundation of China \(NSFC No. 70901002 and the Ph.D. Programs Foundation of Ministry of Education of China \(No. 20091102120014 


REFERENCES 1] R. Agrawal, T. Imielinski, and A. Swami, Mining association rules between sets of items in large databases, in SIGMOD 1993 2] C. Alexander, Market Models: A Guide to Financial Data Analysis. John Wiley & Sons, 2001 3] W. Kuo, T.-K. Jensen, A. Butte, L. Ohno-Machado and I. Kohane, Analysis of matched mrna measurements from two different microarray technologies Bioinformatics, vol. 18, p. 405C412, 2002 4] H. Xiong, X. He, C. Ding, Y. Zhang, V. Kumar, and S. Holbrook, Identi?cation of functional modules in protein complexes via hyperclique pattern discovery in PSB, 2005 5] J. Han, H. Cheng, D. Xin, and X. Yan, Frequent pattern mining: Current status and future directions DMKD, vol. 15, no. 1, pp. 5586, 2007 6] P.-N. Tan, M. Steinbach, and V. Kumar, Introduction to Data Mining. Addison-Wesley, 2005 7] S. Brin, R. Motwani, and C. Silverstein, Beyond market basket: generalizing association rules to correlations, in SIGMOD 1997, Tucson, AZ, 1997, pp 265276 8] E. Omiecinski, Alternative interestmeasures formining associations, TKDE, vol. 15, pp. 5769, 2003 9] H. Xiong, S. Shekhar, P.-N. Tan, and V. Kumar Exploiting a support-based upper bound of pearsons correlation coef?cient for ef?ciently identifying strongly correlated pairs, in KDD 2004, 2004, pp 334343 10] I. Ilyas, V. Markl, P. Haas, P. Brown, and A. Aboulnaga, Cords: Automatic discovery of correlations and soft functional dependencies, in SIGMOD 2004 2004, pp. 647658 11] J. Zhang and J. Feigenbaum, Finding highly correlated pairs ef?ciently with powerful pruning, in CIKM 2006, 2006, pp. 152161 12] H. Xiong, W. Zhou, M. Brodie, and S. Ma, Top-k correlation computation, JOC, vol. 20, no. 4, pp 539552, 2008 13] S. Zhu, J. Wu, and G. Xia, Top-k cosine similarity interesting pairs search, in 


http://datamining.buaa.edu.cn/TopKCos.pdf 14] M. Zaki, Scalable algorithms for association mining, TKDE, vol. 12, pp. 372390, 2000 


enhance item-based collaborative filtering, in 2nd IASTED International Conference on Information and Knowledge Sharing, Scottsdale, Arizona, 2003 476 2010 10th International Conference on Intelligent Systems Design and Applications 


Basi Association Rles Basi c  Association R u les Association is basically connecting or tying up occurrences of Association is basically connecting or tying up occurrences of events Ol dib t f ilt t O n l y d escr ib e se t s o f s i mu lt aneous even t s Cannot describe patterns that iterate over time e g  itemset a  0  b  0  g    Eg If you sense higher data rates on the downlink than normal AND New Route generated Implies high chances of Intrusion AND New Route generated Implies high chances of Intrusion Associative IDS for NextGen Frameworks Dr S Dua LA Tech 20 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


