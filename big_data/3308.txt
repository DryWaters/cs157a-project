Classification Association Rules Mining for Zhongjing Prescriptions Yang Xuemei Lin Duanyi Zhou Changêen Lai Xinmei  1. Information Management Institute, Academy of Integrative Medicine Fujian College of Traditional Chinese Medicine, Fuzhou, 350003, China 2. Department of Computer Science and Engineering Southeast University, Nanjing, 210096, China yxm-2000@tom.com   yxm_wj@sina.com Abstract Zhongjing prescriptions have experienced approxim ately two thousand years of Clinical verification. The prescriptions are commended by the later generation as classi cal prescriptions because of strict compatibility and remarkable treatment result So Zhongjing prescriptions are sel ected as data set in this paper. A classification association rules mining algorithm based on information gain feature selection CARM-IG is achieved, which is used to mine compatibility principles of Zhang Zhongjing for the treatment of cold zheng, heat zheng, deficiency zheng and excess zheng etc. Experiments show that the run time and memory cost of algorithm is reduced, at the same time, the concision and precision of classification compatibility principles is guaranteed because of the deletion of irrelevant Chinese medicines with classification 1. Introduction Zhongjing prescriptions, whi ch are created by Zhang Zhongjing, have experi enced approximately two thousand years of Clini cal verification. The prescriptions are commended by the later generation as classical prescriptions because of strict compatibility and remarkable treatment result. Over the years Zhongjing prescription research is more confined to personal summary about compatibility principle clinical research, experimental study, classification etc Zhongjing prescriptions have not been collected completely to establish a thematic database, and mathematical statistical and data mining methods have not been used to su m up compatibility principle objectively for it so far Then, 268 prescriptions from çShang Han Luné and Jin Ku i Yao Lueé which are written by Zhang Zhongjing are selected from th e prescription database of Traditional Chinese Medicine \(TCM\ation system through excluding duplicate prescriptions and deleting prescriptions without medicine in order to establish data set of Zhongjing prescription in this research firstly. Then Zhongjing prescriptions are classified by experts in the TCM field from two angles of cold-heat and deficiency-excess in eight principles Finally, a classification association rules mining algorithm based on information gain feature selection CARM-IG is applied to mining latent compatibility principle of Zhang Zhongjing, so that the compatibility principles of Zhang Zhongjing for the treatment of cold zheng, heat zheng, deficiency zheng and excess zheng etc. can be summed up objectively 2. Related works Currently, association rules mining 1 cluster 2  correspondence analysis 3 etc. are data mining methods commonly used to discover prescription compatibility principle. Fu rthermore, several improved algorithms based on association rules mining have emerged 4 A prescriptions data mining system integrated data preprocessing, high frequent item sets and association rules mining, cl uster, classification etc is developed in project DartGrid  New research way is explored because of the application of methods above for prescriptions compatibility principle mining which is becoming one of the most important research directions for the interd isciplinary research of TCM in union with computer technology gradually Algorithm CMAR \(Classification based on Multiple Associat ion Rules\ethod is proposed in 2001 7 The objective of algorithm CMAR Proceedings of 2008 IEEE International Symposium on IT in Medicine and Education 768 978-1-4244-2511-2/08/$25.00 ©2008 IEEE 


is to solve the problem of classification through mining classification association rules. But it is assumed that decision attributes \(or category attribute\are closely related any conditional attri butes \(the attributes except decision attributes algorithm CMAR. Higher memory cost and running time will be spend when super high dimension data set such as prescriptions data sets of TCM with hundreds dimensions is processed. The assumption above causes that irrelative attributes exist in mined classification association rules So the index of IF*ICF \(t he inverse class frequency proposed in paper attributes with higher frequent but lower contribution for classification from VCFP-Tree in order to improved the performance of algorithm. The index of IF*ICF usually is useless When the classification number is lower so that a Chinese medicine has appeared in each type of prescription. The method of feature selection based on information gain is introduced in algorithm CMAR. A classification association rules mining algorithm CARM-IG for TCM prescription high di mension data set is established, which can reduce the time and space cost and mine classification associa tion rules with higher quality through the deletion of irrelevant Chinese medicines with classification. Finally, the compatibility principles of Zhongjing prescriptions are summed up from different angles of classification 3. Feature selection based on information gain The basic idea behind feature selection is to compute som e measure that is used to quantify the relevance between an attribute and a given class label Information gain is one of effective measures for feature selection. Such a measure is referred to as an attribute selection measure or a measure of the goodness of split. Information gain is defined as the difference of the expect information needed to classify a give sample 1    m I ss and the expect information needed to classify a give sample based on conditional attributes Specific defin ition is given by  EC 1        m I GC I s s EC 1 Let S be a set consisting of s data samples. Suppose the class label attribute has m distinct values defining m distinct classes D i for i 1 m  s i be the number of samples of S in class D i The expect information needed to classify a give sample is given by 12 1    log   m mi i Is s p p Where p i is the probability that an arbitrary sample belongs to class D i and is estimated by s i  s  Let attribute C have v distinct values 1  v cc  Attribute C can be used to partition S into v subsets 1  v SS where S j contains those samples in S that have value c j of C Let s ij be the number of sample of class D i in a subset S j The entropy, or expected information based on the partitioning into subsets by C is given by  1 1 1       v jmj j mj i ss EC Is s s 3 Where  1  jmj s s s acts as the weight of the j th subset, and is the number of samples in the subset divided by the total number of samples in S  1    j mj I ss is defined as 2 1 log   m ij ij i p p and  ij ij j s p S is the probability that a sample in S j belongs to class D i  Anyway, the smaller the entropy value the greater the information gain value  EC  I GC the greater the purity of the subset partitions, and the higher the correlation between conditional attributes and decision attributes. The lowest threshold of information gain is usually been set, and the attributes whose information gain value is higher than the threshold will be chosen as the distinguish attributes for classification. The threshold of information gain is set to 0.1 in this research refer book [11 4. Classification association rules mining based on information gain feature selection 4.1. Variant of classification frequent pattern tree A variant of classification frequent pattern tree  VCFP-Tree is a compress storage structure of data set in order to mine classification association rules without candidate frequent item sets generation. A VCFP-Tree must satisfy condition below 9  1. A VCFP-Tree consists of one root labeled as null", a set of item prefix subtrees as the children of the root, and a frequent-item header table F-list 2. Each node in the item prefix subtree consists of five fields: item child-link, parent-link, node-link, and class-list, where item registers which item this node represents, child-link links to child nodes of each node parent-link links to parent node of each node, nodelink links to the next node in the VCFP-Tree carrying i 2 Proceedings of 2008 IEEE International Symposium on IT in Medicine and Education 769 


the same item, or null if there is none, and class-list is a classification list which records every class label and its corresponding frequency 3. Each entry in the frequent-item header list consists of three fields, \(1\-name; \(2\start of node-link, which points to the first node in the VCFPTree carrying the same item; \(3\ation gain value of the attribute Table 1. Prescription data set T No. a b c d e classification tag  11010 1 A 21110 0 B 30001 1 A 41101 0 C 51111 0 C     Figure 1. VCFP-Tree based on data set T As shown in table 1 below, Chinese medicines usually is designated as attributes of prescriptions data set such as a and b etc. Whether a Chinese medicine appears in a prescription is been indicated by 0 or 1. Classification attributes often list in the end, such as classification tag. A VCFP-Tree \(i.e figure 1\set T The order of items in the frequent-item header table Flist is a 4b 3c 3d 3 with 3 as the threshold of support count. The information stored in the node indicated by deep color arrow is that there is one prescription containing Chinese medicines abd and belonging to class C  4.2. Classification association rule Class association rule is an implication of the form R  C D where C D   sup  R  min _ sup and conf  R  min _ conf  C indicates conditional attribute \(or item D indicates decision attribute \(or classification attribute Sup  R is the support of rule R  min _ sup is the lowest threshold of support Conf  R  confidence of rule R  min_conf is the lowest threshold of confidence. The support and confidence of rule R is defined below Sup  R  P  C D  4 Conf  R  P  D  C  5 P  C D  oint probability of C and D  P  D  C  belongs to class D under the condition of Chinese medicine C appearance. Rule support and confidence are two important measures of rule interestingness They respectively reflect the usefulness and certainty of discovered rules. The rules with higher support and confidence are usually chosen as interesting pattern For example Ramulus Cinnamomi* Rhizoma Zingiberis Recens* Radix Paeoniae Alba Cold Zheng Support count=15 The frequency of rule former piece=25 Where {Ramulus Cinnamomi*Rhizoma Zingiberis Recens*Radix Paeoniae Alba} before is the rule former piece; {Cold Zheng} after is the rule rear piece. The support count indicates the frequency of three Chinese medicines above appearing in a same prescription and treating Cold Zheng, and is also called the frequency of rule; the frequency of rule former piece indicates the frequency of three Chinese medicines above appearing in a same prescription There are 268 prescriptions in Zhongjing prescriptions So the rule support is 0.056, which is computed through support count 15 divided by 268. The rule confidence is 60%, which is computed through support count 15 divided by the frequency of rule former piece 25 4.3. Efficient mining of classification association rules based on information gain feature selection The concrete steps of algorithm CARM-IG which are used to m ine classification association rule effectively are as following Algorithm CARM-IG Input min _ sup the lowest threshold of support min _ conf the lowest threshold of confidence min _ IG  the lowest threshold of information gain Output CAR the set of classification association rules Begin 1\The information of relevant frequencies is recorded, so that the information gain value of every conditional attribute can been computed completely Proceedings of 2008 IEEE International Symposium on IT in Medicine and Education 770 


2\than min _ sup and min _ IG are selected. The frequent-item header table F-list is constructed in attributes support descending order 3 a VCFP-Tree T is constructed according to section4.1 4 CAR CFP-growth T  min_sup  min_conf  5\eturn CAR  End Algorithm CARM-IG scans Zhongjing prescription database firstly. The frequency i x f for every Chinese medicine x i the frequency j y f for each type of prescription, and the frequency  j i f yx for each type of prescription containing Chinese medicine x i are recorded j y f is used to compute 1    m I ss and  j i f yx is used to computer so that information gain value IG of every conditional attribute can be computed according to Eq 1 conditional attributes \(or Chinese medicines\with  EC i x f  min _ sup and IG  min _ IG are chosen. The header table F-list is constructed in accordance with i x f descending. The objective of the second scan of prescription data set is to create a compress storage structure for Zhongjing prescription, namely a VCFPTree T Finally, the algorithm of CFP-growth \(ClassFrequent Pattern growth 7  1. Based on F-list in Figure 1, the set of classassociation rules can be divided into subsets without overlap 1\the ones having d; \(2 but no d; \(3\the ones having b, but no d  nor c ; and \(4 the ones having only a. So algorithm CARM-IG finds these subsets of class-association rules one by one 2. To find the subset of rules having d, algorithm CAR M-IG traverses nodes having attribute value d and look çupwardé to collect a d-projected database, which a,b,d\three tuples. It contains all the tuples having d. The problem of finding all frequent patterns having d in the whole training set can be reduced to mine frequent patterns in d-projected database 3. Recursively, in d proj ected database a and b are the frequent attribute values, i.e., they pass support threshold. \(In d projected database d happens in every tuple and thus is trivially frequent. We do not count d as a local frequent attribute value.\mine the projected database recursively by constructing VC FPtree s and projected o re details 4. It happens that, in d proj ected database a and b always happen together and thus ab is a frequent pattern a and b are two subpatterns of ab and have same support count as ab To avoid triviality, we only adopt frequent pattern abd Based on the class label distribution information, we generate rule abd C with support count 2 and confidence 100 5. After search for rules having d all nodes of d are merged into their parent nodes, respectively. That is the class label information registered in a d node is registered in its parent node. The VC FP-tree is shrunk as shown in figure 2. Please note that this treeshrinking operation is done at the same scan of collecting the d projected The remaining subsets of rules can be mined similarly Anyway, the inform ation used to compute information gain value for every conditional attribute is collected completely through algorithm CARM-IG without increase in computing time basically. The scale of VCFP-Tree created in memory has been reduced moderately after dimension reduction based on information gain filter. Then the mining of classification association rule becomes more quickly and efficient, and experiments show that the run time and memory cost of algorithm CARM-IG has been reduced respectively     Figure 2. VCFP-Tree after merging nodes d 5. Experiment result Experimental platform configured to Intel 2G / 1G Windows XP. Algorithm C ARM-IG using java programming is achieved. The 268 Zhongjing prescriptions are chosen as data set, which are classified by experts in the TCM field from two angles of cold-heat and deficiency-excess in eight principles The lowest support, confidence and information gain threshold set 2%, 50% and 0.1 respectively Experiment results are summarized below 5.1. The comparison of run time and memory cost Run time and memory cost are compared between algorithms of classification association rules mining Proceedings of 2008 IEEE International Symposium on IT in Medicine and Education 771 


based \(algorithm II\ based \(algorithm I information gain feature selection. Experiment results listed in table 2 show that algorithm II has lower run time and memory cost than algorithm I, and the reduction of memory cost is particularly conspicuous Table 2. The comparison of time and memory cost methods Classifications Attributes  Time s Memory M  cold-heat 49 0.187 10.440 I deficiency excess 49 0.172 10.908 cold-heat 37 0.172 3.124 II deficiency excess 38 0.156 2.940 5.2. The classification compatibility principle of Zhongjing prescriptions The classification association rules of Zhongjing prescription are m ined by algorithm CARM-IG from two angles of cold-heat \(21 rules\and deficiencyexcess \(29 rules\For example Ramulus Cinnamomi * Herba Ephedrae}-> {Cold Zheng} \(Sup=3.36%, Conf=64.28 Jujubae * Radix Paeoniae Alba Deficiency Zheng Sup=7.09%, Conf=65.51 To sum up, the run time and memory cost of algorithm is reduced, at the sam e time, the concision and precision of classification compatibility principles is guaranteed because of the deletion of irrelevant Chinese medicines with classification. The mined rules more follow with the prescription law directed by basic theories of traditional Chinese medicine by experts confirmation preliminary. Follow-up study focuses on the careful and thoroughgoing explanation for classification compatibility principle of Zhongjing prescriptions through literature research and expert advice Acknowledgments This work was supported in part by the National Basic Re search Program of China \(2006CB504701\by Foundation of Science & Technology Project of Fujian 2006Y0016\Health Department Special Project \(YA-204\Integrative Medicine Development Fund of Chen Keji\(905012009 References   Yao Meicun, Ai Lu. Association Rule Analy sis for Compatibility Principle of Diabetes Prescriptions. Journal of Beijing University of Traditional Chinese Medicine, 2002 25\(6   He Qianfeng Zhou Xuezhong, Zhou Zhongmei. The Cluster Analys is based on the Function of Chinese Medicine Chinese Journal of Information on Traditional Chinese Medicine. 2004, 11\(6  Shang Jingsheng, Hu Lisheng, Niu Xin et al The Data Mining Experiment for the Compatibility Principle of Banxia Xiexin Tang. Journal of China-Japan Friendship Hospital 2005, 19\(4   Zhao Dandan The Improvement Algorithm of Apriori and Its Application in Chinese Medicine Knowledge Discovery. Computer and Modernization, 2007 \(8  Wu Zhaohui, Feng Yi. A number of explorations on Knowledge Discovery in Database in Traditional Chinese Medicine field  Chinese Journal of Information on Traditional Chinese Medicine. 2005, 12\(10  Wu Zhaohui, Feng Yi. A number of explorations on Knowledge Discovery in Database in Traditional Chinese Medicine field  Chinese Journal of Information on Traditional Chinese Medicine. 2005, 12\(11   Li W, Han J, Pei J, , et al. CMAR: Accurate & Efficient   In ICDMê01, San Jose, CA, 2001:369-376  Li Bin. Research on Classification based on Multiple Assoc iation Rules of Herbal Formula Database    Nanjing Southeast University  Department of Computer Science and Engineering, 2005   Yang Xuem ei, Lin Duany i. The Classification Association Rule Mining for the Prescriptions of Spleenstomach Damp-heat Symptom in Ming and Qing Dynasties   Chinese Journal of Info rmation on Traditional Chinese Medicine, 2006, 13\(10  Han J, Pei J, Yin Y Mining Frequent Patterns without   In SIGMODê00, Dallas, TX, May 2000  Jiawei Han,Micheline Kam ber. Data Mining Concepts and Techniques.2001 by Morgan Kaufmann Publishers, Inc page 200 Proceedings of 2008 IEEE International Symposium on IT in Medicine and Education 772 


the proposed approach. The evolution of the Pareto fronts along with different generations by the proposed approach is shown in Figure 6             Figure 6. The evolution of the Pareto fronts From Figure 6, it can be observed that the solutions were distributed on the Pareto fronts in different generations. Besides, the final solutions \(after 500 generations\were better than those in other generations. Since the data distribution followed the exponential distribution, the solutions on the Pareto fronts were a little narrow. The proposed approach is thus effective in finding an appropriate set of solutions  6. Conclusion and future works  In this paper, we have proposed a multi-objective genetic-fuzzy mining algorithm for extracting membership functions from quantitative transactions for the SSFM problem. Two objective functions namely suitability and totalNumL1 have been used to find the Pareto-front solutions. Experimental results also show that the proposed approach is effective in finding the Pareto-front solutions. In the future, we will continuously enhance the multi-objective genetic-fuzzy approach for more complex problems, such as for solving the MSFM problem  7. Acknowledgment This research was supported by the National Science Council of the Republic of China under contract NSC 96-2213-E-390-003  8. References   R  A g ra w a l an d R S r i k an t   F ast al go ri t h m f o r m i n i n g  association rules The International Conference on Very Large Databases pp. 487-499, 1994  C  C C h an and W  H A u  M i n i n g f u zzy asso ci at i o n  rules The Conference on Information and Knowledge Management Las Vegas, pp. 209-215, 1997 3 O Co r d  n F. Herrera, an d  P. Villar, ìGenerating the knowledge base of a fuzzy rule-based system by the genetic learning of the data base IEEE Transactions on Fuzzy Systems Vol. 9, No. 4, pp. 667ñ674, 2001 4 C A  C o e llo D  A  Va n Ve l dhu iz e n a n d G  B  L a m ont Evolutionary Algorithms for Solving Multi-objective Problems Kluwer Academic Publishers, 2002 5 K Deb   Multi-objective Optimization Using Evolutionary Algorithms John Wiley & Sons, 2001  C  M  F o n s eca an d  P  J F l em i n g  G en et i c al go ri t h m s  f o r  multiobjective optimization: Formulation, discussion and generalization The International Confidence on Genetic Algorithms pp. 416-423, 1993  T  P  Hon g  C H C h en  Y L   W u an d Y C L e e  A  GA based fuzzy mining approach to achieve a trade-off between number of rules and suitability of membership functions Soft Computing   Vol. 10, No. 11, pp. 1091-1101. 2006 8 T  P  H ong C  S  K u o a n d S  C  C h i   M ini n g a s s o c i a tio n rules from quantitative data Intelligent Data Analysis Vol 3, No. 5, pp. 363-376, 1999 9 T P  H ong C  S. K uo a n d S C  C h i T r a de o f f be t w ee n time complexity and number of rules for fuzzy mining from quantitative data International Journal of Uncertainty Fuzziness and Knowledge-based Systems Vol. 9, No. 5, pp 587-604, 2001 1 F  Herrera  M  L o zan o and J L   V e rd ega y  F u zzy connectives based crossover operators to model genetic algorithms population diversity Fuzzy Sets and Systems Vol. 92, No. 1, pp. 21ñ30, 1997 11 M  Ka y a a nd R A l ha jj I nte g ra ting m u tlti-obje c tiv e  genetic algorithms into clustering for fuzzy association rules mining The IEEE International Conference on Data Mining pp. 431-434, 2004 12 M  Ka y a  M ulti-o b je c tiv e g e ne tic a l g o rithm ba se d approaches for mining optimized fuzzy association rules Soft computing Vol. 10, pp. 578-586, 2006 13 C  K u ok A  Fu a nd M. W ong  M ining f u z z y as s o c i a tion rules in databases SIGMOD Record Vol. 27, No. 1, pp. 4146, 1998 14 Y  C  L e e  T  P  H o n g a n d  W  Y  L i n   M i n i n g f u z z y  association rules with multiple minimum supports using maximum constraints Lecture Notes in Computer Science  Vol. 3214, pp. 1283-1290, 2004 1 H R o ub o s and M   S e t n es Co m p act an d t r an sp aren t  fuzzy models and classifiers through iterative complexity reduction IEEE Transactions on Fuzzy Systems Vol. 9, No 4, pp. 516-524, 2001 16 J. D  Sc ha f f e r M ultiple  o b je c tiv e optim iz a tion w ith vector evaluated genetic algorithms The International Conference on Genetic Algorithms pp. 93-100, 1985  


0 10 20 30 40 50 60 3210.75 minimum support execution time \(minutes GMFI GMAR BASIC Cumulate  Fig. 10 Mining time in DENSE databases  7.  Conclusions  Through several comprehensive experiments, we found that the FCET and IFECT can save a larger amount of storage spaces than Apriori, MaxEclat, and CHARM in both SPARSE databases and DENSE databases. Since the FCET stores fewer elements for a long pattern, when matched with GMFI/GMAR algorithm, it also revealed efficient execution time than BASIC and CUMULATE in mining generalized association rules The time complexity to find the maximal itemsets is O\(log2n\ where n is the total number of maximal itemsets. For a long pattern, we used a partition tree to count the SUB_TID of itemsets, and then got their merged results. Although the memory required for the FCET is still exponentially large, through limiting the size of maximal itemsets and the size of clustering to a reasonable memory requirement, we do save a large amount of storage spaces, especially in dense databases  7.    Conclusions  Through several comprehensive experiments, we found that the FCET and IFECT can save a larger amount of storage spaces than Apriori, MaxEclat, and CHARM in both SPARSE databases and DENSE databases. Since the FCET stores fewer elements for a long pattern, when matched with GMFI/GMAR algorithm, it also revealed efficient execution time than BASIC and CUMULATE in mining generalized association rules The time complexity to find the maximal itemsets is O\(log 2 n\ where n is the total number of maximal itemsets For a long pattern, we used a partition tree to count the SUB_TID of itemsets, and then got their merged results Although the memory required for the FCET is still exponentially large, through limiting the size of maximal itemsets and the size of clustering to a reasonable memory requirement, we do save a large amount of storage spaces especially in dense databases 8. References  1 g ra w a l, T  Im ieli n s k i an d A  S w a m i M in i n g  association rules between sets of items in large databases,î Proc. ACM International Conference on Management of Data \(1993\, pp. 207-216 2 g ra w a l an d R. Srik an t F a s t alg o rit h m s f o r  mining association rules,î Proc. 20th International Conference on Very Large Data Bases \(1994\ pp.487499  J i aW e i Han  J i an  P e i an d YiW en Yi n   Mi n i n g frequent patterns without candidate generation,î Proc ACM International Conference on Management of Data 2000\p. 1-12 4 J  S   Pa r k  M  S  C h en  an d P S  Y u     A n ef f e c t iv e hash-based algorithm for mining association rules,î Proc ACM International Conference on Management of Data 1995\p.175-186 5 A  Sa va se r e  E  O m i e c i ns ki  a n d S N a va t h e    A n  efficient algorithm for mining association rules in large databases,î  Proc. 21st International Conference on Very Large Data Bases \(1995\ pp.432-443 6 Y i n-F u H u a ng a n d  Chi e h M i ng W u   M i n i n g  generalized association rules using pruning techniques Proc. IEEE International Conference on Data Mining 2002\p.227-234 7 a k i a n d C.J  Hsia o   E ff icie n t al g o rith m s f o r  mining closed itemsets and their lattice structure, î  IEEE Transactions on Knowledge and Data Engineering, vol 17, no. 4, April \(2005\p. 462-478   Bu rdick  M. C a li m l i m  an d J. Geh r k e  M AF I A a maximal frequent itemset algorithm for transactional databases,î Proc. 17th International Conference on Data Engineering, \(2001\p.443-452  a k i S  P a rth a s a rat h y   M. Ogih ara, a n d W. Li   New algorithms for fast discovery of association rules Proc. 3rd ACM International Conference on Knowledge Discovery in Databases and Data Mining, \(1997\pp 283-286 10 M  J  Za ki a n d K  G o ud a  Fa s t ve r t i c a l  mi ni ng usi n g  diffsets,î Proc. 9th ACM International Conference on Knowledge Discovery and Data Mining, Aug. \(2003   Mam o u l i s D  W. C h e u ng an d W. L i a n    Similarity search in sets and categorical data using the signature tree,î Proc. 19th International Conference on Data Engineering, \(2003 12 R. Srik a n t a n d R Ag ra w a l   M i n i n g g e n e ralized  association rules,î Proc. 21st International Conference on Very Large Data Bases, \(1995\.407-419    
571 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


