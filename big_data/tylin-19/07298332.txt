CLUSTER BASED DATA REDUCTION METHOD FOR TRANSACTION DATASETS Mohammed Alweshah 1 Wael Ahmad AlZoubi 2  Abdulsalam Alarabeyyat 3  1 3 Prince Abdullah Bin Ghazi Faculty of Information Technology, Al-Balqa Applied University, Salt, Jordan weshah2@yahoo.com  aw_arabiat@hotmail.com  2 Ajloun University College,Al-Balqa Applied University , Ajloun, Jordan alzoubi_wael@yahoo.com  Abstract The common feature of transaction datasets is that it is very huge in size, so it is important to develop a technique for dataset reduction. The process of dataset reduction must not change the features of the original dataset; this will increase the effectiveness and efficiency of extracting association rules from these datasets without affecting the original data. Disjoint clusters that have different number of transactions will be introduced in order to minimize the search space this in turn will decrease the time required to mine the desired rules by dealing with each cluster individually. The support and confidence measures will be used to determine the frequent item sets and exclude the others Keywords: dataset reduction, Association rules mining, Instance selection 1  Introduction Recently, an explosion of information technology has been occurred due to the generation of electronic data, widespread of transferring data through Internet, the extensive use of bar codes, and the improvement in computer systems, all these factors besides the available capabilities of database systems lead to produce and accumulate huge amounts of data these data caused a lot of problems as the data overload and lack of scalability. The tools used to deal with huge datasets are stronger than those used to collect and store the data [1 There is a vital requirement for data mining techniques and tools that can generate useful information and knowledge from huge volumes of data [3, 4 Association rules mining \(ARM come out as an essential method for constructing knowledge base systems has been used to solve real-world problems such as intrusion detection er relationship m anag e m e nt CRM Webb ased decision support problem a ny other problem s [9 Data reduction is an important step of data pre-processing a in g o als is preparing data for mining.  In transaction processing systems, data reduction enables decision makers to know the real dimensionality of the enterprise database\(s\which in turn leads to an informed decision making The huge size of transaction datasets makes the process of generating association rules very difficult task and constructing association graph is impossible or in best cases infeasible, so it is highly required to use a technique that improves the process of frequent itemset generation and reduces the size of the dataset before constructing the association graph to extract the desired association rules, this technique is called d ataset r eduction b ased on  78 978-1-4799-8969-0/15/$31.00 ©2015 IEEE 


c lustering \(DRBC\RBC best utilizes the memory space needed to store the generated frequent itemsets In this work, we investigate the implementations of Instance selection method as among the dataset reduction methods that are commonly use, we proposed dataset reduction based on clustering \(DRBC\technique, which is considered as an improvement to the traditional dataset reduction methods The rest of the paper is organized as follows: Section 2 describes the data reduction methods and approaches that have been employed in previous works. Also, provide an example to give an extra explanation to our proposed algorithm. Section 3 concludes the work presented in this paper 2  Data Reduction Methods Evolutionary algorithms, or genetic algorithms GA\, and instance selection \(IS\ong the dataset reduction methods that are commonly used in the literature. These two methods aim to reduce the search space since data reduction in the process of knowledge discovery in databases KDD\ can be viewed as a search problem. The following subsections illustrate briefly these methods 2.1  Evolutionary Algorithms \(EAs Data reduction in KDD may be considered as a search problem, it may be solved using evolutionary algorithms \(EAs\a reduction can be thought as a strategy in data preprocessing. EAs give promising results when they are used to solve the IS problem  Evolutionary algorithms used to optimize the rules generated by an association rule mining algorithm. Overall, the system can expect the rules that hold negative attributes when the EAs are used over the association rules that are produced by an ARM technique; this is since the rule mining techniques do not take in consideration the pessimistic appearances of attributes. EAs do a global search and deal with attribute interaction better than the greedy rule induction algorithms normally used in data mining, and this is considered as the main motivation for using EAs in the discovery of high-level prediction rule s shown in Fig   1 2.2  Instance Selection \(IS Instance selection method aims to minimize the number of rows in a transaction dataset Instance selection method is a central task in the data training phase of K I t is one of the effective means of data reduction. There are different strategies of IS that can be followed some of these strategies are: \(1\pling, \(2 boosting, \(3\pe selection \(PS\\(4 active learning. Prototype selection \(PS algorithms can be classified as displayed in figure 1 79 


  Figure 1 Prototype Selection \(PS\lgorithms \(Cano et al. 2003 The main two strategies that IS participates in are: \(1\nstance Selection for Prototype Selection \(IS-PS\and \(2\nstance Selection for Training Set Selection \(IS-TSS\In the IS-PS approach, the analysis of the results obtained when choosing prototypes \(instances for a nearest neighbor \(NN\orithm, while in the IS-TSS approach, the selected instances are first used to construct a decision tree, after that the tree is used to organize new patterns 2.2.1  DRBC Method Figure 2 presents the steps of the proposed dataset reduction based on clustering  DRBC technique, which is considered as an improvement to the traditional dataset reduction methods Algorithm DRBC I n p u t Set of m clusters m is the length of the longest transaction record Number of transaction records N O u t p u t Minimized set of clusters For  k 1 k  N  k  Determine infrequent 1-itemsets For  k 2 k  m  k  Use upward closure property to remove infrequent k itemsets   Figure 2 T h e  P r o p o s e d  D a t a s e t  R e d u c t i o n  M e t h o d  A l g o r i t h m i c  S t e p s   As shown in Fig. 2. After building the clustering matrix and assigning each transaction to the proper cluster according to their length and building the clustering matrix, the clustering matrix consists of rows and columns, where the rows are transaction records and the columns are the items, the value 0 is put in the cross of transaction j with item k if this item is not exist in that transaction, otherwise the value 1 is recorded, so all the items are represented as bit vectors to simplify the process of computing the support value for each item and identify the frequent 1-itemsets from other itemsets available in the cluster matrix Infrequent 1-itemsets are determined and their supersets in the k th clusters k  2, are ignored from further processing, which may lead to a minimized set of frequent itemsets with less loss of knowledge, and this improves the efficiency and scalability of the process of mining association rules from the association graph that will be constructed using a graph based association rules mining algorithm   80 


2.2.2  Example of database transactions We provide an example to give an extra explanation to our proposed algorithm; the minimum support threshold is 20%. There are 20 transactions and 5 different items in the database. We assume – as in most of sequential rule mining algorithms – that the items are in lexicographical order. A transaction database example is shown in Table 1; the items are represented by letters rather than numbers to deal with some worst cases, where the numbering step is required. The first step, as we mentioned earlier in the previous section, is scanning the database to determine the length of each transaction, the length means the number of items in a transaction, and at the same time assigning numbers to the items, item A will be given the number 1, item B the number 2 and so on. This will help us in constructing the cluster matrix, after that we don’t need to rescan the database, as we will move to deal with the clustering matrix that can be easily reside in the main memory In our example, the maximum transaction length is 4, and so, there will be at most four clusters Since there are no transactions of length 1, the total number of clusters is 3 as shown in Fig. 3 the table contents are 0s or 1s to denote absence or existence of an item in a transaction, after constructing the matrix, each column is the bit vector for the corresponding item, and so, no need to make further contrasts with the cluster matrix. These bit vectors are used in identifying the infrequent 1-itemsets and exclude them with their supersets from further processing in order to simplify the process of mining the desired association rules Table 1: an example of database transactions TID Items TID Items TID Items TID Items 10 A  B  C 60 C  E 110 A  B  C  E 160 A  C  E 20 B  C 70 B  C  E 120 C  D 170 C  E 30 A  E 80 A  B  C  D 130 B  C  D 180 A  C  D 40 A, C, D, E 90 C, E 140 A, C, E 190 A, E 50 A, C 100 A, B, D 150 B, D, E 200 C, E  By counting the number of 1s in each bit vector we determine the support for each candidate itemset of length 1, as the following: support 1}\\({2}\support 4}\support 5}\hus the frequent             1itemsets are: {{1}, {3}, {4}, {5}} as their support is not less than 45%, itemset {2} and all its supersets, i.e. {2, 3}, {1, 2, 3}, {2, 3, 5}, {1 2, 3, 4}, {1, 2, 4}, {1, 2, 3, 5}, {2, 3, 4} and 2, 4, 5}will be eliminated from the database of transactions 81 


Shaded transactions are those that will be ignored from further processing which will reduce exponentially the time required to generate all frequent itemsets. After that, the frequent 2-itemsets are generated by partially scanning the clusters, i.e. the k th clusters, where k  2, only the frequent 2-itemset is {3, 5 whose support is 0.5 which is greater than the minimum support threshold, i.e. 0.45. So, {1, 4 will be removed from clusters in figure 3 \(b The last itemset is {1, 3, 5} is frequent since its support is 100%, thus all frequent itemsets are generated                     Figure 3 \(a\he cluster tables for the database given in table 1 \(b\he reduced clusters   1 2 3 4 5 T 2  0 1 1 0 0 T 3 1 0 0 0 1 T5 1 0 1 0 0 T7 0 0 1 0 1 T10 1 0 0 1 0 T12 0 0 1 0 1 T14 0 0 1 1 0 T19 1 0 0 0 1 T20 0 0 1 0 1 T1 1 1 1 0 0 T6 1 0 1 0 1 T8 0 1 1 0 1 T11 1 1 0 1 0 T15 0 1 1 1 0 T16 1 0 0 1 1 T17 0 1 0 1 1 T18 1 0 1 1 0 T4 1 0 1 1 1 T9 1 1 1 1 0 T13 1 1 1 0 1  1 3 4 5 T 3 1 0 0 1 T5 1 1 0 0 T7 0 1 0 1 T10 1 0 1 0 T12 0 1 0 1 T14 0 1 1 0 T19 0 1 0 1 T20 0 1 0 1 T6 1 1 0 1 T16 1 0 1 1 T18 1 1 1 0 T4 1 1 1 1 Item Item Cluster 4 Cluster 3 Cluster 2  Transaction Transaction a b 82 


3  Conclusion and Future Work DRBC takes benefit of the representation of data items as bit vectors to simplify the extraction of infrequent 1-itemsets, i.e. itemsets that have only one item, and converts the data matrix into cluster matrices of variable dimensions, such that each one of these matrices \(rows are the transactions’ identifiers and columns are items is processed separately to generate frequent item sets of a specific length, i.e. local frequent itemsets, then by using the upward closure property, it can easily eliminate infrequent itemsets from all cluster matrices in an efficient way. As a future work, DRBC can be expanded to cover different types of data, for example time series data, mix mode data, etc 4  References 1 U. Fay y a d and R. Uthurusam y  Ev o lv ing data into mining solutions for insights Communications of the ACM vol. 45 pp. 28-31, 2002 2 X  B. L i and V S. Jaco b, "Adaptiv e  data reduction for large-scale transaction data European Journal of Operational Research vol. 188, pp. 910-924, 2008 3 D  Anbarasi and K   Viv e kanandan Dimension Reduction Techniques for Market Basket Analysis  S. M. Weiss Predictive data mining: a practical guide Morgan Kaufmann 1998 5 M  C. Okur and M  Buy u kkececi, "Big data challenges in information engineering curriculum," in EAEEIE EAEEIE\Conference  2014, pp. 1-4 6 P e ng et al A descriptive framework for the field of data mining and knowledge discovery International Journal of Information Technology Decision Making vol. 7, pp. 639-682 2008 7 M  J. Shaw et al Knowledge management and data mining for marketing Decision support systems vol. 31, pp. 127-137, 2001 8 J. P  Shim et al Past, present, and future of decision support technology Decision support systems vol. 33, pp 111-126, 2002 9  L   A  K u r g a n a n d  P  M u s i l e k   A s u r v e y  of Knowledge Discovery and Data Mining process models The Knowledge Engineering Review vol. 21 pp. 1-24, 2006    R Can o et al Using evolutionary algorithms as instance selection for data reduction in KDD: an experimental study Evolutionary Computation, IEEE Transactions on vol. 7, pp. 561-575 2003  C R. Reeves and D R. Bush, "Using genetic algorithms for training data selection in RBF networks," in Instance selection and construction for data mining ed: Springer, 2001, pp. 339-356  M  S a g g a r et al Optimization of association rule mining using improved genetic algorithms," in Systems, Man and Cybernetics, 2004 IEEE International Conference on 2004, pp 3725-3729 13  A  I  R  L   A z e v e d o   K D D  S E M M A  a n d  CRISP-DM: a parallel overview," 2008   83 


         


       0.4 0.8 1.2 1.6 250 500 750 1000 1250 1500 of Queries Average time \(sec Scalability of MISA for WKL_1 0.5 1.0 1.5 2.0 100 200 300 400 500 600 of Queries Average time \(sec Scalability of MISA for WKL_2 0 5 10 15 20 25 30 35 500 5500 10500 15500 20500 25500 30500 of Queries Average time \(sec Scalability of MISA for WKL_3 Figure 2 Scalability of Mining Index Selection Approach for Three Workloads  J Kratica I Ljubic and D T osic  A genetic algorithm for the index selection problem In Applications of Evolutionary Computing Tech Rep 2003  D Comer  The Dif culty of Optimum Inde x Selection ACM Trans Database Syst  vol 3 no 4 pp 440…445 Dec 1978 A v ailable http://doi.acm.org/10.1145/320289.320296  techopedia Query Optimizer  2015 http://www.techopedia.com/de“nition/26224/query-optimizer Online accessed Online A v ailable http://www.techopedia.com/de“nition/26224/query-optimizer  J Dong and M Han Bittable An ef cient mining frequent itemsets algorithm Knowl.-Based Syst  vol 20 no 4 pp 329…335 2007 A v ailable http://dblp.unitrier.de/db/journals/kbs/kbs20.html  P  Ameri U Grabo wski J Me yer  and A Streit On the Application and Performance of MongoDB for Climate Satellite Data in 13th IEEE International 2808 


Conference on Trust Security and Privacy in Computing and Communications TrustCom 2014 Beijing China September 24-26 2014  IEEE 2014 pp 652…659 A v ailable http://dx.doi.org/10.1109/TrustCom.2014.84  R Lutz P  Ameri T  Latzk o and J Me yer  Management of Meteorological Mass Data with MongoDB in 28th International Conference on Informatics for Environmental Protection ICT for Energy Ef“eciency EnviroInfo 2014 Oldenburg Germany September 10-12 2014 J.M.G  omez M Sonnenschein U Vogel A Winter B Rapp and N Giesen Eds BIS-Verlag 2014 pp 549…556 Available http://www.enviroinfo2014.org  A Caprara M Fischetti and D Maio Exact and Approximate Algorithms for the Index Selection Problem in Physical Database Design IEEE Trans Knowl Data Eng  vol 7 no 6 pp 955…967 1995  K Aouiche and J Darmont Data Mining-based Materialized View and Index Selection in Data Warehouses CoRR  vol abs/0707.1548 2007 A v ailable http://arxiv.org/abs/0707.1548  N P asquier  Y  Bastide R T aouil and L Lakhal Discovering Frequent Closed Itemsets for Association Rules in Proceedings of the 7th International Conference on Database Theory  ser ICDT 99 London UK Springer-Verlag 1999 pp 398…416 A v ailable http://dl.acm.org/citation.cfm?id=645503.656256  S Agra w al S Chaudhuri and V  R Narasayya  Automated Selection of Materialized Views and Indexes in SQL Databases in Proceedings of the 26th International Conference on Very Large Data Bases  ser VLDB 00 San Francisco CA USA Morgan Kaufmann Publishers Inc 2000 pp 496…505 A v ailable http://dl.acm.org/citation.cfm?id=645926.671701  A Sk elle y  DB2 Advisor An Optimizer Smart Enough to Recommend Its Own Indexes in Proceedings of the 16th International Conference on Data Engineering  ser ICDE 00 Washington DC USA IEEE Computer Society 2000 pp 101 A v ailable http://dl.acm.org/citation.cfm?id=846219.847390  W  G Pedrozo and M S M G V az A T ool for Automatic Index Selection in Database Management Systems in Proceedings of the 2014 International Symposium on Computer Consumer and Control  ser IS3C 14 Washington DC USA IEEE Computer Society 2014 pp 1061…1064 A v ailable http://dx.doi.org/10.1109/IS3C.2014.277  M Zaman J Surabattula and L Gruenw ald An Auto-Indexing Technique for Databases Based on Clustering in Proceedings of the Database and Expert Systems Applications 15th International Workshop  ser DEXA 04 Washington DC USA IEEE Computer Society 2004 pp 776…780 A v ailable http://dx.doi.org/10.1109/DEXA.2004.32  S Chaudhuri and V  Narasayya An Ef cient Cost-Driven Index Selection Tool for Microsoft SQL Server in VLDB  Very Large Data Bases Endowment Inc August 1997 A v ailable http://research.microsoft.com/apps/pubs/default.aspx?id=68349  R Agra w a l and R Srikant F ast algorithms for mining association rules in large databases in Proceedings of the 20th International Conference on Very Large Data Bases  ser VLDB 94 San Francisco CA USA Morgan Kaufmann Publishers Inc 1994 pp 487…499 A v ailable http://dl.acm.org/citation.cfm?id=645920.672836  B Goethals Memory Issues in Frequent Itemset Mining  in Proceedings of the 2004 ACM Symposium on Applied Computing  ser SAC 04 New York NY USA ACM 2004 pp 530…534 A v ailable http://doi.acm.org/10.1145/967900.968012  J Hipp U G  untzer and G Nakhaeizadeh Algorithms for Association Rule Mining Mdash a General Survey and Comparison SIGKDD Explor Newsl  vol 2 no 1 pp 58…64 Jun 2000 A v ailable http://doi.acm.org/10.1145/360402.360421  A Rajaraman and J D Ullman Mining of Massive Datasets  New York NY USA Cambridge University Press 2011  MongoDB Introduction to MongoDB  2015 http://www.mongodb.org/about/introduction Online accessed Online A v ailable http://www.mongodb.org/about/introduction  J Pok orn y  NoSQL Databases A Step to Database Scalability in Web Environment in Proceedings of the 13th International Conference on Information Integration and Web-based Applications and Services  ser iiWAS 11 New York NY USA ACM 2011 pp 278…283 Available http://doi.acm.org/10.1145/2095536.2095583  MongoDB SQL to MongoDB Mapping Chart 2015 http://docs.mongodb.org/manual/reference/sqlcomparison Online accessed Online Available http://docs.mongodb.org/manual/reference/sqlcomparison  BSON BSON Binary JSON  2015 http://bsonspec.or g Online accessed Online A v ailable http://bsonspec.org  MongoDB Inde x T ypes  2015 http://docs.mongodb.org/manual/core/index-types Online accessed Online A v ailable http://docs.mongodb.org/manual/core/index-types  R Core T eam R A Language and Environment for Statistical Computing  R Foundation for Statistical Computing Vienna Austria 2015 A v ailable http://www R-project.or g  MongoDB Inc and M Schmidber ger  rmongodb RMongoDB driver  2014 r package version 1.8.0 Available http://CRAN.R-project.org/package=rmongodb  K Bank er  MongoDB in Action  Minning Publication Co 2012 2809 


 MongoDB Geospatial Query Operators  2015 http://docs.mongodb.org/manual/reference/operator/querygeospatial Online accessed 18 Online A v ailable http://docs.mongodb.org/manual/reference/operator/querygeospatial  M K ormilitsin R Chirk o v a Y  F athi and M Stallmann View and Index Selection for Query-performance Improvement Quality-centered Algorithms and Heuristics in Proceedings of the 17th ACM Conference on Information and Knowledge Management  ser CIKM 08 New York NY USA ACM 2008 pp 1329…1330 A v ailable http://doi.acm.org/10.1145/1458082.1458261 2810 


