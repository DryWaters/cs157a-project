Immune Clone Algorithm for Mining Association Rules on Dynamic Databases Hongwei Mo                                         Lifang Xu Automation College                           Automation College Harbin Engineering University         Harbin Engineering University Harbin,150001,China                      Harbin,150001,China honwei2004@126.com                     mxlfang@sohu.com Abstract The paper seeks to generate large itemsets in a dynamic transaction database using immune clone algorithm. Intra transactions, Inter transactions and distributed transactions are considered for mining association rules. The time of complexity of 
DMARICA\(Dynamic Mining of Association Rules using Immune Clone Algorithm\ is analyzed, with Fast Updata\(FUP\ algorithm for intra transactions and EApriori for inter transactions. The problem of mining association rules in the distributed environment is explored by Distributed DMARICA\(DDMARICA\. The study shows that DMARICA outperforms both FUP and E-Apriori in terms of execution time and scalability, without comprising the quality or completeness of rules generated. DMARICA is also compared with DMARG\(Dynamic Mining of Association Rules using Genetic Algorithm\.And it has 
better performance than that of DMARG 1. Introduction Data mining attempts to extract valuable and hidden information from large databases ranging from gigabytes to terabytes. Mining associate rules is one of the main tasks of data mining Mining optimized association rules with respect to categorical and numerical attributes was introduced in h e tech n i q u e f o r dis cov ering localized  associations in segments of the data using clustering 
was discussed in[5  M i ni ng i n te r tr a n sa c tio n  association rules was first introduced in[6  w h ic h  extended aprori and extended hash apriori were proposed to deal with single and multidimensional inter transaction association rules. DMARG is presented in s es GA to dis cov er th e larg es t  itemsets of association rules in large dynamic database The clone selection principle is used to explain the basic features of an adaptive immune response to an 
antigenic stimulus. It establishes the idea that only those cells that recognize the antigens are selected to proliferate. The selected cells are subject to an affinity maturation process, which improves their affinity to the selective antigens. The general algorithm, named CLONALG, is primarily derived to perform machinelearning and pattern recognition tasks and then it is adapted to solve optimization problems[2 u t i t is  never used to dynamic mining of association rule 
In this paper, we use a computational implementation of the clone selection principle to solve the problem of mining association rules on dynamic database 2. Immune clone algorithm 2.1. The clone selection theory and the shapespace model When an animal is exposed to an antigen, some subpopulation of its bone marrow derived cells \(B lymphocytes\ respond by producing antibodies \(Ab 
Each cell secretes a single type of antibody, which is relatively specific for the antigen By binding to these antibodies \(cell receptors\d with a second signal from accessory cells, such as the T-helper cell, the antigen stimulates the B cell to proliferate \(divide\d mature into terminal \(nondividing\tibody secreting cells, called plasma cells The process of cell division \(mitosis\ generates a clone, i.e., a cell or set of cells that are the progenies of a single cell. While plasma cells are the most active 
antibody secretors, large B lymphocytes, which divide rapidly, also secrete antibodies, albeit at a lower rate On the other hand, T cells play a central role in the Proceedings of the 17th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê05 1082-3409/05 $20.00 © 2005 IEEE 


regulation of the B cell response and are preeminent in cell mediated immune responses, but will not be explicitly accounted for the development of our m Lymphocytes, in addition to proliferating and/or differentiating into plasma cells, can differentiate into long-lived B memory cells. Memory cells circulate through the blood, lymph and tissues, and when exposed to a second antigenic stimulus commence to differentiate into large lymphocytes capable of producing high affinity antibodies, pre-selected for the specific antigen that had stimulated the primary response The shape-space model aims at quantitatively describing the interactions among antigens and antibodies \(Ag-Ab T h e s e t of  f eat u r es t h at  characterize a molecule is called its generalized shape  The Ag-Ab representation \(binary or real-valued determines a distance measure to be used to calculate the degree of interaction between these molecules Mathematically, the generalized shape of a molecule  m er an antibody or an antigen, can be represented by a set of coordinates   1 2   m m m m L which can be regarded as a point in an L dimensional real-valued shape-space m  L S The precise physical meaning of each parameter is not relevant to the development of computational tools. In this work, we consider binary strings to represent the association rules 2.2.Immune clone algorithm for mining itemsets in large database In this work, we consider binary strings to represent the association rules. The immune clone algorithm has eight steps ,which are similar to the algorithm in u t  it has a little difference in 1-3 steps as follows Antigens and antibodies are assumed to be the same length L In the following application L is the length of the encoded transaction. an explicit antigen population Ag, which is made up of different transactions, is to be identified. Without loss of generality, it will be assumed the pre-existence of one memory antibody which is made up of string containing an itemset, from   m Ab to each antigen to be recognized 1. Randomly choose an antigen j Ag  j Ag  Ag  that is, a transaction with a transaction identifier, and present it to all antibodies in the repertoire     m r Ab Ab Ab    r  m  N  2. Determine the vector j Af that contains its affinity to all the N antibodies in Ab     n ransaction Noofitemst ts Nonmatchbi Matchbits Af j   1 where Matchbits is the No. of matching bits gbits Nonmatchin is the No. of non matching bits   n ransaction Noofitemst is the total number of items in the transaction n 3. Select the n highest affinity antibodies from Ab composing a new set j n Ab   of high affinity antibodies with relation to j Ag  The other steps are the same as the algorithm in After presenting all the M antigens\(all transactions from Ag and performing the eight steps above to all of them, we say a generation has been completed In our implementation, it was assumed that the n highest affinity antibodies were sorted in ascending order after Step 3, so that the amount of clones generated for all these n selected antibodies was given by \(2   n i i N round Nc 1   2 where Nc is the total amount of clones generated for each of the antigens  is a multiplying factor N is the total amount of antibodies and round   is the operator that rounds its argument towards the closest integer The clone selection process is driven by antibody antigen interactions. The evolution of the antibody population leads to maturation of the immune response, which features itself by an increase in the affinity. The affinity is important to the cloning expansion process Affinity maturation process is implemented by a relatively high mutation rate \(mutation rate of 10 over an antibody. The best antibody is maintained from last iteration to the next one 3. Algorithms for Association Rules We consider a sliding window of some specific size\(size is the number of transactions to be handled at a time\ generate inter-transaction association rules Mining intra transaction association rules, is the special case of inter transaction association rule mining when the window size is one. Two algorithms-DMARICA and DDMARICA inspired by en ted to s o lv e  the problem of mining associate rules on dynamic database 3.1. Problem Definition and Assumptions Consider a large database D B with horizontal layout consisting of a finite set of transactions     2 1 n t t t Each transaction is represented by a binary string of fixed Proceedings of the 17th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê05 1082-3409/05 $20.00 © 2005 IEEE 


length n where n is the total number of items present. Let an incremental set of new transactions be added to D B  The objective is to generate all the large inter transaction association rules in db DB  without involving D B using scan only once technique through an algorithm DMARICA Assumptions are as follows 1.A transaction is represented by a binary coded string 2.The items in the transaction are ordered 3.The transactions are generated by synthetic data generator 3.2. Algorithm DMARICA The algorithm DMARICA is 1.Initialize database D B  2.Read Window_size\(Window_Size 3.Prepare Database D B Window_Size 4.Initialize itemsets database I  5.Read an itemset i from the incremental database i db Update those Itemsets in I which share one or more common transactions with i 6.Disvover new itemsets Repeat step 4 Apply immune clone algorithm to member of I  i\Clone itemsets with high affinity values\(1- 4 steps of Immune clone algorithm ii\tate certain itemsets to obtain a new itemset, if complete set not found\(5-6 steps of immune clone algorithm iii\ the memory of high affinity itemsets and Updata itemsets until complete set found\(7-8 steps of immune clone algorithm 7.If end of database i db has not been reached, goto step 3 8.Identify large itemsets above minimum support 9.Generate all rules above the minimum confidence 10.If the incremental operation is to be repeated goto Step 1 Given n hosts     2 1 n h h h where each  1  n i h i   contains a local database i db  i db is mined to discover large itemsets local to i h and generate local association rules The local large itemsets are combined to generate global rules. The local databases are horizontally partitioned. Databases are statically distributed across all the nodes of the homogeneous network 3.3 Algorithm DDMARICA 1.The server waits for the request from the clients 2.Clients register themselves within the allotted time frame 3.Serve registers the clients which participate in mining process 4.Server requests the clients to check for the integrity of the executables required for distribute mining 5.Clients return status of association rules mined from the local database i db  6.Sever waits for clients to perform local mining on the database and places it in WAIT state 7.Clients interrupt server when the local mining on i db is completed and generates the local association rules using the algorithm DMARICA 8.Server requests the clients to send the entire rule antibody mined 9.Sever builds the universal database of rule antibody after receiving the complete set of association rules mined in all the registered clients 10.Server generates the global association rules from the result obtained from step 9 11.Server closes connections with all the clients 12.End 4. Performance Analysis Simulations were performed on a system of physical memory capacity 512MB and a Pentium IV processor of speed 1.5GHz. Transaction data, in the form of binary strings of desired length, was generated using a Synthetic Data Generator. The number of items, 25 and the average size of maximal potentially large itemsets was 4. DMARICA was compared with a standard algorithm, FUP\(Fast Update\which works on the principles of Apriori. And it was also compared with DMARG, which works on the principles of GA Figure 1 shows the plot of the execution time of DMARICA and DMARG versus the number of transactions. The transaction databases were varied in the range of[100k-300k h e s u pport con s i dered w a s 1. The low value has been explicitly chosen to exhaustively generate all the possible combinations of itemsets. The execution time varies from 1 minutes to 15 minutes. It is evident from Figure 1, that DMARICA can handle large databases without comprising performance and has excellent scalability And it consumes less time that DMARG does. It is also good at finding small itemset. The difference between DMARICA and FUP is due to the single scan and feature of DMARICA. Like the GA, the immune clone algorithm is highly parallel and presents a fine tractability in terms of computational cost Proceedings of the 17th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê05 1082-3409/05 $20.00 © 2005 IEEE 


 0 50 100 150 200 250 300 0 5 10 15 Number of Transaction Execution Time in Mins DMARG DMARICA 0 10 20 30 40 50 60 70 80 90 100 0 50 100 150 200 250 300 350 400 Number of Transaction Execution Time in Mins DMARICA FUP DMARG Figure 1 Execution time of DMARICA vs No Figure 2  Execution time vs increment database of transactions  intra transaction Figure 2. compares the execution times of DMARICA with the standard algorithm for incremental mining, FUP and also DMARG. The initial database D B was of size 100k.The incremental database i db was varied form 1% to 100% of D B The execution time for  DMARICA was observed to vary from 1.0 seconds to 110.1 seconds. Under identical conditions, the execution time of FUP varied from 9.7 seconds to 401.2 seconds. And DMARG varied from 0.8 seconds to 98.1. DMARICA runs a little faster than DMARG does.   In Figure 3,the variation of the execution time of DMARICA for various decrements of D B is also studied. The initial database D B considered is of size 100k. The decrements were varied from 10% to 50% of D B DMARICA is versatile in handling both increments and decrements to D B For inter transaction association rule the database of size 1,00,000 transactions with 300 items and the window size of three is taken. The execution time of DMARICA is compared with Extended-Apriori and DMARG. It can be seen that DMARICA is faster than DMARG and Extended-Apriori 0 5 10 15 20 25 30 35 40 45 50 0 50 100 150 200 250 300 Decrement in Executtion Time in Secs Extended-Apriori DMARG DMARICA 1 1.5 2 2.5 3 3.5 4 4.5 5 0.4 0.5 0.6 0.7 0.8 0.9 1 No.of Processors Execution time inMins DMARG DMARICA Figure 3  Execution time vs decrement database Figure 4  Execution time vs No.of processors inter transaction  distributed mining In case of distributed mining, simulations were performed on an Ethernet Local Area Network Database is distributed uniformly amongst all the processors. Figure 4 shows the variation of execution time in minutes versus the number of processors. The execution time decreases as the increase in the number of processors. The time taken for the synchronization is negligible. And DDMARICA consumes less time than DDMARG does. In the experiments, the database was kept constant and the number of processors were varied By comparing the proposed algorithm DMARICA with DMARG , FUP and Extended-Apriori, it can be noticed that the DMARICA is also good at such problem. It is mainly because immune clone algorithm can reach a diverse set of local optima solutions, while the GA tends to polarize the whole population of individuals towards the best candidate solution Proceedings of the 17th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê05 1082-3409/05 $20.00 © 2005 IEEE 


Essentially, their coding schemes and evaluation functions are not different, but their evolutionary search processes differ from the viewpoint of inspiration, vocabulary and sequence of steps, though immune clone algorithm embodies the 5 main steps required to characterize evolutionary algorithm So they can solve the same problem with  similar performance 5. Conclusions In this paper, the issue of mining intra and inter transaction association rules for dynamic database was examined The proposed algorithm is effective in generating large itemsets and in addressing updations in the database. The rules generated using DMARICA, are qualitatively sound. The single scan feature reduces computational costs significantly. The algorithm generates inter transaction association rules using the augmented transactions. DMARICA handles identical and skewed database The algorithms DMARICA and DDMARICA are effective and efficient. Efficient is because of the incremental speed and scalability of immune clone algorithm in performing data mining and a low communication overhead achieved when building global knowledge. It is effective because the quality of the rule antibody is not compromised in the process DMARICA uses a single scan technique. The support is required only before the association rule generation to build stronger and relevant rules Like the GA, the immune clone algorithm presents highly parallelism and a fine tractability in terms of computational cost At present, DDMARICA works on an antibody database. With certain modification, i.e., the exchange of details regarding the properties of the database before the mining process, the code can cater to a heterogeneous database as well Acknowledgement This paper is supported by National Nature Science Foundation of China,No.60305007, and Nature Science Foundation of Heilongjiang Province, China, No.F0210 and Basic Theory Research Project Foundation of Harbin Engineering University,No.HEUF04076 References   D eep a S h en o y  M  S r i v i d y a   K Kavi t h a K R V e nu go p a l  and  L.  M   Patnaik. ç Incremental   Mining   of   Association rules using  Genetic   algorithm Proc.   of   Intel.  Conf  on High Performance Computing 2002 2 L  N  de Ca s t ro a n d F. J   Vo n Zube n A rtif ic ia l  I m m une  Systems: Part II Ö A Survey  of  Applicationsé. Tech.  Rep RT DCA 02/00, pp. 65 3 H. M o   The   Principle   and   Application   of    Artificial Immune  System Harbin   Institute  of     Technology    Press Harbin,2002 4 R. Ra s t og i K   Shim  M ini n g O p tim iz e d A s s o c i a tion Rules     with       Categorical     and     Numeric    Attributes IEEE   Transactions    on   Knowledge  and Data Engineering 1\(14\,2002,pp.29-50 5 C. C A g g a r w a l Ce c ilia  P r oc opi uc a n d  Phili p S Yu Finding  Localized  Associations  in   Market  Basket Data IEEE Transaction on Knowledge and Data Engineering 1\(14\,2002.pp.51-62 6 H  L u  L  Fe ng a n d J  H a n  B ey ond Intra T ra ns a c tion Association    Analysis:   Mining   Multi-Dimensional   Inter Transaction     Association      Rulesé,  ACM   Transaction on Information Systems,18\(4\,2000,pp 423-454 7  P  D e e p a  K  G  She noy K  R Sriniv a s a V e n ug opa l a n d L.M.Patnaik,çEvolutionary Approach for Mining Association Rules    on  Dynamic  Databaseé,PAKDD    2003.   Springer Verlag,Berlin Heidelberg, 2003 Proceedings of the 17th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê05 1082-3409/05 $20.00 © 2005 IEEE 


 Insert function InsertN ode \(Ise1ected  Check if T has a child C Where Iselected.item-name=C.item-name then increase Count of the Node form Iselected by 1 Else Search appropriate Location for the Node Create A New Node As Node.item-name=Iselected.item-name  Decrease remaining frequency for Iselected by 1 Increase count of all items of the transaction T's in Count of related Item  2.3. Pattern Decomposition Algorithm The central idea of this approach is to signifIcantly reduce the dataset size for improving performance This algorithm shrinks the dataset when new infrequent item sets are discovered. More specifIcally, the PD algorithm uses a bottom-up search to fInd frequent sets It consists a set of passes starting from pass 1 for a given transaction dataset D1  Each pass of the PD algorithm, say pass k and Db has two phases. First frequent itemsets Lk and ?Lk are generated by counting for all k-itemsets in Dk. Second, PD-decompose algorithm is used to decompose Dk to get DB! such that DB! contain no itemset in ?Lk. The algorithm terminates at a pass z if Dz is empty Let us illustrate how a pattern in the dataset is decomposed on a specifIc pass, see Figure 1 \(in the next page Pass 1: Suppose we are given a pattern p = abedef: 1 E D! where a, b, e, d, e EL! andf ? L!. To decompose p with ?L], we simply deleteffromp, leaving us with a new pattern abede : 1 in D2   Pass 2: Suppose a pattern p = abede : 1 E D2 and ae L2  Since ae cannot occur in a future frequent set, we decompose p = abede : 1 to a composite pattern q abed, bede : 1 by removing a and e respectively from p Pass 3: Suppose a patternp = abed, bede : 1 E D3 and aed ? L3  Since aed . abed, abed is decomposed into abc, abd, bed. Their sizes are less than 4, so they are not qualifIed for D4  Itemset bede does not contain aed so it remains the same and is included in D4 In /3, we remove patterns if their sizes are smaller than the required size of the next dataset. Here, patterns abc and abd with sizes of 3 cannot be in D4 and are deleted In 6, when a part of a given pattern has the same itemset with another pattern after decomposition, we DJ abc del,' } f 1 b c [,t: lH abdh  2  4. t.1 1': t 5, ?:7 be   lede' f l'::I fS Oce Jb ac/ 3 Ll -1,1 {?"IU is lico JS Vee /,?1C/ 4 


is lico JS Vee /,?1C/ 4 4 ii' {IYl/ J ly J 'C \( t?.f j f,/ 4 ill/ 1 J 3 Iii f!/ 2 1 ahed, !&gt;cde  C1 b L&lt  a b d: 'lit 4: b c \(J e L3 -1?3 j  L 1  OCt is OCt 3 ;'acf..f/ 1 al  1 2 c.,:i:",I 'J l. -l J is OC:c Ii-..' Occ bC":.le}2 Figure 1. Pattern decomposition example combine them by summing their occurrence. Here bcde is the itemset of pattern 4 and part of pattern l's itemset after decomposition, so the final pattern is bcde: 2 in D4 Notably, the algorithm first counts for Lk and ?Lk and then decomposes patterns in each pass. It differs fundamentally from previous algorithms in that it avoids candidate set generation and reduces the dataset on each pass. Counting time is thus also reduced The PD-decompose Algorithm Input: An itemset s, its infrequent k-item set ?qk Output: Itemsets, the decomposition results of s PD-decompose\(itemset s, ?qk 1: if\(k=l 2: t = remove items in ?qk from s 3: else 4: build ordered frequency tree r 5: Sbs = Quick-split\( r 6: t = mapping Sbs to itemsets 7: return t The main objective of PD-decompose is to decompose an itemset s by its infrequent k itemsets. It consists mainly of two parts: 1 tree, 2 called Quick-split and returning the resulting itemsets A frequency tree is a tree whose nodes are items. In the tree, items at each level are ordered by the frequency of their occurrence at the level. The most frequent item at each level is placed first. For example suppose we are given a pattern p E D3 where p.IS abcdefgh. In the third pass, we find infrequent 3itemsets {aef, aeg, aeh, afg, ajh, agh, abe, abf, abg abh, ace, acf, acg, ach, ade, adf, adg, adh}. First, we build up a frequency tree, as shown in Figure 2. The first level consists of only a's. The second level consists of items e, I, g, and h, with e occurring the most at its level. The third level is constructed In similar fashion a e b b G 


G f d O??Et c f d tb ? f} g ? h c h d h Figure 2. A frequency tree example After we built a frequency tree, we then use the Quick-split technique to calculate the maximal frequent sets The Quick-split algorithm is given here Input: A frequency tree r Output: An array of BitSets representing Itemsets Quick-split\(Tree r 1: if\(r is leaf   2: for all x E r.subs do 3: subres[x] = Quick-split\(x x 4: result =newBSO 5: for all x E r.subs do 6: result = result &amp; subres[x 7: remove b E result, b.size:::; k 8: return result To speed up calculation, an itemset is represented by a bitset with 0 and I for specifying the absence or presence of an item at a corresponding position respectively. The Quick-split performs a calculation on a frequency tree and returns an array of bitsets, which represent a group of decomposed itemsets. Splitting is accomplished by calculating bitset results in a bottom  up fashion in the tree. In the above example, we have 8 items a, b, . . .  , h corresponding to positions 0-7 in a 8bit bitset. So p.JS = abcdefgh = {I III I Ill}; abcd ll1 10000}; bcdefgh = {OllillII}. The size of the bitset is the number of items in p.JS which is usually much smaller than the total item size in the dataset For the frequency tree in Figure 2, for the itemset abcdefgh and infrequent 3-itemsets {aef, aeg, aeh, afg ajh, agh, abe, abf, abg, abh, ace, acf, acg, ach, ade adf, adg, adh} , Quick-split returns the possible maximal frequent sets {abcd, bcdefgh The PD Algorithm Input: transaction dataset T Output: frequent patterns PD \( transaction-set T I : D 1 = {&lt;t, 1&gt;1 t E T}; k= I 2: while \(Dk   3: for all p E Dk do II counting 4: for all k-itemset s c;;; p.JS do 5: Sup\(s IDk 6: decide Lk and ?Lk Ilbuild Dk 7: Dk+]= PD-rebuild\(Dk, Lk , ?Lk 8: k 9: end 10: Answer = u Lk As shown above, PD is the top-level function that accepts a transaction dataset as its input and returns the union of all frequent sets as the result. At the kth pass steps 3-6 count for every k itemset of each pattern in Dk and then determine the frequent and infrequent sets Lk and ? Lk; step 7 uses Dk , Lk and ?Lk to rebuild Dk PD stops when Dk is empty The PD-rebuild Algorithm Input: Dataset Db frequent Lk, infrequent ?Lk Output: Dataset DB PD-rebuild \(Dlo Llo ?Lk 1: Dk  ht = an empty hash table 2: for all p E Dk do begin 3: / / q k, ?q k can be taken from previous counting qk={sls E p. JS nLk}; ?qk={tltE p. JS n ?Lk 4: u = PD-decompose\(p.JS, ?qk 5: v ={s E ul s is k-item independent in u 


5: v ={s E ul s is k-item independent in u 6: add &lt;u-v, p.Occ&gt; to Dk+1 7: for all s E v do 8: if sin ht then ht. s. Occ += p. Occ 9: else put &lt;S,p.Occ&gt; to ht 10: end 11: Dk+] = Dk+] u {p E ht The PD-rebuild shown above is to determine DB by Dk, Lk and ?Lk' For each pattern p in Db step 3 computes its qk and ?qk; step 4 calls PD-decompose algorithm to decompose p by ?qk. Note that qk is not used here for decomposing p. In steps 5 to 9, we use pattern separation rule to separate p. In steps 7 to 9 PD-rebuild merges the patterns separated from p with their identical ones via a hash table ht. Since PD follows the pattern decomposition rule to decompose patterns and the pattern separation rule for merging identical patterns that yield same support, the answers generated by PD are correct 3. Comparative Study Our experiments were performed on a 600 MHz Pentium PC machine with 256 MB main memory running on Microsoft Windows XP. All three algorithms were written in C++ language. The test dataset was generated in the same fashion as the IBM Quest project . We used dataset T25.I10.DIOOK . In the dataset, the number of items N was set to 1000. The corruption level for a seed large itemset was fixed obtained from a normal distribution with mean 0.5 and variance 0.1. In the dataset, half of all items were corruptible. In the dataset, the average transaction size ITI and average maximal potentially frequent itemset size III are set to 25 and 10, respectively, while the number of transactions IDI in the dataset is set to lOOK 3.1. Comparison of PD with DCP  Pattern Decomposition algorithm does not need to generate candidate sets which are the main improvement on DCP; the reduced dataset contains only itemsets whose subsets are all frequent  PD generates all frequent sets whether it is not certain in DCP. PD significantly reduces the dataset in each pass by reducing the number of transactions and their size to give better performance. So counting time is clearly less than DCP in a reduced dataset  PD is much more scalable than the DCP 8000 7000 6000 5000 lt;.i OOO E f OOO 2000 Minimum Support PD DCP Figure 3. Performance comparison 8 6 4 2 PD DCP owa_L??_L??_L??_L??_L??_L?? o 50 100 150 200 250 Number of Transactions\(k Figure 4. Scalability comparison Figure 3 shows the execution times for different 


Figure 3 shows the execution times for different minimum support. We can see that PD is about 15 times faster than DCP with minimal support at 2% and about 8 times faster than DCP at 0.25%. In Figure 4, to test the scalability with the number of transactions experiments on dataset D are used. The support threshold is set to 0.75 3.2. Comparison of PD with PIP  Predictive item pruning FP-tree algorithm has a complicated data structure in comparison to PD algorithm  PD works by decomposing transactions into short itemsets. Then regular patterns are combined together. Hence data set is being reduced at every pass. As a result, space efficiency is improved. On the other hand, PIP FP-tree, although prunes infrequent items, its hit ratio is not as efficient as PD. So, obviously its space efficiency is not better than PD  As PD always concerns with the current data set less time is required in comparison to PIP FP-tree algorithm  PD is more scalable than the PIP 100r  E F  60 40 0 20 PD  PIP Minimum Support Figure 5. Performance comparison 500 r 400 300 E F  oo 0 100     Number of Transactions\(k   PD  PIP   Figure 6. Scalability comparison 160 In Figure 5, both PIP and PD have good performance on D. But PIP takes substantially more time when minimum support in the range from 0.6% to 2%. When minimum support is less than 0.6%, the number of frequent patterns increased quickly and thus the execution times are comparable. In Figure 6, we compared the scalability of PD with PIP on the dataset D with minimum support = 0.75%. PD was clearly more scalable than that of PIP 4. Conclusion The programs were implemented in C++ very efficiently. Since really large datasets or data warehouse was not available for us, we run the programs of these three algorithms by using test datasets. So performance comparisons are not the absolute values. The results can vary on other 


absolute values. The results can vary on other computers. But it can be guaranteed that performance ratio of the algorithms will remain the same After making the comparisons with sample data, we came to the conclusion that PD algorithm performs significantly better than the other two especially with larger datasets. PD outperforms DCP and PIP regarding running time. On the other hand, since PD reduces the dataset, mining time does not necessary increase as the number of transactions increases and experiments reveals that PD has better scalability than DCP and PIP. So, PD has the ability to handle the large data mine in practical field like market basket analysis and medical report documents mining 5. References 1] R. Agrawal and R. Srikant, "Fast algoritlnns for mining association rules", VLDB'94, pp. 487-499 2] R. J. Bayardo, "Efficiently mining long patterns from databases", SIGMOD'98, pp.85-93 3] J. Pei, J. Han, and R. Mao, "CLOSET: An Efficient Algorithm for Mining Frequent Closed Itemsets \(PDF Proc. 2000 ACM-SIGMOD International Workshop on Data Mining and Knowledge Discovery, Dallas, TX, May 2000 4] Qinghua Zou, Henry Chiu, Wesley Chu, David Johnson, "Using Pattern Decomposition\( PD Finding All Frequent Patterns in Large Datasets", Computer Science Department University of California - Los Angeles 5] J. Han, J. Pei, and Y. Yin, "Mining Frequent Patterns without Candidate Generation \(PDF  SIGMOD International Con! on Management of Data SIGMOD'OOj, Dallas, TX, May 2000 6] S. Orlando, P. Palmerini, and R. Perego, "The DCP algoritlnn for Frequent Set Counting", Technical Report CS2001-7, Dip. di Informatica, Universita di Venezia 2001.Available at http://www.dsi.unive.itl?orlando/TR017.pdf 7] MD. Mamun-Or-Rashid, MD.Rezaul Karim, "Predictive item pruning FP-tree algoritlnn", The Dhaka University  Journal of Science, VOL. 52, NO. 1, October,2003, pp. 3946 8] Park, J. S., Chen, M.-S., and Yu, P. S, "An Effective Hash Based Algoritlnn for Mining Association Rules", Proc ofthe 1995 ACM-SIGMOD Con! on Management of Data 175-186 9] Brin, S., Motwani, R., Ullman, J., and Tsur, S, "Dynamic Itemset Counting and Implication Rules for Market Basket Data", In Proc. of the 1997 ACM-SIGMOD Conf On Management of Data, 255-264 10] Zaki, M. J., Parthasarathy, S., Ogihara, M., and Li, W New Algoritlnns for Fast Discovery of Association Rules In Proc. of the Third Int'l Con! on Knowledge Discovery in Databases and Data Mining, 283-286 11] Lin, D.-I and Kedem, Z. M., "Pincer-Search: A New Algoritlnn for Discovering the Maximum Frequent Set", In Proc. of the Sixth European Conf on Extending DatabaseTechnology, 1998 12] R. Ramakrishnan, Database Management Systems University of Wisconsin, Madison, WI, USA; International Edition 1998 pre></body></html 


tors such as union, di?erence and intersection are de?ned for pairs of classes of the same pattern type Renaming. Similarly to the relational context, we consider a renaming operator ? that takes a class and a renaming function and changes the names of the pattern attributes according to the speci?ed function Projection. The projection operator allows one to reduce the structure and the measures of the input patterns by projecting out some components. The new expression is obtained by projecting the formula de?ning the expression over the remaining attributes [12 Note that no projection is de?ned over the data source since in this case the structure and the measures would have to be recomputed Let c be a class of pattern type pt. Let ls be a non empty list of attributes appearing in pt.Structure and lm a list of attributes appearing in pt.Measure. Then the projection operator is de?ned as follows ls,lm c id s m f p ? c, p = \(pid, s, d,m, f In the previous de?nition, id ing new pids for patterns, ?mlm\(m projection of the measure component and ?sls\(s ned as follows: \(i s usual relational projection; \(ii sls\(s and removing the rest from set elements. The last component ?ls?lm\(f computed in certain cases, when the theory over which the formula is constructed admits projection. This happens for example for the polynomial constraint theory 12 Selection. The selection operator allows one to select the patterns belonging to one class that satisfy a certain predicate, involving any possible pattern component, chosen among the ones presented in Section 5.1.1 Let c be a class of pattern type pt. Let pr be a predicate. Then, the selection operator is de?ned as follows pr\(c p Join. The join operation provides a way to combine patterns belonging to two di?erent classes according to a join predicate and a composition function speci?ed by the user Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE Let c1 and c2 be two classes over two pattern types pt1 and pt2. A join predicate F is any predicate de?ned over a component of patterns in c1 and a component of patterns in c2. A composition function c pattern types pt1 and pt2 is a 4-tuple of functions c cStructureSchema, cDataSchema, cMeasureSchema, cFormula one for each pattern component. For example, function cStructureSchema takes as input two structure values of the right type and returns a new structure value, for a possible new pattern type, generated by the join. Functions for the other pattern components are similarly de?ned. Given two patterns p1 = \(pid1, s1, d1,m1, f1 p2 = \(pid2, s2, d2,m2, f2 p1, p2 ned as the pattern p with the following components Structure : cStructureSchema\(s1, s2 Data : cDataSchema\(d1, d2 Measure : cMeasureSchema\(m1,m2 Formula : cformula\(f1, f2 The join of c1 and c2 with respect to the join predicate F and the composition function c, denoted by c 1   F  c  c 2   i s  n o w  d e  n e d  a s  f o l l o w s    F  c  c 2     c  p 1   p 2   p 1    c 1  p 2    c 2  F   p 1   p 2     t r u e   5.1.3. Cross-over database operators OCD Drill-Through. The drill-through operator allows one to 


Drill-Through. The drill-through operator allows one to navigate from the pattern layer to the raw data layer Thus it takes as input a pattern class and it returns a raw data set. More formally, let c be a class of pattern type pt and let d be an instance of the data schema ds of pt. Then, the drill-through operator is denoted by c c Data-covering. Given a pattern p and a dataset D sometimes it is important to determine whether the pattern represents it or not. In other words, we wish to determine the subset S of D represented by p \(p can also be selected by some query the formula as a query on the dataset. Let p be a pattern, possibly selected by using query language operators, and D a dataset with schema \(a1, ..., an ible with the source schema of p. The data-covering operator, denoted by ?d\(p,D responding to all tuples in D represented by p. More formally d\(p,D t.a1, ..., t.an In the previous expression, t.ai denotes a speci?c component of tuple t belonging to D and p.formula\(t.a1, ..., t.an instantiated by replacing each variable corresponding to a pattern data component with values of the considered tuple t Note that, since the drill-though operator uses the intermediate mapping and the data covering operator uses the formula, the covering ?\(p,D D = ?\(p not be equal to D. This is due to the approximating nature of the pattern formula 5.1.4. Cross-over pattern base operators OCP Pattern-covering. Sometimes it can be useful to have an operator that, given a class of patterns and a dataset, returns all patterns in the class representing that dataset \(a sort of inverse data-covering operation Let c be a pattern class and D a dataset with schema a1, ..., an pattern type. The pattern-covering operator, denoted as ?p\(c,D all patterns in c representing D. More formally p\(c,D t.a1, ..., t.an true Note that: ?p\(c,D p,D 6. Related Work Although signi?cant e?ort has been invested in extending database models to deal with patterns, no coherent approach has been proposed and convincingly implemented for a generic model There exist several standardization e?orts for modeling patterns, like the Predictive Model Markup Language \(PMML  eling approach, the ISO SQL/MM standard [2], which is SQL-based, and the Common Warehouse Model CWM  ing e?ort. Also, the Java Data Mining API \(JDMAPI 3] addresses the need for a language-based management of patterns. Although these approaches try to represent a wide range of data mining result, the theoretical background of these frameworks is not clear. Most importantly, though, they do not provide a generic model capable of handling arbitrary cases of pattern types; on the contrary only a given list of prede?ned pattern types is supported To our knowledge, research has not dealt with the issue of pattern management per se, but, at best, with peripheral proximate problems. For example, the paper by Ganti et. al. [9] deals with the measurement 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


