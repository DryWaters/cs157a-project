Intersection Based Generalization Rules for the Analysis of Symbolic Septic Shock Patient Data Jiirgen Paetz J.W Goethe-Universitat 2003 Biologie und Informatik Institut fur Informatik D-60054 Frankfurt am Main paetz@cs.uni-frankfurt.de www.informatik.uni-frankfurt.de/-paetz Abstract In intensive care units much data is irregularly recorded Here we consider the analysis of symbolic septic shock pa tient dara We show rhar ir could be worth considering rhe generalizarion paradigm \(individual 
cases generalized to more general rules instead of the association paradigm combining single attributes\when considering very indi vidual cases e.g parients and when expecting longer rules than shorter ones We present an algorirhm for rule gen erarion and classification based on heurisrically generated set-based intersections We demonstrate the usefulness of our algorirhm by analysing our septic shock patient data 1 Introduction The septic shock is of prime importance in intensive care medicine l 
Most of the data in our database was symbolic data therapies operations medication admission data Since the patients are very individual in their behavior the patient data is very inhomogenous Although association rules a-priori 121 are a common tool for analysing data we tried a mechanism just the other way round we generalize the patient data beginning with the individual cases We claim that it is worth and in fact reliable considering this generalization paradigm see e.g 31 Our algorithm computes heuristically a kind of closed frequent itemsets 
141 but not all of them and in a different more natural way using no backtracking or generators 5 In Sect 2 we present the new set-based heuristic gen eralization algorithm \223GenIntersect\224 151 for classification able to handle uncertain data with many attributes when ex pecting rather longer than shorter rules that should used not only for classification but also for medical interpretation A winner-takes-all classification procedure that is based on a new measure \223weighted confidence\224 is presented in Sect 3 With the help 
of this measure we evaluate the importance of the attributes and select features In Sect 4 we discuss the main differences of our ap proach and the a-priori approach mainly the robustness and the rule-context characteristics We apply our algorithm in Sect 5 to our symbolic septic shock patient database Additionally we want to emphasize that our aim was not the comparison of several algorithms by evaluating bench mark data although this is an important task Our aim is to show that the fundamental idea 
of generalization to gether with classification and feature selection\is useful in the medical domain 2 Intersection Based Generalization Rules Let us consider an example with two sets of items II ABC\(shonforAABAC BCD,e.g.A 223green\224 B  223big\224 C  2234 doors\224 and D  223fast.\224 Ii and I2 describe the characteristics of two entities e.g. two cars What are the common characteristics of the two cars Both cars are big and have 4 doors Only car 1 
is big and car 2 is fast In set theoretic language we have generated an intersection of Ii and 12 1 n I2  ABC n BCD  BC Thus intersection theory is the natural access to rule generation Of course we can consider different classes for our itemsets The sets of items need not to have the same number of elements Hence a missing item causes no problems when calculating intersections Also we can reduce the number 
of attributes in one step much more than only one attribute The intersection operator could be interpreted as a kind of recombination operator known from evolutionary computing Example 2.1 Let us consider the 8 itemsets ABCD AEFG BEFG CEFG DEFG BE CF and DG In 4 the closure cl\(1 of an item with regard to itemset I is defined as the 0-7695-1754-4102 17.00 Q 2002 IEEE 673 


intersection of all the itemsets that contain I Thus the clo sure of A is cl\(A  4BCD n AEFG  A If cl\(I  I then an itemset I is called closed In our example A is closed Using the algorithm 4 we have to compute all the closures of frequent itemsets successively for itemsets of length 1 2 3  Thus the algorithm 4 computes as much candidates as a-priori Using our intersection paradigma building intersections directly from samples we have a probability of 6/\(7+6+5 4 3+2  1  6/28 FZ 0.21 for generating the closed more interesting, itemset EFG directly The probability for generating the closed less interesting item 4 is only 1/28 This example emphasizes that it is plausible and reliable to generate heuristically gen eralizations instead of generating a-priori-like candidates In the following we denote a finite set of itemsets as ZS We will consider datasets where every sample is an item set Of course identical samples may exist We assume that identical samples are storaged only one time in the database together with the frequency for every class We call an in tersection K of two sets I J nontrivial if K  I K  J and K  0 We use the performance measures \223frequency\224 and \223confidence\224 for rules I e c I E ZS c a class la bel as introduced in Z and we write freq\(I  c resp mf\(I  c 2.1 Generalization Algorithm Our aim is finding rules with high performance mea sures, obtaining a good classification by the rules that we generate and a reliable force of expression of the rules for physicians The following algorithm that is based on the principle of intersections uses the set IS of all training itemsets as a starting point Algorithm 2.2 Genlntersect Input parameters Set of itemsets ZS maxlevel yz mini mum thresholds for the performance measures Output parameters ZSF generalized rules, including the initial rule itemset IS level, startlevel I initialization  ZS level  1 startlevel\(leve1  1 endofalg  false 2 generate level while endofalg  false do startlevel\(level 1  Soew oldIS  flZS pass through actual level fori  startlevel\(level to startlevel\(level+l pass through itemsets without considering itemsets of the preceding levels forj  i  1 to startlevel\(level+l Inter  TSnev\(i n ZSnew\(j if Inter is a nontrivial intersection and Inter ZS then ZS,,,\(#zS  1  Inter end if end for j end for i 3 check termination if zS  oldIS or level 2 maxlevel then endofalg  me ISF  ISnew else end end while 4 filter all the rules that have performance measures level  level fl higher then all 7 2.2 Heuristic Extensions A disadvantage of the algorithm GenIntersect is the com binatorial explosion Monotonic frequency pruning is not possible because the frequency is not monotone with re spect to the generalization process Thus we should use adaptive heuristics This makes sense because we do not need all the optimal rules for classification It is sufficient to go on with generalization until not much more rules are generatedor until the classification result gets not much bet ter within a level We introduce the outergeneralization index to determine online if another generalization level makes sense Addi tionally our inner generalizarion index determines within one level when an intersection process could be stopped so that it can proceed with the next level The idea of the inner generalization index is the calcula tion of a sliding mean using the number of newly generated itemsets per itemset If this sliding mean value M becomes too small we stop the intersection process within the actual level and proceed with another level As our experiments showed this is very performant in combination with a maximal number muznew of itemsets that one itemset is allowed to generate Definition 2.3 We call the sliding mean M mentioned above the inner generalization index G\(\223\221  M j How many levels do we need A heuristic answer is our outer generalization index Definition 2.4 Let m be the maximal possible number of new itemsets within one level and e the number of effectively generated new itemsets, that are not nontrivial and not already gener 674 


ated. Then, we define the nuter generalization index as If G\(O\224\222 E O 11 falls under a predefined threshold then the algorithm terminates If the index is approx 1 then the algorithm should go on with another level More properties use of negotiations optimality of our algorithm can be found in SI 3 Classification Now we describe how we can classify a new test itemset ZS using a generalized itemset R representing a rule basis One problem always arising in this situation is the different a-priori probability e.g 80 class 1 samples and 20 class 2 samples The confidence measure is related to the a-priori percentage and is therefore not suitable for a classifier We introduce an elegant way to solve this problem using an extension of the confidence the weighred confidence Definition 3.1 The weighted confidence wcoli f for class c could he de fined using the confidence together with the class propor tions or easier as the proportion of the c-frequency class c coverage  frequency!\considering all &frequencies The weighted confidence is ideal for a winner-takes-all classification The best fitting rule with respect to the weighted confidence specifies the class We call our algorithm 223WeCoCI\224 abbr for 223Weighted Confidence Classifier:\222 It is a classifier for itemsets with a multiple class alignment Algorithm 3.2 WeCoCI Parameters ZS all itemsets that will be classified R with GenIntersect generated set of rule itemsets filtered with the performance measures 1 for all itemsets of the test itemsets calculate the sets of all 223containing\224 weighted cd-confidence values for all itemsets I E ZS do for all classes d do WCONFd  8 for all itemsets rules J E R do if J c I then end if end for all J end for all I WCONF  WCONFdU{d-,~collf\(J 2 calculate the maximal weighted d-confidence per class these values are noted as muzWCONFd 3 Be 7iluzi71d  i71dez\(maxd{mu~L.WCONFd and SiTldez  i7ld~z\(~laXd,d2,,,i,d{7~lUzWCONFd if mazW\221CONF,,,;,d  ILuxWCONF  E then classify I as class mnzind otherwise we classify I as 223not classifiable.\224 This is always the case if no itemset of the rule set is contained in I end 4 Discussion Generalization vs Association A-priori generates great many superfluous rules Let us consider the rules ABC BCD both of the same class e GenIntersect generates only the additional rule BC A-priori considers the items 4 B C and D of length I and then the combinations of length 2 4D is an itemset that is not contained in ABC or BCD Frequent is the itemset BC but also are the items B and C We do not need to generate all frequent itemsets for classification It leads to an effect we call context smearing resulting from the association process starting with the items while the context is fixed by the itemsets Example 4.1 Let B be the item \223high blood pressure\224 and C 223pH value.\224 The important knowledge is the combinntion of both items BC If we would choose only B or C as a classification rule we claim that a high blood pressure or a low pH value are standalone interesting This may be the case but it need not to be If we present only B as a generalized rule to a physician then he could conclude in a complete false way he could think that giving a blood pressure lowering medicament is a benefit for the patient although this medicament would again lower the pH value for the patient\222s disadvantage because he was not aware of the complete information BC A longer generalization rule is more useful to a physician How senseful is a rule set for the classification of un known data i.e. how robust is a-priori resp GenIntersect Example 4.2 Let us consider again the itemsets ABC und BCD of the same class e GenIntersect generates BC an a-priori classifier would choose the rule B or C Assume the unknown itemset to classify is 4B It is BC q 4B i.e AB would be classified as 223not classifiable.\224 In fact AB could be of another class than BC so that this decision is justified But it is B c AB i.e AB would he classified as class d although AB need not to he a class d itemset There is no reason at all to classify in such a way If we choose C instead of B which seems to be an equal choice we have C  4B Thus in fact B is not an equal choice compared to C if we expect unknown itemsets the usual case in medicine due to individual patients 675 


I I mean result I 96.70141.57 Table 1 Results of the rule generation with Genlntersect and WeCoCI Shorter a-priori rule sets are not robust due to context smearing We state that in the medical domain e.g rules for therapy planning\generalization rules are more reliable than association rules Shorter association rules may even be harmful for patients 5 Septic Shock Patient Data Septic shock is of prime importance in intensive care medicine 111 Our database consists of 362 septic shock patients The data of each patient was given as admission data e.g chronic diagnoses\and daily measurements e.g acute diagnoses, medication and therapies We made three complete experiments with the patient data The results were mean results of these experiments We used the heuristics of Sect 2.2 Initially we used 50 of the data for training our system the other 50 for calcu lating the test results Let us abbreviate the class survived by class s and the class deceaed by class d We made a first run of our algorithm with all the 96 items After filtering the rules by the frequency and con fidence thresholds 23 tules remained Calculating the im portance of the items by only 60 of the 96 items had an importance  0 for one of the two classes Only 27 items had an importance value that was noticeable  0 With this 27 items we made three repetitions of the entire experiment The mean results on the test data is shown in Tab 1 The complete parameter and heuristics setting is documented in 51 The NO items e.g not thrombocyte concentrate not en doscopy, not min 3 antibiotics given appeared more often for class s the YES items e.g. respiration catecholamines given more often for class d An example for a rule of class d is if not traumatic and number of organ failures  2 and respiration and haemofiltration and not dialysis and catecholamines given then class d with wcmi f  89.73 and frey  G.G3 A comparison of Genlntersect to a-priori can be found in 51 GenIntersect needs less memory 2MB instead of approx. 500MB For class d a-priori was not reliable since for class d rules we need frequency thresholds less than 2 6 Conclusion We presented an algorithm for generalizing itemsets. Us ing heuristics it is reliable to generalize even larger data sets The algorithm is very useful if classification tasks are considered We confirmed our inventions by application to important medical real world data the septic shock ICU pa tient data A fundamental comparison to association rules was given It showed that it does make sense to general ize instead of associate due to the two big problems con text smearing and robustness two important circumstances in medical applications The whole work is documented in greaterdetail in 5 Work for the future could be the invention of even more suitable adaptive heuristics Acknowledgment The medical application was supported by the DFG German Research Foundation References I Hanisch E Encke A Intensive Care Management in Abdominal Surgical Patients with Septic Complica tions in E Faist ed Immunological Screening and Immunotherapy in Critically Ill Patients with Abdom inal Infections. Springer-Verlag 2001 71-138 21 Agrawal R and Skrikant R Fast Algorithms for Mining Association Rules Proc 2Oth Int Conf on Very Large Databases \(VLDB\\(1994 487499 31 Mitchell T.M Machine Learning McGraw-Hill  1997 41 Bastide Y et al Mining Minimal Non-Redundant Association Rules Using Frequent Closed Itemsets Proc 1 Int Conf on Computat Logic CL 6'h Conf on Database Systems DOOD 2000 972-986 5 Paetz I Durchschnittsbasierte Generalisierungs regeln Teil I  11 Frankfurter Informatik-Berichte NI 1  2/02, Institut fur Informatik FB Biologie und In formatik J.W Goethe-Univ Frankfurt am Main Ger many ISSN 16 169107 2002 676 


formed by the 2-subitems of AvBvD and BVCVD line 12 Thus CC2  AV B,A V D B V C,B V D In complete A v B is extended with C and D respectively Thus for A V B in complete S  AV B v C,A V B V D As A V B V C is not in LC3 A v B v C is not large As a result A V B cannot be large Thus A V B is removed from CC2 Similarly A v D and B v C are also removed from CC2 Hence after checking against the items in LC3 CC2 is B V D At step 3 LC2 is B V D As all the composite items have been checked the algorithm terminates Thus LCI  LCq U LC3 U LC2  AvBvCvD AVBv D BVCv D BVD 3.3 Finding Large Itemsets Once all the large items are found a procedure based on the algorithm in 3 is used to find large itemsets Modifications to 3 have been made to ac commodate composite items In the algorithm here each large composite item is treated as an indepen dent item like the large atomic items The algorithm finds the large itemsets in several steps At each step a candidate set is formed first The set contains the itemsets which might be large The candidate set is generated according to the large itemsets found at the pervious step After the can didate set is formed the database is scanned to find the large itemsets Modifications to the procedure in 3 have been made to reduce the amount of com putation required In the following A is the set of all the atomic items Lk is the set of large itemsets obtained at step k of the algorithm Ck is the set of candidate large itemsets and Ck is generated in procedure can dzdate-gen 20 L1  a I a E A and a is a large atomic item U ca I ca E LCI 21 for I  2 Lk-1  0 I   do 22 Ck candidate-gen\(Lk-l 23 24 CCt  0 25 26 27 28 fi 29 endfor-each 30 for each transaction t in database do for each itemset c E Ck do if t contains all the items in c then cct  cct U c for each itemset c E CCt do c.count end-for-each 31 endfor-each 32 33 endfor 34 answer Uk Lk Lk  c E Ck I c.count 2 minimum support Initially the large itemsets are formed using the large items line 20 The database is scanned in lines 23-31 Each transaction in the database is checked to see whether it supports the candidates in Ck Set CCt contains all the itemsets being sup ported by transaction t lines 25-29 When scan ning is completed the itemsets whose support ex ceed the minimum support become the large,item sets line 33 These large itemsets are used to gen erate the candidate large itemset for the next step line 22 At the end the large itemsets found at different stages are joined together line 34 Procedure candidategen\(lk-1 35 S  SI  si I sj E Lk-1 where 1 5 j 5 i and Vsm,sn  1 5 m  n 5 i.s and s are different in two items and the two different items do not have common atomic items and  3e E S.{SI  si c e such that I B I k where B  SI U s2 U  U sk where B  SI U s2 U  U Sk 36 5\222\222  SI  sg I SI  sk E S 37 Ck  u~,...,Q I VU  15 i 5 k.a E B such that sl  sk E SI Procedure candidate-gen calculates the large itemset candidates according to the large itemsets obtained at the previous step Each candidate in set Ck is a k-itemset If a k-itemset say S is large then each of the k-1 of S should also be large 3 If one of the k-1 of an k-itemset is not large than the k-itemset is not large 3 In candidate-gen the large k-1 obtained at the previous step are checked to see whether they are the k-1 of some k-itemsets A k-itemset becomes a candidate if all its k-1 are large In candidate-gen firstly several sets of itemsets are formed line 35 Each set will be checked to see whether it contains all the k-1 of a 12 itemset The second conjunct in the condition on line 35 means all the itemsets in SI  si are the k-1 of a k-itemset This is because each pair of the k-1 of a k-itemset are different in two items In addition according to the defini 1371 


tion in 2 each atomic item should appear in an itemset only once e.g A V B,A v C is not a valid itemset as A appears twice Thus two dif ferent items should not have common atomic items Condition 221\221 3e E S.{SI  s C e\224 in line 35 means the subset of any element in S should not be in S e.g if A B A C B C is in S then A B A C should not be in S Each k-itemset has k distinct k-1 Hence in S only the sets which contain exactly k itemsets need to be considered These sets are used to form S\222 line 36 If a set in S\222 say ss with k ele ments itemsets\has k distinct items i.e  B I k in line 3G then ss must contain all the k-1 of a k-iteniset As the itemsets in ss are from Lk-1 all the itemsets in ss are large As a result the k-itemset becomes a candidate line 37 Here is an example showing how candzdate-gen works Assume that L2  A B A B v C A 01 B v c D From line 35 S  A B A,D A B V C A 01 B V Cl D A,B},{A,BvC}}isnotinS,as BandBVC A,D},{BvC,D isnot in S as A,D have B in common BVC D C ABVC A 01 BVC D holds Each candidate in C is a 3-itemset i.e each candi date has three items A B A D only has two elements Thus A B A D cannot contain all the 2-subsets of any Pitemset As a result A B A D will not be considered further As only A B V C A D B V C D has three elements and contains three distinct items i.e A B v C and D according to line 36 S\222  A,B~C},{A,~},{B~C Hence, from line 37 the candidate in C3 is A B V The procedure generating the candidates in 3 consists of two phases First in the join phase the large itemsets generated at the k-1 step are 223joined\224 together to form k-itemsets Then in the prune phase the k-1 of all the k-itemsets gen erated at the join phase are calculated If a k-1 subset of a k-itemset is not one of the large itemsets obtained at the previous step then the k-itemset is removed from the candidate set In contrast procedure candzdate-gen in this paper only adds a k-itemset to the candidate set if all the C 0 k-1 of the k-itemset are large This avoids generating any k-itemset which will be deleted later Also it does not need to calculate the k-1 of any k-itemset Thus the scheme in this paper requires less computation than the one in 3 4 Conclusions The algorithm in this paper allows large itemsets to contain composite items This enables certain useful rules to be discovered in some applications Other work has presented algorithms for mining association rules in the presence of taxonomies over the items 5 GI In those algorithms a transaction is regarded as supporting the upper level of a hier archy if the transaction supports at least one item which is at a lower level in the hierarchy The algo rithms require the users to provide taxonomies If it is intended to investigate the possible relationships between some composite items e.g A V B,A v C and B V C and some atomic items then either sev eral taxonomies have to be provided or each pair of A B and C has to be placed under a hierarchy in a taxonomy In contrast the algorithm in this paper does not require the users to provide a taxonomy The users need only provide the set of atomic items in which they are interested From this set the composite items will be generated by the algorithm automati cally Thus the algorithm in this paper is both easier to use and more flexible References R Agrawal T Imielinski and A Swami, Mining association rules between sets of items in large databases Proc of ACM SIGMOD Conference R Agrawal and J Shafer Parallel mining of as sociation rules IBM Research Report RJl0004 1996 R Agrawal and R Srikant Fast algorithms for mining associations rules Proc of 10th Int Con ference on VLDB 1994 R Agrawal and R Srikant Mining sequential patterns Proc of 11th Int Conference on Data Engzneerang 1995 M Holsheimer M Kersten H Mannila and H Toivonen, A perspective on database and data mining Technical report CS-R9531 CWI The Netherlands 1995 R Srikant and R Agrawal, Mining generalised association rules Proc of 21st Int Conference on VLDB 1995 pp 207-216 1993 1372 


Figure 3 Current beneflts and estimated final benefits when sampling size k Increases up to K  256 for all three datesets. The error range Is 3 IK for 99.7 contldence ILI Im Y na 1 I Y rm Y na no lunpwS*.INmmnCi.m IO im In na 2s sb Wdd C.ul*\223l funm Sk.lNlunmolC.ul mation methods can effectively detect the inaccuracy of the complete model, the user can choose a smaller K We par titioned all three dataset into K  1024 partitions For the adult dataset, each partition contains only 32 examples but there are 15 attributes The estimation results are shown in Figure 4 The first observation is that the total benefits for donation and adult are much lower than the baseline This is obviously due to the trivial size of each data partition The total benefits for the credit card dataset is 750,000 which is still higher than the baseline of 733980 The second ob servation is that after the sampling size k exceeds around as small as 25 out of K  1024 or 0.5 the error bound becomes small enough implying that the total benefits by the complete model is very unlikely 99.7 confidence to increase At this point the user should cancel the learn ing for both donation and adult datasets The reason for the 223bumps\224 in the adult dataset plot is that each dataset is too small and most decision trees will always predict N most of the time At the beginning of the sampling, there is no variations or all the trees make the same predictions; when more trees are introduced it starts to have some diversities However the absolute value of the bumps are less than 50 as compared to 12435 3.5 Training Efficiency We recorded both the training time of the hatch mode single model plus the time to classify the test data and the training time of the multiple model with k  30 K clas sifiers plus the time to classify the test data k times We then computed ratio of the recorded time of the single and mul tiple models called serial improvement It is the number of limes that training the multiple model is faster than training the single model In Figure 5 we plot the serial improve ment for all three datasets using C4.5 as the base learner When K  256 using the multiple model not only provide higher accuracy but the training time is also 80 times faster for credit card 25 times faster for both adult and donation 4 Related Work Online aggregation has been well studied in database community It estimates the result of an aggregate query such as avg AGE during query processing One of the most noteworthy work is due to 7 which provides an in teractive and accurate method to estimate the result of ag gregation One of the earliest work to use data reduction techniques to scale up inductive learning is due to Chan I in which he builds a tree of classifiers In BOAT 6 Gehrke et al build multiple bootstrapped trees in memory to ex amine the splitting conditions of a coarse tree There has been several advances in cost-sensitive learning 3 Meta Cost 4 takes advantage of purposeful mis-labels to max imize total benefits In 181 Provost and Fawcett study the problem on how to make optimal decision when cost is not known precisely 5 Conclusion In this paper, we have demonstrated the need for a pro gressive and interactive approach of inductive learning where the users can have full control of the learning process An important feature is the ability to estimate the accuracy of complete model and remaining training time We have im plemented a progressive modeling framework based on av eraging ensembles and statistical techniques One impor tant result of this paper is the derivation of error bounds used in performance estimation We empirically evaluated our approaches using several inductive learning algorithms First, we find that the accuracy and training time by the pro gressive modeling framework maintain or greatly improve over batch mode learning Second the precision of estima tion is high The error hound is within 5 of the true value when the model is approximately 25  30 complete Based on our studies, we conclude that progressive mod eling based on ensemble of classifiers provide an effective 169 


Figure 4 Current benefits and estimated final estimates when sampling size k increases up to K  1024 for all three datasets To enlarge the plots when k is small we only plot up to k  50 The error range is 3 u\(uK for 99.7 confidence  Figure 5 Serial improvement jir4 for all three datasets when early stopping Is used 3 10 I 1 8 10 m s Irn 1m m zra m rn 1 m zra m Irn Im m ma WC4P.n WdP./Cn WdPb solution to the frustrating process of batch mode learning References 6 I Gehrke V Ganti R Ramakrishnan and W.-Y Loh BOAT-optimistic decision tree construction In Pro ceedings of ACM SICMOD International Conference on Management of Data SICMOD 1999 1999 7 J M Hellerstein P I Haas and H 1 Wang On line aggregation In Proceedings ofACM SIGMOD In ternationul Conference on Management of Data SIC I P Chan An Exrensible Meru-leurning Approach for Scalable and Accurare Inductive Learning PhD the sis Columbia University Oct 1996 21 W G Cochran Sampling Techniques John Wiley and Sons 1977 131 T Diettench D Margineatu E Provost, and P Tur ney editors Cost-Sensirive Learning Workshop ICML-00 2000 141 P Domingos MetaCost a general method for making classifiers cost-sensitive In Proceedings of Fifth In rernarional Conference on Knowledge Discovery and Dura Mining KDD-99 San Diego, California 1999 51 W Fan H Wang P S Yu and S Slolfo A framework for scalable cost-sensitive learning based on combin ing probabilities and benefits In Second SIAM In ternationul Conference on Datu Mining SDM2002 April 2002 MOD'97 1997 8 F Provost and T Fawcett Robust classification for imprecise environments Machine Learning 42203 23 I 2000 9 S Stolfo W Fan W Lee A Prodromidis and P Chan Credit card fraud detection using meta learning Issues and initial results In AAAI-97 Work shop on FraudDerecrion andRisk Management 1997 Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers In Proceedings of Eigk teenth International Conference on Machine Learning ICML'Z001 2001 IO B Zadrozny and C Elkan 170 


association-cube, base-cube and population-cube are derived from the volume cube; the confidence-cube is derived from the association cube and population cube and the support-cube is derived from the associationcube and base-cube. The slices of these cubes shown in Figure 2 correspond to the same list of values in dimension merchant, time, area and customer_group  Multidimensional and multilevel rules Representing association rules by cubes and underlying cubes by hierarchical dimensions, naturally supports multidimensional and multilevel rules. Also these rules are well organized and can be easily queried  First, the cells of an association cube with different dimension values are related to association rule instances in different scopes. In the association cube CrossSales cell CrossSales product \221A\222, product2 \221B\222  customer_group 221engineer\222, merchant \221Sears\222, area \221Los Angeles\222, time 221Jan98\222 represents the following multidimensional rule x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x,\221B\222  275 customer_group = \221engineer\222, merchant = \221Sears\222, area 221Los  Angeles\222, time =  \221Jan98\222 If this cell has value 4500, and the corresponding cell in the population cube has value 10000, then this rule has confidence 0.45 Next as the cubes representing rules can have hierarchical dimensions, they represent not only multidimensional but also multi-level association rules. For example, the following cells CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221Jan98\222 CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221 Year98 222 represent association rules at different area levels \(i.e the city level and the state level\d different time levels \(i.e., the month level, the year level x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221Jan98\222 x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221 Year98 222 The cell CrossSales\(product \221A\222, product2 \221B\222,  customer_group 221top\222, merchant \221top\222, area \221top\222,  time \221top\222 represents the customer-based cross-sale association rule for all customers, merchants, areas, and times in the given range of these dimensions, expressed as x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222 4.3  Generating Association Rule Related Cubes The basic task of our OLAP based association rule mining framework, either at the GDOS or at an LDOS is to convert a volume cube i.e. the cube representing the purchase volumes of customers dimensioned by product  area etc, into an association cube a base cube and a population cube These cubes are then used to derive the confidence cube and the support cube of multidimensional association rule instances. The following general steps are involved in cross-sale association rule mining 267  Roll up the volume cube SaleUnits by aggregating it along merchant, time, area dimensions 267  Derive cube NumOfBuyers from SaleUnits based on the antecedent condition SaleUnits 0 267  Populate cube NumOfShoppers by the counts of customers dimensioned by merchant, area  time not by product\at satisfy the antecedent conditions 267  Derive cube CrossSales from SaleUnits based on the association conditions SaleUnits  product  p 1  0 and SaleUnits  product2  p 2 0 267  Derive cube Confidence and cube Support using cell-wise operations 214  Confidence = CrossSales  NumOfBuyers 214  Support  CrossSales  NumOfShoppers  Cubes Confidence  Support  CrossSales are dimensioned by product  product2 customer_group,merchant  time, area NumOfBuyers is dimensioned by product  customer_group, merchant time, area  NumOfShoppers is dimensioned by customer_group, merchant  time, area Rules with confidence and support that exceed specified thresholds  may be considered interesting 4.4. Rules with Conjoint Items Cubes with conjoint dimensions can be used to represent refined multidimensional association rules For example, using OLAP, we can derive association rules across time  Time-variant or temporal association rules such as 


x 316 Customers buy_product\(x,\222 A\222, \221 Jan98\222  336 buy _product\(x, \221B\222, \221 Feb98\222   275 area = \221Los Angeles\222 can be used to answer such questions as \223 How are  the sales of B in Feb98  associated with the sales of A in Jan98 224 The items in this rule are value pairs of dimensions product and time In order to specify this kind of association rule we introduce a conjoint dimension product, time and mirror it with dimension product2, time2 This allows a cell in the association cube to cross two time values. Accordingly, the cubes related to association rule mining are defined as follows Association cube  CrossSales.2 \(<product, time>, <product2, time2 customer_group, merchant, area  Population cube  NumOfBuyers.2  \(<product, time>, customer_group merchant, area Base cube  NumOfShoppers.2  \( customer_group, merchant, area Confidence cube Confidence.2 \(<product, time>, <product2, time2 customer_group, merchant, area Support  cube  Support.2  product, time>, <product2, time2 customer_group, merchant, area  The steps for generating these cubes are similar to the ones described before. The major differences are that a cell is dimensioned by, besides others product, time and product2, time2 and the template of the association condition is  SaleUnit s  product p 1 time t 1  0 and  SaleUnits  product2 p 2 time2 t 2  0 where, in any instance of this condition, the time expressed by the value of time2 is not contained in the time expressed by the value of time The template of the antecedent condition is SaleUnits   product p 1 time t 1  0 In general, other dimensions such as area may be added to the conjoint dimensions to specify more refined rules 4.5. Functional Association Rules A multidimensional association rule is functional if its predicates include variables, and the variables in the consequent are functions of those in the antecedent.  For example, functional association rules can be used to answer the following questions, where a_month and a_year are variables q  What is the percentage of people in California who buy a printer in the next month after they bought a PC x 316 Customer buy_product\(x, \221PC\222, a_ month 336 buy_product\(x, \221printer\222, a_month+1  275 area = \221California\222 q  What is the percentage of people who buy a printer within the year when they bought a PC  x 316 Customer: buy_product\(x, \221PC\222, a_ year 336 buy_product\(x, \221printer\222, a_year 275 area = \221California\222 To be distinct, we call the association rules that are not functional as instance association rules; e.g x 316 Customer: buy_product\(x,\222 PC\222, \221Jan98\222 336 buy_product\(x,\222 printer\222, \221Feb98\222  275 area =  \221California\222 Time variant, functional association rules can be derived from time variant, instance association rules through cube restructuring. Let us introduce a new dimension time_delta that has values one_day, two_day 205, at the day level, and values one_month, two_month, \205, at the month level, etc. Then, let us consider the following functional association rule related cubes Association cube  CrossSales.3 \(product, product2, customer_group merchant, area, time_delta  Population cube  NumOfBuyers.3 \(product, customer_group, merchant area Base cube  NumOfShoppers.3 \( customer_group, merchant, area Confidence cube  Confidence.3 \(product, product2, customer_group merchant, area, time_delta Support cube  Support.3 \(product, product2, customer_group, merchant area, time_delta The association cube CrossSales.3  can be constructed from CrossSales.2   The cell values of CrossSales.2  in the selected time and time2 ranges are added to the corresponding cells of CrossSales.3 For example, the count value in cell  CrossSales.2\(<PC, Jan98>, <printer, Feb98>\205 is added to cell \(bin CrossSales.3\(PC, printer, one_month,\205 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


