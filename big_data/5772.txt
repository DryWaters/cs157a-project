Scaling Concurrency of Personalized Semantic Search over Large RDF Data Haizhou Fu HyeongSik Kim Kemafor Anyanwu North Carolina State University Raleigh NC USA  hfu hkim22 kogan  ncsu.edu Abstract Recent keyword search techniques on Semantic Web are moving away from shallow information retrieval-style approaches that merely nd  keyword matches  towards more interpretive approaches that attempt to induce structure from keyword queries The process of query interpretation is usually guided by structures in data and schema and is often supported by a graph exploration procedure However graph explorationbased interpretive techniques are impractical for multi-tenant scenarios for large databases because separate expensive graph exploration states need to be maintained for different user queries This leads to signiìcant memory overhead in situations of large numbers of concurrent requests This limitation could negatively impact the possibility of achieving the ultimate goal of personalizing search In this paper we propose a lightweight interpretation approach that employs indexing to improve throughput and concurrency with much less memory overhead It is also more amenable to distributed or partitioned execution The approach is implemented in a system called  SKI  and an experimental evaluation of SKIês performance on the DBPedia and Billion Triple Challenge datasets shows orders-of-magnitude performance improvement over existing techniques Keywords  Scalability Concurrency Keyword Query Interpretation Personalization Big RDF data Semantic Web I I NTRODUCTION There is increasing interest in exploiting the growing amount of structured semantic data on the Web to improve the quality of keyword search The idea is to be able to better identify a userês intended semantics for a query so as to provide the appropriate set of results To achieve this techniques must go beyond the IR or meaning as match interpretation styles used by traditional search engines Most techniques proposed for keyword queries over structured/semi-structured data are also based on such interpretation styles with relational databases  2 3 4 5 6 and XML databases 7 8 The limitation of these approaches are well known i result lists that are semantically incongruent so that users have to manually lter out results that are irrelevant to the userês querying context ii inability to deal with moderately complex queries One example is the class of list queries that describes a set of entities rather than a speciìc entity For example the query patents big data management is likely intended to return a list of patents on topics related to big data management For example a query for patent big data management may miss patents that do not contain the phrase big data management but contain keywords like scalable MapReduce and so on Processing such queries with current IR techniques presents challenges due to the length of such queries In addition some keywords in the queries are descriptive which should not be considered as data explicitly associated with relevant results but as metadata To address this limitation semantic search techniques have been proposed 10 11 where an  interpretation  phase is inserted prior to query evaluation The goal of interpretation is to map a keyword query to a structured query thereby giving the query a xed semantics for which semantically congruent results can then be computed However identifying a unique semantics is often difìcult because the terse nature of keyword queries makes them ambiguous Consequently most techniques address this as a top-K problem producing a list of K structured queries representing the K most likely intended semantics for a given keyword query The structurization process involves computing connected subgraphs of keyword hits on a summarized representation of schema and data graphs These graphs are mapped to structured queries by distinguishing the roles of keywords in a query i.e metadata keywords vs data keywords For example for the query  conferences tutorial big data  depending on the roles assigned to the word conferences the interpretation could be  the list of conferences with tutorials on big data  or could be the  list of tutorials at conferences on big data  Heuristics for ranking connected subgraphs typically assign the intended semantics of a query to the probability of the query deìned in terms of the frequency or occurrence of subgraph structures This is similar in spirit to search engines determining most relevant results based on click patterns in query and click-through logs However such techniques are impersonal and fail to customize interpretation of a query to an individual userês needs As an example while the most frequent intention associated with the query  magic sets  may be toys because the shopping context is very general and has a large population of users However the same query could be intended to mean the query optimization technique which is well known but in a much smaller community Unfortunately such a relatively unpopular context would not be considered even if intended It is important to note that this is not simply a question of proìling users and determining interests of a user because users have different interests and interact with the Web in different contexts For example a database researcher issuing the query magic sets may intend the context of toys when shopping for some gifts Another example is  Jaguar Speed  a computer science researcher may be investigating the supercomputer then at a different time be buying a car and yet another time be helping a middle school child with a science homework Therefore the ideal situation is to personalize query interpretation by identifying the intended interpretation for a speciìc user at a speciìc time 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 556 


In our previous work 10 we addressed the problem of  personalizing  keyword query interpretation by capturing and exploiting a userês ambient query context in terms of the queries preceding a query being considered The underlying hypothesis is that users often pose multiple queries related to a speciìc need or task so that earlier queries can provide some contextual basis for future queries The approach proposed modeled this context-aware query interpretation as computing top-K connected subgraphs over a context-aware summary graph  The context-aware summary graph was modeled as a dynamic weighted graph model in which keywords in a query ascribe weights transfer summary graph and the impact The result is that for a given query being issued at a given time connection subgraphs involving concepts and relationships with lower costs higher weights are ranked higher in the topK list decay model A key challenge addressed in previous work is the indexing and management of the dynamic weight graph model and efìcient algorithm for computing the Steiner Trees Motivation Like the non-personalized techniques for query interpretation our earlier proposed context-aware interpretation technique is heavy-weight because it is based on graph exploration on memory resident summary graphs For big data the number of hits per keywords and the degree of connectivity between nodes rises increasing the exploration search space for connection subgraphs and thus memory requirements For example the keyword  conference  has 19 occurrences in DBPedia 3.6 1  which includes 272 concept nodes and around 3000 properties edges Further it has 1785 hits in the Billion Triple Challenge datasets BTC 2009 2  which contains 150232 class nodes and around 1.3 million edges BTC is a superset of DBpedia In personalized query interpretation each user has a different dynamic weighted graph because of differences in their query contexts This could imply a separate graph exploration process for each user For example the memory requirements for interpreting a single query on the BTC dataset is around 500MB which leads that a server with 8GB RAM can handle only 17 user queries concurrently Consequently we are faced with the limitation of only being able to meet the goal of personalized and contextual search under low concurrency/throughput which is impractical for real multiuser search environments Contributions and Scope In this paper we focus on increasing the practicability of personalized semantic search in big data contexts by introducing lightweight interpretation techniques Speciìcally we propose an approach called SKI that uses a dual indexing scheme to maximize the concurrency and throughput of the query interpretation process while minimizing the latency SKI indexes user-speciìc query context information separately from the more general user-independent information about concepts and their relationships The former is captured in an index  personalized query context map PCM and the latter in dense path index DPI  an index of subgraph structures The data-speciìc indexes inform a graph exploration-free interpretation algorithmês  GeFree  decisions about which substructures to prune and how to assemble substructures into complete interpretations GeFree avoids the need for graph exploration and is fast and memory-efìcient reducing both latency and memory requirements of query  1 http://blog.dbpedia.org/2011/01/17/dbpedia-36-released 2 http://km.aifb.kit.edu/projects/btc-2009 interpretation Impact We posit that integrating our proposed scalable query interpretation approach with many ongoing-efforts on scalable query-answering techniques will result in scalable Semantic Web search architectures The rest of the paper is organized as follows Section 2 includes the preliminaries of the keyword query interpretations and the problem deìnition Section 3 describes the overview of the SKI architecture Section 4 discusses the details of the index structures and algorithms used in the SKI Section 5 and 6 present performance evaluation results and related work II P ROBLEM D EFINITION Let  be a universe of words An RDF schema graph is a labeled graph G S  V S E S  S  where the nodes represents classes and edges represent properties  S is a labeling function that maps a graph element node or edge to l   that contains words that make up the label of the class or property An RDF data graph is also a labeled graph G D  V D E D  D  D  that has V D  E D and  D are similarly deìned with  D mapping a graph element to set of words contained in the string literal associated with that element  D maps each data graph element node or edge g D to a set of schema graph elements those to which g D is connected by a  rdf:type  relation Given a schema graph G S and data graph G D an annotated schema graph G A is a tuple  G S   A  where  A maps each graph element g S such that  A  g S    S  g S    D  g D   g S   D  g D    In other words the function  A lifts labels on elements of data graph to the schema elements that they are instances of thereby annotating the schema graph with keywords from the data graph A keyword query Q is a set of words  k 1 k 2   k  Q   from the universe   An element g i of an annotated schema graph G A is called a hit for keyword k if k   A  g i  The set of all hits for a keyword k is denoted as hit  k  An interpretation  Q  of a keyword query Q is a subtree of G A  that contains one hit for each keyword in Q  Since there are potentially multiple interpretations of a keyword query due to different possible combinations of hits for all keywords we focus on a speciìc top interpretation for Q which we denote  Q    but ignore for now how to choose  Q   from the set of alternatives for Q  DEFINITION 2.1 A context-aware annotated graph or CAG for short associates weights with an annotated schema graph based on the sequence of queries that have been posed on that graph A sequence of queries Q 1   Q m produces a sequence of weighted graphs G 0  G 1   G m where G t  0  t  m  is the weighted annotated schema graph produced after Q t  G t is deìned as  G A   t  where  t  W  W is a family of weighting functions s.t   0  W is an init function that assigns weights to all elements of the annotated schema graph prior to Q 1    t  W is deìned s.t for g   Q t    i.e in the top interpretation for Q t   t  g     t  1  g   In other words after Q t  the weights of a graph elements in the top interpretation of Q t  is a function of their weights after 557 


                                 Fig 1 An example context-aware annotated graph CAG and its interpretation processes The example CAG contains the two possible interpretations on the the keyword query Mississippi River Bank the bank of the Missippippi River a nancial institute or the Missippippi River a large natural stream of water Q t  1  This leads to the deìnition of an annotated schema graph as a dynamically weighted graph whose elements weights change with queries As mentioned earlier the details of how to select  Q    the top interpretation for a keyword query Q  are omitted However this can be thought of as the lowest weighted interpretation when the weights of all graph elements in that interpretation are combined The combination cost function and  are discussed in b u t w e illustrate the concept of a CAG with an example Context-Aware Interpretation Example Fig 1\(a shows the example CAG with some classes and literal nodes which are annotated with literals from the data graph Given a keyword  Mississippi River Bank  we nd matches in this graph and select the highest weighted connecting subtree as the top interpretation In Fig 1\(a the graph has been initialized with some weights e.g the class nodes  Mississippi River Bank  and  Bank  are initialized with the weight 1.0 and 2.0 respectively For example the green subtree involving the class nodes Mississippi River Bank and Bank represents the interpretation of the query as the bank of the Missippippi River while the purple subtree involving nodes Mississippi River and River represent the interpretation the Missippippi River  However based on the weight assignment it is difìcult to determine which interpretation should be selected as a highest weighted one since the total weight of subgraphs are the same i.e 3.0 and 3.0 for both subtrees Note that the method for computing the initial weights can vary and is not our primary concern here We calculate the total weights as the sum of the weights of nodes and edges not the sum of the weights of all paths represented in the subgraph Fig 1\(b-d show how context-aware interpretations are processed with two paths originating from the initial graph Each path represents a sequence of queries performed by a user the top path  Q 1 and Q 2  by the user X and the bottom path  Q 3 and Q 4  by the user Y  In the top path Fig 1\(b shows that the keyword query Q 1 Mortgage Rate results in increasing the weight of the nodes Mortgage Loan and Rate from 1.0 to 2.0 and the weight of its neighboring node Bank from 2.0 to 2.5 These changes make the top interpretation of the keyword query Q 2 becomes the subtree consisting of the nodes Mississippi River Bank and Bank in Fig 1\(c because the sum of the weights of those nodes are the highest one i.e 2  5+1  0=3  5  Similarly in the bottom path the weight of the node River is increased from 1.0 to 1.5 because of the keyword query Q 3 Fishing Activity in Fig 1\(d This increment leads to the subtree with the nodes Mississippi River  River  and River Bank as the top interpretation of the subsequent query Q 4 because its weight now becomes the highest one i.e 1  0+1  5+1  0=3  5  Note that Q 2 from the user X and Q 4 from the user Y are the same queries but produce different top interpretations Although our example illustrates the changes in weights as increasing weights the real situation is the inverse higher weight leads to lower cost Since we are solving this as a translation of the Steiner Tree problem which is a minimization problem the top interpretations should actually be the smallest weighted ones Consequently our concept of increasing weights in example to reîect importance is actually done as decreasing weights The problem we address in this paper is supporting personalized CAGs  i.e for two users X and Y with query sequences QS X and QS Y resp CAG X and CAG Y are two possibly different CAGs associated with X and Y resp The conceptual model of CAGs extends very naturally to multiple users However the key challenge is how to scale up the number of concurrent users since a different CAG has to be managed for each user Particularly in the presence of big annotated schema graphs The following section discusses our approach III T HE SKI A PPROACH A Architecture Overview In this section we present the SKI strategy for scalable personalized query interpretation that consists of three main components 558 


1 a distributed master-slave architecture for scalability 2 an index for user query-context information called a personalized query context map PCM  which is separate from a more heavy-weight index of user-independent information about graph structures i.e concepts and relationships in the schema graph 3 captured in a couple of other indexes After each query has been interpreted PCM is updated to reîect this new information about query context i.e  U i t   U i t 1 for user U i  To avoid the overhead of maintaining all dynamic weighted graphs for supporting multiple concurrent interpretations PCM maintains only node-weight maps for each user 3 an algorithm GeFree for computing top-k connection subgraphs for each user query in which user-independent indexes i.e dense path index  DPI  are shared across interpretation instances for each user and are then combined with their user-speciìc context information in the PCM GeFree is graph exploration free making it more efìcient and allowing higher throughput and the sharing of user-independent indexes minimizes amount of memory required for interpretation to allow increased concurrency 4 form the query interpretation layer which is interfaced with the query answering layer that uses standard RDF query processing engines such as SESAME In this paper we focus on the interpretation layer The slave nodes are grouped into clusters or interpretation clusters for and the master node is responsible for scheduling user query requests to interpretation clusters and exchanging information such as resulting interpretations and query answers between users and interpretation clusters Each interpretation cluster is responsible for processing a speciìc group of users query requests Indexes are replicated across all slave nodes with one exception where a partitioning of PCM in an interpretation cluster contains only the context map for the group of users maintained by this cluster it is replicated across slave nodes within this cluster Each slave node in an interpretation cluster runs a separate thread for each interpretation instance Each interpretation instance has 3 key steps depicted in Fig 2 1 Step 1.1 Initialize Keywords Hits by looking up the KS-Map inverted index that maps keywords to schema graph elements 2 Step 1.2 Identify Candidate Roots identiìes all possible roots of candidate connecting trees linking at least one keyword hit for all keywords in a query This process is supported by Rabit Index  a group reachability index for rapid identiìcation of root nodes of connecting trees 3 Step 1.3 Top-K Graph Patterns Generation uses the GeFree algorithm to assemble and generate top-K graph patterns representing top-K interpretations based on candidate roots computed in 1.2 GeFree is a graph exploration free algorithm that uses DPI for fast construction of interpretations It uses materialized path information stored in DPI and Rabit to enable rapid assembly of the top-K minimum connecting trees representing the top-k interpretations To personalize the interpretation process it uses the userês information in the PCM partition assigned to that slave node  3 In the rest of the paper when we say schema graph we actually mean annotated schema graph Fig 2 Architecture and workîow for a single slave node After the personalized top-K candidate graph patterns are generated Steps 2 to 4 take care of sending them to the user layer and sending the highest ranked queries to the query answering layer for processing Since the ranking is based on heuristics and may not be perfect sending the top-K list of graph patterns allows a user to select a different top-1 pattern if the highest ranked pattern is not their intended interpretation When this happens the processing of the originally dispatched top-1 query is halted and query processing for the newly selected graph pattern begins Elaborating on the algorithms requires the introduction of some concepts  path covers and  graph exploration graphs which deìne structural information maintained by DPI and GeFree Candidate root is a key concepts for Rabit index and GeFree B Keyword Query Interpretation in SKI 1 Foundations DEFINITION 3.1 A  path cover of a node r in graph G is denoted by   r,G   p i   where p i is a path such that  p i  p i    r,G  if and only if r  G is the source node of p i and  p i    i.e the path cover   r,G  contains all reachable paths of length less than or equal to  from r For example a 2-path cover of node D in the graph shown in Fig 3 is  D D  A  D  B  D  E  D  X  D  X  N  D  X  I  D  H  M  D  H  I  D  H  E  D  E  H  D  E  I  D  E  F   DEFINITION 3.2 A  Exploration Graph of a node r in graph G denoted by g  r  is a rooted subgraph of G  which contains all edges in   r,G   For example in Fig 3 let G S be the whole graph the subgraph in the box is a 2-exploration graph rooted at node Z  g 2 Z  which contains all edges in  2 Z,G S   Z  Q Z  Q  M Z  Q  S Z  Q  R   DEFINITION 3.3 Given a keyword query Q and a schema graph G S a candidate root of Q is the root node r of a  exploration graph g  r  which contains at least one hit of each 559 


Fig 3 Example for the deìnitions of  path cover and  Exploration Graph Arrows represents exploration states on an undirected schema graph keyword in Q  For example in Fig 3 I is a candidate root for keyword query k 1 k 2 k 3  because the 2-exploration graph g 2 I contains a hit P matches k 1  K matches k 2  and Y matches k 3  Those hits such as P K Y in g 2 I  that are covered by the  exploration graph g  r rooted at a candidate root r are called productive hits  Productive hits can always reach a candidate root following a path of length less than or equal to   2 Identifying Candidate Roots Different from the graph exploration based approaches where roots are identiìed by exploring paths originated from each hits in this section we introduce the idea of fast candidate roots and productive hits identiìcation Group Reachability Bitmap Index Given a keyword query Q   k i   in order to identify a group of candidate roots a group reachability  question that needs to be answered is Given a set of groups of nodes whether every node in a group of nodes CR can reach at least one node in ALL the other groups within  hops We propose a group reachability bitmap index Rabit to identify candidate roots and productive hits more efìciently For each node u i in the graph we propose to use a bit vector denoted by  i  b i 1 b i 2  b i  V  G S    to represent the nodes that can reach u i within  hops Notice that in Fig 3 each bit vector for each node is 26 bits long because there are 26 nodes in that graph which can then be represented using a 32-bit integer such that we can use one integer to represent the  exploration graph for each node Let  be a function that maps a set of nodes U   u 1  u m  to bit vector such that only b u i 1 iff u i  U Now,wecan check the reachability of two nodes using bitwise AND i.e   in the following way we can check check if node H can reach A in 2 hops by calculating  A     H    56885252  262148  262148  where    H    262148  Using the Rabit index we can identify for each group of keyword hits a set of neighboring nodes that can reach at least one hit in that group by using bitwise OR operation For example in Fig 3 x  GetN eighbors   A C   2   A   C  56885252  12255232  67043332  11111111110000000000000100 2  which means the set of node  A B C D E F G H I J X  is the union of neighbors of  A C   Then we can identify a bit vector that represents the common nodes in all groups of neighboring nodes This can be achieved by bitwise AND operation and those nodes are candidate roots For example for keyword query  k1,k2,k3  in Fig 3 the candidate roots can be calculated in the following steps the 3 groups of hits for each keyword are G 1   A P   G 2   K T   G 3   Y R   For k 1  x 1 GetN eighbor  G 1  2  57023492  for k 2  x 2 GetN eighbor  G 2  2  1295074  for k 3  x 3 GetN eighbor  G 3  2  1303427  Then x 1 x 2 x 3  131072  00000000100000000000000000 2  which means only the node I is the candidate root Those hits that can NOT reach any candidate roots are un-productive hits and therefore should be pruned Those un-productive hits can be identiìed using bitwise AND operation in the similar way 3 GeFree A Graph Exploration Free Approach After identifying candidate roots we present another important index called dense path index DPI  Having unproductive hits pruned given a set of candidate roots CR   r i   for keyword query Q  and each root r i  there are many possible interpretations rooted at r i  However only knowing the roots is not enough to identify the top-K interpretations among all possible interpretations that rooted at each root in CR  The paths from a root node to all hits need to be computed and assembled to construct an interpretation Dense Path Index  The dense path index is a two-level hierarchical index The rst layer is called node-exploration graph map or NE map  which is organized as a hash table that maps any node r in G S to a  exploration graph g  r  NE  r  g  r denotes such mapping The second layer of the dense path index is called destination-path map or DP map that maps a destination node to a set of paths For each  exploration graph the  path cover of r  i.e   r,g  r  is pre-computed and those paths in   r,g  r  are grouped by destination nodes   r,g  r   P v 0  P v 1    P v m   where P v i is called a path group of v i  where v i  V  g  r  is a destination node and  i j  P v i  P v j    DP  v g  r  P v denotes the mapping for g  r from a destination node v to the path group P v of v  The  path cover can be organized as a hash table that maps any destination node v j  V  g  r  to the path group of v j  i.e P v j  For each root-keyword pair they are updated and stored in PCM after each query generation and retrieved from PCM during query time  at most the top-K minimum weighted paths are necessary for computing top-K interpretations For example given a keyword query Q  w 1 w 2   considering a candidate root r  assuming that we are only interested in top-2 interpretations let P 1  p 1 1 p 1 2 p 1 3  be the top-3 paths from r to any hits of w 1  P 2  p 2 1 p 2 2  be the top-2 paths from r to any hits of w 2  any combinations of paths including p 1 3 will not form a top-2 interpretation and should not be investigated during the processing of generating top-K interpretations The GeFree algorithm is illustrated in Algorithm 1 GeFree Algorithm  For each keyword w i  only productive hits are left and stored in H  i  for w i line 4 For each candidate root r i  the corresponding  exploration graph is returned using NE map line 7 Then for each keyword w j  H  j  contains a set of productive hits for w j  for each productive hit v jk  DP map returns a path group P k line 11 For a candidate root r i and a keyword w j  Top-K rootkeyword paths are computed by merging topK-heaps from each path groups of all productive hits of w j into a priority queue PQ j of size K line 12 and line 13 In line 14 for the speciìc root r i  a sorted list S j containing K paths 560 


 Algorithm 1 GeFree 1 Input Q   w i   CR   r i   PKE   u i   2 TOPK   is a priority queue for maintaining top-K interpretations 3 for all w i  Q do 4 H  i  hit  w i   PKE  5 end for 6 for all r i  CR do 7 g  r i  NE  r i   8 for all w j  Q do 9 PQ j   Initialize a priority queue for maintaining top-K paths 10 for all Hit v jk  H  j  do 11 PG  DP  v jk g  r i   12 Calculate P k  TopK paths in PG as a heap 13 P k  PQ j  14 Sorted Path Group S j  PQ j HeapSort   15 end for 16 end for 17 TOPK  T opKCombinations   S j    18 end for 19 return TOPK  is computed for each keyword  Q  lists are computed in total T opKCombinations   S j   is an algorithm to quickly generate top-K combinations of paths from  Q  sorted path lists The algorithm is based on the T opKCombinations algorithm proposed in This algorithm suggests an early termination strategy such that top-K combinations will be generated without enumerating all combinations of paths and has guaranteed O  K  Q   time complexity The time complexity of GeFree is O   CR   K  Q   K  PKE  lg K   where CR is the set of candidate roots and PKE is the set of all productive hits K is the number of top-K interpretations generated Because in the worst case CR may contain all nodes in the graph and PKE is equal to a set of all possible hits the worst case time complexity is O  n  K  Q   Km lg K   where n   V  G S   and m    hit  w i   is the total number of hits In comparison the time complexity for graph exploration based algorithm as reported in CoSi is O  nd  K  Q    where d is the degree of the graph IV E VALUATION We evaluated the scalability of the proposed multi-tenant query interpretation system SKI by comparing its performance with other systems  CoSi  and TKQ2S  In this e v aluation we mainly focused on measuring the efìciency improvement and scalability in terms of concurrency because the effectiveness of the personalized techniques proposed in CoSi has been studied in our prior work Datasets and Testbed Two real-life datasets were employed a DBPedia 3.6 approx 300 classes 3K property types 3.5 million entities  678 million triples and b the Billion Triple Challenge 2009 or BTC 1.19 billion triples For the BTC we created the schema graph from its dataset which includes 150232 classes nodes and around 1.3M properties edges For each experiment we rst initialized random weights and used the same set of initial weighs for all personalized algorithms to represent the bootstrapping context of user query history The evaluation was conducted on a cluster of up to 30 slave nodes each of which is equipped with Intel dual core 2.0 GHz CPU and 8GB RAM All algorithms were implemented with C   and all results were averaged over at least 5 trials Tasks and Metrics We designed two tasks to evaluate the performance of each approach The rst task measured the performance metrics such as execution time and DOTA for each query and dataset We introduced DOTA or Degree Of Data/Appr Graph PCM KS-MAP DPI RABIT EstMaxU BTC-S 337 2.8 221.94 155.66 25.6 2857 BTC-C/T 445     17 DBPedia-S 1.1 0.07 38.83 6.75 1.33 114286 DBPedia-C/T 1.4     5714 TABLE I M EMORY CONSUMPTION AND CONCURRENCY M B  SKI\(S C O S I TKQ2S\(C/T Term Ambiguity in our previous work that characterizes the ambiguity of a query as a reîection of how much work i.e exploration search space is needed for its interpretation DOTA is deìned as a function of the number of combinations of all keyword hits The second tasks measured the latency of query responses while varying the MPL  Multi-Programming Level  i.e the number of concurrent running programs Queries We employed 8 real life queries from Semantic Search Challenge 2011 4  The additional details can be found in the project page 5  A Task 1 Measuring Execution Time and DOTA We recorded the execution time and DOTA of the 8 queries  Q 1 Q 8  in the query set Fig 4\(a shows that SKI generally outperformed CoSi for both datasets Fig 4\(b shows that the queries tend to have much higher DOTA in the BTC dataset than in DBPedia because all keywords appeared in both datasets but with varying frequencies because BTC was a superset of the DBPedia dataset Note that the queries are ordered by the DOTAs of queries over DBPedia not BTC The performances of both CoSi and SKI dropped when interpreting the same queries on a dataset with larger scale since the degree of ambiguity increases for the same keyword query For queries Q 6 to Q 8  the DOTA increased signiìcantly SKI performed much stable on both BTC and DBPedia while the performance of CoSi dropped considerably because the search space increased signiìcantly with the growth of DOTA In particular the DOTA of Q 2 over BTC was larger than the DOTAs of Q 1 and Q 3  therefore the time spent by CoSi for Q 2 on BTC was higher than that for Q 1 and Q 3  B Task 2 Measuring Query Latency In this task we increased the MPL and measured the corresponding response latency of concurrently running queries to compare the degree of concurrency Due to the resource contention between multiple programs the performance and memory consumption of the personalized interpretation algorithm impact the response latency of concurrently running queries for different users Fig 4\(c shows the impact of MPL on the latency in milliseconds for interpretation algorithms Although all approaches experience increased latency as MPL increases the rate of increase is higher with the explorationbased approaches than the index approach of SKI This suggests that the concurrency overhead for CoSi and TKQ2S is higher than SKI Note that SKI supports much higher multiprogramming level while keeping low latency CoSi or TKQ2S were not able to maintain reasonable latency for MPL greater than 5 due to high memory consumption and CPU overhead As an explanation for this behavior we report the amount of memory consumption of SKI CoSi and TKQ2S in Table I for each user Note that the reported memory usage of graph for graph exploration-based algorithms includes the overhead of 4 https://km.aifb.kit.edu/ws/semsearch11 5 http://research.csc.ncsu.edu/coul/SKI/experiments.xlsx 561 


Fig 4 a The execution time of the three approaches SKI CoSi and TKQ2S and b The DOTAs for the query set QS  c The latency of the three approaches while varying the MPL  SC5 SC10 SC15 SC20 SC25 SC30 1000 54.67 78.88 89.24 107.57 150.37 166.39 5000 81.52 129.69 178.43 229.45 278.85 265.45 10000 80.45 152.59 215.47 268.22 307.46 353.27 TABLE II T HROUGHPUT maintaining intermediate graph exploration states e.g paths explored and cursors generated The column EstMaxU shows the estimation of the maximum number of user queries that can be running concurrently based on the memory consumption of each approach In case of BTC dataset each user in CoSi/TKQ2S requires around 445MB for maintaining graph exploration states associated with the summarized schema graph while the SKI consumes only 2.8MB for PCM and other indexes for each user Thus CoSi/TKQ2S can only support up to 17 users using 8GB memory but SKI can handle up to 2857 users concurrently Additionally we conducted experiments on the interpretation slave clusters SC with SKI deployed on them and report the throughput in Table II Six groups of experiments were conducted with varying sizes of an interpretation cluster up to 30 slave nodes and different sizes of the query mixes 1k to 10k queries The throughput queries per second of the larger cluster was higher V R ELATED W ORK For approaches that do not require graph exploration BLINKS proposes a similar idea to pre-compute i node to keyword index and ii keyword to node index However BLINKS employs a distinct root semantics where a root node identiìes a unique sub-graph that connects all keywords by shortest paths In this paper we propose to save and manage richer path information not only shortest path and generate answers with Steiner tree semantics  which implies larger search space In addition BLINKS maintains mappings from keyword to lists of nodes ordered by costs EASE proposes EI-index where ordered weighted r-radius graphs for pair wise tokens are pre-computed However adapting these kinds of indexes to personalized query interpretation such as CoSi has some challenges Because after e v ery query  the classes and properties associated with the query as well as related concepts and properties will need to have their weights increased to reîect the impact of the query on the context model the ordering of all indexes containing such classes and properties will need to be updated For existing techniques this may require a complete rebuild after each query which is impractical VI C ONCLUSION This paper presents an approach for interpretive search that scales well under high concurrent workloads The approach achieves this by relying on a suitable set of indexes rather than expensive graph exploration based algorithms Comprehensive evaluation over real world datasets showed very promising results VII A CKNOWLEDGMENT This work was partially funded by NSF grant IIS-0915865 R EFERENCES  B Aditya et al  BANKS Browsing and Keyword Searching in Relational Databases in Proc VLDB  2002 pp 1083Ö1086  S Agra w a l et al  DBXplorer Enabling Keyword Search over Relational Databases in Proc SIGMOD  2002 p 627  H He et al  BLINKS Ranked Keyword Searches on Graphs in Proc SIGMOD  2007 pp 305Ö316  V  Hristidis and Y  P apak onstantinou DISCO VER K e yw ord Search in Relational Databases in Proc VLDB  2002 pp 670Ö681  Y  Luo et al  SPARK Top-k Keyword Query in Relational Databases in Proc SIGMOD  2007 pp 115Ö126  Y  Luo et al  SPARK2 Top-k Keyword Query in Relational Databases IEEE Trans Knowl Data Eng  vol 23 no 12 pp 1763 1780 2011  Z Liu and Y  Chen Answering K e yw ord Queries on XML Using Materialized Views in Proc ICDE  2008 pp 1501Ö1503  Z Liu and Y  Chen Identifying Meaningful Return Information for XML keyword Search in Proc SIGMOD  2007 pp 329Ö340  H Fu and K An yanwu Ef fecti v ely Interpreting K e yw ord Queries on RDF Databases with a Rear View in Proc ISWC  2011 pp 193Ö208  H Fu et al  CoSi Context-Sensitive Keyword Query Interpretation on RDF databases in Proc WWW  2011 pp 209Ö212  T  T ran et al  Top-k Exploration of Query Candidates for Efìcient Keyword Search on Graph-Shaped RDF Data in Proc ICDE  2009 pp 405Ö416  G Li et al  EASE An Effective 3-in-1 Keyword Search Method for Unstructured Semi-structured and Structured Data in Proc SIGMOD  2008 pp 903Ö914 562 


pp. 1-5 2009 3 S  Ag a r wa la  D Ja d a v  a n d L  A B a t h e n   i C o st a l e  Ad a p t i v e  C o st  Optimization for Storage Clouds," in pp. 436-443, 2011 4 M  Ar mb ru st  A Fo x   R  Gri f fi t h  A D  J o s e p h   R  Ka t z  A Kon wi n sk i   G. Lee, D. Patterson, A. Rabkin, I. Stoica, and M. Zaharia, "A View of Cloud Computing vol. 53, pp. 50-58, 2010 5 M   S  Av i l a Ga rc i a  X  Xi on g A E   T r e f e t h e n   C   C r i c h t on  A T s u i   a n d  P. Hu, "A Virtual Research Environment for Cancer Imaging Research in pp. 1-6 2011   J  Ba l i g a R W   A y re  K   H int o n and R  S   T u ck er  G reen c lo u d  computing: Balancing energy in processing, storage, and transport vol. 99, pp. 149-167, 2011   R. B o s e and J   F r ew   L in e a ge R e tr i e va l  for S c i e n tific Data Pro c es s ing  A  Survey vol. 37, pp. 1-28, 2005   A   Bu r t o n and A  T r elo a r P ub l i s h M y D a ta  A  C o m p os iti o n of  Serv i c e s  from ANDS and ARCS," in pp. 164-170, 2009   P. Ch e n  B Pl a l e, and M S A k tas  T em por a l  re pres e n t a ti o n  fo r s c i e n tific data provenance," in pp. 1-8, 2012   Y  Cui, H  W a ng and X   Ch e n g C hann el A llocati o n  in W i reles s  Data Center Networks," in pp. 1395-1403, 2011   E  Deelm an  G  S i n g h M L i v n y  B  B e rr im an, and J   G o o d  T h e C o s t  of  Doing Science on the Cloud: the Montage Example," in pp. 1-12, 2008   I  F o s t er J   Vo ck ler M. W i l d e   and Z. Yo n g  C him er a  A V ir tu a l  D a ta System for Representing, Querying, and Automating Data Derivation," in pp. 37-46, 2002   I  F o s t er, Z. Y o n g I   Ra icu and S  L u  C lo ud C o m putin g and G r id  Computing 360-Degree Compared," in pp. 1-10, 2008   S K  G a rg   R  Bu y y a and H   J   Si egel  T i m e and C os t T r ad e Off  Management for Scheduling Parallel Applications on Utility Grids vol. 26, pp. 1344-1355, 2010   P. K  G unda L   R a v indr an ath C A  T h ekkath, Y  Y u   and L  Z huan g  Nectar: Automatic Management of Data and Computation in Datacenters," in pp. 1-14, 2010   X. H u an g  Z  L u o and B  Yan C y b er infr a s t r uctu re  and e-S ci e n c e  Application Practices in Chinese Academy of Sciences," in pp. 348-354 2011   M  H u m p h r ey  N  B e e k w i l d er J   L   G ood a l l  and M B E r c a n Calibration of watershed models using cloud computing," in pp. 1-8, 2012   G  J u ve E. Deelm an, K  V a hi  and G   M e h t a, "D ata S h a r in g O p ti o n s  for  Scientific Workflows on Amazon EC2," in pp. 1-9, 2010   D. K ond o   B. J a v a di  P Ma lec o t, F   Capp ello and D  P  A n d e r s o n   C o s t Benefit Analysis of Cloud Computing versus Desktop Grids," in pp. 1-12, 2009 20  X  L i u  Z  Ni  D  Yu a n  Y  J i a n g Z   Wu  J  C h en   a n d  Y  Ya n g   A N o v e l Statistical Time-Series Pattern based Interval Forecasting Strategy for Activity Durations in Workflow Systems vol. 84, pp. 354-376, 2011   B  L uda s c h e r   I   A ltint as  C B e r k ley  D H i gg in s  E J a eger, M  J o n e s  and E. A. Lee, "Scientific Workflow Management and the Kepler System pp. 1039Ö1065 2005   K  K  Munis w am y R e dd y  P  Mack o  and M. Selt zer   P rove nanc e  for th e  Cloud," in pp. 197-210, 2010   H Ng u y en and D A b r a m s o n  W ork W a y s   I n t e r acti ve W o r k flow b a s e d  Science Gateways," in pp. 1-8, 2012   L  J  Os t e rw ei l  L  A   C l a r k e A   M E llis o n   R  Po d o roz hn y A  W i s e   E   Boose, and J. Hadley, "Experience in Using A Process Language to Define Scientific Workflow and Generate Dataset Provenance," in pp. 319-329, 2008   J  Q iu, J  Ekana y ak e  T  G una r a thn e   J   Y Ch o i, S  H  Ba e, H  L i  B   Zhang, Y. Ryan, S. Ekanayake, T.-L. Wu, A. Hughes, and G. Fox Hybrid Cloud and Cluster Computing Paradigms for Life Science Applications vol. 11, 2010 26  X  Su  Y M a   H  Ya n g   X C h a n g  K  N a n  J  Xu  a n d  K N i n g   An Op en Source Collaboration Environment for Metagenomics Research," in pp. 7-14, 2011 27  A   S  S z al ay a n d J   G r ay    S c i e n ce i n a n  Ex po ne n t i a l  W o r l d    vol 440, pp. 23-24, 2006   S. Toor  M. S a b e s a n, S  H o lm gre n and T   R i s c h, "A S c a l ab le A r chit ecture for e-Science Data Management," in pp. 210-217, 2011   D W a r n e k e and O  K a o  E x p loiting Dy na m i c R e s o u r c e A llocati o n  for  Efficient Parallel Data Processing in the Cloud vol. 22, pp. 985-997, 2011   Y. Y a n g K  L i u, J   Ch e n X  L i u, D   Yuan and H   J i n   A n A l gorithm  in SwinDeW-C for Scheduling Transaction-Intensive Cost-Constrained Cloud Workflows," in pp. 374-375, 2008  L   Yo un g Ch oo n and A  Y Z o m a y a   E n e rgy C o ns ci o u s Sch e du l in g for  Distributed Computing Systems under Different Operating Conditions vol. 22, pp. 13741381, 2011   D Y u an  Y   Y a n g  X   L i u  and J  Ch e n   A C o st-Effecti ve S t r a t e gy  for Intermediate Data Storage in Scientific Cloud Workflows," in  pp. 1-12, 2010   D Y u an Y   Y a n g   X   L i u and J  Ch e n  O nd e m a nd Minim um C os t  Benchmarking for Intermediate Datasets Storage in Scientific Cloud Workflow Systems vol 71, pp. 316-332, 2011 34  D  Yu a n   Y  Ya n g X L i u  W  L i  L   C u i   M  Xu  a n d  J   C h en    A Hi gh l y Practical Approach towards Achieving Minimum Datasets Storage Cost in the Cloud vol 24, pp. 1234-1244, 2012 35  D  Yu a n   Y   Ya n g  X L i u  G Z h a n g  a n d  J  C h e n    A Da t a D e p e n d e n c y  Based Strategy for Intermediate Data Storage in Scientific Cloud Workflow Systems vol. 24, pp. 956-976, 2012   M. Z a h a r i a, A  K o n w ins ki A  D   J o s e ph R. K a t z and I  S t o ica  Improving MapReduce Performance in Heterogeneous Environments," in pp. 29-42, 2008  
real world, the price of cloud storage is different according to different usages. In the future, we will incorporate more complex pricing models in our datasets storage cost model Furthermore, methods for forecasting dataset usage frequency can be further studied, with which our T-CSB algorithm can be adapted to different types of applications more easily A CKNOWLEDGMENT  The research work reported here is partly supported by Australian Research Council under DP110101340 and LP130100324, Shanghai Knowledge Service Platform Project No. ZF1213. We are also grateful for the discussions on Finite Element Modelling application with Dr. S. Xu from Faculty of Engineering and Industrial Sciences, Swinburne University of Technology R EFERENCES    A m a zo n C l o ud S e rv ic es    http://aws.amazon.com  2  I  A d a m s  D  D  E  L o n g  E  L   M i l l e r   S  P a s u p a t h y  a n d  M  W  S t o r e r   Maximizing Efficiency by Trading Storage for Computation," in 
Workshop on Hot Topics in Cloud Computing \(HotCloud'09 IEEE International Conference on Cloud Computing \(CLOUD2011 Communication of the ACM 7th International Conference on E-Science \(e-Science2011 Proceedings of the IEEE ACM Computing Surveys 5th International Conference on E-Science \(eScience'09 8th International Conference on E-Science \(eScience2012 IEEE INFOCOM 2011 ACM/IEEE Conference on Supercomputing \(SC'08 14th International Conference on Scientific and Statistical Database Management, \(SSDBM'02 Grid Computing Environments Workshop \(GCE'08 Future Generation Computer Systems 9th Symposium on Operating Systems Design and Implementation \(OSDI'2010 7th International Conference on E-Science \(e-Science2011 8th International Conference on E-Science \(e-Science2012 ACM/IEEE Conference on Supercomputing \(SC'10 23th IEEE International  Parallel & Distributed Processing Symposium IPDPS'09 Journal of Systems and Software Concurrency and Computation: Practice and Experience 8th USENIX Conference on File and Storage Technology  FAS T'10 8th International Conference on E-Science \(eScience2012 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering Journal of BMC Bioinformatics 7th International Conference on E-Science \(e-Science2011 Nature 7th International Conference on EScience \(e-Science2011 IEEE Transactions on Parallel and Distributed Systems 4th International Conference on E-Science \(eScience2008 IEEE Transactions on Parallel and Distributed Systems 24th IEEE International Parallel & Distributed Processing Symposium \(IPDPS'10 Journal of Parallel and Distributed Computing IEEE Transactions on Parallel and Distributed Systems Concurrency and Computation: Practice and Experience 8th USENIX Symposium on Operating Systems Design and Implementation \(OSDI'2008 
292 


Jorda Polo, David Carrera, Yolanda Becerra, Malgorzata Steinder  and Ian Whalley. Performance-driven task co-scheduling for  mapreduce environments. In Network Operations and Management  Symposium \(NOMS\2010 IEEE, pages 373 Ö380, 19-23 2010 12 K. Kc and K. Anyanwu, çScheduling hadoop jobs to meet deadlines  in 2nd IEEE International Conference on Cloud Computing  Technology and Science \(CloudCom\, 2010, pp. 388 Ö392 13 Xicheng Dong, Ying Wang, Huaming Liao çScheduling Mixed Real time and Non-real-time Applications in MapReduce Environment  In the proceeding of 17th International Conference on Parallel and  Distributed Systems. 2011, pp. 9 Ö 16 14 Xuan Lin, Ying Lu, J. Deogun, and S. Goddard. Real-time divisible  load scheduling for cluster computing. In Real Time and Embedded  Technology and Applications Symposium, 2007. RTAS ê07. 13th  IEEE pages 303 Ö314, 3-6 2007 15 HDFS  http://hadoop.apache.org/common/docs/current/hdfsdesign.html  16 Chen He, Ying Lu, David Swanson. çMatchmaking : A New  MapReduce Scheduling Techniqueé. In the proceeding of 2011  CloudCom, Athens, Greece, 2011, pp. 40 Ö 47 17 Matei Zaharia, Dhruba Borthakur, Joydeep Sen Sarma and Khaled  Elmeleegy, Scott Shenker, and Ion Stoica, çDelay scheduling: a  simple technique for achieving locality and fairness in cluster  schedulingé. In the proceedings of the 5th European conference on  Computer systems, 2010.  pp 265-278 18 Zhuo Tang, Junqing Zhou, Kenli Li, and Ruixuan Li "A MapReduce  task scheduling algorithm for deadline constraints.", Cluster  Computing, Vol. 15,  2012 19 Eunji Hwang, and Kyong Hoon Kim. "Minimizing Cost of Virtual  Machines for Deadline-Constrained MapReduce Applications in the  Cloud." Grid Computing \(GRID\, 2012 ACM/IEEE 13th  International Conference on. IEEE, 2012 20 Micheal Mattess, Rodrigo N. Calheiros, and Rajkumar Buyya  Scaling MapReduce Applications across Hybrid Clouds to Meet Soft  Deadlines." Technical Report CLOUDS-TR-2012-5, Cloud  Computing and Distributed Systems Laboratory, the University of  Melbourne, August 15, 2012 21 
 
11 
                
Chen He, Ying Lu, David Swanson. çReal-Time Application Scheduling in Heterogeneous MapReduce Environments Technical Report TR-UNL-CSE2012-0004, University  of Nebraska-Lincoln, 2012 Available: http://cse apps.unl.edu/facdb/publications/TR-UNL-CSE20120004.pdf 22 T. Condie, N. Conway, P. Alvaro, J. M. Hellerstein, K  Elmleegy, and R. Sears. çMapreduce Onlineé. In NSDI 2010 23 A. D. Ferguson, P. BodÌk, S. Kandula, E. Boutin, and R  Fonseca. çJockey: Guaranteed Job Latency in Data Parallel Clusters. In EuroSys, 2012 24 G. Wang, A. R. Butt, P. Pandey, and K. Gupta. çA Simulation Approach to Evaluating Design Decisions in MapReduce Setupsé. In MASCOTS 2009 25 H. Herodotou and S. Babu. Profiling, çWhat-if Analysis and Cost-based Optimization of MapReduce Programs In VLDB 2011 26 H. Herodotou, F. Dong, and S. Babu. çNo One \(Cluster Size Fits All: Automatic Cluster Sizing for Dataintensive Analyticsé. In SoCC 2011  
1544 
1544 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


