Set-Valued Multiple-Target Tracking Wynn Stirling and Jeffrey Thompson Electrical  Computer Engineering Department CB459 Brigham Young University Provo UT 84602 Abstract The performance of a MTT multiple target tracking method should degrade gracefully as the conditions of the collection become less favorable to optimal operation By stressing the avoidance, rather tlian the explicit minimization of error we obtain a decision rule for trajectory-data association that does not require the resolution of all conflicting hypotheses when the database does not contain sufficient informa tion to do so reliably I INTRODUCTION Multiple Target Tracking 
MTT is a time-varying joint decision and estimation problem consisting of a rule t,o associate sensor outputs and target vehicle trajecto rics and an estimator to calculate the trajectories based upon the associated sensor outputs The solution to this problem requires the extraction from the data of as much iiiformation as possible about the number of targets and the trajectory of each It is desirable to distinguish reli ably all targets of interest from background noise and to associate accurately each target with the available data We 
develop a trajectory-data association decision rule khat is expressed in terms of two probabilities one govern ing the informational value of the association hypothesis and one governing the subjective belief or credal prob ability of the association hypothesis 2 We also define a criterion of serious possibility and all trajectory-data associations that are seriously possible are retained it is tiot necessary to resolve all conflicting hypotheses before processing more data We employ an set-valued estimator rather than the conventional point-valued estimator to describe the evo lut,ion 
of t,he vehicle trajectory The set-valued estimator is based upon the set-valued Kalman filter 3 which com prit,es a convex set of trajectories all with equal claims t,o validity given the observations If a trajectory is com Manuscript received 29 July 1991 This work was supported by ESL Inc Sunnyvale CA pletely observable the radius of the convex set decreases to zero as the quantity of data increases and the set-valued estimate asymptotically becomes point-valued Under less favorable circumstances 
the radius of the set remains fi nite and may even grow thus providing a more comprehen sive characterization of the trajectory than does a single point estimate 11 EPISTEMIC UTILITY THEORY For an inquiry under investigation, suppose there are finitely many hypotheses that may be considered Let U denote this set of possible answers and assume that exactly one element of U is correct and that all elements of U are consistent with our present state of knowledge U is said to be an ultimate 
partition, and a potential answer is the collection of hypotheses remaining after we have rejected all members of a subset of U Each element of a poten tial answer is said to be a serious possibility A potential answer is degenerate if no elements of U are rejected and we may not reject all members of U Epistemic utility is a probability that is composed of a convex combination of two  one measuring the importance of acquiring new information the other measuring the importance 
of avoiding error For any g c U we define the utility of accepting g in the interest of avoiding error as 7\(9;1  1 if C  true and 7\(g;l  0 if C  false In addition to the cost of error we also apportion a unit of informational value to each hypoth esis hi E U by assigning to elements of U non-negative real values such that their sum is unity If we enumerate 
U  hl hz   hn and let M\(hj 2 0 denote the value assigned to hj then C;=lM\(hj  1 and for any set g c U we define M\(s p@j 1 hjEg as the informational value of rejecting g The utility of accepting g in the interest of acquiring new information regardless of its truth-value is then C\(g  1  M\(g 773 ISSN 0-7803-0233-8/91$1.0001991 IEE 


We may address the conflict that exists between the goals of avoiding error and acquiring information by defin ing an epistemic utility function for acquiring error-free knowledge that is making a decision as the convex com bination u\(g e  al\(g e  1  a g The quantity Y represents the relative importance attached to avoiding error versus acquiring new information We must restrict 5 a 5 1 to ensure that no erroneous answer is pre ferred to any correct answer Since all utility functions that are related by a positive linear transformation are equivalent we may simplify this utility function by defin ing u"\(g;e  iu\(g   The resulting utility func trion for accepting g in the interest of both avoiding error and acquiring new knowledge is where b   is the coefficient of boldness This coeffi cient is constrained to lie in the interval 0 11 The closer b is to unity the less caution is exercised that error will be int,roduced increased boldness in accepting hypotheses t,he closer b is to zero the lower the risk of error decreased 11 o 1 d n ess  We must also establish a probabilistic measure of belief for the hypotheses that are available Credal probability is probability formed on the basis of subjective judgment represents the likelihood that an option is true and is i ridependent of any informational value or demand that nriglit be associated with the option For a given ultimate partition U let Q\(g denote the credal probability assignment to any element g c U For g c U the expected utility is EQ"~\(S  1  bM\(g g  bM\(g the demand for knowl edge renders the risk of error worthwhile We may adopt any set of hypotheses in the Boolean algebra generated by the elements of the ultimate parti t,ion U This expands our possibilities we are not con strained to select only the elementary hypotheses hi but itlay choose any subset of them. This decision philosophy inlay be summarized as follows Levi's Rule of Expected Utility l page 531 Given n jiirile ultimate partition U an information-determining Ilrohability function M defined over the Boolean algebra oj eleriients of U an expectation-determining probability funclion Q defined over the same algebra and an index of boldness b the agent should reject all and only those elements of hi E U satisfying hi  bM\(hi 111 MULTIPLE TARGET TRACKING A Track Initialization We assume that each track is characterized by a linear stochastic dynamics model of the form xjt  Fjt-lxjt-1  Gjt-1ujt-1 3 where j  1,2    and ujt-1  N\(0 Qjt-1 i.e uit-1 is Gaussian with mean 0 and covariance matrix Qjt-1 there are  active tracks at time t  is unknown The number of tracks is allowed to vary since tracks may be initiated or terminated at any time We assume that all tracks lie in the same state space In the interest of brevity we shall restrict attention to the outputs of a single sensor and assume that this output may be characterized by a linear stochastic model of the form Zit  Hitxjt  Vit i  1  St 4 where st is the number of observations vectors at time t and zit is an Tit-dimensional random vector and vjt  N\(0  Each observation vector therefore lies in an rit dimensional space corresponding to the column space of Hit We assume that each such space is a subset of the state space rit 5 n We do not however require all observations to lie in the same dimensional subspace of the state space and we permit the dimensionality of these column spaces to be time-varying Before data are collected we characterize the target environment with one set-valued track defined by an initial credal matrix KO an initial centroid state col and a prior covariance matrix IIo The initial state-vector set is XOlO   x  NI polo  c E X0lO I where T 1 XOlO  I E 3  c qo solo c-cop L 1 1 Polo  no is a positive-definite matrix and Solo  Ii'oIi'T For sample times t  1,2   we observe st observations z  g Let us suppose at time t that we have 1 sets of predicted from time t  1 random variables of the form Xj,t-l  x  N J$-J c E x:lt-l  where X;lt-l  I 200 J2  c   1 Tj stIt-l I  4ItJ 5 I 1 5 774 


for j  1   rt-1 We shall use the same Ft Gt and Qt iriatrices for each track thus it will not be necessary to ititlex these matrices with their track identifications 11 The Ultimate Partition At each time t there exist st observation sample vec t,ors of the random variables and rt-1 prrtlicted track sets x~lt-l We wish to make deci sions regarding the association of each zit with each track set xi t  We desire to apply Levi\222s decision-making inetliodology to this problem and will therefore adopt Iflie strategy of accepting all track/data associations that r;i.nnot be rejected on the basis of Levi\222s rule of epistemic 11 1 il i t,y We must define an ultimate partition for each sample vcct,or resulting in a set of st ultimate partitions of the forin U  hitl,...,hitTt--llhitTt i  l,...,st where where 0 signifies the hypothesis that none of the track sct,s associate with the observation zit We shall say that 1,rack set xi,t-l is associated with observation sample vec or zrt if we fail to reject the hypothesis hitj Each ultimate I UZt has the property that exactly one element ih riie alt,hough each is logically possible According to Lcvi\222s theory of expected utility we may reject only those tiic~ttibcrs of the ultimate partition that are not seriously possible We do not insist that the decision that one and only one element of Vit be chosen as the association deci sion C Calculataon of the M-Functaon 221l\222lie M-function is intended to measure the information value of rejecting an association rather than the truth viiltle of an association A measure of the information viiliie of rejecting the association of a predicted track set with an observation zit is the distance between sattiple values of the observation and and the track set if t,he distance is small there is little value in rejecting the association in other words there is great value in accept ing it For g E x:lt-l c Rn and zit E Rrst we define the generalized distance between them as def T 3  d\(c,%t  IHitc-LitII  Hitc-z Hit zit We desire to normalize this distance to permit its inter pretation as a probability density function This normal ization is accomplished by defining a region of the column space of Hit which we may assume contains all predicted observation values that may be feasibly associated with the given value for zit and restricting attention only to this region termed the seriously possible region for zit which we shall denote as zit a convex set centered at zit of the form zit  C E 221  I  zitel 5 epie,.e  1 Tit  9 where Rit  diag pi\224   P and 0 a given constant Since we wish M to be a probability we require that the distance function d\(g t be normalized thereby ad mitting the interpretation as an information-determining probability density function We must normalize this func tion by the seriously possible region that is for fixed zit let for all whose projection onto the space spanned by the columns of Hit lies in Zit We shall denote this space by Zit  c E Rn  Pig E Zit where Pit is the projection operator onto the space spanned by the columns of Hit The function mil may be viewed as a normalized distance between zit and g E ft-l and is a measure of the information gained by rejecting the association of  2 E x&-1 and zit Intuitively the greater the distance between the predicted observation and the actual obser vat ion the greater the value in rejecting the hypothesis that the predicted state estimate and the observation are associated Let B  be a ball with center at E x{,t-l The information value of rejecting the association of E  with zit is 11 where Pit is the projection of the ball  onto the space spanned by the columns of Hit The vector Pitg is the projection of g onto the same column space We empha size that the informational-value determining probability places all of the probability mass in the column space of Hit This result is appropriate since there is no way of assessing the informational value of rejecting track/data associations by means of components of the state that lie in the subspace orthogonal to the column space of Hit 


For g E and g E with g  g let the diameter of the balls l and  become arbitrarily small Then the condition Mit 2 M;t indicates that the information value of accepting the association of g with L is greater than the value of accepting the association of g and zit D Calculation of the Q-Functions The credal probability or Q-function is the probability that a given track-data association is correct This prob ability is a function of the statistical descriptions of the t.arget dynamics and of the observation errors and is char acoerized by the conditional distribution of the observation given the track For each j  1 t-l the set of pre dicted random vectors is given by xilt-l  x NC  3 E x:lt-l  For each observation random vector zit we represent the c,orresponding set of filtered random vectors by it  x  x  x Wi,[Zit  HitX x E X:lt-l 1 where Wit is the Kalman gain matrix given via the Kalman I i I er  The probability distributions of x E X are cliaracterized by the family of posterior distributions g E xjlt-l obtained via Bayes rule We shall restrict consideration to the case where x E X:lt-l and zit are jointly ,normal Under this hypothesis the posterior density pytlt\(<;g is also normal and it is well known t.liat the mean and variance associated with this condi t.ional distribution is given by the Kalman filter For each olwrvat zit  ziti we calculate the filtered set-valued c'st,iii-iate according to for J  1   t-1 with W the Kalman gain defined by lv~jt  p;lt-lHs HztP~lt-lH  R,t  The posterior tlcnsit,ies assume the form 1 where we define The posterior density is with x a function of 3 given by 17 The family of densities piJtlt\(g;g g E pro vides a measure of the truth-value of the association of each g E  with the observation zit We may view this family of densities as credal probability densities and interpret them as characterizing the belief that the above association is correct Let E be a ball with center at g The credal prob ability that the true state lies in  conditioned on the observational value zit is For g E x:lt-l and g E with g  g the condition Qijt g  Qijt 2 indicates that the association of g with zit is more credible or believable than the as sociation of g and zit E The association decision problem is to determine whether or not the track x:lt-l is associated with the observation zit We shall assume a conservative attitude and say that the entire track set 2C:lt-l is associated with zit if any  3 E x:lt-l is associated with zit The decision rule may be formulated in terms of the inf0rmation:determining probability density mit g where g E xilt-l and the family of subjective be lief or credal probability density functions functions g E X:lt-l We desire to apply Levi's rule of expected utility to this problem For g E Xt,t-l let B be a ball with center at g Using 11 and 19 Levi's rule of expected utility indicates we may not reject the association of B and zit if The Association Likelihood Ratio Test  Qijt\(&;g  2 bMit\(B 20 Now let the radius of B  go to zero and define the function 


IV EXAMPLE 1 e-i\(g  aT[P  2 Jt\(C  24 IP Since the densities are continuous at E a necessary condi tion for 20 to hold for all balls  is that sume that angle-of-arrival data are available further we assume that the sensor is sufficiently far from the target If 22 holds for any 2 E xilt-l we may not reject the association of g with zit consequently we may not reject tlie association of the track set z:lt-l with zit Since To illustrate the concept of the ALRT methodology consider the case of tracking targets constrained to planar motion 4 Let 2  x y k ylT denote the kinematic state of a target in some convenient coordinate system n  4 The dynamics equation is that is rit 5 2 Let 2  x1,x2,x3,z4]T E  ziti zitzIT and suppose Rit  diag{pf,pi Then Given the information-value determining probability density 10 and the credal probability density 23 we e~eriously possible region is I nay forrn the Association Likelihood Ratio Test ALRT zit     tcl  Zitli 5 oPl 1c2  Zit2i I ep2 I Let be a predicted track set and let zit be 1 sniiiple value of the observation vector zit We shall sciy that x:lt-l and zit are associated with boldness b if qijt _ bmit\(g for some 2 E zitng<lt-l That is ihere and the information-value determining probability density is  25 exists E Zit n such that x1  Zit1  22  Zit2 mit E  i2 s!;L1 Jmdcldcz 1  2 7r I P$l i e--:\(H,12  fit  H.iC  zit Each 2 E x{lt-l represents the mean value of a pre dicted conditional distribution For each such g there ex  24 ists a filtered conditional density of the form IIHitZ  Lit11 b J t~~itg gitlld pFtlt\(f  N{~E  wijt\(git  HitE I  it~itI~tlt-1 211 which case yt obtained via 12 is a filtered track scl If io be dissociated and the set z:it has no meaning and is discarded If survives the ALRT for fit then the likeli hood that is associated with zit is greater than the information value gained by rejecting the association If b  1 then the decision strategy is maximally bold in tlie sense that as many associations lt-l,st will be re.iccted as possible As b approaches zero the decision strategy is maximally cautious and the likelihood dimin islies that a correct association indeed any association will be rejected n Sit  8 then we deem zit and 26  x E xi,t-l and evaluating this expression at f  c 27r P I f qijt  g21 Gal 22  ZitZ]W;*[P;;,]-lW,,1   z   e 27 The ALRT is Associate and zit  zitl,~itz if for any g  xi xz,23 241T E Xtp-1 qijt\(2 2 bmit\(E Figure l\(a illustrates a family of three crossing tra jectories The tracks move generally from left to right at time increases the lines correspond to the x and y posi tion components and the  symbols correspond to noise corrupted observations Figure 1 b displays the filtered 777 


track sets corresponding to this simulation with the el lipses representing the projections of the track sets onto position space The initial predicted track set Xo1-1 in cludes the entire field of view and is not shown The three large elliptical regions correspond to the the filtered track sets after the first set of observations have been processed As time increases the size of these track sets decreases rapidly for the first few observations corresponding to Tracks 1 and 3 however there are multiple associations since the tracks are fairly close and the track sets are still fairly large As more confidence is obtained in the associ ations these tracks become uniquely associated and the elliptical regions decrease rapidly in size and will asymp t,otically become point tracks At sample ten Tracks 1 and 2 nearly intersect and both tracks associate with the observations These multiple associations persist for a few samples but as the tracks diverge the associations again become unique Track 2 is unambiguously associated and estiniatfed as evidenced by the radius of the track set con verging to zero and the set-valued estimates asymptoti cally become point-valued Y V CONCLUSION The use of epistemic utility combined with set-valued est,iination makes it possible to design a multiple tar get tracking procedure that extracts as much information as possible from a given database without attempting to squeeze more information from the database than it con tains REFERENCES l I Levi The Enterprise of Knowledge MIT Press Cambridge, Massachusetts 1980 2 W C Stirling and D R Morrell Convex Bayes Deci sion Theory IEEE Transacttons on Systems Man and Cybernetics 21 1 January/February 1991 3 D R Morrell and W C Stirling Set-Valued Fil tering and Smoothing IEEE Transactions on Sys tents Man and Cybernetics 21\(1 Jan uary/February 1991 4 W C Stirling D R Morrell G R Morell and J B Thompson Convex Bayes Decision Theory and Set Valued Estimation A New Approach to Multiple Tar get Tracking Technical Report TR-S107-91.1, Electri cal and Comput,er Engineering Department  Brigham Young University Provo Utah 1991 X X Figure 1 Three Crossing Tracks a simulated trajectories and observations b filtered track sets 778 


Figure 3 Current beneflts and estimated final benefits when sampling size k Increases up to K  256 for all three datesets. The error range Is 3 IK for 99.7 contldence ILI Im Y na 1 I Y rm Y na no lunpwS*.INmmnCi.m IO im In na 2s sb Wdd C.ul*\223l funm Sk.lNlunmolC.ul mation methods can effectively detect the inaccuracy of the complete model, the user can choose a smaller K We par titioned all three dataset into K  1024 partitions For the adult dataset, each partition contains only 32 examples but there are 15 attributes The estimation results are shown in Figure 4 The first observation is that the total benefits for donation and adult are much lower than the baseline This is obviously due to the trivial size of each data partition The total benefits for the credit card dataset is 750,000 which is still higher than the baseline of 733980 The second ob servation is that after the sampling size k exceeds around as small as 25 out of K  1024 or 0.5 the error bound becomes small enough implying that the total benefits by the complete model is very unlikely 99.7 confidence to increase At this point the user should cancel the learn ing for both donation and adult datasets The reason for the 223bumps\224 in the adult dataset plot is that each dataset is too small and most decision trees will always predict N most of the time At the beginning of the sampling, there is no variations or all the trees make the same predictions; when more trees are introduced it starts to have some diversities However the absolute value of the bumps are less than 50 as compared to 12435 3.5 Training Efficiency We recorded both the training time of the hatch mode single model plus the time to classify the test data and the training time of the multiple model with k  30 K clas sifiers plus the time to classify the test data k times We then computed ratio of the recorded time of the single and mul tiple models called serial improvement It is the number of limes that training the multiple model is faster than training the single model In Figure 5 we plot the serial improve ment for all three datasets using C4.5 as the base learner When K  256 using the multiple model not only provide higher accuracy but the training time is also 80 times faster for credit card 25 times faster for both adult and donation 4 Related Work Online aggregation has been well studied in database community It estimates the result of an aggregate query such as avg AGE during query processing One of the most noteworthy work is due to 7 which provides an in teractive and accurate method to estimate the result of ag gregation One of the earliest work to use data reduction techniques to scale up inductive learning is due to Chan I in which he builds a tree of classifiers In BOAT 6 Gehrke et al build multiple bootstrapped trees in memory to ex amine the splitting conditions of a coarse tree There has been several advances in cost-sensitive learning 3 Meta Cost 4 takes advantage of purposeful mis-labels to max imize total benefits In 181 Provost and Fawcett study the problem on how to make optimal decision when cost is not known precisely 5 Conclusion In this paper, we have demonstrated the need for a pro gressive and interactive approach of inductive learning where the users can have full control of the learning process An important feature is the ability to estimate the accuracy of complete model and remaining training time We have im plemented a progressive modeling framework based on av eraging ensembles and statistical techniques One impor tant result of this paper is the derivation of error bounds used in performance estimation We empirically evaluated our approaches using several inductive learning algorithms First, we find that the accuracy and training time by the pro gressive modeling framework maintain or greatly improve over batch mode learning Second the precision of estima tion is high The error hound is within 5 of the true value when the model is approximately 25  30 complete Based on our studies, we conclude that progressive mod eling based on ensemble of classifiers provide an effective 169 


Figure 4 Current benefits and estimated final estimates when sampling size k increases up to K  1024 for all three datasets To enlarge the plots when k is small we only plot up to k  50 The error range is 3 u\(uK for 99.7 confidence  Figure 5 Serial improvement jir4 for all three datasets when early stopping Is used 3 10 I 1 8 10 m s Irn 1m m zra m rn 1 m zra m Irn Im m ma WC4P.n WdP./Cn WdPb solution to the frustrating process of batch mode learning References 6 I Gehrke V Ganti R Ramakrishnan and W.-Y Loh BOAT-optimistic decision tree construction In Pro ceedings of ACM SICMOD International Conference on Management of Data SICMOD 1999 1999 7 J M Hellerstein P I Haas and H 1 Wang On line aggregation In Proceedings ofACM SIGMOD In ternationul Conference on Management of Data SIC I P Chan An Exrensible Meru-leurning Approach for Scalable and Accurare Inductive Learning PhD the sis Columbia University Oct 1996 21 W G Cochran Sampling Techniques John Wiley and Sons 1977 131 T Diettench D Margineatu E Provost, and P Tur ney editors Cost-Sensirive Learning Workshop ICML-00 2000 141 P Domingos MetaCost a general method for making classifiers cost-sensitive In Proceedings of Fifth In rernarional Conference on Knowledge Discovery and Dura Mining KDD-99 San Diego, California 1999 51 W Fan H Wang P S Yu and S Slolfo A framework for scalable cost-sensitive learning based on combin ing probabilities and benefits In Second SIAM In ternationul Conference on Datu Mining SDM2002 April 2002 MOD'97 1997 8 F Provost and T Fawcett Robust classification for imprecise environments Machine Learning 42203 23 I 2000 9 S Stolfo W Fan W Lee A Prodromidis and P Chan Credit card fraud detection using meta learning Issues and initial results In AAAI-97 Work shop on FraudDerecrion andRisk Management 1997 Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers In Proceedings of Eigk teenth International Conference on Machine Learning ICML'Z001 2001 IO B Zadrozny and C Elkan 170 


association-cube, base-cube and population-cube are derived from the volume cube; the confidence-cube is derived from the association cube and population cube and the support-cube is derived from the associationcube and base-cube. The slices of these cubes shown in Figure 2 correspond to the same list of values in dimension merchant, time, area and customer_group  Multidimensional and multilevel rules Representing association rules by cubes and underlying cubes by hierarchical dimensions, naturally supports multidimensional and multilevel rules. Also these rules are well organized and can be easily queried  First, the cells of an association cube with different dimension values are related to association rule instances in different scopes. In the association cube CrossSales cell CrossSales product \221A\222, product2 \221B\222  customer_group 221engineer\222, merchant \221Sears\222, area \221Los Angeles\222, time 221Jan98\222 represents the following multidimensional rule x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x,\221B\222  275 customer_group = \221engineer\222, merchant = \221Sears\222, area 221Los  Angeles\222, time =  \221Jan98\222 If this cell has value 4500, and the corresponding cell in the population cube has value 10000, then this rule has confidence 0.45 Next as the cubes representing rules can have hierarchical dimensions, they represent not only multidimensional but also multi-level association rules. For example, the following cells CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221Jan98\222 CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221 Year98 222 represent association rules at different area levels \(i.e the city level and the state level\d different time levels \(i.e., the month level, the year level x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221Jan98\222 x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221 Year98 222 The cell CrossSales\(product \221A\222, product2 \221B\222,  customer_group 221top\222, merchant \221top\222, area \221top\222,  time \221top\222 represents the customer-based cross-sale association rule for all customers, merchants, areas, and times in the given range of these dimensions, expressed as x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222 4.3  Generating Association Rule Related Cubes The basic task of our OLAP based association rule mining framework, either at the GDOS or at an LDOS is to convert a volume cube i.e. the cube representing the purchase volumes of customers dimensioned by product  area etc, into an association cube a base cube and a population cube These cubes are then used to derive the confidence cube and the support cube of multidimensional association rule instances. The following general steps are involved in cross-sale association rule mining 267  Roll up the volume cube SaleUnits by aggregating it along merchant, time, area dimensions 267  Derive cube NumOfBuyers from SaleUnits based on the antecedent condition SaleUnits 0 267  Populate cube NumOfShoppers by the counts of customers dimensioned by merchant, area  time not by product\at satisfy the antecedent conditions 267  Derive cube CrossSales from SaleUnits based on the association conditions SaleUnits  product  p 1  0 and SaleUnits  product2  p 2 0 267  Derive cube Confidence and cube Support using cell-wise operations 214  Confidence = CrossSales  NumOfBuyers 214  Support  CrossSales  NumOfShoppers  Cubes Confidence  Support  CrossSales are dimensioned by product  product2 customer_group,merchant  time, area NumOfBuyers is dimensioned by product  customer_group, merchant time, area  NumOfShoppers is dimensioned by customer_group, merchant  time, area Rules with confidence and support that exceed specified thresholds  may be considered interesting 4.4. Rules with Conjoint Items Cubes with conjoint dimensions can be used to represent refined multidimensional association rules For example, using OLAP, we can derive association rules across time  Time-variant or temporal association rules such as 


x 316 Customers buy_product\(x,\222 A\222, \221 Jan98\222  336 buy _product\(x, \221B\222, \221 Feb98\222   275 area = \221Los Angeles\222 can be used to answer such questions as \223 How are  the sales of B in Feb98  associated with the sales of A in Jan98 224 The items in this rule are value pairs of dimensions product and time In order to specify this kind of association rule we introduce a conjoint dimension product, time and mirror it with dimension product2, time2 This allows a cell in the association cube to cross two time values. Accordingly, the cubes related to association rule mining are defined as follows Association cube  CrossSales.2 \(<product, time>, <product2, time2 customer_group, merchant, area  Population cube  NumOfBuyers.2  \(<product, time>, customer_group merchant, area Base cube  NumOfShoppers.2  \( customer_group, merchant, area Confidence cube Confidence.2 \(<product, time>, <product2, time2 customer_group, merchant, area Support  cube  Support.2  product, time>, <product2, time2 customer_group, merchant, area  The steps for generating these cubes are similar to the ones described before. The major differences are that a cell is dimensioned by, besides others product, time and product2, time2 and the template of the association condition is  SaleUnit s  product p 1 time t 1  0 and  SaleUnits  product2 p 2 time2 t 2  0 where, in any instance of this condition, the time expressed by the value of time2 is not contained in the time expressed by the value of time The template of the antecedent condition is SaleUnits   product p 1 time t 1  0 In general, other dimensions such as area may be added to the conjoint dimensions to specify more refined rules 4.5. Functional Association Rules A multidimensional association rule is functional if its predicates include variables, and the variables in the consequent are functions of those in the antecedent.  For example, functional association rules can be used to answer the following questions, where a_month and a_year are variables q  What is the percentage of people in California who buy a printer in the next month after they bought a PC x 316 Customer buy_product\(x, \221PC\222, a_ month 336 buy_product\(x, \221printer\222, a_month+1  275 area = \221California\222 q  What is the percentage of people who buy a printer within the year when they bought a PC  x 316 Customer: buy_product\(x, \221PC\222, a_ year 336 buy_product\(x, \221printer\222, a_year 275 area = \221California\222 To be distinct, we call the association rules that are not functional as instance association rules; e.g x 316 Customer: buy_product\(x,\222 PC\222, \221Jan98\222 336 buy_product\(x,\222 printer\222, \221Feb98\222  275 area =  \221California\222 Time variant, functional association rules can be derived from time variant, instance association rules through cube restructuring. Let us introduce a new dimension time_delta that has values one_day, two_day 205, at the day level, and values one_month, two_month, \205, at the month level, etc. Then, let us consider the following functional association rule related cubes Association cube  CrossSales.3 \(product, product2, customer_group merchant, area, time_delta  Population cube  NumOfBuyers.3 \(product, customer_group, merchant area Base cube  NumOfShoppers.3 \( customer_group, merchant, area Confidence cube  Confidence.3 \(product, product2, customer_group merchant, area, time_delta Support cube  Support.3 \(product, product2, customer_group, merchant area, time_delta The association cube CrossSales.3  can be constructed from CrossSales.2   The cell values of CrossSales.2  in the selected time and time2 ranges are added to the corresponding cells of CrossSales.3 For example, the count value in cell  CrossSales.2\(<PC, Jan98>, <printer, Feb98>\205 is added to cell \(bin CrossSales.3\(PC, printer, one_month,\205 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


