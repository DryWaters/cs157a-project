Pipelined Compaction for the LSM-tree Zigang Zhang yx  Yinliang Yue y  Bingsheng He z  Jin Xiong y  Mingyu Chen y  Lixin Zhang y  Ninghui Sun y y SKL Computer Architecture ICT CAS x University of Chinese Academy of Sciences z Nanyang Technological University Abstract 227Write-optimized data structures like Log-Structured Merge-tree 050LSM-tree\051 and its variants are widely used in key-value storage systems like BigTable and Cassandra Due to deferral and batching the LSM-tree based storage systems need background compactions to merge key-value entries and keep them sorted for future queries and scans Background compactions play a key role on the performance of the LSM-tree based storage systems Existing studies about the background compaction focus on decreasing the compaction frequency reducing I/Os or con\002ning compactions on hot data key-ranges They do not pay much attention to the computation time in background compactions However the computation time is no longer negligible and even the computation takes more than 60 of the total compaction time in storage systems using 003ashbased SSDs Therefore an alternative method to speedup the compaction is to make good use of the parallelism of underlying hardware including CPUs and I/O devices In this paper we analyze the compaction procedure recognize the performance bottleneck and propose the Pipelined Compaction Procedure 050PCP\051 to better utilize the parallelism of CPUs and I/O devices Theoretical analysis proves that PCP can improve the compaction bandwidth Furthermore we implement PCP in real system and conduct extensive experiments The experimental results show that the pipelined compaction procedure can increase the compaction bandwidth and storage system throughput by 77 and 62 respectively Index Terms 227storage system LSM-tree compaction pipeline I I NTRODUCTION The massive Internet services like cloud computing cloud storage search engine social networking and so on generate more and more data over the time Storage systems must scale out to support the above applications Key-value storage systems known for scalability and ease of use including distributed key-value stores such as BigTable Dynamo  and PNUTS 12 and local k e y-v alue stores lik e Le velDB and Berk ele y DB 13 are widely used to support the network service workloads Since distributed key-value stores comprise many individual local key-value stores the performance of local key-value stores plays a signi\002cant role on the performance of distributed key-value stores This paper focuses on the performance improvements of the local keyvalue store Many applications have stringent latency requirements According to the study of Y ahoo the ratio of writes is increasing in comparison with that of reads Write-optimized data structures are widely used in storage systems to reduce the write latency because most read requests are absorbed by multi-level caches implemented by W eb bro wser s cache CDN Redis Memcached 15 16 17 and OS page cache and modi\002ed data must be written to persistent storage devices such as HDDs and SSDs 050\003ash-based Solid State Disk\051 to ensure data persistence LSM-tree and its variants are the most commonly used write-optimized data structures For example LSM-tree COLA and SAMT 7 are used in LevelDB Hadoop HBase and Cassandra 18 respectively LSM-tree has one memory buffer component and multiple disk resident components When memory buffer is full the buffer data is dumped into a SSTable on disks whose key range may overlap with those of existing SSTables To bound the latency of upcoming point queries and scans background compactions are needed to compact SSTables and keep key-value entries sorted Background compactions move data downwards from upper components to lower components Slow data movements incur write pauses That is the storage system can not serve updates any more until the background compaction completes So background compactions play an important role on the LSM-tree based storage systems performance Some researches have been done to improve the LSM-tree based storage system performance VT-tree uses the stitching technique to avoid unnecessary data movement bLSM  uses the replacement-selection sort algorithm to reduce the compaction frequency PE and bLSM partition the key range to con\002ne compactions in hot key ranges COLA and FD-tree use forw ard pointers to impro v e the lookup performance bLSM uses bloom 002lters to avoid unnecessary I/Os While GTSSL uses the reclamation and re-insert techniques to put data at upper components to expedite lookups it also uses SSDs as the upper components storage media and read data cache However all these researches ignore the computation time in the compaction procedure and do not exploit the parallelism between the I/O resource and the computation resource In storage systems data compression and checksum are always used to reduce the amount of I/Os and verify data integrity which account for a lot of additional computation cycles Besides new types of storage devices like 003ash-based SSDs have been widely used for their high bandwidth and lower latency which decrease the I/O time As a result the computation time is no longer negligible in comparison with the I/O time In practice the computation may take more than 60 of the total compaction time in storage systems using 003ash-based SSDs Each compaction merges the key-value pairs from the 
2014 IEEE 28th International Parallel & Distributed Processing Symposium 1530-2075/14 $31.00 © 2014 IEEE DOI 10.1109/IPDPS.2014.85 777 


050a\051 LSM-tree data structure 050b\051 SSTable layout Fig 1 The basic LSM-tree data structure and SSTable layout adjacent two components in a speci\002ed key range There are multiple data blocks in the key range In the existing compaction procedure the data blocks are scheduled in an ordered manner and the steps of the compaction procedure of each data block are executed sequentially We name the above compaction procedure as S equential C ompaction P rocedure 050SCP\051 There exists two types of resources in storage system i.e the computation resource and the I/O resource Some steps in one data block's compaction procedure utilize the computation resource and the other steps utilize the I/O resource Due to the ordered scheduling of data blocks in SCP either the computation resource or the I/O resource is utilized over a period which results in the underutilization of both the computation resource and the I/O resource Motivated by the fact that there is no data dependency among the data blocks in the same component and the steps of each data block can be scheduled on different hardware components we propose the P ipelined C ompaction P rocedure 050PCP\051 In PCP we partition the compaction key range into multiple sub-key ranges and each sub-key range comprises one or more data blocks Thus the compaction is divided into multiple subtasks Each sub-task is in charge of merging the key-value entries in one sub-key range We exploit the parallelism of the computation resource and the I/O resource to parallelize sub-tasks to further improve the compaction bandwidth To the best of our knowledge this is the 002rst paper to exploit the parallelism of the I/O resource and the computation resource to improve the compaction bandwidth The compaction bandwidth of the PCP depends on the bottleneck stage which has the smallest bandwidth in all stages In the pipelined compaction procedure either the CPU or the storage device may be the bottleneck For the above two cases we propose two parallel variants of PCP named C omputationP arallel P ipelined C ompaction P rocedure 050CPPCP\051 and S torageP arallel P ipelined C ompaction P rocedure 050S-PPCP\051 When the CPU is the bottleneck C-PPCP can be used for background compactions Otherwise S-PPCP is used The I/O-bound cases can be transformed to CPU-bound cases when excessive storage devices are used for reads and writes and the CPU-bound cases can be transformed to I/O-bound cases when excessive CPUs are used for computation We implemented the pipelined compaction procedure on LevelDB which is a representative LSM-tree implementation Compared with LevelDB the pipelined compaction procedure increases the compaction bandwidth by 77 and improves the throughput by 62 The parallel pipelined compaction procedure improves the compaction bandwidth and throughput by 89 and 64 respectively The rest of this paper is organized as follows Background and motivation are presented in Section II Section III analyzes the SCP and use the 050Parallel\051 Pipelined Compaction Procedure to improve the compaction bandwidth Implementation and performance evaluation through extensive experiments appear in Section IV Related work is discussed in Section V Section VI concludes the paper II B ACKGROUND AND M OTIVATION A Background LSM-tree is a disk-based data structure designed to provide indexing for the workloads that have a high rate of records inserts 050updates and deletes\051 over an extended period LSM-tree is composed of a memory buffer component C 0 and multiple disk components C 1      and C k  Each component size is limited to a prede\002ned threshold which grows exponentially Figure 1\050a\051 illustrates one basic LSM-tree data structure LSMtree supports inserts updates deletes point queries and scans LSM-tree uses an algorithm that defers and batches index updates cascading the changes from a memory buffer through many disk components The algorithm gathers random small I/Os into sequential large I/Os 
778 


Fig 2 The compaction procedure Each disk component consists of multiple SSTables whose key ranges do not overlap with each other except those in C 1  Figure 1\050b\051 presents the layout of the SSTable Each SSTable contains multiple data blocks and one index block The data blocks contain the sorted key-value pairs The index block indexes all the data blocks containing the start key the end key and the offset of each data block in the SSTable As more key-value entries are inserted into memtable the memtable is 002lled up and the buffer data is dumped into one C 1 SSTable onto disk For example the new built SSTable is T 12 in Figure 1\050a\051 The key range of T 12 may overlap the key range of the existing T 11  When C i exceeds its threshold size the compaction procedure for C i is triggered The key-value pairs in a speci\002c key range from the corresponding SSTables in C i and C i 1 are merged into multiple size-limited SSTables in C i 1  Through this way the data 003ows downwards from C i to C i 1  For example when C 2 exceeds its threshold size the compaction procedure for C 2 is triggered The compaction procedure picks T 22 in C 2 and T 32 and T 33 in C 3 whose key ranges overlap with the key range of T 22  Then T 22  T 32 and T 33 are compacted into T 35  T 36 and T 37 in C 3  As a result the data in T 22 003ows downwards from C 2 to C 3  One compaction merges the key-value pairs in a speci\002c key range which consists of multiple data blocks In the existing compaction procedure the data blocks are scheduled orderly The compaction procedure of each data block comprises seven steps illustrated as Figure 2 And the steps of one data block are executed sequentially for data dependency The compaction procedure iterates the following steps for each data block until the last data block has been compacted 1\051  Step 1\050S1\051 READ Read one data block along with its checksum from disk into memory 2\051  Step 2\050S2\051 CHECKSUM Calculate the checksum of the data block and compare it with the original checksum which is read from disk in Step 1 to verify the integrity of the data block 3\051  Step 3\050S3\051 DECOMPRESS Decompress the data block and restore the original key-value pairs 4\051  Step 4\050S4\051 SORT Merge the key-value pairs from the two components and build new data block 5\051  Step 5\050S5\051 COMPRESS Compress the new built data block to improve the future read and write performance 6\051  Step 6\050S6\051 RE-CHECKSUM Calculate the checksum of each compressed data block for future integrity check in Step 2  7\051  Step 7\050S7\051 WRITE Write the compressed data block along with its checksum which is calculated in Step 6 to the disk B Motivation In the existing compaction procedure the data blocks are scheduled in an ordered manner and the steps of the compaction procedure of each data block are executed sequentially for the data dependency Step 1 and Step 7 utilize the I/O resource and all the other steps utilize the computation resource Due to the ordered scheduling of data blocks only one type of resource either the computation resource or the I/O resource is used over a period which results in the severe underutilization of both the computation resource and the I/O resource Because the key ranges of different data blocks in the same component do not overlap there is no data dependency among them So it's not necessary to schedule the data blocks in an ordered manner Motivated by the fact that there is no data dependency among different data blocks and the steps of each data block can be scheduled on different hardware components we propose the Pipelined Compaction Procedure 050 PCP 051 In PCP  we partition the compaction key range into multiple sub-key ranges thus the compaction is divided into multiple sub-tasks Each sub-task compacts one or more data blocks PCP exploits the parallelism of the computation resource and the I/O resource to parallelize the sub-tasks to improve the compaction bandwidth III D ESIGN This section discusses the design of the Pipelined Compaction Procedure First we analyze the existing Sequential Compaction Procedure 050SCP\051 and point out its inef\002ciency Then we improve the SCP with pipeline model and propose the Pipelined Compaction Procedure 050PCP\051 For different hardware con\002gurations PCP suffers different bottleneck stages maybe I/O-bound or CPU-bound Furthermore we propose two parallel variants of PCP Storage-Parallel Pipelined Compaction Procedure 050S-PPCP\051 and ComputationParallel Pipelined Compaction Procedure 050C-PPCP\051 to alleviate the I/O bottleneck and CPU bottleneck When excessive storage devices are used for I/Os the I/O-bound cases can be transformed to CPU-bound cases and when excessive CPUs are used for computation the CPU-bound cases can be transformed to I/O-bound cases A Sequential Compaction Procedure 050 SCP 051 As described in Section II-A one compaction compacts multiple data blocks and the compaction procedure of each data block comprises seven steps In existing conventional compaction procedure the data blocks are scheduled orderly as Figure 3\050a\051 Step 1 and Step 7 utilize the I/O resource i.e HDD or SSD All the other steps utilize computation resource i.e CPU or GPU Suppose that the storage resource and computation resource in the storage system are HDD and CPU The 
779 


050a\051 Sequential execution in Sequential Compaction Procedure 050b\051 CPU and Disk utilization in Sequential Compaction Procedure Fig 3 The execution process and utilization of CPU and Disk of the Sequential Compaction Procedure Fig 4 Ideal Pipelined Compaction Procedure Step 1 and Step 7 are scheduled on disk and all the other steps are scheduled on CPU utilization of resources in Sequential Compaction Procedure is depicted as Figure 3\050b\051 As one data block is processed on disk and CPUs orderly when the HDD transfers data from disk to memory in Step 1 or in the reverse direction in Step 7 through direct memory access the CPU is idle When the CPU does the computation Step 2 030 Step 6 including integrity verifying decompressing compressing and merging the HDD is idle The HDD and CPU do work sequentially and wait for each other for input Over a period only one type of resource is used which results in severe underutilization of both the I/O resource and the computation resource In this paper we use compaction bandwidth as the metric to evaluate the performance of compaction procedure Compaction bandwidth indicates the amount of data compacted in one time unit l represents the length of one data block t S i represents the execution time of Step i for one data block We can get the compaction bandwidth of SCP as Equation 1 B scp  l  7 P i 1 t S i 0501\051 B Pipelined Compaction Procedure A naive approach to improve the compaction bandwidth for SCP is to parallelize the steps of one data block For example one can do the Step 1 and Step 2 simultaneously to reduce the execution time However the steps of one data block can not be parallelized due to data dependency The good news is that although there are many dependencies within the steps of one data block there are no data dependencies across subsequent data blocks in the same component due to that their key ranges do not overlap Motivated by the fact that there are no data dependencies among different data blocks in the same component and the steps of each data block can be scheduled on different hardware components we exploit the pipeline model to SCP and propose the Pipelined Compaction Procedure 050 PCP 051 Hence our approach is to exploit the inter-data block parallelism to overlap the I/O time and the computation time of one data block with that of other data blocks PCP partitions the compaction key range into multiple sub-key ranges Each sub-key range consists of one or more data blocks PCP exploit the parallelism between the disk and CPU A natural question is that how many stages the Pipelined Compaction Procedure should be divided into A straightforward way is that Step 1 and Step 7 are two individual stages and the other steps are partitioned into multiple stages evenly according to the execution time That is Step 2 030 Step 6 are partitioned into multiple stages Different stages are executed on different CPUs Unfortunately the above method has the following disadvantages Firstly it is dif\002cult to divide the adjacent compaction steps evenly because the execution times of different steps differ signi\002cantly And it is meaningless to make nonadjacent steps into a stage Secondly this results in low dcache performance Data blocks must 003ows through multiple processors The processors may not share the dcache or the data blocks are evicted from dcache when they are waiting for processing Besides if nonadjacent stages are assigned to one processor this worsen the dcache performance further Thirdly it will result in load imbalance Lastly the above method does not scale well So we do not divide Step 2 030 Step 6 further and let them as one stage Thus the pipelined compaction procedure is divided into three stages i.e stage read  stage compute  and stage write  We schedule Step 1 and Step 7 on disk and the other 
780 


050a\051 on HDD 050b\051 on SSD Fig 5 The execution time breakdown of sequential compaction procedure into three parts 050a\051 on HDD 050b\051 on SSD Fig 6 The practical Pipelined Compaction Procedure on HDD and SSD steps on CPU However the difference with SCP is that the compaction procedures of adjacent sub-tasks are parallelized as Figure 4 For PCP we can get the compaction bandwidth as Equation 2 when one sub-task consists of only one data block B pcp  l  max f t S 1  6 P i 2 t S i  t S 7 g 0502\051 From Equation 1 and Equation 2 we get the ideal performance speedup of PCP as Equation 3 Equation 3 shows that in comparison with SCP PCP improves the compaction throughput obviously B pcp  B scp  7 P i 1 t S i  max f t S 1  6 P i 2 t S i  t S 7 g 0503\051 Suppose that Step 1 and Step 7 are scheduled on HDDs The bandwidth of HDD is two order of magnitude smaller than DRAM and becomes even worse for random I/Os Although Step 2 030 Step 6 are scheduled on CPU CPU may also wait for the HDD For HDD one data transfer may consume more than ten milliseconds due to mechanical disk head seek and rotation latency Besides Step 1 and Step 7 are scheduled on HDD and the read and write requests contend to the same disk which aggravates the randomness We pro\002le the sequential compaction procedure on HDD and break down the compaction time into three parts as Figure 5\050a\051 The Step 1 takes more than 40 of the compaction time step 7 takes less than 20 and all the computation steps take about 40 So read is the bottleneck operation The Step 1 and Step 7 take about 60 of the compaction time so HDD is the bottleneck So the pipelined compaction procedure resembles Figure 6\050a\051 Under this scenario the pipelined compaction procedure is I/O-bound Suppose that Step 1 and Step 7 are scheduled on 003ash-based solid state drives which are known for high throughput and low latency in comparison with hard disk drives Because there are no mechanical parts in 003ash-based SSDs the bandwidth of SSD may be over 002ve times larger than HDD especially for random I/Os We pro\002le the sequential compaction procedure on SSD and break down the compaction time into three parts as Figure 5\050b\051 The computation steps take more than 60 of the compaction time Both Step 1 and Step 7 take less than 40 of the compaction time totally Obviously the CPU becomes the bottleneck as Figure 6\050b\051 Under this scenario the pipelined compaction procedure is CPU-bound C Parallel Pipelined Compaction Procedure 050 PPCP 051 In this section we propose two parallel variants of PCP to remove the performance bottleneck Present data center nodes are always equipped with multi-core processors or multiple processors and multiple disks including HDDs and SSDs In the PCP  for the I/O-bound case we exploit the parallelism of multiple storage devices to improve the I/O bandwidth to alleviate the I/O performance bottleneck which is named as Storage-Parallel Pipelined Compaction Procedure 050 S-PPCP 051 For the CPU-bound case we exploit the parallelism of multiple cores or processors to improve the computation bandwidth to alleviate the computing performance bottleneck which is named as Computation-Parallel Pipelined Compaction Procedure 050 C-PPCP 051 1\051 S-PPCP In S-PPCP  we use multiple disks for Step 1 and Step 7  Step 1 and Step 7 of different sub-tasks are scheduled on different disks For example we use 2 disks to do the I/Os as Figure 7\050a\051 Step 1 of sub-task 1 is scheduled 
781 


050a\051 S-PPCP 050b\051 C-PPCP Fig 7 Parallel Pipelined Compaction Procedure on disk 1 and Step 1 of sub-task 2 is scheduled on disk 2 Thus I/Os of different sub-tasks parallelize Suppose that there are k disks for Step 1 and Step 7 and one sub-task consists of one data block we can get the compaction bandwidth of S-PPCP as Equation 4 B s 000 ppcp  l  max f t S 1  k  6 P i 2 t S i  t S 7  k g 0504\051 When k  max f t S 1 t S 7 g  6 P i 2 t S i  the pipelined compaction procedure is still I/O-bound When k  max f t S 1 t S 7 g  6 P i 2 t S i  the pipelined compaction procedure becomes CPU-bound For the latter case even if we use more storage devices the compaction bandwidth doesn't increase any more Note that when using excessive storage devices in the pipelined compaction procedure the pipelined compaction procedure may become CPUbound From Equation 2 and Equation 4 we can get the ideal performance speedup of S-PPCP as Equation 5 The ideal speedup is at most min f k max f t S 1 t S 7 g  6 P i 2 t S i g  B s 000 ppcp  B pcp  max f t S 1  6 P i 2 t S i  t S 7 g  max f t S 1  k  6 P i 2 t S i  t S 7  k g 0505\051 2\051 C-PPCP For the CPU-bound case we don't increase the pipeline depth to increase the parallelism as described in Section III-B Instead we schedule the computation steps of different sub-tasks on different cores or processors That is multiple sub-tasks are processed at the same time but the computation steps of one sub-task are done on the same processor For example there are two cores to do the computation as Figure 7\050b\051 Then S 2 030 S 6 of sub-task 1 are scheduled on core 1 and S 2 030 S 6 of sub-task 2 are scheduled on core 2 Suppose that there are k cores to do the computation and each sub-task consists of one data block We get the compaction bandwidth of C-PPCP as Equation 6 B c 000 ppcp  l  max f t S 1  6 P i 2 t S i  k  t S 7 g 0506\051 When k  6 P i 2 t S i  max f t S 1 t S 7 g  the CPU is still the performance bottleneck When k  6 P i 2 t S i  max f t S 1 t S 7 g  the storage device becomes the performance bottleneck For the latter case even if we use more processors the compaction bandwidth can't increase any more The pipelined compaction procedure becomes I/O-bound From Equation 2 and Equation 6 we can get the ideal performance speedup of C-PPCP as Equation 7 The ideal speedup can not exceed min f k 6 P i 2 t S i  max f t S 1 t S 7 g g  B c 000 ppcp  B pcp  max f t S 1  6 P i 2 t S i  t S 7 g  max f t S 1  6 P i 2 t S i  k  t S 7 g 0507\051 IV E VALUATION In this section we present the experimental methodology and workload Then we analyze the pro\002ling result for the sequential compaction procedure to claim the necessity of pipelined compaction procedure Thirdly we evaluate the performance improvement of the pipelined compaction procedure and the impacts of sub-task size and sub-task count on the compaction bandwidth Last we evaluate the performance improvement of the parallel pipelined compaction procedure and present that I/O-bound and CPU-bound cases of Pipelined Compaction Procedure may be transformed A Experimental Setup Implementation Details The pipelined compaction procedure is implemented based on LevelDB which is a representative LSM-tree implementation We leave out the details of LevelDB and instead focus on components speci\002c to our optimization We use multiple threads to execute the pipelined compaction procedure In current implementation the pipeline depth is three i.e stage read  stage compute and stage write  For PCP  we assign one thread for each stage For S-PPCP  we use multiple disks with the md driver to build RAID0 and create multiple threads for I/Os For C-PPCP  we create multiple additional threads for the computation stage and assign one thread for each sub-task Between the adjacent stages we create a queue for data communication 
782 


050a\051 Execution time breakdown on HDD 050b\051 Execution time breakdown on SSD Fig 8 The execution time breakdown of sequential compaction procedure for different key-value sizes 050a\051 Execution time breakdown on HDD 050b\051 Execution time breakdown on SSD Fig 9 The execution time breakdown of sequential compaction procedure for different sub-task sizes Measurement Methodology We conducted experiments on one machine running Linux CentOS 6.0 002nal with kernel 2.6.32  The machine includes a two-socket Intel Xeon E5645 0506 cores with hyper-threading 2.4GHz 12MB L3 cache\051 The machine has 16GB of DRAM To test out of DRAM performance we booted it with 512MB The machine has 11 1TB 7200RPM SATA III disks one as the system disk and ten as data disks Besides it also has a 240GB SATA III 2.5in Intel X25-M Solid State Drive Except that we claim explicitly we used the following parameter values for all experiments That is the memtable size is 4MB the SSTable size is 2MB the data block size is 4KB and the compression algorithm is snappy  In all experiments we use insert-only workloads to do the compaction pro\002ling and evaluate the compaction bandwidth improvements The key and value size is 16B and 100B respectively The number of key-value pairs is 002fty million B Sequential Compaction Procedure Pro\002ling In this section we pro\002le the compaction procedure to evaluate the execution time of the seven compaction steps Figure 8 depicts the breakdown of the execution time for different key-value sizes from 64B to 1024B Figure 9 depicts the execution time for different sub-task sizes from 64KB to 4MB To avoid the impact of page cache we use direct I/O for reads and writes As depicted in Figure 8\050a\051 and Figure 9\050a\051 the step read takes more than 40 of the compaction time Firstly due to the mechanical characteristics of hard disk drives one data transfer may consume more than 10ms Secondly the SSTables are dynamically allocated As a result the data can not be placed on disk sequentially Although the I/O size in the compaction is large the disk arm may suffer seeks due to that there are multiple sub-tasks in one compaction Thirdly the step write contends to use the same disk However the write bandwidth is better than step read as the write request is considered completed after the data has been written into the disk write buffer rather than the disk Besides the read requests and write requests interleave which gives the disk a chance to write back the buffer data to disk With the addition of the step write  the input and output of the compaction take more than 60 of the compaction time which illustrates that the HDD is the performance bottleneck As depicted in Figure 8\050b\051 and Figure 9\050b\051 the step write takes more time than step read  which is due to the writeafter-erase feature of 003ash-based solid stage disks All the computation steps take about 60 of the time Obviously the CPU may be the performance bottleneck in storage systems using 003ash-based SSDs As the key-value size increases step sort takes less time due to the decreasing amount of key-value entries The execution time of step write decreases as the sub-task size increases because of that larger I/O size can exploit the internal parallelism of SSD and increase the bandwidth of HDD No matter on SSD or HDD either step crc or step re-crc takes less than 
783 


050a\051 IOPS on HDD 050b\051 Compaction bandwidth on HDD 050c\051 Speedup on HDD 050d\051 IOPS on SSD 050e\051 Compaction bandwidth on SSD 050f\051 Speedup on SSD Fig 10 The performance of Sequential Compaction Procedure and Pipelined Compaction Procedure on HDD and SSD 5 of the compaction time and step decomp takes the least amount of time In all the computational steps step comp is almost the most costly C Pipelined Compaction Procedure In this section we evaluate the performance improvement of the pipelined compaction procedure We increase the size of working set from ten million to eighty million entries as Figure 10 Figure 10\050a\051 and Figure 10\050d\051 show that when the data set size increases the throughput of both Sequential Compaction Procedure and Pipelined Compaction Procedure on HDD and SSD decreases As more key-value entries are inserted into LSM-tree based storage system there are more components in the storage system Thus key-value entries 003ow downwards through more components which results in lower throughput Figure 10\050b\051 and Figure 10\050e\051 depict the compaction bandwidth Figure 10\050b\051 presents that as more entries are inserted the compaction bandwidth deceases slightly on HDD Because the data set size increases on HDD the seek cost increases However as there are no mechanical parts in solid state drives the compaction bandwidth on SSD does not decrease in Figure 10\050e\051 We normalize the IOPS and compaction bandwidth speedup of Pipelined Compaction Procedure against Sequential Compaction Procedure as Figure 10\050c\051 and 10\050f\051 PCP improves the throughput at least by 25 on HDD in Figure 10\050c\051 and 45 on SSD in Figure 10\050f\051 PCP improves the compaction bandwidth at least by 45 on HDD and 65 on SSD Compared with the ideal compaction bandwidth speedup which is calculated using the values in section IV-B the practical compaction bandwidth speedup is lower by about 10 This is due to the overhead of the pipeline compaction procedure 002lling and draining The throughput speedup is lower than the speedup of the compaction bandwidth by 20 approximatively Because in the storage system there are other operations including database consistence maintaining garbage collecting and other operations which are not pipelined by now Besides we also evaluate the performance improvement of Pipelined Compaction Procedure for different sub-task sizes and for different compaction sizes on SSD As depicted in Figure 11\050a\051 the sub-task size increases from 64KB to 4MB with 002xed compaction size in which the input size of upper component is 4MB While the sub-task size increases the compaction bandwidth of Sequential Compaction Procedure increases Due to that the I/O size is equal to the sub-task size large I/O size can exploit the internal parallelism of SSDs and improve the bandwidth of SSDs However the compaction bandwidth of Pipelined Compaction Procedure 002rst increase and then decrease Because too small I/O size can not exploit the internal parallelism of SSDs Too large I/O size decreases the sub-task count which decreases the ef\002ciency of PCP The compaction bandwidth of PCP using 512KB sub-task size is the highest In Figure 11\050b\051 we increase the input size of upper component from 1MB to 10MB while the sub-task size is set 1MB For Sequential Compaction Procedure  the compaction bandwidth does not increase as the compaction size increase The compaction bandwidth of PCP keeps on increasing until the sub-task count reach 6 Large compaction size increase the ef\002ciency of PCP Figure 11 shows that Pipelined Compaction Procedure can improve the compaction bandwidth for all sub 
784 


050a\051 050b\051 Fig 11 Compaction bandwidth with different subtask sizes and compaction sizes task sizes and compaction sizes D Parallel Pipelined Compaction Procedure In this section we evaluate the performance of Parallel Pipelined Compaction Procedure  Figure 12 presents the performance improvement on throughput compaction bandwidth and the corresponding speedup For S-PPCP  we increase the number of disks to alleviate the disk bottleneck The multiple disks are built into RAID0 As depicted in Figure 12\050a\051 the throughput increases when more disks are used for input and output The throughput does not increase any more when the disk count reaches 5 since the CPU becomes the performance bottleneck and then Pipelined Compaction Procedure becomes CPU-bound For C-PPCP  we increase the number of threads to do the computation work As depicted in Figure 12\050d\051 the throughput increases when another thread is added to do the computation work After that the Pipelined Compaction Procedure becomes I/O-bound When more threads are added to do the computation work the throughput and the compaction bandwidth decrease This is due to the overhead of creation and synchronization of multiple threads V R ELATED W ORK Improve the lookup performance  COLA and FD-tree  e xploit fractional cascading technique 20 to con\002ne the search data extent COLA puts the every eighth element of the 050 k  1\051 st array in the k th array as the forward pointer FD-tree puts the 002rst element of every page of the 050 k  1\051 st array in the k th array as the search fence Thus in each level point queries just do one I/O bLSM uses bloom 002lter to a v oid disk I/Os for the level which does not contain the sought-after key GTSSL uses the reclamation and re-insert techniques to put key-value pairs in upper levels to decrease the count of levels that point queries go through bLSM and the partitioned exponential 002le partition the k e y range into multiple sub-key ranges and then point queries just search one smaller sub-key range Improve the compaction performance  bLSM uses the replacement-selection sort algorithm in the compaction procedure of component C 0 to increase the length of sorted key-value pairs which decreases the frequency of compactions for all the components VT-tree uses the stitching technique to avoid unnecessary disk I/Os for sorted and non-overlap key range But the stitching technique may incur fragmentation which degrades the performance of scans and compactions bLSM and PE 19 partition the k e y range into multiple sub-key range and con\002ne compactions in hot data key ranges which accelerate the data 003ow Exploit 003ash-based SSDs  GTSSL tar gets h ybrid HDDSSD systems and uses SSDs as the storage media of lower components and the cache of hot data Based on the observation that random writes with good locality have performance similar to sequential writes on 003ash-based SSDs FD-tree uses SSDs as the storage media In theory the pipelined compaction procedure is orthogonal to existing studies VI C ONCLUSION Background compactions play an important role on the performance of LSM-tree based storage systems The existing studies on background compactions focus on decreasing compaction frequency reducing I/Os or con\002ning compactions on hot key-ranges to improve compaction performance However they ignore computation time and don't exploit the parallelism of I/O resource and computation resource In practice the compaction may be CPU-bound and the computation takes more than 60 of the total compaction time in storage systems using faster storage devices like 003ash-based SSDs In this paper we analyze the compaction procedure and recognize the performance bottleneck Then we propose the pipelined compaction procedure For one compaction we divide it into multiple sub-tasks and exploit the parallelism of I/O resource and CPU resource to parallelize multiple sub-tasks Furthermore we improve the pipelined compaction procedure with multiple storage devices or CPUs to alleviate the performance bottleneck Theoretical analysis proves that the pipelined compaction procedure can improve the compaction bandwidth Extensive experimental results show that the 050parallel\051 pipelined compaction procedure improves the compaction performance signi\002cantly VII A CKNOWLEDGEMENTS We thank the anonymous reviewers for their helpful feedback This work is supported in part by National Basic Research Program of China under grant No.2011CB302502 National Science Foundation of China under grants No.61379042 No 61303056 No 60925009 
785 


050a\051 IOPS on HDD 050b\051 Compaction bandwidth on HDD 050c\051 Speedup on HDD 050d\051 IOPS on SSD 050e\051 Compaction bandwidth on SSD 050f\051 Speedup on SSD Fig 12 The performance of Sequential Compaction Procedure and Parallel Pipelined Compaction Procedure on HDD and SSD and No 61221062 and Huawei Research Program YB2013090048 R EFERENCES   Russell Sears and Raghu Ramakrishnan bLSM A General Purpose Log Sturctured Merge Tree  In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data  2012   Fay Chang Jeffrey Dean Sanjay Ghemawat Wilson C Hsieh Deborah A Wallach Mike Burrows Tushar Chandra Andrew Fikes and Robert E Gruber Bigtable A Distributed Storage System for Structured Data  In ACM Trans Comput Syst 26\0502\051 1-26   Shimin Chen Anastassia Ailamaki Phillip B Gibbons and Todd C Mowry Improving Hash Join Performance through Prefetching  In ACM Trans Database Syst  2007   Parrick O'Neil Edward Cheng Dieter Gawlick Elizabeth O'Neil The Log-Structured Merge-Tree 050LSM-Tree\051  In Acta Informatica 1996   Pradeep Shetty Richard Spillane and el at Building WorkloadIndependent Storage with VT-Trees  In 11th USENIX Conference on File and Storage Technologies 2013   M A Bender M Farach-Colton J T Fineman Y R Fogel B C Kuszmaul and J.Nelson Cache-oblivious streaming b-trees  In Proceedings of the nineteenth annual ACM symposium on Parallel algorithms and architectures 2007   Richard P Spillane Pradeep J Shetty and etc An Ef\002cient Multi-Tier Tablet Server Storage Architecture  In Proceedings of the 2nd ACM Symposium on Cloud Computing 2011   Yinan Li Bingsheng He and etc Tree indexing on Solid State Drives  In Proc VLDB Endow 2010   HBase main page http://hbase.apache.org   LevelDB main page https://code.google.com/p/leveldb   Giuseppe DeCandia Deniz Hastorun Madan Jampani Gunavardhan Kakulapati Avinash Lakshman Alex Pilchin Swaminathan Sivasubramanian Peter Vosshall Werner Vogels Dynamo amazon's highly available key-value store  In Proceedings of twenty-\002rst ACM SIGOPS symposium on Operating systems principles 2007   Brian F Cooper Raghu Ramakrishnan Utkarsh Srivastava Adam Silberstein Philip Bohannon Hans-Arno Jacobsen Nick Puz Daniel Weaver Ramana Yerneni PNUTS Yahoo!'s hosted data serving platform  In Proc VLDB Endow 2008   Michael A Olson Keith Bostic and Margo Seltzer Berkeley DB  In USENIX ATC 1999   Redis http://redis.io    Memcached http://memcached.org    Berk Atikoglu Yuehai Xu Eitan Frachtenberg Song Jiang Mike Paleczny Workload Analysis of a Large-Scale Key-Value Store  In SIGMETRICS 2012   Rajesh Nishtala Hans Fugal Steven Grimm Marc Kwiatkowski Herman Lee Harry C Li Ryan McElroy Mike Paleczny Daniel Peek Paul Saab David Stafford Tony Tung Venkateshwaran Venkataramani Scaling Memcache at Facebook  In 10th USENIX Symposium on Networked Systems Design and Implementation 2013   Avinash Lakshman Prashant Malik Cassandra a decentralized structured storage system  In SIGOPS Oper Syst Rev 2010   Christopher Jermaine Edward Omiecinski Wai Gen Yee The partitioned exponential 002le for database storage management  In The VLDB Journal Volume 16 Issue 4 Pages 417-437  2007   B Chazellel and L J Guibas Fractional cascding I a data structuring technique  Algorithmica 1\0502\051 1986   Qi Huang Ken Birman Robbert van Renesse Wyatt Lloyd Sanjeev Kumar and Harry C Li An Analysis of Facebook Photo Caching  In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles 050SOSP 13\051 
786 


 11  general circulation models exhibit difficulty in producing the same magnitude, spatial, and te mporal variat ions in SSS during MJO propaga tions when compared with Aquarius data This affirms that Aquarius derived SSS will be a useful tool for data assi milation model advancement improvement of initialization and better representation of both ocean ic and atmospheric processes during intraseasonal variations Accurate SSS observations collected by the Aquarius salinity mission at the current spatial coverage and duration are unprecedented. It has now been shown that the measurements gathered from thi s mission will broaden the  understanding in various modes of climate variability even on intraseasonal scales  VI  FUTURE  WORK  This year is expected to yield significant scientific discoveries for satellite salinity measurements A landmark collection of research papers will address early scientific results of the mission and include many aspects of how rainfall, evaporation river outflows and melting ice are linked to salinity ocean current and climate variations  The satellite salinity data are now bei ng tested in numerical ocean mod els that are used for ocean forecasting will soon be used to enhance the skill of long term climate prediction models  The SPURS measurements will conclude in September, and the science teams will focus on data analysis stu dying the relationships between the in situ observations and the satellite salinity data  In the future a second SPURS may be carried out in an area that is dominated by intense annual rainfall in direct contrast to the arid region of the first survey i n order to fully understand ocean processes that regulate the reduced surface salinities and climatic changes in those regions as well  At the end of 2014 Aquarius will complete a baseline three year mission designed to measure the seasonal cycle and inte r annual climate variability Provided that the onboard systems continue to work as needed NASA and CONAE  plan to maintain the mission operations into the future and continue to gather this important new data set for studying ocean dynamics, climate chang e and the global water cycle  VII  SUMMARY  The global map generated from Aquarius data clearly shows the predomin ant and well known climatologi cal ocean salinity features such as higher salinity in the subtropics higher average salinity in the Atlantic Ocean compared to  the Pacific and Indian oceans and lower salinity in rainy belts near the equator in the northe rnmost Pacific Ocean and else where These features are related to large  scale patterns of rainfall and evaporation over the ocean  river outflow and ocean cir culation Th e values depicted in the South ern Ocean retain significant uncertainties associated with high winds and low surface temperatures that complicate the retrieval processing  Other important regional features are clearly evident, inclu ding a contrast between the arid, high salinity Arabian Sea west of the Indian subcontinent and the low salinity Bay of Bengal to the east, which is dominated by the Ganges River and southern Asia monsoon rains The data also reveal important smaller detai ls such as pro minent low salinity water asso ciated with outflow from the Amazon and Orinoco rivers Aquarius will monitor how these various larger  and smaller sc ale fea tures change over time and provide data to study their link to climate and weather var iations  Aquarius  also reveals considerable sur face salinity variation at much smaller scales than expected  In the global maps produced to date the  lower salinity region coincides with the zonal heavy precip itation band in the Intertropi cal Convergence Zone. The current vectors reveal meandering and eddy features associ ated with the salinity patterns. Such detail of surface dynamical structure is not resolved with the conventional salinity observations emphasizing the new capabilities now avail able for studying the ocean and new disco v eries that are in store  Algori thm updates and data reprocess ing will b e frequent and unscheduled dur ing the firs t year. Evaluation data are pub  licly available at http://podaac.jpl.nasa.gov SeaSurfaceSalinity/Aquarius Re gistration is required and users must recognize that the data are preliminary and are neither fully validated nor  officially released for scient ific purposes. Comments on problems and issues with the data are welcome Validated data are to be released in late 2012 and new findings are expected in the months and years to come  VIII  ACKNOWLEDGMENT  The Aquarius/SAC D success is a tribute to the extraordinary talent dedication and hard work  of the many scientists manag ers engineers and technicians in the Unit ed States Argentina, Brazil, Italy, France, and Canada whose individual contributions made this mission possible  The research described in this paper carried out at the Jet Propulsion Laboratory, California Institute of Technology, was done so under a co ntract with the National Aeronautics and Space Administration  IX  REFERENCES  Dohan K and N Maximenko 2010 Monitoring ocean currents with satellite sensors Oceanogra phy  23 4 94 320 103 doi:10.5670/oceanog.2010.08  Jeffrey R. Piepmeier David M. Levine, Simon H. Yueh, Frank Wentz Christopher Ruf  322aquarius radiometer performance early on orbit calibration and results\323  Lagerloef G et al 2008 The Aquarius/SAC D mission Designed to meet the salinity remote sensing challenge Oce anography  21 1\, 68 320 81, doi:10.5670/oceanog.2008.68  


 12  Lagerloef, G., F. Wentz, S. Yueh, H. Kao, G. John  son, and J Lyman 2012 Aquarius satellite mis  sion provides new detailed view of sea surface salinity, in State of the Climate in 2011 Bull. Amer Meteor. Soc in press  1  L e  V i n e   D  M    L a g e r l o e f  G  S  E    C o l o m b   F  R    Y u e h   S.H Pellerano F.A Aquarius An Instrument to Monitor Sea Surface Salinity From Space IEEE Trans Geoscience and Remote Sensing, 45\(7\, 2040 2050, July 2007  2  P e l lerano, F.A., J.R. Piepmeier, M. Triesky, K. Horgan, J Forgione J Caldwell W.J Wilson S Yueh M Spencer D McWatters A Freedman 322The Aquarius ocean salinity mission high stability L band radiometer,\323 Int. Geosci. Remote Sensing Symposium \(IGARS S\, Denver, CO, July 2006  3  R u f  C  S    322 V i c a r i o u s  C a l i b r a t i o n  o f a n  O c e a n  S a l i n i t y  Radiometer from Low Earth Orbit,\323 AMS J Atmos Oceanic Tech., 20\(11\, 1656 1670, 2003   4  R u f  C  S    S  J   K e i h m   M  A   J a n s s e n   322 T O P E X P o s e i d o n  Microwave Radiometer T MR I Instrument description and antenna temperature calibration,\323 IEEE Trans. Geoscience and Remote Sensing, 33\(1\, 125 137, 1995  5  P i e p m e i e r   J   a n d  F   P e l l e r a n o   322 M i t i g a t i o n  o f T e r r e s t r i a l  Radar Interference in L Band Spaceborne Microwave Radiomet ers,\323 Proc. IEEE Int. Geoscience and Remote Sensing Symposium IGARSS Denver CO pp 2292 2296 July 30 Aug 4, 2006  6  M i s r a   S    a n d  C   R u f  322 D e t e c t i o n  o f R a d i o Frequency Interference for the Aquarius Radiometer,\323 IEEE Transactions Geoscience Remote Sensing, 46\(10\, 2008  7  P i e p m e i e r   J  R    F  A   P e l l e r a n o   M   T r i e s k y   K   H o r g a n   J   Forgione and J Caldwell Lessons Learned from the Aquarius Radiometer Engineering Model Int Geosci Remote Sensing Symposium \(IGARSS\, Barcelona, July 2007  Gary Lagerloef 322Satellite Mission Monitors Ocean Surface Salinity\323 EOS Transactions American Geophysical Union VOLUME 93 NUMBER 25 19 JUNE 2012  Gary Lagerloef Simon Yueh and Jeffrey Piepmeiner 322NASA\325s Aquarius Mission Provides New Ocean View\323, Sea Technol ogy Magazine, Jan 2013 issue  Tong Lee Gary Lagerloef Michelle M Gierach Hsun Ying Kao Simon Yueh  and Kathleen Dohan 322Aquarius reveals salinity structure of tropical instability waves\323 GEOPHYSICAL RESEARCH LETTERS VOL 39 L12610 doi:10.1029/2012G L052232, 2012  Michelle M Gierach,1 Jorge Vazquez Cuervo,1 Tong Lee,1 and Vardis M T 322Aquarius and SMOS detect effects of an extreme Mississippi River flooding event in the Gulf of Mexico\323, GEOPHYSICAL RESEARCH LETTERS, VOL. 40 1 320 6, doi:10.1002/grl.509 95, 2013  Semyon A. Grodsky,1 Nicolas Reul,2 Gary Lagerloef,3 Gilles Reverdin,4 James A Carton,1 Bertrand Chapron,2 Yves Quilfen,2 Vladimir N Kudryavtsev,5,6 and Hsun Ying Kao 322Haline hurricane wake in the Amazon/Orinoco plume AQUARIUS/SACD and SMOS obs ervations\323 GEOPHYSICAL RESEARCH LETTERS VOL 39 L20603 doi:10.1029/2012GL053335, 2012  Gary Grunseich,1 Bulusu  Subrahmanyam,2 and Bin Wang 322The Madden Julian oscillation detected in Aquarius salinity observations\323 GEOPHYSICAL RESEARCH LETTERS VOL. 40, 1 320 6, doi:10.1002/2013GL058173, 2013  X  BIBLIOGRAPHY  Amit Sen is a Space Flight Missions Program  Manager at NASA\325 s Jet Propulsion Laboratory JPL  Mr Sen was t he Aquarius Project Manager since the inception of the project through its commissioning in space  He has extensive spacecraft launch vehicle and mission implementation experience working  with agencies such  as ESA Europe  JAXA Japan CNES France\ ASI \(Italy AEB  Brazil\ CONAE Argentina\ CS A  Canada  DLR Germany ISRO India  and their associate contractors   Gary  Lagerloef, Space Research institute in Seattle Washington is the principal investigator of the NASA Aquarius satellite mission He earned his Ph.D in oce anography at the University of W ashington His research focuses on ocean physics and cli matology, emphasizing new ap plications of remote sensing with more than four dozen scien tific publications   Tong  Lee is the Aquarius Project Scientist and a Supervisor of the Oceans and Ice Group at the Jet Propulsion Laboratory in Pasadena California His research includes Ocean circulation and its relation to climate variability on seasonal to decadal time scal es in particular upper ocean heat  salt balance meridional transports inter basin linkages tropical extratropical exchanges data assimilation adjoint sensitivity analysis  He has over 50 publications to date in his areas of research      


                  


             


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


