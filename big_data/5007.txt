Support for data-intensive computing with CloudMan  Y. Kowsar 1 andE. Afgan 1,2  1 Victorian Life Sciences Computation Initiative \(VLSCI\University of Melbourne, Melbourne, Australia 2 Centre for Informatics and Computing \(CIR\ Ruer Boùkovi Institute \(RBI\Zagreb, Croatia ykowsar@student.unimelb.edu.au, enis.afgan@[unimelb.edu.au    Abstract - Infrastructure-as-a-Service \(IaaS\ compute infrastructure model has showcased its ability to transform how access to compute resources is realized; it delivered on 
the notion of Infrastructu re-as-Code and enabled a new wave of compute adaptability. In previous work we have been developing CloudMan \(usecloudman.org\atile solution for enabling and managing compute clusters in cloud environments via a simple web interface or an API However, CloudMan only supported batch processing workloads. As the magnitude of the data produced and processed in digital form grows, the need to support big data applications in clusters in the cloud becomes more evident. In this paper, we have extended the batch processing capability of CloudMan and presenteda novel architecture for supporting big data analysis workloads in 
cluster-in-the-cloud environment.We also implemented the details through CloudMan using established big data platforms I I NTRODUCTION  As the information age continues, the rate at which data is produced is continuing its exponential growth; it is fueled by everything from news services and social media activities to sensor networks and research-driven devices Recent advances in biology and the advent of Next Generation Sequencing \(NGS\stems is a prime example of such developments. This data growth has led to what is often referred to as the data deluge and has posed a shift in the research problems at ha Se veral  
years ago, the rate at which, for examplebiological, data was produced was a limiting research factor. Namely biologists would generate genomic sequences one nucleotide at a time and this allowed the data to be easily stored and analyzed even on a researcherês personal computer With the availability of NGS systems those days have been surpassed. Today, it is possible to sequence an entire human genome, consisting of ~3 billion nucleotides, in just a few days, at better quality, and at a fraction of cost compared to just a few years ago. This ability to generate so much data has, however, led to major challenges when dealing with storage and analytics from the data Although often primarily described as a challenge and 
an obstacle, the reality is that the availability of the increasing data volume presents enormous opportunities The real power of the data will not come just from the sheer volume, but from the ability to analyze it. It is thus vital to provide flexible yet accessible solutions that enable researchers and companies alike to move beyond the data collection and step into the world of data analytics Although IaaS is an established approach for providing computing facilities on the cloud, many workloads still execute only in a more structured and traditional cluster computing environment where jobs are handed off to a job manager and possibly executed in parallel.As a step in this direction, we have been developing a cloud resource 
ma a t fac ili t a t e s cre ati on of a  compute platform o v i d i n g c lusters in the cloud  environment. CloudMan iscapable of handling a range of workloads, including biological analyses \(se Cl oud computing in general allows compute and storage resources to be requested, provisioned, and utilized to handle the necessary scaling of a computational problem at hand. However, those resources are often provisioned as bare virtual machines and disks without application context or coordination. CloudMan helps in this regard by orchestrating all the steps required to provision a functional compute and application environment by establishing a traditional batch queue job manager atop flexible cloud resources.This setup provides a user with an 
accessible and functional data analysis platform that is rooted in familiar cluster-based concepts but leverages cloud resources instead  In this paper, we describe recent advances in CloudMan where support for data intensive workloads has been added. Until now, CloudMan provisioned a functional and scalable batch queue processing system through Sun Grid Engine \(SGE\compute cluster in the cloud as well as a range of bioinformatics tools as part of CloudBioLinux [5 a nd Galaxy[6 7], [8].Exten ding the  CloudMan capabilities,we have now  introduced the bigdata-platform-on-demand concept by presenting a new architecture through adopting and utilizinga well established big data platform component;H In 
addition we have addedsupport for federated computing concept into CloudMa advances empower the next wave of data analysis workloads to be seamlessly executed in the cloud environment and easily integrated with the already existing and more traditional pipelines. Support for these types of workloads also facilitates easier development of Hadoop-based tools \(due to the accessibility of the cloud and the functional execution/development environment and provision groundwork for federation of clusters across multiple clouds and/or data centers. It is worth explicitly stating that CloudMan is not limited to biological 012\015		\012\015\015\015\015 243 


workloads and can just as readily be utilized for any other workload where a compute cluster is necessary This paper is organized as follows. In chapter II we provide an overview on the two main components namely Hadoop and HTCodor, used in the presented architecture and provide details on how CloudMan benefits from this architecture with a brief overview of other existing solutions.  In chapter III we present our architecture and its implementation details. In chapter IV test case studies and usage have been provided. Finally, in chapter V the conclusion and future direction have been stated II B ACKGROUND  A Big Data: Hadoop Big data is a concept introduced as a result of recent explosion of available data in various fields. This also applies to sciences that were traditionally not considered data-intensive and is having a ripple effect on how research is being transformed. One possible approach for tackling big data problems comes in the form of distributed and cloud computing where use of commodity computers enables feasible and accessible solution for a variety of problem domains. These approaches are providing opportunities for researchers and companies alike to store, retrieve and interpret their data withina reasonable amount of time and budget. Hadoop is an example of such a solution; it is a framework for analyzing big data across clusters of computers using theMapReduce programming model. In this model a problem is divided into several independent smaller problemswhere each sub-problem can be solved over a single node of a cluster the map phase The process of solving the problem continues by collecting and merging the results from each sub-problem thereduce phase o shape a single output. Programs written in this style can be automatically parallelized and executed on a large cluster of commodity machines[9  Combining this c o mputat ional  model with the readily available computing infrastructure and making it all accessible will go a long way toward tackling the big data problem B Federated Computing: HTCondor Complementing the notion of distributed computing is the notion of federated computing. In this model, the aim is to assemble distributed computing resources that are possibly geographically and administratively disparate into a cooperating unit. Joining of such resources makes it possible to achieve higher workload throughput and resource utilization; the aim is to utilize all the known and available resources available over a period of time without assuming high availability of those resources  Joining a shared pool of resources brings other complexities such as enforcing each organizationês policy and maintaining service layer agreements. In these scenarios, flexibility is a key for managing complex system.To achieve this goal HTCondor has introduced twoconcepts: flocking and gliding[1   I n flocki ng  scenar i o s t h e overflow workload will be sent to other, off-site HTCondor resources to keep the runtime promises. Flocking was first introduced by the HTCondor team as a solution to submit the work overflow to other HTCondor remote resources In this scenario, remote resources managed by a HTCondor component can be acquired when necessary given they are not currently being utilized. Although flocking can fulfill off-site resource rental, it will not apply when remote resources are not managed by a HTCondor component. To solve this problem, gliding was introduced. The gliding scenario is a dynamic, on-demand deployment of HTCondorês necessary daemons over remote resources that are otherwise not controlled by a HTCondor component. In this solution, use of resources managed by other job manager \(such as SGE or PBS be achieved. This is accomplished through a Globus Toolk ion st r i ng which is shared between remote resources. Although traditionally utilized across a dedicated set of resources, it is possible to envision use of federated computing across multiple clouds and/or clouds and dedicated resources. Enabling application execution environments and consequently applications to run in such environments would help minimize vendor lock-in increase code portability, and create an equilibrium market C Beyond the state of the art Several projects exist in the field of data-intensive and/or federated computing atop cloud resources; the ones most relevant to the work being presented in this paper are presented in Table 1  This work was, in part, supported by the Genomics Virtual Laboratory \(GVL\ant from the National eResearch Collaboration Tools and Resources \(NeCTAR TABLE I L IST AND OVERVIEW OF PROJECTS RELATED TO THE WORK PRESENTED HERE  StarCluster   StarCluster is an open source project created by STAR group at MIT that allows anyone to easily create and manage their own cluster computing environments hosted on Amazon's Elastic Compute Cloud \(EC2\ without needing to be a cloud expert The project also provides support for Hadoop based workloads Amazon Elastic MapReduce   Amazonês Elastic MapReduce \(EMR\ is a generalpurpose web service that utilizes a hosted Hadoop framework running on Amazon EC2 and Amazon Simple Storage Service \(S3\llowing anyone to process data-intensive workloads on Amazon Web Services \(AWS\cloud ConPaaS   This projects provides an open source system in which resources that belong to different operators are integrated into a single homogeneous federated cloud that users can access seamlessly. ConPaaS is a PaaS layer providing a set of elastic high-level services and runtime environments, including support for MapReduce jobs, Java and PHP applications, as well as as TaskFarming service. ConPasS is compatible with AWS and OpenNebula clouds HTCondor   HTCondor is a workload management system for compute-intensive jobs. It provides a job management mechanism that automatically chooses when and where to run the jobs. In addition to being a traditional job manager, HTCondor scavenges CPU resources from potentially federated computers and makes those available via a single job queue Standalone, HTCondor can also submit jobs to Amazonês EC2 resources Crossbow   Crossbow is a domain-specific tool that focuses on whole genome resequencing analysis. It combines Bowtie and SoapSNP with Hadoop in a fixed data analysis software pipeline to enable fast data analysis on AWS or a local workstation 244 


CloudMan brings forward parts of each of the projects listed in Table 1 and yet delivers a solution that caters to users not supported by any of the existing projects. At its core, CloudMan is an accessible cloud manager that establishes functional application execution environments It operates on a range of cloud providers \(namely, AWS OpenStack, OpenNebula, and Eucalyptus\aking it the most versatile cloud manager available. Built with CloudBioLinux and Galaxy, it provides ready access to over 100 bioinformatics tools and hundreds of gigabytes of biological reference datasets that can be utilized via Galaxyês web interface, Linux remote desktop, or the command line interface. Nonetheless, CloudMan is not limited to biological workloads and can also be utilized for any workload where a compute cluster is necessary CloudMan can be installed \(via automation; see CloudBioLinux\ntu machine image, thus instantly turning that machine image into a functional cloud cluster. Alternatively, prebuilt CloudMan images can be started without any installation via a single webform \(http://biocloudcentral.org\Once launched CloudMan provides a web interface for managing cloud and cluster resources. Our new architecture extends the already extensive set of provided features and provides support for the new wave of applications and use cases of big data problems III I MPLEMENTATION  The CloudMan platform is currently available for immediate instantiation on the AWS public cloud and is also compatible with private or academic clouds based on OpenStack, OpenNebula, or Eucalyptus middleware \(visit biocloudcentral.org\Deploying CloudMan on a private cloud requires creation of a custom machine image configuration of which is automated via CloudBioLinux configuration scripts \(see http://github.com/chapmanb cloudbiolinux\ An example of such a deployment is available on Australiaês national cloud, NeCTAR, which is based on OpenStack middleware CloudMan itself is written in Python as a standalone web application and is open source licensed under the MIT license. Internally, CloudMan is based on serviceoriented architecture; each of the components within CloudMan \(e.g., file systems, job managers, applications are defined as self-contained services. Services implement a standard interface and control all actions related to the given service. The master control thread manages and coordinates all of the running services. Instructions on how to use all of the features of CloudMan are available at http://usecloudman.org\le the source code is available at \(http://bitbucket.org/galaxy/cloudman A Service Implementation: Hadoop Along with the cloud resource management, resource allocation and management in a distributed system is a task, which requires a lot of effort. Various systems have been implemented to fulfill this need. Among these systems, Sun Grid Engine \(SGE\has gained reputation for being a stable, efficient and reliable system. On the other hand, having the necessary infrastructure to ease the development of solutions for big data problems by using the computing power from distributed systems is quite demanding and Hadoop has been introduced as a solution for this task. Integrating Hadoop with SGE \(i.e., providing an available and ready to use distributed architecture to the end user\gives us the capability of having the grid engine manage distributed resources while having Hadoop as a platform service available on demand. This approach was pioneere Hadoop sy st em s was p r einstalled over clusterês nodes while the Sun Grid Engine allocated the necessary resources to its users on the fly Although this solution is satisfactory for systems where clusterês nodes are pre-configured and Hadoop is the most demanding sub-system used, running Hadoop daemons over a cluster can be seen as waste of resource as it might not be in use over the entire lifespan of given cluster node Furthermore, the available solution has separated Hadoop jobs from other jobs in the system, which would decrease job throughput by reducing the systemês coherency We reengineered the available solution to increase system throughput by having Hadoop sub-system be provided by the grid engine on-demand \(vs. static this purpose, a grid engine integration script will inspect jobs submitted to SGE and whenever a Hadoop job is found, requested computing resources will be acquired from SGE and a Hadoop environment setup dynamically In other words, before assigning a job to the resources Hadoop will be installed on the acquired subset of the SGE cluster. The process of inspecting the job is realized in a constant amount of time  and is handled by a job submission template script, which is made available to the end users. Each time a Hadoop job is detected, the script will read the allocated resources from SGE and dispatch the required binaries to each resource. This process happens in parallel across all the nodes so the time complexity of deploying Hadoop over the allocated nodes is minimum and reduced to the amount of time required to dispatch Hadoop onto a single system. After Hadoop installation is completed, the job is sent to the Hadoop master node to run. Once the job completes, grid engine integration script cleans all the resources by removing any unnecessary files dispatched to each node and killing all the daemons. This process is necessary as CloudMan is providing a hybrid platform to be used by different cluster tools and techniques. Therefore it is desirable to remove the Hadoopês files and daemons from every single node and get the system ready for any other nonHadoop/Hadoopuses to keep the system layer agreement On the other hand, as mentioned the process of deploying Hadoop gets only a constant amount of time, around 4 minutes \(please see figure 2 for details\which in comparison with the actual jobês running time can be ignored; as big data analysis are often computing intensive tasks which take a while to run.  For a visual representation of this process please see figure 1 B Service implementation: HTCondor Using private or community resources while having ondemand access to additional processing units from the cloud, to spread the workload when necessary or desired is the promise that a hybrid IaaS solution offers. In the presented architect we have adopted flocking and gliding concepts from HTCondor \(Please refer to section IIB for their definition\ solution for providing a hybrid IaaS solution through CloudMan. In this approach we have targeted scenarios where multiple clusters with in a cloud 245 


a o a m t w j r r r b a r r p c i p H c u C r p r c a re managed b o ther organiz a a mong users m inimizing th e t he scenarios w hilstother cl u j obs will be r esource is m r emote resour c r esources. Th e b eneficial is w a limited co m r esource can b s ending the r esource.Usin g p romises wit h c osts.For this t i s necessary p rovide the c H TCondor. A l c onnection fil u ntil the reso u C loudMan u s r emote resou r p rivileges to r s end the job r emote resour c c onnection str i Figure 1:  Cloud M b y an organiz a a tions and the to maxim i e cost. The th r where one o u sters are hea v flocked \(es p m anaged by C c e to have loa d e other scenari o w hen a huge p o m puting reso u e acquired or r job poolês g this feature C h the minimu m t o be happeni n for the remo c onnectionstri n l though acces s e, gliding sce u rce provider g s er to run n e r ce. After ha v r un the daem o to the remot e c e IP or DNS a i ng M an architecture a p federated com p a tion or in col l ses resources i ze the thr o r oughput can b o r multiple cl u v ily loaded. In p ecially if th e C loudMan\r d balancing sy s o where our h y o ol of jobs has u rce. In this c r ented b y the o overflow t o C loudMan wil l m amount o f n the context o te organizati o n g to the Clo u s will be gran t nario will no t g rants enough p e cessary dae m v ing been gra n o ns, the local H e resource b y a nd the path to p proach for supp o p uting fashion l aboration wit h can be share d o ughput whil e b e optimized i n u sters are fre e these cases th e e remote f r e e glidedinto th e s tem across th e y brid solution i s to be taken b y c ase a remot e o rganization fo o the remot e l keep the SL A f overflow jo b f CloudMan i o n resource t o u dManês loc a t ed through th e t be function a p rivilege to th e m ons over th e n ted sufficie n H TCondor ca n y receiving th e the Globus fil e o rting big data in a h  d  e  n  e  e  e  e  e  s  y  e  r e  A  b  i t o  a l e  a l e  e  n t n  e  e   Integ r our solut i as a solu t HTCond o settings i n sets up t h to the sy s node to t h shared p o flocking resource they sho u when a n CloudM a resource CloudM a DNS an d provider for whic h web int e configur a clusterês section f o Sub m paramete r b y Clou d job requi r submitti n SGE job  The c h b oth SG E each oth e b alanced  Hadoop w for Hado o this poin t system u n completi o Figur e environ m the given Figure 2 r ating these t w i on represents t ion for hybri d o r is achieved n CloudMan a h e HTCondor m s tem, CloudM a h e established o ol. To shar e scenario, a n e provider and r u ld first come n d what reso u a n. After r ea c provider sho u a nês HTCondo r d HTCondor s DNS or IP h a form has e rface. Cloud M a tion, letting H resources as o r an example  IV U m itting a Ha d r s in a templat d Man and spe c r es and a pat h n g this job is e  Please see fig u h allenge in thi s E and Hadoo p e r while keep  This goal is a w here SGE p r o p and then h a t on Hadoop h n til the job has o n and cleans t h e 2 captures t h m ent within SG E process 2  The Hadoop j o s w o concepts f r a first step in d IaaS. The ac t by pre config u a nd, whenever m aster node A a n automatical condor pool e the created e gotiation bet w r esource cons u into a level o f u rce can be c hing a level u ld grant suf f r This is achie v s configurati o should be pas been integrat e M an will the n TCondor neg o required \(see   U SAGE AND RE S d oop job re q e script file. T h c ifies the num b h to the job e x e quivalent to s u re 3 for a sam p s approach wa s p to work wit h ing the resou r a chieved by d e r epares the ne c a nds over the j o h andles the jo b done. SGE is t h e environme n h e process of c E and also sh o o b preparation pro c s ample timeline r om HTCond o enabling Clo u t ual integratio n u ring the HT C CloudMan st a A s workers are ly adds each w resulting in a resource poo l w een the two p u mer\ is requir e f agreement o n used throug h of agreeme n f icient privile g v ed via Cloud M o n file. Lastl y sed into Clou d e d into Cloud M n set the nec e o tiate and use r Usage and R S ULTS  q uires settin g h e script is pr o b er of CPU sl o x ecutable. Fro m s u b mitting any p le job details s to find a way h out interferin g r ce utilization e coupling SG E c essary enviro n o b to Hadoop b as a self-con t hen informed n t c reating the H o ws sample ti m  c ess through SGE o r into u dMan n with C ondor a rts, it added w orker single l in a p arties e d i.e n how h the n t, the g es to M anês y the d Man M anês e ssary emote R esults g two o vided o ts the m here other  to get g with wellE from n ment From tained of job adoop m es for and a 246 


Submitting HTCondor jobs in a CloudMan cluster requires a HTCondor job submission script to be created Once composed, the script is submitted to HTCondor as shown in figure 4 Although employing HTCondor as an engine for the federated cloud resource management gives us the capability of utilizing the disparate resources, the big data migration challenge remains an issue to be taken care of Job files can be broadly grouped into two categories: the systematic ones, in which the system should take care of and the user input data. In the context of CloudMan, a simple but effective solution for handling systematic data has been adopted; namely, any cluster managed by CloudMan will have the same copies of files in their local directories. This includes installed applications application configuration files, as well as reference data which is often required by a variety of bioinformatics tools and reaches multiple terabytes. Therefore, jobs requiring access to these data can be used without any data copying from one CloudMan cluster to another Whilst this is an efficient solution for systematic files, the user input data is a bottleneck in job dispatching. This is because flocking a job into a remote cloud or cluster requires transferring the input data next to the job, which means carrying the required amount of data over the network. It is worth mentioning that although tackling this bottleneck is an interesting question going under research in this paper, our focus is on introducing the necessary architecture and its implementation details to provide an accessible platform for enabling further research in this field. Currently, CloudMan is relying on HTCondor job schedulerês decision for submitting jobs into the remote resources. We are exploring alternative scheduling approaches to utilize data locality between clusters with the aim of submitting data intensive tasks local to the data while sending compute intensive tasks to the available remote resources V C ONCLUSION  Infrastructure-as-a-service \(IaaS\s the delivery of hardware and its associated software as a CloudMan was first introduced to facilitate management of such a service with a focus on easing the process of preparing a basis for providing bioinformatics tools on the cloud. Successful projects, such as Galaxy, the Genomics Virtual i oLi n ux and SARA Clou ng or provi ding clou d se rvi c es t o the  end users based on CloudMan In this paper we extended the capabilities of CloudMan to accommodate managing cloud resource in a more versatile and federated environment. This was achieved by integrating Hadoop platform into CloudMan and thus providing Hadoop-as-a-Service. With Hadoopês ability to process big data, availability of such a platform enables easier access to a new wave of \(bioinformatics tools for both, development and usage. In addition, we have integrated HTCondor into CloudMan. With HTCondor integrated, it is possible to easily span boundaries of a single compute cluster and extend the capacity of any administratively single resource regardless of whether it is in the cloud or not. This enables overflowing excess workloads to a different set of computational resources \(e.g., local cluster to a cloud or from one cloud provider to a different cloud provider This also opens a new door towards federated job execution and data-locality based job execution. The architecture and the developed platform discussed here is a step towards further research in the space of big data movement and job scheduling to exploit data locality. This Figure 3 HTCondor flocking example. A is the script to be run by HTCondor in this exmaple it is named python_random_lines.sh. B is the HTCondor script which should be submitted to HTCondor named remote_job.submit. Here we submit these scripts multiple times into AWS cluster \(From Figure 3\ using multiple different input files. It is worth noting that the Exit 42 at the end of python_random_lines.sh will let HTCondor detect when the job is finished. Then afterwards it can transfer the related files back to the local HTCondor.Part C shows how to sumit a Hadoop job into Sun Grid Engine Figure 4 The CloudMan cluster setup for HTCondor flocking scenario. In this picture two CloudMan cluster has been created: one on the Amazon cloud \(AWS\nd one on the NeCTAR cloud \(NeCTAR The flocking happen from AWS to NeCTAR 247 


is due to the fact that by using CloudMan it has become feasible to have a federated cluster computing over the cloud; something that otherwise requiresconsiderable effort on behalf of researchers or users Adding these features to CloudMan significantly extends the feature set of CloudMan and confirms its role as an accessible cloud management console and a provider of big-data-platform-on-demand.Based on our knowledge and supported by the relevant works described in Section II, this is the first solution that delivers an efficient approach for providing ready to use big-data-analysisplatform over a federated cluster on the cloud environment; our solution utilizes existing technologies to deliver a novel architecture and functionality to cloud environments, minimizing required investment and yielding an innovative approach A CKNOWLEDGMENT  The CloudMan is being developed by the Galaxy http://wiki.galaxyproject.org/GalaxyTeam teams \(http://www.vlsci.org.au/page/lscc Funding This work was, in part, supported by the Genomics Virtual Laboratory \(GVL\grant from the National eResearch Collaboration Tools and Resources NeCTAR R EFERENCES     The da ta de l u ge  Nature Cell Biology vol. 14 August 2012   E Afga n, D  Ba ker N. Coraor B. Chapm an A  Nekrutenko, and J. Taylor, "Galaxy CloudMan delivering cloud compute clusters BMC Bioinformatics vol. 11, p. S4, 2010   E  Afga n, B Cha pma n, and J. Ta ylor  CloudMan as a platform for tool, data, and analysis distribution BMC Bioinformatics vol 13, pp. 315-315, 2012   E Afgan D Ba ker  N. Coraor H Goto  I  M Paul, K. D. Makova, A. Nekrutenko, and J Taylor, "Harnessing cloud computing with Galaxy Cloud Nature Biotechnology vol. 29 pp. 972-974, 2011   K  Kram pis, T. Boot h, B. Cha pma n B  Tiw a ri M. Bicak, D. Field, and K. E. Nelson, "Cloud BioLinux: pre-configured and on-demand bioinformatics computing for the genomics community BMC Bioinformatics vol. 13, pp 42-42, 2012   J  Goecks  A. Ne krutenko, and J. Ta yl o r  Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences Genome Biol vol. 11, p. R86, 2010 7  E Af gan, J. Goecks, D. B a ker  N Corao r  T G  Team, A. Nekrutenko, and J. Taylor, "Galaxy - a Gateway to Tools in e-Science," in Guide to eScience: Next Generation Scientific Research and Discovery K. Yang, Ed., ed: Springer 2011 pp. 145-177   A  Nekrute n ko a nd J Ta yl or Next-generation sequencing data interpretation: enhancing reproducibility and accessibility  9 J De an an d S  Gh e m a w at M a p R e d u c e Simplified Data Processing on Large Clusters Communications of the ACM vol. 51, pp. 107113, 2008  D  Tha in, T Ta nne nbaum  and M Li vny Distributed computing in practice: the Condor experience CONCURRENCY AND COMPUTATION vol. 17, pp. 323-356, 2005 1 C. Ivica J. T Rile y, and C  S huber t  S t a rH P C  Teaching parallel programming within elastic compute cloud Proceedings of the ITI 2009 31st International Conference on Information Technology Interfaces p. 353, 2009 1  Amazon Web Services Available http://aws.amazon.com   G   P i e rre and C. Stra t a n, "ConPa a S: A Platfor m for Hosting Elastic Cloud Applications IEEE Internet Computing vol. 16, pp. 88-92, 2012  B. La ngme ad, M  C S c hatz J. Lin, M P op, and S. L. Salzberg, "Searching for SNPs with cloud computing Genome Biol vol. 10, p. R134 2009 1  High performance cluster computing. 1 Architectures and systems vol. 1: Upper Saddle River P r entice  Hal l 1999 1999   S  Bhar dwaj L. Jain, a nd S. Ja in, "C l oud Computing: A Study of Infrastructure as a Service \(IAAS International Journal of Engineering and Information Technology vol 2, p. 3, 2010  D   H J Epem a M Li vny R. va nDantz i g  X  Evers, and J. Pruyne A worldwide flock of Condors: Load sharing among workstation clusters   I  Foster a n d C Kesse l m an  T he Gl obus  Toolkit," in The Grid: Blueprint for a New Computing Infrastructure I. Foster and C Kesselman, Eds., ed San Francisco, California Morgan Kaufmann, 1999, pp. 259--278  S  Krish na n, "m yHadoop: Hadoop-on-Demand on Traditional HPC Resources," ed. The UC Cloud Summit 2011: University of California Los Angeles, 2011 2  Genomics Virtual Lab \(GVL Available https://genome.edu.au  M  d Holl a n de r  T h e Cloud for B i ol ogists   e d  SARA HPC Cloud Day, Amsterdam, 2011   248 


  7  Lorenz, R  D., Experi m e n t s i n Timel a pse C a m e r a  Observations of Dust Devil Activity at El Dorado Playa Nevada, Abstract #1573, 42nd Lunar and Planetary Science Conference, Lunar and Planetary Institute Houston, TX, 2011  Lorenz R  D., B  Jackson and J. B a rnes, Inexpensi v e Timelapse Digital Cameras for Studying Transient Meteorological Phenomena : Dust Devils and Playa Flooding, Journal of At mospheric and Oceanic Technology, 27, 246-256, 2010  C a st ano, A., A. F ukanag a J. B i esadecki, L. Neakrase, P Whelley, R. Greeley, M. Lemmon, R. Castano, S. Chien Automatic detection of dust devils and clouds on Mars Machine Vision and Applications, 19, 467-482, 2008  Lorenz R  D. and A Val d ez, Var i abl e W i nd R i p p l e  Migration at Great Sand Dunes National Park, Observed by Timelapse Imagery, Geomorphology, 133, 1-10, 2011 19 B a l m e  M. R A  Pa th a r e, S.M Me tzg e r  M C. To w n er  S.R. Lewis, A. Spiga, L.K. Fenton, N.O. Renno, H.M Elliott, F.A. Saca, T.I. Michae ls, P. Russell, J. Verdasca Field measurements of horizontal forward motion velocities of terrestrial dust devils: Towards a proxy for ambient winds on Mars and Earth, Icarus, 221, 632ñ645 2012  Koch, W  On B a y e si an Tracki ng and Dat a Fusi on  A Tutorial Introduction with Examples, IEEE Aerospace and Electronics Systems, 25, 29-51, 2010  Biographies Ralph Lorenz is a planetary scientist at the Johns Hopkins University Applied Physics Laboratory, with interests in atmospheres surfaces and their interactions, especially on Titan and Mars.  He worked for the European Space Agency on Phase B of the development of the Huygens probe to Titan, and subsequently built part of the instrumentation of the probeís Surface Science Package SSP\. Prior to joining APL in 2006, he spent 12 years in various positions at the Lunar and Planetary Laboratory at the University of Arizona, where he led observation planning for the Cassini RADAR investigation, and served on the science team of the New Millennium DS-2 mission to Mars. He is th e author of several books including ëSpinning Flightí, ëTitan Unveiledí, and ëSpace Systems Failures. He has a B.Eng in Aerospace Systems Engineering from the University of Southampton \(UK and a Ph.D. in Physics from the University of Kent at Canterbury \(UK\. He is the recipient of 5 NASA Group Achievement Awards   


  8  


Virtual Social Networks Analysis in Computational Social Network Analysis  ser Computer Communications and Networks A Abraham A.-E Hassanien and V Sn  ael Eds London Springer London 2010 ch 1 pp 3Ö25  J K orner  Fredman-k olmo s bounds and information theory   SIAM J Algebraic Discrete Methods  vol 7 no 4 pp 560Ö570 Oct 1986  T  Leighton and S Rao Multicommodity max-îo w min-cut theorems and their use in designing approximation algorithms J ACM  vol 46 no 6 pp 787Ö832 Nov 1999  M Bastian S He ymann and M Jacomy  Gephi An open source software for exploring and manipulating networks 2009  A.-L Barabasi and R Albert Emer gence of scaling in random networks Science  vol 286 no 5439 pp 509Ö512 1999 


application or middleware platform to collect request ows Thus it is much easier to deploy FChain in large-scale IaaS clouds Blacksheep correl a t e s t he change poi nt of s y s t em-l e v el metrics e.g cpu usage with the change in count of Hadoop application states i.e events extracted from logs of DataNodes and TaskTrackers to detect and diagnose the anomalies in a Hadoop cluster Kahuna-BB correl a t e s b l ack-box dat a system-level metrics and white-box data Hadoop console logs across different nodes of a MapReduce cluster to identify faulty nodes In comparison FChain is a black-box fault localization system which is application-agnostic without requiring any knowledge about the application internals We believe that FChain is more practical and attractive for IaaS cloud systems than previous white-box or gray-box techniques V C ONCLUSION In this paper we have presented FChain a robust blackbox online fault localization system for IaaS cloud computing infrastructures FChain can quickly pinpoint faulty components immediately after the performance anomaly is detected FChain provides a novel predictability-based abnormal change point selection scheme that can accurately identify the onset time of the abnormal behaviors at different components processing dynamic workloads FChain combines both the abnormal change propagation knowledge and the inter-component dependency information to achieve robust fault localization FChain can further remove false alarms by performing online validation We have implemented FChain on top of the Xen platform and conducted extensive experimental evaluation using IBM System S data stream processing system Hadoop and RUBiS online auction benchmark Our experimental results show that FC hain can achieve much higher accuracy i.e up to 90 higher precision and up to 20 higher recall than existing schemes FChain is light-weight and non-intrusive which makes it practical for large-scale IaaS cloud computing infrastructures A CKNOWLEDGMENT This work was sponsored in part by NSF CNS0915567 grant NSF CNS0915861 grant NSF CAREER Award CNS1149445 U.S Army Research Ofìce ARO under grant W911NF-10-1-0273 IBM Faculty Awards and Google Research Awards Any opinions expressed in this paper are those of the authors and do not necessarily reîect the views of NSF ARO or U.S Government The authors would like to thank the anonymous reviewers for their insightful comments R EFERENCES   A m azon E las tic Com pute Cloud  h ttp://a w s  a m azon com ec2   V i rtual c om puting lab  http://vcl ncs u  e du  P  Barham  A  D onnelly  R I s aacs  a nd R M o rtier   U s ing m agpie f or request extraction and workload modelling in 
 2004  M  Y  Chen A  A ccardi E  K icim an J  L lo yd D  P atters on A  F ox and E Brewer Path-based failure and evolution management in  2004  R F ons eca G  P o rter  R H  K atz S  S h enk e r  and I  S toica X T race A pervasive network tracing framework in  2007  I  Cohen M  G o lds z m i dt T  K elly  J  S ym ons  a nd J  S  Chas e Correlating Instrumentation Data to System States A Building Block for Automated Diagnosis and Control in  2004  I  C ohen S  Z h ang M  G o lds z m i dt J  S ym ons  T  K elly  a nd A  F ox Capturing indexing clustering and retrieving system history in  2005  S  D uan S  Bab u  a nd K  M unagala F a A s ys tem for a utom ating failure diagnosis in  2009  S  K andula R Mahajan P  V erkaik S  A garw al J  P a dhye a nd V  Bahl Detailed diagnosis in computer networks in  2009  A  J  O liner  A  V  K ulkarni and A  A ik en  U s ing c orrelated s u rpris e to infer shared inîuence in  2010  P  Bahl R Chandra A  G r eenber g  S  K andula D  A  M altz and M Zhang Towards highly reliable enterprise network services via inference of multi-level dependencies in  2007  Z  G ong X  G u  a nd J  W ilk es   P RE S S  P Redicti v e E las tic ReS ource Scaling for Cloud Systems in  2010  H  N guyen Y  T a n and X  G u P A L  P ropagation-a w are a nom aly localization for cloud hosted distributed applications in  2011  B Gedik H Andrade K L  W u P  S  Y u and M  D oo SP ADE  t he system s declarative stream processing engine in  2008  A pache H adoop S y s tem   http://hadoop apache  or g/co re   Rice uni v e rs ity bidding s y s tem   http://rubis  objectw eb  o r g   M Ben-Y e huda D  B reitgand M F actor  H  K o lodner  V  K r a v ts o v  and D Pelleg NAP a building blo ck for remediating performance bottlenecks via black box network analysis in  2009  Y  T a n X  G u  a nd H  W a ng  A dapti v e s ys tem anom aly prediction f or large-scale hosting infrastructures in  2010  D  L  M ills   A b rief his t ory o f N T P tim e m e m o irs o f a n i nternet timekeeper  2003  Y  T a n H  N guyen Z  S h en X  G u C V e nkatram ani and D  R ajan PREPARE Predictive Performance Anomaly Prevention for Virtualized Cloud Systems in  2012  M  Bas s e ville and I  V  N ikiforo v   Prentice-Hall Inc 1993  L  Cherkaso v a  K  O zonat N Mi J  S ym ons a nd E  Sm irni  Anom aly application change or workload change towards automated detection of application performance anomaly and change in  2008  P  Barham and e t al   X e n a nd the a rt of virtualization  i n  2003  T he ircache p roject  h ttp://www.ircache.net  H ttperf  h ttp://code google com  p htt p er f  S  K u llback  T h e ku llback-leibler distance  1987  X  Chen M  Z hang Z  M  M a o and P  B ahl  A utom ating n etw ork application dependency discovery experiences limitations and new solutions in  2008  M Y u  A  G reenber g  D  M altz J  Re xford L  Y u an S  K andula and C Kim Proìling network performance for multi-tier data center applications in  2011  M K  A guilera J  Mogul J  W iener  P  R e ynolds  a nd A  Muthitacharoen Performance debugging for distributed systems of black boxes in  2003  S  A g arw ala F  A l e g re K  S chw a n and J  M ehalingham  E 2E P r of A utomated end-to-end performance management for enterprise systems in  2007  P  Re ynolds  J  L  W iener  J  C M ogul M  K  A guilera and A  V ahdat  WAP5 black-box performance debugging for wide-area systems in  2006  R Apte L  Hu K  S chw a n and A  G hosh L ook W ho s T alking Discovering dependencies between virtual machines using cpu utilization in  2010  G Khanna I  L aguna F  A rshad an d S Bagchi Distr ibuted diagnosis of failures in a three tier e-commerce system in  2007  R R S a m b as i v an A  X  Z heng M  D e Ros a  E  K re v at S  W h itm an M Stroucken W Wang L Xu and G R Ganger Diagnosing performance changes by com paring request ows in  2011  J  T a n a nd P  N a ras i m h an  RA M S and B lackS h eep I nferring w h ite-box application behavior using black-box techniques CMU Tech Rep 2008  J  T a n X  P a n E  Marinelli S  K a vulya R  G andhi a nd P  N a ras i m h an Kahuna Problem diagnosis for mapreduce-based cloud computing environments in  2010 
OSDI NSDI NSDI OSDI SOSP ICDE SIGCOMM DSN SIGCOMM CNSM SLAML SIGMOD ICAC PODC Computer Communication Review ICDCS Detection of abrupt changes theory and application DSN SOSP The American Statistician OSDI NSDI SOSP DSN WWW HotCloud SRDS NSDI NOMS 
207 
30 
30 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





