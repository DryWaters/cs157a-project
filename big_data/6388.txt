   mm14 067   1   Abstract 227  The tensor product, denoted by  Kronecker Product\, may be applied in different contexts to vectors matrices, tensors, vector spaces, algebras etc  The element block wise inverse Jacket \(BIJM\ is simply calculated for big data which we called big data \221Fork\222 matrix. We use matrix decomposition method to compress or minimize the big matrix to smaller matrix. In this paper, based on the BIJM, a unifi ed fast hybrid diagonal block wise transform \(FHDBT\lgorithm is proposed. A new fast diagonal block matrix decomposition is made by the matrix product of successively lower order diagonal Jacket matrix and Hadamard matrix  The FHDBT, which is able to con vert a newly developed discrete cosine transform \(DCT II discrete sine transform \(DST II, discrete Fourier transform DFT\, and Haar based wavelet transform \(HWT  Comparing with pre existing DCT II, DST II, DFT, and HWT, it is shown that the proposed F HDBT exhibits less the complexity as its matrix size gets larger. From the numerical experiments, it is shown that a better performance can be achieved by the use of DCT/DST II compression scheme compared with the DCT II only compression method   Index Ter ms 227 Diagonal block \(element wise inverse Jacket matrix \(BIJM\, Hadamard matrix, Kronecker product  I  I NTRODUCTION  OW adays  Big Data has become a promising area to do the research. As of 2012, limits on the size of data sets that are feasible to process in a reasonable amount of time were on the order of exabytes of data. Researchers frequently encounter limitations due to large data sets in many areas including meteorology, genomics, connectomics, and biological and environmental research. The limitatio ns also affect Internet search, finance and business informatics. The world's technological per capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes \(2.5\3271018\ of data were created Th erefore, the challenge for large enterprises is determining who should own big data initiatives that straddle the entire organization. The BIJM is simply calculated for big data which we called big data \221Fork\222 matrix. We use matrix T his work was supported by the MEST 2012 002521, NRF, Korea  Moon Ho Lee and Md. Hashem Ali Khan  are  with the Dept  of Electronics Information Engineering  Chonbuk National University  Jeonju City Jeonbuk, South Korea 561 756  email hashem05ali@jbnu  ac.kr  moonho@jbnu.ac.kr   C orresponding author to provide phone  82 63 270 2463, fax: +82 63 270 4166  decomposition method to compress or minimize the big matrix to smaller matrix. The more detailed example, applications are BPSK, QPSK, 8PSK, Alamouti MIMO schemes, the MIMO LTE downlink Alamouti: non binary BIJM. The DFT element wise Inverse Jacket matrix: precoding, the element wise inverse diagonal weighted Jacket matrices for closed loop MIMO precoding: 4 antennas, 8 antennas, and the Fast DST II/DCT II/DFT/Wavelet hybrid transform based on BIJM. The last decade based on orthogonal transform has been seen a quiet revolution in digital video technology such as moving picture experts group \(MPEG 4, H.264, and high efficiency video coding \(HEVC 226 5 T h e  discrete cosine transform  DCT II is popular compression structures for MPEG 4 H.264, and HEVC, and is accepted as the bes t suboptimal transformation since its performance is very close to that of the statistically optimal Karhunen Loeve transform \(KLT 5  Practical considerations, such as underlying the H.264 advanced video coding \(AVC\ intra mode, dictate the transfor m coding implementation within a block coder with a typical block of size up to 16 \327 16. However, since a DCT based block coder suffers from blocking effect, i.e., a disturbing discontinuity at the block boundaries, much research effort has been leveraged to reduce the blocking effect. The discrete signal processing based on the discrete Fourier transform  DFT\ is popular in orthogonal frequency division multiplexing \(OFDM\ wireless mobile communication systems [3 s u ch as 3 r d g en er at i o n  p ar t n er s h i p  project  long term evolution \(3GPP LTE\, mobile worldwide interoperability for microwave access \(WiMAX\, international mobile telecommunications advanced \(IMT Advanced\ as well as wireless local area network \(WLAN\. In addition wireless personal area network \(WPAN\, and broadcasting related applications \(digital audio broadcasting \(DAB\, digital video broadcasting \(DVB\, digital multimedia broadcasting DMB\ are based on DFT. Furthermore, the Haar based wavelet transform HWT\ is also very useful in the joint phot ographic experts group committee in 2000 \(JPEG 2000 standard [2 6     Definition Let   N ij a J  be a matrix, then it is called the Jacket matrix when   1 1  1 T N ij a N J   That is, the inverse of the Jacket matrix can be dete rmined by its element wise inverse [7 8  9   Big Data \221Fork\222: Tensor Product for DCT II  DST II/ DFT/HWT  Moon Ho Lee  Md. Hashem Ali Khan   N     


   mm14 067   2   Proposition With the use of the Kronecker product and Hadamard matrices, a higher order block wise inverse Jacket matrix \(BIJM\ can be recursively obtained by   22 2 NN N  J JH   1  then  1 22 1 T NN N JJ  2  where the lowest order Hadamard matrix is defined by 2 11 11    H    Proof    Eq. \(2\ holds for 2 N  8. Now we assume that the  BIJM N J satisfies \(2\, i.e 2 T NN N N JJ I Since 22 T NN JJ    2 2 22 T TT N N NN  JHJH JJ HH 2 2 2 N N  II 2 N N I this proposition is proved by mathematical induction th at \(4\ holds for all 2 N If 1 N certainly 22 2 T JJ I   Thus, different applications require different types of unitary matrices and their decompositions. From this reason, in this paper we w ill propose a unified hybrid algorithm which can be used in the mentioned several applications in different purposes. We propose the diagonal sparse matrix factorization for a unified hybrid algorithm based on the properties of the Jacket matrix [7 8    a n d t h e de c om pos i t i on of t h e s pa r s e  matrix. It has been shown that this matrix decomposition is useful in developing the fast algorithms and characters [9   Individual DCT II [1 226 3 D S T II [4 1 0  1 2  D F T 4   10 1 1  a n d H W T 6  m a t ri c e s  c a n be decomposed to one orthogonal character matrix and a corresponding special sparse matrix   The rest of this paper is organized as follows: in Section II proposed hybrid for fast computations of DCT II, DST II DFT and HWT matrices. In Section III, we introduce hybrid fast algorithm. Numerical simulations are presented in Section IV and the conclusion is given in Section V  II  P ROPOSED H YBRID FOR F AST C OMPUTATIONS OF DCT II  DST II  DFT  AND HWT  M ATRICES   We have derived recursive formulas for DCT II, DST II DFT, and HWT [16   T he d e r i ve d  r e s ul t s  s ho w  t ha t  D C T II DST II, DFT, and HWT matrices can be unified by using a similar sparse matrix decomposition algorithm, which is based on the block wise Jacket matrix and diagonal recursive architecture with diff erent characters. The conventional method is only converted from DFT to DCT II, DST II, and DWT. But our proposed method can be universally switching from DCT II to DST II, DFT and HWT. Let us motivate to develop universal hybrid architecture via switching  mode selection Moreover, the butterfly data flow graphs have 2 log N  stages We can generate Fig. 1 according to the following proposed ways  A  From DCT II to DST II   The N point DCT II of   x n  is given by   1 0 2  2 1 2     cos 2 1 0 where  0,1 1 and 1 2  0 N DCT N m mN n m mn X m c xn c C N NN m mn N c m  x    3  The N point DST II of   x n is given by  1 0 2  1 2 1 2     sin 2 1 1 where  0,1 1 and 1 2  1 N DST N m mN n m mn X m s xn s S N NN mN mn N s mN    x    4  Let N C   and N S  be orthogonal  NN  DCT II and DST II matrices, respectively T x x xN x         01 1  denotes the column vector for the data sequence   x n  Substituti ng    mNk k N  1 12  into \(3\, we have  N N Nk n n Nk C N k c xn NN kN          cos     1 0 2 21 1 1 2 0 12 1  5  Using the following trigonometric identity  n n nk N n nk N n nk N nk N                 cos   cos cos   sin sin      sin 21 21 1 22 21 21 1 22 21 21 1 22 21 1 1 2  6  5\  becomes  N n N Nk n nk C N k c x n NN           sin 1 0 2 21 1 11 2   7  where  N CNk 1  represents the reflected version of  N Ck  and this can be achieved by multiplying the reversed identity matrix N I  to N C Equation \(7\ can be represented in a mo re compact matrix multiplication form [12   N NNN N NNN  I S CM C ISM  8  


   mm14 067   3  where NN   22 M MI and 2 10 01 M  Then, the DST II matrix is resulted from the DCT II matrix Note that compatibility property exists in the DCT II and DST II     a  Conversion block diagram from DFT to DCT II and DST II     b  B lock diagram  of hybrid DCT II DSTII DFT/HWT     c\ Diagonal block wise source modulation channel  Fig. 1 Diagonal block wi se DCT II/DST II/DFT/HWT for switching and modulation channel  B  From DCT II & DST II to DFT   We develop a relation for the circular convolution operation in the discrete cosine and sine transform domains We need to measure half of the total coefficients The main advantage of a proposed new relation is that the input sequences to be convolved need not be symmetrical or asymmetrical. Thus, the transform coefficients can be either symmetric or asymmetric [17     From \(3\ and \(4\, it changes to coefficient for circular convolution \(C\ format. Thus, we see this equation as below    1 0 21   2 cos  0 1  1 2 N DCT IIC N n mn m xn m N N  X  and    1 0 21   2 sin  1   2 N DST IIC N n mn m xn m N N X  9  We can write DFT as follows   1 2 0  0,1 1 N j mn N n m x ne m N  X 10  Now multiplying \(10\ by  2 j mN e  we can get           1   2 0 1  2 0 21 1 0 1 0 22 2 2 21 cos 2 21 sin N j mN j mN j mnN n N j m N j mn N n mn N j N n N n e m e x ne xne e xne mn N xn mn j N         X  11  Comparing the first term of \(9\ with the first one of \(11\, it can be seen that   1 0 21 2 cos N n mn xn N    is decimated and asymmetrically extended of \(9\ with index 0 1 mN   Similarl y   1 0 21 2 sin N n mn xn N     is decimated and symmetrically extended of \(9\ with index 1 mN The circular convolution of cosine and sine periodic sequences in time/spatial domain is equivalent to the multiplication in the DFT domain Then, the DFT matrix is resulted from the DCT II and DST II matrix  C  From DCT II to HWT   In this subsection, if a discrete wavelet transform \(DWT\ is discretely sampled, it becomes the two point discrete Fourier transform, where we allow to use \275 and 1 i n the matrix and its inverse. The orthogonal matrix has 1 2  in both. The DCT DST and DFT based matrix is 11 11    otherwise the HWT becomes the Haar matrix as 11 1 11 2     


   mm14 067   4   1 11 11 1 11 11 2    then   2 11 11 1 11 11 2    I  12  Since for a 22 Haar matrix   1 1 2 1 2 1 1 1 11 2 1 2 1 2 rr rr        L B  13  where L is low pass and B is high pass band, we can have   2 TT TT TT   L L L LB LB I B BL BB 14  Note that the  2 I matrix in \(12\ and \(14\ has been well matched to the propose based Jacket Hadamard matrix. Thus after multiplying the diago nal NN  matrix, the HWT can be obtained as  2 2 2 2 2 2 2 2 N NN HWT N NN N N r    I 0 II MI 0 H I I 15   III  H YBRID F AST A LGORITHM  Based on the above conversions from the proposed decomposition of DCT II, we can form a hybrid fast algorithm that can cover DCT II, DST II, DFT, and HWT. The general block diagram of the proposed hybrid fast algorithm is shown in Fig 1 The common basis function of  11 22 1 2 11 22 hh h N hh  II I II is multiplied by the sequence with different transforms as like as bracket     The requiring computational complexity of individual DCT II, DST II, DFT and HWT is summarized in Table I and Table II  We show a complexity comparison among the proposed decomposition and other methods in Table III  It can be seen in Fig  2 that the proposed hybrid algorithm scheme requires  much  less computation al complexity  in addition  and multiplication compared to Wang 222 s 12  Chen 222 s 13  and Andrews and Caspari\222s result 15    Addition, compared to these transforms the proposed hybrid fast algorithm can be efficiently extensible  to larger transform sizes due to its diagonal block wise inverse operation  of recursive structure Moreover, the proposed hybrid structure  is easily extended to cover different applications  For example, a base  station wireless communication terminal delivers a compressed version of multimedia data via wireless communications network. Either DCT II or DST II can be used   in compressed multimedia data as block diagonal can significantly reduce its complexity com ing from its simple structure [9, 18  d e p e nd i ng o n t he  va r i a nc e  o f  t he  o r i gi na l  multimedia source. The DCT image coding can be easily implement in the proposed hybrid structure as shown in Fig 1\(b  If the DCT II is multiplied by NN N I CM then we get DST. If the DCT/DST II is convolved in time/frequency domain and multiplied by  2 j mN e then it can be obtained the DFT. Finally, if the DCT is multiplied by  2 2 2 2 2 2 2 2 N NN N NN N N r    I 0 II I 0 H I I then we get HWT Thus, the propose d hybrid algorithm enables the terminal to adapt to different transforms efficiently  Table I: The comparison of computation complexity of conventional independent the DCT II, DST II, DFT, Haar transform and hybrid DCT II/DST II/DFT/HWT    Table II Compu tational Complexity: DCT II/DST II/DFT/HWT   IV  N UMERICAL S IMULATIONS  As shown  in 19    the coding performance of DST outperforms DCT at high correlation values   and  it  is very close to that of the KLT T he interesting  points are  that the basis vectors of DCT maximize their energy distribution at both ends, hence the discontinuity appears at block boundaries due to quantization effects   However, since  the basis vectors of DST minimizes their energy distribution at other end DST  


   mm14 067   5  p rovides smooth transition between neighboring blocks  Therefore  the  proposed hybrid transform coding scheme provides consistent  reconstruction and preserves more details  as shown in Fig  3 with a size of 512 512 and 8 bits quantization                                                   50 100 150 200 250 0 500 1000 1500 2000 2500 3000 Matrix Size,N Operations of addition   Chen DCT-II Wang DST-II Cooey DFT, Proposed DCT-II,DST-II,DFT Andrews HWT Proposed HWT  a  Addition counts                                                               50 100 150 200 250 0 200 400 600 800 1000 1200 1400 1600 1800 Matrix Size,N Operations of Multiplication   Chen DCT-II Wang DST-II Proposed DCT-II,DST-II Cooey DFT & Prposed DFT Andrews HWT Proposed HWT  b  Multiplication counts   Fig 2 Comparison plot for computational complexity of conventional methods and the proposed ones    Now consider a NN  block of pixels, X containing i,j=1,2,...,N  ij x We ca n write 2 D transformation of  the k th block X as and T S S k C Ck Y T QX Q Y T X  Depending on the availability  of boundary values \(in top  boundary and left boundary\ in images   the hybrid coding scheme accomplishes the 2 D transform of a block pixels in two sequential 1 D transforms separately performed on rows and columns. Therefore   the choice of the 1D transform for each direction is dependent on the corresponding prediction boundary condition   Vertical transform for each column vector employ DST if to p boundary is  used for prediction; otherwise use DCT   Horizontal transform for each row vector employ DST if left boundary  is used for prediction; otherwise use DCT     Fig 3 Image Coding Results showing DCT II only and jointly optimized DCT/DST II co mpression    What we have observed from numerical experiments is that the combined scheme over DCT II performs better only in perceptual clarity as well as PSNR  Jointly optimized spatial prediction and block transform \(see Fig 3 e\and \(f\sing DCT DST II PSNR 35.12dB compression outperforms DCT II PSNR 32.38dB\ compression only. Less blocky artifacts are revealed compared to that of DCT II  24dB  Without  a priori  knowledge  of boundary condition  DCT II performs better than any other block trans forms  coding The worst result is obtained by using only DST II  V  C ONCLUSION  Now adays Big Data has become a promising area to do the research T his product is also referred to as outer product The term "tensor product" is also used in relation to monoid al categories  In this paper, we have derived a unified fast hybrid diagonal block wise transform based on Jacket Hadamard matrix Therefore, the complexity of the proposed unified hybrid algorithm is much less as its matrix size gets larger  Jointly optimized DCT/DST II compression scheme reveals a better performance \(about 3dB  over the DCT only compression method  R EFERENCES  1  K. R. Rao and P. Yip Discrete Cosine Transform: Algorithms Advantages, Applications Boston, MA: Academic Press, 1990  2  I. E. Rich ardson The H.264 Advanced Video Compression Standard  2nd ed. Hoboken, New Jersey: John Wiley and Sons  3  K. R. Rao, D. N. Kim, and J. J. Hwang Fast Fourier Transform Algorithm and Applications New York, N.Y.: Springer, 2010  4  A. K. Jain Fundamentals of Digital Image Processing Prentice Hall 1987  


   mm14 067   6  5  R. Wang Introduction to Orthogonal Transforms: With Applications in Data Processing and Analysis Cambridge University Press Cambridge, UK  2012  6  G. Strang and T. Nguyen Wavelets and Filer Banks Wellesley  MA Wellesley Cambridge Press, 1996  7  M H Lee, \223A new reverse Jacket tra nsform and its fast algorithm\224 IEEE Trans. Circuits Syst. II vol. 47, no. 1, pp. 39 226 47, Jan. 2000  8  Z. Chen, M  H Lee, and G. Zeng, \223 Fast cocyclic Jacket transform\224  IEEE Trans. Si gnal Process vol. 56, pp. 2143 226 2148  May 2008  9  M. H  Lee Jacket Matrices Construction and Its Application for Fast Cooperative Wireless Signal Processing  LAP LAMBERT Academic publishing, Germany, November, 2012  10  ITU T SG16 WP3/JCT VC, CE 7.5, \223Performa nce analysis of adaptive DCT/DST selection,\224 July 2011  11  M H Lee, \223High speed multidimensional systolic arrays for discrete Fourier transform,\224 IEEE Trans. Circuits Syst. II  vol. 39, no. 12, pp 876 226 879, Dec. 1992  12  Z. Wang, \223Fast Algorithm for the Discre te W Transform and for the Discrete Fourier Transform,\224 IEEE Trans. on Acoustics, Speech and Signal Process vol. 32, No. 4, pp. 803 226 816, Aug. 1984                                                    13  W H. Chen, C. H. Smith, and S. C. Fralick, \223A fast co mputational algorithm for the discrete cosine transform,\224 IEEE Trans. Commun  vol. 25, no. 9, pp. 1004 226 1009, Sep. 1977  14  J. W. Cooley, J. W. Tukey, \223An algorithm for the machine calculation of complex Fourier series,\224 Mathematics of Computation vol. 19, no. 4 pp 297 301, 1965  15  H. C. Andrews, K. L. Caspari, \223A g eneralized t echnique for s pectral a nalysis,\224 IEEE Trans. Computers vol. 19, no. 1, pp 16 17, 1970  16  M. H. Lee, M. H. A. Khan, K. J. Kim, D  Park 223 A Fast Hybrid Jacket Hadamard Matrix Based Diagonal  Block Wise Transform 224, accepted at Elsevier Signal Processing: Image Communication December, 2013   17  V. G. Reju, S. N. Koh, I. Y. Soon, \223Convolution using discrete sine and cosine transforms\224 IEEE Signal Processing Letters vol. 14, no. 7, July 2007  18  Q  H. Spencer, A. L Swindlehurst, M. Haardt, \223Zero forcing methods for downlink spatial multiplexing in multiuser MIMO channels\224 IEEE Trans. Signal Process vol. 52,no. 2, Feb. 2004  19  J. Hai, A. Saxena, V. Melkote, and K. Rose, \223Jointly optimized spatial p rediction and block transform for video and image coding,\224 IEEE Trans. Image Process vol. 21, no. 4, pp. 1874 226 1884, April 2012    


IncrementalScan IncrementalScan T T T T T T T mf gr mf gr  Some of these belong to the real-time table while the others belong to the data cube An OLTP query is submitted to one of the servers and stored in of the it belongs to If the size of the reaches its upper bound the data are written into HDFS as a  Once the update is written to HBase-R it is streamed to a mapper in HStreaming based on the key of this update In the mappers of HStreaming the change of a cell for each cuboid is computed and shuf\224ed to reducers On the reduce side the real-time data cube is updated and cached in local disk At time interval HStreaming materializes its local data cube into HBase-R and noti\223es with the timestamp of the latest data cube The compaction process is then launched to compact the versions of data before data cube is refreshed When an OLAP query arrives it acquires a timestamp from the  together with the statistics of the real-time table stored in HBase-R It is then transformed to a MapReduce job based on the data statistics and submitted to the system Each mapper starts a scan operation over its input belonging to either the real-time table or the data cube At the end of the job the results of OLAP query are stored in HBase-R V R EAL T IME OLAP Section III to IV described in detail the architecture and implementation of R-Store In this section we discuss how the real-time OLAP queries are processed In R-Store if the input of the MapReduce job is only the data cube the performance of the scan phase on the map side is maximized but the result might be stale To maximize the freshness of the OLAP query all the updated key/value pairs before the submission time of the query must be considered Thus not only the data cube but also the real-time table must be scanned Suppose the creation time of the data cube is  For each updated key after running on the realtime table returns both the old version before  if its two parameters are set to respectively By merging these two versions with the numeric values of each cuboid the latest cuboid value can be computed on demand and the freshness of the OLAP query can be satis\223ed In the following subsection we present the query processing algorithm called  making use of the operation We implement so that each MapReduce job can scan the data of multiple tables and the scan operation of each table can be con\223gured as either full scan or incremental scan Using this input format the MapReduce job for can access two types of input tables one is the cuboid table for which a full scan is performed and the other is the real-time table over which the incremental scan is used  Algorithm 4 describes the map function The mappers 223lter the cell and the real-time tuple based on the 223ltering condition The cells and tuples that will be aggregated are assigned the same partition key and shuf\224ed to the same  Map Function for IncreQuerying Algorithm   KeyValueList kvlist Context context null value null kvlist.size  1 extractKe e y key is not 223ltered value  alue value.setTag\(\215Q\216 Emit\(key value  extractKe alue key is not 223ltered value extractV alue value.setTag\(\215 216 context.write\(key value value extractV alue value.setTag\(\215 216 Emit\(key value    reducer The output value for the cell is the selected numeric value while the output value for the real-time tuple is the original value which will be used to re-compute the numeric value The value is attached with a tag 215Q\216 215-\216 or 215+\216 to indicate whether it is the cell value of a cuboid the old value of a key/value pair or the new value respectively This phase is similar to the map phase of incrementally updating the data cube except that a 223ltering process is added and the partition key could be different from the dimension attributes of the data cube  The reduce function calculates the new value of each cell based on the old cell value the change of the cell and the aggregation function The cell key of the reduce function is different from that of Algorithm 3 For example for the TPCH  215Manufacturer#13\216 the key of the reduce function is the combination of the attributes  to  after removing  Figure 3 shows the data 224ow of alogrithm for an OLAP query on a two-dimensional cuboid    The query computes the summation of for each brand produced by 215M1\216 To ensure the freshness of the results all the data of the queried table and the cuboid are scanned to process the real-time query Note that the row key of the stored data cuboid is the combination of the dimension attributes Therefore if the 223ltering condition contains some attributes that could form a pre\223x of the row key such as 215Manufacturer#1\216 and 215Brand#13\216 the range scan function of HBase-R can be used to avoid scanning the entire data cube The min key for the range scan is 215Manufacturer#1,Brand#13\216 and the max key is 215Manufacturer#1,Brand#14\216 Operator  TABLE I D ATA C UBE O PERATIONS   setNumericAttribute  aggregation function name    Map Algorithm 4 input then  then  then  Reduce part 003 003 003 regions regions region memstore region memstore store\223le MetaStore MetaStore region IncreQuerying A Querying Incrementally-Maintained Cube MultiTableInputFormat IncreQuerying brand container IncreQuerying mfgr brand price 46 1 2 if 3 4 if 5 6 7 8 else  9 10 if 11 12 13 14 15 16 Parameters     server handles several and the submission time of the query is  and the latest version before and table to compute a rectangular subset of the cube  attribute name function value    group-by attribute    numeric attribute name   DC DC DC DC  key key key Q Q Q 003 003 003 003 212 addFilter  addGroupBy  setAggregationFunc  


MR  HBase 327                       T 327 f  n              IncrementalScan IncrementalScan FullScan FullScan IncrementalScan FullScan IncrementalScan IncrementalScan FullScan IncrementalScan FullScan 1 2 3 4 5 7 De\223nition     Algorithm 5 input Part  M1,B1,831  101  mfgr,brand  M2,B1  2440  v2  M1,B2,940   Mapper1  Mapper2    M2,B1,690  M2,B2  3513  M1,B2  1945 Filter Condition M1  brand  B1  B2  1945,Q   brand  B1  B2  940 B1  540 Reducer2         B1  B2  1005 Fig 3 Data Flow of IncreQuerying  HBase  HBase  Q Q k Q k Q Q k Q k T T    d T f T T    s T C d C n C Q s Q d Q n Q w c m m B  T  327  To guarantee correctness if the query needs to scan a table several times the scan process on each node always returns the data before time T T T k T T T T T T T s  T  s  T  d  T    T  d  T  C T  S s  Q  327   T  m d  Q  Q  47 number of tuples in table number of distinct keys updated    size of the tuple in table percentage of the keys that are updated since the last data cube refresh    number of cells in the selected cuboid    size of dimension attributes of cuboid    size of numeric attribute of cuboid    number of tuples in the query result    223ltering selectivity of the query    size of query result key    size of query result value    cost ratio of local I/Os    number of mappers for table T    number of mappers for the cuboid    block size   scan  L  T  C  sh sh Example Data Cube Query   DataCube cub cub.addFilter\(\215mfgr\216 215=\216 215Manufacturer#1\216 cub.addFilter\(\215brand\216 215=\216 215Brand#13\216 cub.addGroupBy\(\215type\216 cub.setNumericAttribute\(\215retailprice\216 cub.setOutputTable\(\215resultTable\216 SubmitQuery\(cub  To relieve users from having to merge the real-time data and the historic data cube we de\223ne new data cube operators and automatically translate these operators into a MapReduce job The processing of the real-time data is transparently encapsulated into the operators shown in Table I Algorithm 5 shows an example that computes the summation of the for all the parts with 215Brand#13\216 produced by 215Manufacturer#1\216 grouped by  When an OLAP query is submitted to the system a timestamp is acquired for this query from the  However in a distributed system although clocks can be synchronized to a certain extent there might still be some difference between the clocks of different nodes If the current timestamp on a certain node is smaller than  the next scan process on this node would return some data between and  which leads to an inconsistent state To avoid this inconsistency if the timestamp is larger than  the scan process is blocked for a while until is equal to or smaller than  Since clock synchronization can achieve one millisecond accuracy in local area networks under ideal conditions the delay of the scan process can be ignored compared to the processing time The algorithm discussed above is not always better The scans not only the realtime table but also the data cube incurring a higher cost In addition it shuf\224es two versions for each updated key to MapReduce When there are fewer OLTP transactions or the OLTP transactions access a small range of keys algorithm is better because only transfers a small amount of data to the mappers An alternative implementation of real-time querying is similar to re-computing the data cube a operation is used to return one version for each key/value pair regardless TABLE II P ARAMETERS   of whether or not it has been updated When the updates are uniformly distributed across all the keys this baseline implementation could be more ef\223cient To be able to select a more ef\223cient approach we propose a cost model Table II shows the parameters of the cost model The most important one is  which is the percentage of the keys that are updated after refreshing the data cube   or adaptive discussed in Section III-B1 and shuf\224ing these data to mappers The scans all the of the real-time table while the adaptive scans fewer when is small and the is activated depends on the status of each HBase-R node and cannot be easily estimated Thus we assume that the cost of reading the local data on each HBase-R node are the same for and  The difference is in the number of tuples transferred from HBase-R to mappers Thus we base our analysis on the network transfer cost The operation shuf\224es one version for each key and the cost of the scan phase of the MapReduce job is estimated as The mapper outputs one key/value pair for each tuple Thus the size of the map output is retailprice type B Correctness of Query Results MetaStore C Cost Model IncreQuerying IncreQuerying 1 Cost Analysis of the Baseline Method store\223les store\223les cub.setAggregateFunc\(\215sum\216 The baseline algorithm is a MapReduce job that is essentially similar to re-computing the entire data cube First we estimate the cost of the scan phase on the map side The scan phase consists of two parts scanning the local data on each HBase-R node  6 Parameter  sh Cube  v1  M1,B1  2954  M1,B1,540  2954,Q  831  brand  3245  MO  FullScan Incremental Scan   key  price  101  price  price  Reducer1  price  memstore cost ratio of shuf\224ing from HBase-R    cost ratio of HBase-R writes    cost ratio of shuf\224ing in MapReduce    has enough number of keys However whether the adaptive 


part MO MO MR MO MO MO MO MO MO MR sh sh sh sh 327 327 327 327  327             write write IncreQuerying part A Performance of Maintaining Data Cube part T L L R C R T C C R T L R R C C L C C L 48 reduce reduce reduce reduce 327 327 327  327 327  327 327 327 327 327 327 327 327 327  327 327 327  327 2 Cost Analysis of IncreQuerying Algorithm HBase HBase S  T n c T n Q n f C n T s n S n S C S T s n c T s n Q n and the cost of external sorting the map output is When the mappers complete the reducers start to pull the sorted key/value pairs from the mappers The cost of shuf\224ing is The pulled data are cached in the local 223le system whose cost can be ignored since the shuf\224ing process and the local writing process are pipelined The data shuf\224ed from the mappers are then merged into one sorted run using the multi-way mergesort method which only requires reading and writing the 223les once Finally the result table is written into HBase-R whose cost is computed as After the scan phase the real-time data and the data cube are sorted The size of the map output for these two types of data is and the cost of external sorting the map output is The data on all the mappers are shuf\224ed to the reducers after the mapper completes The cost of shuf\224ing is In the reduce phase the cost of sort merging process is and the cost of writing the data into HDFS is Based on the cost model discussed above the more ef\223cient approach is dynamically selected when a real-time query is submitted VI E VALUATION In this section we evaluate the R-Store on our in-house cluster of 144 nodes Each node is equipped with Intel X3430 2.4 GHz processor 8 GB of memory 2x500 GB SATA disks each of which is connected by a gigabit Ethernet and running CentOS 5.5 The cluster nodes are evenly placed onto three racks We adopt TPC-H data for the experiments However TPC-H updates only append new keys while we need online transactions that update the existing keys Therefore we write our own scripts to update the TPC-H data The scripts can update the keys based on either a uniform distribution or Zipf distribution In most of the following experiments we use the TPC-H table is loaded into HBase-R Figure 5 shows the processing time of the two methods The distribution of updated keys follows a Zipf distribution We adjust the factor of the Zipf distribution so that about 1 keys are updated while the number of updates is increased from 8 million to 1,600 million Since HBase-R does not remove the previous version of the data 0.024 GB to 4.8 GB of new data are inserted into each HBase-R node The processing time of re-computation has two parts the blue rectangle ReCompScan is the scan time of the real-time table and the yellow rectangle ReCompExe is the execution time of the MapReduce job after the scan phase As the number of updates increases the data stored on each HBase-R node increases as well Thus more data are scanned at the HBaseR side for the re-computation approach and the running time of the scan phase for re-computation is increased over time However as illustrated in Figure 5 the running time of the ReCompExe decreases as the number of updates increases which is counterintuitive We expected that the execution time of the MapReduce job should remain the same in different settings as they process the same number of key/value pairs The reason for the decrease in ReCompExe is that ReCompScan and ReCompExe are pipelined The more time ReCompScan takes the more these two phases overlap reducing the time ReCompScan takes In contrast the processing time of incremental update consists of only one part the red rectangle the time it takes to write data cube into HBase-R This is because our real-time data cube maintenance algorithm is fast enough to update the 327 327 327 327 327 327 327 327 327 327 327 327 327 327 327 327 327            2              2                        2    2                   The MapReduce job for reads both the data cube and the updated data Therefore its cost is different from that of recomputation At 223rst the mappers scan both the real-time data and the data cube The cost of shuf\224ing the real-time data and data cube to mappers is while the cost of scanning the data cube is table to build the data cube In this experiment we 223rst measure the throughput of our real-time data cube maintenance algorithm to ensure that it has suf\223ciently high processing capacity to handle the update streams from HBase-R As can be seen in Figure 4 when HStreaming is con\223gured with 10 nodes the algorithm can process more than 100K updates per second which is even higher than the throughput of HBase-R with 40 nodes the throughput of HBase-R will be discussed in Section VI-C We compare the two methods for refreshing the data cube re-computation and incremental update We deploy the system on 100 nodes with 40 nodes for MapReduce 40 nodes for HBase-R and 20 nodes for HStreaming The scale factor of the TPC-H data is set to 8000 so that there are 1,600,000,000 keys for table On each HBase-R node there are 4.8 GB data The data cube is built after the log log log C m c S B C s Q d Q Q C s Q d Q Q C w d Q Q C T T s T C d C C S s Q T m d Q Q C m s Q d Q Q C m c S  B m c S  B C s Q T C d Q Q C s Q T C d Q Q C w d Q Q 212 212 212 212 212 212 212 212 212 212 212 212 212 212 212 212 212 2 2  shuf f ling shuf f ling merge merge scan scan sort sort sort map map map B B B HBase HBase 2   1         1   1 2  2   


Manufacture B Performance of Real-Time Querying IncreQuerying Baseline Baseline IncreQuerying IncreQuerying Baseline Baseline Baseline Baseline mft IncreQuerying Baseline shipdate Lineitem shipdate return\224ag linestatus IncreQuerying Baseline IncreQuerying Baseline IncreQuerying y y IncreQuerying 11 In this experiment we investigate the performance of realtime querying First we compare the Fig 4 Throughput of Real-Time Data Cube Maintenance Algorithm                                                              Fig 5 Performance of Data Cube Refresh    Fig 6 Scalability real-time data cube with the data streams from HBase-R Thus the latency of periodically refreshing the data cube in HBaseR equals to the time of writing the real-time data cube into HBase-R This time is related to the the size of the data cube and does not change as the number of updates increases We also evaluate the scalability of R-Store In this experiment the number of nodes and the data size increase with the same ratio The percentage of updates is set to 1 for different scalability settings As can be seen in Figure 6 the running time of both re-computation the brown line and incremental update blue line do not change much as the number of nodes increase which demonstrates the scalability of R-Store 1 algorithm which optimizes the real-time query using the data cube with the algorithm implemented with the operation The cluster settings are the same as those of Figure 5 except that we 223x the number of updates to 8,000 million and vary the percentage of the keys updated Figure 7\(a shows the processing time of both algorithms for a typical data cube slice query algorithm consists of two parts the black rectangle ReCompScan is the time to scan the real-time table and the yellow rectangle ReCompExe is the execution time of the MapReduce job after the scan phase In contrast the processing time of consists of three parts the red rectangle CubeScan is the time to scan the data cube the blue rectangle UpdateScan is the time to scan the performs much better than  It outperforms the approach for two reasons 1 by using adaptive incremental scan it scans fewer data in HBase-R and shuf\224es fewer data to MapReduce 2 its MapReduce job processes fewer data than that of re-computation However as the percentage of updated keys increases more data are shuf\224ed from HBaseR to MapReduce Thus both the scan time and the execution time increase In contrast for  since the always shuf\224es one version for each key to MapReduce the amount of data shuf\224ed from HBase-R is constant As a result the running time of is almost constant Due to the existence of the 223ltering condition on attribute  most tuples of the table are 223ltered and fewer data are sorted and shuf\224ed during the execution of the MapReduce job As a result the difference between the execution times is not so signi\223cant In general algorithm outperforms algorithm when the percentage of keys being updated is low In addition to the data cube slice query we also evaluate TPC-H Q1 Figure 7\(b with the same experimental settings We did not illustrate other benchmark queries as they involve multiple tables which will not be able to illustrate as clearly the effectiveness of the basic operators supported in R-Store The parameter of TPC-H Q1 is set to 215365\216 days and only about 15 of the tuples are 223ltered  table since we only build the data cube on attributes  and  the data cube is much smaller than the real-time table and the time to scan the data cube is around 20 seconds Overall Figure 7\(b demonstrates that the performance of is signi\223cantly better than that of  To select the better querying method among the two we use the cost model Section V-C to estimate the number of I/Os Figure 8 shows the running time of  and the I/Os estimated for both and algorithms The axis on the left is the processing time of the query while the axis on the right is the estimated I/Os The estimated number of I/Os for the blue line increases linearly with almost the same slope the histogram as the processing time of the query while the estimated number of I/Os for the Baseline the brown line is constant which is around 2.52 10 FROM part WHERE mft 0 2,000 4,000 6,000 8,000 10,000 ReComp Update ReComp Update ReComp Update ReComp Update ReComp Update Processing time \(s Number of Updates 8M 400M 800M 1,200M 1,600M Update ReCompExe ReCompScan 100    200    300    400    500  10  20  30  40  50  60  70  0    1000    2000    3000    4000    5000    6000    7000    8000  50  75  100  125  145  IncrementalUpdate                            0    Updates Per Second \(K Number of Nodes Throughput          Processing Time \(s Number of Nodes ReComputation              The processing time of the table in HBase-R and the grey rectangle UpdateExe is the execution time of the MapReduce job after the scan phase When only a small range of keys are updated larger than 21512-01-1998\216 Thus the execution time of the MapReduce job after the scan phase is longer than that of Figure 7\(a For the  This result hence veri\223es the accuracy of our cost model Compared to querying only the data cube RTOLAP queries require two additional steps which incur additional cost scanning the real-time data from HBase-R and merging the real-time data with the data cube on demand in MapReduce SELECT sum   327 215 002\002    49 FullScan FullScan part shipdate prices GROU P BY brand type size container 


method increases slightly which is due to two reasons 1 the data before In this experiment we investigate the performance of OLTP queries when OLAP queries are running The workload is update-only and the keys being updated are uniformly distributed We launch ten clients to concurrently submit the updates when the system is deployed on 100 nodes Each client starts ten threads each of which submits one million updates 100 updates in batch Another client is launched to submit the data cube slice query That is one OLAP query and approximately 50,000 updates are concurrently processed in R-Store The system reaches its maximum usage in this setting based on our observation When the system is deployed on other number of nodes the number of clients submitting updates is adjusted accordingly Figure 11\(a shows the throughput of the system The throughput increases as the number of nodes increases which demonstrates the scalability of the system However when OLAP queries are running the update performance is lower than running only OLTP queries This result is expected because the OLAP queries compete for resources with the OLTP queries We also evaluate the latency of updates when the system is approximately fully used As shown in Figure 11\(b the aggregated response time for 1000 updates are similar with respect to varying scales VII C ONCLUSION MapReduce is a parallel execution framework which has been widely adopted due to its scalability and suitability in 0    500    1000    1500    2000  0  10  20  30  40  50  60  70  80  90  100  IncreQueryScan             IncreQueryExe              DC DC DC  Q i i i i T part  Q a Data Cube Slice Query                                                                                                b TPC-H Q1 Fig 7 Performance of Querying    Fig 8 Accuracy of Cost Model    Fig 9 Performance vs Freshness On each HBase-R node the key/values are stored in format Though only one or two versions of the same key are returned to MapReduce HBase-R has to scan all the of the table Since the is materialized to HDFS when it is full these 223les are sorted by time Thus instead of scanning all the and between  only the between   are scanned The value of decides the freshness of the result There is a trade-off between the performance of the query and the freshness of the result the smaller is the fewer real-time data are scanned Figure 9 shows the query processing time with different freshness ratios which is de\223ned as the percentage of the real-time data we have to scan for the query In this experiment  1600 million and 800 million updates on 1 distinct keys are submitted to HBase-R When the freshness ratio is 0 the input of the query is only the data cube Thus the cost of scanning the real-time data is 0 When the freshness ratio increases to 10 the cost of scanning the real-time data is around 1500 seconds because the cost of scanning the real-time table dominates the OLAP query As the freshness ratio increases the running time of and  and when it is not  and  We submit 800 million updates to the server each day and the percentage of keys updated is 223xed to 1 The data cube is refreshed at the beginning of each day and the OLAP query is submitted to the server at the end of the day Since the data are compacted after the data cube refresh the amount of data stored in the real-time table are almost the same at the same time of each day The processing time of and are thus almost constant In contrast when the compaction scheme is turned off HBase-R stores much more data and the cost of locally scanning these data becomes larger than the cost of shuf\224ing the data to MapReduce As a result the processing time of and increases over time and and a user speci\223ed timestamp still need to be scanned and 2 the amount of data shuf\224ed to mappers are roughly the same with different ratios Figure 10 depicts the effectiveness of our compaction scheme In this experiment we measure the processing time of the data cube slice query when the compaction scheme is applied  0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan Processing Time \(s I/Os \(X10 11  Percentage of Keys Updated CubeScan        Processing Time \(s Freshness Ratio CubeScan                                                                                                            50 0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan store\223le store\223les part memstore store\223les memstore store\223les IncreQuerying Baseline IncreQuerying Baseline-NC IncreQuerying-NC Baseline IncreQuerying Baseline-NC IncreQuerying-NC C Performance of OLTP 0    1200    2400    3600    4800    6000  1  5  10  15  20  25  0  0.8  1.6  2.4  3.2  4  IncreQueryScan        IncreQueryExe        I/Os estimated for IncreQuery                               I/Os estimated for  Baseline                 T T T T T T T T 


3000    6000    9000    12000  1  2  3  4  5  6  7  IncreQuerying                                   Baseline-NC                   IncreQuerying-NC                       51 002 Fig 10 Effectiveness of Compaction    a Throughput    b Latency Fig 11 Performance of OLTP Queries a large scale distributed environment However most existing works only focus on optimizing the OLAP queries and assume that the data scanned by MapReduce are unchanged during the execution of a MapReduce job In reality the real-time results from the most recently updated data are more meaningful for decision making In this paper we propose R-Store for supporting real-time OLAP on MapReduce R-Store leverages stable technology HBase and HStreaming and extends them to achieve high performance and scalability The storage system of R-Store adopts multi-version concurrency control to support real-time OLAP To reduce the storage requirement it periodically materializes the real-time data into a data cube and compacts the historical versions into one version During query processing the proposed adaptive incremental scan operation shuf\224es the real-time data to MapReduce ef\223ciently The data cube and the newly updated data are combined in MapReduce to return the real-time results In addition based on our proposed cost model the more ef\223cient query processing method is selected To evaluate the performance of R-Store we have conducted extensive experimental study using the TPCH data The experimental results show that our system can support real-time OLAP queries much more ef\223ciently than the baseline methods Though the performance of OLTP degrades slightly due to the competition for resources with OLAP the response time and throughput remain good and acceptable A CKNOWLEDGMENT The work described in this paper was in part supported by the Singapore Ministry of Education Grant No R-252000-454-112 under the epiC project and M.T 250 Ozsu\220s work was partially supported by Natural Sciences and Engineering Research Council NSERC of Canada We would also like to thank the anonymous reviewers for their insightful comments R EFERENCES  http://hbase.apache.or g  http://hstreaming.com  http://www comp.nus.edu.sg epic  M Athanassoulis S Chen A Ailamaki P  B Gibbons and R Stoica Masm ef\223cient online updates in data warehouses In  pages 865\205876 2011  Y  Cao C Chen F  Guo D Jiang Y  Lin B C Ooi H T  V o S W u and Q Xu Es2 A cloud data storage system for supporting both oltp and olap ICDE pages 291\205302 2011  S Ceri and J W idom Deri ving production rules for incremental vie w maintenance In  pages 577\205589 1991  T  Condie N Conw ay  P  Alv aro J M Hellerstein K Elmelee gy  and R Sears Mapreduce online In  pages 313\205328 2010  J Dean S Ghema w at and G Inc Mapreduce simpli\223ed data processing on large clusters In  pages 137\205150 2004  L Golab T  Johnson and V  Shkapen yuk Scheduling updates in a real-time stream warehouse ICDE pages 1207\2051210 2009  M Grund J Kr 250 uger H Plattner A Zeier P Cudre-Mauroux and S Madden Hyrise a main memory hybrid storage engine  4\(2 Nov 2010  A Gupta I S Mumick and V  S Subrahmanian Maintaining vie ws incrementally extended abstract In  pages 157\205166 1993  S H 264 eman M Zukowski N J Nes L Sidirourgos and P Boncz Positional update handling in column stores In  pages 543\205 554 2010  D Jiang G Chen B C Ooi and K.-L T an epic an e xtensible and scalable system for processing big data 2014  D Jiang B C Ooi L Shi and S W u The performance of mapreduce an in-depth study  3\(1-2 Sept 2010  D M Kane J Nelson and D P  W oodruf f An optimal algorithm for the distinct elements problem PODS 22010 pages 41\20552  A K emper  T  Neumann F  F  Informatik T  U Mnchen and DGarching Hyper A hybrid oltp&olap main memory database system based on virtual memory snapshots In  2011  T W  K uo Y T  Kao and C.-F  K uo T w o-v ersion based concurrenc y control and recovery in real-time client/server databases  52\(4 Apr 2003  K Y  Lee and M H Kim Ef 223cient incremental maintenance of data cubes In  pages 823\205833 2006  F  Li B C Ooi M T  250 Ozsu and S Wu Distributed data management using mapreduce In  2014  I S Mumick D Quass and B S Mumick Maintenance of data cubes and summary tables in a warehouse In  pages 100\205111 1997  A Nandi C Y u P  Bohannon and R Ramakrishnan Distrib uted cube materialization on holistic measures In  pages 183\205194 2011  L Neume yer  B Robbins A Nair  and A K esari S4 Distrib uted stream computing platform In  pages 170\205177 2010  C Olston B Reed U Sri v asta v a R K umar  and A T omkins Pig latin a not-so-foreign language for data processing In  pages 1099\2051110 2008  K Ser ge y and K Y ury  Applying map-reduce paradigm for parallel closed cube computation In  pages 62\20567 2009  M Stonebrak er  D J Abadi A Batkin X Chen M Cherniack M Ferreira E Lau A Lin S Madden E O\220Neil P O\220Neil A Rasin N Tran and S Zdonik C-store a column-oriented dbms In  pages 553\205564 2005  A Thusoo J S Sarma N Jain Z Shao P  Chakka S Anthon y  H Liu P Wyckoff and R Murthy Hive a warehousing solution over a mapreduce framework  2\(2 2009  P  V assiliadis and A Simitsis Near real time ETL In  volume 3 pages 1\20531 2009  C White Intelligent b usiness strate gies Real-time data w arehousing heats up  2012 SIGMOD VLDB NSDI OSDI SIGMOD SIGMOD Proc VLDB Endow In ICDE IEEE Trans Comput VLDB ACM Computing Survey SIGMOD ICDE ICDMW SIGMOD DBKDA VLDB PVLDB Annals of Information Systems DM Review 0    Processing Time \(s Time since the Creation of Data Cube \(day Baseline                  Updates Per Second \(K Number of Nodes Updates only                  Response Time for 1000 Updates\(s Number of Nodes Updates only                  0    20    40    60    80    100  10  20  30  40  50  60  70  Updates + OLAP                                    0    2    4    6    8    10  10  20  30  40  50  60  70  Updates + OLAP                                    Proc VLDB Endow 


  13    1  2   3   4   5   6   7   8  9  10  11   


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


