A New Improvement on Apriori Algorithm Lei Ji Baowen Zhang Jianhua Li Information Security Engineering School Shanghai Jiaotong University,Shanghai,China jerryl2l4@sjtu.edu.cn Abstract Efficiency has been concernedfor several years in the research of association rules mining In this paper based on the improvement on the classical Apriori algorithm a high-dimension oriented Apriori algorithm is proposed Unlike existed Apriori improvements our algorithm adopts a new method to reduce the redundant generation of sub-itemsets during 
pruning the candidate itemsets which can obtain higher efficiency of mining than that of the original algorithm when the dimension of data is high Theoretical proof and analysis are given for the rationality of our algorithm Experiments with datasets of KDD Cup 1999 validate our work 1 Introduction Since the first proposal of association rules mining by R Agrawal 1 2 many researches have been done to make mining frequent itemsets efficient and scalable As one of the focuses 
at present the Apriorilike algorithms find frequent itemsets based upon an iterative bottom-up approach to generate candidate itemsets But Apriori-based algorithms suffer from several deficiencies too many scans of the transaction database when seeking frequent itemsets too large amount of candidate itemsets generated unnecessarily the redundant generation of identical sub-itemsets and the repeated search for them in the database and so on In this paper the High-Dimension Oriented Apriori algorithm 
the HDO Apriori for short is proposed for miming the association rules in the high-dimensional data Based on the classical Apriori the new algorithm can cut down the redundant generation of identical sub-itemsets from candidate itemsets by means of pruning the candidate itemsets with the infrequent itemsets with lower dimension So it can obtain a higher efficiency than that of the original algorithm when the dimension of data is high In this paper the related work is presented 
in Section 2 One of the deficiencies of the classical Apriori algorithm to be tackled by the HDO Apriori algorithm is introduced in Section 3 Section 4 presents the principle and the process of the new algorithm Section 5 shows the experimental results and analysis Section 6 offers conclusion of this work 2 Related Work To improve the performance of the association rules mining different ways have been proposed One of them is updating the algorithms to reduce scans 
of the database or redundant generation of itemsets The existing approaches are either FP-growth 3 based or Apriori-like In the FP-growth algorithm without the high time cost in the generation of candidate itemsets the elevation of the efficiency is attributed to the condensation of data into the structure FP-tree which can facilitate to both the generation of association rules and the update of the mining result by fewer scans of the new data And there are also some 
improved algorithms based on it such as P-tree algorithm 4 However Apriori-like algorithms also put emphasis on reduction of candidate itemsets For example the improved Apriori algorithm based on Granular Computing 5 makes use of information granules to symbolize the different characteristics in candidate itemsets for indications which reflect whether the itemsets can be used to generate candidate itemsets with others or not The essence of this algorithm lies in reducing redundant candidate itemsets 
during their generation There are also other techniques applied to improving the Apriori algorithm such as hash-based techniques partitioning transaction reduction sampling and dynamic itemset counting 6-9 Different from other improved algorithms on Apriori in this paper the HDO algorithm adopts a new method to prune candidate itemsets with infrequent itemsets instead of frequent itemsets By the property of infrequent itemsets candidate itemsets can be 1-4244-0605-6/06/$20.00 C2006 IEEE 840 


pruned validly with the infrequent itemsets with lower dimension And the deficiency redundant generation of identical sub-itemsets from candidate itemsets of the original algorithm can be tackled So the efficiency of the association rules mining can be improved when the dimension of data is high 3 Problem Description Before investigating the classical Apriori algorithm to illustrate its deficiency two variables are to be declared Lk denotes the set of k-dimensional frequent itemsets Ck denotes the set of k-dimensional candidate itemsets From the detailed Apriori algorithm in 2 we can find the main parts of the Apriori Algorithm are composed of three essential procedures the function Join the function Prune and the validation of candidate itemsets The function Prune is used to delete the unnecessary candidate itemsets in advance so the time complexity can be reduced by fewer scans of the database when validating candidate itemsets But the deficiency just lies in this function because as one of its important steps it's time-consuming to verify whether all the k-subsets of a k+1 are frequent itemsets or not Actually in the original algorithm a large amount of k-dimensional subitemsets which include identical items but are contained in different k+1 can be both generated and verified repeatedly For example Let C4 be a 4-itemset and set C4  a,b,c,d a,b,c,e b,c,d,e a,b,d,e  where a,b,c,d,e are the instances of different items When the function Prune is called a,b,c is generated as the subsets of a,b,c,d and a,b,c,e respectively Moreover in this procedure it is also verified twice Hence during the process of miming the same verification recurs unnecessarily at a high frequency which seriously deteriorates the efficiency To tackle the deficiency a new opinion about deleting multiple infrequent itemsets during just one scan of candidate itemsets is proposed in this paper If we can obtain multiple infrequent supersets in Ck+1 of a certain infrequent itemset by a single scan of Ck the efficiency will be greatly promoted 4 Improved Algorithm 4.1 Principle of the Improvement The improvement is mainly in the function Prune based on the following property of infrequent itemset Property 1 Let Y 5I and X5c Y If the X is an infrequent itemset Yis also an infrequent itemset 10 This property enables the deletion of the infrequent itemsets from Ckj1 with the infrequent itemsets in Ck Some k+1 share the identical k-subsets so by the property multiple infrequent supersets in Ck+1 of a given infrequent itemset can be searched out by a single database query when the query condition is set to be the attributes of the infrequent itemset Compared with the large amount of repeated generation of subitemsets and only a single result from one database query in the function Prune of the classical Apriori algorithm the efficiency is highly promoted 4.2 Algorithm in Detail To implement the improvement a new variable Lk is introduced Lk denotes the complement of set Lk where the universal set is Ck And if an itemset is verified to be an infrequent itemset it will be inserted into Lk instead of being deleted Hence to indicate whether an itemsets is a frequent itemset or not a flag item is added to the itemset And correspondingly the procedures of validating Ck to generate Lk are changed first select the itemsets included in Ck but not in Lk then compare their supports to the minimum support And the function Prune is modified into the following procedures first select each of the itemsets in Lk sequentially and then use it as the query condition to search for their supersets in Ck+1 and finally insert the searching result into Lk+1l So the particular modifications are given as follows 1 The procedures in the main function of Apriori algorithm is modified as follows Improved Apriori Algorithm Input DB minsupport Output frequent itemsets Method begin Lo 0  Frequentltemsets 0 C1  i ie I k 1 Lk  ce Ck Isupport\(c while Ck#&0  Lk:={CE CkI support\(c minsupport A c E Lk  Lk C C E Ck A C Lk  update Lk 1-4244-0605-6/06/$20.00 C2006 IEEE 841 


as follows Function Prune Input Ck+i Lk Output Lk+l Method begin for or not is to be discussed in the following parts 4.3 Validity of the HDO Algorithm To prove the validity of the HDO algorithm equals to validate the completeness of screening Ck+1 with Lk that is before the supports of itemsets are compared with the minimum support Lk+1 has contained all the itemsets which are members of Ck Proposition 3 Given the candidate k+1 Ck+1 if c is an itemset in Nk+1 then there exists at least one k-subset of c in Lk Here is the proof of Proposition 2 Since Proposition 2 is true Lk+l  end Though the efficiency is highly improved the validity still needs to prove Whether Lk+1 screened out from Ck+1 by Lk has contained all the infrequent itemsets one k-subset of c either in Lk or beyond Ck Hence if c is can be deleted by the function Prune of the classical Apriori algorithm Hence the definition of Nk can be reduced If c is Ck  Apriori gen Lk k k 1 Frequentltemsets Frequentltemsets U Lk  return Frequentltemsets end 2 The function Prune is changed an itemset in Ck+1 then all the ksubsets of c an itemset in Nk 1 then all the k-subsets of c an itemsets in Nk 1 then there exists V c Lk for Vse Ck+j if 3 ksubsets\(s  an itemset in Nk+1 are members of Ck And by Proposition 1 the following statement an itemset in Nk 1 then there exists at least c insert s into a new variable Nk is defined to denote the set of all the kitemsets which ensures such are contained in Lk by the procedures of the function Prune of the classical Apriori algorithm the itemset can't be selected into Nk+1 It is contradictory to the definition of Nk+1 Thus this proposition holds And this proposition is are to be proved ahead Proposition 2 If c is we can infer by Proposition 2 If c is we can assert that before validating the candidate itemsets Lk+1 is equal to Nk+1 So the validity of the HDO algorithm is proved 1-4244-0605-6/06/$20.00 C2006 IEEE Proposition Given V se Ck 1 if c is a k-subset of s then V ce Ck Proof For k1I the proposition is evidently true For k=2 assume s is a 3-itemset generated from two frequent 2-itemsets x a and x b where x a b denote different items Since x a and x b are frequent itemsets hence a and b are frequent 1-itemsets Then a b is a third 2-subset of s and a b E C2 Thus the proposition is true For k>=3 given se Ck 1 assume s is generated from two frequent k-itemsets Xk 1 a and Xk 1 b where Xk 1 denotes a k-1 that doesn't contain the corresponding items including a or b Then because subsets of frequent itemsets are frequent itemsets Xk is also a frequent itemset Let c be an arbitrary k-subset of s For c Xk 1 a or Xk 1 b since ce Lk hence CE Ck For cX Xk1 a and c  Xkl b c must be Yk-2 a b whereyk-2 is a k-2 of Xk 1I Since Yk-2 a is a k-1 of Xk1 a then Yk-2 a is a frequent itemset Similarly Yk-2 b is also a frequent itemset Then by the definition of Ck k-2 a b e Ck hence V ce Ck Thus the proof is complete 842 are members of Lk Otherwise if all the k-subsets of one k-subset of c in Lk or more Since Proposition 3 holds by the procedures of the function Prune a proposition to be true Proposition 1 Not all the k-subsets of a certain itemset in Nk+1 a necessary condition for the proof of the validity Then to validate the completeness the following propositions can be deleted when the function Prune of the classical Apriori algorithm is performed So for the proof of the validity 


4.4 Estimation of the Performance Since the validity of the improved algorithm has been proved above in this part the theoretical efficiency of the performance will be estimated Let ts be the time cost of a single scan of the database Let t be the time cost of generating Ckj1 from Lk The variable mk is set to be the amount of itemsets in Ck without Nk the variable kj1 is set to be the amount of itemsets in Ck 1 and the variable nk is set to be the amount of itemsets in Lk A denotes the amount of the records in the database and N denotes the dimension of the data Set R nkllk Then the total runtime of the classical Apriori algorithm is N k+1 I n Z tS x mk  tC  Ik+l x 2 x ts xA and the total runtime of the HDO Apriori algorithm is N I Z ts x mk  tc  lk+lxtS x k 2 n=1l From the formulas the difference in the runtime between the two algorithms depends on N the dimension of the data and R the ratio of Lk to Ck If N is very large and on the condition that the growth rate of R is lower than that of k the dimension of itemsets the runtime can be highly reduced And also compared with the classical Apriori algorithm when other parameters are fixed the efficiency of the HDO Apriori algorithm can be improved relatively with a large value of R 5 Experiments and Analysis For different requirement two rounds of experiments are performed to compare the performance between the HDO Apriori algorithm and the classical Apriori algorithm Results and analysis are given in the following parts 5.1 Data Preparation All the experiments are performed on a 2.8GHz Intel Pentium PC with 512MB memory running on the Windows XP Professional OS Programs are coded in C on the platform of C Builder 6.0 The dataset is obtained from the KDD Cup 1999 Data KDD99 without the records of DOS Denial Of Service There is a large amount of recurrences in the records of DOS and the high recurrences can make the amount of candidate itemsets explosively increase as the geometric series So for cutting down the time consumptions during the experiments the elimination of this type of records is performed ahead Moreover for the convenience of data processing the character strings have been classified and replaced by digital symbols And some numerical attributes whose values cover a larger range but too few recurrences such as the length of the datagram have been clustered into less than 20 classes by the K-means algorithm After the preprocessing of the dataset 59436 records are obtained with 42 attributes And 12 of the attributes are selected for the association rules mining 5.2 Experiments and Analysis In the first round the experiment is designed to display the influence of N the dimension of the data During the experiment the minimum support is set to be 0.836 500/59436 and the dimension is set to 5,8,12 respectively The result is given as follows Table 1 The comparison of the runtime at different dimensions between the classical Apriori algorithm and the HDO Apriori algorithm Classical HDO Apriori Apriori N=5 minsupport 0.836 228s 251s N=8 minsupporh 0.836 151m 145m N=12 minsupporh 0.836 12.3h 11.5h We can contrast the changes in efficiency from the result when N is relatively small the classical Apriori algorithm still prevails over the HDO Apriori algorithm in the runtime but with the increase of N to some content the HDO Apriori algorithm tends to prevail and with the further increase of N the advantage of the HDO Apriori algorithm can be greatly enhanced Actually in this process the given minimum support and the large value of N cause the growth rate of R to be fairly lower than that of k so the runtime is reduced comparatively In the second round the experiment is designed to display the influence of R Because the growth of the ratio R results only from the reduction of the minimum support when N is given and the minimum support is the only parameter input manually the experiment is operated to display the influence of the minimum support instead During the experiment the dimension k is set to be 5 and the minimum support is set to 0.084 50/59436 0.252 150/59436 0.836 500/59436 respectively The result is given as follows 1-4244-0605-6/06/$20.00 C2006 IEEE 843 


a method to figure them out because they partly depend 0.836 844 can be divided into two sorts by support and there is can obtain higher efficiency when the dimension of data is high Meanwhile for different data can implement Reference 1 R Agrawal T Imielinski and A Swami Mining Association Rules Between Sets of Items in Large Databases Proceedings of the ACM SIGMOD Conference on Management of data 207-216 May 1993 2 R Agrawal and R Srikant Fast Algorithms for Mining Association Rules In Proc VLDB 1994 pp 487-499 3 C Liu H Lu J X Yu W Wang X Xiao AFOPT An Efficient Implementation of Pattern Growth Approach In SDM 2003 4 Hao Huang Xindong Wu and Richard Relue Association Analysis with One Scan of Databases University of Vermont Computer Science Technical Report CS-02-3 2002 5 Taorong Qiu Xiaoqing Chen Qing Liu Houkuan Huang An Algorithm of Association Rules Extracting Based on Granular Computing and Its Application May 2005 6 J.S Park M.S Chen and P.S Yu An Effective Hashbased Algorithm for Mining Association Rules SIGMOD'95 San Jose CA May 1995 7 A Savasere E Omiecinski and S Navathe An Efficient Algorithm for Mining Association Rules in Large Databases VLDB'95 432-443 Zurich Switzerland 8 H Toivonen Sampling Large Databases for Association Rules VLDB'96 134-145 Bombay India Sept 1996 9 S Brin R Motwani J D Ullman and S Tsur Dynamic Itemset Counting and Implication Rules for Market Basket Analysis SIGMOD'97 Tucson Arizona May 1997 10 Pei-qi Lin Zeng-zhi Li Yin-lung Zhao Effective Algorithm of Mining Frequent Itemsets for Association Rules Proceedmgs of the Third International Conference on Machine Leaming and Cybemetics Shanghai 26-29 August 2004 6 Conclusion In this paper the HDO Apriori algorithm is 1-4244-0605-6/06/$20.00 C IEEE Time s 700 2 626 600 Figure 1 The comparison of runtime at different minimum supports between the original Apriori and the improved Apriori From Figure 1 we can know that with the reduction of the minimum support the efficiency of the HDO Apriori algorithm _ I 26  300 4 3 31251 original Apriori 200 228 100 2.48 PlgnisPot*2 1 75 2 nrmusupport=0.252 3 splog minsupport*A*2 rnimusupp or 0.084 nursupp or on efficiency 5.3 Discussions For different datasets the proper values of minimum support to show the advantages of the HDO Apriori algorithm on the number of frequent itemsets Hence for the ratio R has increased just 5 500 2 561 2.48 441 I 5  improvzed Apriori 400 we have not found can change correspondingly from can also be obtained are different but on the internal characteristic of data For example assume some types of data can influence just a little a little the efficiency changes hardly On the contrary if the supports of data distribute averagely the influence of the minimum support can be evident Hence the HDO algorithm will be surely applicable to the latter types of data In contrast when the dimension of data is high the advantage of the HDO Apriori algorithm is evident no matter what the internal characteristic of data is That's why our algorithm is high-dimension oriented But if we want to precisely predict how much the efficiency can be improved at given data for the influence of the differences in data further research is also required proposed to update the classical Apriori algorithm Through pruning candidate itemsets by the infrequent itemsets with lower dimension the present algorithm can reduce the redundancy while generating subitemsets and verifying them in the database Validated by the experiments it we still need further research to find methods to estimate how much improvement the HDO Apriori algorithm a lower level to a higher level than that of the classic Apriori algorithm Verified by the experiments when the dimension of data is high the efficiency of the HDO algorithm is greatly improved And also if the minimum support is small enough the comparative improvement a large gap in the values of the supports between the two sorts of data So a small change of the minimum support within the gap 


5 uniform membership functions and its definitive interval is bounded within [0.05,0.15 With linguistic minimum support, the process of finding the set of large itemsets proceeds as illustrated next. Assume the linguistic minimum support value is given as  Low  First, this value is transformed into a fuzzy set of minimum support, namely \(0.05, 0.075, 0. l as shown in Figure 4. Second, the fuzzy weighted set of the given minimum support is calculated. Finally, the weighted support of each item or itemset is compared to the fuzzy weighted minimum support by fuzzy ranking. If the weighted support is equal to or greater than the weighted minimum support, then the corresponding itemset is considered large Fig. 3 Membership functions and base variablesof attribute ir poni  SUP Fig. 4 Membership functions of the minimum support for the fitness function 4.1. CHROMOSOME ENCODING Our target in using GAS is to cluster the values of quantitative attributes into fuzzy sets with respect to a given fitness evaluation criteria. For this purpose, each individual represents base values of membership functions of a quantitative attribute in the database. In our experiments, we used membership functions in triangular shape because it is in general the most appropriate shape To illustrate this, consider a quantitative attribute ik and assume it has 3 corresponding fuzzy sets Membership functions for, attribute ik and their base variables are shown in Figure 3. Each base variable takes finite values. For instance, the search space of base value bi lies between the minimum and maximum values of attribute ik, denoted mW4 The search intervals of all the base values and intersection point Rib of attribute ik are bf. : [min\(D D Rj, : [min\(Di b: : [min\(D b: : [ R , ,  m a W bl :[min\(D D So, based on the assumption of having 3 fuzzy sets per attribute, as it is the case with attribute ik, a chromosome consisting of the base lengths and the intersection points is represented as: b ~ b ~ R i , b i : b ~ b ~ ~ b , : R i ~ b ~ b ~  . . b ~ b ~ R i - b ~ b We use real-valued coding, where chromosomes are represented as floating point numbers and their genes are the real parameters  Ihese chromosomes form the input to the fitness function described in the next section 4.2. FITNESS EVALUATION The fitness function measures the goodness of an individual in a given population. It is one of the key issues to a successful GA; simply because the main task in a GA is to optimize a fitness function. Consequently the fitness function should be carefully set, by 4.3. SELEC~ION PROCESS During each generation, individuals with higher fitness values survive while those with lower fitness values are destroyed. In other words, individuals who are strong according to parent selection policy are candidates to form a new population. Parent selection mimics the survival of the best individuals in the given population Many selection procedures are currently in use However, Holland  s original fitness-proportionate selection is one of the simplest selection procedures [ l  I So, we used this selection policy in our experiments Let firness\(x,t r the fitness of individual x and the average fitness of the population during evolution phase t .  Then, the usage value of individual x as a parent is: rsr \(x , t xJ 1 After selecting chromosomes with respect to the evaluation function, genetic operators such as, crossover and mutation, are applied to these individuals 


and mutation, are applied to these individuals Crossover refers to information exchange between individuals in a population in order to produce new individuals. The idea behind the crossover operation utilized in our study is as follows. It takes as input 2 individuals, selects random points, and exchanges the subindividuals behind the selected points. Since the length of the chromosomes is long, the multi-point crossover strategy has been used with the crossover points determined randomly On the other hand, mutation means a random change in the information of an-individual. It is very important 113 for populations. It is an operation that defines a local or global variation in an individual. Mutation is traditionally performed in order to increase the diversity of the genetic information. Otherwise, after several generations, the diversity of the chromosomes decreases and some chunks of the chromosomes may end up being the same for all population members and the information they contain may not evolve further. A probability test determines whether a mutation will be carried out or not. ?he probability of mutation depends on the following condition average fitness of new generation &lt;average fitness of old generation Since the initial population can be a subset of all possible solutions, an important bit of each chromosome may be inverted, i.e., 0 appears as 1 or vice versa Crossover may not solve this and mutation is inevitable for the solution Finally, after generating each individual in the initial population, the executed GA includes the following steps Algorithm 4.1 \(Generating Association Rules 1 2 3 4 5 6 7 8 9 Using the given membership functions about item importance, transform each linguistic term, which reflects the importance of item i,, I l k  c m ,  into a fuzzy set wt of weights Specify population size N and generate initial chromosomes According to the current chromosome, transform the quantitative value f ,A of each item it in each transaction f j ,  1 5 j 5 n , into a fuzzy set f Calculate the fuzzy weighted support of each item fizzy set pair \( j k 9 f Compute the weighted fuzzy set of the given minimum support value as Find the large itemsets based on the weighted fuzzy set of the given minimum support value Evaluate each chromosome with respect to the already specifie.6 fitness function Perform selection, crossover and mutation If not end-test then go to Step 3 otherwise return the best chromosome WMinS=S.\(the weight of 10. Generate all possible association rules from each identified large weighted fuzzy itemset 11. From the rules generated in step 10, identify strong rules based on the specified fuzzy weighted confidence 12. From the rules identified in step 11, decide on interesting association rules by calculating the interestingness value for each strong rule Algorithm 4.1 employs CA to return interesting 


Algorithm 4.1 employs CA to return interesting association rules. The process considers fuzzy importance of items and involves fuzzy weighted support and confidence. This algorithm has been implemented and tested, the results are presented next in Section 5 5. EXPERIMENTAL RESULTS We used real-life dataset and conducted some experiments to assess the effectiveness of the CA-based fuzzy weighted mining approach presented in this paper All of the experiments were performed using a Pentium 111, 1.4GHz CPU with 512 MB of memory and running Windows 2000. As experimental data, we used lOOK transactions dataset taken from the adult data of United States census in 2000. In the experiments, we have used 6 quantitative attributes, each with three corresponding fuzzy sets. Finally, we have used three linguistic intervals for which random linguistic weights have been generated namely \(Important, Very-Important Ordinary Important Unimportant, Ordinary 0-1 and UI-0, respectively 500 2 400 300 &lt; 200 1 0  1 100 Very Low Medium High Very Low High Minimum Support Fig. 5 Number of large itemsets for linguistic terms of minimum support M i n .  cont A Fig. 6 Membership function for nunimum confidence The first experiment tests, for the above three different linguistic weight intervals, the correlation between expressing minimum support in linguistic terms and the number of large itemsets produced. The obtained results are reported in Figure 5, which shows that the number of large itemsets decreases as a function of the linguistic minimum support yI 100 d, 80 1 60 I 5 40 0 B 20 i o Very Low Medium High Very LOW High MininumConfidencc Fig. 7 Number of interesting rules for different linguistic terms of nunimumconfidence; nunimum support fixed as  nuddle   In the second experiment, the minimum support is fixed at the linguistic value  middle  and we tested, for the three linguistic weight intervals, the effect of using linguistic te rm to express minimum confidence, as shown in Figure 6, on the number of generated interesting 114 association rules. The achieved results are reported in Figure 7. The obtained results do meet our expectations i.e., more rules are generated for higher weights However, the number decreases, for all cases, as the linguistic confidence threshold increases 140 120 TI00 80 60 2 40 20 0 0 20 40 60 80 100 Number ofTransactions \(K 


Fig. 8 Runtime for GAS to find fuzzy sets for the three linguistic intervals The last experiment is dedicated to investigate the performance for the three linguistic intervals. In particular, we examined how the performance varies with the number of transactions. This is reflected in Figure 8 which shows the runtime as we increase the number of input records from 10K to 100K, for the three different cases. The results plotted in Figure 8 show that the method scales quite linearly for the census dataset used in the experiments 6. CONCLUSIONS In this paper, we proposed a clustering approach to solve the problem of interval partitioning in favor of the maximum number of large itemsets based on linguistic minimum support and confidence. The main achievement of the proposed approach is employing GAS to dynamically adjust and optimize membership functions which are essential in finding interesting weighted association rules from quantitative transactions, based on support and confidence specified as linguistic terms Compared to previous mining approaches, the proposed approach directly manages linguistic parameters, which are more natural and understandable to humans. Results of the experiments conducted on a real life census dataset demonstrated the effectiveness and applicability of the proposed approach r31 r41 r51 REFERENCES R. Agrawal, T. Imielinski and A. Swami  Mining association rules between sets of items in large databases  Proc. of ACM SIGMOD, pp.207-216, 1993 W.H. Au and K.C.C. Chan  An Effective Algorithm for Discovering Fuzzy Rules in Relational Databases  Proc C.H. Cai, et al  Mining Association Rules with Weighted Items  Proc. of IDEAS, pp.68-77, 1998 K.C.C. Chan and W.H. Au  Mining Fuzzy Association Rules  Proc. of ACM CIKM, pp.209-215, 1997 B.C. Chien, ZL. Lin and T.P. Hong  An Efficient Clustering Algorithm for Mining Fuzzy Quantitative Association Rules  IFSA World Congress and NAFIPS International Conference, Vo1.3, pp.1306-1311,2001 A.W.C. Fu, et al  Ending Fuzzy Sets for the Mining of Association Rules for Numerical Attributes  Proc. of the OfIEEE-FUZZ, pp.1314-1319,1998 115 International Symposium of Intelligent Data Engineering and Learning, pp.263-268, Oct. 1998 D.E. Goldberg, Genetic Algorithms in Search Optimization, and Machine Learning, Addison-Wesley Reading, MA, 1989 S. Guha, R. Rastogi and K. Shim  CURE: An Efficient Clustering Algorithm for Large Databases  Information Systems, Vo1.26, No.1, pp.35-58,2001 A. Gyenesei  A Fuzzy Approach for Mining Quantitative Association Rules  TUCS Technical Report No.336 2000 K. Hirota and W. Pedrycz  Linguistic Data Mining and Fuzzy Modelling  Proc. of IEEE-FUZZ, pp.1448-1496 1996 J.H. Holland, Adaptation in Natural and Artificial Systems, The MIT Press, Cambridge, MA, MIT Press edition, 1992. First edition: University of Michigan Press 1975 T.P. Hong, C.S. Kuo and S.C. Chi  A fuzzy data mining algorithm for quantitative values  Proc. of the International Conference on Knowledge-Based Intelligent Information Engineering Systems, pp.480483, 1999 T.P. Hong, C.S. Kuo and S.C. Chi  Mining Association 


Rules from Quantitative Data  Intelligent Data Analysis Vo1.3, pp.363-376, 1999 T. P. Hong, M. J. Chiang and S. L. Wang  Mining from Quantitative Data with Linguistic Minimum Supports and Confidences  Proc. of IEEE-FUZZ, pp. 494-499,2002 H. Ishibuchi, T. Nakashima and T. Yamamoto  Fuzzy Association Rules for Handling Continuous Attributes   Proc. of IEEE International Symposium on Industrial Electronics, pp. 1 1 8 - 12 1, 200 1 M. Kaya, R. Alhajj, F. Polat and A. Arslan  Efficient Automated Mining of Fuzzy Association Rules  Proc. of DEXA, 2002 C.M. Kuok, A.W. Fu and M.H. Wong  Mining fuzzy association rules in databases  SIGMOD Record, Vol. 17 No.1, pp.41-46, 1998 B. Lent, A. Swami and J. Widom  Clustering Association Rules  Proc. of IEEE ICDE!, pp.220-23 1 1997 R.J. Miller and Y. Yang  Association Rules over Interval Data  Proc. ofACM SIGMOD, pp.452-461, 1997 R. Ng and J. Han  Efficient and effective clustering methods for spatial data mining  Proc. of VLDB, 1994 W. Pedrycz  Fuzzy Sets Technology in Knowledge Discovery  Fuzzy Sets and Systems, 98, pp.279-290 1998 R. Srikant and R. Agrawal  Mining quantitative association rules in large relational tables  Proc. of ACM W. Wang and S.M. Bridges  Genetic Algorithm Optimization of Membership Functions for Mining Fuzzy Association Rules  Proc. of the International Conference on F m y  Theory &amp; Technology, pp. 13 1-134,2000 R.R. Yager  Fuzzy Summaries in Database Mining   Proc. of the Conference on Artificial Intelligence for Application, pp.265-269, 1995 S .  Yue, el al  Mining fuzzy association rules with weighted items  Proc. of IEEE SMC Conference pp.1906-1911,2000 L.A., Zadeh  Fuzzy Sets  Information and Control W. Zhang  Mining Fuzzy Quantitative Association Rules  Proc. of IEEE ICTRI, pp.99-102, 1999 SIGMOD, pp.1-12, 19 V01.8, pp.338-353, 1965 pre></body></html 


efficiency then AOFI. However utilization of fuzzy concept hierarchies provides more flexibility in reflecting expert knowledge and so allows better modeling of real-life dependencies among attribute values, which will lead to more satisfactory overall results for the induction process. The drawback of the computational cost may additionally decline when we notice that, in contrast to many other data mining algorithms, hierarchical induction algorithms need to run only once through the original \(i.e. massive dataset. We are continuing an investigation of computational costs of our approach for large datasets ACKNOWLEDGMENT Rafal Angryk would like to thank the Montana NASA EPSCoR Grant Consortium for sponsoring this research REFERENCES 1] J. Han , M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann, New York, NY 2000 2] J. Han, Y. Cai, and N. Cercone  Knowledge discovery in databases: An attribute-oriented approach  Proc. 18th Int. Conf. Ver y Large Data Bases, Vancouver, Canada, 1992, pp. 547-559 3] J. Han  Towards Efficient Induction Mechanisms in Database Systems  Theoretical Computing Science, 133, 1994, pp. 361-385 4] J. Han, Y. Fu  Discovery of Multiple-Level Association Rules from Large Databases  IEEE Trans.  on KD E, 11\(5 5] C.L. Carter, H.J. Hamilton  Efficient AttributeOriented Generalization for Knowledge Discovery from Large Databases  IEEE Trans. on KDE 10\(2 6] R.J. Hilderman, H.J. Hamilton, and N. Cercone  Data mining in large databases using domain generalization graphs  Journal of Intelligent Information Systems, 13\(3 7] C.-C. Hsu  Extending attribute-oriented induction algorithm for major values and numeric values   Expert Systems with Applications , 27, 2004, pp 187-202 8] D.H. Lee, M.H. Kim  Database summarization using fuzzy ISA hierarchies  IEEE Trans . on SMC - part B, 27\(1 9] K.-M. Lee  Mining generalized fuzzy quantitative association rules with fuzzy generalization hierarchies  20th NAFIPS Int'l Conf., Vancouver Canada, 2001, pp. 2977-2982 10] J. C. Cubero, J.M. Medina, O. Pons &amp; M.A. Vila  Data Summarization in Relational Databases through  Fuzzy Dependencies  Information Sciences, 121\(3-4 11] G. Raschia, N. Mouaddib  SAINTETIQ:a fuzzy set-based approach to database summarization   Fuzzy Sets and Systems, 129\(2 162 12] R. Angryk, F. Petry  Consistent fuzzy concept hierarchies for attribute generalization  Proceeding of the IASTED Int. Conf. on Information and Knowledge Sharing, Scottsdale AZ, USA, November 2003, pp. 158-163 13] Toxics Release Inventory \(TRI available EPA database hosted at http://www.epa.gov/tri/tridata/tri01/index.htm The 2005 IEEE International Conference on Fuzzy Systems790 pre></body></html 


the initial global candidate set would be similar to the set of global MFIs. As a result, during the global mining phase the communication and synchronization overhead is low  0 2 4 6 8 1 0 Number of Nodes Figure 5. Speedup of DMM 4.4.2 Sizeup For the sizeup test, we fixed the system to the 8-node con figuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local database sire at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remained the same as the local database size was increased. This is different from the speedup test, where the database repartitioning was per formed when the number of nodes was increased. The per formance of DMM is affected by the database repartitioning to some extent, although it is usually very small. During the sizeup test, the local mining result of DMM is not changed at all at each node The results shown in Figure 6 indicate that DMM has a very good sizeup property. Since increasing the size of local database did not affect the local mining result of DMM at each node, the total execution time increased just due to more disk U 0  and computation cost which scaled almost linearly with sizeup 5 Conclusions In this paper, we proposed a new parallel maximal fre quent itemset \(MFI Max-Miner \(DMM tems. DMM is a parallel version of Max-Miner, and it re quires low synchronization and communication overhead compared to other parallel algorithms. In DMM, Max Miner is applied on each database partition during the lo 0 45 90 135 180 225 270 Amwnt of Data per Node \(ME Figure 6. Sizeup of DMM cal mining phase. Only one synchronization is needed at thc end of this phase to construct thc initial global candi date set. In the global mining phase, a top-down search is performed on the candidate set, and a prefix tree is used to count the candidates with different length efficiently. Usu ally, just a few passes are needed to find all global maximal frequent itemsets. Thus, DMM largely reduces the number of synchronizations required between processing nodes Compared with Count Distribution, DMM shows a great improvement when some frequent itemscts are large \(i.e long patterns employed by DMM for efficient communication between nodes; and global support estimation, subset-infrequency based pruning, and superset-frequency based pruning are used to reduce the size of global candidate set. DMM has very good speedup and sizeup properties References I ]  R. Agrawal and R. Srikant  FdSt Algorithms for Mining As sociation Rules  Pmc. o f f h e  ZOrh VLDB Conf, 1994, pp 487499 2] R. Agrawal and I. C. Shafer  Parallel Mining of Association Rules  IEEE Trans. on Knowledge and Dura Engineering Vol. 8, No. 6, 1996, pp. 962-969 3] R. I. Bayardo  Efficient Mining Long Patlems from Databases  Proc. ofrhe ACM SIGMOD Inf  l Conf on Man ogemenr ofDara, 1998, pp. 85-91 4] S.  M. Chung and J. Yang  A Parallel Distributive Join Al gorithm for Cube-Connected Multiprocessors  IEEE Trans on Parallel and Disrribured Systems, Vol. 7, No. 2, 1996, pp 127-137 51 M. Snir, S. Otto. S. Huss-Lederman, D. Walker, and J. Don gana, MPI: The Complete Reference, The MIT Press, 1996 


gana, MPI: The Complete Reference, The MIT Press, 1996 6] R. Rymon  Search through Systematic Set Enumeralion   Pmc. of3rd Inr  l Con$ on Principles of Knowledge Repre sentation and Reasoning, 1992, pp. 539-550 507 pre></body></html 


sketch-index in answering aggregate queries. Then Section 5.2 studies the effect of approximating spatiotemporal data, while Section 5.3 presents preliminary results for mining association rules 5.1 Performance of sketch-indexes Due to the lack of real spatio-temporal datasets we generate synthetic data in a way similar to [SJLL00 TPS03] aiming at simulation of air traffic. We first adopt a real spatial dataset [Tiger] that contains 10k 2D points representing locations in the Long Beach county \(the data space is normalized to unit length on each dimension These points serve as the  airbases  At the initial timestamp 0, we generate 100k air planes, such that each plane \(i uniformly generated in [200,300], \(ii, iii destination that are two random different airbases, and iv  the velocity direction is determined by the orientation of the line segment connecting its source and destination airbases move continually according to their velocities. Once a plane reaches its destination, it flies towards another randomly selected also uniform in [0.02, 0.04 reports to its nearest airbase, or specifically, the database consists of tuples in the form &lt;time t, airbase b, plane p passenger # a&gt;, specifying that plane p with a passengers is closest to base b at time t A spatio-temporal count/sum query has two parameters the length qrlen of its query \(square number qtlen of timestamps covered by its interval. The actual extent of the window \(interval uniformly in the data space \(history, i.e., timestamps 0,100 air planes that report to airbases in qr during qt, while a sum query returns the sum of these planes  passengers. A workload consists of 100 queries with the same parameters qrlen and qtlen The disk page size is set to 1k in all cases \(the relatively small page size simulates situations where the database is much more voluminous specialized method for distinct spatio-temporal aggregation, we compare the sketch-index to the following relational approach that can be implemented in a DBMS. Specifically, we index the 4-tuple table lt;t,b,p,a&gt; using a B-tree on the time t column. Given a count query \(with window qr and interval qt SELECT distinct p FROM &lt;t,b,p,a&gt WHERE t?qt &amp; b contained in qr The performance of each method is measured as the average number of page accesses \(per query processing a workload. For the sketch-index, we also report the average \(relative Specifically, let acti and esti be the actual and estimated results of the i-th query in the workload; then the error equals \(1/100 set the number of bits in each sketch to 24, and vary the number of sketches The first experiment evaluates the space consumption Figure 5.1 shows the sketch index size as a function of the number of sketches used \(count- and sum-indexes have the same results more sketches are included, but is usually considerably smaller than the database size \(e.g., for 16 signatures, the size is only 40% the database size 0 20 40 60 80 


80 100 120 140 160 8 16 32 number of sketches size \(mega bytes database size Figure 5.1: Size comparison Next we demonstrate the superiority of the proposed sketch-pruning query algorithm, with respect to the na  ve one that applies only spatio-temporal predicates. Figure 5.2a illustrates the costs of both algorithms for countworkloads with qtlen=10 and various qrlen \(the index used in this case has 16 sketches also illustrate the performance of the relational method which, however, is clearly incomparable \(for qrlen?0.1, it is worse by an order of magnitude we omit this technique Sketch-pruning always outperforms na  ve \(e.g., eventually two times faster for qrlen=0.25 increases with qrlen, since queries returning larger results tend to set bits in the result sketch more quickly, thus enhancing the power of Heuristics 3.1 and 3.2. In Figure 5.2b, we compare the two methods by fixing qrlen to 0.15 and varying qtlen. Similar to the findings of [PTKZ02]4 both algorithms demonstrate  step-wise  growths in their costs, while sketch-pruning is again significantly faster The experiments with sum-workloads lead to the same observations, and therefore we evaluate sketch-indexes using sketch-pruning in the rest of the experiments 4 As explained in [PTKZ02], query processing accesses at most two paths from the root to the leaf level of each B-tree regardless the length of the query interval Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE sketch-pruning naive relational 0 100 200 300 400 500 600 700 800 900 0.05 0.1 0.15 0.2 0.25 number of disk accesses query rectangle length 300 0 100 200 400 500 600 1 5 10 15 20 number of disk accesses query interval length a qtlen=10 b qrlen=0.15 Figure 5.2: Superiority of sketch-pruning \(count As discussed in Section 2, a large number of sketches reduces the variance in the resulting estimate. To verify this, Figure 5.3a plots the count-workload error of indexes 


using 8-, 16-, and 32- sketches, as a function of qrlen qtlen=10 error \(below 10 it increases slowly with qrlen used, however, the error rate is much higher \(up to 30 and has serious fluctuation, indicating the prediction is not robust. The performance of 16-sketch is in between these two extremes, or specifically, its accuracy is reasonably high \(average error around 15 much less fluctuation than 8-sketch 32-sketch 16-sketch 8-sketch relative error 0 5 10 15 20 25 30 35 0.05 0.1 0.15 0.2 0.25 query rectangle length relative error 0 5 10 15 20 25 30 35 1 5 10 15 20 query interval length a qtlen=10, count b qrlen=0.15, count relative error query rectangle length 0 5 10 15 20 25 0.05 0.1 0.15 0.2 0.25 relative error query interval length 0 5 10 15 20 25 30 1 5 10 15 20 c qtlen=10, sum d qrlen=0.15, sum Figure 5.3: Accuracy of the approximate results The same phenomena are confirmed in Figures 5.3b where we fix qrlen to 0.15 and vary qtlen 5.3d \(results for sum-workloads number of sketches improves the estimation accuracy, it also leads to higher space requirements \(as shown in Figure 5.1 Figures 5.4a and 5.4b show the number of disk accesses for the settings of Figures 5.3a and 5.3b. All indexes have almost the same behavior, while the 32-sketch is clearly more expensive than the other two indexes. The interesting observation is that 8- and 16-sketches have 


interesting observation is that 8- and 16-sketches have almost the same overhead due to the similar heights of their B-trees. Since the diagrams for sum-workloads illustrate \(almost avoid redundancy 32-sketch 16-sketch 8-sketch number of disk accesses query rectangle length 0 50 100 150 200 250 300 350 400 0.05 0.1 0.15 0.2 0.25 number of disk accesses query interval length 0 50 100 150 200 250 300 350 1 5 10 15 20 a qtlen=10 b qrlen=0.15 Figure 5.4: Costs of indexes with various signatures Summary: The sketch index constitutes an effective method for approximate spatio-temporal \(distinct aggregate processing. Particularly, the best tradeoff between space, query time, and estimation accuracy obtained by 16 sketches, which leads to size around 40 the database, fast response time \(an order of magnitude faster than the relational method average relative error 5.2 Approximating spatio-temporal data We proceed to study the efficiency of using sketches to approximate spatio-temporal data \(proposed in Section 4.1 as in the last section, except that at each timestamp all airplanes report their locations to a central server \(instead of their respective nearest bases maintains a table in the form &lt;time t, plane p, x, y&gt;, where x,y with parameters qrlen and qtlen distinct planes satisfying the spatial and temporal conditions. For comparison, we index the table using a 3D R*-tree on the columns time, x, and y. Given a query, this tree facilitates the retrieval of all qualifying tuples, after which a post-processing step is performed to obtain the Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE number of distinct planes \(in the sequel, we refer to this method as 3DR method introduces a regular res  res grid of the data space, where the resolution res is a parameter. We adopt 16 sketches because, as mentioned earlier, this number gives the best overall performance Figure 5.5 compares the sizes of the resulting sketch indexes \(obtained with resolutions res=25, 50, 100 the database size. In all cases, we achieve high compression rate \(e.g., the rate is 25% for res=25 evaluate the query efficiency, we first set the resolution to the median value 50, and use the sketch index to answer workloads with various qrlen \(qtlen=10 


workloads with various qrlen \(qtlen=10 size \(mega bytes database size 0 20 40 60 80 100 120 140 160 25 50 100 resolution Figure 5.5: Size reduction Figure 5.6a shows the query costs \(together with the error in each case method. The sketch index is faster than 3DR by an order of magnitude \(note that the vertical axis is in logarithmic scale around 15% error observations using workloads with different qtlen Finally, we examine the effect of resolution res using a workload with qrlen=0.15 and qtlen=10. As shown in Figure 5.6c, larger res incurs higher query overhead, but improves the estimation accuracy Summary: The proposed sketch method can be used to efficiently approximate spatio-temporal data for aggregate processing. It consumes significantly smaller space, and answers a query almost in real-time with low error 3D Rsketch number of disk accesses query rectangle length 1 10 100 1k 10k 0.05 0.1 0.15 0.2 0.25 16 14% 15 15% 13 relative error number of disk accesses query interval length 1 10 100 1k 10k 1 5 10 15 20 16 15% 15% 12% 11 relative error a qtlen=10, res=25 b qrlen=0.15, res=25 0 500 1000 1500 2000 2500 25 50 100 number of disk accesses resolution 20% 15% 14 relative error c qrlen=0.15, qtlen=10 


c qrlen=0.15, qtlen=10 Figure 5.6: Query efficiency \(costs and error 5.3 Mining association rules To evaluate the proposed algorithm for mining spatiotemporal association rules, we first artificially formulate 1000 association rules in the form \(r1,T,90 with 90% confidence i randomly picked from 10k ones, \(ii in at most one rule, and \(iii Then, at each of the following 100 timestamps, we assign 100k objects to the 10k regions following these rules. We execute our algorithms \(using 16 sketches these rules, and measure \(i  correct  rules divided by the total number of discovered rules, and \(ii successfully mined Figures 5.7a and 5.7b illustrate the precision and recall as a function of T respectively. Our algorithm has good precision \(close to 90 majority of the rules discovered are correct. The recall however, is relatively low for short T, but gradually increases \(90% for T=25 evaluated in the previous sections, the estimation error decreases as the query result becomes larger \(i.e., the case for higher T 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 precision HT 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 recall HT a b Figure 5.7: Efficiency of the mining algorithm Summary: The preliminary results justify the usefulness of our mining algorithm, whose efficiency improves as T increases Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 6. Conclusions While efficient aggregation is the objective of most spatio-temporal applications in practice, the existing solutions either incur prohibitive space consumption and query time, or are not able to return useful aggregate results due to the distinct counting problem. In this paper we propose the sketch index that integrates traditional approximate counting techniques with spatio-temporal indexes. Sketch indexes use a highly optimized query algorithm resulting in both smaller database size and faster query time. Our experiments show that while a sketch index consumes only a fraction of the space required for a conventional database, it can process 


required for a conventional database, it can process queries an order of magnitude faster with average relative error less than 15 While we chose to use FM sketches, our methodology can leverage any sketches allowing union operations Comparing the efficiency of different sketches constitutes a direction for future work, as well as further investigation of more sophisticated algorithms for mining association rules. For example, heuristics similar to those used for searching sketch indexes may be applied to improve the brute-force implementation ACKNOWLEDGEMENTS Yufei Tao and Dimitris Papadias were supported by grant HKUST 6197/02E from Hong Kong RGC. George Kollios, Jeffrey Considine and were Feifei Li supported by NSF CAREER IIS-0133825 and NSF IIS-0308213 grants References BKSS90] Beckmann, N., Kriegel, H., Schneider, R Seeger, B. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD, 1990 CDD+01] Chaudhuri, S., Das, G., Datar, M., Motwani R., Narasayya, V. Overcoming Limitations of Sampling for Aggregation Queries. ICDE 2001 CLKB04] Jeffrey Considine, Feifei Li, George Kollios John Byers. Approximate aggregation techniques for sensor databases. ICDE, 2004 CR94] Chen, C., Roussopoulos, N. Adaptive Selectivity Estimation Using Query Feedback. SIGMOD, 1994 FM85] Flajolet, P., Martin, G. Probabilistic Counting Algorithms for Data Base Applications JCSS, 32\(2 G84] Guttman, A. R-Trees: A Dynamic Index Structure for Spatial Searching. SIGMOD 1984 GAA03] Govindarajan, S., Agarwal, P., Arge, L. CRBTree: An Efficient Indexing Scheme for Range Aggregate Queries. ICDT, 2003 GGR03] Ganguly, S., Garofalakis, M., Rastogi, R Processing Set Expressions Over Continuous Update Streams. SIGMOD, 2003 HHW97] Hellerstein, J., Haas, P., Wang, H. Online Aggregation. SIGMOD, 1997 JL99] Jurgens, M., Lenz, H. PISA: Performance Models for Index Structures with and without Aggregated Data. SSDBM, 1999 LM01] Lazaridis, I., Mehrotra, S. Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure. SIGMOD 2001 PGF02] Palmer, C., Gibbons, P., Faloutsos, C. ANF A Fast and Scalable Tool for Data Mining in Massive Graphs. SIGKDD, 2002 PKZT01] Papadias,  D., Kalnis, P.,  Zhang, J., Tao, Y Efficient OLAP Operations in Spatial Data Warehouses. SSTD, 2001 PTKZ02] Papadias, D., Tao, Y., Kalnis, P., Zhang, J Indexing Spatio-Temporal Data Warehouses ICDE, 2002 SJLL00] Saltenis, S., Jensen, C., Leutenegger, S Lopez, M.A. Indexing the Positions of Continuously Moving Objects. SIGMOD 2000 SRF87] Sellis, T., Roussopoulos, N., Faloutsos, C The R+-tree: A Dynamic Index for MultiDimensional Objects. VLDB, 1987 TGIK02] Thaper, N., Guha, S., Indyk, P., Koudas, N Dynamic Multidimensional Histograms 


SIGMOD, 2002 Tiger] www.census.gov/geo/www/tiger TPS03] Tao, Y., Papadias, D., Sun, J. The TPR*Tree: An Optimized Spatio-Temporal Access Method for Predictive Queries. VLDB, 2003 TPZ02] Tao, Y., Papadias, D., Zhang, J. Aggregate Processing of Planar Points. EDBT, 2002 TSP03] Tao, Y., Sun, J., Papadias, D. Analysis of Predictive Spatio-Temporal Queries. TODS 28\(4 ZMT+01] Zhang, D., Markowetz, A., Tsotras, V Gunopulos, D., Seeger, B. Efficient Computation of Temporal Aggregates with Range Predicates. PODS, 2001 ZTG02] Zhang, D., Tsotras, V., Gunopulos, D Efficient Aggregation over Objects with Extent PODS, 2002 Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE pre></body></html 


