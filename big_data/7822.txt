Learning on Big Graph Label Inference and Regularization with Anchor Hierarchy Meng Wang Member IEEE  Weijie Fu Shijie Hao Hengchang Liu and Xindong Wu Fellow IEEE Abstract Several models have been proposed to cope with the rapidly increasing size of data such as Anchor Graph Regularization AGR The AGR approach signiﬁcantly accelerates graph-based learning by exploring a set of anchors However when a dataset becomes much larger AGR still faces a big graph which brings dramatically increasing computational costs To overcome this issue we propose a novel Hierarchical Anchor Graph Regularization HAGR approach by exploring multiple-layer anchors with a pyramid-style structure In HAGR the labels of datapoints are inferred from the coarsest anchors layer by layer in a coarse-to-ﬁne manner The label smoothness regularization is performed on all datapoints and we demonstrate that the optimization process only involves a small-size reduced Laplacian matrix We also introduce a fast approach to construct our hierarchical anchor graph based on an approximate nearest neighbor search technique Experiments on million-scale datasets demonstrate the effectiveness and efﬁciency of the proposed HAGR approach over existing methods Results show that the HAGR approach is even able to achieve a good performance within 3 minutes in an 8-million-example classiﬁcation task Index Terms Semi-supervised learning graph-based learning label smoothness regularization label inference  1I NTRODUCTION S EMI SUPERVISED learning SSL methods which exploit the prior knowledge from unlabeled data to improve classiﬁcation performance have been widely used to handle datasets where only a portion of data are labeled Most of these methods are developed based on the cluster assumption or the manifold assumption 1 The former supposes that nearby points are likely to have the same label while the latter assumes that each class lies on a separate low-dimensional manifold embedded in a higher dimensional space In recent years various semi-supervised learning methods have been developed under these assumptions including mixture methods co-training  semi-supervised support vector machines 18 and graph-based methods In this paper we focus on the family of graph-based semi-supervised learning GSSL methods where the label dependencies among datapoints are captured by a weighted graph These methods rst construct adjacency relationships between all datapoints and then propagate labels from labeled data to unlabeled data with the above adjacency edges Since many forms of real-world data such as handwritten digits faces medical data and speech data exhibit such a kind of intrinsic graph structure GSSL has been applied to many applications and achieves satisfying performance 41 42 Meanwhile this roadmap can be extended to building other advanced graph models such as hypergraph 48 and multi-graph 7 37 to describe more complex relationships among real-world entities like multimodal media contents 13 27 In spite of the progress made in recent years most GSSL methods remain challenging mainly due to their cubic complexity in optimization Facing the ever increasing data size these approaches tend to be inefﬁcient in dealing with large-scale datasets To address this issue recent works seek to employ anchors in scaling up graph-based learning models such as Anchor Graph Regularization AGR and Efﬁcient Anchor Graph Regularization EAGR for simplicity we call both of them AGR without differentiation except in comparative experiments In these models anchors refer to the points that roughly cover the data distribution AGR then builds an anchor graph to model the inter-layer adjacency between the data layer and the anchor layer For clarity a few inter-layer edges in the anchor graph built on a two-moon dataset are shown in Fig 1a The efﬁciency of these approaches lies in two steps 1 they build the intra-layer adjacency relationships based on the anchors instead of computing all pair-wise adjacencies between the datapoints in an exhaustive way and 2 they infer the labels of datapoints from the anchors based on their inter-layer adjacency relationships As the number of anchors can be much smaller than datapoints both the graph construction and the learning process become much faster than those in traditional graph-based approaches However to obtain a reasonable accuracy anchors need to be sufﬁciently dense in order to build effective adjacency relationships 001 M Wang W Fu and S Hao are with the School of Computer Science and Information Engineering Hefei University of Technology Hefei 230009 China E-mail eric.mengwang fwj.edu hfut.hsj}@gmail.com 001 H Liu is with the Department of Computer Science University of Science and Technology of China Hefei Shi 230022 China E-mail hcliu@ustc.edu.cn 001 X Wu is with the School of Computer Science and Information Engineering Hefei University of Technology Hefei 230009 China and the School of Computing and Informatics University of Louisiana at Lafayette Lafayette LA 70504 E-mail xwu@louisiana.edu Manuscript received 4 Sept 2016 revised 8 Nov 2016 accepted 7 Jan 2017 Date of publication 17 Jan 2017 date of current version 30 Mar 2017 Recommended for acceptance by H.T Shen For information on obtaining reprints of this article please send e-mail to reprints@ieee.org and reference the Digital Object Identiﬁer below Digital Object Identiﬁer no 10.1109/TKDE.2017.2654445 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING VOL 29 NO 5 MAY 2017 1101 1041-4347 001 2017 IEEE Personal use is permitted but republication/redistribution requires IEEE permission See http://www.ieee.org/publications_standards/publications/rights/index.html for more information 


Therefore when dealing with extremely large-scale datasets the computational costs of existing anchor-graph-based approaches will dramatically increase and become even practically intractable One possible way is to use only a small number of anchors but too sparse anchors will degrade performance as the label inference and the label smoothness regularization cannot be performed reliably To address this issue in this paper we introduce a novel hierarchical anchor graph and develop a scalable SSL approach named Hierarchical Anchor Graph Regularization HAGR Different from the existing graphs our proposed graph contains multiple layers of anchors in a pyramid-style structure It consists of a layer of original datapoints and multiple layers of anchors that describe the original datapoints from ne to coarse as illustrated in Fig 1b Based on the proposed graph model we infer the labels of datapoints from the coarsest anchors layer by layer based on inter-layer adjacency relationships Although the label smoothness regularization is performed on all datapoints we demonstrate that the optimization only involves a reduced Laplacian matrix with the size of the coarsest anchor layer Therefore the HAGR approach overcomes the limitation of anchorgraph-based approaches and well compromises classiﬁcation performance and computational efﬁciency The HAGR approach is quite exible as we can set different layers of anchors according to the scales of classiﬁcation tasks We show that it will degrade to AGR when there is only one anchor layer In order to further improve the efﬁciency of the construction of this hierarchical anchor graph we also investigate an Approximate Nearest Neighbor Search ANNS technique to build inter-layer adjacency relationships fastly The main contributions of our work are as follows 1 We make a deep analysis on the existing anchor graph and point out its limitations in dealing with large-scale datasets That is AGR faces either an intractable computational cost with dense anchors or a degraded performance with sparse anchors 2 We propose to build hierarchical anchor graph with a pyramid structure and develop a scalable classiﬁer based on it The labels of datapoints are inferred from the coarsest anchors layer by layer and the optimization only involves a small-size reduced Laplacian matrix The proposed approach overcomes the limitations of AGR and is able to efﬁciently accomplish large-scale classiﬁcation with a good performance detailed computational costs will be shown in Section 4.3 3 We introduce a fast hierarchical anchor graph construction process in which the ANNS technique is employed to build inter-layer adjacency relationships The rest of this paper is organized as follows In Section 2 we brieﬂy introduce related work on the graph-based learning In Section 3 we analyze the traditional AGR approach and its limitations The proposed approach is described in Section 4 In Section 5 we validate our method and make comparisons with other approaches on large-scale datasets We also evaluate different graph structures of HAGR to test its exibility as well as robustness We nally conclude this paper in Section 6 2R ELATED W ORK Zhu et al rst introduced the formulation of learning problem based on a Gaussian random eld and analyzed its intimate connections with random walks and spectral graph theory Zhou et al subsequently suggested an effective algorithm to obtain the solution of a classiﬁcation function which is sufﬁciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled datapoints Later Zhu et al developed an improved nonparametric kernel approach by incorporating order constraints during the convex optimization in learning Zelnik et al introduced a local scale in computing the afﬁnity between each pair of datapoints for the weighted edge Meanwhile inspired by locally linear embedding many works that focus on improving the weight estimation of the graph via sparse representation are proposed For example Wang et al presented a linear neighborhood model for label propagation which assumes Fig 1 An illustrative example of anchor graph consisting of 5,000 datapoints and an anchor layer with 250 anchors and hierarchical anchor graph consisting of 5,000 datapoints and multiple anchor layers with 1,000 500 250 and 100 anchors respectively For simplify only a tiny fra ction of inter-layer edges are shown 1102 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING VOL 29 NO 5 MAY 2017 


that each datapoint can be linearly reconstructed from its neighborhoods with l 2 minimization Similarly Cheng et al 6 pr op os ed a we ig ht es ti ma ti o n me th o d wh ic h op ti mi ze s the sparse reconstruction coefﬁcients on a l 1 graph Since selecting local neighbors may lead to disjoint components and incorrect neighbors in graph Tian et al 3 ad vo ca te d learning a nonnegative low-rank graph to capture global linear neighborhoods under the assumption that each datapoint can be linearly reconstructed from weighted combinations of its direct neighbors and reachable indirect neighbors The above graph-based approaches show impressive performances in various applications However they are not sufﬁciently scalable which imposes limitations in handling larger datasets With the rapid increase in data size researchers have paid more attention to designing novel approaches to reduce the computational cost of graph-based learning Tsang et al formulated a sparsiﬁed manifold regularizer as a center-constrained minimum enclosing ball problem to produce sparse solutions with lower time and space complexities Wang et al proposed a multiple random divide-and-conquer approach to construct an approximated neighborhood graph and presented a neighborhood propagation scheme to further enhance the accuracy Chen et al presented a method to combine both the original kernel and the graph kernel for scalable manifold regularization More recent works seek to employ anchors in scaling up the graph model Different from the conventional graphs the anchor-based approaches build the adjacency relationships between original datapoints based on anchors Zhang et al 46 rst suggested using a set of anchors to perform an effective low-rank approximation of the data manifold and to span a model suffering the minimum information loss Liu et al rst presented the anchor graph model and introduced it into the graph-based learning tasks Wang et al subsequently proposed an improved algorithm which shows better performance and computational efﬁciency Compared with the conventional graphs these anchor-graph-based approaches can largely reduce the complexity in graph construction and have been widely used in many applications 20 25 38 However the two-layer anchor graph structure is still limited for processing large-scale learning tasks which will be analyzed in detail in the following 3A NCHOR G RAPH B ASED L EARNING In this section we rst present a brief description of the anchor-graph-based approach and then give a detailed analysis on its limitations For convenience some important notations used throughout the paper and their explanations are listed in Table 1 3.1 Formulations of AGR We consider a standard multiclass SSL problem Given a dataset Xºf x 1  x 2    x n g2 R d 003 n with the rst l samples being labeled from c distinct classes anchor-graph-based methods start with clustering a set of representative anchors Uºf u 1  u 2    u m 1 g2 R d 003 m 1  m 1 004 n   which share the same feature space with original datapoints To be consistent with the notations in HAGR here we let L 0 denote the layer of datapoints and L 1 denote the layer of anchors as illustrated in Fig 1a Different from the conventional graph denoted by an n 003 n adjacency matrix an anchor graph G is represented by an n 003 m 1 nonnegative matrix Z 0  1  which models inter-layer adjacency relationships between points in L 0 and L 1  Speciﬁcally the entries in each row of Z 0  1 are the weights between datapoint x i and its k nearest anchors which can be deﬁned by NadarayaNatson kernel regression Z 0  1 is  K s  x i  u s  P s 0 2h i i K s  x i  u s 0  8 s 2h i i   1  TABLE 1 Notations and Deﬁnitions Notation Deﬁnition GºfX  U  Eg An anchor graph or hierarchical anchor graph where X and U indicate datapoints and anchors respectively and E indicates the sets of inter-layer adjacency edges between different sets of points h The number of anchor layers L b The b th layer in the pyramidal graph structure where L 0 is the layer of original data and L b  b 005 1  denotes the b th anchor layer m b The number of points in L b  Z a;b The inter-layer adjacency matrix between L a and L b  By default we have b  a  1 for estimating the adjacencies between neighboring layers Z a;b is The inter-layer adjacency weight between point i in L a and point s in L b  W The intra-layer adjacency matrix used in label smoothness regularization L b The diagonal matrix of the degrees of the anchors in L b  A The soft label matrix of anchors F The soft label matrix of datapoints Y L The class indicator matrix on labeled datapoints L The reduced Laplacian matrix in the anchor graph or hierarchical anchor graph n The number of datapoints c The number of classes in the dataset l The number of labeled datapoints in the dataset Z H The accumulated inter-layer adjacency matrix in the hierarchical anchor graph WANG ET AL LEARNING ON BIG GRAPH LABEL INFERENCE AND REGULARIZATION WITH ANCHOR HIERARCHY 1103 


where the notation h i i\006 1 m 1 007 is the indices of the k closest anchors of x i  Given the labels of anchors and the above inter-layer adjacency the label of each datapoint can be estimated as a weighted average of them i.e f  x i  X m 1 s  1 Z 0  1 is f  u s    2  where f is a prediction function that assigns each point a soft label Then to smooth these inferred labels one can also construct an intra-layer adjacency matrix of datapoints based on inter-layer adjacency relationships W  Z 0  1  L 1  010 1 Z 0  1 T 2 R n 003 n   3  where the diagonal matrix L 1 is deﬁned as L 1 ss  P n i  1 Z 0  1 is  From Eq 3 we can see that W ij  0 means the two datapoints share at least one common anchor and otherwise W ij  0  It is likely that datapoints sharing common anchors would have similar labels Let Y L  y T 1    y T l 007 T 2 R l 003 c denote the class indicator matrix on labeled datapoints where y ir  1 if x i belongs to class r  and y ir  0 otherwise Let A  a 1 T  a 2 T    a m 1 T 007 T 2 R m 1 003 c denote the soft label matrix of the anchors in L 1  To deal with the standard multi-class SSL problem Anchor Graph Regularization is formulated by minimizing Q A as Q A  X l i  1 k Z 0  1 i 011 A 010 y i k 2  002 2 X n i;j  1 W ij k Z 0  1 i 011 A 010 Z 0  1 j 011 A k 2   4  where 002 0 is the trade-off parameter balancing different terms and Z 0  1 i 011 is the i th row of Z 0  1  From the above equation we can see that the labels of datapoints in both the tting and smoothness terms are inferred from the anchors Note that there are other alternative methods for manifold regularization such as 5 As it is not the main point of this paper we simply follow the idea of AGR Meanwhile for the degree of each datapoint we have D ii  P j W ij  P sj Z 0  1 is  L 1 ss  010 1 Z 0  1 js  P s Z 0  1 is  1  Therefore we obtain the diagonal matrix D  I  and Eq 4 is reformulated into a matrix form as Q A k Z 0  1 L A 010 Y L k 2 F  002 tr  A T Z 0  1 T  I 010 W  Z 0  1 A  k Z 0  1 L A 010 Y L k 2 F  002 tr  A T e LA    5  where Z 0  1 L is the labeled part of Z 0  1  and e L  Z 0  1 T Z 0  1 010  Z 0  1 T Z 0  1  L 1  010 1  Z 0  1 T Z 0  1 2 R m 1 003 m 1 is the reduced Laplacian matrix in AGR Differentiating Q A with respect to A and setting it to zero we can obtain an optimal solution in the closed-form A  Z 0  1 L T Z 0  1 L  002 e L  010 1 Z 0  1 L T Y L   6  Clearly this matrix inversion takes a time cost of O  m 3 1   Finally AGR employs the solved labels associated with the anchors in L 1 to infer the hard label of any unlabeled datapoint in L 0 b y i  argmax r 2f 1   c g Z 0  1 i 011 003 A 011 r b r i  l  1   n  7  where A 011 r is the r th column of A  and b r  1 T Z 0  1 A 011 r is the normalization factor which balances skewed class distributions 3.2 Limitation of AGR A Dilemma Compared with the traditional graph models anchor graph additionally introduces an anchor set into the graph construction As the number of these anchors can be much smaller than datapoints both the graph construction and the optimization especially the inverse computation in Eq 6 become much faster AGR thus becomes a popular tool to handle relatively large datasets However AGR still has limitation in dealing with extremely large-scale datasets Speciﬁcally it faces a dilemma between performance and computational cost If we only employ a relatively small number of anchors the performance of the AGR approaches will degrade as label smoothness regularization and label inference cannot be performed effectively For the label smoothness regularization it will introduce many noisy intra-layer edges between dissimilar datapoints by Eq 3 as they tend to share a distant anchor For the label inference it will lead to unreliable integration of label information from k nearest anchors as inter-layer adjacencies are estimated with too sparse anchors that can be far away from the datapoint Therefore to obtain a reasonable accuracy anchors in the AGR approaches need to be sufﬁciently dense to build effective adjacency relationships According to Eq 6 it results in a dramatically increase of computational cost which makes the approach practically intractable 4H IERARCHICAL A NCHOR G RAPH R EGULARIZATION We rst introduce the deﬁnition of hierarchical anchor graph and how we use this graph to build a scalable GSSL approach Then we present the efﬁcient graph construction based on ANNS followed with the analysis on time complexity and other discussions 4.1 Label Inference and Regularization in HAGR For graph-based SSL to obtain good performances in largescale classiﬁcation tasks it always requires an effective smoothness term for regularization and an efﬁcient solution for model optimization To build such a scalable graphbased classiﬁer we extend the anchor graph to a pyramidlike structure and propose a novel graph model called hierarchical anchor graph For clarity an illustrative example of the hierarchical anchor graph is shown in Fig 1b Deﬁnition 1 Hierarchical Anchor Graph GºfX  U  Eg is a multiple-layer pyramidal graph where X indicates the data set U indicates the collection of anchor sets and E indicates the collection of the adjacency matrices of inter-layer edges between neighboring layers Suppose the original datapoints X2 R d 003 n locate in the bottom layer  L 0  of the pyramid The remaining layers  L b b  1   h  are all composed of multiple anchor sets U i s from ne to coarse where the size of 1104 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING VOL 29 NO 5 MAY 2017 


U b 2 R d 003 m b  b  1   h  is gradually reduced namely m 1  011\011\011 m h  All the layers are linked up to a complete graph with h sets of inter-layer adjacency edges represented by Eºf Z 0  1    Z h 010 1 h g2 R f n 003 m 1   m h 010 1 003 m h g  in which Z b 010 1 b denotes the adjacencies between points in L b 010 1 and L b  Based on the hierarchical anchor graph we can construct a scalable graph-based classiﬁer for multi-class classiﬁcation tasks of which the following two key parts are presented in detail 1 inter-layer adjacency relationships for label inference which is designed to reduce the number of parameters and make the learning more efﬁcient and 2 intra-layer adjacency relationships for label smoothness which is to build an effective regularization and ensure the learning accuracy We rst pay attention to the former one Based on the collection of the inter-layer adjacency relationships we propose to infer the labels of datapoints from L h layer by layer throughout the whole graph As a result we only need to learn the labels of the coarsest anchors in L h  Denote Z H as the adjacency matrix that estimates the accumulated interlayer relationships from L 0 to L h  and we can compute Z H as Z H  Z 0  1  Z h 010 1 h 2 R n 003 m h   8  Let A denote the soft label matrix of the anchor set in L h  and F denote the inferred label matrix of datapoints in L 0  With the above accumulated matrix we can conduct the label inference from L h to L 0 in a coarse-to-ﬁne manner F  Z 0  1 Z h 010 1 h A  Z H A   9  Next we consider the label smoothness regularization In traditional graph-based learning we prefer sparse intralayer adjacency matrix because a sparse graph has much less spurious connections between dissimilar points and tends to exhibit high quality Zhu also pointed out that fully-connected dense graphs perform worse than sparse graphs empirically Denoting W as the intra-layer adjacency matrix used in label smoothness regularization we therefore formulate W only based on the inter-layer adjacencies between the data layer L 0 and the nest anchor layer L 1 in the hierarchical anchor graph W  Z 0  1  L 1  010 1 Z 0  1 T 2 R n 003 n   10  where the diagonal matrix L 1 is deﬁned as L 1 ss  P n j  1 Z 0  1 js  Based on the inferred label matrix F and the intra-layer adjacency matrix W  we nally obtain Hierarchical Anchor Graph Regularization argmin A k Z H L A 010 Y L k 2 F  002 2 X n i;j  1 W ij k Z H i 011 A 010 Z H j 011 A k 2   11  where Z H L is the labeled part of Z H  Similar to Eq 4 we have D ii  P j W ij  1  and the above expression can be written in the matrix form argmin A k Z H L A 010 Y L k 2 F  002 tr  A T Z H T  I 010 W  Z H A   or argmin A k Z H L A 010 Y L k 2 F  002 tr  A T  LA    12  where  L is the reduced Laplacian matrix in HAGR computed by  L  Z H T  I 010 W  Z H  Z H T Z H 010 Z H T Z 0  1  L 1  010 1  Z 0  1 T Z H 2 R m h 003 m h   13  As we can see although our label smoothness regularization is rst performed on the labels of all datapoints with the nest anchor layer the optimization only involves a reduced Laplacian matrix with the size of the coarsest anchor layer Therefore HAGR can overcome the limitation of AGR and improve the computation in matrix inversion Note that since Z H is the product of a series of k sparse adjacency matrices we will show that the computation of  L is also efﬁcient The detailed computational costs of HAGR will be analyzed later With simple derivations we obtain a global optimal solution for the soft label matrix of the anchor set in L h as A  Z H L T Z H L  002  L  010 1 Z H L T Y L   14  Based on the learnt labels of the coarsest anchors and the inter-layer adjacency matrix Z H  we can nally infer the hard label for any unlabeled datapoint b y i  argmax r 2f 1   c g Z H i 011 003 A 011 r b r p r i  l  1   n  15  where b r  1 T Z H A 011 r is the normalization factor and p r is the desirable proportion for class r  From the deﬁnition of hierarchical anchor graph we can see its exibility We can vary the number of anchor layers and the number of anchors in each layer We leave the speciﬁc analysis on the parameter settings in the experimental section In particular we nd that if our hierarchical anchor graph only contains one anchor layer  h  1   it degrades to the anchor graph and correspondingly HAGR becomes equivalent to AGR 4.2 Ef\036cient Graph Construction Like anchor graph the construction of a hierarchical anchor graph involves two issues i.e the generation of anchor sets and the inter-layer adjacency estimation between neighboring layers For the rst issue we can simply follow the anchor graph models in 2  3 8 to us e a fa st cl us te r in g al go r it h ms to ha n dle it As for the issue of the weight estimation besides the standard kernel regression method formulating it as a geometric reconstruction problem is an alternative choice 2  3  Ho we ve r  th e ke r ne l re gr es si on ta ke s O  dnm 1  time complexity while the geometric based methods need more time in solving an optimization problem For extremely large datasets both of them can bring intractable computational costs To improve the efﬁciency of the graph construction we investigate an ANNS technique to accelerate the weight estimation To obtain adjacency relationships between points in L b 010 1 and L b  we rst build a Kmeans tree T upon points in L b  Then for each point in L b 010 1  we nd its k nearest points WANG ET AL LEARNING ON BIG GRAPH LABEL INFERENCE AND REGULARIZATION WITH ANCHOR HIERARCHY 1105 


in L b with T  and compute a set of l 1 normalized weights between them With this operation for example estimating the adjacencies between n datapoints and m 1 anchors can be efﬁciently implemented in O  dn log m 1   The whole process is summarized in Algorithm 1 Note that aiming at further efﬁciency other state-of-the-art techniques such as Hashing 39 40 can be considered Algorithm 1 Kmeans-Tree Based Inter-Layer Weight Estimation Input pointsin L b 010 1 pointsin L b  number of nearest neighbors k  1 Employ Kmeans tree building algorithm on points in L b  and obtain a Kmeans tree T  For each point v i in L b 010 1 2 Employ Kmeans tree searching algorithm for v i on tree T  and obtain indices of its k approximate nearest neighbors h i i  with the corresponding distances d h i i  3 Compute the l 1 normalized inter-layer weights z h i i  exp 010 d h i i s    z  where  z  P exp  010 d h i i s   End for 4 Construct a sparse inter-layer adjacency matrix Z b 010 1 b with the above indices h i i s and weights z h i i s Output  Z b 010 1 b  Note  The details about Kmeans tree building and searching algorithms can be found in 4.3 Computational Cost of HAGR We now analyze the computational cost of HAGR As interlayer adjacency matrices in HAGR are all k sparse we rst introduce the following theorem for the fast sparse matrix multiplication According to it for example the time cost of the accumulated inter-layer matrix Z H scales as O  knm h   Theorem 1 Let P and Q be two a 003 b matrices If Q contains at most c non-zero entries the naive algorithm can obtain product O  PQ T with ac multiplications The similar bound is obtained when P contains at most c non-zero entries The number of additions required is also bounded by the required number of multiplications The proof of the above theorem can be found in Then the steps of HAGR and the corresponding time costs are summarized as follows 1 Construct a hierarchical anchor graph with Algorithm 1 The computational cost of computing the adjacency matrices is O  P h b  1 dm b 010 1 log m b   where m 0  n  Since practically we usually have n 012 m b  this cost can be approximated as O  dn log m 1   2 Calculate the reduced Laplacian matrix  L via Eq 13 As the main cost of this step is the sparse matrix multiplication based on Theorem 1 the total cost here scales as O  knm h   3 Carry out the graph regularization via Eq 14 The complexity of the matrix inversion is O  m 3 h   4 Predict the hard labels of unlabeled datapoints via Eq 15 As we have obtained Z H in step 2 it can be conducted efﬁciently in O  nm h c   To sum up the time complexity of HAGR scales as O  dn log m 1  knm h  m 3 h  nm h c   where d is the number of feature dimensions m b is the number of anchors in the b th layer k is the number of nearest neighbors in adjacency estimation and c is the number of classes Here we also summarize the computational costs of Learning with Local and Global Consistency LLGC a typical graph-based SSL method AGR and HAGR in Table 2 in which Algorithm 1 is applied into all these methods for a fair comparison From the table we can observe that although their complexities in graph construction become linear with respect to the data size and can be comparable LLGC still has a cubic-time complexity in graph regularization AGR faces a dramatically increase of computational cost when anchors need to be sufﬁciently dense for a reasonable accuracy However as the scale of the coarsest anchors can be much smaller than the nest anchors i.e m 1 012 m h  HAGR has a much less computational cost and is able to deal with large-scale datasets 4.4 Discussion on Adjacency Designs In Section 4.1 we suggest to build the inter-layer adjacency matrix Z H with anchors layer by layer and the intra-layer adjacency matrix W only based on points in L 0 and L 1  Now we present an in-depth analysis on these two aspects 4.4.1 On the Inter-Layer Adjacency In HAGR we model inter-layer adjacency relationships from L 0 to L h layer by layer and then infer the labels of datapoints in a coarse-to-ﬁne manner According to Eq 7 it leads to the adaptive relationships between datapoints and the coarsest anchors That is when the datapoint is inside the convex envelope of its k nearest anchors in L h  this datapoint only has connection with these k anchors When the datapoint is close to the convex envelope’s margin it can build extra inter-layer edges with other nearest anchors in L h  Otherwise if we build adjacencies between points in L 0 and L h in one step we are only able to obtain the inﬂexible relationships between datapoints and their xed k nearest anchors in L h  In the label inference the above adaptive relationships lead to more reliable integration of label information from the coarsest anchors Without loss of generality we demonstrate this by a toy example in Fig 2 where we have k  3 and h  2  In this example we want to infer the labels of datapoints  x i i  1  2  assisted with the labels of the nearby anchors  u s s  1  2  3  4  in L 2  In our coarse-to-ﬁne manner the datapoint x 1  which is inside the convex envelope of its 3 nearest anchors in L 2  receives labels from these 3 anchors Meanwhile the datapoint x 2  which is near to a margin of its convex envelope can receive label information from both u 1  u 2  u 3 and u 4  due to the transitional anchor u 0 4 TABLE 2 Comparison of Computational Complexities of Three Graph-Based Methods Methods LLGC AGR HAGR Graph construction O  dn log n  O  dn log m 1  O  dn log m 1  Regularization O  n 3  O  knm 1  m 3 1  O  knm h  m 3 h  Inference O  nm 1 c  O  nm h c  1106 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING VOL 29 NO 5 MAY 2017 


in L 1  On the contrary suppose we conduct the one-step label inference between datapoints and their xed k coarsest anchors When k  3  the label information on u 4 is ignored in predicting y 2  When k  4  the noisy labels of the coarsest anchors far away are introduced while inferring y 1  In Section 5.1 we will empirically demonstrate that the classiﬁcation accuracy can be obviously improved due to this characteristic of Z H  although the labels of datapoints are still inferred from sparse anchors namely the coarsest anchors 4.4.2 On the Intra-Layer Adjacency Note that we compute the intra-layer adjacency matrix as W  Z 0  1 L 010 1 1 Z 0  1 T  That means we build W only based on the anchors in L 1 to force the intra-layer edges stay between similar datapoints We can also build W based on anchors in coarser layers such as using the h th layer i.e W  Z 0 h L 010 1 h Z 0 h T 2 R n 003 n  h 1  But note that using different anchor layers to build W leads to nearly the same computational costs of HAGR Meanwhile too sparse anchors will arise many incorrect intra-layer edges between dissimilar datapoints since they possibly share a common anchor That is why we compute W only using the anchors in L 1  5E XPERIMENT In this section we investigate both the effectiveness and efﬁciency of our proposed HAGR on real-world datasets All the experiments are implemented on a PC with E5-2620 v2 2.10 GHz and 64G RAM Here we use the following ve datasets with scales varying from 20,000 to 8,100,000 The descriptions of these datasets are given in below and some statistics of them are listed in Table 3 1 Letter  The dataset contains 20,000 samples of capital letters from A to Z in the English alphabet Each sample is converted into 16 primitive numerical attributes statistical moments and edge counts 2 MNIST  It contains 70,000 samples of handwritten digits from 0 to 9 Each of the ten classes contains about 7,000 samples which are images centered in a 28 003 28 eld by computing the center of mass of the pixels We directly use the normalized grayscale value as the feature 3 Extended MNIST  The extended MNIST is widely used in many large-scale graph-based works  24 The dataset is constructed by translating the original images in MNIST one pixel in each direction As a result there are 630,000 samples in 900 dimensions by using the normalized grayscale values as features 4 Extended USPS  The original USPS dataset contains 7,291 training samples of handwritten digits in ten classes All the digits from 0 to 9 are extended by shifting the 16 003 16 images in all directions for up to ve pixels There are 882,211 samples in 676 dimensions in total 5 MNIST8M  The MNIST8M dataset has been used in  22 26 to verify the effectiveness of large-scale learning algorithms It contains totally 8,100,000 samples in 784 dimensions In this dataset the rst 70,000 samples belong to the standard MNIST dataset and each remaining example is generated by applying a pseudo-random transformation to the MNIST training example Similar to and 36 the above datasets are categorized into small medium and large sizes Speciﬁcally in our experiments we regard Letter and MNIST as small-size datasets Extended MNIST and Extended USPS as mediumsize datasets and MNIST8M as a large-size dataset 5.1 On the Effectiveness of Intra-Layer and Inter-Layer Adjacency Matrices We conduct experiments on small-size datasets i.e Letter and MNIST to validate the effectiveness of two adjacency designs discussed in Section 4.4 We rst construct a hierarchical anchor graph with two anchor layers where the size of L 1 is empirically set to   Fig 2 An illustration of anchor-based label inference in a hierarchical anchor graph Note that we ignore other points as they have no inﬂuence on the label inference here TABLE 3 Details of the Five Databases Used in Our Experiments Letter MNIST Extended MNIST Extended USPS MNIST8M  of instances 20,000 70,000 630,000 882,211 8,100,000  of categories 26 10 10 10 10  of dimensions 16 784 900 676 784 WANG ET AL LEARNING ON BIG GRAPH LABEL INFERENCE AND REGULARIZATION WITH ANCHOR HIERARCHY 1107 


n 10  and the size of L 2 varies from 100 to 500 For a clear comparison we change the formulation of AGR to argmin A k Z L A 010 Y L k 2 F  002 2 X n i;j  1 W ij k Z i 011 A 010 Z j 011 A k 2   16  where W is the intra-layer adjacency matrix for label smoothness regularization and Z is the inter-layer adjacency for label inference Then based on the above formulation and the hierarchical anchor graph we deﬁne three intermediate versions for HAGR 1 HAGR base  which has the same structure to AGR and is a baseline for comparison It only employs anchors in L 2 and datapoints in L 0 to build adjacency matrices for both the label smoothness regularization and label inference 2 HAGR W  which is an improved version of HAGR base with a change on label smoothness regularization This method is compared in order to validate the intra-layer adjacency design Compared with HAGR base  the only difference is that for the label smoothness regularization it builds W as Z 0  1  L 1  010 1 Z 0  1 T in Eq 16 3 HAGR Z  which is an improved version of HAGR base with a change on label inference This method is compared in order to validate the inter-layer adjacency design with a coarse-to ne manner Compared with HAGR base  it builds an accumulated inter-layer matrix Z as Z 0  1 Z 1  2 in Eq 16 The differences of HAGR and the above methods are summarized in Table 4 For the other parameters we set k to 3 to make the graph sparse and tune 002 to its optimal values In this way we can provide a fair comparison for these algorithms to validate different adjacency designs We randomly select 260 and 100 labeled samples for Letter and MNIST respectively and leave the remaining ones unlabeled for SSL models Table 5 shows the classiﬁcation accuracies of the above methods From this table we have three observations First  by comparing HAGR W with HAGR base  we can see that although two methods build the same inter-layer relationships between points in L 0 and L 2  HAGR W obtains higher accuracies than HAGR base  The main reason is that by introducing a much sparser intra-layer adjacency matrix HAGR W can better smooth the labels of datapoints Second  by comparing HAGR Z with HAGR base  we can see that the former obtains better classiﬁcation performances than the latter which shows the effectiveness of our adaptive interlayer adjacency relationships in label inference Third  when the size of L 2 increases the accuracies of all these approaches increase and HAGR consistently outperforms the other three methods 5.2 Comparison with Existing Methods To demonstrate both the efﬁciency and effectiveness of the proposed HAGR we compare it with several stateof-the-art anchor-based SSL models such as AGR and EAGR We also report the performance of several baseline methods including 1NN linear SVM For clarity here we use HAGRm 1 m 1 m 2 m 2 011\011\011 m h m h  to denote the HAGR method built upon a hierarchical anchor graph with h h anchor layers For example HAGR-5000-500 means there are two anchor layers in its graph structure which contain 5,000 and 500 anchors respective ly The methods for comparison are described in below 1 The nearest neighbor method which determines the label of a sample by referring to its closest sample in the labeled set The method is denoted as 1NN 2 Linear SVM We use the SVM implementation from LIBLINEAR which is a library for large-scale linear classiﬁcation The method is denoted as LSVM 3 Anchor graph Regularization which is built upon an anchor graph with single anchor layer It is the prime counterpart in our experiments and we denote it as AGR TABLE 4 The Differences of HAGR base  HAGR W  HAGR Z  and HAGR in Terms of Label Inference and Label Smoothness Regularization Approaches Adjacency Matrix Label Inference Term Label Smoothness Regularization Term HAGR base Z 0  2 Z 0  2  L 2  010 1 Z 0  2 T HAGR W Z 0  2 Z 0  1  L 1  010 1 Z 0  1 T HAGR Z Z 0  1 Z 1  2 Z 0  2  L 2  010 1 Z 0  2 T HAGR Z 0  1 Z 1  2 Z 0  1  L 1  010 1 Z 0  1 T TABLE 5 Accuracy  Comparison of HAGR base  HAGR W  HAGR Z  and HAGR on the Letter and MINST Datasets Dataset m 1 m 2 HAGR base HAGR W HAGR Z HAGR Letter 2,000 100 45  19 013 1  53 45  61 013 1  59 49  19 013 0  70 49  57 013 1  02 49  57 013 1  02  l  260  200 50  91 013 1  40 51  30 013 1  31 54  75 013 1  21 54  93 013 1  05 54  93 013 1  05 300 53  72 013 1  54 54  76 013 1  73 57  00 013 1  43 57  78 013 1  48 57  78 013 1  48 400 55  58 013 1  66 56  99 013 1  57 58  30 013 1  34 59  88 013 1  42 59  88 013 1  42 500 56  80 013 1  65 58  00 013 1  69 59  51 013 1  18 60  86 013 1  36 60  86 013 1  36 MNIST 7,000 100 79  17 013 1  39 80  33 013 1  30 82  33 013 1  15 84  59 013 0  89 84  59 013 0  89  l  100  200 83  28 013 1  21 83  92 013 1  17 85  01 013 0  87 86  79 013 0  66 86  79 013 0  66 300 84  44 013 0  98 85  03 013 1  02 85  89 013 0  97 88  01 013 1  00 88  01 013 1  00 400 85  30 013 1  31 85  92 013 1  26 86  21 013 1  15 88  32 013 1  15 88  32 013 1  15 500 86  35 013 1  80 86  82 013 1  74 86  84 013 1  18 88  66 013 1  23 88  66 013 1  23 1108 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING VOL 29 NO 5 MAY 2017 


4 Efﬁcient Anchor graph Regularization which is an improved version of AGR The method is denoted as EAGR 5 HAGRm 1 m 2  which denotes a two-anchor-layer HAGR method We build the corresponding hierarchical anchor graph by adding a coarser anchor layer above the anchor graph in methods 3-4 The purpose of comparing our approach with this method is to demonstrate the efﬁciency by introducing a smaller anchor layer 6 HAGRm 1 m 2 m 3  which denotes a three-anchorlayer HAGR method Based on the graph structure in method 5 we add a ner anchor layer with sampled points between its data layer and two anchor layers This three-anchor-layer HAGR is our proposed approach for the classiﬁcation on the medium-size and large-size datasets Note that here we do not further compare our approach with several other SSL or large-scale classiﬁcation methods such as conventional graph-based SSL the Eigenfu nctions method introduced in the Laplacian SVM method introduced in and the Prototype Vector Machines method introduced in 46 due to the follow ing two facts First several methods can hardly be implemented on very large datasets Second existing studies have already demonstrated the performance superiority of AGR and EAGR over these methods 36 and thus the superio rity of HAGR will be greater if it outperforms AGR and EAGR For fair comparisons we consistently apply the proposed ANNS-based weight estimation algorithm for methods 3-6 in their graph construction and corresponding kernel widths are set by cross validation For the above methods we tune 002 to the optimal values 5.2.1 Medium-Size Datasets We rst conduct experiments on the Extended MNIST and Extended USPS datasets To accelerate the running speed we follow 23 and per fo rm PCA to re duc e th e fe atu re dim en sio n to 86 For the two medium-size datasets we consistently construct the anchor graph with 20,000 anchors to build AGR and EAGR Then by further adding anchor layers we build two HAGR methods as HAGR-20,000-5,000 and HAGR200,000-20,000-5,000 respectively As for the setting of semisupervised learning we vary the number of labeled samples l f 100  200    1  000 g  while the rest samples remain as unlabeled data Averaged over 20 trials the classiﬁcation accuracies of the Extend MNIST and Extend USPS datasets are shown in Tables 6 and 7 respectively The time costs of SSL methods are listed in Table 8 From these tables the following observations can be made First  the performances of all graph-based SSL approaches stay at a higher level than Linear SVM and 1NN This demonstrates the usefulness of unlabeled data in SSL Second  compared with AGR and EAGR the accuracies of HAGR-20,000-5,000 are slightly lower However the performance gap is quite limited-compared with AGR the accuracy loss of this two-anchor-layer HAGR is smaller than 0.5 percent in most cases It means that by building an intra-layer adjacency matrix based on a xed-size anchor set for label smoothness regularization the effectiveness of the anchor-based learning can be almost maintained even TABLE 6 Classiﬁcation Accuracies    with Different Number of Labeled Samples on the Extended MNIST Dataset  of labeled samples 1NN LSVM AGR EAGR HAGR 20,000-5,000 HAGR 200,000-20,000-5,000 100 60  95 013 0  59 58  58 013 2  11 88  42 013 1  36 88  48 013 1  29 87  97 013 1  28 90  75 013 1  03 90  75 013 1  03 200 69  33 013 0  99 64  07 013 1  58 90  23 013 0  44 90  80 013 0  42 89  81 013 0  56 92  21 013 0  42 92  21 013 0  42 300 73  51 013 0  89 66  99 013 0  53 91  10 013 0  38 91  84 013 0  35 90  65 013 0  43 93  13 013 0  31 93  13 013 0  31 400 75  80 013 0  60 69  50 013 0  86 91  35 013 0  34 92  15 013 0  29 91  15 013 0  33 93  39 013 0  18 93  39 013 0  18 500 77  77 013 0  41 71  47 013 1  21 92  12 013 0  18 92  66 013 0  17 91  96 013 0  12 93  73 013 0  19 93  73 013 0  19 600 79  09 013 0  53 72  69 013 1  30 92  47 013 0  10 92  99 013 0  13 92  27 013 0  11 93  95 013 0  11 93  95 013 0  11 700 80  17 013 0  29 73  69 013 1  96 92  54 013 0  11 93  11 013 0  13 92  43 013 0  12 94  05 013 0  11 94  05 013 0  11 800 81  10 013 0  41 75  22 013 1  40 92  79 013 0  13 93  39 013 0  09 92  61 013 0  10 94  15 013 0  06 94  15 013 0  06 900 81  94 013 0  45 75  93 013 1  11 93  09 013 0  09 93  63 013 0  10 92  93 013 0  09 94  23 013 0  06 94  23 013 0  06 1,000 82  60 013 0  38 76  69 013 0  95 93  23 013 0  11 93  77 013 0  08 93  09 013 0  10 94  28 013 0  09 94  28 013 0  09 TABLE 7 Classiﬁcation Accuracies    with Different Number of Labeled Samples on the Extended USPS Dataset  of labeled samples 1NN AGR EAGR HAGR 20000-5000 HAGR 200000-20000-5000 100 38  37 013 0  71 63  82 013 1  33 64  09 013 0  85 63  43 013 1  11 68  06 013 1  55 68  06 013 1  55 200 47  96 013 1  14 73  60 013 0  90 73  75 013 0  96 73  48 013 0  96 77  90 013 1  33 77  90 013 1  33 300 53  48 013 1  06 78  47 013 0  91 78  88 013 0  78 78  28 013 0  67 82  28 013 0  92 82  28 013 0  92 400 57  61 013 1  00 81  41 013 0  60 81  75 013 0  45 81  00 013 0  57 84  50 013 0  81 84  50 013 0  81 500 61  19 013 0  80 83  51 013 0  94 84  09 013 0  75 84  05 013 0  78 86  35 013 0  94 86  35 013 0  94 600 63  94 013 0  64 84  50 013 0  88 85  28 013 0  76 84  09 013 0  65 87  11 013 0  84 87  11 013 0  84 700 65  90 013 0  44 85  80 013 0  70 86  52 013 0  56 85  31 013 0  42 88  10 013 0  49 88  10 013 0  49 800 67  79 013 0  38 86  31 013 0  93 87  59 013 0  74 86  29 013 0  69 88  91 013 0  65 88  91 013 0  65 900 69  18 013 0  38 86  95 013 0  68 88  23 013 0  59 86  95 013 0  50 89  44 013 0  32 89  44 013 0  32 1,000 70  71 013 0  60 87  87 013 0  55 88  82 013 0  35 87  55 013 0  47 90  01 013 0  31 90  01 013 0  31 WANG ET AL LEARNING ON BIG GRAPH LABEL INFERENCE AND REGULARIZATION WITH ANCHOR HIERARCHY 1109 


we signiﬁcantly reduce the size of to-be-learned anchor set from 20,000 to 5,000 When taking into account the running time of the learning process shown in Table 8 this accuracy loss becomes acceptable in real applications Third  by increasing the size of L 1 over 20,000 the performances of the anchor-based approaches can be further improved due to better adjacency relationships Based on this HAGR200,000-20,000-5,000 consistently outperforms AGR and EAGR When the number of labeled samples is small this advantage is more obvious e.g improvements of about 2 and 4 percent on Extended MNIST and Extended USPS respectively It veriﬁes the effectiveness of HAGR by introducing a ner anchor layer to improve the graph regularization Fourth  although the ANNS-based graph construction can be applied to all graph-based approaches it is particularly suitable for our HAGR Due to this efﬁcient graph construction and the fast optimization HAGRs overcome the limitation of AGR and EAGR and achieve good performances with less computational costs 5.2.2 Large-Size Dataset To demonstrate the scalability of HAGR we conduct experiments on the MNIST8M dataset where the dimension of examples is also reduce to 86 by PCA We rst construct an anchor graph with 30,000 anchors to build AGR and EAGR Then we build two HAGR methods i.e HAGR-300005,000 and HAGR-300,000-30,000-5,000 respectively By repeating the similar evaluation process we display the classiﬁcation accuracies over 20 trials in Table 9 The time costs of SSL approaches are listed in Table 10 From these tables we have the following observations First  compared with AGR and EAGR the accuracy loss of HAGR-30,000-5,000 is acceptable while its time cost is much less Second  as the number of labeled data varies from 100 to 1,000 the performances of all methods increase and our HAGR-300,000-30,000-5,000 consistently outperforms the other methods It is also worth noting that in our experimental results we actually have put more emp hasis on the performance superiority of the three-anchor-layer HAGR One may argue that we can increase the number of anchors in AGR and EAGR say setting 300,000 anchors But this will dramatically increase the time costs of these methods increased by about 10 3 times which are practically intractable 5.3 On the Structure of Hierarchical Anchor Graph We can see that HAGR is a quite exible approach and thus designing the structure of the hierarchical anchor graph can be important in classiﬁcation tasks Therefore we conduct additional experiments by varying the number of anchor layers  h  and the size of each anchor layer  m b  in the proposed graph model trying to investigate the impact of these parameters For convenience Extended USPS dataset is used in this experiment We build three anchor graphs with 5,000 10,000 and 20,000 anchors for implementing AGR and EAGR as comparisons For clarity in the pictures illustrating results we use black/blue/red lines to show the results of HAGR built with 2/3/4 anchor layers respectively Among them each dot-curve denotes HAGRs with the varying anchor size m b  and the straight line without dots means the size of each anchor layer of this HAGR is xed The speciﬁc size is displayed at the bottom of each subﬁgure TABLE 8 The Comparison of Time Costs in Seconds of AGR EAGR and HAGR Methods on Medium-Size Datasets Dataset AGR EAGR HAGR 20,000-5,000 HAGR 200,000-20,000-5,000 Extended MNIST 152.81 151.58 12.12 17.06 Extended USPS 163.31 162.75 17.55 24.18 TABLE 9 Classiﬁcation Accuracies    with Different Number of Labeled Samples on the MNIST8M Dataset  of labeled samples 1NN LSVM AGR EAGR HAGR 30,000-5,000 HAGR 300,000-30,000-5,000 100 60  16 013 1  96 59  67 013 2  19 89  87 013 1  78 90  27 013 0  18 89  46 013 1  24 91  36 013 0  70 91  36 013 0  70 200 68  66 013 1  29 64  46 013 2  37 91  15 013 0  59 91  76 013 0  57 90  85 013 0  50 92  46 013 0  42 92  46 013 0  42 300 72  78 013 0  81 66  79 013 2  25 92  21 013 0  51 92  37 013 0  51 91  66 013 0  42 93  05 013 0  37 93  05 013 0  37 400 75  33 013 0  60 68  33 013 1  97 92  47 013 0  44 92  73 013 0  38 92  16 013 0  36 93  43 013 0  37 93  43 013 0  37 500 77  24 013 0  55 70  65 013 1  49 92  70 013 0  41 93  05 013 0  29 92  50 013 0  29 93  78 013 0  24 93  78 013 0  24 600 78  58 013 0  54 72  64 013 1  36 92  80 013 0  34 93  17 013 0  27 92  64 013 0  26 93  90 013 0  27 93  90 013 0  27 700 79  87 013 0  70 73  80 013 1  27 93  12 013 0  31 93  41 013 0  30 92  92 013 0  28 94  10 013 0  25 94  10 013 0  25 800 81  02 013 0  50 73  87 013 1  18 93  19 013 0  23 93  51 013 0  15 93  06 013 0  16 94  21 013 0  15 94  21 013 0  15 900 81  76 013 0  49 73  97 013 0  96 93  29 013 0  36 93  63 013 0  21 93  18 013 0  26 94  28 013 0  16 94  28 013 0  16 1,000 82  51 013 0  42 76  95 013 1  13 93  49 013 0  22 93  79 013 0  15 93  37 013 0  16 94  39 013 0  12 94  39 013 0  12 TABLE 10 The Comparison of Time Costs in Seconds of AGR EAGR and HAGR Methods on the MNIST8M Dataset Dataset AGR EAGR HAGR 30,000-5,000 HAGR 300,000-30,000-5,000 MNIST8M 665.07 662.60 104.97 137.54 1110 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING VOL 29 NO 5 MAY 2017 


We rst test the two-anchor-layer HAGR method Similar to the process in Section 5.2 for each of the three anchor graphs we add a coarser anchor layer L 2 above the original L 1 to construct our hierarchical anchor graph  h  2  The performance curves of HAGR m 1 m 2 with respect to the size of L 2 are shown in Fig 3 As we can see the accuracy of HAGR increases rapidly at the rst stage It means that although we can build a large-scale nest anchor layer in HAGR to improve the performance the size of to-belearned anchors cannot be too small Otherwise it tends to restrict the performance of HAGR When m 2 reaches about m 1  4  the accuracy of the HAGR becomes stable We note that this number of to-be-learned anchors m h can still be much smaller than the number of anchors m 1 used in label smoothness regularization Then we investigate the three-anchor-layer HAGR method Based on each two-anchor-layer graph above we add a ner anchor layer L 1 with m 1 anchors between the original data layer and two anchor layers to construct new graph  h  3  The number of the coarsest anchors is set to the empirical value and the speciﬁc settings can be found in Fig 4 The performance curve of HAGR m 1 m 2 m 3 with respect to m 1 is shown in Fig 4 As we can see larger m 1 brings higher accuracy The reason is that by increasing the size of the nest anchor layer the representation power of these anchors becomes stronger Then we can obtain more Fig 3 Average performance curves with respect to the variation of m 2 in HAGR m 1 m 2  Fig 4 Average performance curves with respect to the variation of m 1 in HAGR m 1 m 2 m 3  Fig 5 Average performance curves with respect to the variation of m 2 in HAGR m 1 m 2 m 3 m 4  WANG ET AL LEARNING ON BIG GRAPH LABEL INFERENCE AND REGULARIZATION WITH ANCHOR HIERARCHY 1111 


effective adjacency relationships for both the label smoothness regularization and label inference We also build the four-anchor-layer HAGR method  h  4  by further adding an anchor layer between original L 1 and L 2 in each three-anchor-layer graph above The performance curve of HAGR m 1 m 2 m 3 m 4 with respect to current m 2 is shown in Fig 5 As we can see further increasing the number of anchor layers can improve the accuracy which demonstrates the effectiveness of exploring multiplelayer anchors to model the coarse-to-ﬁne label inference To summarize when xing the number of anchor layers in hierarchical anchor graph the performance of HAGR is fairly robust to the number of anchors in each layer over a large range as we can see in each subﬁgure and the to-belearned anchors can be much fewer than the anchors used in label smoothness regularization Meanwhile increasing the number of anchor layers can improve accuracy under a comparison from Figs 3 to 5 and in our experiments we empirically nd that three or four anchor layers are already able to well handle million-scale datasets 5.4 On the Trade-Off Parameter 002 We also test the sensitivity of parameter 002 in the proposed approach For simplicity we only illustrate the results of HAGR with m h  5  000 on the Extended USPS dataset But similar observations can also be obtained in other cases As we can see in Fig 6 the performance of HAGR will not severely degrade when 002 varies in a wide range and increasing the number of anchor layers does not change the robustness of 002  6C ONCLUSION This work proposes a novel Hierarchical Anchor Graph Regularization approach by exploring multiple-layer anchors in a pyramid-style structure It generalizes the conventional graph-based and anchor-graph-based SSL methods to a hierarchical approach In HAGR we perform label smoothness regularization on all datapoints based on the nest anchors By inferring the labels of datapoints started from the coarsest anchors we obtain an efﬁcient optimization which only involves a small-size reduced Laplacian matrix It overcomes the limitations of existing AGR approach in dealing with extremely large datasets We also investigate ANNS to improve the efﬁciency of the construction of the hierarchical anchor graph Experiments on publicly available large-scale datasets of various sizes have demonstrated the effectiveness of our approach in terms of computational speed and classiﬁcation accuracy A CKNOWLEDGMENTS This work is partially supported by the National 973 Program of China under grants 2014CB347600 and 2013CB329604 the Program for Changjiang Scholars and Innovative Research Team in University PCSIRT of the Ministry of Education China under grant IRT13059 and the National Nature Science Foundation of China under grant 61432019 R EFERENCES  M Belkin P Niyogi and V Sindhwa ni Manifold regulariza tion A geometric framework for learning from labeled and unlabeled examples J Mach Learn Res  vol 7 no 11 pp 2399–2434 2006  A Blum and T Mitche ll Combinin g labeled and unlabeled data with co-training in Proc Conf Comput Learn Theory  1998 pp 92–100  D Cai and X Chen Large scale spectral clusterin g with landmark-based representation IEEE Trans Cybern  vol 45 no 8 pp 1669–1680 Aug 2015  V Castelli and T M Cover On the exponen tial value of labeled samples Pattern Recog Lett  vol 16 no 1 pp 105–111 1995  L Chen I W Tsang and D Xu Laplaci an embedded regression for scalable manifold regularization IEEE Trans Neural Netw Learn Syst  vol 23 no 6 pp 902–915 Jun 2012  B Cheng J Yang S Yan Y Fu and T S Huang Learn ing with l 1 graph for image analysis IEEE Trans Image Process  vol 19 no 4 pp 858–866 Apr 2010  C Deng R Ji W Liu D Tao and X Gao Visual reranking through weakly supervised multi-graph learning in Proc IEEE Int Conf Comput Vis  2013 pp 2600–2607  R.-E Fan K.-W Chang C.-J Hsieh X.-R Wang and C.-J Lin Liblinear A library for large linear classiﬁcation J Mach Learn Res  vol 9 no 9 pp 1871–1874 2008  R Fergus Y Weiss and A Torralba  Semi-su pervised learning in gigantic image collections in Proc Conf Advances Neural Inf Proc Syst  2009 pp 522–530  P Foggia G Percannella and M Vento Graph matching and learning in pattern recognition in the last 10 years Int J Pattern Recog Artif Intell  vol 28 no 1 pp 178–215 2014  P W Frey and D J Slate Letter recognition using holland-st yle adaptive classiﬁers Mach Learn  vol 6 no 2 pp 161–182 1991  L Gao J Song F Nie Y Yan N Sebe and H T Shen Optimal graph learning with partial tags and multiple features for image and video annotation in Proc IEEE Conf Comput Vis Pattern Recog  2015 pp 4371–4379  M Guillaumin J Verbeek and C Schmid Multimoda l semisupervised learning for image classiﬁcation in Proc IEEE Conf Comput Vis Pattern Recog  2010 pp 902–909  T J Hastie R J Tibshiran i and J H Friedman The Elements of Statistical Learning Data Mining Inference and Prediction  Berlin Germany Springer 2010  M Hein and S Setzer Beyond spectral clustering-tight relaxations of balanced graph cuts in Proc Conf Advances Neural Inf Process Syst  2011 pp 2366–2374  C.-J Hsieh S Si and I S Dhillon A divide-and-c onquer solver for kernel support vector machines in Proc Int Conf Mach Learn  2014 pp 566–574  S Huang  M Elhoseiny A Elgamm al and D Yang Learning hypergraph-regularized attribute predictors in Proc IEEE Conf Comput Vis Pattern Recog  2015 pp 409–417 Fig 6 Average performance curves of HAGR with respect to the variation of 002  1112 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING VOL 29 NO 5 MAY 2017 


 T Joachims Transdu ctive inferenc e for text classiﬁcatio n using support vector machines in Proc Int Conf Mach Learn  1999 pp 200–209  M Karlen J Weston A Erkan and R Collobert Large scale manifold transduction in Proc Int Conf Mach Learn  2008 pp 448–455  S Kim and S Choi Multi-vie w anchor graph hashing in Proc IEEE Int Conf Acoust Speech Signal Process  2013 pp 3123–3127  Y Lecun L Bottou Y Bengio and P Haffner Gradient based learning applied to document recognition Proc IEEE  vol 86 no 11 pp 2278–2324 Nov 1998  M Li J T.-Y Kwok and B L  u Making large-scale nystr  om approximation possible in Proc Int Conf Mach Learn  2010 pp 631–638  W Liu J He and S.-F Chang Large graph construction for scalable semi-supervised learning in Proc Int Conf Mach Learn  2010 pp 679–686  W Liu J Wang and S.-F Chang Robust and scalable graphbased semisupervised learning Proc IEEE  vol 100 no 9 pp 2624–2638 Sep 2012  W Liu J Wang S Kumar and S.-F Chang Hashing with graphs in Proc Int Conf Mach Learn  2011 pp 1–8  L Bottou Large-scale kernel machine s MIT press 2007  D Machin Introduction to Multimodal Analysis  London U.K Bloomsbury Publishing 2016  S Melacci and M Belkin Laplac ian support vector machines trained in the primal J Mach Learn Res  vol 12 no 5 pp 1149 1184 2009  M Muja and D G Lowe Scalable nearest neighbor algorithms for high dimensional data IEEE Trans Pattern Anal Mach Intell  vol 36 no 11 pp 2227–2240 Nov 2014  M Norou zi A Punjani and D J Fleet Fast exact search in hamming space with multi-index hashing IEEE Trans Pattern Anal Mach Intell  vol 36 no 6 pp 1107–1119 Jun 2014  S T Roweis and L K Saul Nonlinea r dimensi onality reduction by locally linear embedding Science  vol 290 no 5500 pp 2323 2326 2000  Z Tian and R Kuang Global linear neighborh oods for efﬁcient label propagation in Proc SIAM Int Conf Data Mining  2012 pp 863–872  I W Tsang J T Kwok and P.-M Cheung Core vector machines Fast SVM training on very large data sets J Mach Learn Res  vol 6 no 1 pp 363–392 2005  F Wang and C Zhang Label propagation through linear neighborhoods IEEE Trans Knowl Data Eng  vol 20 no 1 pp 55–67 Jan 2008  J Wang J Wang G Zeng Z Tu R Gan and S Li Scalable k NN graph construction for visual descriptors in Proc IEEE Conf Comput Vis Pattern Recog  2012 pp 1106–1113  M Wang W Fu S Hao D Tao and X Wu Scalable semi-supervised learning by efﬁcient anchor graph regularization IEEE Trans Knowl Data Eng  vol 28 no 7 pp 1864–1877 Jul 2016  M Wang X.-S Hua R Hong J Tang G.-J Qi and Y Song Uniﬁed video annotation via multigraph learning IEEE Trans Circuits Syst Video Technol  vol 19 no 5 pp 733–746 May 2009  B Xu J Bu C Chen C Wang D Cai and X He EMR A scalable graph-based ranking model for content-based image retrieval IEEE Trans Knowl Data Eng  vol 27 no 1 pp 102 114 Jan 2015  Y Yang Y Luo W Chen F Shen J Shao and H T Shen Zeroshot hashing via transferring supervised knowledge in Proc ACM Conf Multimedia  2016 pp 1286–1295  Y Yang F Shen H T Shen H Li and X Li Robust discrete spectral hashing for large-scale image semantic indexing IEEE Trans Big Data  vol 1 no 4 pp 162–171 Dec 2015  Y Yang Y Yang H T Shen Y Zhang X Du and X Zhou Discriminative nonnegative spectral clustering with out-of-sample extension IEEE Trans Knowl Data Eng  vol 25 no 8 pp 1760–1771 Aug 2013  G Yu G Zhang Z Zhang Z Yu and L Deng Semi-su pervised classiﬁcation based on subspace sparse representation Knowl Inf Syst  vol 43 no 1 pp 81–101 2015  R Yuster and U Zwick Fast sparse matrix multiplica tion ACM Trans Algorithms  vol 1 no 1 pp 2–13 2005  L Zelnik-Ma nor and P Perona Self-tuning spectral clustering  in Proc Conf Advances Neural Inf Process Syst  2005 pp 1601 1608  K Zhang J T Kwok and B Parvin Prototype vector machine for large scale semi-supervised learning in Proc Int Conf Mach Learn  2009 pp 1233–1240  K Zhang L Lan J T Kwok S Vucetic and B Parvin Scaling up graph-based semi-supervised learning via prototype vector machines IEEE Trans Neural Netw Learn Syst  vol 26 no 3 pp 444–457 Mar 2015  D Zhou O Bousquet T N Lal J Weston and B Sch  olkopf Learning with local and global consistency in Proc Conf Advances Neural Inf Process Syst  2004 pp 321–328  D Zhou J Huang and B Sch  olkopf Learning with hypergraphs Clustering classiﬁcation and embedding in Proc Conf Advances Neural Inf Process Syst  2006 pp 1601–1608  X Zhu Semi-s upervised learning literature survey Comput Sci Univ Wisconsin-Madison Madison WI USA Tech Rep 1530 2005  X Zhu Z Ghahramani and J Lafferty Semi-sup ervised learning using gaussian elds and harmonic functions in Proc Int Conf Mach Learn  2003 pp 912–919  X Zhu and A B Goldberg Introduct ion to semi-supe rvised learning Synthesis Lectures Artif Intell Mach Learn  vol 3 no 1 pp 1–130 2009  X Zhu J Kandola Z Ghahramani and J D Lafferty Nonparametric transforms of graph kernels for semi-supervised learning in Proc Conf Advances Neural Inf Process Syst  2004 pp 1641–1648 Meng Wang received the BE and PhD degrees from the Special Class for the Gifted Young Department of Electronic Engineering and Information Science University of Science and Technology of China USTC Hefei China in 2003 and 2008 respectively He is a professor with the Hefei University of Technology China His current research interests include multimedia content analysis computer vision and pattern recognition He has authored more than 200 book chapters and journal and conference papers in these areas He received the ACM SIGMM Rising Star Award 2014 He is an associate editor of the IEEE Transactions on Knowledge and Data Engineering and the IEEE Transactions on Circuits and Systems for Video Technology  He is a member of the IEEE Weijie Fu is currently working toward the PhD degree in the School of Computer and Information Hefei University of Technology HFUT His current research interest focuses on machine learning and data mining Shijie Hao received the BE MS and PhD degrees from the School of Computer and Information Hefei University of Technology HFUT He is an associate professor with HFUT China His current research interests include machine learning and image processing WANG ET AL LEARNING ON BIG GRAPH LABEL INFERENCE AND REGULARIZATION WITH ANCHOR HIERARCHY 1113 


Hengchang Liu received the PhD degree from the University of Virginia in 2011 under the supervision of Professor John Stankovic He is currently an assistant professor with USTC His research interests mainly includes big data mining cyber physical systems and mobile systems Xindong Wu received the bachelor’s and master’s degrees in computer science from the Hefei University of Technology China and the PhD degree in artiﬁcial intelligence from the University of Edinburgh Britain He is a Yangtze River scholar in the School of Computer Science and Information Engineering Hefei University of Technology China and a professor of computer science with the University of Louisiana at Lafayette His research interests include data mining knowledge-based systems and Web information exploration He is the steering committee chair of the IEEE International Conference on Data Mining ICDM the editor-in-chief of Knowledge and Information Systems  KAIS  by Springer and a series editor of the Springer Book Series on Advanced Information and Knowledge Processing  AI  KP  He was the editor-in-chief of the IEEE Transactions on Knowledge and Data Engineering  TKDE  by the IEEE Computer Society between 2005 and 2008 He served as program committee chair co-chair for ICDM 03 the 2003 IEEE International Conference on Data Mining KDD-07 the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining and CIKM 2010 the 19th ACM Conference on Information and Knowledge Management He is a fellow of the IEEE  For more information on this or any other computing topic please visit our Digital Library at www.computer.org/publications/dlib 1114 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING VOL 29 NO 5 MAY 2017 


iao 16\32018 2008  G  B  H uang 322W hat a r e e x tr em e l ear ning m achines  F illing t he gap 323 2007  G B H uang X Ding and H  Z d 323 400\320405  X Bi X  Z hao G W a ng P  Z h ang and C W ang 322Dis trib uted e x trem e 323 2015  M Z a haria 2013\3202025 2015  Z  Bai G B H uang D W a ng H W a ng and M  B  W es to v e r  322Spars e 323 1\3202  H Z hou G B Huang Z  L i n H W a ng and Y  C  S oh 322Stack ed e xtrem e 323 ence and puting  3 2014  L  L  C Kas un Y  Y a ng G B Huang and Z  Z hang 322Dim ens i on 323 o 2603511  Y  L ecun L  Bottou Y  Bengio and P  H af fner  322 Gradient-bas ed learn\323 AN  2015  B W a ng S Huang J  Qiu Y  L i u an equen\323  2015  G B H uang L  Chen a nd C K S ie w  322Uni v e rs al approxim a tion u s i ng hidden 323  2016  G B H uang and L  C hen 322E nha n\323  2014  J  Chen   t ark 323 Duan i i n of 2003 ity  ull with the puting papers he s puting the T ON C TERS he e of nd China National ha 1985 n ity arch a puting operating   5  J  Chen G  Z heng and H  C hen 322E lm m apr e duce M a pr educe acceln  Science of  China e nd ning acn 2006  G B H uang and L  Chen 322Con v e x i ncrem e ntal e x trem e l earning 323 11 726791   EEE ss w  is t putted co loud hybrid puting unication  the T ON P AND D IS UTED S TEMS T N C TERS E T N C LOUD C NG e 74 2010  G B H uang 322 A n i ns ight into e x trem e l earning m achines  Random 323 This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination al al  ocomputing ocomputing ocomputing ocomputing ocomputing  nt n n Comput Comput 


225 225\225 225 225 1 111 225 225 
i 
GnuPG and Open SSL Results finds any vulnerabilities that noxious programmers could use to access any PC you have associated with a system GnuPG is an entire and free execution of the OpenPGP standard as characterized by RFC4880 otherwise called PGP  GnuPG permits to encode and sign your information and correspondence highlights a flexible key administration framework and additionally get to modules for a wide range of open key indexes  
 
26 
tt 26 
c e 
140 120 g 0 25 
System A 180    O penSSLD 9 7LR 
E Ubgcryp t 1 6LR             29 
Ubgcrypt 1 6FR 
 
 
_ OpenSS LD 9 7FR _ OpenSSLD  9.7FR 180    O penSSLD 9 7LR O pen SSL1 0.1 FR   III   Op enSSL1 0.1 LR 160 PolarSS L 1.3  3FR   0   Po l arSS L1 3 3LR 
B 
27 29 Encr ypt ions   Ill   O penS SL1 0.1 LR 160 PolarSSL1  3.3FR   0   PolarSS L1  3.3LR Ubgcrypt1 6FR Ubg c rypt1  6LR 27 Encr y pt i ons  2 System 
  
28 28 
140 25 
 


Conclusion 
1 
References 
1 
machine 8 https www.toptal.com/linux/separation-anxiety-isolating-your-system-with-linu\x-namespaces 9 Isolation in Cloud Computing and Privacy-Enhancing Technologies Suitability of PrivacyEnhancing Technologies for Separating Data Usage in Business Processes Prof Dr Noboru Sonehara Prof Dr Isao Echizen Dr SvenWohlgemuth National Institute of Informatics 2-1-2 Hitotsubashi Chiyoda-ku Tokyo sonehara@nii.ac  jp  10  Performance Isolation and Fairness for Multi-Tenant Cloud Storage David Shue Michael J Freedman and Anees Shaikh Princeton University ylBM TJ Watson Research Center 
An Updated Performance Comparison of Virtual Machines and Linux Containers Wes Felter Alexandre Ferreira Ram Rajamony Juan Rubio IBM Research Austin TX fwmf apferrei rajamony rubiojg@us.ibm.com 2 A Unified Operating System for Clouds and Manycore fos David Wentzlaff Charles Gruenwald III Nathan Beckmann Kevin Modzelewski Adam Belay Lamia Youseff Jason Miller and Anant Agarwal 3 Containers and Cloud From LXC to Docker to Kubernetes DAVID BERNSTEIN 4 Containers and Clusters for Edge Cloud Architectures a Technology Review Claus Pahl Irish Centre for Cloud Computing and Commerce IC4  Lero the Irish Software Research Centre Dublin City UniversityDublin 9 Ireland 5 Containerisation and the PaaS Cloud Claus Pahl 6 http://www.slideshare.net/BodenRussell/kvm-and-docker-lxc-benchmarking-w\ith-openstack 7 http://stackoverflow com q uestio ns/1604 7306/how-is-docker d ifferent from-a-no rma I-vi rtua 
It has been observed from experiments that container provides much more isolation among multiple users multi-tenants in cloud virtualization as compared to virtual machines Taking the example of Docker container which is light weight more secure and fast processing virtualization technique and getting much more familiarity due to its characteristics Also Container provides isolation at every instance of virtualization like at process level at file system level network level and at inter process communication lPe level 
 


