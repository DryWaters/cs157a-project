2006 International Symposium on Evolving Fuzzy Systems September 2006 Using a Genetic Algorithm to Derive a Linguistic Summary of Trends in Numerical Time Series Janusz Kacprzyk Fellow IEEE Anna Wilbik and Slawomir Zadrozny AbstractThe purpose of this paper is to propose a new easily implementable approach to a linguistic summarization of trends that may occur in temporal data to be more specific time series To characterize the trends in time series we use three parameters dynamics of change duration and variability and apply to them the fuzzy linguistic summaries of data databases in the sense of Yager cf Yager 13 Kacprzyk and Yager 7 and Kacprzyk Yager and Zadrozny 8 which in the 
form of natural language-like sentences subsume the very essence of a set of data A genetic algorithm is used to generate the linguistic summaries sought I INTRODUCTION By providing tools and techniques to model and process imprecise data information relations etc fuzzy logic based approaches make it possible to model and describe real life human centric systems in a more human-consistent and concise way A basic understanding of a system might be obtained via an analysis of its numerical characteristic features measured over a time span In the simplest case such a characteristic feature may be treated as a real-valued function of time i.e a time-series data Here we are interested in such a description of time series data 
that accounts for a proper understanding of the main trends regarding the system's behavior There are many techniques providing such a description including those deeply rooted in statistics However usually they lack human-consistency that in many cases is what is primarily needed In order to overcome this difficulty and supplement known strict methods we propose to employ linguistic summaries in the sense of Yager cf Yager 13 Kacprzyk and Yager 7 and Kacprzyk Yager and Zadrozny 8 Linguistic summaries of data are meant as a concise human-consistent description of a data set In this approach a data set e.g the content of a database is summarized using a natural 
language like expression such as Most employees are young Semantics of a particular linguistic terms involved such as most and young and of the whole summary is determined using Zadeh's calculus of linguistically quantified propositions 14 In order to describe time-series data by linguistic summaries first we need to pre-process the data An analyzed period of time is partitioned into intervals in such a way that in each Janusz Kacprzyk and Anna Wilbik are with Systems Research Institute Polish Academy of Sciences ul Newelska 6 01-447 Warsaw Poland Email kacprzyk,wilbik  ibspan.waw.pl Slawomir Zadrozny is with Systems Research Institute Polish Academy of Sciences ul Newelska 6 01-447 Warsaw Poland 
and Warsaw Information Technology WIT ul Newelska 6 01-447 Warsaw Poland 
Email zadrozn ibspan.waw.pl 0-7803-971 9-3706/$20.00 2512006 IEEE 1 1 3 interval a certain trend might be observed in data In each such an interval the time-series is approximated by a line segment Then each interval is characterized by a number of numerical attributes concerning the corresponding line segment such as its slope length etc Finally linguistic summaries of the intervals described as above meant to represent trends in timeseries data are derived There is no generally adopted method for the generation of linguistic summaries Kacprzyk and Zadrozny 9 proposed the use of fuzzy association rules George and Srikanth 3 
as well as Kacprzyk and Strykowski 5 employed genetic algorithms to derive the summaries In what follows we will employ the latter approaches and adapt them to a specific setting of timeseries data II CHARACTERIZATION OF TIME SERIES In our approach time series data  Xi Yi  are approximated by a piece-wise linear function f such that for a given E  0 there holds Vi f\(xi y  1 There exist many algorithms that find such approximations Our starting point is the Sklansky and Gonzalez algorithm 12 that seems to be a good choice due to its simplicity and efficiency We modified it in the following way The algorithm 
constructs the intersection of cones starting from a point pi of the time series and including the circle of radius E around the subsequent data points Pi+j j 1 2   until this intersection becomes empty If for Pi+k the intersection is empty then the points pi Pi+\261 Pi+1kare approximated by a straight line segment and to approximate the remaining points we construct a new cone starting at Pi+k-1 Figure 1 presents the idea of the algorithm The family of possible solutions i.e straight line segments to approximate points P1 and P2 is indicated with a dark gray area To make it more intuitively appealing we will now present the algorithm 
as a pseudocode First denote by  p_0 
the current starting point  p_1 a pair of angles defining the cone constructed to check p_2 i.e the cone starting at point p_0 and inscribing the circle of radius E around the point p_2 7 cf 2 2 in Figure 1 
the last point successfully checked i.e for which the intersection of the cones starting at p_0 is non-empty  p_2 the next point to be checked  Alpha_01 a pair of angles yi/31 meant as an interval that defines the current cone as shown in Figure 1 indicated by light gray and dark gray area  Alpha 02 


 32 2 x Fig 1 An illustration of the algorithm 12 for an uniform approximation  function read-point   fetches the next data point  function f ind  finds a pair of angles defining the cone starting at point p-0 and inscribing the circle of radius Earound of the point p 2 The pseudocode of the procedure that extracts the trends is depicted in Figure 2 read point\(p_0 read-point p_1 do p_2  p_1 Alpha_02  Alpha_01  do find  Alpha_02 Alpha_01  Alpha_01 n Alpha_02 pl=p_2 read-point\(p_2 Alpha_02  find while\(Alpha_01 n Alpha_02  0 save_found_trendo p_O  pi1 p1 p2 Fig 2 Pseudocode of the procedure for extracting trends The bounding values of Alpha_02 Y2 32 computed by function find  are the slopes of the two lines such that  they are tangent to the circle of radius E around point p_2  they start at point p_0 Let AX  XO-\2432 and Ay  Yo Y2 Then the angles Y2 can be expressed by the formulas Y2  arctg  Ax 2 32  arctg AX AY  1 AX arcug AX 2 AY Then as an approximation of points po P p1 we assume either a single straight line segment chosen as e.g a bisector or one that minimizes the distance e.g assumed as the sum of squared errors SSE from the approximated points or the whole family of possible solutions i.e the segments of rays of the cone This method is fast as it requires only a single pass through the data We characterize the trends meant as behavior of a sequence of the straight line segments of the uniform 243approximation described above using the following three features  dynamics of change  duration and  variability In what follows we will briefly discuss these factors A Dynamics of change Under the term dynamics of change we understand the speed of changes It can be described by the slope of a line representing the trend cf any angle rq r C e   in Figure 1 Thus to quantify the dynamics of change we may use the interval of possible angles r C 90 90 or their trigonometric transformation However it might be impractical to use such a scale directly while describing trends Therefore we may use a fuzzy granulation in order to meet the users needs and task specificity The user may construct a scale of linguistic terms corresponding to various slopes steepnesses of a trend line as e.g  quickly decreasing  decreasing  slowly decreasing  constant  slowly increasing  increasing and  quickly increasing Clearly there may be more or less linguistic terms assumed but as it is well known the so-called Miller's magic number 7 261 2 is a good choice as it has a strong psychological motivation Figure 3 illustrates the lines corresponding to the particular linguistic terms In fact each term represents a fuzzy granule of directions In 1 2 there are presented many methods of constructing such a fuzzy granulation The user may define a membership function of a particular linguistic term depending on his or her needs 138  


f t quickly increasin increasing   slowlyincreasing constant t decreasing qtuickly  decreasing decreasing Fig 3 t Fig 4 Example of membership function describing the term long concerning the trend duration duration depend on the perspective assumed by the user He or she analyzing the data may adopt this or another time horizon implied by his or her needs The analysis may be part of a policy strategic or tactical planning and thus may require a global or local look respectively C Variability Variability refers to how much spread out in the sense of values taken on a group of data is There are five frequently used statistical measures of variability  the range maximum a The actual definitions of linguistic terms describing the dgo database e.g the set of workers minimum Although this range is computationally the easiest measure of variability it is not widely used as it is only based on two extreme data points This make it very vulnerable to outliers and therefore may not adequately describe a real variability  the interquartile range IQR calculated as the third quartile minus the first quartile that may be interpreted as representing the middle 50 of the data It is resistant to outliers and is computationally as easy as the range  the variance is calculated as 1/n Ei\(xix where x is the mean value  the standard deviation a square root of the variance Both the variance and the standard deviation are affected by extreme values and  the mean absolute deviation MAD calculated as 1 nEi xix  While it has a natural intuitive definition as the mean deviation from the mean the introduction of the absolute value makes analytical calculations using this statistics much more complicated We propose to measure the variability of a trend as a distance of data points covered by this trend from a linear uniform E-approximation that represents a given trend For this purpose we propose to employ a distance between a point and a family of possible solutions indicated as a dark gray cone in Figure 1 Equation 1 assures that the distance is definitely smaller than E We may use this information for the normalization The normalized distance equals 0 if the point lays in the dark gray area In the opposite case it is equal to the distance to the nearest point belonging to the cone divided by A visual representation of angle granules defining dynamics of change We map a single value r1 or the whole interval of the angles corresponding to the gray area in Figure 1 characterizing the dynamics of change of a trend identified using the algorithm shown in Figure 2 into a fuzzy set best matching a given angle Then we will say that a given trend is e.g decreasing to a degree 0.8 if Ildecreasing\(T1  0.8 where Idecreasing is the membership function of a fuzzy set representing the linguistic term decreasing that is a best match for the angle r1 characterizing the trend under consideration B Duration Duration describes the length of a single trend Again we will treat it as a linguistic variable An example of its linguistic labels is long defined as a fuzzy set whose membership function might be assumed as in Figure 4 where OX is the axis of time measured with units that are used in the time series data under consideration 1 I_ E Alternatively we may bisect the cone and then compute the distance between the point and this ray Again the measure of variability is treated as a linguistic variable whose values are linguistic terms labels modeled by fuzzy sets defined by the user III LINGUISTIC SUMMARIES AND THEIR APPLICATION TO TREND SUMMARIZATION A linguistic summary as presented in 10 11 is meant as a natural language-like sentence that subsumes the very essence of a set of data This set is assumed to be numeric and is usually large not comprehensible in its original form by the human being In Yager's approach cf Yager 13 Kacprzyk and Yager 7 and Kacprzyk Yager and Zadrozny 8 the following context for mining linguistic summaries is assumed  Y  Yi y y is a set of objects records in 


 A  A Am is a set of attributes characterizing objects from Y e.g salary age etc in a database of workers and Aj\(yi denotes a value of attribute Aj for object yi A linguistic summary of a data set D consists of  a summarizer P i.e an attribute together with a linguistic value fuzzy predicate defined on the domain of attribute Aj e.g low salary for attribute salary  a quantity in agreement Q i.e a linguistic quantifier e.g most  truth degree validity T of the summary meant as a proposition with fuzzy linguistic quantifiers that is calculated using Zadeh's calculus linguistically quantified propositions i.e a number from the interval 0,1 assessing the truth degree of the summary usually only summaries with a high value of T are interesting  optionally a qualifier R i.e i.e another attribute Ak together with a linguistic value fuzzy predicate defined on the domain of attribute Ak determining a fuzzy subset of Y e.g young for attribute age In what follows we will often for brevity identify summarizers and qualifiers with linguistic terms they contain In particular we will refer to membership functions Up or PR of the summarizer or qualifier as to be meant as membership functions of respective linguistic terms Thus a linguistic summary may be exemplified by T\(most of employees earn low salary  0.7 2 A richer form of a linguistic summary may include a qualifier e.g young as in e.g T\(most of young employees earn low salary  0.9 3 Thus basically the core of a linguistic summary is a linguistically quantified proposition in the sense of Zadeh 14 A linguistically quantified proposition corresponding to 2 may be written as Qy's are P 4 and the one corresponding to 3 may be written as QRy's are P 5 Then the component of a linguistic summary T i.e its truth degree directly corresponds to the truth degree of 4 or 5 This may be calculated by using either the original Zadeh's calculus of linguistically quantified propositions cf 14 or via other interpretations of linguistic quantifier based aggregations In order to characterize the summaries of trends we will refer to Zadeh's concept of a protoform cf Zadeh 15 16 Basically a protoform is defined as a more or less abstract prototype template of a linguistically quantified proposition Then summaries mentioned above might be represented by protoforms of the following form We may consider a short form of summaries Q trends are P 6 like e.g 140 Most of trends have a large variability  We may also consider an extended form of the summary represented by the following protoform QR trends are P 7 and exemplified by Most of slowly decreasing trends have a large variability Using Zadeh's 14 fuzzy logic based calculus of linguistically quantified propositions a proportional nondecreasing linguistic quantifier Q is assumed to be a fuzzy set in the interval 0,1 as e.g Q\(x  2x 0 for x>0.8 0.6 for 0.3  x  0.8 for x<0.3 8 The truth degrees from 0,1 of 6 and 7 are calculated respectively as T\(s  T\(Qy's are P  pQ  E UP Yi 9 T\(s  T\(QRy's are P  HQ E 1 PR yi A tip\(yi 10 Ei=l 1 R Yi  Both the fuzzy predicates P and R are assumed above to be of a rather simplified atomic form referring to just one attribute They can be extended to cover more sophisticated summaries involving some confluence of various attribute values as e.g slowly decreasing and short IV A GENETIC ALGORITHM The basic idea of a genetic algorithm GA was originally proposed by Holland 4 GA is inspired by mechanism of natural selection in which stronger individuals are more likely to win in a competitive environment Genetic algorithms are probabilistic non-deterministic algorithms in which populations of individuals are generated each individual being a potential solution of the problem under consideration The individuals are characterized by a set of parameters These parameters are regarded as genes of a chromosome and can be structured by a string of values in a binary form A genetic algorithm tries to find a best solution The quality of a solution is evaluated using a so called fitness function Thus the value of this function for a given individual known as a fitness value reflects the degree of goodness of the individual A new population of individuals is generated using genetic operators notably the mutation and crossover The mutation operator changes randomly alleles in every chromosome The crossover operator from two individuals called parents forms a new solution an offspring that has some genes from each of its parents The selection operator indicates individuals who will create a new population A 


Genetic algorithms involve a continuous adaptation of the population to the environment Unfortunately there is no guarantee that the process of adaptation will terminate providing a most desirable individual However under some conditions it is possible to prove that the probability of success increases and tends asymptotically to 1 We apply genetic algorithms to derive linguistic summaries in the following way Summaries are defined by their parameters i.e a quantifier a summarizer and sometimes a qualifier and are represented as chromosomes We will discuss the details on an example Let us consider the generation of summaries concerning the stock quotations on the Warsaw Stock Exchange where shares of almost 140 companies are quoted and traded We use the binary coding however the use of the integer coding is equally possible A chromosome representing an individual a linguistic summary may have the form Q R y p where Q is a coded value of a label of a quantifier R stands for a qualifier attribute and a label of its linguistic value The absence of the qualifier is encoded in a special way y is an encoded name of share or group of shares e.g high tech companies or banks P is a summarizer encoded similarly to a qualifier If we assume a simplistic situation that we characterize the extracted trends using three features only as discussed in Section II dynamics of change duration and variability and each factor as well as the quantifier is described by seven labels then there are about 2200 possible summaries In our examples there are approximately 400000 possible summaries The linguistic summaries may be generated for a data set concerning longer or shorter time horizon If various time horizons are to be considered during the generation then the number of possible summaries grows rapidly In the basic form of our approach we employed here the fitness function 9 in a simpler form without a qualifier T\(s  T\(Qy's are P  UQ  ZPI\(i or 10 in a extended form with a qualifier T\(s  T\(QRy's are P  linguistic labels low variability moderate variability and high variability There are 7 labels describing the quantifier very few few below half around half above half most and all Then a solution is coded with 13 bits as DDD DD DDD DD DDD Q R labelR P labelp For example 0011001011011 denotes a summary few long trends have a high variability In this basic version we propose to use the one point crossover operator Other crossover operators like the two point or uniform crossovers are also possible First a crossover point on the parent organism string is selected All data beyond that point in the chromosome string is swapped between the two parents yielding two offsprings as depicted in Figure 5 parents DDDDDDDDDF DDDD EEEEEEEE Eu offsprings DDDDDDDDD FlUME EEEEEEEEE1 DDD Fig 5 Example of the one point crossover Additionally the offspring is randomly mutated For each gene a random number is generated If its value is smaller than a certain probability of mutation then the value of this gene is changed from 0 to 1 or from 1 to 0 Afterwards the chromosome is checked if it represents a proper summary and if needed it is corrected For this simple example we have obtained a few summaries such as  most trends are very short with the truth degree 1.0  few trends are slowly decreasing with the truth degree 0.8609  over half trends with a high variability are constant with the truth degree 1.0  most very short trends have a low variability with truth value 0.7467 V CONCLUDING REMARKS Z=l\(i\(yi A iP\(,,V We proposed a simple yet efficient method for temporal n i PR v  data summarization We identify trends in time-series data L L 1 iI'R\(Yij zusing an effective and efficient modification of the algorithm where s is a summary proposed in 12 Then having trends spotted we propose We are interested in getting all summaries having the truth to characterize them with three basic attributes concerning degree high enough greater than a certain context-dependent their dynamics variability and duration as proposed in 6 threshold value defined by the user Thus all such summaries Finally the data set obtained is described by a set of linguistic are recorded at the end of each genetic algorithm iteration summaries in the sense of Yager To overcome numerical Let us consider the following toy example in which we difficulties associated with a need to maximize a highly nononly summarise the quotation of one investment fund We linear function we use a genetic algorithm assume the granulation of dynamics of change as depicted in The results of some preliminary attempts to linguistically Figure 3 The trends may be characterized as very short short summarize trends of share quotations at the Warsaw Stock average long or very long Variability is described by three1 Exchange seem to be promising 


New York Physica-Verlag 2001 concept of a protoform The next step will consist in using a more sophisticated form of the fitness function We will take into account the idea of so called constraint and constituent descriptors proposed by George and Srikanth 3 A further research will also focus on the search for other possible ways/attributes to characterize the trends REFERENCES 1 Batyrshin J and Yager R.R and Zadrozny S based approach to linguistic summaries of databases International Journal I On granular derivatives and the solution of to data mining Computer Science vol 10 J and Zadrozny S Fuzzy linguistic data summaries user adaptable solution B and Leiviska pp 33-154 2001 8 Kacprzyk as a human consistent of Applied Mathematics and Systems Exist Gabrys K and Strackeljan pp 813-834 2000 9 Kacprzyk pp 115-139 10 Kacprzyk pp 523-525 16 Zadeh L.A Toward L Perception Based Functions in Natural and Artificial American Fuzzy Information Processing Society NAFIPS 2002 2002 Computers and Mathematics with Applications vol 9 pp 3-12 6 Kacprzyk J H Adaptation to the summarization of data Information Sciences vol 28 to fuzzy quantifiers to adding deduction R and Srikanth R Data summarization using genetic algorithms and fuzzy logic Herrera F and Verdegay J L Eds Heidelberg:Physica-Verlag 1996 Systems Ann Arbor The University of Michigan Press 1975 5 Kacprzyk J and Strykowski P Linguistic data summaries for intelliJ and Wilbik A and S Zadrozny Linguistic summarization of trends an approach in press 7 Kacprzyk J and Yager R.R Linguistic summaries of data using fuzzy logic International Journal of General Systems vol 30 A fuzzy logic Pattern Recognition vol 12 no 5 pp 327-331 1980 13 in Proceedings of the Annual Meeting of the North J and Zadrozny S Fuzzy linguistic summaries via association rules J Eds Berlin Heidelberg New York Springer 2005 pp 321-339 12 Sklansky J and Gonzalez V Fast polygonal approximation of digitized curves Management Planning and Optimization Dortmund Germany 1990 capabilities to search engines the Sciences no 173 a granular initial value problem International Journal of Applied Mathematics and Computer Science vol 12 no 3 a generalized theory of uncertainty GTU outline Information Sciences vol 172 pp 403-410 2002 2 Batyrshin I and Sheremetov pp 599-611 4 Holland H Eds Heidelberg and pp 281-304 2005 11 Kacprzyk pp 69-86 1982 14 Zadeh L.A pp 149-184 1983 15 Zadeh L.A pp 11-40 2005 142 Yager R.R A new approach Last M and Bunke gent decision support in Qualitative Forecasting to appear 3 George in Genetic Algorithms and Soft Computing in Proceedings of EFDAN'99 4th European Workshop on Fuzzy Decision Analysis and Recognition Technology for in Data Mining and Computational Intelligence Kandel A and J and Zadrozny S Linguistic database summaries and their protoforms toward natural language based knowledge discovery tools Information in Do Smart Adaptive A computational approach in natural languages A prototype-centered approach 


6 Intelligent Aggregation of Purchase Orders in e-Procurement In this section we use concrete examples to show how the rule-based aggregation engine works with corporate agreement policy engine and negotiation service in the intelligent aggregation facility 6.1 Aggregation POs under Corporate Agreements In Section 3 we defined an information model for products in e-procurement In particular rules and constraints can be associated with attributes and between attributes of any product type This is an important extension to current XML schemas for product representation because it enables dynamic information rules and constraints to go together with static information product attributes Again we use plain entity-attribute representation instead of XML schemas for clarity while the conversion between them can be easily achieved We again use computing equipment as the type of goods an enterprise wants to purchase Also assume that a corporate agreement has been established between a buyer and a supplier Assume that the agreement states that packaging cost is included in the purchase price but shipping cost is extra It defines a pricing policy which consists of a set of rules for calculating price depending on product categories standard PC non-standard PC factory upgrade items and third party products A discount rate for standard products or a markup rate for third party products is defined for each category Buyer\222s total cost is the price adding the service and warranty cost and shipping cost as well as applicable taxes While the pricing policy looks complicated they can easily be represented as asetofrules 6.1.1 Purchase Orders Suppose we have the following purchase orders Larry from organization A requests for a standard desktop PC a 19\222 monitor and additional 256MB of memory Jon also from organization A requests for a standard desktop PC PO1 Attribute Buyer  Organization 223A\224 User 223Larry\224 Location 223PS\224 Attribute Supplier  mpany 223Dell\224 Catalog  http://\205/dell Attribute Product  223Optiplex GX150MT\224  a standard configuration desktop PC Attribute Quantity 1 Attribute BIN  11381  The required delivery date is a Range constraint any date within the next 30 days Attribute Required-Delivery-Date  today today+30 days Attribute S&H1 query\(UPS Product   a COMPUTED value say it\222s 39.95 Attribute Value  query\(Catalog Product a COMPUTED value Attribute Price1  Attribute Total1  Inter-attribute constraints in PO1  Pricing rule for standard configuration from corporate agreement discount_rate x Price1  Value  1-x Total1  Price1+S&H1\1.088 assume 8.8 tax rate PO2 Attribute Buyer Organization 223A\224 User 223Larry\224 Location 223PS\224 Attribute Supplier  mpany  223Dell\224 Catalog  http://\205/dell Attribute Product  Monitor Size 19 Manufacturer Mitsubishi Attribute Quantity 1 Attribute BIN  11381 Attribute Required-Delivery-Date  today today+30 da    a R a nge c o nst r a i nt Attribute S&H2  query\(UPS Product   say it\222s 29.95 Attribute Value query\(Catalog Product a COMPUTED value Attribute Price2  Attribute Total2  Inter-attribute Constraints in PO2  PO2\222s delivery date must be the same as PO1\222s delivery date 1 PO2.Required-Delivery-Date  PO1.RequiredDelivery-Date  Pricing rule for THIRD_PARTY component from corporate agreement markup_rate y 2 Price2  Value  1+y 3 Total2  Price2  S&H2  1.088  assume 8.8 tax rate PO3 Attribute Buyer Organization 223A\224 User 223Larry\224 Location 223PS\224 Attribute Supplier  mpany  223Dell\224 Catalog  http://\205/dell Attribute Product  Memory Size 256 Unit 223MB\224 Attribute Quantity 1 Proceedings of the 2005 Ninth IEEE International ED OC Enterprise Computing Conference \(EDOC\22205 0-7695-2441-9/05 $20.00 \251 2005  IEEE 


Attribute BIN  11381 Attribute Required-Delivery-Date  today today+30 da    a R a nge c o nst r a i nt Attribute S&H3  query\(UPS Product   say it\222s 9.95 Attribute Value query\(Catalog Product a COMPUTED value Attribute Price3  Attribute Total3  Inter-attribute Constraints in PO3  PO3\222s delivery date must be the same as PO1\222s delivery date 1 PO3.Required-Delivery-Date  PO1.RequiredDelivery-Date  Pricing rule for additional parts from corporate agreement discount_rate z 2 Price3  Value  1-z 3 Total3  Price3  S&H3  1.088  assume 8.8 tax rate PO4 Attribute Buyer  Organization 223A\224 User 223Jon\224 Location 223PS\224 Attribute Supplier  mpany  223Dell\224 Catalog  http://\205/dell Attribute Product  223Optiplex GX150MT\224  a standard configuration desktop PC Attribute Quantity 1 Attribute BIN  11381 Attribute Required-Delivery-Date  today today+45 days Attribute S&H4  query\(UPS Product   say it\222s 39.95 Attribute Value  query\(Catalog Product a COMPUTED value Attribute Price4  Attribute Total4 Inter-attribute constraints in PO4  Pricing rule for standard configuration discount_rate x Price4  query\(Catalog Product   1-x Total1  Price4  S&H4  1.088  assume 8.8 tax rate 6.1.2 Attribute-based PO Aggregation While an enterprise may generate millions of POs every year the number of distinct products and services the enterprise purchases is actually much smaller in the hundreds rather than in the millions This means that many POs are purchases of the same product or service Attribute-based PO Aggregation technique combines POs based on their common attributes values or types trying to achieve a business goal in doing so Business goals are parameterized Reducing cost is an important parameter for achieving business goals The rule-based aggregation engine in the intelligent aggregation facility Figure 2 attempts to combine the POs and see whether the combined PO can better achieve business goals e.g reducing cost It queries the Corporate Agreement policy engine for discount rates of different product categories r example by aggregating PO1 PO2 and PO3 based n their common User name 223Larry\224 would create the following PO PO123 Attribute Buyer  Organization 223A\224 User 223Larry\224 Location 223PS\224 Attribute Supplier  mpany  223Dell\224 Catalog  http://\205/dell Attribute Product  223Optiplex GX150MT\224 223Monitor\224 223Memory\224 Attribute Quantity 1 Attribute BIN  11381  The required delivery date is a Range constraint any date within the next 30 days Attribute Required-Delivery-Date  today today+30 days Attribute S&H query\(UPS Product   say it is 59.95 Attribute Value1  query\(Catalog Produc.1t  Attribute Value2  query\(Catalog Product.2  Attribute Value3  query\(Catalog Product.3  Attribute Price Attribute Total  Inter-attribute constraints in PO123 Price  Value1  1-x  Value  1+y  Value3 1-z Total  Price  S&H  1.088 Total  Total1  Total2  Total3 This constraint implies that the total cost of this aggregate PO must not more than the sum of the costs of individual POs In this particular example the obvious saving is the shipping cost 39.95+29.95+9.95-59.95\1.088  21.65 Similarly PO1 PO2 PO3 and PO4 could be aggregated by their common Organization name 223A\224 PO14  aggregation of PO1 and PO4 Attribute Buyer  Organization 223A\224 User 223Larry\224 223Jon\224 Location 223PS\224 Attribute Supplier  mpany  223Dell\224 Catalog  http://\205/dell Attribute Product  223Optiplex GX150MT\224 Attribute Quantity 2 Attribute BIN  11381 Proceedings of the 2005 Ninth IEEE International ED OC Enterprise Computing Conference \(EDOC\22205 0-7695-2441-9/05 $20.00 \251 2005  IEEE 


 The required delivery date is a Range constraint any date within the next 30 days Attribute Required-Delivery-Date  today today+30 days Attribute S&H query\(UPS Product   say it is 59.95 Attribute Value  query\(Catalog Product  Attribute Price  Attribute Total  Inter-attribute constraints in PO14 Price  Quantity  Value  1-x Total  Price  S&H  1.088 Total  Total1  Total4 In this case the Quantity attribute value has changed to 2 by adding both requests together Furthermore the Delivery-Date attribute value is a result of finding a common range of the two The obvious saving in this case is 2*$39.95 59.95\1.088  21.71 Whether a bunch of POs should be aggregated in a particular way depend on whether costing savings can be achieved while satisfying all the constraints 6.2 Intelligent Aggregation of Purchase Orders in e-Procurement with Negotiations Aggregation under dynamic negotiation is harder because supplier side could be revising its own strategies and parameters on the fly While human intervention in the aggregation process is possible we focus on automated aspects of the aggregation in this paper Suppose we have a simple supplier side rule buy one and get second one half price from LT a supplier of mice keyboard and trackball Suppose we have requests to buy Mice as follows PO5 Attribute Buyer  Organization 223B\224 User 223Joe\224 Location 223PS\224 Attribute Supplier  mpany 223LT\224 Catalog  http://\205/LT Attribute Product  223Optical Mouse\224 Attribute Quantity 1 Attribute Required-Delivery-Date  01/21/05 02/21/05   a r an g e of dat e s  order dat e  deadline d Attribute S&H9  query\(UPS Product   say it\222s 4.95 Attribute Value  query\(Catalog Product  Attribute Price10  Value  say it\222s 29.95 Attribute Total10  Total10  Price10  S&H10  1 tax rate results in a value 29.95  4.95\1.088  39.97 PO6 Attribute Buyer  Organization 223C\224 User 223Al\224 Location 223PS\224 Attribute Supplier  mpany 223LT\224 Catalog  http://\205/LT Attribute Product  223Optical Mouse\224 Attribute Quantity 1 Attribute Required-Delivery-Date  01/25/05 02/28/05   a r an g e of dat e s  order dat e  deadline d Attribute S&H10  query\(UPS Product   say it\222s 4.95 Attribute Value  query\(Catalog Product sayit\222s 29.95 per mouse Attribute Price10 Value Attribute Total10  Total10  Price10  S&H10  1 tax rate results in a value 29.95  4.95\1.088  39.97 6.2.1 PO Aggregation Under Negotiation The rule-based aggregation engine uses the Negotiation service to understand supplier\222s offers and tries to take advantage of the terms in the offers For example by aggregating PO5 and PO6 can be aggregated as follows PO56 Attribute Buyer  Organization B C User 223Joe\224 223Al\224 Location 223PS\224 Attribute Supplier  mpany 223LT\224 Catalog  http://\205/LT Attribute Product  223Optical Mouse\224 Attribute Quantity 2 Attribute Required-Delivery-Date  01/25/05 02/21/01 Attribute S&H10  query\(UPS Product   say it\222s 4.95 Attribute Value  query\(Catalog Product sayit\222s 29.95 per mouse Attribute Price10 1.5*Value Attribute Total10  Total10  Price10  S&H10  tax  1.5 29.95  4.95\1.088  54.26 A saving of 39.97  2 54.26  25.68 or over 32 of savings Note the changes of the 223Quantity\224 and 223RequiredDelivery-Date\224 attributes after aggregation The quantities are added up and the required delivered dates are merged for a common range Due to the constraints on object attributes aggregation may require complex constraint solving Proceedings of the 2005 Ninth IEEE International ED OC Enterprise Computing Conference \(EDOC\22205 0-7695-2441-9/05 $20.00 \251 2005  IEEE 


7 Conclusions and Future Work This paper describes an Intelligent Aggregation facility in enterprise e-Procurement process This facility introduces an information model a rule-based aggregation engine corporate agreement policies and negotiation in aggregating large volume of POs in enterprise eprocurement to reduce cost and maximize efficiency This information model includes extensive use of constraints for and among attributes in a PO These constraints guard the integrity of POs as they are aggregated The intelligent aggregation facility can be inserted as a value-added service in the enterprise e-Procurement workflow An enterprise generates millions of POs every year but the number of distinct products and services the enterprise purchases is actually much smaller in the hundreds rather than in the millions This presents cost saving opportunities by aggregating POs that makes best use of terms and conditions in corporate agreements or supplier offers Some concrete examples are used to show the idea of automated aggregation and the opportunities in reducing procurement cost As millions of POs are generated even a small percentage of savings would mean substantial savings for large enterprises The ideas described in this paper have not been fully implemented in our prototype One area needs more work is the formal representation of policies in corporate agreements which would allow the aggregation engine to automatically explore aggregation opportunities before POs are made to suppliers Another is the semantic model of products which would enable more semantics-based aggregation of POs 8 References  e bX M L  h t t p   w w w ebxml  org  2 e n g  J  S u  S  Y  W  L a m H   a n dH e l a l S   223Achieving Dynamic Inter-Organizational Workflow Management by Integrating Business Processes Events and Rules,\224 Proceedings of the 35th Hawaii International Conference on System Sciences HICSS35 Hawaii USA January 2002 3 u  S  Y  W  L a m H   L e e  M  B a i S   a n dS h e n  Z   An Information Infrastructure and E-services for Supporting Internet-based Scalable E-business Enterprises Proceedings of the 5th International Enterprise Distributed Object Computing Conference Seattle Washington USA September 2001 4 S u S Y  W   H ua ng C  H a mme r J   H u a ng Y   L i  H   Wang,L.,LiuY.,Pluempitiwiriyawej,C.,Lee,M and Lam H 223An Internet-based Negotiation Server for E-commerce,\224 VLDB Journal Vol 10 No 1 2001 pp.72-90 5 M o r r i s S l o m a n  223 P o l i c y D ri v e n M an ag em e n t f o r Distributed Systems\224 Journal of Network and Systems Management Plenum Press Vol 2 No 4 1994  M aarten S teen  J oh n D errick  223 For m ali s ing ODP Enterprise Policies\224 Proceedings of the 3 rd International nterprise Distributed Object Computing Conference Mannheim Germany IEEE CS Press September 1999  J am e s H a ns on  Z oran M i l o s e v i c 223 C o n v e r s at i onOriented Protocols for Contract negotiations\224 Proceedings of the 7th International Enterprise Distributed Object Computing Conference Brisbane Australia IEEE CS Press September 2003  S  N eal J  C ole P.F L i n i ng ton  Z  Milose v i c S Gibson S Kulkarni 223Identifying Requirements for Business Contract Language a Monitoring Perspective\224 Proceedings of the 7th International Enterprise Distributed Object Computing Conference Brisbane Australia IEEE CS Press September 2003 9 T  D im itrak o s  I  D j o rd j e v i c Z  Milo sev i c A  J o san g  C Phillips 223Contract Performance Assessment for Secure and Dynamic Virtual Collaborations\224 Proceedings of the 7th International Enterprise Distributed Object Computing Conference Brisbane Australia IEEE CS Press September 2003 Proceedings of the 2005 Ninth IEEE International ED OC Enterprise Computing Conference \(EDOC\22205 0-7695-2441-9/05 $20.00 \251 2005  IEEE 


absolute values. The results can vary on other computers. But it can be guaranteed that performance ratio of the algorithms will remain the same After making the comparisons with sample data, we came to the conclusion that PD algorithm performs significantly better than the other two especially with larger datasets. PD outperforms DCP and PIP regarding running time. On the other hand, since PD reduces the dataset, mining time does not necessary increase as the number of transactions increases and experiments reveals that PD has better scalability than DCP and PIP. So, PD has the ability to handle the large data mine in practical field like market basket analysis and medical report documents mining 5. References 1] R. Agrawal and R. Srikant, "Fast algoritlnns for mining association rules", VLDB'94, pp. 487-499 2] R. J. Bayardo, "Efficiently mining long patterns from databases", SIGMOD'98, pp.85-93 3] J. Pei, J. Han, and R. Mao, "CLOSET: An Efficient Algorithm for Mining Frequent Closed Itemsets \(PDF Proc. 2000 ACM-SIGMOD International Workshop on Data Mining and Knowledge Discovery, Dallas, TX, May 2000 4] Qinghua Zou, Henry Chiu, Wesley Chu, David Johnson, "Using Pattern Decomposition\( PD Finding All Frequent Patterns in Large Datasets", Computer Science Department University of California - Los Angeles 5] J. Han, J. Pei, and Y. Yin, "Mining Frequent Patterns without Candidate Generation \(PDF  SIGMOD International Con! on Management of Data SIGMOD'OOj, Dallas, TX, May 2000 6] S. Orlando, P. Palmerini, and R. Perego, "The DCP algoritlnn for Frequent Set Counting", Technical Report CS2001-7, Dip. di Informatica, Universita di Venezia 2001.Available at http://www.dsi.unive.itl?orlando/TR017.pdf 7] MD. Mamun-Or-Rashid, MD.Rezaul Karim, "Predictive item pruning FP-tree algoritlnn", The Dhaka University  Journal of Science, VOL. 52, NO. 1, October,2003, pp. 3946 8] Park, J. S., Chen, M.-S., and Yu, P. S, "An Effective Hash Based Algoritlnn for Mining Association Rules", Proc ofthe 1995 ACM-SIGMOD Con! on Management of Data 175-186 9] Brin, S., Motwani, R., Ullman, J., and Tsur, S, "Dynamic Itemset Counting and Implication Rules for Market Basket Data", In Proc. of the 1997 ACM-SIGMOD Conf On Management of Data, 255-264 10] Zaki, M. J., Parthasarathy, S., Ogihara, M., and Li, W New Algoritlnns for Fast Discovery of Association Rules In Proc. of the Third Int'l Con! on Knowledge Discovery in Databases and Data Mining, 283-286 11] Lin, D.-I and Kedem, Z. M., "Pincer-Search: A New Algoritlnn for Discovering the Maximum Frequent Set", In Proc. of the Sixth European Conf on Extending DatabaseTechnology, 1998 12] R. Ramakrishnan, Database Management Systems University of Wisconsin, Madison, WI, USA; International Edition 1998 pre></body></html 


tors such as union, di?erence and intersection are de?ned for pairs of classes of the same pattern type Renaming. Similarly to the relational context, we consider a renaming operator ? that takes a class and a renaming function and changes the names of the pattern attributes according to the speci?ed function Projection. The projection operator allows one to reduce the structure and the measures of the input patterns by projecting out some components. The new expression is obtained by projecting the formula de?ning the expression over the remaining attributes [12 Note that no projection is de?ned over the data source since in this case the structure and the measures would have to be recomputed Let c be a class of pattern type pt. Let ls be a non empty list of attributes appearing in pt.Structure and lm a list of attributes appearing in pt.Measure. Then the projection operator is de?ned as follows ls,lm c id s m f p ? c, p = \(pid, s, d,m, f In the previous de?nition, id ing new pids for patterns, ?mlm\(m projection of the measure component and ?sls\(s ned as follows: \(i s usual relational projection; \(ii sls\(s and removing the rest from set elements. The last component ?ls?lm\(f computed in certain cases, when the theory over which the formula is constructed admits projection. This happens for example for the polynomial constraint theory 12 Selection. The selection operator allows one to select the patterns belonging to one class that satisfy a certain predicate, involving any possible pattern component, chosen among the ones presented in Section 5.1.1 Let c be a class of pattern type pt. Let pr be a predicate. Then, the selection operator is de?ned as follows pr\(c p Join. The join operation provides a way to combine patterns belonging to two di?erent classes according to a join predicate and a composition function speci?ed by the user Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE Let c1 and c2 be two classes over two pattern types pt1 and pt2. A join predicate F is any predicate de?ned over a component of patterns in c1 and a component of patterns in c2. A composition function c pattern types pt1 and pt2 is a 4-tuple of functions c cStructureSchema, cDataSchema, cMeasureSchema, cFormula one for each pattern component. For example, function cStructureSchema takes as input two structure values of the right type and returns a new structure value, for a possible new pattern type, generated by the join. Functions for the other pattern components are similarly de?ned. Given two patterns p1 = \(pid1, s1, d1,m1, f1 p2 = \(pid2, s2, d2,m2, f2 p1, p2 ned as the pattern p with the following components Structure : cStructureSchema\(s1, s2 Data : cDataSchema\(d1, d2 Measure : cMeasureSchema\(m1,m2 Formula : cformula\(f1, f2 The join of c1 and c2 with respect to the join predicate F and the composition function c, denoted by c 1   F  c  c 2   i s  n o w  d e  n e d  a s  f o l l o w s    F  c  c 2     c  p 1   p 2   p 1    c 1  p 2    c 2  F   p 1   p 2     t r u e   5.1.3. Cross-over database operators OCD Drill-Through. The drill-through operator allows one to 


Drill-Through. The drill-through operator allows one to navigate from the pattern layer to the raw data layer Thus it takes as input a pattern class and it returns a raw data set. More formally, let c be a class of pattern type pt and let d be an instance of the data schema ds of pt. Then, the drill-through operator is denoted by c c Data-covering. Given a pattern p and a dataset D sometimes it is important to determine whether the pattern represents it or not. In other words, we wish to determine the subset S of D represented by p \(p can also be selected by some query the formula as a query on the dataset. Let p be a pattern, possibly selected by using query language operators, and D a dataset with schema \(a1, ..., an ible with the source schema of p. The data-covering operator, denoted by ?d\(p,D responding to all tuples in D represented by p. More formally d\(p,D t.a1, ..., t.an In the previous expression, t.ai denotes a speci?c component of tuple t belonging to D and p.formula\(t.a1, ..., t.an instantiated by replacing each variable corresponding to a pattern data component with values of the considered tuple t Note that, since the drill-though operator uses the intermediate mapping and the data covering operator uses the formula, the covering ?\(p,D D = ?\(p not be equal to D. This is due to the approximating nature of the pattern formula 5.1.4. Cross-over pattern base operators OCP Pattern-covering. Sometimes it can be useful to have an operator that, given a class of patterns and a dataset, returns all patterns in the class representing that dataset \(a sort of inverse data-covering operation Let c be a pattern class and D a dataset with schema a1, ..., an pattern type. The pattern-covering operator, denoted as ?p\(c,D all patterns in c representing D. More formally p\(c,D t.a1, ..., t.an true Note that: ?p\(c,D p,D 6. Related Work Although signi?cant e?ort has been invested in extending database models to deal with patterns, no coherent approach has been proposed and convincingly implemented for a generic model There exist several standardization e?orts for modeling patterns, like the Predictive Model Markup Language \(PMML  eling approach, the ISO SQL/MM standard [2], which is SQL-based, and the Common Warehouse Model CWM  ing e?ort. Also, the Java Data Mining API \(JDMAPI 3] addresses the need for a language-based management of patterns. Although these approaches try to represent a wide range of data mining result, the theoretical background of these frameworks is not clear. Most importantly, though, they do not provide a generic model capable of handling arbitrary cases of pattern types; on the contrary only a given list of prede?ned pattern types is supported To our knowledge, research has not dealt with the issue of pattern management per se, but, at best, with peripheral proximate problems. For example, the paper by Ganti et. al. [9] deals with the measurement 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


