K-Means Divide and Conquer Clustering Madjid Khalilian, Farsad Zamani Boroujeni, Norwati Mustapha Md. Nasir Sulaiman  Universiti  Putra Malaysia \(UPM\lty of Computer Science and Information Technology\(FSKTM Selangor Darul Ehsan, Malaysia Ma.khalilian@gmail.com  frs_zamani@yahoo.com  norwati@fsktm.upm.edu.my  nasir@fsktm.upm.edu.my    Abstract Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. Most clustering techniques ignore the fact about the different size or levels – where in most cases, clustering is more concern with grouping similar objects or samples together ignoring the fact that even though they are similar, they might be of different levels. For really large data sets, data reduction should be performed prior to applying the data-mining techniques which is usually performing dimension reduction, and the main question is whether some of these prepared and preprocessed data can be discarded without sacrificing the quality of results Existing clustering techniques would normally merge small clusters with big ones, removing its identity. In this study we propose a method which uses divide and conquer technique to improve the performance of the K-Means clustering method  Keywords Clustering, K-Means, Euclidean Space, Object Similarity High Dimensional Data  I  I NTRUDUCTION Clustering aims to partition a set of records into several groups; such that “similar” records are to be in the same group according to some similarity function and identifying similar subpopulations in the data. The samples in one group are similar and samples belonging to different groups are not similar for example a cluster could be a group of customers with similar purchase histories interactions, and other factors. Samples for clustering are represented as a vector of measurements, or as a point in a multidimensional space.  The similarity between two vectors, for example X and Y, is computed using a similarity measure, such as the cosine Y X Y X Y X s t 000G 000G 000G 000G 000G 000G 002      t X 000G is a transposition of vector X 000G  X 000G is the Euclidean normal of vector X 000G  Similar samples will have small value for the cosine; i.e the angle between their vectors will be small Many studies have been done in clustering are more focused on the distance but not the level or size of the samples. Samples might be of the same group but different level – for example fisherman and fishing company. In human related data sets it could be educational level, level of income, wealth and so on. Fig.1 shows two similar objects based on cosine similarity but they are of different in terms of level/size There is a lack of studies in large datasets with high dimensionality. Most studies have been done with high dimensional data are proposed the number of dimensions However, they suffer from the inaccuracy and unreliability of the result. By working with smaller subspaces, we can use Euclidean distance without worrying about the complexity and system resources. Thus we are able to improve the existing clustering techniques in terms of efficiency and accuracy   Fig.1. An example of two similar vectors with different levels Our proposed method is not to reduce dimension but to select subspaces by clustering and perform clustering based on these subspaces. We expect that it would be able to cluster samples of similar size and level. This method can be more efficient and accurate than a single one pass clustering. Some hypothesis has been considered   H1 - Proposed method would be able to group samples of similar size and find similarity among them   H2 - Proposed method is faster than single step clustering due to use of divide and conquers technique   H3 - Proposed method is more accurate than a single step clustering   H4 – Proposed method would allow Euclidean distance to be used in high dimensional data In this study we assume that the space is orthogonal and dimensions for all objects are the same and finally we use ordinal data type because of the application. Next section describes related works and in continue framework and methodology will be explained. Finally results of experiments is presented and discussed about it II  R ELATED WORKS There is a general categorization for high dimensional data set clustering 1-Dimension reduction, 2-Parismonious models, 3-Subspace clustering r e s e lection an d f eat u r e ex traction are m o s t  popular techniques in dimension reduction. It is clear that in both methods we will have losing information which naturally affects accuracy. Ref. [2 rev i e w ed th e literat u re o n  parsimonious models and Gaussian models from the most complex to simplest which yields a method similar to the KMeans approach. When we have low dimensional spaces these methods aren’t able to work well. There are two main approaches for subspaces methods: in first class centers are 
International Conference on Computer and Automation Engineering 978-0-7695-3569-2/09 $25.00 © 2009 IEEE DOI 10.1109/ICCAE.2009.59 306 


considered on a same unknown subspace and in second each class is located on specific subs T h e idea of s u btopics  or subgroups is appropriate for document clustering and text mining e n s or f actorizat ion as a po w e rfu l techn i qu e  h a s  been used in con s is te n c y has been shown in those public data sets because of outlier Ref opos ed a f r a m ew ork  w h ic h in te g r ates su bs p ace selection and clustering. Equivalency between kernel K-Means clustering and iterative subspace selection has been shown Ref ed a g e n e ral cl u s t e ri n g i m prov er s c h e m e  w h i c h  is included two main steps. In first step it uses intrinsic properties of the data set for dimension reduction after that several iteration of a clustering algorithms are applied, each with different parameter. Base on BIC criterion the best result will be selected. There are some weaknesses for this method e.g. since BIC fits a model to specific data distribution it can not be used to compare models of different data sets. ref. [8  presented a semi-supervised clustering method base on spherical K-Means via feature projection which is tailored for handling sparse high dimensional data. They first formulated constraint-guided feature projectio n then applied the constraint spherical K-Means algorithm to cluster data with reduced dimension Ref os ed t w o m e t h ods of co m b i n i n g obj ect i v e function clustering and graph theory clustering   Method 1: incorporates multiple criteria into an objective function according to their importance, and solves this problem with constrained nonlinear optimization programming   Method 2: consists of two sequential procedures a\ traditional objective function Clustering for generating the initial result b\n auto associative additive system based on graph theory clustering for modifying the initial result  Ref. [10 P r o p o s ed seq u en tia l co m b in atio n  m e th o d s  f o r d a ta clustering   In improving clustering performance they proposed the use of more than one clustering method   They investigated the use of sequential combination clustering as opposed to simultaneous combination and found that sequential combination is less complex and there are improvements without the overhead cost of simultaneous clustering In clustering points lying in high dimensional spaces formulating a desirable measure of “similarity” is more problematic. Recent research shows that for high dimensional spaces computing the distance by looking at all the dimensions is often useless, as the farthest neighbor of a point is expected to be almost as close as its nearest neigh  To compute clusters in different lower dimensional subspaces, recent work has focused on projective clustering  defined as follows: given a set P of points in R d and an integer K, partition into subsets that best classify into lower dimensional subspaces according to some objective function Instead of projecting all the points in the same subspace, this allows each cluster to have a different subspace associated with it  Pro pos ed m odel  w i l l be m o re ef f ect i v e an d ach i e v e s significant performance improvement over traditional method for most clustering. It will be able to cluster samples of similar size and level and be more efficient and accurate than a single one pass clustering. There are some delimitations for this method. First space should be orthogonal it means there is no correlation among attributes of an object. Second base on application that is used all attributes in an object have the same kind of data types. Without losing generality it is possible to extend proposed method to other kinds of data types. Samples are considered in a same number of dimensions In this paper we present K-means divide and conquer clustering by select subspaces by clustering and perform clustering based on these subspaces  III  R ESEARCH MODEL In this study, our framework includes five main steps as shown in fig.2. After preprocessing and normalization we apply 000 2 i x for each object to calculate the object’s length After calculating the object’s length, we will cluster the objects base on their length and in final step we separately cluster each group based on their features 1\ Preprocessing   The data has to be cleaned before we do the mining Since the data is obtained from a survey in a Persian Website, some of the labels need to be renamed   There are also values which are outside the range. The range is supposed to be between 1 to 5   If it is less than 1, set to 1   If it is greater than 5, set to 5   There are also missing or unfilled values   Ignore the object by removing it   Assign 1 to cell   Fig. 2.  Framework for clustering high dimensional data set  2\ Normalization The data are of ordinal type therefore it has to be normalized by using the following formula  1. Preprocessing 2. Normalize Ordinal Data 3. Apply x 2 4. Divide subspace clustering 5. Cluster the Subspace clustering 1 1    f old new m V V 
307 


  Where V new is the normalized value V old is the previous value which we want to be normalized and m f is the maximum value of the considered range 3 Combinational method for Clustering In this step 000 2 i x is used to calculate the object size then we put the samples with the same size/level into the same cluster 4 Divide subspace clustering  In this step we divide the data set into a number of subspaces. Fig.2 shows that subspaces will be further clustered based on their features. It should be noted that in this step we separate objects based on their size not their features or attributes  5\bspace clustering Finally, we cluster each subspace that was created by the previous K-means clustering. After the second clustering we should have objects of almost the same size/level and similarity value in the same cluster IV  M ETHODOLOGY Using the frame work that we mentioned before, we developed an experiment for evaluating our method. We have conducted a comparison of data mining tools to select a suitable platform for conducting our experiments. We decided to use SPSS Clementine against its alternatives such as Oracle Data Mining and WEKA platforms. SPSS Clementine 12.0 is a good support for visual analysis and enabled us to query records that belong to each cluster. In case of our experiment SPSS Clementine 12.0 supports wider range of algorithms and has better data preparation    Fig. 3.  Proposed clustering step    Fig.4. Building a K-Means model \(first clustering     Fig.5. Building a K-Means model \(second clustering  V  E XPERIMENTAL PROCEDURE In our study a data set from a Persian website www.peivand.org\as been used. In this web site there are some psychology questions which are answered by users. Base on user’s answers a table is extracted that includes different features of user’s personality. For each user 80 feature is recorded, each of which is ranked between 1 and 5. Based on this dataset \(which is included 750 records\n first clustering step we will find people who are generally in the same level but different personality. In second clustering step people who have similar personality is found VI  R ESULTS AND DISCUSSION In this section, we report and discuss the results of our experiments. For evaluating the effect of utilizing our proposed method, we compared the number of iterations required to converge the K-Means algorithm. In the first experiment, the number of clusters is 8 and clustering has been done without any grouping. The error is defined as the summation of the sample distances from the center of each cluster 
308 


 Fig. 6. The results of the K-Means clustering without subspaces  In second experiment, the proposed method has been used  Fig. 7. The results of the proposed method   In the first step, objects categorized to four groups base on their length and each group clustered base on similarity. By comparing the results of these two experiments, we observe that the number of iterations in the second experiment is reduced. It means that with the same number of clusters in both experiments we have less iteration in proposed method. On the other hand we are able to extract clusters with small number of members whereas they are merged with bigger clusters in regular K-Means clustering. Thus we can conclude that the proposed method is more accurate and efficient than original K-Means clustering method VII  C ONCLUSIONS AND FUTURE WORKS Using two steps clustering in high dimensional data sets with considering size of objects helps us to improve accuracy and efficiency of original K-Means clustering. When objects are clustered base on their size, in fact, we are using subspaces for clustering. It causes achieving more accurate and efficient results. For this purpose we should consider orthogonal space which means that there should be no correlation among attributes of objects and size of dimension should be equal in all objects For future work we will investigate applying this method in different domain e.g. text mining. Also effects of different parameters in clustering for example K and number of dimensions should be studied   R EFERENCES   Bou v e y r an S. Girard , C  Schm id : T ech n i cal R e port 1083M, LMC-IMAGE 2008   F r al ey a n d A   R a ft er y   m odel bas e d cl u s t e ri n g   discriminate analysis and density estimation, journal of American statistics association, 97, 611-631 ,2002  H B o ck on t h e i n t e rf ace b e t w e e n cl u s t e r an al y s i s  p r i n ci p a l   component clustering and multidimensional scaling, 1987 4 J e f f e r y L  Solk a  te x t da ta  m i ning T h e o r y a nd m e thods   vol 2,  94-112, 2008 5 H  H u a n g  C  D i ng D  L u o T  L i s i m u lta ne ous te ns or s u bs pa c e  selection and clustering, KDD08, lasvegas USA ,2008 6 J  Y e Z. Z h a o  M W u dis c r i m i na tiv e K Me a n s  f o r c l us te ring  proceeding of the annual conference 2007  Vars h a v s ky D. itorn a n d M. L i n i al, clus ter alg o rit h m   optimizer: A framework for large datasets , ISBRA pp 85 96,springer 2007 8 W  T a ng H  X i o ng S  Z h a ng J  W u  E nha nc i ng se mi  supervised clustering, KDD07 california USA , ACM 2007 9 Y unt a o Q i a n a n d Chi n g Y  Sue n  C l u st e r i ng Co m b i n a t i o n  Method, IEEE 2000  n t ao Qian an d C h i n g Y. Su e n Sequ e n tial co m b i n ati o n  methods for data clustering, Journal of computer science,2002   R a f t er y a n d N. Dean  v a riable s e lection  f o r m odel based clustering, journa of American statistical association, 101\(473\168-178,2006 12  P a n k a j K. A g ar w a l a n d Nab il H. Mu sta f a, KMea n s Projective clustering, PODS Paris France, ACM 2004  
309 


experiments, good candidates were proposed in s oc a lle d Big-Bang embedding algorithm\ an a f a st  m e t h od f o r creating 2D representations of large graphs, which consists of two steps: embedding the graph in a very high dimension usually associated with the number of nodes- and then project it into the 2D plane using principal components analysis In this step we embed the graphs SN  t 0  SN  t 1  SN  t n n Euclidean 2D space, where each node is represented by a point with given coordinates. The resulting sets of points SN 0  SN 1  SN n represent the temporal network images Embedding algorithm assures us that the Euclidean distances between points \(nodes\ fit in the best possible way the distances in a social space \(relation strengths in original graphs\ the representation of social system in which the network is seen as an assembly of N particles \(the nodes of social network\. The procedure we have chosen for our experiments was a classic multidimensional scaling algorithm \(MDS  ltidi m ension al scali ng defines a call of methods often used in information visualization and exploring similarities or dissimilarities in data. An MDS algorithm starts with a matrix of similarities between objects \(similarity relation doesn’t have to be symmetrical\en assigns a location of each item in a lowdimensional space. In other words – it estimates the coordinates of a set of objects in a space of specified dimensionality on the basis of measuring the distances which, however do not have to be metric\ between pairs of objects. A variety of models can be used that include different ways of computing distances and various functions relating the distances to the actual data. We computed a square non-symmetical \(because our graph is directed matrix of the distances between the nodes in social graph as an input for the MDS procedure C Setting up the dynamic molecular model Because the sets of network nodes in SN  t 0  SN  t 1  SN  t n are equal, each point \(node\ represented in any of the sets SN 0  SN 1  SN n In fact, in email-based social network it doesn’t have to be so, but to check our model we excluded users which were not continually active \(about 10 of all\from our analysis. It is planned to model dynamic sets of the users in the future model development. active However the position of each node, depending of its  distances to other points, is subject to change. We may think of these points as of the particles moving in result of interactions \(email communication\ between them At this point we use the formalism of molecular dynamics see section 3\sociate a force potential U with any particle \(network node\he actual characteristic of this potential depends on the behaviour of the particles changing their positions in time instants t 0 t 1  t n First experiments were carried using standard Lennard-Jones potential function. The analysis of server logs has revealed some features of the dynamics of email communication – the growing intensity of communication is always followed by the periods of less frequent email activity. This resembles the repelling force emerging between particles when their distance becomes less then some r min We noticed that intense e-mail communication \(which results in very small distances in social graph\ is never sustained for longer time From the other hand fading communication is \(in most cases\ followed by frequent message exchanges. It should be stressed that the Lenard-Jones potential was used only for first experiments, in the next steps we intend to develop social network-specific potential function on the basis of available data The force potentials introduced in section IV allow us to simulate the changes between communication patterns in consecutive time instants. The force potentials associated with the nodes reflect their abilities and tendency to establish future connections with the neighbours – the nodes which are close in terms of social space \(thus changing the distances in social space which is analogous to the behaviour of particles moving under influence of electrical/gravitational forces\at this approach allows also general interpretations because “being close in 2D social space” does not imply direct connection between nodes \(users\elonging to the same clique of the social graph has similar effect VI E XPERIMENTS The experiments were conducted in order to track the temporal changes in the network structure. This was performed by utilizing the time frame of 30 days moving by 10 days. A set of 60 separate \(partially overlapping, as in  f r am e s  w a s created f r o m th e e m ail log s  g a th ered in t h e period of February 2006 – October 2007. As stated above, in order to enable modelling the dynamics via the molecular simulation the users who were active in all 60 periods were chosen and this resulted in the creation of the set of 262 users and the relationships existed between them. This subnetwork was used in the further stages of our experiments Fig. 2 presents the results of running non-metric MDS procedure on the matrix of distances between social graph nodes Fig. 2 Initial positions of 262 particles 2009 IEEE Congress on Evolutionary Computation CEC 2009 557 


Molecular simulation was run on the set of particles with initial coordinates as showed on the Fig.2 and assuming that their interactions were characterized by standard LennardJones potential function. However, first results have shown that the exponent 14 in one of the terms of Equation 8 has to be decreased. It was responsible for rapidly growing repelling force between particles close to each other and has to be decreased to 10. After this change in the definition of the potential the system became stable thus allowing further simulations The configuration of particles moving in social space after 100 time steps in shown on Fig.3 Fig. 3 Positions after 100 time steps As we may see from comparing Fig. 2 and Fig. 3, the configuration of particles is stable, however their relative positions are changing. The analysis of the results of the first 100 time steps of molecular simulations allowed drawing interesting conclusions about the behavior of the systems of social particles First, there is one important information which cannot be inferred from the above picture. Small number of particles less then 6% of the total 262\ located far outside the area visible on Fig. 3, in few cases with their coordinates exceeding 1.0E+10. This observation led to analysis of their and – afterwards, of all the others\ as the elements of the social graph. We noted that in the case of low node centrality \(i.e. equaling one\he social distance metric may assign extremely close distance between low centrality node and its neighbour.  An interesting conclusion is that the accuracy of the molecular model depends on the position of the nodes within the social graph. We observed that  Nodes with minimal \(i.e. equaling 1\n- and out-degree centrality \(connected only to one neighbor in social network\ when moved to social 2D space tend to approach the other nodes \(particles\ to very short distances. Apparently, this reflects the fact that their relative distance is based only on single social relation This in turn results frequently in the emergence of large repelling force. The particle gains high speed and is thrown out – after that it moves along linear path, in our case the length of simulations was not enough to cause such a particle return in result of attracting force coming from interaction of the others. Obviously, this is not a behavior observed in social space and this feature should be carefully addressed during the future experiments  On the other hand, nodes with high degree centrality were observed to be stable during the interactions with the others In order to check the ability of our model to predict the social distances between particles we checked if there are correlations between node-to-node distances stored in distance matrices generated for all 60 temporal images of considered social network and the distances in 2D social space in consecutive steps of simulation. This was done separately for the four classes of nodes  Class 1 Nodes with degree centrality equaling one. \(with only single connection existing between them and the rest of the network  Class 2 Nodes with high in-degree centrality \(many – at least two – relations pointing to them, zero or one outcoming relations  Class 3 Nodes with high out-degree centrality \(many at least two – relations originating in given node, zero or one incoming relations  Class 4 Nodes with balanced in- and out-degree centrality both equaling at least two In all cases distances between any given node and all its counterparts in the network were computed for all temporal images of social network and the equal number of time steps of the molecular simulation. These vectors of distances were compared using the Pearson correlation coefficient. The average results for the members of the all considered classes are as follows  Class 1 0.0794  Class 2  0.3726  Class 3  0.4311  Class 4  0.6617 We may see that there is visible correlation between the predictions of the model and the behavior of the particles/nodes well-connected to the network. It maybe explained by their communication patterns being dependent from the other members of the community. For the nodes characterized by broadcast type of communication \(outdegree hubs\ or information sinks \(in-degree hubs\ this correlation is not so strong, however visible The communication patterns of the nodes loosely connected to the network are not predictable in terms of the proposed model, their behavior is independent from their network counterparts. During forthcoming experiments this phenomena will be addressed by redefining the social force potential function \(in order to better reflect the data gathered as stated in sec. V.B\d altering the 2D embedding procedure VII C ONCLUSIONS AND FUTURE WORK The results presented above are preliminary and 558 2009 IEEE Congress on Evolutionary Computation CEC 2009 


summarize the first experiments on creating the predictive evolutionary model for large internet-based social networks The same formalism may be also applied to other social networks based on communication technologies, for example, network of mobile phone users etc. There are many details which have to be addressed during future works and experiments. Among them the most important are  Force potential issues The actual shape and equation of the force potential characteristic for given network should be estimated by analysing the history of interactions between pairs of users \(nodes, particles we used “classic”, modified version of the Lennard-Jones potetial. The question if there is one definition of the potential which may be applied to all members of the network remains open  Choosing the time step of the molecular simulation In the case of our first experiments we analysed only simple correlations between changes in the distances in social space and communication graphs of time-changing social network  Dynamically changing sets of network nodes Fading and emergence of the nodes is the usual phenomena in social networks. The future versions of our framework will take it into account as well  2D social graph embedding procedure There are other options which may be applied instead of the MDS used in our simulations. They may also affect the results and the accuracy of the model Future experiments will be carried on larger social networks \(data sets based on logs from a 5000-users corporate server are being prepared for the forthcoming research\d will be preceded by developing methods of tuning the potential equations and the length of time steps The approach proposed in th is article aims to predict the evolution of social net-work of arbitrary size on the basis of known records. The existence of email-based social networks gives us the opportunity to use precise data \(mail server logs\which hold information about the activity of users. As a result it was possible to propose a computational approach with good simulation potential and compatible with known solutions from complex systems field Additionally, our approach was general and may be applied to any internet-based social network \(their size and the length of history records do not impact the procedure However, there are also several challenges and questions that must be answered in order to apply the method with required accuracy A CKNOWLEDGMENT The work was supported by The Polish Ministry of Science and Higher Education, grant no. N N516 264935 R EFERENCES 1 H o l l a nd J  H i d d e n O r de r  H o w  A d ap tat io n B u il ds  Co m p l e x ity H e l i x Books, USA, 1995 2 S c h w e i t zer  F  B r ow n i a n A g en t s and A c t i v e P a r t i c les   C o llec t i v e  Dynamics in the Natural and Social Sciences, Sporinger Series in Synergetics, New York, 2007 3 B o cal e tti a S  e t al   Co m p l e x ne tw o r ks  S t r u c t ur e a n d dy nam i cs    Physics Reports 424 \(2006\ 175 – 308 4 W ei d l i c h W  P h y s i c s an d S o c i a l S c i e nc e – Th e A p p r oa ch of  Synergetics, Physical Reports, \(204\-163, 1991 5 W ol f r am S  T h e o ry and A ppl ic at io n o f Ce l l u l a r  A u to m a ta W o r l d Scientific, Singapore, 1986 6 W atts D  S m al l W o rl ds  Dy nam i c o f N e tw o r ks be tw e e n O r de r and  Randomness, Princeton University Press, 2002 7 B a y s C  C a ndi da t e s for t h e ga m e  of li fe i n t h ree d i m e n s i o n s  C o m p lex Systems, 1\(2\:373-400,  1987 8 S h a vit t Y T a nk el T   B i g-Ba n g S i m u la ti on for E m b e d d i n g Net w ork  Distances in Euclidean Space, IEEE/ACM Transactions on Networking, 12\(6\ 2004 9 H a r el D   K O R E N Y   G r a p h  Dra w i n g b y  Hi gh Di m en si ona l Embedding, Journal of Graph Algorithms and Applications 8\(2 2004  Da n g a l ch ev C  G e n e ra t i on M o d e ls for S c a l e-fre e N e t w ork s  P h y s i c a A  338 \(2004\ 659 – 671  W a t t s D  St roga t z S  C o lle c t i v e Dy n a mi c s of Sma ll W o r l d Ne t w o r k s   Nature, 393, 440-442 12 Car r i n g t o n P S c o t t J W a sse r m an S   e ds Mo de l s an d Me t h o d s i n  Social Networks Analysis, Cambridge University Press, 2005  J  B  K r u s ka l Mu lt id i m en s i ona l s c a lin g b y op t i m i z in g good n e s s of-fi t  to a non-metric hypothesis, Psychometrika, 29, str. 1-27, 1964 14 D y ne s S G l o o r P L a ubac h e r R Z h ao Y  T e m p o r al V i sual iz a tio n  and Analysis of Social Networks, NAACSOS Conference, Pittsburgh PA, June 27 - 29, 2004 15 K r us kal  J  B a n d W i s h  M  Mul t i d i m e ns io nal S c al ing  S a g e  University Paper series on Quantitative Application in the Social Sciences, 07-011. Beverly Hills and London: Sage Publications, 1978 16 Br o n s t e i n, A  M, Br o n s t e i n M M and K i m m e l  R., G e ne r a l i z e d  multidimensional scaling: a framework for isometry-invariant partial surface matching, Proc. National Academy of Sciences \(PNAS\ Vol 103/5, 2006, pp. 1168-1172 17 Be r g e r W o l f T  Y S a ia J A  F r am ew o r k f o r  A n aly s i s o f Dy nam i c  Social Networks, Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining 2006, Philadelphia, PA, USA  Helb i n g D  Qu ant i t a t i v e Soc i od y n a m i c s  St och a s t ic M e th od s  a nd Models of Social Interaction Processes, Springer, Theory and Decision Library Series B: Mathematical and Statistical Methods, Vol 31 2009 IEEE Congress on Evolutionary Computation CEC 2009 559 


  0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0 100 200 300 400 500 600 700 800 900 H norm F P  R a t e  Figure 4 FP rate by entropy measure for HTML                  0 100 200 300 400 500 600 700 800 900 1000 0 16 32 48 64 80 96 112 128 144 160 176 192 208 224 240                    10 20 30 40 50 60 70  t 48 t 16 Popul ar i t y Scor e Rank Score  Offset                             0 100 200 300 400 500 600 700 800 900 1000 0 16 32 48 64 80 96 112 128 144 160 176 192 208 224 240                    10 20 30 40 50 60 70  t 48 t 16 Popul ar i t y Scor e Rank Score  Offset  Figure 5 Rank and popularity scores for a 320-byte snippet L 320 B 64 W 64  0 100 200 300 400 500 600 700 800 5 152535455565758595 Data Overlap C o m m o n  F e a t u r e s 64-16 64-24 64-32 64-40 64-48 64-56 64-64  Figure 6 Feature selection performance on random data Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


 Therefore, we could choose the doc distribution as our reference point and use it on all the sets. Going back to Figure 1, we can see that it almost serves as a composite of the rest with several local peaks. Actual data will effectively mask out parts of the ranking e.g. compressed data will have little use for entropy ranks to the left of its bell, whereas text data will not need anything to the right of its bell The answer to the second question is a confident yes even random data exhibits a bell-shaped distribution of entropy scores so even within a relatively small of window of 64-128 bytes, a few of the features are likely to stand out as statistically improbable, relative to the rest  6. Measuring File Similarity  We are now ready to go back to our original problem of finding similar objects. To establish a baseline measurement of the effectiveness of the described methods, we ran a controlled experiment using random data and known amounts of overlapping content. Specifically, we used two randomly generated files and produced mixed versions of them in the following fashion: take x  percent of file #1 and mix with 100x percent from file #2. The mixing was done in blocks of 512 bytes and we used 21 values for x 0, 5, 10 100. Note that we selected the blocks at random so even the 100% case is not an identical file but a file containing the same data block in a random permutation Further, we fixed W 64, and varied t from 16 to 64 with a step of 8. Figure 6 summarizes the results As we can see from the chart, the number of common features increases linearly with the increase of the amount of data in common and the slope of the increase is determined by the threshold parameter t  Generally, a lower the value for t means that more features are retained, whereas a higher value selects fewer features and improves compression. In this case t 16 retains an average \(over the different runs\ 847 features, whereas t 64 only 110. Using an MD5 hash to compress the feature representation would mean that our storage requirements would be 13,552 and 7,040 bytes, respectively. This yields corresponding compression ratios of 3.8:1 and 7.2:1 that can be further improved 10 times by using a Bloom filter with 10 bits per element and 0.8% FP rate. Further reductions are possible by selecting a bigger value for W such as 128, 256, 512 The above results can readily be replicated for compressed data types such as jpg, pdf and gz For text, the results are quite similar for unrelated text however, things like style and common topic do tend to yield higher results as the syntactic similarity increases. Generally, the results for non-random data need to be evaluated with respect a baseline FP rate which is relatively straightforward to obtain from a representative set of files. Alternatively, one could use the set to generate a set of duplicate features and ignore them in the actual data A large-scale experimental validation of an early prototype implementation on 4.6GB of real data Table 1\ is the subject of a separate paper. In summary, we were able to detect files within simulated network traffic based on a single packet with 0.9987 to 0.9844 accuracy \(depending on file type\or feature size of 64 and threshold of 16. Our current implementation is capable of 100MB/s sustained throughput on a quad-core processor this includes feature selection, feature hashing, and comparison with a reference set. We expect that an improved implementation would need no more than two cores to sustain that rate  7. Conclusions  In this paper, we presented a new approach to selecting syntactic features for similarity measurements. Our work makes the following contributions   Through empirical data, we have shown that a basic entropy measure can provide a valuable guideline with respect to the uniqueness of a particular feature   We have shown that different data types exhibit different behavior and that similarity measures can be tuned to keep a lid on overall false positive rate   We proposed a new method for selecting characteristic features that does not rely on a Rabin scheme to be content-sensitive. Instead we use an entropy measure and empirical distribution data to sel ect statistically improbable features   The new method is very stable and predictable with the respect to the coverage it produces, and unlike previous work, can easily be tuned to the underlying data  In the immediate future we plan to full develop a practical, high-performance tool that can be used to correlate evidence on a large scale      Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


 8. References  1  B. Bloom Space/Time Tradeoffs in Hash Coding with Allowable Errors  Communications of the ACM  vol 13 no 7, pp. 422-426, 1970 2  S. Brin, J. Davis, H. Garcia-Molina Copy detection mechanisms for digital documents In Proceedings of the ACM SIGMOD Annual Conference San Francisco, CA, May 1995 3  A. Broder, M. Mitzenmacher Network Applications of Bloom Filters: A Survey  Internet Mathematics  vol 1 no 4, pp. 485-509, 2005 4  A. Broder, S. Glassman, M. Manasse, and G. Zweig Syntactic Clustering of the Web In Proceedings of the 6 th International World Wide Web Conference pp 393-404, 1997 5  M. S. Charikar Similarity Estimation Techniques from Rounding Algorithms  In Proceedings of the 34 th Annual ACM Symposium on Theory of Computing 2002 6  C. Y. Cho, S. Y. Lee, C. P. Tan, and Y. T. Tan Network forensics on packet fingerprints. 21st IFIP Information Security Conference \(SEC 2006 Karlstad, Sweden, 2006 7  Henziger, M Finding Near-Duplicate Web Pages: A Large-Scale Evaluation of Algorithms In Proceedings of the 29 th Annual International ACM SIGIR Conference on Research & Development on Information Retrieval Seattle 2006 8  R. Karp, M. Rabin. "Efficient randomized patternmatching algorithms". IBM Journal of Research and Development 31 \(2\249-260, 1987 9  A. Kirsch, M. Mitzenmacher Distance-Sensitive Bloom Filters  Proceedings of the Algorithms Engineering, and Experiments Conference ALENEX 2006 10  J. Kornblum Identifying almost identical files using context triggered piecewise hashing  Proceedings of the 6 th Annual DFRWS Aug 2006, Lafayette, IN 11  U. Manber. Finding similar files in a large file system In Proceedings of the USENIX Winter 1994 Technical Conference, pages 1-10, San Fransisco, CA, USA 1994 12  M. Mitzenmacher Compressed Bloom Filters  IEEE/ACM Transactions on Networks 10:5, pp. 613620, October 2002 13  K. Monostori, R. Finkel, A. Zaslavsky, G. Hodasz, M Pataki Comparison of Overlap Detection Techniques In Proceedings of the 2002 International Conference on Computational Science Amsterdam The Netherlands, \(I\pp 51-60, 2002 14  M. Ponec, P, Giura, H. Brnnimann, J. Wein Highly Efficient Techniques for Network Forensics, In Proceedings of the 14th ACM Conference on Computer and Communications Security, 2007 Alexandria, Virginia 15  H. Pucha, D. Andersen, M. Kaminsky Exploiting Similarity for Multi-Source Downloads using File Handprints In Proceedings of the Forth USENIX NSDI, Cambridge, MA. Apr, 2007 16  M. O. Rabin Fingerprinting by random polynomials Technical report 15-81, Harvard University, 1981 17  S. Rhea, K. Liang, and E. Brewer. Value-based web caching. In Proceedings of the Twelfth International World Wide Web Conference, May 2003 18  V. Roussev, Y. Chen, T. Bourg, G. G. Richard III md5bloom Forensic filesystem hashing revisited  Proceedings of the 6 th Annual DFRWS Aug 2006 Lafayette, IN 19  V. Roussev, G. Richard III and L. Marziale, "Multiresolution similarity hashing", Proceedings of the Seventh Digital Forensic Research Workshop, 2007 20  K. Shanmugasundaram, H. Bronnimann, N. Memon Payload Attribution via Hierarchical Bloom Filters  Proceedings of the ACM Symposium on Communication and Computer Security CCS'04 2004 21  N. Shivakumar and H. Garcia-Molina SCAM: a copy detection mechanism for digital documents In Proceedings of the International Conference on Theory and Practice of Digital Libraries June 1995 22  N. Shivakumar, H. Garcia-Molina Building a scalable and accurate copy detection mechanism In Proceedings of the ACM Conference on Digital Libraries March 1996\, 160-168 23  N. Shivakumar, H. Garcia-Molina Finding nearreplicas of documents on the web In Proceedings of the Workshop on Web Databases March 1998\ 204212 24  S. Schleimer, D. S. Wilkerson, and A. Aiken Winnowing: local algorithms for document fingerprinting. In SIGMOD '03: Proceedings of the 2003 ACM SIGMOD international conference on management of data, pages 76-85, New York, NY USA, 2003. ACM Press Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


This figure presents the data flow through main blocks that can be used a few times and special blocks that must execute only particular functions Here also Configurator of chain and Sequences library for WiMAX/UMTS modules are equipped First one is responsible for right construction of next processing path and second one stores necessary number of elements After signal verification system is reconfigured according to input data and with using the chosen protocol type The OUTPUT connects to radio link and then signal must be transmitted over one of six channel types And at the reception side we configure the receiver in relation to transmitted mode 5 HARDWARE PLATFORM SELECTION Current technologies in a hardware environment allow to test our system in real-time implementation There are a couple of DSP based platforms that can be selected for validation from Lyrtech Inc the Small Form Factor SFF Software Communication Architecture SCA Development the Small Form Factor SFF Softwaredefined Radio SDR Development Platform the Small Form Factor SFF Software-defined Radio SDR Evaluation Module 20 All these platforms are based on TMS320DM6446 DMP SoC from Texas Instruments 21 For the proposed SDR based system we chose the SFFSDR Evaluation board see Figure 13 as far as this platform supports WiMAX technology model based design tools accelerating prototyping implementation of all protocols layers for complete radio stacking extra boards operates with 297MHz ARM926EJ-s RISC CPU and 594MHz C64x DSP in sense of power management this module has MSP430 MCU Due to an availability of Virtex-4 SX35 FPGA from Xilinx 22 this module can perform implementation of full modem processing functions that is very important feature in meaning of multi-protocol architecture of our system We are able to vary our requirements to each protocol inside the same hardware structure Figure 13 SFF SDR development platform by Lyrtech 6 PROTOTYPING THE WiMAX/UMTS SYSTEM The WiMAX/UMTS system is implemented in high-level language as C with the class library The main accent was done on the correct form of the signal processing sequence The path for WiMAX or UMTS signal is determined in the beginning and system should verify its entity leads the system in a relevant direction The main goal of this software implementation is to check how our system can handle the input signal sequence The simulation was carried out for following parameters for each subsystem For UMTS we consider transport block with 1280 bits size frame size is 2400 bits channelization code with 16 chips sequence For WiMAX we generate the bit block is equal to 1280 bits however during channel coding operation this block is divided on turbo coding block that include 384 bits The size of turbo coding block forms from the block determination corresponding modulation type and number of subchannels During software verification we obtained that our proposed model can separate different paths subject to a type of an inter sequence in the software environment The framework of our WiMAX/UMTS system went through one scenario step and now we are directed to an extension of this model For the more detailed system visualization we integrate our C code modules into MATLAB library by means of proper dynamic linking libraries dll compiled by using the MATLAB C compiler Each block can be formed with extended parameters Modules with C code configurate the system work in a host This host implementation will present a prototype This prototype will help to analyze future real-time hardware implementation MATLAB prototype can also provide debugging of real system the result of real system must be equal to our MATLAB prototype Next step of the work development and verification is to implement it into the hardware platform based on DSP DSP based platform allows to organize signal processing in a digital presentation that serves SDR based part of our general system 7 CONCLUSIONS AND FUTURE WORK In this paper we considered the framework of WiMAX/UMTS baseband level system for mobile device in UL transmission direction We presented the different signal processing structures based on OFDM and WCDMA physical layer procedures Our research work is mainly devoted to developing the approach of seamless switching between different subsystems that can be realized by SDR technology implementation To this end we proposed a possible solution to allow coexistence of different data transmission technologies 11 


The position of SDR blocks in common UMTS/WiMAX architecture for mobile terminal was shown in this paper We presented the different blocks of each subsystem and have identified three blocks which can be implemented as common SDR blocks These blocks include channel coding module interleaver module and data mapping module We also demonstrated the work of our system in the software environment Next steps of the UMTS/WiMAX system development are preparation of the specification and implementation of all possible scenarios Each scenario will include particular blocks parameters and common description of main blocks But we have to be carefully in case of main blocks description because there are a plenty of features 8 ACKNOWLEDGMENTS We gratefully acknowledge the company Arslogica that kindly provided us the hardware support for our experimental studies 9 REFERENCES 1 A Samukic UMTS Universal Mobile Telecommunications System development of standards for the third generation Proc of 1998 IEEE GLOBECOM Conf Sydney AUS Nov 8-12 1998 vol.4 pp.19761983 2 N Fourty T Val P Fraisse and J.-J Mercier Comparative analysis of new high data rate wireless communication technologies From Wi-Fi to WiMAX Proc of the IEEE Autonomic and Autonomous Systems and International Conf on Networking and Services ICAS-ICNS 05 Oct 23-28 2005 pp.66-66 3 M Komara SDR Architecture Ideally Suited for Evolving 802.16 WiMAX AirNet Communications SDR Forum Exhibition 2004 4 I Held 0 Klein A Chen C.-Y Huang and V Ma Receiver Architecture and Performance of WLAN Cellular Multi-Mode and Multi-Standard Mobile Terminals Proc of 2004 IEEE VTC Fall Conf Los Angeles CA Sept 26-29 2004 vol 3 pp 2248 2253 5 IEEE Standard for Local and Metropolitan Area Networks Part 16 Air Interface for Fixed Broadband Wireless Access Systems 2004 6 R Weigel and L Maurer  D Pimingsdorfer A Springer RF Transceiver Architectures for W-CDMA Systems Like UMTS State of the Art and Future Trends Proc of the Intern Symp on Acoustic Wave Devices for Future Mobile Communication Systems Chiba JP March 5-7 2001 pp 25-34 7 P-W Fu and K.C Chen A Programmable Transceiver Structure of Multi-rate OFDM-CDMA for Wireless Multimedia Communications Proc of 2001 IEEE Vehicular Technology Conf VTC-Fall 2001 Atlantic City NJ Oct.7-11 2001 vol 3 pp 1942-1946 8 L Zhigang L Wei Z Yan G Wei A Multi-standard SDR Base Band Platform Proc of 2003 International Conference on Computer Networks and Mobile Computing Shanghai PRC Oct 20-23 2003 pp 461 464 9 C Moy A A Kountouris L Rambaud and P Le Corre  Full Digital IF UMTS Transceiver for Future Software Radio Systems  Proc of ERSA 01 Conf Las Vegas NV June 25-28 2001 10 3GPP TS 25.201 Physical layer general description 11 K.R Santhi and G.S Kumaran Migration to 4 G Mobile IP based Solutions Proc of International Conference on Internet and Web Applications and Services/Advanced International Conference Feb 2006 pp 76 76 12 S Zhu M Song Y Li J Song and F Ren Simulation platform of WCDMA based on software defined radio Proc of 2nd ACM International Conference on Mobile Technology Applications and Systems Nov 2005 pp 1-5 13 L Ma and D Jia The Competition and Cooperation of WiMAX WLAN and 3G Proc of 2nd International Conference on Mobile Technology Applications and Systems Nov 2005 pp 15 14 J Mitola III Software Radio Architecture ObjectOriented Approaches to Wireless Systems new ed Wiley New York 2004 15 R Seungwan 0 Donsung S Gyungchul and K Han Perspective of the next generation mobile communications and services Proc of IEEE 2004 Int Symp on Personal Indoor and Mobile Radio Communications PIMRC 2004 Barcelona SP 5-8 Sept 2004 vol.1 pp 643-647 16 E Biglieri Coding for Wireless Channels  Springer New York 2005 12 


17 3GPP TS 25.212 Multiplexing and channel coding FDD 18 IEEE Standard for Local and metropolitan area networks Part 16 Air Interface for Fixed and Mobile Broadband Wireless Access Systems Amendment 2 Physical and Medium Access Control Layers for Combined Fixed and Mobile Operation in Licensed Bands and Corrigendum 1 2006 pp 0_1 822 19 3GPP TS 25.211 Physical channels and mapping of transport channels onto physical channels FDD 20 Data sheet from Lyrtech Inc http available at htp c hwwkneopusff _.l/p.s/lrtehs _sr d21]D ateforomTdf 21 Data sheet from Texas Instruments http available at 22 Data sheet from Xilinx http available at httll/www.xilinx.com 23 L Hanzo W Webb and T Keller Singleand Multicarrier Quadrature Amplitude Modulation  Wiley New York 2000 titled Wireless and Satellite Communications  The research interests of Dr Sacchi are mainly focused on wideband mobile and satellite transmission systems based on space time andfrequency diversity multi-user receivers based on non conventional techniques neural networks genetic algorithms higher-order statistics-based receivers etc cross-layer PHY-MAC design and high-frequency broadband satellite communications He is currently local coordinator for University of Trento of research projects dealing with reconfigurable communication platforms based on MIMO techniques and space-time signal processing ICONA project funded by MIUR and with exploitation of W-band for broadband satellite communications WA VE programs funded by ASI Claudio Sacchi is author and co-author of more than 50 papers published in international journals and conferences and reviewer for international journals and magazines IEEE Transactions on Communications IEEE Transactions on Wireless Communications IEEE Communications Letters IEEE Transactions on Aerospace and Electronic Systems Electronics Letters Wireless Networks IEEE Communications Magazine etc Dr Sacchi is member of the Organizing Committees and Technical Program Committees of international conferences like ICIP ICC GLOBECOM ACM-MOBIMEDIA etc Claudio Sacchi is member of IEEE M'01 SM'07 BIOGRAPHIES Olga Zlydareva is a PhD student of the University of Trento Italy She obtained her Master degree in Design Electronics Systems with specialization in High Radio Frequency Devices from MATI Moscow State Aviation Technological University named after KE Tsiolkovsky Moscow Russia Her research interests have oriented on the Software Defined Radio Technology Wireless Technologies Cellular Technologies Tunable devices Multi-standard systems Multi-protocol systems Physical layer of mobile devices Reconfigurability and Reprogramming of mobile devices The recent research focuses on the development of the baseband level of multistandard mobile devices based on SDR technology Claudio Sacchi was born in Genoa Italy in 1965 He obtained the Laurea degree in Electronic Engineering and the Ph.D in Space Science and Engineering at the University of Genoa Italy Since August 2002 Dr Sacchi has been holding aposition as assistant professor at the Faculty of Engineering of the University of Trento Italy In 2004 he was appointed by the Department of Information and Communication Technology of the University of Trento as leader of the Research Program 13 


  14  Figure 5:  Site B1 Terrain horizon ma sk with 1 degree azimuth spacing  Figure 6:  Site B1 Terrain horizon mask with 1 de gree azimuth spacing, in e quatorial coordinates 


  15  Figure 7: Lunar South Pole Solar Illumination Yearly Average  Figure 8:  Lunar South Pole DTE Visibility Yearly Average 


  16  Figure 9: Lunar North Pole Sola r Illumination Yearly Average  Figure 10:  Lunar North Pole D TE Visibility Yearly Average 


  17  Figure 11: Site A1 Elevation Topography  Figure 12: Site A1 Yearly Average Solar Illumination and DTE visibility, Medium Resolution 


  18   Figure 13:  Site LB Te rrain Horizon Mask  Figure 14:  Theory and Computed values of Average Yearly Solar Illumination 


  19  Figure 15:  Theory and Computed values of Average Yearly DTE Communication  Figure 16:  Heliostat Mirror Design to Eliminate Cable Wrap 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


