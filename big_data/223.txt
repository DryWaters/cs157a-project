Generating Implicit Association Rules from Textual Data Ch Cherif Latiri Faculty of Sciences of Tunis Computer Science Department Campus Universitaire, 1060 Tunis, Tunisia E-mail:chiraz.latiri@gnet.tn Abstract The need of sophisticated analysis of textual data is be coming very apparent In the general context of knowledge discovery, Textmining techniques aim to discover additional information from hidden patterns in unstructured large tex tual collection. Hence in this papec we are interested espe cially in the extraction of the associatiomJiom unstructured database The objective 
of this paper is twofold First to propose a conceptual approach, based on the formal con cept analysis I I  and a semantic pruning, in order to dis cover explicit association rules from large textual corpus Second to introduce an algorithm to derive additional and implicit association rules using an associated taxonomy Ji.m the already discovered association rules Key words Databases and data engineering Textual data Knowledge discovery Formal concepts Textmining Im plicit rule 1 
Introduction Nowadays, a lot of database systems are built for storing documents and textual data Thus, it is necessary to pro vide automatic tools for analyzing large texhial collections Accordingly, in analogy to datamining to structured data textmining is defined for textual data  131 In fact we define textmining to be the science of extract ing additional information fiom hidden patterns in unstruc tured large textual collection 21 It is all about extracting associations previously unknown fkom large text databases 
In this paper we are interested especially in the extrac tion of the associations from textual database The aim of this paper is twofold First, to propose a conceptual ap proach, based on the formal concept analysis  1 11 and a semantic pnining in order to discover explicit association rules from large textual corpus Second to introduce an al gorithm to derive additional and implicit association rules 5 6 798 101 S BenYahia Faculty of 
Sciences of Tunis Computer Science Department Campus Universitaire 1060 Tunis, Tunisia E-mail:sadok.benyahia@fst.rnu.tn using an associated taxonomy fkom the already discovered association rules The proposed approach in this paper is based on the use of Galois connection  1 1,201. It operates in a top-down ap proach by finding first the largeformal concepts In order to be efficient, only one pass over the database is performed once all the required information have been recorded dur ing this unique database pass The proposed approach 
is developed in the following steps  1 We extract explicit formal concepts  In this step we introduce a semantic pruning using the association de gree of two linguistic concepts given in the taxonomy 2 From explicit formal concepts discovered in the pre vious step and using an associated taxonomy we propose an algorithm to extract implicit association rules These rules represent additional and hidden knowledge about the documents corpus and the do main Hence we can discover unsuspected associa 
tions holding in texts The rest of the paper is organized as follows In sec tion 2 we present basic definitions related to textmining In section 3 we present the basic formalism of mining asso ciation rules in the context of textmining Section 4 intro duces formal concepts analysis and introduce the algorithm to discover formal concepts. In section 5 we present an al gorithm for discovering implicit association rules fkom tex tual data An illustrative example 
is also given Section 6 concludes this paper and points out some perspectives 2 Basic definitions related to textmining Many interesting works on textmining was reported in the literature 4 7 8 9 12 17 211 In this paper we are interested especially in the association extraction fiom tex tual database the full text approach\Hence we first intro duce the basic definitions related to textmining 0-7695-1165-1/01 10.00 0 2001 IEEE 137 


Figure 1 The linguistic concepts taxonomy Definition 1 21 A document collection is a set of docu ments texts representing an application domain Definition 2 4 211 A linguistic concept is defined to be a meaningfiul word phrase acronym or name that has been extractedfrom unstructured components of the text, includ ing blocks of text, abstract, headings, paragraph,etc It is also defined as a linguistic term while each document refers to a set of concepts or terms Definition 3 4 A taxonomy is a hierarchy of terms lin guistic concepts\representing the documents Each node of the taxonomy corresponds to a linguistic concept It is viewed as a graph structure, which represents the relation ship between two linguistic concepts in a document collec tion, as well as the weight of each relationship The taxonomy is necessary in textmining system in order to allow the system to focus on the relevant set of linguistic concepts in the knowledge discovery process Example 1 Semantic relationships between linguistic con cepts are represented by the taxonomy 21 as depicted in figure 1 Each concept C is a node and each relation ship is a weighted edge Single directional edges pointj-om generic to specific concepts, while bidirectional arrows ex ist between similar sibling concepts For every pair of con cepts C and cb the relationship r\(Ca Cb measures the association degree between the two linguistic concepts In formation retrieval techniques 19 are used to determine the association degree based on the frequency of appear ance of C and cb in the same document 3 Basic formalism of mining association rules The basic formalism of mining associations fiom text is similar to the one presented by Agrawal et a1 2 in the context of datamining A textmining context is a triple IC  V,7,R de scribing a finite set V  01  Dm of documents a finite set 7 Cl  Cn of terms or linguistic concepts and a binary relation R i.e R 23 x 7 Each couple Di C E R means that the document Di E 23 is labeled with the linguistic concept C E 7 Let LI  7 be a set of k linguistic concepts The support of Lk is the percentage of documents in V contain ing all terms in LI The itemset LI is said to be large or frequent if its support is greater than or equal to minsupp2 An association rule is an implication among itemsets de noted by R  L  Lj where itemsets L L s 7 are subsets of linguistics concepts and La n Lj  0 The con fidence of the rule R is the conditional probability that a document contains L  given that it contains La Hence Coafidence\(R  support\(L U All known algorithms l 2 15 18,22,23 for generat ing association rules operate in two phases Given a finite set 7 Cl  Cn of linguistic concepts and a collection of indexed documents V the extraction of associations sat isfying given minsupp and minconf is performed as follows Lj La PI 1 2 by first generating all fi-equent itemsets The frequent set generation is the most computationally expensive step 16 then by generating all the association rules that can be derived fiom the produced large itemsets and that satisfy mineon 4 Discovering formal concepts 4.1 Formal concepts Analysis In this section, we present some classical notions of data mining context Galois connection, formal concepts and for mal concept lattice 1 1,201 Data mining 'context A data mining context is a triple 2  0,Z R describing a finite set 0 of objects, a finite set Z of database items and a binary relation R i.e R C 0 x Z Each couple 0 i E R means that the object o E 0 has the item i E Z Galois connection ll ing context For 0  0 and I C Z we define  Let V  0 Z R be a datamin f\(0  P\(0  P\(I f\(0  2 I vo 0 E 0 3 0 i E R h\(1  P\(1  P\(0 h\(I  0 I v2,i E I  0,i ER suppmt\(Lk  Di E DlLk C Di  minsupp and minconf are user-defined thresholds 138 


We can remark that 0 x f\(0 is the biggest relation of the form 0 x X C R and that h\(I x I is the biggest relation of the form I x X C R  111 In other words f computes the maximal range for a domain 0 and h com putes the maximal domain for a range I The operators h of in 0 and f o h in Z 3 are called Galois connection operators  1 11 Properties related to Galois connection f h are given in 3 Proposition 1 Let I1,12 E Z Then h I1 U 12  h\(I1 n h\(I2 PI Formal concept Let C G I be a set of items C is called a concept if and only if it is equal to its closure i.e C  f o h\(C h\(C is called the domain of C 4.2 Discovering large formal concepts The pseudo-code for discovering large forme1 concepts  is given in Algorithm 1 Notation and parameters used in this algorithm are summarized in table 1 In each iteration the algorithm constructs a set of candidate formal concepts CLFC prunes this set with respect to minsupp yielding a set of non-redundant large formal concepts Finally, using this set it computes the generators set that will be used during next iteration CLFCk r Set of large candidate k-itemsets Set of large k-itemsets Each element of these set has three fields i gen the generator ii supp the support and iii clos the closure i.e f o h\(gen Table 1 Notations Discovering Large Formal Concepts Input  D Output  LFC  UiLFCi Begin CLFC1  1  itemsets For i  1 CLFCi.gen  0 i   do begin CLFCi.clos  0 CLFCi.supp  0 LFCi  Gen-concepts\(CLFCi CLFCi+l  Gen-next\(LFCi End End Algorithm 1 Large Formal Concepts Discovery Hence initially CLFC1 is formed by the 1-itemsets Each iteration is composed of two phases 1 The hction Gen-concept is applied to each genera tor in CLFCi determining its support and its closure 2 The set of the generators used in the next iteration i.e CLFCi+l is computed by applying the hction Gen-next to LFCi The algorithm terminates when no more generators to The pseudo-code of the functions Gen-concept and process i.e that CFCi is empty Gen-next are detailed in  141 5 Discovering implicit association rules from textual data 5.1 The Semantic pruning in the context of textmin ing The large formal concepts LFCi discovered by the knowledge discovery algorithm, described in the previous section, represent in the context of textmining.oniy poten tial large itemsets resulting after a syntactic pruning In this paper we associate to the knowledge discovery pro cess an additional operation performed using an associated taxonomy to the textmining domain in order to extract only relevant itemsets of linguistic concepts We mean by rele vant itemsets that to have a k-large-itemset containing k linguistic concepts, such that these concepts must form a hlly connected subgraph in the taxonomy Hence it is use ful to exploit an additional information in textmining, such as the association degree between two linguistic concepts given in an associated taxonomy We can add a seman tic knowledge to the final association rules set which il lustrates the strength of the semantic relationships between linguistic concepts in a given rule 139 


Proposition 3 I41 Given a taxonomy G and a k-large itemset Lk in the set LFC we compute the association de gree between the k linguistic concepts r\(Cl    Ck in Lk by performing the minimum operator on the all association degrees r\(C C;+l for every pair of linguistic concepts C and C;+l Example 2 The association degree r\(C1  C2 C3 of the subgraph illustrated in figure 1 is computed as fol lows r\(C1  C2 C3  min  r\(C1 C2 C2 C3  min 0.8,0.45  0.45 In order to introduce a semantic pruning we introduce the function Semgruning which tests for all Lk  CI C2  Ck E LFC whether the large itemset Lk forms a fully connected subgraph in the given taxonomy and if its association degree is greater than or equal to a user-defined threshold 6 According to the conceptual textmining approach pre sented in this paper we propose the following definition  Definition 4 Explicit Formal Concept A formal concept Lk is said to be explicit in the context of textmining fit is included in LFC and it is not discarded ajier the semantic pruning step The pseudo-code of Semgruning function is given be low Function Semqrunlng Input LFC and a taxonomy G Output ELFC  LFC  not relevant itemsets Lk  ELFC represents the set ofexplicit formal concepts Begin Let ELFC  LFC Forall LI  cl  ck E LFC do begin  Lk is a k-large-itemset represented by a formal concept in LFC*I If subsaph Lk  false OR r\(Lk  cl  ck 5 6Then  the function subqraph tests whether Lk forms a fully connected subgraph End IF End End remove Lk from LFC Function Semgruning 5.2 Discovering implicit association rules The problem of generating valid association rules fiom the extracted explicit formal concepts can be solved in a straightfonvard manner in main memory once all large itemsets and their support are known 16 An adapted ver sion of Apriori rule generation algorithm is given in 16 The principle of this algorithm is described as follows  for every large itemset Li in LFC all subsets Lj of L are derived and the ratio4 s.llpport\(L L  Lj is computed If the result is at least equal to minconx then the rule Lj  Li  Lj is generated The valid rule genera tion algorithm is detailed in Let us consider in the context of textmining 1 LI   C1    Ck a k-large-set of linguistic concepts included in ELFC 2 ERS the explicit rules set generated by the valid rule generation algorithm proposed in 16 3 X  Y an explicit rule such that Xu Y  Lk  Cl 21 Cn 4 X a subset of X such that X    Cj U X  Ci where Cj are more specific linguistic concepts than C in the taxonomy Hence it is interesting to derive ii-om a rule X  Y where X contains generic linguistic concepts a valid rule X  Y such as the linguistic concepts in X are more specific than those in the large itemset X according to the given taxonomy In fact we discover an implicit additional knowledge by exploiting the taxonomy called implicit rules set ZRS in the following way  1 first we must check for each concept C in X if it exists a subset C such that C is a more specific linguistic concept than C and forms a fully connected subgraph with the subset  X  C in the taxonomy If it is the case we check whether r\(C  X  C 2 8 2 seFond to conclude whether the implicit rule ZR X r Y such that X  Cj U X  Ci is a valid rule we mus compute its confidence which is equal to support X  Y  3 If conJidence\(XR X  Y is greater than or equal to minconJ then the rule ZR X  Y is considered as a valid rule, and it is added to ZRS The pseudo-code of Genjmplicit-rules function is given below 4the ratio The support of L is computed as  support\(LJ/support\(LJ reprsents the confidence of the rule L  L  Lj Ilh\(Lt II IPll 140 


Function Gen-implicit-rules nput  ELFC RS and a taxonomy G Output  ZRS:{Implicit Rules set Begin For each rule I  x  Y E ERS do Begin For each ci E x do Begin Extract G cj   cj is a subset of linguistic concepts more specific than Ci in the taxonomy If Exist\({Cj ThenLetX  cj Ux  ci If subxraph x true and r\(X 2 6 then  The function subsaph tests whether  c U x  ci forms a fully connected subgraph Compute-support x conf  Compute-confidence x  Y I*X  Cl cz  Cn}*l If conf 2 minconfthen End IF ZRS  ZRSU X r Y End IF End IF End End End Function Gen-implicit-rules c7 Ca 5.3 Illustrative Example  4 2,3,8,10 6 5,10 12,15,17,19 In this subsection we consider the motivating example given in  to illustrate our conceptual approach for dis covering implicit association rules in context of textmining Let us consider the linguistic relation given by table 2 c1c3  c7 c1c7  c3 I ID I Numb.Docs I DocumentIDlist I suP;\(r 3 7 suP&:cy  2/7 suP~{c~~3  2/2 supp\({C1 c 1,3,4,6,7,8,1 1,16,1 8,20 1,3,6,8,11,16,18,20 5,7,8,10,12,15,16,18 2,9,13,19 5.6.10 Table 2 The linguistic concepts Relation 5.3.1 Explicit concepts discovery Explicit formal concepts discovery process is depicted in figure 2 Figure 2 Formal Concepts Discovery minsupp=2 5.3.2 Semantic pruning Explicit formal concepts, discovered in the previous step represent potential large itemsets It should be noted that in order to have a k-large-itemset containing k concepts all the concepts in the large itemset must form a filly con nected subgraph. According to the given taxonomy in figure 1, we assume that 6 is equal to 0.4 The result of the seman tic pruning step, is the following large itemsets which are filly connected subgraphs and verify the minimal associa tion degree  2-large-itemset CiC3 C4C8 C1C4 C1C7 C3C4 3-large-itemset  C1C3C&1C3C7 We notice that the large itemset {C4C is discarded fi-om the final output since it is not a fully connected sub c3c7 c4c71 graph 5.3.3 Implicit association rules discovery Let us consider the large itemset Li  C1C3C7 included in LFC and a minconf equal to 0.1 The valid association rules generated from Li using the valid association rules generation algorithm, given in  161 are as follows  I Explicit Rules ER 1 Confidence 141 


I ER I Imolicitrules ZR I Confidence I c7c3  c7 c4c3 3 c7 1 Valid rule I C,C6--.C7 Io I Notvalid I I C1C4+C7 I 1/4 1 Valid rule ClC7*C3 I C4C7+C3 1 1/4 I Validrule Notice that C3 and C are similar sibl6g concepts in the taxonomy 4 and C is more specific than Cl 6 Conclusion and Future Work Textmining is an emerging research area whose goal is to discover additional information fiom hidden patterns in unstructured large textual collection It is all about ex tracting associations previously unknown fiom large text databases In this paper we have presented a conceptual approach to discover knowledge formally represented by association rules fiom large textual corpus Once the dis covered association rules were obtained we proposed also an algorithm to derive additional and implicit association rules using an associated taxonomy We shall consider as a future work fuzzy conceptual text mining approach where textual data are represented by a fuzzy relation References l R Agrawal T Imielinski and A Swami Mining Associ ation Rules between sets of items in large Databases ACM SIGMOD Records pages 207-216 1993 2 R Agrawal and R Skirant Fast algorithms for mining as sociation rules In Proceedings of the 20th Int I Conference on Very Large Databases pages 478499 June 1994 3 S BenYahia and A Jaoua Discovering knowledge from fuzzy concept lattice In A Kandel M Last and H Bunke editors Data mining and Computational Intelligence Stud ies in Fuzziness and Soft Computing Vol 68 chapter 7 pages 167-190 Physica Verlag Heidelberg March 2001 4 R. Feldman Y Aumann A Zilberstein and Y Ben-Yehuda Trend graphs: Visualizing the evolution of concept relation ships in large document collection In Proceedings of the Second European Symposium on Principles of Data Mining and Knowledge Discovery PKDD '98\Nantes September 5 R Feldman and I Dagan Knowledge discovery in textual databases kdt In proceedings of the Jirst international Conference on Knowledge Discovery KDD'9.5, Montreal pages 112-1 17 August 1995 23-26 1998 1 42 6 R Feldman I Dagan and H Hirsh keyword based brows ing and analysis of large document sets In proceedings of SDAIR96 Las Egas Nevada pages 191-208 April 1996 7 R Feldman I Dagan and W Kloegsen Efficient algo rithm for mining and manipultationg associations in texts In proceedings of EMCRS96 Vienna Austria pages 949 954, April 1996 SI R Feldman M Fresko Y Kinar Y Lindell 0 Liphstat Y M Rajman, and 0.Zamir Text mining at the term level In Proceedings of the Second Europeen Symposium on Princi ples of Data Mining and Knowledge Discovery PKDD'98 Nantes September 23-26 1998 9 R Feldman and H Hirsh Mining associations in text in the presence of background knowledge In Proceedings of the Second International Conference on Knowledge Discovery from Databases \(KDD96\Portland pages 343-346 August 1996  101 R Feldman W Klosgen and A Zilberstien Document ex plorer Discovering knowledge in document collections In Proceedings of ISMIS97 lecture Notes in AI Springer Er lag NC, USA October 1997  111 B Ganter and R Wille Formal Concept Analysis Springer Verlag Heidelberg, 1999  121 Y Kodratoff Datamining ind textmining Inproceediizgs of ECD 2000 Tunis, Tunisia pages 6-19 8-9May 2000 13 D Landeau Y Aumark R Feldman M Fresko Y Lin dell 0 Lipshat and 0 Zamir Textvis An integrated vi sual environment for textmining In Proceedings of the Sec ond Europeen Symposium on Principles ofData Mining and Knowledge Discovery PKDD'98\Nantes September 23 26,1998 September 23-26 1998 14 C. C Latiri and S Benyahia Textmining Discovering ex plicit formal concepts from unstructutred data In Proceed ings ofthe XXime Congris Znformatique des Organisations et Systhes d'hformation et de De'cision to appear Avril 2001 15 H Manilla H Toinoven and I Verkamo Efficient algo rithms for discovering association rules In AAAI Worshop on Knowledge Discovery in Databases pages 181-192 July 1994 16 N Pasquier Y Bastide R Touil and L Lakhal Pruning closed itemset lattices for association rules In Proceedings of 14th Intl. Conference Bases de donne'es Avance'es Ham mamet, Tunisia pages 177-196,2630 October 1998 17 M Rajman and R. Besangon Data mining and Reverse En gineering Chapter 3 Text Mining Natural language tech niques and Text Mining Application pages 5 1-64 1997  181 R Rastogi and K Shim Mining optimized association rules with categorial and numeric attributes In Proceedings ofthe 14 International conference on Data Engineering IEEE Ed ROrlando 1998 I91 G Salton and C Buckely Term weighting approaches in au tomatic text retrieval In Information Processing and Man agement pages 513-523 May 1988 20 G Schmidt and T Stroehlein Relations and Graphs Dis crete Mathematics for Computer Scientits Springer-Verlag 1993  


21 L Singh B Chen R Haight and P Scheuermann An al gorithm for constrained association rule  Mining in semi structured data In Proceedings of the third Pacific-Asia Conference PAKDD 99 Beijing China 1999 22 R Srikant and R Agrawal Mining generalised associations rules. In Proceedings VLDB 21 International conference on Ely Large Databases Zurich Switzerland pages 407-419 1995 23 R Srikant Q Vu and R Agrawal Mining association rules with item constraints In Proceedings of the 3rd Int I Confer ence on Knowledge Discovery in Databases and Data Min ing Newport Beach, California August 1997 1 43 


225 There exists no rule for which  225 for all rules such that  After groups are processed, any rule is removed if there exists some group such that and Because the search explores the set of all rules that could potentially prove some rule in does not have a large improvement, all rules without a large improvement are identified and removed Our post-processor includes some useful yet simple extensions of the above for ranking and facilitating the understanding of rules mined by Dense-Miner as well as other algorithms. The improvement of a rule is useful as an interestingness and ranking measure to be presented to the user along with confidence and support. It is also often useful to present the proper sub-rule responsible for a rule\222s improvement value. Therefore, given an arbitrary set of rules, our post-processor determines the exact improvement of every rule, and associates with every rule its proper subrule with the greatest confidence \(whether or not this subrule is in the original rule set\le-sets that are not guaranteed to have high-improvement rules \(such as those extracted from a decision tree\, the sub-rules can be used to potentially simplify, improve the generality of, and improve the predictive ability of the originals 8.     Evaluation This section provides an evaluation of Dense-Miner using two real-world data-sets which were found to be particularly dense in [4  1 The first data-set is compiled from PUMS census data obtained from It consists of 49,046 transactions with 74 items per transaction, with each transaction representing the answers to a census questionnaire. These answers include the age, taxfiling status, marital status, income, sex, veteran status, and location of residence of the respondent. Similar data-sets are used in targeted marketing campaigns for identifying a population likely to respond to a particular promotion. Continuous attributes were discretized as described in  though no frequently occurring items were discarded. The second data-set is the connect-4 data-set from the Irvine machine learning database repository It consists of 67,557 transactions and 43 items per transaction This data-set is interesting because of its size, density, and a minority consequent item \(\223tie games\224\ accurately predicted only by rules with low support. All experiments presented here use the \223unmarried partner\224 item as the consequent with the pums data-set, and the \223tie games\224 item with the connect-4 data-set; we have found that using other consequents consistently yields qualitatively similar results Execution times are reported in seconds on an IBM IntelliStation M Pro running Windows NT with a 400 MHZ Intel Pentium II Processor and 128MB of SDRAM. Execution time includes runtime for both the mining and post-processing phases The minsup setting used in the experiments is specified as a value we call minimum coverage where In the context of consequent constrained association rule mining, minimum coverage is more intuitive than minimum support, since it specifies the smallest fraction of the population of interest that must be characterized by each mined rule 8.1  Effects of minimum improvement The first experiment \(Figure 5\hows the effect of different minimp settings as minsup is varied. Minconf in these experiments is left unspecified, which disables pruning with the minimum confidence constraint. The graphs of the figure plot execution time and the number of rules returned for several algorithms at various settings of minimum support Dense-miner is run with minimp settings of .0002, .002, and 02 \(dense_0002, dense_002, and dense_02 respectively We compare its performance to that of the Apriori algorithm optimized to exploit the consequent constraint \(apriori_c This algorithm materializes only those frequent itemsets that contain the consequent itemset The first row of graphs from the figure reveals that apriori_c is too slow on all but the greatest settings of minsup for both data-sets. In contrast, very modest settings of minimp allow Dense-Miner to efficiently mine rules at far lower supports, even without exploiting the minconf constraint. A natural question is whether mining at low supports is necessary. For these data-sets, the answer is yes simply because rules with confidence significantly higher than the consequent frequency do not arise unless minimum coverage is below 20%. This can be seen from Figure 7 which plots the confidence of the best rule meeting the minimum support constraint for any given setting 2 This property is typical of data-sets from domains such as targeted marketing, where response rates tend to be low without focusing on a small but specific subset of the population The graphs in the second row of Figure 5 plot the number of rules satisfying the input constraints. Note that runtime correlates strongly with the number of rules returned for each algorithm. For apriori_c, the number of rules returned is the same as the number of frequent itemsets containing the consequent because there is no minconf constraint specified. Modest settings of minimp dramatically reduce the number of rules returned because most rules in these data-sets offer only insignificant \(if any\ predictive advantages over their proper sub-rules. This effect is particularly pronounced on the pums data-set, where a minimp setting of .0002 is too weak a constraint to keep the number of such rules from exploding as support is lowered. The increase in runtime and rule-set size as support is lowered is far more subdued given the larger \(though still small\inimp settings 1 Both data-sets are available in the form used in these experiments from http://www.almaden.ibm.com/cs/quest rR 316 hg  r 314 conf r   uconf g  226 minimp 263 rR 316 hg  r 314 rR 316 g hg  r 314 conf r   conf hg   226minimp  R http://augustus.csscr.washington.edu/census/comp_013.html http://www.ics.uci.edu/~mlearn/MLRepository.html 2 The data for this figure was generated by a version of Dense-Miner that prunes any group that cannot lead to a rule on the depicted support/confidence border. This constraint can be enforced during mining using the confidence and support bounding techniques from section 5 minimum coverage minsup sup C  244  


FIGURE 5 Execution time and rules returned versus minimum coverage for the various algorithms FIGURE 6 Execution time of dense_0002 as minconf is varied for both data-sets. Minimum coverage is fixed at 5% on pums and 1% on connect-4 FIGURE 7 Maximum confidence rule mined from each data-set for a given level of minimum coverage   1 10 100 1000 10000 100000 0 10 20 30 40 50 60 70 80 90 Execution time \(sec Minimum Coverage connect-4 apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 1e+06 0 10 20 30 40 50 60 70 80 90 Number of Rules Minimum Coverage connect-4 apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 0 10 20 30 40 50 60 70 80 90 Execution Time \(sec Minimum Coverage pums apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 1e+06 1e+07 0 10 20 30 40 50 60 70 80 90 Number of Rules Minimum Coverage pums apriori_c  dense_0002   dense_002   dense_02    0 500 1000 1500 2000 2500 3000 3500 20 25 30 35 40 45 50 55 60 65 Execution time \(sec minconf pums  connect-4  1 10 100 1000 10000 100000 1e+06 20 25 30 35 40 45 50 55 60 65 Number of Rules minconf pums  connect-4    0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100 Highest Rule Confidence Minimum Coverage pums  connect-4 


8.2  Effects of minimum confidence The next experiment \(Figure 6\ws the effect of varying minconf while fixing minimp and minsup to very low values. With connect-4, we used a minimum coverage of 1%, and with pums, a minimum coverage of 5%. Minimp was set to .0002 with both data-sets. As can be extrapolated from the previous figures, the number of rules meeting these weak minimp and minsup constraints would be enormous As a result, with these constraints alone, Dense-Miner exceeds the available memory of our machine The efficiency of Dense-Miner when minimum confidence is specified shows that it is effectively exploiting the confidence constraint to prune the set of rules explored. We were unable to use lower settings of minconf than those plotted because of the large number of rules. As minconf is increased beyond the point at which fewer than 100,000 rules are returned, the run-time of Dense-Miner rapidly falls to around 500 seconds on both data-sets 8.3  Summary of experimental findings These experiments demonstrate that Dense-Miner, in contrast to approaches based on finding frequent itemsets achieves good performance on highly dense data even when the input constraints are set conservatively. Minsup can be set low \(which is necessary to find high confidence rules as can minimp and minconf \(if it is set at all\This characteristic of our algorithm is important for the end-user who may not know how to set these parameters properly. Low default values can be automatically specified by the system so that all potentially useful rules are produced. Refinements of the default settings can then be made by the user to tailor this result. In general, the execution time required by Dense-Miner correlates strongly with the number of rules that satisfy all of the specified constraints 9.     Conclusions We have shown how Dense-Miner exploits rule constraints to efficiently mine consequent-constrained rules from large and dense data-sets, even at low supports. Unlike previous approaches, Dense-Miner exploits constraints such as minimum confidence \(or alternatively, minimum lift or conviction\ and a new constraint called minimum improvement during the mining phase. The minimum improvement constraint prunes any rule that does not offer a significant predictive advantage over its proper sub-rules. This increases efficiency of the algorithm, but more importantly it presents the user with a concise set of predictive rules that are easy to comprehend because every condition of each rule strongly contributes to its predictive ability The primary contribution of Dense-Miner with respect to its implementation is its search-space pruning strategy which consists of the three critical components: \(1\functions that allow the algorithm to flexibly compute bounds on confidence, improvement, and support of any rule derivable from a given node in the search tree; \(2\proaches for reusing support information gathered during previous database passes within these functions to allow pruning of nodes before they are processed; and \(3\ item-ordering heuristic that ensures there are plenty of pruning opportunities. In principle, these ideas can be retargeted to exploit other constraints in place of or in addition to those already described We lastly described a rule post-processor that DenseMiner uses to fully enforce the minimum improvement constraint. This post-processor is useful on its own for determining the improvement value of every rule in an arbitrary set of rules, as well as associating with each rule its proper sub-rule with the highest confidence. Improvement can then be used to rank the rules, and the sub-rules used to potentially simplify, generalize, and improve the predictive ability of the original rule set References 1 w a l  R.; Im ie lin ski  T   a n d S w a m i, A. 1 9 9 3   M i n i ng As so ciations between Sets of Items in Massive Databases. In Proc of the 1993 ACM-SIGMOD Int\222l Conf. on Management of Data 207-216 2 raw a l R.; M a n n ila, H Sri k an t  R T o i v o n en  H.; an d  Verkamo, A. I. 1996. Fast Discovery of Association Rules. In Advances in Knowledge Discovery and Data Mining AAAI Press, 307-328 3 K Ma ng a n a r is S a n d Sri k a n t, R 19 97  P a rtia l Cl a ssif i cation using Association Rules. In Proc. of the 3rd Int'l Conference on Knowledge Discovery in Databases and Data Mining 115-118 4 a rd o  R. J 1 9 9 8  Ef f i c i en tly Min i n g  Lo n g  P a ttern s fro m  Databases. In Proc. of the 1998 ACM-SIGMOD Int\222l Conf. on Management of Data 85-93 5  Mi c h ae l J. A a n d  Lin o f f G  S 1 9 9 7  Data Mining Techniques for Marketing, Sales and Customer Support John Wiley & Sons, Inc 6 Bri n, S  M o t w a n i, R.; Ullm a n J.; a n d  Tsu r S. 19 9 7 Dyn a m i c  Itemset Counting and Implication Rules for Market Basket Data. In Proc. of the 1997 ACM-SIGMOD Int\222l Conf. on the Management of Data 255-264 7 h e n  W   W   1 9 9 5 F a st Ef fecti v e Ru le In d u ctio n   In  Proc. of the 12th Int\222l Conf. on Machine Learning 115-123 8 In tern atio n a l Bu sin e s s Mac h in e s   1 9 9 6  IBM Intelligent Miner User\222s Guide Version 1, Release 1 9 m e t tin e n M   Ma nn ila  P  Ro nk a i ne n  P   a n d V e rk a m o  A  I. 1994. Finding Interesting Rules from Large Sets of Discovered Association Rules. In Proc. of the Third Int\222l Conf. on Information and Knowledge Management 401-407 10  Ng   R  T    L a k s hm ana n   V   S    Ha n  J   an d P a ng A  1 9 9 8   Exploratory Mining and Pruning Optimizations of Constrained Association Rules. In Proc of the 1998 ACM-SIGMOD Int\222l Conf. on the Management of Data 13-24 11 Ry mo n  R 1 9 9 2   Search  t h ro u g h Sy s t e m atic S e t En u m era tion. In Proc. of Third Int\222l Conf. on Principles of Knowledge Representation and Reasoning 539-550 1  Sha f e r  J  A g r a w a l R   an d Me ht a M 19 98  SPR I N T   A  Scalable Parallel Classifier for Data-Mining. In Proc. of the 22nd Conf. on Very Large Data-Bases 544-555 13  S m y t he P  and  Go od man   R  M 19 92 An I n f o r m at i o n Th eo retic Approach to Rule Induction from Databases IEEE Transactions on Knowledge and Data Engineering 4\(4\:301316 14  S r i k a n t   R    V u  Q an d Ag r a w a l  R  19 97 M i ni ng  A ssoc i a tion Rules with Item Constraints. In Proc. of the Third Int'l Conf. on Knowledge Discovery in Databases and Data Mining 67-73 15 W e bb, G. I 1 9 9 5 OP U S An Ef f i c i e n t Adm i ssible Algo rit h m for Unordered Search. In Journal of Artificial Intelligence Research 3:431-465 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


