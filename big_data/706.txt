Mining the smallest association rule set for predictions Jiuyong Li, Hong Shen and Rodney Topor School of Computing and Information Technology Griffith University Brisbane, Australia QLQ 4 1 1 1 jiuyong, hong, rwt cit.gu.edu.au Abstract Mining transaction databases for association rules usu ally generates a large number of rules most of which are unnecessary when used for subsequent prediction In this paper we define a rule set for a given transaction database 
that is much smaller than the association rule set but makes the same predictions as the association rule set by the con fidence priority We call this subset the informative rule set The informative rule set is not constrained to particular tar get items and it is smaller than the non-redundant associ ation rule set We present an algorithm to directly generate the informative rule set, i.e without generating all frequent itemsets first, and that accesses the database less often than other unconstrained direct methods 
We show experimen tally that the informative rule set is much smaller than both the association rule set and the non-redundant association rule set and that it can be generated more eficiently 1 Introduction The rapidly growing volume and complexity of modern databases makes the need for technologies to describe and summarise the information they contain increasingly im portant The general term to describe this process is data mining Association rule mining is the process of generat ing associations or more specifically, association rules in 
transaction databases Association rule mining is an im portant subfield of data mining and has wide application in many fields Two key problems with association rule min ing are the high cost of generating association rules and the large number of rules that are normally generated Much work has been done-to address the first problem Methods for reducing the number of rules generated depend on the application because a rule may be useful in one application but not another In this paper we are particularly concerned with gener ating rules for prediction For example, given 
a set of as 0-7695-1 119-8/01 17.00 0 2001 IEEE sociation rules that describe the shopping behavior of the customers in a store over time, and some purchases made by a particular customer, we wish to predict what other pur chases will be made by that customer The association rule set l can be used for prediction if the high cost of finding and applying the rule set is not a concern The constrained and optimality association sets 4 31 can not be used for 
this prediction because their rules do not have all possible items to be consequences The non redundant association rule set  171 can be used, after some extension but can be large as well We propose the use of a particular rule set, called the in formative association rule set, that is smaller than the as sociation rule set and that makes the same predictions under natural assumptions described below The general method of generating association rules by first generating frequent itemsets can be unnecessarily ex pensive as many 
frequent itemsets do not lead to useful as sociation rules We present a direct method for generating the informative rule set that does not involve generating the frequent itemsets first Unlike other algorithms that gener ate rules directly our method does not constrain the con sequences of generated rules as in 3 41 and accesses the database less often than other unconstrained methods  161 We show experimentally using standard synthetic data that the informative rule set is much smaller than both the association rule set and the non-redundant rule set and that it 
can be generated more efficiently 2 Related work Association rule mining was first studied in l Most research work has been on how to mine frequent itemsets efficiently Apriori 2 is a widely accepted approach, and there have been many enhancements to it 6,7,9,11 131 In addition, other approaches have been proposed 5 14 181 mainly by using more memory to save time For exam ple the algorithm presented in  organizes a database into a condensed structure to avoid repeated database ac 361 


cesses, and algorithms in  14 181 use the vertical layout of databases Some direct algorithms for generating association rules without generating frequent itemsets first have previously been proposed 4 3 161 Algorithms presented in 4 31 focused only on one fixed consequence and hence is not ef ficient for mining all association rules The algorithm pre sented in  161 needs to scan a database as many times as the number of all possible antecedents of rules As a result, it may not be efficient when a database cannot be retained in the memory There are also two types of algorithms to simplify the association rule set, direct and indirect Most indirect algo rithms simplify the set by post-pruning and reorganization as in 15 8 lo which can obtain an association rule set as simple as a user would like but does not improve effi ciency of the rule mining process There are some attempts to simplify the association rule set directly The algorithm for mining constraint rule sets is one such attempt  It produces a small rule set and improves mining efficiency since it prunes unwanted rules in the processing of rule min ing However, a constraint rule set contains only rules with some specific items as consequences, as do the optimality rule sets They are not suitable for association prediction where all items may be consequences The most significant work in this direction is to mine the non-redundant rule set because it simplifies the association rule set and retains the information intact  171 However the non-redundant rule set is still too large for prediction 3 The informative rule set 3.1 Association rules and related definitions Let I  l,2    m be a set of items and T C I be a transaction containing a set of items An itemset is defined to be a set of items, and a Ic-itemset is an itemset containing k items A database D is a collection of transac tions The support of an itemset e.g X is the ratio of the number of transactions containing the itemset to the num ber of all transactions in a database, denoted by sup\(X Given two itemsets X and Y where X n Y  0 an asso ciation rule is defined to be X  Y where sup\(X U Y and sup\(X U Y X are not less than user specified thresholds respectively sup\(X U Y X is called the conjidence of the rule denoted by conf\(X  Y The two thresholds are called the minimum support and the min imum confidence respectively For convenience, we abbre viate X U Y by XY and use the terms rule and association rule interchangeably in the rest of this paper Suppose that every transaction is given a unique identi fier A set of identifiers is called a tidset Let mapping t\(X be the set of identifiers of transactions containing the item set X It is clear that sup\(X  It\(X In the follow ing, we list some basic relationships between itemsets and tidsets 1 x c Y 5 t\(X 2 t\(Y 222 222 2 t\(X t\(Y  t\(X2 2 t\(Y2 for any 2 and 3 t\(XY  t\(X n t\(Y We say that rule X  Y is more general than rule X\221  Y if X c X\221 and we denoted this by X  Y c X\221  Y Conversely X\221  Y is more spec than X  Y We define the covered set of a rule to be the tidset of its an tecedent We say that rule X  Y identij5es transaction T if XY C T We use Xz to represent XU{z and sup\(X-2 for sup\(X  sup\(X2 3.2 The informative rule set Let us consider how a user uses the set of association rules to make predictions Given an input itemset and the association rule set. Initiate the prediction set to be an emp tyset Select a matched rule with the highest conference from the rule set and then put the consequence of the rule into prediction set We say that a rule matches a transac tion if its antecedent is a subset of the transaction To avoid repeatedly predicting on the same item\(s remove those rules whose consequences are included in the prediction set Repeat selecting the next highest confidence matched rule from the remaining rule set until the user is satisfied or there is not rule to select We have noticed that some rules in the association rule set will never been selected in the above prediction proce dure so we will remove those rules from the association rule set and form a new rule set This new rule set will pre dict exactly the same as the association rule set, the same set of prediction items in the same generated order Here, we consider the order because a user may stop selection at any time and we will guarantee to obtain the same prediction items in this case Formally given an association rule set R and an itemset P we say that the predictions for P from R is a sequence of items Q The sequence of Q is generated by using the rules in R in descending order of confidence For each rule r that matches P i.e for each rule whose antecedent is a subset of P each consequent of T is added to Q After adding a consequence to Q all rules whose consequences are in Q are removed from R To exclude those rules that never been used in the pre diction, we present the following definition Definition 1 Let RA be an association rule set and Ri the set of single-target rules in RA A set RI is informative over RA if\(1 RI c Ra 2 Vr E RI r\221 E RI such that r\222 c r and cmf\(r\222 2 conf\(r and 3 VT\224 E Ra  RI 3r E RI such that r\224 3 r and conf\(~\224 5 conf\(r 3 62 


The following result follows immediately Lemma 1 There exists a unique informative rule set for any given rule set We give two examples to illustrate this definition Example 1 Consider the following small transaction database 1  a,b,c},2  a,b,c},3  a,b,c},4  a b d 5  a c d 6  b c d Suppose the minimum support is 0.5 and the minimum confidence is 0.5 There are 12 association rules that exceed the support and con fidence threshholds They are a  b\(0.67,0.8 a  c\(0.67,0.8  c\(0.67,0.8  a\(0.67,0.8  a\(0.67,0.8  b\(0.67,0.8  c\(0.50,0.75  b\(0.50,0.75 bc  a\(0.50,0.75\a  bc\(0.50,0.60 b  aq0.50,0.60 c  ab\(0.50,0.60 where the numbers in parentheses are the support and conjidence respectively Every transaction identijied by the rule ab  c is also iden tified by rule a  c or b  c with higher confidence So ab  c can be omitted from the informative rule set without losing predictive capability Rule a  b and a  c pro vide predictions b and c with higher conjidence than rule a  bc so rule a  bc can be omitted from the infor mative rule set Other rules can be omitted similarly, leav ing the informative rule set containing the 6 rules a  b\(0.67,0.8  c\(0.67,0.8  c\(0.67,0.8  a\(0.67,0.8  a\(0.67,0.8  b\(0.67,0.8 Example 2 Consider the rule set a  b\(0.25,1.0\a  c\(0.2,0.7  c\(0.2,0.7  d\(0.3,1.0  d\(0.25 l.O Rule ab  c may be omitted from the in formative rule set as the more general rule a  c has equal confidence. Rule a  d must be included in the informa tive rule set even though it can be derived by transitivity from rules a  b and b  d Otherwise if it were omitted item d could not be predicted from the itemset a as the definition of prediction does not provide for reasoning by transitivity rule set Theorem 1 Let RA be an association rule set Then the informative rule set RI over RA is the smallest subset of RA such that for any itemset P the prediction sequence for P from RI equals the prediction sequence for P from RA Now we present the main property of the informative Proof We will prove this theorem from two aspects Firstly a rule omitted by RI does not affect prediction from RA for any P Secondly a rule set omitted one rule from RI cannot present the same prediction sequences as RA for any P Firstly we will prove that a rule omitted by RI do not affect prediction from RA for any P Consider a single-target rule r\222 omitted by RI there must be another rule r in both RI and RA such that the r c T\222 and con f I 2 cun f T\222 When T\222 matches P r does If both rules have the same confidence, omitting T\222 does not affect prediction from RA If conf\(r  conf\(r\222 T\222 must be automatically omitted from RA after r is selected and the consequence of T is included in the prediction sequenc So omitting T\222 does not affect prediction from RA Consider a multiple-target rule in RA e.g A  bc there must be two rules A\221  b and A\221  c in both RI and RA for A\222 C A such that conf\(A\221  b 2 cunf\(A  bc and conf\(A\222  c 2 conf\(A  c When rule A  bc matches P A\222  b and A\221  c do It is clear that if conf\(A\222  b  conf\(A\222  c  conf\(A  bc then omitting A  bc does not affect pre diction from RA If conf\(A\222  b  cmf\(A  bc and conf\(A\221  c  conf\(A  bc rule A  bc must be automatically omitted from RA after A\222  b and A\222  c are selected and item b and c are included in the prediction sequence Similarly we can prove that omit ting A  bc from RA does not affect prediction when conf\(A\222  b  conf\(A\221  c  conf\(A  bc or conf\(A\222  c  conf\(A\221  b  conf\(A  bc So omitting A  bc from RA does affect prediction. Similarly we can conclude that a multiple-target rule in RA does not affect its prediction sequence Thus a rule omitted by RI does not affect prediction from RA Secondly we will prove the minimum property. Suppose we omit one rule X  c from the RI Let P  X  there must be a position for c in the prediction sequence from RA determined by X  c because there is not other rule X\222  c such that X\222 c X and conf X\222  c 2 conf\(X  c When X  c is omitted from RI there may be two possible results for the prediction sequence from RI One is that item c does not occur in the sequence. The other is that item c is in the sequence but its position is determined by another rule X\222  c for X\221 C X with smaller confidence than X  c As a result the two prediction sequences would not be the same Hence, the informative rule set is the smallest subset of RA that provides the same predictions for any itemset P Consequently the theorem is proved 0 Finally we describe a property that characterises some rules to be omitted from the informative rule set We can divide the tidset of an itemset X into two parts on an itemset consequence t\(X  t\(X2 U t\(X4 If the second part is an empty set then the rule X  2 has 100 confidence Usually the smaller is It\(X4 the higher is the confidence of the rule Hence It\(X is very important in determining the confidence of a rule Lemma 2 Zft\(X4 C t\(Y+Z then rule XY  2 does not belong to the informative rule set 363 


Proof Let us consider two rules XY  2 and X  2 We know that conf\(XY  2  S~/\(SI  TI where s1  It\(XY2 and r1  It\(XY-Z and conf\(X  2  sa/\(s2  2 where s2  It\(X2 and 7-2  r1  t\(XYIZ   It\(X4\222 n t\(Ylz  It\(xlz It\(X4  r2 5-1  It\(XY2 5 It\(X2  s2 As a result conf\(XY j 2 5 conf\(X  2 Hence rule XY  2 must be excluded by the informative rule set 0 This is an important property for the informative rule set since it enables us to predict rules that cannot be included in the informative rule set in the early stage of association rule mining We will discuss this in detail in the next section 4 Upward closed properties for generating informative rule sets Most efficient association rule mining algorithms use the upward closed property of infrequency of itemset if an itemset is infrequent so are all its super itemsets Hence many infrequent itemsets are prevented from being gener ated in association rule mining and this is the essence of Apriori If we have similar properties of the rules excluded by the informative rule set then we can prevent generation of many rules excluded by the informative rule set As a result algorithm based on the properties will be more effi cient First of all we discuss a property that will facilitate the following discussions It is convenient to compare support of itemsets in order to find subset relationships among their tidsets This is because we always have support information when mining association rules We have a relationship for this purpose Lemma3 t\(X C_ t\(Y if and only if sup\(X  sup\(XY We have two upward closed properties for mining the informative association rule set In the following two lem mas we use a description that is easy to use in algorithm design but may not be very good in terms of mathematical simplicity As a direct result of Lemma 2 and 3 we have Lemma4 If sup\(X4  sup\(XY4 then rule XY  2 and all more spec$c rules do not occur in the informative rule set The following special case is useful in practice Lemma 5 If sup\(X  sup\(XY then for any 2 rule XY  2 and all more spec$c rules do not occur in the informative rule set These two lemmas enable us to prune unwanted rules in a 223forward\224 fashion before they are actually generated In fact we can prune a set of rules when we prune each rule not in the informative rule set in the early stages of the compu tation This allows us to construct efficient algorithms to generate the informative rule set 5 Algorithm 5.1 Basic idea and storage structure We proposed a direct algorithm to mine the informative rule set Instead of first finding all frequent itemsets and then forming rules, the proposed algorithm generates infor mative rule set directly An advantage is that it avoids gen erating many frequent itemsets that produce rules excluded by the informative rule set The proposed algorithm is a level wise algorithm which searches for rules from antecedent of 1-itemset to an tecedent of l-itemset level by level. In every level we select qualified rules which could be included in the informative rule set, and prune those unqualified rules The efficiency of the proposed algorithm is based on the fact that a number of rules excluded by the informative rule set are prevented from being generated once a more general rule is pruned by Lemma 4 or 5 Consequently, searching space is reduced after every level\222s pruning The number of phases of scan ning a database is bounded by the length of the longest rule in the informative rule set In the proposed algorithm we extend a set enumeration tree  121 as the storage structure, called candidate tree A simplified candidate tree is illustrated in Figure 1 The tree in Figure 1 is completely expanded but in practice only a small part is expanded We note that every set in the tree is unique and is used to identified the node, called identity set We also note that labels are locally distinct with each other under a parent node in a layer and labels along a path from the root to the node form exactly the identity set of the node This is very convenient for retrieving the itemset and counting its frequency In our algorithm a node is used to store a set of rule candidates 5.2 Algorithm for mining the informative rule set The set of all items is used to build a candidate tree A node in the candidate tree stores two sets A 2 A is an itemset the identity set of the node, and 2 is a subset of the identity itemset, called potential target set where every item can be the consequence of an association rule For example abc ab is a set of candidates of two rules namely bc  a and ac  b It is clear that the potential target set is initialized by the itemset itself When there is a case satis fying Lemma 4 for example sup\(a7c  sup\(abyc then 3 64 


aMp 23 24 34 Figure 1 A fully expanded candidate tree over the set of items 1,2,3,4 we remove c from the potential target set and accordingly all rules such as abX  c cannot be generated afterwards We firstly illustrate how to generate a new candi date node For example we have two sibling nodes abc}, {ab and abd ad then the new candidate is abcd ad where ad  ab U d n ad U c Hence the only two candidate rules that could be in cluded in the informative rule set in this case are bcd  a and abc  d given abcd is frequent We then show how to remove unqualified candidates One way is by the frequency requirement for example if sup\(abcd  D then we remove the node whose identity set is abcd simply called node abcd Please note here that a node in the candidate tree contains a set of candidate rules Another method is by the properties of the informative rule set and again consists of two cases Firstly, given a candi date node A 2 where A means that A is a Z-itemset For an item z E 2 when there is sup\(\(A'\\z  sup\(\(Al-'\\z for A1 3 A"-'\\z thenremove the z from Z by Lemma 4 Secondly we say node A 2 is restricted when there is sup\(A  sup\(A for A 3 A A restricted node does not extend its potential target set and keeps it as that of node A1 2 The reason is that all rules Al-'X j c for any X and c are excluded from the informative rule set by Lemma 5 so we need not generate such candidates This potential target set is remov able by Lemma 4 and a restricted node is dead when its potential target set is empty All super sets of the itemset of a dead node are unqualified candidates so we need not generate them We give the top level of the informative rule mining algorithm as the following Algorithm The informative rule miner minimum confidence  Input Database D the minimum support n and the Output The informative rule set R Set the informative rule set R  8 Count support of 1 itemsets Initialize candidate tree T Generate new candidates as leaves of T While new candidate set is non-empty Count support of the new candidates Prune the new candidate set Include qualified rules from T to R Generate new candidates as leaves of T Return rule set R The first 3 lines are general description, and we do not explain them here We will emphasize on two functions Candidate generator in line 4 and 9 and Pruning in line 6 They are listed as follows We begin with introducing notations in the functions ni is a candidate node in the candidate tree It is labeled by an item ini and it consists of an identity itemset Ani and a potential target set 2 7 is the 1-th level of candidate tree Pl\(A is the set of all l-subsets of A n is a node whose identity itemset is A The set of items are ordered lexically Function Rule candidate generator 1 for each node ni E Tl 2 3 ni such that 5 for each sibling node nj inj  ini generate a new candidate node nk as a son of Combining 4 A  A U Anj Z  zni U inj n Znj U ini Pruning if 3A E P'\(A but n  Ti then remove else if TIA is restricted then mark nk 6 7 restricted and let Z  Z fl Z 9 nk 8 else z  znA U A,,\\A n z if nk is restricted and Z  0 remove node nk We generate the I  1\candidates from the 1 layer nodes Firstly we combine a pair of sibling nodes and insert their combination as a new node in the next layer Secondly if any of its I-sub itemset cannot get enough support then we remove the node If an item is not qualified to be the target of a rule included in the informative rule set then we remove the target from the potential target set Please note that in line 6 not only a super set of an infrequent itemset is removed but also a super set of a frequent itemset of a dead node is removed The former case is common in association rule mining, and the latter case is unique for the informative rule mining A dead node is removed in line 9 Accordingly in the informative rule 365 


mining we need not to generate all frequent itemsets Function Pruning 1 2 3 4 if 3nj E for Anj c Ani such that then mark ni restricted and let for each ni E T~+I if sup\(Ani  0 remove node ni and return if ni is not restricted node do sup  sup Zni  2 fl Znj 5 6 that sup Anj z U iz  sup Ani z U iz 7  Lemma 4 for each z E Zni if 3nj E Zl for Anj\\z c Ani\\z such then Zi  Zi  z I Lemma 5 if ni is restricted and Zni  8 remove node ni We prune a rule candidate from two aspects the fre quency requirement for association rules and the qualifica tion requirement for the informative rule set The method for pruning infrequent rules is the same as that of a general association rule mining algorithm As for the method in pruning unqualified candidates for the informative rule set we restrict the possible targets in the potential target set of a node a possible target is equivalent to a rule candidate and remove a restricted node when its potential target set is empty 5.3 Correctness and efficiency Lemma 6 The algorithm generates the informative rule set properly It is very hard to give a closed form of efficiency for the algorithm However we expect certain improvements over other association rule mining algorithms based on the fol lowing reasons Firstly it does not generate all frequent itemsets because some frequent itemsets cannot contain rules being included in the informative rule set Secondly it does not test all possible rules in every generated frequent itemset because some items in an itemset are not qualified as consequences for rules being included in the informative rule set The phases of scanning a database is bounded by the length of longest rule in the informative rule set 6 Experimental results In this section we show that the informative rule set is much smaller than both the association rule set and the non-redundant association rule set We further show that it can be generated more efficiently with less number of in teractions with a database Finally we show that the effi ciency improvement gains from the fact that the proposed algorithm for the informative rule set accesses the database fewer times and generates fewer candidates than Apriori for the association rule set Since the informative rule set contains only single target rules for a fair comparison, the association rule set and the non-redundant rule set in this section contain only single target rules as well The reason for the comparison with the non-redundant rule set is that the non-redundant rule set can make the same predictions the association rule set The two testing transaction databases T10.16.DlOOK.N2K and T20.16.DlOOK.N2K are gen erated by the synthetic data generator from QUEST of IBM Almaden research center Both databases have 1000 items and contain 100,000 transactions We chose the minimum support in the range such that 70 to 80 of all items are frequent and fixed the minimum confidence as 0.5 0\222 T1016DlOXMK II  0 25 03 Tb mmm e I x 015 12 05 1 0  03 h Figure 2 Sizes of different rule sets Sizes of different rule sets are listed in Figure 2 It is clear that the informative rule set is much smaller than both the association rule set and the non-redundant rule set The size difference between an informative rule set and an asso ciation rule set becomes more evident when the minimum support decreases, and as does the size difference between an informative rule set and a non-redundant rule set This is because the length of rules becomes longer when the mini 366 


mum support decreases, and long rules are more likely to be excluded by the informative rule set than short rules There is little difference in size between an association rule set and a non-redundant rule set So in the following comparisons we only compare the informative rule set with the associa tion rule set Now we will compare generating efficiency of the in formative rule set and the association rule set We imple mented Apriori on the same data structure as the proposed algorithm and generated only single target association rules Our experiments were conducted on a Sun server with two 200 MHz UltraSPARC CPUs I iln Figure 3 Generating time for different rule sets The generating time for association rule sets and infor mative rule sets is listed in the Figure 3 We can see that mining an informative rule set is more efficient than mining a single target association rule set This is because the in formative rule miner does not generate all frequent itemsets and does not test all items as targets in a frequent itemset The improvement of efficiency becomes more evident when the minimum support decreases This is consistent with the deduction of rules being excluded from an association rule set as shown in Figure 2 I L I i 33 Figure 4 The number of times for scanning the database Further, the number of times of scanning a database for generating an informative rule set is smaller than that for an association rule set as showed in Figure 4 This is because the proposed algorithm avoids generating many long fre quent itemsets that contain no rules included in an informa tive rule set. From the results we also know that long rules are easier to be excluded by an informative rule set than short rules Clearly this number is significantly smaller than the number of different antecedents in the generated rule set which are needed to scan a database in aother direct algorithm To better understand of improvement of efficiency of the algorithm for mining the informative rule set over that for the association rule set we list the number of nodes in a candidate tree for both rule sets in Figure 5 They are all frequent itemsets for the association rule set and partial frequent itemsets searched by mining the informative rule set We can see that in mining the informative rule set, the searched itemsets is less than all frequent itemsets for form ing association rules So this is the reason for efficiency im provement and reduction in number of scanning a database 7 Conclusions We have defined a new informative, rule set that gener ates prediction sequences equal to those generated by the association rule set by the confidence priority The infor mative rule set is significantly smaller than the association rule set especially when the minimum support is small We have studied the upward closed properties of informative rule set for omission of unnecessary rules from the set and presented a direct algorithm to efficiently mine the informa tive rule set without generating all frequent itemsets first 367 


10  B 8 B  6 4 2\222 0 0 15 02 0 25 03 Figure 5 The number of candidate nodes The experimental results confirm that the informative rule set is significantly smaller than both the association rule set and the non-redundant association rule set, that can be gen erated more efficiently than the association rule set The experimental results also show that this efficiency improve ment results from that the generation of the informative rule set needs fewer candidates and database accesses than that of the association rule set The number of database accesses of the proposed algorithm is much smaller than other direct methods for generating unconstrained association rule sets Although the informative rule set provides the same pre diction sequence as the association rule set there may exist other definitions of 223interesting\224 in different applications How to use the informative rule set to make predictions un der such different criteria remains a subject of future work References I R Agrawal T Imielinski and A. Swami Mining associa tions between sets of items in massive databases In Proc of the ACM SIGMOD Int\222l Conference on Management of Data 1993 2 R Agrawal and R Srikant. Fast algorithms for mining asso ciation rules in large databases In Proceedings of the Twen tieth International Conference on Very Large Databases pages 487-499 Santiago Chile, 1994 3 R Bayardo and R Agrawal Mining the most interesting rules In S Chaudhuri and D Madigan editors Proceed ings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining pages 145-1 54 N.Y Aug 15-18 1999. ACM Press 4 R Bayardo R Agrawal and D Gunopulos Constraint based rule mining in large dense database In Proc of the 15th Int\222l Con on Data Engineering pages 188-197 1999 5 J Han J Pei and Y Yin Mining frequent patterns with out candidate generation In Proc 2000 ACM-SIGMOD Int Con5 on Management of Data SIGMOD\222OO pages 1  12 May 2000 6 M. Holsheimer, M. Kersten H Mannila and Toivonen A perspective on databases and data mining In Ist Intl Conf Knowledge Discovery and Data Mining Aug 1995 7 M Houtsma and A Swami Set-oriented mining of associ ation rules in relational databases In 11th Intl Conj data Engineering 1995 8 B Liu W Hsu and Y Ma Pruning and summarizing the discovered associations In SIGKDD 99 1999 9 H Mannila H. Toivonen and I Verkamo Efficient algo rithms for discovering association rules In AAAI Wkshp Knowledge Discovery in Databases July 1994 lo R Ng L Lakshmanan J Han and A Pang Exploratory mining and pruning optimizations of constrained associa tions rules In Proceedings of the ACM SIGMOD Interna tional Conference on Management of Data SIGMOD-98 volume 27 of ACM SIGMOD Record pages 13-24 New York Junel-4 1998 ACM Press  111 J S Park M Chen, and P S Yu An effective hash based algorithm for mining association rules In ACM SIGMOD Intl Conf Management of Data May 1995  121 R Rymon Search through systematic set enumeration In W Nebel Bernhard Rich, Charles Swartout editor Pro ceedings of the 3rd International Conference on Principles of Knowledge Representation and Reasoning pages 539 552, Cambridge, MA oct 1992. Morgan Kaufmann 13 A Savasere R Omiecinski, and S Navathe An efficient algorithm for mining association rules in large databases In 21st VLDB Conf 1995 14 P Shenoy J R Haritsa S Sudarshan G Bhalotia M. Bawa and D Shah Turbo-charging vertical mining of large databases In Proceedings of the ACM SIGMOD In ternational Conference on Management of Data \(SIGMOD 99 pages 22-33 151 H. Toivonene M Klemettinen P RonKainen K Hatonen and H. Mannila Pruning and grouping discovered associa tion rules Technical report Department of Computer Sci ence University of Helsinki Finland, 1998 161 G I Webb Efficient search for association rules In R Ra makrishnan S Stolfo R Bayardo and 1 Parsa editors Pro ceedinmgs of the 6th ACM SIGKDD International Confer ence on Knowledge Discovery and Data Mining KDD-00 pages 99-107 N Y Aug 20-23 2000 ACM Press 171 M J Zaki Generating non-redundant association rules In 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining pages 34  43 August 2000 181 M J Zaki S Parthasarathy M Ogihara, and W Li New al gorithms for fast discovery of association rules In D Heck erman H Mannila D Pregibon and R Uthurusamy edi tors Proceedings of the Third International Conference on Knowledge Discovery and Data Mining KDD-97 page 283 AAAI Press 1997 368 


 0 5 10 15 20 25 30 35 40 10 15 20 25 30 average number of items in transactions T I=6 D=200K  o f d ata p rocessed 0 50 100 150 200 250 300 350 400 450 500 ti m e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 5 Pruning and CPU time varying T 0 2000 4000 6000 8000 10000 12000 14000 10 15 20 25 30 average number of items in transactions T I=6 D=200K number o f r andom I  Os SG-table SG-tree Figure 6 Random I/Os varying T 0 5 10 15 20 25 30 35 40 6121824 average length of large itemsets I T=30 D=200K  o f d ata p rocessed 0 50 100 150 200 250 300 350 400 450 500 ti m e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 7 Pruning and CPU time varying I 0 2000 4000 6000 8000 10000 12000 14000 6121824 average length of large itemsets I T=30 D=200K numbe r of ra ndom I  O s SG-table SG-tree Figure 8 Random I/Os varying I 0 2 4 6 8 10 12 14 16 T=10,I=6 T=20,I=12 T=30,I=18 T=40,I=24 T=50,I=30 Varying T and I I/T=0.6 D=200K  o f d ata p rocessed 0 20 40 60 80 100 120 140 160 tim e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 9 Pruning and CPU time 336xed I/T 0 200 400 600 800 1000 1200 1400 1600 1800 2000 T=10,I=6 I=12 T=30,I=18 I=24 T=50,I=30 Varying T and I I/T=0.6 D=200K numbe r of ra ndom I  Os SG-table SG-tree Figure 10 Random I/Os 336xed I/T 0 1 2 3 4 5 6 7 100 00 300 400 500 Data set cardinality T=10 I=6  o f d ata p rocessed 0 10 20 30 40 50 60 70 80 90 ti m e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 11 Pruning and CPU time varying D 0 10 20 30 40 50 60 0 1 to 3 4 to 10 11 to 20 20 distance of nearest neighbor T30.I18.D200K  o f d at a p rocessed 0 100 200 300 400 500 600 700 800 time m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 12 Pruning and CPU time var 000 000 003 005 007 t  83  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


shows a pattern similar to the CPU cost as in the previous experiments During the experiments we observed that queries having a close nearest neighbor were processed fast using both structures whereas for cases with distant neighbors the SG\226tree was signi\036cantly faster than the SG\226table We validated this observation by running 1000 queries on the T30.I18.D200K dataset and averaging the query costs for various distance ranges of the nearest neighbor Figure 12 shows the average pruning an d CPU cost for 036ve distance ranges When the distance is small search is fast for both methods actually for distances in the range 1\2263 the SG\226 table outperforms the SG\226tree However the distant cases are handled much faster by the SG\226tree showing that this access method is more robust to 221outlier\222 queries As a general conclusion from this set of experiments the SG\226tree is a more ef\036cient and robust access method than the SG\226table in addition to its other inherent advantages dynamic data handling independence to hard-wired constants In the next subsection we compare the indexes for other query types on both synthetic and real data 5.4 Real data nd other queries Figures 13 and 14 show the performance of the indexes for 000 NN queries on the T30.I18.D200K synthetic dataset and the CENSUS dataset respectively for various values of 000  The results for each experimental instance were averaged over 100 queries In both 036gures for small to medium values of 000 the SG\226tree is signi\036cantly faster than the SG\226 table When 000 is large  001 003 005 005 005  the fraction of the data that need to be visited becomes too large for the indexes to be useful This is due to the fact that the search space becomes less appropriate for search For example when 000 t 003 005 005 005 005 we observed that the average distance of the 000 th neighbor is very large 31.81 for T30.I18.D200K and 18.06 for CENSUS and very close to the average distance of all transactions from f  This is due to the 221dimensionality curse\222 effect 3 o ften o b s erv e d i n h ig h d i men s io n a l search problems Observe that the SG\226tree is less sensitive to this effect since its performance degenerates at a smaller pace especially for the real dataset We also compared the indexes for similarity range queries Figures 15 and 16 The same datasets and queries as before are used and the distance threshold from the query varies from 2 to 10 For r t 020  the SG\226table outperforms the SG\226tree on the synthetic dataset In all other cases the tree is much faster Observe that on the real dataset in particular for both 000 NN queries and range queries the performance difference quite large in favor of the tree This indicates that the structure can perform very well in real life cases  0 10 20 30 40 50 60 70 80 90 100 1 10 100 1000 10000 k-nn search varying k T30.I18.D200K  o f d ata p rocessed 0 200 400 600 800 1000 1200 1400 time\(msec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 13 021 NN queries T30.I18.D200K 0 10 20 30 40 50 60 70 80 90 100 1 10 100 1000 10000 k-nn search varying k CENSUS  o f d ata p rocessed 0 100 200 300 400 500 600 tim e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 14 022 NN queries CENSUS 5.5 Dynamic data changes In this experiment we compare the structures simulating a case where the nature of the data changes dynamically We generated a synthetic dataset T10.I6.D100K and built an SG\226table and SG\226tree for it We then gradually updated the structures by inserting batches of 100K transactions each with the same characteristics i.e T=10 I=6 but putting different seeds to the random generator i.e the large itemsets used were different for each batch We ran nearest neighbor queries on the two structures after each insertion phase The queries for phase 023 after batch 023 has been inserted 024 026 023 026 032  are generated as follows For each query i a random number 033 from 1 to 023 is chosen and ii the generator parameters i.e large itemsets for batch 033 are used to produce the query For example a query for the phase where the dataset contains 300K data is generated using randomly one of the generators of batches 1 2 or 3 Figure 17 shows the average pruning ef\036ciency and CPU time of the two structures Initially both have similar performance but as more data with different characteristics are inserted into the structures the performance of the SG\226table degenerates since it is optimized for the 036rst 100K data  84  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


 0 5 10 15 20 25 30 35 40 246810 similarity range queries varying epsilon T30.I18.D200K  o f d at a p r o cessed 0 50 100 150 200 250 300 350 400 tim e m s e c  SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 15 Range queries T30.I18.D200K 0 10 20 30 40 50 60 70 80 246810 similarity range queries varying epsilon CENSUS  o f d ata p r o cessed 0 50 100 150 200 250 300 350 400 ti me\(msec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 16 Range queries CENSUS On the other hand the SG\226tree is robust to updates and exhibits very good query performance since each batch contains skewed data generated from a different collection of large itemsets 6 Conclusions and Future Work We presented a hierarchical indexing method for similarity search in sets and categorical data The SG\226tree is a disk-based height-balanced tr ee that organizes 036xed-length bitmaps and is appropriate for various query types We have shown how several branch-and-bound methods which apply on R\226tree-like structures can be adapted for ef\036cient similarity search on the SG\226tree Extensive experimental evaluation has shown that the SG\226tree is in most cases much faster than the SG\226table a previous hash-based index The advantages of the SG\226tree can be summarized as follows 000 It is ef\036cient and robust to various data types both categorical and set data and characteristics cardinality density dimensionality It is a versatile structure that can be used for several query types 000 The tree is dynamically adapted to updates and re0 2 4 6 8 10 12 100 200 300 400 500 Dataset cardinality T=10 I=6  o f d ata p rocessed 0 20 40 60 80 100 120 140 160 180 200 tim e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 17 NN search after dynamic updates quires no preprocessing of the data Thus it can be useful for analyzing data which change dynamically over time 001 It relies on no hardwired constants and requires no tuning using a-priori de\036ned parameters 001 It is a disk-based paginated data structure so it can operate with limited memory resources and dynamically changing memory resources Caching policies previously used for the B 002 226tree and the R\226tree can be seamlessly applied on this structure There are several directions for extending the current work In our study we used hamming distance as the similarity metric However the SG\226tree can also be de\036ned tuned and searched for other set theoretic similarity metrics For example if the Jaccar d coef\036cient is used the lower distance bound in fact the upper similarity bound for nearest neighbor search can be de\036ned by 003 005 007 b n f 016 007 020 021 023 024 026 030 003 005 007 b n f 026  We plan to test the effectiveness of the structure using alternative metrics Another direction or future work is to study methods for bulk-loading SG\226trees instead of inserting the data oneby-one We can adapt categor ical clustering algorithms 12 for t hi s purpos e Anot her a pproach i s t o s o rt t h e transactions using gray codes as key in analogy to using space-\036lling curves for bulk-loading multidimensional data to an R\226tree 17  A lternati v ely  hashing t echniques can be used to group similar signatures together The resulting 221globally-optimized\222 tree could have much better quality characteristics while being built faster In a reverse direction we can investigate whether the SG\226tree can be used for clustering large dynamic collections of set and categorical data The cost of existing methods is at least 035 n   026 and the tree could be used to derive good clusters much faster e.g by merging the leaf nodes using their signatures as guides Finally we plan to empirically test the ef\036ciency of the tree to the query types discussed in Section 4.2 In  85  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


addition for some data types search can be further optimized For example if the indexed categorical data have 223xed-dimensionality 000 we know that the area of each indexed signature is 223xed to 000  We can use this property to derive stricter lower bounds for the directory node entries 001  instead of the rather relaxed 002 004 006 006 t 013 r 001 020 022 004 025 027 For this example a better bound is 002 004 006 006 t 013 r 001 020 022 004 025 027 033 t 000 037    t 001 020 022 004 025 r 013 027 027  We plan to study such search optimizations using domain properties or statistics from the indexed data References  C  C  A ggarw al  J  L  W ol f and P  S  Y u A N e w Method for Similarity Indexing of Market Basket Data SIGMOD Conference  pages 407\205418 1999  R  A gra w al and R  S ri kant  F as t A l gori t h ms for M i n ing Association Rules in Large Databases VLDB Conference  pages 487\205499 1994  K  S  B e y er  J  G ol ds t e i n  R  R amakri s hnan and U Shaft When Is 215Nearest Neighbor\216 Meaningful International Conference on Database Theory  pages 217\205235 1999  T  B ri nkhof f H.-P  K ri e g el  a nd B  S e e g er  E f 223 ci ent Processing of Spatial Joins Using R-Trees SIGMOD Conference  pages 237\205246 1993  A  C orral  Y  Manol opoul os  Y  T heodori d i s  a nd M Vassilakopoulos Closest Pair Queries in Spatial Databases SIGMOD Conference  pages 189\205200 2000  A  P  d e V ries N  M amoulis N  N es a nd M K e r sten Ef\223cient k-NN Search on Vertically Decomposed Data SIGMOD Conference  pages 322\205333 2002  U  D eppisch S-T r ee A D ynamic B alanced Signature Index for Of\223ce Retrieval ACM SIGIR Conference  pages 77\20587 1986  V  G aede a nd O G 250 unther Multidimensional Access Methods ACM Computing Surveys  30\(2\170\205231 1998  V  G ant i  J  Gehrk e  a nd R  R a makri s hnan C A C T US 205 clustering categorical data using summaries ACM SIGKDD Conference on Knowledge Discovery and Data mining  pages 73\20583 1999  D Gi bs on J  M Kl ei nber g  a nd P  R a gha v a n C l us tering Categorical Data An Approach Based on Dynamical Systems VLDB Conference  pages 311\205322 1998  A Gi oni s  D Gunopul os  a nd N K oudas  Ef 223 c i e nt and Tunable Similar Set Retrieval SIGMOD Conference  2001  S  Guha R  R as t ogi  a nd K S h i m  R OC K A R obust Clustering Algorithm for Categorical Attributes International Conference on Data Engineering  pages 512\205521 1999  A Gut t m an R T rees  A Dynami c I nde x S t r uct u re for Spatial Searching SIGMOD Conference  pages 47\205 57 1984  S  Hel m er and G  M oerk ot t e  A S t udy of F our Inde x Structures for Set-Valued Attributes of Low Cardinality Technical Report University of Mannheim  number 2/99 1999  G R Hjaltason a nd H Samet Distance Bro w sing in Spatial Databases TODS  24\(2\265\205318 1999  A K J a i n and R  C  D ubes  Algorithms for Clustering Data  Prentice-Hall 1988  I Kamel a nd C  F a louts o s  Hilbert R tree An Improved R-tree using Fractals VLDB Conference  pages 500\205509 1994  F  K o rn N  S i d i r opoul os  C  F al out s o s  E S i e g el  a nd Z Protopapas Fast Nearest Neighbor Search in Medical Image Databases VLDB Conference  pages 215\205 226 1996  N K oudas a nd K C  S e vci k  H i g h D i m ens i onal S i m i larity Joins Algorithms and Performance Evaluation International Conference on Data Engineering  pages 466\205475 1998  N R ous s opoul os  S  K el l e y  and F  V i n cent  Neares t Neighbor Queries SIGMOD Conference  pages 71\205 79 1995  Y  S a kurai  M  Y os hi ka w a  S  U emura and H  K oj i m a The A-tree An Index Structure for High-Dimensional Spaces Using Relative Approximation VLDB Conference  pages 516\205526 2000  The U C I KDD Archi v e ht t p    kdd.i c s  uci  edu 23 R W e b e r  H.-J S ch ek  a n d S Blo tt A Q u a n titative Analysis and e Study for SimilaritySearch Methods in High-Dimensional Spaces VLDB Conference  pages 194\205205 1998  86  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


