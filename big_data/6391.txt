Enhancing Parallel Data Mining Performance on a Large Cluster using UCE Scheduling  Nunnapus Benjamas and Putchong Uthayopas Department of Computer Engineering, Kasetsart University, Bangkok 10900, Thailand g4885043,pu}@ku.ac.th   Abstract In this paper, we propose an algorithm called Unified Communication and Execution Scheduling \(UCE\at combines the execution and communication scheduling for parallel data mining application together.  This algorithm enables a better utilization of hardware and interconnection in a multicore cluster system for the data mining application. The idea is to choose a proper task execution sequence combine with a communication scheduling that avoids the communication conflict in the interconnection network switch. The simulation results show that a substantial performance improvement can be obtained especially with the large multicore cluster systems I I NTRODUCTION  Currently, Human society is striving to understand more about the relationship among complex data collected from everywhere. This is substantially important in many domains such as inferring patterns from web page access logs bioinformatics, and the analysis of medical, scientific, and commercial da T h us a f a s t a n d s c a l able d ata m i n i ng  technique becomes increasingly more important. Faster processing speed enables users to experiment with various approaches and processes a much larger data set. Therefore many researchers have worked on improving and inventing the large-scale parallel and distributed data mining algorithms on scalable parallel computing systems 3 4  5  6  7    1 0  1 1  1 2 13    1 4  1 5 1 6   1 7  Th ese include the shared memory multiprocessor system [9 a n d  distributed memory multicomputer system [1 17 o r  cluster.  Since the parallel data mining application typically generates a lot of communication messages, the optimization of the communications structure by employing special characteristics of the computing system can result in a great performance enhancement In this paper, we propose a scheduling strategy called Unified Communication and Execution Scheduling  UCE In our approach, the communication and execution of the subtasks of parallel data mining will be controlled together to improve the performance. The parallel FI-growth data mining 30 is used to de m o n s t r at e  th e pr o p osed con cept. Usi ng  simulation, UCE scheduling is shown to give a substantially improved performance for parallel FI-Growth application The rest of this paper is organized as follows: Section II describes related researches in this area. Section III presents the system model followed by Section IV that discusses the design of a right scheduling strategy. Section V shows our experimental evaluation. Finally, Section VI presents the conclusion and future work II L ITERATURE R EVIEW  The Apriori algorithm 2 22] has e m erg ed as on e o f  t h e best classical algorithm for the association rule mining since 1994. The approach used is based on the çgenerate-and-test paradigm. To enhance the performance, Park et al proposed the DHP algorithm standing for direct hashing and pruning\ which is an extension of the Apriori algorithm. This hash-based technique can reduce number of generated candidate itemsets, so that it affects less computational cost in later iterations. Han et al. [2 presented  a new  alg or ith m  for mining association rules called the FP-growth algorithm This algorithm finds a complete set of frequent itemsets using a Frequent Pattern tree \(FP-tree\ucture to avoid candidate itemsets generation. Recently, Amphawan and Sur  presented the Frequent Item growth \(FI-growth algorithm, for creating a new FP-tree called a Frequent Item tree \(FI-tree They have shown that the complete set of frequent itemsets can be generated using a single tree. Anyway, as the size of the data grow, use of parallel data mining algorithms becomes necessary to obtain the result in a reasonable time Several parallel data mining algorithms have been designed for association rule mining [2 2 2 Many o f  t h e m  employ the generation-and-test approach 1   example, Agrawal and Shaf h ree pa rall el  approaches for mining association rules: count distribution data distribution, and candidate distribution. There are some works that use pattern growth   1 3  2  based o n  FP-tree For t h e p a r a l lel ver sio n of  FPg r o wth al go r i t h m   the existing parallel attempt on cluster of shared-nothing environment was reported in [2  The PDM algo r i t h m  2 9 is a parallel version of the DHP, i.e., each processor determines the global supports of 1-itemsets through an all-to-all broadcast and then approximates the counts for 2-itemsets with a hash table. Zaki [25 also d isc ussed several par all el  association rule-mining algorithms on various parallelization schemas. Recently, efficiently partitioning database methods for optimal load-balance and minimum communication overheads have also been reported  e sp ectively Hardware platform issues are also addressed in the parallel implementation of association rule mining, such as shared memory [9   19   and d i stribut ed m e m o r y  1 7], [13  28 Most of the works address how an association rule-mining algorithm will perform on an ideal distributed system Nevertheless, parallel data mining applications usually generates a lot of communication messages. Thus, a proper optimization of these communications on an infrastructure has 17 


a potential to substantially increase the performance of a parallel data mining application III P ARALLEL FIGROWTH A PPLICATION  In order to understand the proposed approach, the Parallel FI-growth [30 u s ed f o r the st udy is e x plai ne d This al gorith m  is an algorithm for parallel data mining that parallelizes the association rule mining process. The algorithm employs a data parallelism technique on a multicore cluster \(see the workflow in [3 The work f low consists o f  3 p h ases  w hich  are preprocessing phase, FI-tree construction phase, and mining phase In the preprocessing phase, we first partition the transaction database into several portions, and distribute them to different processors for computation. In the FI-tree construction phase each processor independently constructs its own local FI-tree structure and discovers corresponding frequent itemsets However, all processors need to perform a one-time synchronization to exchange their sub-trees before the last two steps in the mining phase  A Application model In this paper, the parallel application is modeled using a Directed acyclic graph \(DAG which is used as the input to the scheduling simulator                  a b  Fig 1. \(a\on model and \(b\ystem model  First, let T  T 0 T 1 T n denotes  a set of tasks to be executed, a directed acyclic graph can be defined as a graph G= \(V, E where V is a set of nodes v 1  v 2  v n and E is a set of directed edges e ij The edge e ij in the DAG connecting nodes v i and v j represents the communication and precedence constraints among the nodes. Let the weight of an edge W  e ij  denotes the communication cost between two tasks. \(See Fig 1a\. A node v i in the DAG corresponds to task T i Each node of the task graph has a weight W  v i represents the computing cost In this application model, we assume that the execution task is an indivisible unit of computation, and that a processor executes one task at a time. Upon receiving all the input data the task can start the execution. For communication contention, a task cannot receive data from all its predecessors simultaneously, but it receives data sequentially from the same communication channel  B System model In this paper, we use a multicore cluster system as the target architecture. A multicore cluster system is a set machine M   m 1  m 2  m M of |M| machines, and each machine m i  includes a set processor P i  p 1  p 2  p pi of P i  processors. Each processor p j of machine m i has multiple core C i  c 1  c 2  c Cij of C ij cores. The total number of processors and the total number of cores is      1 M i i total p P and      1   1 M i p j ij total i C C respectively In Fig. 1b, the multicore cluster system is a tree-structure with cores c as leaves, processors p as intermediate nodes being a parent for cores, machines m as intermediate nodes combining processors and the entire machine or system S  root node. Let S  V  E  W V  W E   V  v 1  v 2  v n is a set of nodes, and E is a set of undirected edges V represents the set of cores in the system, and an edge e ij    E represents communication link between core v i and v j  W V  v i d W E  e ij  are the computational cost of core v i and the communication cost between core v i and v j respectively In general, a multi-core cluster system is connected together using a hierarchy of communication networks.  First, an intercore communication uses the shared-cache between cores on the same processor. Second, an inter-processor communication uses shared-memory between cores on the different processor. Finally, an inter-machine communication  has to go through interconnection network that links these machines together. In this work, we assume that the bandwidth between cores is 20 GBps, bandwidth between processors is 10 GBps, and bandwidth between machines is 1 GBps It is assumed that the number of processors and the number of tasks are static and known beforehand.  In this paper, we use an all-to-all personalized communication to illustrate that a proper communication scheduling of FI-growth data mining can substantially increase the speed of the application. In general, interconnection network in a cluster usually consists of a switch fabric and a network links, which connect computation node with a certain bandwidth such as 1Gbps When more than one communication session is initiated to a single target node, bandwidth linking the switch and node will be shared. The bandwidth is then reduced, and more latency is inserted into the computation. An example is given in Fig. 2 In Fig. 2a, it is a fully connected network. Thus, every machine \(m\ has its own link with every other machine via switch \(S  In Fig. 2b, that task T 2 and task T 4 need to send messages to the task T 1 in the same time. To avoid this problem, we proposed that the communication should be schedule. The benefit of our proposed strategy is that the communication between a pair of node is contention free, as    p m p c c c c p m p c c c c S 1  5   0  9   13   17   21   25   2  6   10   14   18   22   3  7   11  15   19  23  4  8  12   16   20   24   18 


shown in Fig. 2c. \(Communication scheduling is presented in greater detail in Section IV.\ress communication contentions, we assume that only a limited number of communications can pass from the processor into the network and from the network into the processor at one communication in the same time   S S S S m  m  m  m  T 2 T 3 T 1 T 4 T 2 T 3 T 1 T 4 a   b  c  Fig. 2. \(a\twork model, \(b\ore than one communication sessions are initiated to a single target node. \(c\ The communication between a pair of node is contention free  In our previous s e d a s t r a t egy call ed    Execution Only Scheduling \(EO to improve the application performance. In EO method, the scheduling is divided into two steps; ordering of the tasks, and the mapping of the tasks onto processing units. The detail is as given below Ordering step The task graph represented as a DAG is assigned a priority. We use smallest-numbered available task first or Left-to-Right \(L-R to determine the priorities of nodes For example, a scheduling list from task graph in Fig. 1a is T 0  T 1  T 2  T 3  T 4  T 5  T 6  T 7  T 8  T 9  T 10  T 11  T 12  T 13  T 14  T 15  T 16  T 17  T 18  T 19  T 20  T 21 T 22  T 23  T 24 and T 25  Mapping step The mapping considers two factors; the largest data transfer size and the earliest start-time. First, task will be mapped onto the same node as its parents that generate the largest data transfer size to that task. If more than one parent nodes exist with the same largest transfer size, the processing unit with earliest start-time first is selected In this work, we propose a strategy based on the previous that can even give a better performance than the last scheme This is explained in the following section IV D ESIGNING THE S CHEDULING S TRATEGIES  In order to further enhance the performance, we propose an approach called Unified Communication and Execution Scheduling  UCE The concept is to combine both computation and communication scheduling together to achieved a much better performance In UCE scheduling, the execution is decomposed into two steps the nodes partitioning and communication scheduling  These two steps execute alternately. For each step, the execution will proceed as follows Partitioning step A set of n compute node is divided into two balanced partitions and then recursively divided the two partitions until the number of node of each partition is one Communication step In each partitioning step, all nodes in each partition will be scheduled to send/receive message to/from all nodes within another partition. The number of communication steps of each partitioning step will be equal to number of nodes in the divided partition. The total number of communication steps is n-1 In this approach, the communication is schedule with computation such that the communication can take place in a step that avoids the conflict in bandwidth sharing. Thus, the communication bandwidth between nodes can be fully utilized. Hence, the faster communication can takes place and a lower total execution time is achieved To help clarify the proposed strategy, an example partitioning step is given in Fig. 3  1 st Partitioning step    Comm step 2 partitions 0 1 2 3}  {4  5  6 7 Sender 0 1 2 3  4  5  6 7 Receiver 4 5 6 7  0  1  2 3 I  5 6 7 4  1  2  3 0 II  6 7 4 5  2  3  0 1 III  7 4 5 6  3  0  1 2 IV 2 nd Partitioning step           4 partitions 0 1} {2 3}  {4  5}  {6 7  Sender 0 1 2 3  4  5  6 7 Receiver 2 3 0 1  6  7  4 5 V  3 2 1 0  7  6  5 4 VI 3 rd Partitioning step           8 partitions 0 1 2 3}  {4}  {5}  {6 7  Sender 0 1 2 3  4  5  6 7 Receiver 1 0 3 2  5  4  7 6 VII  Fig. 3. Example of results of bandwidth scheduling strategy of 8 nodes  The example in Fig. 3 is a bandwidth scheduling for 8 nodes system. In the first partitioning step, there are two partitions, {0, 1, 2, 3} and {4, 5, 6, 7}. There are 4 nodes per partition. The number of communication steps is also equal to the number of nodes per partition \(Comm. step I-IV\. Let an order pair a  b denotes the communication from node a to node b Then, the 8 scheduled communications in the first communication step \(Comm. step I\ is given by a set: {\(0,4 7,3\}. The total number of communication steps is 7 \(Comm. step I-VII V E XPERIMENTAL E VALUATION  A database of 60 million transaction database is mined using the parallel FI-growth program enerate the t est  task graph. We utilized the standard çIBM synthetic data g o synthesize a tr an s a c tion dat abase We u s e d  1000 unique items to create 60 million records, which have average transaction length of 10. The communication volume associated with each edge varies from 96 bytes to 768.8 Kbytes. After the test graph is generated, we can run a scheduling simulator on this task graph using different scheduling strategies. All the scheduling strategies in our simulator have been written in Java programming language 19 


and running under Microsoft Windows. To evaluate these scheduling strategies, we ran all of the considered strategies on variation architecture as shown in Table I  TABLE I  M ULTICORE S INGLE S YSTEM AND M ULTICORE C LUSTER S YSTEM V ARIATION A RCHITECTURE  Multicore Single system CPU # of CPUs Multicore Cluster system CPU # of machines M1 Dual-core 1  C1 Dual-core 2 M2  2  C2 4 M3  4  C3 8 M4 Quad-core 1  C4  16 M5  2  C5 32 M6  4  C6 Quad-core 2 M7 Hexa-core 1  C7  4 M8  2  C8 8 M9  4  C9 16 M10 Octo-core 1  C10  32 M11  2  C11 Hexa-core 2 M12  4  C12 4 C13  8 C14  16 C15  32 C16 Octo-core 2 C17  4 C18  8 C19  16 C20  32  Fig. 4a, 4b, 4c, and 4d illustrate the run times for UCE and EO scheduling strategies on dual-core, quad-core, hexa-core and octo-core single system, respectively. The results clearly show the UCE scheduling strategy gives a lower execution time than the EO scheduling strategy. In the best case, the runtime is reduced as much as 14.70 Fig. 5a, 5b, 5c, and 5d show the run times for UCE and EO scheduling strategies on dual-core, quad-core, hexa-core, and octo-core multicore cluster system, respectively. The results show that run-time drops rapidly as the number of core increase until 32 cores, and then run-time drops more slowly as additional cores are used. This is caused by the additional communication overhead incurred. For all variation multicore cluster systems, the largest runtime is 51.45%. We also found that the decreased run time of UCE and EO on multicore cluster system was more than multicore single system in case of equal number of cores. This is the result of a better utilization of interconnection network used to build the system   0 100 200 300 400 500 M1 M2 M3 Run Time sec Dual-core single system variation architecture Dual-core single system UCE EO  a  0 100 200 300 400 500 M4 M5 M6 Run Time sec Quad-core single system variation architecture Quad-core single system UCE EO  b 0 100 200 300 400 500 M7 M8 M9 Run Time sec Hexa-core single system variation architecture Hexa-core single system UCE EO  c 0 100 200 300 400 500 M10 M11 M12 Run Time sec Octo-core single system variation architecture Octo-core single system UCE EO  d  Fig. 4. Run time of parallel FI-growth of the UCE and EO using a\ual-core, \(b\uad-core, \(c\Hexa-core, and \(d\Octo-cores on multicore single system variation architectures  20 


 0 100 200 300 400 500 C1 C2 C3 C4 C5 Run Time sec Dual-core cluster system variation architecture Dual-core cluster system UCE EO  a  0 100 200 300 400 500 C6 C7 C8 C9 C10 Run Time sec Quad-core cluster system variation architecture Quad-core cluster system UCE EO  b   0 100 200 300 400 500 C11 C12 C13 C14 C15 Run Time sec Hexa-core cluster system variation architecture Hexa-core cluster system UCE EO  c  0 100 200 300 400 500 C16C17C18C19C20 Run Time sec Octo-core cluster system variation architecture Octo-core cluster system UCE EO  d  Fig. 5. Run time of parallel FI-growth of the UCE and EO using a\ Dual-core, \(b\ Quad-core, \(c and \(d on multicore cluster system variation architectures  VI C ONCLUSIONS  In this paper, a new strategy for parallel-FI growth data mining called UCE \(Unified Communication and Execution is proposed. By combining the efficient execution and communication scheduling together on a multicore cluster can result in a substantial performance improvement. Currently we are working on how to extract performance from the large hybrid architecture such as GPU based cluster. In addition, we are looking forward to applying this approach to other algorithm. We hope that this work can lead to the development of data mining algorithm that can handle a massive scale data faster and lead us to discover more knowledge that is useful for users  R EFERENCES  1  X. Z h ang, Y Zhu  and N. Hua, "Pr ivacy Par allel Algo r ith m for Mining Association Rules and its Application in HRM," in 2009 Second International Symposium on Computational Intelligence and Design  2009, pp. 296-299 2  S. K. Tanbeer, C F. Ah m e d, and  B.S J e ong, "Pa rallel and Distr ibu ted  Frequent Pattern Mining in Large Databases," in 11th IEEE International Conference on High Performance Computing and Communications 2009, pp. 407-414 3  M. C hen, X   Ga o a n d  H Li, "An efficient parallel FP-Growth algorithm," in International Conference on Cyb er-Enabled Distributed Computing and Knowledge Discovery 2009, pp. 283-286 4  C. Xiaoyun H. Yanshan, C Pengfei, M. Shengfa, S. Weiguo, and Y Min, "HPFP-Miner: A Novel Parallel Frequent Itemset Mining Algorithm," in Fifth International Conference on Natural Computation  2009, pp. 139-143 5  H L i  Y  Wa ng D  Zhang M  Zha ng a nd E Y   C h ang Pf p: para lle l  fp-growth for query recommendation," in Proceedings of the 2008 ACM conference on Recommender systems Lausanne, Switzerland, 2008, pp 107-114 6  J. Hu and X. Yang L i, "A Fast P a rallel Ass ociatio n Rules Mining Algorithm Based on FP-Forest," in Advances in Neural Networks - ISNN 2008 2008, pp. 40-49 7  K.-M Yu, J   Zh ou  a nd W  C  Hs iao Load B a lanc in g A pproac h Parallel Algorithm for Frequent Pattern Mining," in Parallel Computing Technologies 2007, pp. 623-631 8  A. Sava se re  E. O m i e cin s ki, a n d S Navathe, "An Effi cient Algorith m  for Mining Association Rules in Large Databases," in Proceedings of the 21th International Conference on Very Large Data Bases 1995, pp 432-444 9  O. R Z a ane M El-Hajj, and  P. Lu F a s t P a rallel  Ass o ciation Rule Mining without Candidacy Generation," in Proceedings of the 2001 IEEE International Conference on Data Mining 2001, pp. 665-668 1 S K Tanbeer C  F. Ah m e d B  S. Jeong, and Y.-K. Lee, "Efficient single-pass frequent pattern mining using a prefix-tree Information Sciences; Inf. Sci vol. 179, pp. 559-583, 2009 1 D Che n   C  La i  W   H u   W   C hen, Y Zha n g  and W Zheng Tr ee  partition based parallel frequent pattern mining on shared memory systems," in 20th International Parallel and Distributed Processing Symposium \(IPDPS 2006 Rhodes Island, Greece, 2006 1 R  A g ra wa l and J  C Shaf e r    P ara llel M i n i ng of  Ass ociation Rules IEEE Trans. on Knowl. Data Eng vol. 8, pp. 962-969, 1996 1 A. Jave d and A. Kho kha r, "Fr equent Pa tte rn Mining on Message Pa s s ing Multiprocessor Systems Distributed and Parallel Databases vol. 16 pp. 321-334, 2004 1 K. D Shah and S. Maha j a n Maximizing the Efficiency of Parallel Apriori Algorithm," in International Conference on Advances in Recent Technologies in Communication and Computing 2009, pp. 107-109 1 P W e ihao and S J i ngu ang  R esearch f o r parallel ap r i ori algorith m  based on MPI," in 2nd IEEE International Conference on Computer Science and Information Technology 2009, pp. 443-446 21 


16  K  M Yu an d J  L Zhou  A we ighted load-balancing parallel Apriori algorithm for association rule mining," in IEEE International Conference on Granular Computing 2008, pp. 756-761 17  J. S Park, M  S C h en and P. S Yu An e f fective h a sh b ased algorith m  for mining association rules SIGMOD Rec vol. 24, pp. 175-186, 1995 18  R Sr ikant S y n t h etic Data Ge neration C o d e f o r  A s s o c i a tion  and  Sequential patternsé, Available from the IBM Quest web site at http://www.almaden.ibm.com/cs/projects/iis/hdb/Projects/data_mining/d atasets/syndata.html 19  M J. Zaki, M  Ogi h ara S Parthasarathy, and W. Li, "Parallel data mining for association rules on shared-memory multi-processors," in Proceedings of the 1996 ACM/IEEE conference on Supercomputing CDROM Pittsburgh, Pennsylvania, United States, 1996 20  N. Be n j a m as and P Uthayopa s, "An Im pac t o f  S c he dulin g Stra tegy to  Parallel FI-Growth Data Mining Algorithm," in Advances in Information Technology vol. 55: Springer Berlin Heidelberg, 2009, pp. 39-47 21  R Agrawal T. Imi eli ski and A. Swam i    M ining Associatio n Rules between Sets of Items in Large Databases," in Proceedings of the ACM SIGMOD International Conference on Management of Data  Washington, DC, USA, 1993, pp. 207-216 22  R. Agrawal  an d R. Srikan t, "Fas t Algorith ms f o r Min i n g As sociation  Rules in Large Databases," in Proceedings of the 20th International Conference on Very Large Data Bases 1994, pp. 487-499 23  J. Han, J. Pei, and Y. Yin Mining Frequent Patter ns without Candidate Generation," in Proceedings of the 2000 ACM SIGMOD international conference on Management of data Dallas, Texas, United States, 2000 24  K A m phaw a n a n d A. Surare rk s A n A p proa c h  of  F r e quent I t e m  T r ee  for Association Generation," in Artificial Intelligence and Soft Computing Benidorm, Spain, 2005 25  M. J Z a ki, "P aral lel an d Dis tributed As socia tion Mining A Survey    IEEE Concurrency vol. 7, pp. 14-25, 1999 26  R   C  Agarw a l  C   C  A g g a rw al, a nd V  V V Pra sa d  A tree pro j e c tion  algorithm for generation of frequent item sets J. Parallel Distrib Comput vol. 61, pp. 350-371, 2001 27  E.-H. Han, G  Kary p is, and V. Ku mar, "Scalable paralle l data mining for association rules," in Proceedings of the 1997 ACM SIGMOD international conference on Management of data Tucson, AZ, USA 1997 28  I. Pra m udiono and M. Kitsureg a wa, "Parallel FP-Growth on PC Cluster," in Advances in Knowledge Discovery and Data Mining 2003 pp. 570-570 29  J. S  Park M.S   Chen and P. S. Yu  E ff icient par all el data m i ning f o r  association rules," in Proceedings of the four th international conference on Information and knowledge management Baltimore, MD, USA 1995 30  B Manaskasem sak, N. Benj a m as A. Rungsawang, A Surarerks, and P Uthayopas, "Parallel association rule mining based on FI-growth algorithm," in 2007 International Conference on Parallel and Distributed Systems 2007, pp. 1-8    22 


J.W. Chen, J Zhang, D.W. Wu et al, \215Development of experience data warehouse for FDHss in treatment of major infectious diseases and construction its intelligent mining platform\216 Fuzhou, China 2009, 40 \(20\.38-40 3 Y.B. Li, Y. Yang, M. Cui, et al, \215Data mining of the information of 664 cases of uterine bleeding medical cases\216 Chinese Journal of Information on Traditional Chinese Medicine Fujian Journal of Traditional Chinese Medicine        Thus, from the data mining results, the experience common understanding of FHDs in prevention and treatment of infectious diseases can be found, which can provide a reference basis for herbal prescription of clinical practice and is helpful for heritage and development of academic thinking of FHDs V experience against infectious diseases is the useful explore in this area. In addition, the results of data mining can also provide a reference for the consensus program of infectious diseases prevention and treatment expert or the development of diagnosis and treatment guideline, for example: the data mining technology was used in Traditional Chinese Medicine Prevention programs for H1N1 influenza \(2009 Edition 6  In summary, the application of data mining techniques in the study of the type of literature information of FHDs can greatly improve the utilization efficiency of medical cases, medical works, papers, expert interview contents of FDHs, thus contributing to the conciseness and inheritance of the academic experience of FHDs; also contribute to the formation of theory, treatment guidelines or consensus program for guiding clinical practice However, data mining techniques also have limitations Firstly, data mining, the use of data mining techniques in analysis of literature type data is still in its starting stage with no mature application framework formed yet Secondly, syndrome differentiation and treatment requires individual treatment programs, but treatment simply based on computer technology can not completely replace the individualized academic ideas of FHDs. Thus, although the data mining techniques \(such as text mining etc.\ is continuing to make progress, the results obtained through mining of literature type information still requires processing and supplementation of human brain ACKNOWLEDGMENT This research was supported by the 11th five years key special projects for National Science and Technology L. Ma, \215Comparative study of cluster analysis based on data mining and traditional cluster analysis\216 Beijing, China, 2008, 21\(5\, pp.530-531 6 Data Mining: Concepts and Techniques toxic heat. If accompanied by stagnation of QI due to depression of the liver, then radix bupleuri, Rhizoma Cyperi, Magnolia officinalis and Fructus gardeniae are added appropriately. Rhizoma Cyperi and Radix bupleuri can be used for hypochondriac pain; Radix codonopsis Poria cocos, Pericarpium Citri Reticulatae, Semen Coicis and hawthorn are mostly used for spleen deficiency. Radix codonopsis and radix astragali are usually used for Qi deficiency; Fructus aurantii, Magnolia officinalis and Pericarpium Citri Reticulatae can be used for abdominal distention; pachyma cocos, hawthorn, Herba Eupatorii and Radix codonopsis can be used for appetite; pachyma cocos and Semen Coicis are usually used for edema China Machine Press, Beijing. China, 2006 5 Beijing, China, 2011 20 \(4\ pp.42-47 2  Journal of Mathematical Medicine J.W. Han Chinese Journal of Medical Library and Information Science Project No.:2008ZX10005-013 and 2009ZX10001-018 REFERENCES 1 X.J. Li, T.S. Zhang, S. Zhang, et al, \215Data mining in exploring the medication laws of Dong Jianhua for treatment of senile diseases\216 Beijing, China, 2009 16 \(11\, pp.95-96 4 DISCUSSION With the explosive growth of information, data mining technology has been widely used in finance telecommunications, insurance, retails, medical science and other fields. In recent years, the combination of the technology with the research of traditional Chinese medicine has become more closely and has penetrated deeply into the analysis of medical literature, which is of great significance for learning of Chinese medical cases Medical monographs and the excavation and succession of the academic ideas of FDHs. Application of the data mining technology in exploring the FDHs State Administration of Traditional Chinese Medicine of People\220s Republic of China, Traditional Chinese Medicine Prevention programs for H1N1 influenza \(2009 Edition http://www.satcm.gov.cn/web2010/zhengwugongkai/yizhengguanli gongzuodongtai/2010-10-11/9915.html 790 


Target values have different meanings for different data mining problems Now, let us consider the application of strati?ed sampling and Neyman allocation to differential rule mining and association mining. For differential rule mining in the form of A = a D1\(t t D1\(t t denote the number of data records sampled across the strata, and ni denote the number of data records drawn from the ith stratum The size of population containing tuples under the space of A = a is denoted by Ni for the ith stratum. Now, let the expression y denote D1\(t t average of the ith stratum. Then, using strati?ed sampling, the mean value of y is estimated as ys H i=1 Ni N yi \(1 It turns out that ys is an unbiased estimation , which implies that E\(ys y variance for ys is 2s H i=1 N2i N2  ni 2 i \(2 In the stratum whose variance is unknown, ?2i is estimated by 326 sample variance, i.e S2i  y2ij ni   yij ni  


Here, yij is a sampled record from the ith stratum These expressions are also applied to association rule mining in the form of A = a ? Y , with the goal being to estimate the con?dence supports\(A=a?Y A=a the expression y to denote whether Y is contained in the data record under the space of A = a: if the data record under space of A = a contains Y , let y = 1; otherwise, y = 0. The con?dence for the association rule is calculated by estimating the mean value of y based on Expression 1, and the variance of estimation for the con?dence over the entire population is still calculated by Expression 2 Formally, with Neyman allocation, the sample allocation can be stated as ni n j=1Nj?j Ni?i \(3 where ?i are calculated as described above While Neyman Allocation ef?ciently reduces the variance of estimation, it does not consider the sampling cost in the deep web. Speci?cally, sampling cost is de?ned as the number of queries submitted to deep web data sources in order to obtain a ?xed number of data records under the particular space. We assume that for each query submitted to the deep web, a single data record is obtained. In the process of sampling data records corresponding to A = a over the deep web, the probability of A = a in each stratum is not considered in Neyman allocation \(or other existing methods that a large sample might have to be drawn from a stratum where the probability of a data element satisfying A = a is small. This will result in a high sampling cost The following example shows the inef?ciency of Neyman Allocation in the case of differential rule mining on the deep web. For a rule A = a ? D1\(t t three strata. The columns one through four of the Table I show the stratum ID, population size \(Ni of D1\(t t being true in any given record in this stratum TABLE I EXAMPLE OF APPLICATION OF NEYMAN ALLOCATION FOR DEEP WEB MINING ID Size Variance Probability of A = a Sample size 


1 800 10000 0.9 100 2 800 40000 0.8 200 3 800 90000 0.1 300 By assuming that the total sample size is 600, the sample size ni according to Neyman Allocation for each stratum is shown in the ?fth column of Table I. According to the Expression 2, the resulting estimation is 66.67, and the estimated sampling cost is 3361.1. In this example, the sampling cost is very high, because a large sample is allocated to the stratum 3, which happens to have a low probability of A = a. If we could allocate a smaller sample to the stratum 3 and a larger sample to the strata 1 and 2, the sampling costs would be decreased, though possibly with some increase in the sampling variance C. Our Approach As we stated in the previous section, randomly constructing and submitting queries can not ensure that the output results contain data records within the space of A = a. Thus, ef?ciently acquiring samples under the space of output attribute A = a and accurately estimating the parameters \(con?dence or mean value challenging problems Since sampling on the deep web can only be performed by constructing and submitting queries, strati?cation needs to be performed on the query space, i.e., involving input attributes only. In order to ef?ciently acquire samples under the space of a speci?c value of an output attribute \(i.e. A = a between input attributes and output attributes is important. In other words, we need to know which input queries are more likely to yield output web pages containing tuples under the space of A = a. Furthermore, in order to accurately estimate the parameters \(con?dence or mean value under the space of A = a, similar values of target attributes under A = a are required to be in the same group. For this purpose we are proposing a greedy strati?cation method. Our method considers both ef?ciency and accuracy, so that input queries that are more likely to yield output web pages containing tuples under the space of A = a are identi?ed, and similar values of target attributes under A = a are grouped in the same sub-population In our approach, the relation of input attribute and output attribute as well as the data distribution of target attributes are learnt from the pilot sample The second component of our approach is a novel sample 


allocation method. To ef?ciently acquire samples under the space of output attribute A = a, large sample should be drawn from the query subspace that is more likely to yield output web pages containing tuples under the space of A = a. On the other side, to accurately estimate the parameters \(con?dence or mean value target attributes under the space of A = a, a large sample should be drawn from the query subspace that has a larger variance of the target value under the space of A = a. We have developed an optimized sample allocation algorithm which integrates the two aspects \(ef?ciency and accuracy IV. MAIN TECHNICAL APPROACH In this section, we introduce our strati?ed sampling approach in the context of association rule mining and differential rule mining on deep web data sources. As we stated earlier, our approach comprises of a strati?cation method and a sample allocation method. These are described in the next two subsections. Towards the end of this section, we describe how differential rule mining can be performed using these two methods A. A Greedy Strati?cation Method Strati?cation refers to partitioning the population being sampled. Traditional strati?cation methods such as Recursive strati?ed sampling [22], [8] aim at grouping similar values into the 327 Algorithm 1 Strati?cation\(N,Q, PS, s, Lf 1: Cost\(N N N 2: if Cost\(N 3:  integrated cost is smaller than 4: Lf = Lf ?N 5: else 6:  partition the query space of N 7: splitCost ? maxV alue 8: M ? null 9: for all IA ? PS do 10: CH ?strata generated by IA 11: Cost\(CH CH CH 12: if Cost\(CH 13: splitCost=Cost\(CH 14: M=IA 15: end if 16: end for 17: DM ? domain of M 18: ASM ? sample allocation 


19: for all mi ? DM do 20: Strati?cation\(CHi, Q ? {M = mi}, PS ?M,asi, Lf 21: end for 22: end if same subpopulation, in order to maximally reduce variance of estimation. As we have stated previously, two related issues make the sampling for a deep web data source different. The ?rst is because of the limited access to the database, i.e., samples can only be obtained by querying through a simple interface Thus, obtaining additional samples meeting A = a is non-trivial Second, ef?ciency of sampling is an important consideration since each deep web query is slowed down due to the data delivery delay between the server and the client We now describe our greedy strati?cation method which partitions the population of the deep web recursively. Strati?cation is performed on the query space composed of input attributes in the query interface. In other words, the population of a deep web source can be considered to correspond to the entire query space, and the subpopulations correspond to query subspaces More precisely, a subpopulation comprises of the data records that can be obtained by submitting queries from the corresponding subspace To establish this correspondence, the relationship between input attributes and output attributes is learnt from the pilot sample. In this fashion, the subpopulations or strata that are more likely to contain tuples where A = a is true are identi?ed. We de?ne sampling cost to be the estimated number of data records that will need to be drawn from the deep web, in order to obtain a speci?ed number of records where A = a is true. Let ?i represents the probability of ?nding a data record with A = a in the stratum i. This value can be estimated based on the pilot sample. Thus, to obtain ni data records where A = a is true in ith stratum, the estimated number of data records drawn from ith stratum is ni?i . Overall, in order to obtain our desired n data records where A = a is true from the entire population, the estimated sampling cost will be SampCost  i ni i 4 


While focusing on the sample cost and using the strata that are more likely to contain tuples where A = a is clearly important, variance of estimation is another important issue, since it corresponds to the error of estimation. Thus, we de?ne the integrated cost by taking into account the variance of estimation 2s and the sampling cost SampCost Cost = ?s  SampCost + ?v  ?2s \(5 where ?s and ?v are weights for sampling cost and estimate variance, respectively, such that their sum is 1. Users could set the two weights based on the importance they attach to these two factors. For example, if response to a query is needed quickly sampling cost should have a higher weightage The relation between input attributes and output attributes, as well as data distribution, are modeled by a tree built on the query space recursively. Each node in the tree represents a query subspace, and is associated with a query that comprises speci?c values for a subset of input attributes, the sample size, as well as the potential splitting attributes. The sample size shows how many data records would be drawn from the subpopulation of the node Algorithm 1 shows the process of splitting a node N , which as we stated above, is associated with the query Q, the sample size s, and a list of potential splitting attributes, PS. The inputs of the algorithm also include set Lf , which represents leaf nodes of the tree. At the beginning, the entire query space is represented by the root node. The corresponding query of the root node is null the sample size n is the size of the sample that is to be drawn from the entire population, and the potential splitting attributes list is the complete set of input attributes of this data source. The initial set of leaf nodes is empty The main goal of the algorithm is to split the query tree in a greedy way. For each potential splitting attribute, the integrated cost after splitting is computed according to the Expression 5. The input attribute which brings the most reduction in the integrated cost is selected. More speci?cally, ?rst, the integrated cost for the subpopulation of the node N is calculated. If this value is smaller than a prede?ned threshold ?, node N is set to be a leaf node \(Lines 2-5 considering the set of potential splitting attributes PS \(Lines 621 would be |DIA| strata under the space of node N . The integrated cost on |DIA| strata is computed. Let M ? PS with domain DM 


denote the input attribute with the minimum integrated cost after the split, and let the set of allocated sample sizes, computed by the sample allocation method explained later, be ASM . Then DM | children are generated for the node M . For each child CHi, i = 1, . . . , |DM |, the associated query is QN ?{M = mi sample size is asi ? ASM , and potential splitting attributes is PSN ? M . The process of splitting is then recursively applied to children of node N In the process of calculating costs during the strati?cation process, we need to perform sample allocation, i.e., divide the parent nodes sample size among the potential children nodes This is required for calculating the integrated cost for the potential split. This is based on our sample allocation method, which we 328 describe next in Section IV-B. Furthermore, for calculating the integrated cost, the variance of target value ?2i and probability of output attribute A = a, ?i, for each stratum is computed based on the pilot sample Initially, the strati?cation process on the query space begins by calling Stratification\(R, null, F IA, n, null root node. The process of strati?cation would stop if there is no leaf node with integrated cost larger than a prede?ned threshold Each leaf node in the tree is a ?nal stratum for sampling, and the associated sample size denotes the number of data records drawn from the subpopulation of the stratum B. An Optimized Sample Allocation Method Now, we introduce our optimized algorithm for sample allocation which integrates variance reduction and sampling cost As introduced in section IV-A, integrated cost is de?ned by taking into account of variance of estimation and sampling cost The goal of our sample allocation algorithm is to minimize the integrated cost by choosing the sample size, ni, for each stratum In our algorithm, we adjust the value of SampCost and ?2s so that their in?uences on the integrated cost are in the same unit SampCost SampCost SmpCost\(r where SmpCost\(r entire population, and ?r denotes the probability of A = a being true for the entire population 2 2s 


2r where ?2r 2 n denotes the variance of estimation of the target value on the entire population The key constraint on the values of the sample sizes for each strata is that their sum should be equal to the total sample size A vector n = {n1, n2, . . . , nH} is used to represent the sample sizes, where the ith element, ni, is the sample size for the ith stratum By including sampling cost and variance of estimation into the integrated cost, our sample size determination task leads to the following optimization problem Minimize Cost\(n  i\(?s ni i v N2i niN2 2 i subject to  i ni = n 6 where Ni denotes the population size of data records under the space of A = a in ith stratum. Note that this value may not be known if A is an output attribute. However, the total number of records in the ith stratum is typically known, and can be denoted as DNi. Then Ni can be estimated by ?iDNi, and the population size of A = a on the entire population is estimated by N  i ?i DNi For ?nding the minimum of integrated cost, we utilize Lagrange multipliers, a well know optimization method. Lagrange multipliers aims at ?nding the extrema of a function object to constraints. Using this approach, a new variable ? called a Lagrange multiplier is introduced and de?ned by n n  


 i ni ? n If n is a minimum solution for the original constrained problem then there exists a ? such that \(n Lagrange function. Stationary points are those points where the partial derivatives of ?\(n n,??\(n 7 In our problem, by conducting partial derivatives on Formula 7 a group of equations are yielded as follows s i v N 2 i n2iN2 2 i + ? = 0 i = 1, ..H i ni = n 8 where the solution n leads to the minimum value of integrated cost in Formula 5 However, it is dif?cult to solve the group of equations directly Thus, we use numerical analysis to approximate the real solution. Newtons method is utilized for ?nding successively better approximations to the zeroes \(or roots Given an equation f\(x x tive of function f\(x method iteratively provides, xt+1, a better approximation of the root, based on xt, the previous approximation of root according to the following formula xt+1 = xt ? f\(xt f ?\(xt The iteration is repeated until a suf?ciently accurate value is reached, i.g. |f\(xt In our problem of Formula 8, there are H + 1 equations F \(xt xt xt n1, .., nH , ?}, where the equations in F \(xt fi\(xt s i v N 


2 i n2iN2 2 i + ? i = 1, .., H fH+1\(xt  i ni ? n The Newtons method is also applied iteratively via the system of linear equations JF \(xt xt+1 ? xt xt 9 where JF \(xt H + 1 H + 1 equation system F \(xt vector xt. The entry JF \(i, j d\(fi\(x dxj where fi\(x xt x From the Expression 9, a better approximation xt+1 is obtained based on previous approximation xt. The iterative procedure would be stopped if ?i|fi\(xt threshold, and then the sample size ni is allocated for each stratum so that the integrated cost is minimized. In reality, we are required to pick an integral number of records from each stratum during the sampling step. Thus, we round down each ni to its nearest integer, ni + 0.5 In the example shown in Table I, suppose we set both weights v and ?s to be 0.5. Further, assume the variance of the entire population, ?2r , to be 80000. The probability of A = a over the entire population ?r is 0.242. By using the proposed optimized sample allocation method, the sample sizes for the three strata 329 are 162, 299, and 139, respectively. In this case, the variance of estimation according to the Expression 2 is 93.66, and the estimated cost is 1943.7. We can see that, compared with Neyman allocation, the sampling cost is decreased by 42.1%, but results some increase in variance. Overall, this example shows that we can achieve lower sampling cost by trading off some accuracy C. Overall Sampling Process Algorithm 2 DiffRuleSampling\(DW1, DW2, F IA, t, St 1: PS ? a pilot sample from DW1, DW2 


2: DR ? identi?ed rules from PS 3: OA ? output attributes of DW1, DW2 4: for all R : X ? DW1\(t t 5: if X ?OA = null then 6: Acquire St data records from the space of X 7: else 8: R ? root node 9: Lf ? null 10: Strati?cation\(R,null, F IA, St, Lf 11: for all N ? Lf do 12: s ? sample size of N 13: Draw s data records from the subpopulation of N 14: end for 15: end if 16: Update the mean value of DW1\(t t 17: end for Algorithm 2 shows the overall sampling process for differential rule mining on two deep web data sources, DW1 and DW2 and with differential attribute t. The inputs of the algorithm also contain the full set of input attributes FIA as well as the sample size St. The algorithm starts with a pilot sample PS, from which the differential rules are identi?ed. For the rule R : X ? DW1\(t t St data records are directly drawn from the space of X \(Lines 5-6 t t containing output attributes, query spaces of DW1 and DW2 are strati?ed and sample is recursively allocated to each stratum with corresponding query subspace \(Line 10 of the tree built by strati?cation, a sample is drawn according to its sample size \(Line 13 t t is updated by the further sample \(Line 16 for association rule mining is very similar and not shown here V. EVALUATION STUDY We evaluate our sampling methods for association mining and differential rule mining on the deep web using two datasets described below US Census data set: This is a 9-attribute real-life data set obtained from the 2008 US Census on the income of US households. This data set contains 40,000 data records with 7 categorical attributes about the race, age, and education level of the husband and wife of each household and 2 numerical attributes about the incomes of husband and wife 


Yahoo! data set: The Yahoo! data set, which consists of the data crawled from a subset of a real-world hidden database at http://autos.yahoo.com/. Particularly, we download the data on used cars located within 50 miles of a zipcode address. This yields 30,000 data records. The data consists of 7-attribute with 6 categorical attributes about the age, mileage, brand, etc, of the cars and one numerical attribute, which is the price of the car Variance of Estimation is estimated for the target value \(i.e mean value in differential rule mining, and con?dence in association rule mining is calculated according to the Expression 2. Since variance of estimation reveals the variation of the estimated value from the true value, smaller variance suggests better estimation Sampling Cost is estimated by the number of queries submitted to data sources in order to acquire a certain number of data records containing target output attributes, i.e. A = a. Larger sample size implies higher sampling costs in a deep web setting where the queries are executed over the internet Estimate accuracy is estimated by Absolute Error Rate \(AER Small AER value indicates higher accuracy. For an estimator on variable Y with true value y and estimated value y, the AER of the estimator is calculated by AER\(y A. Association Rule Mining In this section, we present the results of our method for association rule mining. Using our overall approach, we have created four different versions, which correspond to four different sets of weights assigned to variance of estimation and sampling costs. 1 the weight ?v = 1.0 and ?s = 0.0, 2 the weight ?v = 0.7 and ?s = 0.3, 3 v = 0.5 and ?s = 0.5, and 4 weights ?v = 0.3 and ?s = 0.7. In addition, we also compare these approaches with a simple random sampling method, which is denoted by Random We focus on the queries in the form of A = a ? B = b where A and B are output categorical attributes. Other categorical attributes in the data set are considered as input attributes Our goal is to estimate Supp\(A=a,B=b A=a association rules are randomly selected from the datasets. Each of the 50 association rules are re-processed 100 times using 100 different \(pilot sample, sample iterations result is the average result for 5000 executions 


In all charts reported in this section, the X-axis is k, which denotes the size of sample under the space of a target rule drawn from deep web. The sample size for each point on X-axis is k x, where x is a ?xed value for our experiment, and depends upon the dataset. At each time, queries are issued to obtain kx data records under the space of a target rule. Overall, all our experiments show the variance of estimation, sampling costs and sampling accuracy with varying sample size Figure 1 shows the result from our strati?ed sample methods on the US census data set. The size of pilot sample is 2000, from which all of the 50 initial rules are derived. In this experiment the ?xed value x is set to be 300, which means the smallest sample size at k = 1 is 300, and the largest sample size at k 10 is 3000. Figure 1 a the ?ve sampling procedures. Figure 1 b cost for the sampling procedures. In order to better illustrate the experiment result, in each execution of sampling, the variance of 330 6DPSOLQJ9DULDQFH            9D UL DQ FH R I V WL PD WL RQ  


9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW           6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF            5 


 9DU 9DU 9DU 5DQG c Fig. 1. Evaluation of Sampling Methods for Association Rule Mining on US Census Dataset estimation and sampling cost for the sampling procedures var7 var5, var3, and rand are normalized by the corresponding values of Full Var. Thus, in our experiment, the values of sampling cost and variance of estimation for sampling procedure Full Var are all 1. Furthermore, Figure 1 c sampling procedures From Figure 1 a pared with sampling procedures Var7, Var5 and Var3, Full Var has the lowest estimation variance and the highest sampling cost. From sampling procedures Var7, Var5, and Var3, we can see a pattern that the variance of estimation increases, and the sampling cost decreases consistently with the decrease of the weight for variance of estimation. At the largest sample size of k = 10, the estimation variance of sampling procedure Var3 is increased by 27% and the sampling cost is decreased by 40 compared with sampling procedure Full Var. The experiment shows that our method decreases the sampling cost ef?ciently by trading off a percent of variance of estimation. Similar to variance of estimation, the sampling accuracy of these procedures also decreases with the decrease of the weight on variance of estimation. For the largest sample size at k = 10, we can see that the AER of sampling procedure Var3 is increased by 20 compared with sampling procedure Full Var. However, for many users, increase of the AER will be acceptable, since the sampling cost is decreased by 40%. By setting the weights for sampling variance and sampling cost, users would be able to control the trade-off between the variance of estimation, sampling cost, and estimation accuracy In addition, compared with sampling procedure of Full Var Var7, Var5, and Var3, sampling procedure Random, has higher estimation of variance, sampling cost and lower estimation accuracy. Thus, our approach clearly results in more effective methods than using simple random sampling for data mining on the deep web Figure 2 shows the experiment result of our proposed strati?ed 


sampling methods on the Yahoo! data set. The size of pilot sample on this data set is 2,000, and the ?xed value x for sample size is 200. The results are similar to those from the US census dataset. We can still see the pattern of the variance of estimation increasing with the decrease of its weight. Besides, the sampling accuracy is also similar to the variance of estimation. However although the variance estimation of sampling procedure Random is 60% larger than sampling procedure Full Var, the sampling cost of Random is 2% smaller than Full Var. This is because Full Var does not consider sampling cost. It is possible that Full Var assigns a large sample to a stratum with low ?, which denotes the probability of containing data records under the space of A = a, resulting the larger sampling cost than that of simple random sampling. Sampling procedures Var7, Var5, Var3 consider sampling cost as well, and have smaller variance estimation and sampling cost, compared with Random. Furthermore, Random has smaller sampling accuracy than Full Var, Var7 and Var5, but has larger sampling accuracy than Var3. This is because Var3 assigns much more importance to the sampling cost, and loses accuracy to a large extent To summarize, our results shows that our proposed strati?ed sampling are clearly more effective than simple random sampling on the deep web. Moreover, our approach allows users to tradeoff variance of estimation and sampling accuracy to some extent while achieving a large reduction in sampling costs B. Differential Rule Mining In this section, we present results from experiments based on differential rule mining. Particularly, we look at the rules of the form A = a ? D1\(t t categorical attribute and t is an output numerical attribute, while other categorical attributes in the data set are considered as input attributes In this experiment, we also evaluate our proposed method with different weights assigned to variance of estimation and sampling cost. Five sampling procedures, Full Var, Var7, Var5,Var3 and Random, have same meanings with those in the experiments of association rule mining. Similarly, 50 rules are randomly selected from the datasets, and each of the 50 differential rules are reprocessed 100 times using 100 different \(pilot sample, sample iterations 5000 runs First, we evaluated the performance of these procedures on 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


