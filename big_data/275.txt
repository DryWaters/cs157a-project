Discovery of Association Rules in Tabular Data G Richards and V J Rayward-Smith School of Information Systems, University of East Anglia, Norwich NR4 7TJ UK Graeme.Richards @uea.ac.uk, vjrs @sys.uea.ac.uk Abstract In this paper we address the problem of finding all association rules in tabular data An Algorithm, ARA for finding rules that satisfy clearly specified constraints in tabular data is presented ARA is based on the Dense Miner algorithm but includes an additional constraint and an improved method of 
calculating support ARA is tested and compared with our implementation of Dense Miner it is concluded that ARA is usually more eflcient than Dense Miner and is often considerably more so We also consider the potential for modifying the constraints used in ARA in order to find more general rules weight c height2 A colour  green v colour  red a wheels  6 v Engine  diesel We refer to rules of this type as 223general association 
rules\224 or simply as 223rules\224 The problem with generalisation is that the search space is often very large In order to limit the search space constraints may be imposed on rules mb-rules in tabular data may be defined by constraining general association rules so that the antecedent and consequent are conjunctions of attribute tests ATs where each AT is an equality test of the form 223attribute  value\224 and ATs can be formed for each attribute  value pair ATs are then synonymous with items in mb-rules An 
example of this type of rule is 1 Introduction model  sports A colour  red risk  high The problem of mining association rules was introduced by Agrawal et al in 1993 l in the context of the analysis of transaction data often referred to as market basket data A transaction database comprises a set of transactions where each transaction comprises a set of items. The set ofall items in the database is U Association rules were presented as expressions of 
the form antecedent a consequent where antecedent c U  consequent c U and antecedent n consequent    they express an association between the antecedent and consequent We refer to rules of this type as 223market basket association rules\224 or mb-rules Several algorithms have been developed to find mb rules in transaction data The most well known of these is Apriori others include AIS and SETM 4 However, association rules can be considered to be 
much more general, particularly when applied to tabular data A tabular database comprises a set of records A record specifies values for each attribute in a set of attributes The antecedent and consequent may each then describe any subset of records in a database They could for example, include feature constructions, conjunctions disjunctions, negations, equality and inequality tests etc An example of a general association rule is When mb-rules are expressed in this format the constraints are clearly defined and it is also clear how they can be modified The consequences 
of this are discussed further in section 3 Algorithm ARA All Rules Algorithm for finding mb-rules in tabular data is presented in section 2 Section 2.9 describes the evaluation of ARA The final section describes ways in which ARA may be developed 2 All Rules Algorithm ARA is based on Dense Miner 5 and 61 but applies directly to tabular data includes additional pruning functions and collects more support information to enable the calculation of tighter bounds for pruning Dense Miner is 
described in greater detail in 5 and ARA in 7 2.1 Notation The antecedent of a rule r is designated r  c where c is the consequent of the rule Operations on conjunctions of ATs AM and A P are defined in terms of the equivalent operations on the associated sets of ATs M and P as follows 0-7695-1119-8/01 $17.00 0 2001 IEEE 465 


A rule r is a subrule of a rule r if the consequents of the two rules are identical c and r c c r  c  The support of a conjunction of ATs or a single AT M sup\(M   is the number of records in the database for which M holds 2.2 Constraints The constraints imposed by ARA are described below 2.2.1 Rule Structure Constraints U is the set of all ATs that can be formed from attribute  value pairs in the database Consequent constraint A single AT c from U is designated as the consequent for all rules in a single search This reduces considerably the number of valid rules The justification for incorporating the consequent constraint is that often there is a single target class of interest Antecedent Constraint I is the subset of ATs in U with the same attribute as the consequent I  U  I The antecedent of a rule is a conjunction of ATs drawn from I MaxATs Constraint A user-defined value maxATs sets the maximum number of ATs that can be used in the antecedent of a rule Introducing this constraint can considerably reduce the search space and also improve the bounds used for pruning The justification for using it is that when the rules are required for gaining insight into data shorter rules are generally preferable 2.2.2 Rule Value Constraints Three parameters minSup minConf and minlmp are specified by the user Support constraint The support for a rule r sup\(r is defined as sup\(r-c A c Rule r satisfies the support constraint iff sup\(r 2 min Sup  This constraint ensures that rules have generality Confidence constraint The confidence of a rule r conf\(r is defined as sup\(r  Rule r satisfies the confidence constraint iff conf\(r 2 min Conf  This constraint ensures the predictive value of rules Improvement constraint The improvement of a rule r imp\(r was introduced in 5 and is defined as conf\(r r where r is the subrule of r with the greatest confidence sup  c Rule r satisfies the improvement constraint iff imp\(r h minlmp  This constraint ensures that all ATs in a rule add significantly to its value 2.3 Search Strategy Dense Miner and ARA are constraint based searches The search space of rules is explored by use of a set enumeration tree \(SE-Tree as described in SI Each node in the SE-Tree is represented by a structure called a group A group g has a headH\(g a tail The head is a conjunction of ATs that represents the antecedent of a rule The tail is a set of ATs that can be added to the head to form a new antecedent A total order 2 is defined on the tail ATs of a group g In practice the tail ATs are stored in a vector that is sorted according to the ordering The head and tail of the group at the root of the SE Tree are H\(roor   and T\(root  I where I is defined in section 2.2.1 A group g is expanded to create a set of groups in the next level of the SE-Tree by producing one child for each of the ATs in T\(g  For each child h of group g H\(h is a conjunction of the head of the parent and a single AT v from the tail of the parent T\(h consists of each of the tail ATs in g that follow v in the ordering T\(g g  H\(g g h H v for some v T\(g and then T\(h wl WE T\(g and w 8 v  Tail ordering determines the order of expansion of groups It thus leads to more effective pruning by concentrating unpromising groups in one part of the tree Tail ordering is carried out within each group by sorting the tail vector of ATs according to some heuristic it is discussed in more detail in section 2.8 Figure 2 1 shows an SE-Tree for the set of ATs A B C the ordering of tail ATs is indicated by their positions in the tails An arbitrary reordering of the tail ATs has been carried out at each level to demonstrate that the completeness of the tree is not compromised but no pruning is carried out, i.e the tree is complete It can be seen that the antecedent of every possible rule that can be constructed from the ATs is enumerated in the heads of the groups A group h is derivable from a group g iff i.e. a group is derivable from itself g is an ancestor of h iff h is derivable from g fI\(h AT\(h H\(g Am and H\(dc H\(h 466 


A rule r is derivable from a group g iff r-cE H\(g AT\(g and H\(g C I 2003 I A  BAAI~J n CAA I\(B CAB I I I CAAABI~J Figure 2-1 SE-Tree for the set A B C 2.4 Top level of algorithm Database D group root c create-rootsroup\(D set of groups G t root set of rules R t I while G f Cp and headkngth  maxATs  Gctail-sort G G evaluate\(G D Rcextract-rules\(G R G prune G split G G prune\(C R tpostjrocess R 0 Figure 2-2 Top Level of Algorithm The algorithm enumerates the antecedents of all rules that potentially satisfy the constraints by expanding an SE-Tree The create-root_group function creates the group at the root of the SE-Tree as described in section 2.3 A set of groups G is defined initially this comprises the single group root The set of rules R is used to return the set of all rules that satisfy the constraints The main loop of the algorithm executes until either all groups have been pruned from G or the maximum length of antecedent has been reached The order of the ATs in the tails of each group in G are redefined using the tail-sort function before G is evaluated. The necessity for ordering before evaluation is described in section 2.5 The evaluate function collects support information for each group in G for the purpose of calculating rule support and confidence and for use in the pruning function The function extract-rules adds rules that potentially satisfy the constraints to R The split function takes the set of groups G and expands each g E G as described in section 2.3 the children of g replace g in G forming the next level of the SE-Tree The prune function is applied both before and after the split function It removes some groups from G and is described in section 2.7 The posrgrocess function removes any rules that do not satisfy the constraints from R It is the same as described in 5 and is not repeated here 2.5 Evaluate Function A candidate set of conjunctions is defined for each group. The evaluation function computes the support for each of the conjunctions in the candidate set of each group in G A group is described as an evaluated group when the supports for all the conjunctions in its candidate set have been computed The candidate set used in Dense Miner for each group g is H\(g H\(g and,foreach t~T\(g H\(g and H\(g Hk Am and H\(g A T\(d c 9 The candidate set used in ARA is H\(g H\(g  andforeach t T\(g H\(g H\(g and H\(g A{wlweT\(g t  Thus ARA collects more support information for each group. This requires additional time to collect and more storage but means that tighter bounds can be calculated for use in the pruning functions as described in section 2.7 Also note that 2 must be defined before g is evaluated Each group has a counter associated with each conjunction in its candidate set As in Dense Miner, each record is considered in turn A trie is used to find all groups in C for which the antecedent of the rule represented by the head of a group holds for the record For each group g that is found the counter for H\(g incremented and if the conclusion holds for the record the counter for H\(g is incremented. Then, for each group that is found the tail ATs are considered in reverse order If a tail AT I holds for the record then the counter for H\(g t is incremented if the conclusion also holds for the record then the counter forH\(g is incremented if the conclusion does not hold for the record and every previously considered AT in T\(g held for the record then the counter associated with 8 467 


H\(g A wIW E T\(g and w 2 f lc is incremented Thus after considering all of the records in the database the supports for all of the conjunctions in the candidate sets of each of the groups in G have been computed The purpose of collecting this support information is to maximise the pruning opportunities as described in section 2.7 2.6 Extract Function Extract-rules extracts from the set of evaluated groups rules that can potentially satisfy the constraints these rules are added to the set of rules R For each g E G the rules that are considered are A rule can potentially satisfy the constraints and is H\(g C v t\200 T\(g added to R iff AND sup\(H\(g t c sup\(H\(g t A c 2 min sup conf\(H\(g c suP\(H\(g t A 4 2 Conf sup g  t  AND conf\(H\(g 3 c g 2 minlmp  where Cmax\(g is the maximum confidence of any rule represented by the head of an ancestor of g or g itself The minlmp constraint is fully enforced in the post processing stage as described in SI 2.7 Prune Function The pruning function is applied to a set of groups after evaluation and after a new level of the SE-Tree has been created A group g is said to be pruneable if it can be determined that no rule derivable from g can satisfy the constraints If g is pruneable and is a member of G then g can be removed from G If a group g is not pruneable it may be possible to remove some ATs from its tail A new group g can be formed from g by moving a single AT v from the tail into the head If g is pruneable then no rule derivable from g that includes v can satisfy the constraints and v can be removed from the tail of g If some ATs are removed from the tail of g then tighter bounds for pruning can be achieved Therefore the pruning function is re applied until either the whole group is pruned or no tail ATs are removed 2.7.1 Pruning based on support If sup\(H\(g  A c  min Sup then no rule derivable from g can meet the minSup constraint and g is pruneable The value sup\(H\(g c is available from the candidate set of g where g is the last evaluated ancestor of g 2.7.2 Pruning based on confidence Let r be the rule with maximum confidence that is derivable from a group g The maximum confidence of any rule derivable from g is max Conf\(g  conf\(r If max Conf\(g  min Conf then g is pruneable maxConf\(g can be expressed as maxConf\(g X Y where x  sup\(r and y  sup  c sup\(r  maxConf\(g is monotonically increasing in x and monotonically decreasing in y Therefore x may be replaced with an upper bound x and y with a lower bound X X y then maxConf\(g  x'+y When attempting to prune an evaluated group Dense Miner uses the bounds x sup\(H\(g c and y y where y  sup\(H\(g AT\(g ARA however uses the maxATs constraint It is then possible to calculate another lower bound on y y2 Lemma I y2 is a lower bound on y  sup  c sup\(r where y2  sup\(H\(g A 1c sup\(H\(g A d A 7c where S comprises the n tail ATs t of T\(g with the highest values of sup\(H\(g A TC and n  min\(rnaxATs IH\(gl,lT\(g The proof is initially given for the general case y I sup\(u  c sup\(u where U is any rule derivable from a group g as this is required for improvement pruning and then for the particular case y2 I sup  c sup\(r I\200 s Proof Rule U may be written as H\(g M a conjunction of ATs from T\(g Then c  where M is sup\(u  c sup\(u  SUp\(H\(g M SUp\(H\(g M A C  SUp\(ff\(g M A iC  sUp\(H\(g 1C sUp\(H\(g iM A IC NOW M m Am A...Am and 44 rn Am2 A...A lm vlm2 v...vlm 468 


Since sup\(u v b sup\(u sup sup\(a b it follows that I sup\(a sup\(b xSUp\(H\(g SUp\(H\(g A-IC l\200M In the presence of maxATs lMll min\(m,ATs-\(H\(gl,lT\(gI IS1 Given that S comprises the n tail ATs with the highest values of sup\(H\(g and ISI2IMI it follows that xsuP\(H\(g r A1 c 2 xsup  Ay c 1E s l\200M Therefore SUp\(ff\(g C H\(g I I A I C I\200 s I SUp\(H\(g IC SUp\(H\(g 4 A IC Hence SUp\(H\(g IC x SUp\(H\(g i t A I C l\200S I sup\(u  c sup\(u and y I sup\(u  c sup\(u Therefore since r is a rule derivable from g y2 2 sup\(r  c sup  y  When attempting to prune an evaluated group the supports are available to calculate x and both y1 and y2 y may then be set to max{y y2 for use in the pruning function. However when considering a group g that is not evaluated it may not be possible to calculate x'or y1 or y2 Then, since x'is an upper bound it may be set to sup\(H\(h c where h is the last ancestor of g to have this value available in its candidate set since sup\(H h A C 5 sup\(H\(g  A c It is also possible to calculate values y and y such that y I y,and y I y The values yl and y2 are lower bounds on y therefore so are y and y and so y may be set to When y1 cannot be calculated, Dense Miner calculates m~b:lY;l SUPb g  A T\(g A 4 y  max sUp\(H\(g tsTf8'0 xSUp\(ff\(g where g is the. parent of g in the SE-Tree Both of these values are less than or equal to yl and can be calculated from the supports of the conjunctions in the candidate set of g  ARA with its larger candidate set can often find a higher value for yi and also a value for y  Let g be a group that is derived from an evaluated group g Two cases are considered; case 1 when H g  H\(g i.e. some ATs have been removed from the tail of g to create g and case 2 when H\(g  H v where v is a single AT from T\(g i.e g is either a group created for the purpose of tail pruning or g is in the next level of the SE-Tree from g Case1 H\(g  H\(g y  SUp\(H\(g A T\(g c then ARA calculates y;=sup\(H\(g A{w I wcT\(g 5 A~c where p is the minimal AT in T\(g the ordering 2  Provided that the ordering of g is the same as g yi is less than or equal to yI The issue of tail ordering and how the ordering can be maintained is discussed in section 2.8 sup\(H\(g wI w~T\(g r]Alc is If the supports are not available to calculate 8 available for every t E T\(g since these conjuncts are members of the candidate set of g and is therefore available for the minimal AT p y  sup\(Hfg A IC C sup\(H\(g A It A Tc are available from the candidate set of H\(g since H\(g The supports necessary to calculate rs S  H\(g case 2 H\(g  g v yI  sup\(H\(g A T\(g Then ARA calculates y  sup\(H\(g A w I WE T\(g  w r U A IC where U is the minimal AT in T\(g under the ordering 2  y is less than or equal to yl provided that the ordering of g is the same as g sup\(H\(g A{wI weT\(g  tj~~c is available for every t E T\(g v since these conjuncts are members of the candidate set of g and is therefore available for the AT U Again it may not be possible to calculate 469 


It may also not be possible to calculate y2  sup\(H\(g A 7c c sup\(H\(g A d A lc However sup\(H\(g A c  sup\(H\(g   v A 7c and for every t E T\(g Therefore I\200 s sup\(H\(g lt A 1c sup\(H\(g lt A c sup\(H\(g   A v A 1c c sup\(H\(g   A lf A lc I sup\(H\(g A 1c c sup\(H\(g A d A c y2 where S consists of the IS1 tail ATs of g with the maximum values of sup\(H\(g  d A lc ARA therefore calculates t\200 S I\200 s The support values necessary to calculate yi are available from the candidate set of g 2.7.3 Pruning based on improvement Let r be the rule derivable from a group g with the maximum improvement Then imp\(r maxImp\(g If maxlmp g I minlmp then g is pruneable In order to find an upper bound on maxImp\(g the same methods as Dense Miner are used However because the value of y calculated for confidence pruning is re-used and may be higher in ARA than in Dense Miner the upper bound on maxImp\(g may be lower The methods are presented below without proofs These can be found in 51 or 71 Method 1 maxZmp\(g I maxConf\(g conf\(r where r is the highest confidence rule enumerated in any ancestor group of g X X Method 2 maxlmp g I   x'+y x'+y'+/l where x  rnaxkin sup rnin\(sup\(H\(g c J y is defined above and B I sup\(\(H\(g Z lz A lc where z is the AT in H\(g that minimises this expression In order to be used in improvement pruning y must be a lower bound on sup c sup where r is the rule derivable from g with the maximum improvement. It was shown in lemma 1 that y'l sup\(u  c sup\(u where U is any rule derivable from g Therefore y is a lower bound on sup\(r  c sup\(r Example of pruning Let A I B,C D E F,G H be an evaluated group g Assume that g is not pruneable therefore try to remove some tail ATs To do this we form a group g A A B I C D E F,G H It is found that no rule derivable from g can satisfy the constraints, i.e g is pruneable and so B can be removed from the tail of g After considering each of the tail ATs in the same way it is found that ATs B and D may be removed from the tail of g We form a new group g AI{c,E,F,G,H which replaces g in the SE-Tree Now because some ATs have been removed from the tail and tighter bounds may be available for y an attempt is made to prune g When attempting to prune gl and when attempting to prune ATs from the tail of gl ARA will use the value sup A c A D A E A F A G A H A c as a value for y I y sup A c A E A F A G A H A~C which is potentially higher than sup\(A A B A C A D A E A F A G A H A 7c as used in Dense Miner. Also y2 is greater than or equal to the other value used in Dense Miner Assume it is found that g is not pruneable and no ATs can be removed from its tail A new set of groups in the next level of the SE-Tree are created which replace gl Amongst these is the group h  A A F I G H Now in determining if h is pruneable ARA has available and will use y  sup\(A A F A G A H A lc Dense Miner will use sup\(A BA C A DA E A F AG A H A~c which is likely to be lower. Again yz is greater than or equal to the other value used in Dense Miner sup\(H\(g 1c csup\(H\(g  11 A 7 c This example shows how tighter bounds can be achieved for pruning using the additional support information collected by ARA If the maxATs constraint is specified even tighter bounds can be achieved IS Tfx 2.8 Tail ordering As described in 5 tail ordering is essential for efficient algorithm performance ARA requires that the tail of a group g is ordered before evaluation and the order maintained in groups derived from g until pruning is complete Therefore the vector of tail ATs is sorted before a group is evaluated The ordering used to sort the vector of tail ATs of a group g is descending value of sup\(H\(g A 7c g is the last evaluated ancestor of g An initial evaluation is carried out as the basis for sorting the tail of the root group. For clarity this has not been shown in Figure 2-2 470 


2.9 Results ARA was evaluated on two databases; the connect 4 database 9 and a census database IO Due to limitations of space only the results of the connect 4 tests are reported here The results achieved using the census data base were similar and are presented in 7 Connect 4 has 65,577 records and 42 predictive attributes each of which is categorical with 3 possible values, the consequent was defined as 223result  draw\224 of which there are 6,449 records The algorithm was evaluated according to the following measures Execution Time. Times are for the search phase only, post processing time is not included Number of groups evaluated. This represents the size of the search tree Number of tail ATs evaluated. This is required because a small number of groups with large tails may be more time consuming to evaluate than a large number of groups with small tails In order to compare ARA with Dense Miner a version of Dense Miner was implemented in accordance with the description given in 5 this version of Dense Miner is referred to as ARA-Dense ARA-Dense is identical to ARA in all respects except for the support information collected and the pruning functions any variation in performance can thus be attributed to these differences alone The execution times for ARA-Dense were compared with the times for Dense Miner reported in 5 and found to be slower by a factor of 1.5  1.8 This variation may be attributable to hardware or compiler differences or the detail of implementation All tests were carried out on a Dell Dimension XPS PI11 450 with 128MB memory 1 2 3 2.10 Commentary on test results When rninConf was set to zero it was found that there was no significant difference in the execution times of the two algorithms over a wide range of tests; indicating that the advantage of a smaller search tree was offset by the overhead for collecting more information Figure 2.1 shows the effect of using confidence for pruning by varying the values specified for rninConf The size of tree searched by ARA was smaller than ARA-Dense by a factor of 1.2 to 2.5 the higher the setting of rninConf the greater the reduction in tree size. The number of tail items evaluated varied by a similar order Execution time also varied in proportion to the size of the search tree indicating that the overhead for collecting the additional support information was outweighed by the benefit of a smaller tree when confidence was used for pruning The second set of tests, reported in figure 2.2 shows the effect of introducing the rnuxATs Constraint In order to demonstrate the effect of the maxATs constraint a further algorithm ARA-Part was implemented This is identical to ARA but does not include the additional pruning functionality based on the maxATs constraint ARA-Dense and ARA-Part simply stop searching when the number of ATs in the consequent of extracted rules is equal to maxATs The improvement of ARA-Part over ARA-Dense is thus attributable to the additional support information collected The improvement of ARA over ARA-Part is attributable to the additional pruning using the dTs constraint When muxATs is lower than 11 the maximum antecedent length of any rule that satisfies the other constraints there are considerable benefits to using the constraint For example with muxATs set to 6 the execution times for ARA and ARA-Part were 149 and 493 seconds respectively a saving of over 5 minutes. The time for ARA-Dense was 818 seconds which is over 5 times or 11 minutes longer It is concluded that when minConf is specified ARA generally outperforms ARA-Dense When the dTs constraint is specified further significant gains in performance can be achieved 3 Future Work An attribute test in ARA is a 3 tuple attribute operator  where operator is 223=\224 However it is straightforward to construct ATs with different operators For example an AT could be 223age c 50\224 or 221\221colour  red\224 allowing a richer variety of rules to be constructed A problem with increasing the variety of ATs is that the search space and the number of discovered rules may be vastly increased To counteract this additional constraints may be necessary. However these could be problem specific rather than algorithm specific For example, one possible constraint is to define which ATs or combinations of ATs are of interest or conversely which are not of interest\before the search is undertaken As a result some generality of the search will be lost but the search can be targeted at areas of interest, thus making the problem more tractable and avoiding rules that are not likely to be interesting One of the problems of finding all rules is specifying suitable values for rninCov and rninConf In this research we have experimented with the use of the heuristic search techniques in the DataLamp package developed by the University of East Anglia and the Lanner Group ll DataLamp has been used to find a good rule according to its fitness function then ARA used to find all rules with coverage and confidence greater than that rule 47 1 


04 I 250,000  B 200.000  2  100,000  n 5 50,000  A  z UY 150,000     4\222 f 221 07 225 20 30 40 50 60 minConf  I 7,000,000 ui 6,000,000 I 4.000.000 c,j 3,500,000  2 3,000,000  Z 2,500.000     c 5 2,000.000  7  0  1,500,000  2 500,000    n E i,ooo,ooo  L  07 1 4 OC I 20 30 40 50 60 minConf  I  2,500 h 0 g 2,000 g 1,000 Y E 1.500  c  e 8 500 m   maxATs  42 all minSup=65 minlmp=0.02 Figure 2-1 Varying minConf 4 References  11 Agrawal R Imielinski T Swami A Mining association rules between sets of items in large databases Proceedings of the ACM SIGMOD International Conference on Management of Data May 1993 pp 207  216 ACM Press 1993 2 Agrawal R Srikant R Fast Algorithms for Mining Association Rules Proceedings of the 20th International Conference on VLDB September 1994 pp 487  499 Morgan 3 Agrawal R Imielinski T Swami A Mining association rules between sets of items in large databases Proceedings of the ACM SIGMOD International Conference on Management of Data May 1993 pp 207  216 ACM Press 1993 4 Houtsma M Swami A Set Oriented Mining of Association Rules in Relational Databases Proceedings of the Eleventh International Conference on Data Engineering March 1995 pp 5 Bayardo R Agrawal R Gunopulos D Constraint-Based Rule Mining in Large Dense Databases Proc of the 15th 222 Kaufmann 1994 25  33 IEEE 1995 1,400 7 minSup  65 minConf  40 minlmp  0.02 Figure 2-2 Varying maxATs International Con on Data Engineering March 1999 pp 188  197. IEEE, 1999 6 Bayardo R Agrawal R Mining the Most Interesting Rules Proceedings of the 5th International Conference on Knowledge Discovery and Data Mining KDD 99 August 1999 pp 145  153 AAA1 Press 1999 7 Richards G Mining All Rules University of East Anglia Technical report SY S-COO 10 8 Rymon R Search Through Systematic Set Enumeration Proceedings of the 3rd International Conference on Principles of Knowledge Representation and Reasoning KR 92 October 1992 pp 539  550 Morgan Kaufmann 1992 9 Connect 4 data base http://www.ics.uci.edu mlearn/MLSummary.html IO Census data base http://kdd.ics.uci.edu databaseslcensus income/census-income. html  1 I Lanner Web Site http://www.lanner.coml solutions/datamining html 472 


local support count X.sup must be smaller than the local threshold s x D Following from the discus sion in Subsection 3.3 X.supq is bounded by the value min\(maxsupq X s x D  1 Hence an upper bound of X.sup can be computed by the sum x.supj  jEX.large-sites 2 min\(mazsupq\(X s x Dq  1 q=l q+?X.large-sites In FDM-LPP Si calls p-upper-bound to compute an upper bound for X.sup according to the above for mula This upper bound can be used to prune away X if it is smaller than the global support threshold 0 As discussed before both FDM-LUP and FDM LPP may have less candidate sets than FDM-LP How ever they require more storage and communication messages for the local support counts Their efficiency comparing with FDM-LP will depend largely on the data distribution 5 Performance Study of FDM An in-depth performance study has been performed to compare FDM with CD We have chosen to im plement the representative version of FDM FDM LP and compare it against CD Both algorithms are implemented on a distributed system by using PVM Parallel Virtual Machine 6 A series of three to six RS/6000 workstations running the AIX system are connected by a 10Mb LAN to perform the experi ment The databases in the experiment are composed of synthetic data In the experiment result the number of candidate sets found in FDM at each site is between 10  25 of that in CD The total message size in FDM is between 10  15 of that in CD The execution time of FDM is between 65  75 of that in CD The reduction in the number of candidate sets and message size in FDM is very significant The reduction in execution time is also substantial However it is not directly proportional to the reduction in candidate sets and message size This is mainly due to the overhead of running FDM and CD on PVM What we have ob served is that the overhead of PVM in FDM is very close to that in CD even though the amount of mes sage communication is significantly smaller in FDM From the results of our experiments it is also clear that the performance gain of FDM over CD will be higher in distributed systems in which the commu nication bandwidth is an important performance fac tor For example if the mining is being done on a distributed database over wide area or long haul net work The performance of FDM-LP against Apriori in a large database is also compared. In that case the response time of FDM-LP is only about 20 longer Interpretation transaction mean size mean size of maximal potentially large itemsets number of potentially large itemsets Number of items Clustering size Pool size Correlation level Multiplying factor Parameter ITI III ILI N sq Ps Mf Cr Value 10 4 2000 1000 5-6 50  70 0.5 1260  2400 Table 5 Parameter Table than 1/n of the response time of Apriori where n is the number of sites This is a very ideal speed-up In terms of total execution time FDM-LP is very close to Apriori The test bed that we use has six workstations Each one of them has its own local disk, and its partition is loaded on its local disk before the experiment starts The databases used in our experiment are synthetic data generated using the same techniques introduced in 2 lo The parameters used are similar to those in lo Table 5 is a list of the parameters and their values used in our synthetic databases Readers not familiar with these parameters can refer to 2  In the following we use the notation Tx.Iy.Dm to denote a database in which D  m in thousands IT1  x and 111  y T10.14.D200K s  3 4 5 6 Number of Nodes FDM CD Figure 1 Candidate Sets Reduction n  3 4 5 6 5.1 Candidate Sets and Message Size Re duction The sizes of the databases in our study range from 200K to 600K transactions and the minimumsupport threshold ranges from 3 to 3.75 Note that the number of candidate sets at each site are the same in CD and different in FDM In our experiment we witnessed a reduction of 75  90 of candidate sets on 39 


T10.14.D200K, n  3 T10.14.D200K, n  3 60  I S 8 3.00 3.25 3.510 3.75 YO  I YO Minimum support FDM kCD  gs 3.00 3.25 3.50 3.75 Minimum support FDM CD Figure 4 Message Size Reduction Figure 2 Candidate Sets Reduction average at each site when FDM-LP is compared with CD In Figure 1 the average number of candidate sets generated by FDM-LP and CD for a 200K transaction database are plotted against the number of partitions FDM-LP has a 75  90 reduction in the candidate sets The percentage of reduction increases when the number of partitions increases This shows that FDM becomes more effective when the system is scaled up In Figure 2 the same comparison between FDM-LP and CD is presented for the same database with three partitions on different thresholds In this case, FDM LP experienced a similar amount of reduction T10.14.D200K s  30/0 I 150 100 50 0 3 4 5 6 Number of Nodos FDM CB Figure 3 Message Size Reduction n  3 4 5 6 The reduction in candidate sets should have a pro portional impact on the reduction of messages in the comparison Moreover as discussed before the polling site technique guarantees that FDM only requires O\(n messages for each candidate set which is much smaller than the O\(n2 messages required in CD In our experiment FDM has about 90 reduction in the total message size in all cases when it is compared with CD In Figure 3 the total message size in FDM and CD for the same 200K database are plotted against the number of partitions In Figure 4 the same compari son on the same database of three partitions with dif ferent support thresholds are presented Both results confirm our analysis that FDM-LP is very effective in cutting down the number of messages required T10.14.D200K s  3 90 E3 28  U 70 cc Q 8 a c 50 c xs w  I 3 4 5 6 Number of Nodes FDM CD Figure 5 Execution Time n  3 4 5 6 T10.14.D200K n  3 3.00 3.25 3.50 3.75 Minimum Support E-FDM A-CD Figure 6 Execution Time 5.2 Execution Time Reduction We have also compared the execution time between FDM-LP and CD The execution time of FDM-LP and CD on a 200K database are plotted against the number of partitions in Figure 5 FDM-LP is about 40 


25  35 faster than CD in all cases In Figure 6 the comparison is plotted against different thresholds for the same database on three partitions Again FDM LP is shown to have similar amount of speed-up as in Figure 5 n  3 D  60011 s  2 I Apriori I FDM-LP response time sec I 1474 I 387 I total execution time sec I 844.7 I 842.9 I Table 6 Efficiency of FDM-LP We have also compared FDM-LP on three sites against Apriori with respect to a 600K transactions database in order to find out its efficiency in large database The result is shown in Table 6 The re sponse time of FDM-LP is only slightly 20 larger than 1/3 of that of Apriori In terms of the total ex ecution time FDM-LP is very close to Apriori For a large database FDM-LP may have a bigger portion of the database residing in the distributed memory than Apriori Therefore it will be much faster than running Apriori on the same database in a single ma chine This shows that FDM-LP on a scalable dis tributed system is an efficient and effective technique for mining association rules in large databases The performance study has demonstrated that FDM generates a much smaller set of candidate sets and requires a significantly smaller amount of mes sages when comparing with CD The improvement in execution time is also substantial even though the overhead incurred from PVM prevents FDM from achieving a speed-up proportional to the reduction in candidate sets and message size Even though we have only compared CD with FDM-LP there is enough evidence to show that FDM is more efficient than CD in a distributed environment In the follow ing sections we will discuss our future plan of imple menting the other versions of FDM 6 Discussions In this discussion we will first discuss the issue of possible extension of FDM for fast parallel mining of association rules Following that we will discuss two other related issues 1 the relationship between the effectiveness of FDM and the distribution of data and 2 support threshold relaxation for possible reduction of message overhead The CD and PDM algorithms are designed for share-nothing parallel environment. In particular CD has been implemented and tested on the IBM SP2 machine In designing algorithm for parallel mining of association rules not only the number and size of messages required should be minimized but also the number of synchronizations which is the number of rounds of message communication CD has a simple synchronization scheme It requires only one round of message communication in every iteration Besides the second iteration PDM also has the same synchro nization scheme as CD If FDM was used in the paral lel environment it has a shortcoming even though it requires much less message passings then CD it needs more synchronizations However FDM can be modi fied to overcome this problem In fact in each itera tion the candidate set reduction and global pruning techniques can be used to eliminate many candidates and then a broadcast can be used to exchange the local support counts of the remaining candidates This ap proach will generate less candidate sets than CD and has the same number of synchronization Therefore it will perform better than CD in all cases Performance studies has been carried out in a 32-nodes IBM SP2 to study several variations of this approach and the result is very promising Another interesting issue is the relationship be tween the performance of FDM and the distribution of the itemsets among the partitions From both The orem 1 and Example 1 it is clear that the number of candidate sets decreases dramatically if the distribu tion of itemsets is quite skewed among the partitions If most of the globally large itemsets were locally large at most of the sites the reduction of candidate sets in FDM would not have been as significant In the worst case if every globally large itemset is locally large at all the sites the candidate sets in FDM and CD will be the same Therefore data skewness may improve the performance of FDM in general Special partitioning technique can be used to increase the data skewness to optimize the performance of FDM Some further study is required to explore this issue The last issue that we want to discuss is the pos sible usage of the relaxation factor proposed in ll In FDM if a site sends not only those candidate sets which are locally large but also those that are almost locally large to the polling sites the polling sites may have local support counts from more sites to perform the global pruning of candidate sets For example if the support threshold is lo every site can send the candidate sets whose local support counts exceed 5 to their polling sites In this case for some candi date sets their polling sites may receive local sup port counts from more sites than the no relaxation case Hence the global pruning may be more effec tive However there is a trade-off between sending more candidate sets to the polling sites and the prun ing of candidate sets at the polling sites More study is necessary on the detailed relationship between the relaxation factor and the performance of the pruning 7 Conclusions In this paper we proposed and studied an efficient and effective distributed algorithm FDM for mining association rules Some interesting properties between 41 


locally and globally large itemsets are observed which leads to an effective technique for the reduction of can didate sets in the discovery of large itemsets Two powerful pruning techniques local and global prun ings are proposed Furthermore the optimization of the communications among the participating sites is performed in FDM using the polling sites Sev eral variations of FDM using different combination of pruning techniques are described A representative version FDM-LP is implemented and whose perfor mance is compared with the CD algorithm in a dis tributed system The result shows the high perfor mance of FDM at mining association rules Several issues related to the extensions of the method are also discussed The techniques of can didate set reduction and global pruning can be inte grated with CD to perform mining in a parallel envi ronment which will be better than CD when consider ing both message communication and synchronization Further improvement of the performance of the FDM algorithm using the skewness of data distribution and the relaxation of support thresholds is also discussed Recently there have been interesting studies on the mining of generalized association rules multiple level association rules quantitative association rules etc Extension of our method to the min ing of these kinds of rules in a distributed or parallel system are interesting issues for future research Also parallel and distributed data mining of other kinds of rules such as characteristic rules 7 classification rules, clustering 9 etc is an important direction for future studies For our performance studies an im plementation of the different versions of FDM on an IBM SP2 system with 32 nodes has been carried out and the result is very promising References l R Agrawal and J C Shafer Parallel mining of association rules Design implementation and experience In IBM Research Report 1996 2 R Agrawal and R Srikant Fast algorithms for mining association rules In Proc 1994 Int Conf Very Large Data Bases pages 487-499 Santiago Chile, September 1994 3 R Agrawal and R Srikant Mining sequential patterns In Proc 1995 Int Conf Data Engi neering pages 3-14 Taipei, Taiwan March 1995 4 D.W Cheung J Wan V Ng and C.Y Wong Maintenance of discovered association rules in large databases An incremental updating tech nique In Proc 1996 Int\222l Conf on Data Engi neering New Orleans, Louisiana Feb 1996 5 U M Fayyad 6 Piatetsky-Shapiro P Smyth and R Uthurusamy Advances zn Knowledge Dis covery and Data Mining AAAI/MIT Press 1996 6 A Geist A Beguelin J Dongarra W Jiang R Manchek and V Sunderam PVM Parallel Virtual Machine A Users\222 Guide and Tutorial for Networked Parallel Computing MIT Press 1994 7 J Han Y Cai and N Cercone Data driven discovery of quantitative rules in relational databases IEEE Trans Knowledge and Data En gineering 5:29-40 1993 Discovery of multiple-level association rules from large databases In Proc 1995 Int Conf Very Large Data Bases pages 420-431 Zurich Switzerland Sept 1995 8 J Han and Y Fu 9 R Ng and J Han Efficient and effective cluster ing method for spatial data mining In Proc 1994 Int Conf Very Large Data Bases pages 144-155 Santiago Chile, September 1994 lo J.S Park M.S Chen and P.S Yu An effec tive hash-based algorithm for mining association rules In Proc 1995 ACM-SIGMOD Int Conf Management of Data pages 175-186 San Jose CA May 1995 ll J.S Park M.S Chen, and P.S Yu Efficient par allel mining for association rules In Proc 4th Int Conf on Information and Knowledge Manage ment pages 31-36 Baltimore Maryland Nov 1995 12 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases In Proc 1995 Int Conf Very Large Data Bases pages 432-443 Zurich Switzerland Sept 1995 13 A Silberschatz M Stonebraker and J D U11 man Database research Achievements and op portunities into the 21st century In Report of an NSF Workshop on the Future of Database Sys tems Research May 1995 14 R Srikant and R Agrawal Mining general ized association rules In Proc 1995 Int Conf Very Large Data Bases pages 407-419 Zurich Switzerland Sept 1995 association rules in large relational tables In Proc 1996 ACM-SIGMOD Int Conf Manage ment of Data Montreal Canada June 1996 15 R Srikant and R Agrawal Mining quantitative 42 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


