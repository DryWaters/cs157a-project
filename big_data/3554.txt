Detecting Hostile Accesses through Incremental Subspace Clustering Masaki Narahashi Einoshin Suzuki Electrical and Computer Engineering Yokohama National University 79-5 Tokiwadai Hodogaya Yokohama 240-8501 Japan narahashi@slab.dnj.ynu.ac.jp suzuki@ynu.ac.jp Abstract In this paper we propose an incremental subspace clustering method for exibly detecting hostile accesses to a Web site Typical log data for Web accesses are huge contain irrelevant information and exhibit dynamic characteristics We overcome these difìculties through data squash 
ing subspace clustering and an incremental algorithm We have improved by modifying its data squashing functionality our subspace clustering method SUBCCOM so that it can exploit previous results Experimental evaluation conìrms superiority of our I-SUBCCOM in terms of precision recall and computation time 1 Introduction Hostile accesses to Web sites have caused serious damage to our community Such examples include the DoS attacks to 13 root servers which are spread in the world in October 2002 and the SQL slammers which caused serious damage to Korea and other countries in January 2003 
Recently the Internet has signiìcantly increased its importance in our daily life and hostile accesses to Web sites can cause fatal problems In order to detect such hostile accesses automatically or semi-automatically application of fraud detection techniques which are based on a machine learning method is expected to be promising Such methods include association rule classiìcation 7 statistical test 5 meta learning and outlier detection 13 The last approach might lack in reliability but can potentially detect novel kinds of attacks We believe that clustering can substitute for outlier detection Various methods have been proposed for clustering 
which partitions a given set of examples to a set of similar groups called clusters W e attrib ute the reason to v arious deìnitions of a cluster and various functionalities in the procedure For instance we have deìned a cluster in terms of compressibility based on a distance measure and proposed a fast clustering method SUBCCOM which can select relevant attributes by squashing data for time efìciency Alternatively SUBCCOM can cluster a huge amount of data with irrelevant attributes efìciently In order to semi detect hostile accesses to a Web site with clustering the following three condi 
tions are considered to be mandatory efìcient handling of huge data Web access log data are typically huge detection of relevant attributes such data typically contain irrelevant attributes and an incremental algorithm such data are incrementally updated Since SUBCCOM cannot fulìll the last requirement we propose its incremental version I-SUBCCOM in this paper In the rest of the paper we deìne the subspace clustering problem and explain SUBCCOM in section 2 Section 3 introduces Web access log clustering for hostile access detection and I-SUBCCOM is proposed as a time-efìcient solution We experimentally evaluate its performance in section 
4 and give conclusions in section 5 2 SUBCCOM 2.1 Description of the Subspace Clustering Problem The input of subspace clustering includes m examples x 1  x 2    x m each of which is represented with n numerical attributes We denote the set of these n attributes as V  Note that an example x i 
represents a point in an n dimensional space The output of subspace clustering represents a partition  1  2    c  each of which represents a cluster of x 1  x 2    x m  and a set v   V  of relevant attributes Note that attributes in 
v span the subspace in which clusters exist A pattern extraction procedure in data mining is typically interactive the user modiìes the condition of the procedure in a series of application of methods and investigation of results In this paper we assume that the user speciìes the number l of attributes in v  Proceedings of the IEEE/WIC International Conference on Web Intelligence \(WIê03 0-7695-1932-6/03 $17.00 © 2003 IEEE 


2.2 CF Tree for Data Squashing The main stream of conventional data mining research has concerned how to scale up a learning/discovery algorithm to cope with a huge amount of data Contrary to this approach data squashing concerns ho w t o scale do wn such data so that they can be dealt by a conventional algorithm A CF clustering feature tree which represents a data structure for data squashing was proposed in a fast clustering algorithm BIRCH A CF tree represents a height-balanced tree which is similar to a B tree A node of a C F tree contains as entries to its child nodes a set of CF vectors each of which corresponds to an abstracted expression of a set of examples For a set of examples x 1  x 2    x N to be squashed a CF vector CF consists of the number N of the examples the add-sum vector  N i 1 x i of the examples and the squaredsum  N i 1  x i  2 of the attribute values of the examples Since the CF vector satisìes additivity and can be thus updated incrementally BIRCH requires only one scan of the training data set Moreover various inter-cluster distance measures can be calculated with the corresponding two CF vectors only This signiìes that the original data set need not be stored and clustering can be performed with their CF vectors only A CF tree is constructed with a similar procedure for a B tree When a new example is read it follows a path from the root node to a leaf then nodes along this path are updated Selection of an appropriate node in this procedure is based on a distance measure which is speciìed by the user In a leaf the example is assigned to its closest entry i.e a CF vector if the distance between the example and the examples of the entry is below a given threshold L An entry of a leaf represents a set of squashed examples and is the smallest unit in a CF tree 2.3 Compressibility as an Evaluation Criterion It is widely accepted that compressibility is deeply related to inductive learning For instance MDL Minimum Description Length principle which has been successfully employed in inductive learning as an evaluation criterion favors a model which effectively compresses the information content of a given data set and a model A CF tree highly depends on the set of attributes which are employed An entry in a leaf of a CF tree which we explained in the previous section contains a set of examples which are close to each other Therefore the number of entries in leaves is related to the number of dense regions in the example space and can be used to deìne an index of compressibility In this paper we propose    u  as an evaluation criterion of a subspace spanned by u  where   u  represents the number of entries in leaves of the corresponding CF tree 2.4 Heuristic Search for the Relevant Attributes Note that subsets of attributes form a complete lattice of which inìmum and supremum are an empty set and V respectively We have realized search for a set v of relevant attributes as a search procedure for an appropriate subset of V in the complete lattice guided by the evaluation criterion in the previous section Forward search and backward search represent brute force search from the inìmum and the supremum respectively Jumping search which we proposed in represents a heuristic search method which is expected to be timeefìcient without sacriìcing accuracy Our method rst builds a CF tree for each attribute a i with L  T 1  then obtains a set u of attributes each of which evaluation criterion value    a i  is no smaller than the average value In case  u  l  remaining attributes which have the largest evaluation criterion values are added to u so that  u  becomes l  Finally our method from u  performs a backward search with L  T 2  Since a single attribute exhibits high compressibility T 1 is typically settled to a much smaller value than T 2  Our subspace clustering system which we call SUBCCOM rst obtains a set u of possibly relevant attributes and the entries y 1  y 2    y p of leaves of the corresponding CF tree with jumping search then applies the k means algorithm to y 1  y 2    y p  We show the algorithm of SUBCCOM below where the procedure k means is modiìed so that it clusters a set of squashed examples y 1  y 2    y p  Algorithm  SUBCCOM  x 1  x 2    x m  l  v   1  2     c  Input  data set x 1  x 2    x m  number of relevant attributes l Output  subspace v  clusters  1  2    c Parameter  entries y 1  y 2    y p in leaves of the nal CF tree  1 For  i 1 i  n  i  2 Build a CF tree using values for a i with L  T 1 3 u   4 Foreach  a i such that    a i    n i 1    a i  n  5 u  u  a i  6 While   u  l  7 u  u   a   where a  argmax a   u     a   8 While   u  l  9 Foreach  a i  u  10 Build a CF tree using values for u  a i  with L  T 2 11 u  u  a   where a  argmax a    u  a   12 Build a CF tree  using values for u with L  T 2 13 v  u Proceedings of the IEEE/WIC International Conference on Web Intelligence \(WIê03 0-7695-1932-6/03 $17.00 © 2003 IEEE 


14   1  2    c  k means  y 1  y 2    y p  3 Proposed Method 3.1 Web Access Log Clustering for Hostile Access Detection Each time a user accesses to a Web site an access log is generated We show below an example of a Web access log kali.slab.dnj.ynu.ac.jp ningen 04/Apr/2001:16:48:57  GET algodata HTTP/1.0 401 476 This log which represents a daily access consists of seven parts They are from left to right the name of the remote host 1  the name of the user in the remote host 2  the name of the user in the identiìcation the day and the time of the accesses the content of the request the status code which was returned to the request and the bytes which were sent The content of the request consists of an HTTP command the name of the requested le and the version of HTTP This example shows that a user from kali.slab.dnj.ynu.ac.jp tried to obtain les below algodata with an identiìcation name ningen and obtained a status code 401 which represents a denial of identiìcation probably due to a misuse of a password Other than daily accesses accesses to a Web site also include hostile accesses such as attacks with worms e.g Nimda Code Red For instance an attack with the Code Red has generated the following log 3  202.39 02/Aug/2001:06:54:59 GET/default.ida?NNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNN%u9090%u6858%ucbd3%u7801%u90 90%u6858%ucbd3%u7801%u9090%u6858%ucbd3%u78 01%u9090%u9090%u8190%u00c3%u0003%u8b00%u53 1b%u53ff%u0078%u0000%u00=a HTTP/1.0 400 331 An administrator is required to detect hostile accesses from Web access log data and take appropriate measures as 1 Here a remote host name can be replaced by its IP address 2 A  is shown when the remote host does not employ an identiìcation service a daemon which gives information of the user for identiìcation A typical host does not employ such services 3 We modify the IP address to protect privacy of the user soon as possible Fast detection is however usually difìcult since Web access log data consist of a huge number of examples are updated continuously and reîect dynamic characteristics of accesses Suppose we are given 100,000 web access logs of which 1  are hostile If an administrator inspects 100 randomly-chosen logs of the 100,000 logs s/he has little chance of detecting hostile accesses In the example however suppose the 100,000 logs are segmented into ve groups each of which contains 95,000 4,000 700 200 or 100 logs If each group consists of similar logs a small cluster would contain peculiar logs each of which is expected to have a better chance of being hostile than a typical log Therefore the administrator has a much better chance of detecting hostile accesses by inspecting the small groups than random inspection We thus employ clustering as preprocessing in order to facilitate detection of hostile accesses It should be noted again that Web access log data are huge and dynamic These characteristics favor an incremental clustering which substitutes the latest data for the oldest data Below we deìne Web access log clustering for hostile access detection The input represents s sets of examples w 1  w 2    w s in chronological order and w j is called a processing unit The processing units which are clustered simultaneously are called a processing object and we assume that a processing object consists of at most h processing units Suppose a processing unit w i  h 1 is added to a processing object w i 1  w i 2    w i  h  the oldest processing unit w i 1 should be deleted beforehand since a processing object consists of at most h processing units Alternatively this method rst grows its sliding window from size 1 to h  then xes the size to h until the end In this framework a method outputs results for each processing object The performance of a method is evaluated with precision and recall in terms of hostile access logs Since it is difìcult for a user to inspect all access logs the user samples a xed number of logs from each cluster as we have stated The precision P and the recall R are deìned in terms of the sampled examples where  represents the number P   of hostile access logs in the sampled examples  of sampled examples R   of hostile access logs in the sampled examples  of hostile access logs 3.2 I-SUBCCOM For the problem in the previous section SUBCCOM which we explained in section 2.4 is inefìcient since it resolves the problem from scratch each time the processing object is updated In order to circumvent this problem we propose I-SUBCCOM which is an incremental version of SUBCCOM Proceedings of the IEEE/WIC International Conference on Web Intelligence \(WIê03 0-7695-1932-6/03 $17.00 © 2003 IEEE 


I-SUBCCOM exploits previous results by allocating a CF vector for each processing unit in a leaf of a CF tree A leaf  i of a CF tree stores CF vectors y i 1  y i 2    y ij  1  j  h  for processing units 4  thus examples from a processing unit are squashed to an identical CF vector A leaf has also a set of entries each y i of which represents an add-sum of y ij in terms of j  We employ y i for deciding allocation of an entry of a leaf to an example In case a processing unit w k is deleted each y ik is rst deleted then each y i is updated In case adding a processing unit to the processing object our algorithm employs entries of leaves of the previous CF tree and the examples of the processing unit If the processing object consists of h processing units the entries each of which corresponds to the oldest processing unit are deleted in advance A CF tree is constructed by rst using the remaining entries then the examples of the latest processing unit We show the algorithm of I-SUBCCOM below Algorithm  I-SUBCCOM  w 1  w 2    w s  l  Input  processing units w 1  w 2    w s  number of relevant attributes l Output  a set of clusters  1   2     s 1   2 For  i 1 i  s  i  3   build-itree    4 Let the examples in w i be x 1  x 2    x m 5 SUBCCOM2    x 1  x 2    x m  l  u   i 1  i 2     ic  6  i    i 1  i 2    ic  7 Output  i 8 If  i>h  9 Foreach  j  10 Delete y ji  h 1 from leaves of  11 Update y j 12 Foreach  j  13     y j  In the algorithm the procedure build-itree returns the CF tree  deìned in this section from entries y j in   or returns a null tree when    The procedure SUBCCOM2 employs  as its initial CF tree and employs the k medoids algorithm instead of the k means algorithm in SUBCCOM 5  For instance suppose we have three processing units w 1  w 2  w 3  and h 2  Firstly the processing object consists of w 1  and SUBCCOM outputs the rst results from it Secondly w 2 is added to the processing object The entries 4 The number of CF vectors is at most h since a leaf does not necessarily contain examples from all processing units 5 While SUBCCOM assumes that an example represents a point in an n dimensional space I-SUBCCOM assumes that an example represents a Web access log In the latter a mean of a cluster can represent a nonexistent Web access log in leaves of the previous CF tree are employed to construct an initial CF tree which is then updated with examples in w 2  and we obtain the second results It should be noted that in a leaf examples of w 2 are stored in a set of CF vectors different from a set of CF vectors for w 1  Thirdly w 1 should be deleted before adding w 3 to the processing object The CF vectors of leaves for w 1 are deleted and the entries of leaves are updated I-SUBCCOM obtains the third results by constructing an initial CF tree with the entries then updating it with examples in w 3  4 Experimental Evaluation 4.1 Preprocessing of Web Access Logs We employ Web access log data of our Web site www.slab.dnj.ynu.ac.jp in the experiments The data consist of 205,590 examples measured during 99 weeks Since I-SUBCCOM assumes numeric attributes for distance calculation we transformed a nominal attribute to a numerical attribute as follows We attributed an integer value for each category of a part of a Web access log the remote host name the user name in identiìcation the HTTP command the requested directory the status code the bytes which were sent and the requested le  Remote host name we categorized remote host names into those inside our laboratory our university our country 6  and the rest and allocated them 0 10 20 30 respectively Since most of the accesses were domestic we gave them detailed categories  User name we categorized user names into those without names either of YNU dnj member ningen student and the rest We allocated them 0 10 20 respectively Each of the proper nouns represents a user name provided by our administrators We believe that most of the rest are due to mistypes thus considered to be more similar to the proper nouns than those without names  HTTP command we categorized HTTP commands into GET and the rest and allocated them 0 and 30 respectively The value 30 is chosen since an HTTP command other than GET is rare and we considered balance with other attributes  Requested directory we categorized requested directories into those which were accessed at least 1000 times and the rest and allocated them 10 and 0 respectively In addition we allocate 30 as the value if the le name consists of more than 49 ASCII characters This is justiìed since a hostile access such as 6 Until here the smallest category applies to a remote host name Proceedings of the IEEE/WIC International Conference on Web Intelligence \(WIê03 0-7695-1932-6/03 $17.00 © 2003 IEEE 


worms and requests for non-existent les tends to request a le with a long name  Status code we transformed each status code into the rst digit multiplied by 10 Status codes are sorted from successes to errors and from user-driven to server-driven 7  Since a status code in the data is within 200 and 505 there are only four kinds of values i.e 20 30 40 and 50  Bytes which were sent we allocated 0 to 0 499 bytes 10 to 500 1149 bytes 20 to 1150 4499 bytes and 30 to the rest The threshold values were determined so that each bin contains approximately fty thousands of examples  Requested le the requested les were categorized by their extensions We allocated 0 to a typical nonexecutable le c csv dat doc gif hdml htm html ico jpg pdf png ppt ps txt xbm xls xpm or zip 10 to a typical executable le asp cgi dll exe ida php php3 or pl and 20 to the rest We believe that a web access log is more likely to be hostile in this order 4.2 Experiments The preprocessed data were employed in the experiments for evaluating the performance of our I-SUBCCOM and its alternatives We settled the length of a processing unit to one week h 4  and the number of clusters to 5 The parameters for I-SUBCCOM were set to T 1 0  01 T 2 2  0  and those for BIRCH L 2  0  We assume that the user is willing to inspect at most 100 examples thus the user samples at most 20 examples from each of 5 clusters for inspection If a cluster consists of less than 20 examples the user investigates all examples in the cluster It should be noted that no one would easily inspect all access logs nor recognize all hostile accesses This signiìes that we need an approximate measure for performance evaluation thus we employ Nimda and Code Red both of which have numerous examples and are relatively easy to be recognized 8  We measured the precision and the recall described in section 3.1 by assuming that a hostile access corresponds to either Nimda or Code Red Each precision and recall is an average of 100 trials since the user samples examples to be inspected randomly The number l of relevant attributes in a cluster was settled to l 4  5  6 for I-SUBCCOM For comparative purpose we have chosen one of the latest algorithms which select relevant attributes PROCLUS 1  l 4  5  6 Two 7 Status codes are grouped into 2XX Success 3XX Redirection 4XX Client Error and 5XX Server Error 8 We recognized Nimda and Code Red if either of the following strings root.exe cmd.exe or default.ida is contained in an access log representatives of whole-space clustering BIRCH and the k medoids algorithm were also emplo yed  F or a f air comparison BIRCH was modiìed to an incremental algorithm by substituting the representation and the procedure of I-SUBCCOM for its CF tree In order to evaluate the beneìts of machine learning approach we also employ a naive method which samples 100 examples from the processing object randomly We show the results for precision recall and computation time in gures 1 2 and 3 respectively The results for precision and recall begin from the 29th week when Code Red appears in the access logs for the rst time 9  For visibility each value represents an average of ve successive processing objects i.e ve successive updates and the results of l 5  6 are omitted due to their inferior performance  0 0.05 0.1 0.15 0.2 0.25 0.3 30 40 50 60 70 80 90 I-SUBCCOM \(l=4 PROCLUS \(l=4   BIRCH   k-medoids   no clustering   week precision Figure 1 Results for precision 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 30 40 50 60 70 80 90 I-SUBCCOM \(l=4 PROCLUS \(l=4   BIRCH   k-medoids   no clustering   week recall Figure 2 Results for recall 9 There are neither Code Red nor Nimda till the 28th week Proceedings of the IEEE/WIC International Conference on Web Intelligence \(WIê03 0-7695-1932-6/03 $17.00 © 2003 IEEE 


 0.01 0.1 1 10 100 1000 10000 10 20 30 40 50 60 70 80 90 SUBCCOM \(l=4 PROCLUS \(l=4   BIRCH   k-medoids   week computation time [s Figure 3 Results for computation time Note that the vertical axis is in a log scale 4.3 Analysis of Experimental Results From gures 1 and 2 some readers might consider that I-SUBCCOM exhibits lower precision and lower recall than expected The purpose of Web access log clustering however is a preprocessing method for detecting hostile accesses as we stated in section 3.1 We believe that these results signify that our I-SUBCCOM is useful in organizing a huge amount of Web access logs in order to give alerts to administrators In gure 1 our I-SUBCCOM  l  4 almost always outperforms other methods in precision and we attribute the reason to appropriateness of our approach to this problem The k medoids algorithm is ranked the second probably due to its lack of subspace clustering capability BIRCH is inferior to the k medoids algorithm probably due to its use of data squashing PROCLUS  l 4 is often outperformed by the other clustering-based methods since its use of random sampling in search is considered to be inadequate for this problem The no clustering approach which approximately corresponds to random sampling almost always exhibits the worst performance and this shows effectiveness of the clustering-based approach An administrator would thus beneìt from the use of a machine learning method in this problem domain Since recall represents a detection rate of the whole hostile accesses by sampling it measures performance of a more difìcult problem than that of precision Therefore the differences of the methods in gure 2 are typically much smaller than those in gure 1 However it can be safely stated that our I-SUBCCOM  l  4 represents the best method and we believe that this is due to appropriateness of our approach Analysis of gure 2 is similar to that of gure 1 In terms of computational time BIRCH is the fastest probably due to its incremental nature and data squashing capability as we see in gure 3 Our I-SUBCCOM  l 4\is second to BIRCH probably due to its overhead of selecting a subspace for clustering However the overall performance in terms of precision recall and computational time shows that our I-SUBCCOM is the best method for detecting hostile accesses The other two non-incremental methods are much slower than BIRCH and I-SUBCCOM and the difference can be more than 100 times PROCLUS is faster than the k medoids algorithm probably due to its use of sampling in search It should be also noted that each clustering-based method facilitates detection of rare accesses as a side-effect For instance GET represents an outstanding majority in the HTTP commands and it is difìcult to detect other kinds of HTTP commands with the no-clustering approach On the other hand the Web access logs with these commands are almost always detected in each clustering-based method since the logs are separated in small clusters We believe that this fact implicitly validates the use of a clusteringbased method for detection of novel hostile accesses which are dissimilar from a typical access 5 Conclusions In this paper we proposed the use of clustering as a promising preprocessing method for detecting hostile accesses to a Web site Since Web access log data are huge contain irrelevant attributes and are updated incrementally an incremental subspace clustering method seems appropriate in order to fulìll this objective We have upgraded SUBCCOM which we proposed in 11 to an incremental algorithm by modifying its data squashing functionality in order to satisfy all the three requirements Experimental evaluation shows that our I-SUBCCOM exhibits promising results in terms of precision recall and computation time and outperforms other alternatives by its overall performance Our future work includes effective transformation of a nominal attribute to a numerical attribute and application of our approach to other kinds of fraud detection problems References  C C Agg arw al et al F ast algorithms for projected cluster ing In Proc 1999 ACM SIGMOD Intêl Conf on Management of Data SIGMOD  pages 61Ö72 1999  P  K Chan and S J Stolfo T o w ard scalable learning with non-uniform class and cost distributions A case study in credit card fraud detection In Proc Fourth Intêl Conf on Knowledge Discovery and Data Mining KDD  pages 164 168 1998 Proceedings of the IEEE/WIC International Conference on Web Intelligence \(WIê03 0-7695-1932-6/03 $17.00 © 2003 IEEE 


 D Comer  The ubiquitous B-tree ACM Computing Surveys  11\(2 June 1979  W  DuMouchel et al Squashing at les atter  I n Proc Fifth ACM Intêl Conf on Knowledge Discovery and Data Mining KDD  pages 6Ö15 1999  W  DuMouchel and M Schonlau A f ast computer intrusion detection algorithm based on hypothesis testing of command transition probabilities In Proc Fourth Intêl Conf on Knowledge Discovery and Data Mining KDD  pages 189 193 1998  V  Esti vill-Castro Wh y s o man y clustering algorithms  a position paper SIGKDD Explorations  4\(1 June 2002  T  F a wcett and F  Pro v ost Adapti v e fraud detection Data Mining and Knowledge Discovery  1\(3 1997  R Fielding et al Hyperte xt transfer protocol  http/1.1 http://www.ietf.org/rfc/rfc2616.txt 1999 sections 10.210.5 current July 3 2003  L Kaufman and P  J Rousseeuw  Finding Groups in Data  Wiley New York 1990  W  Lee S J Stolfo and K W  Mok Mining audit data to build intrusion detection models In Proc Fourth Intêl Conf on Knowledge Discovery and Data Mining KDD  pages 66Ö72 1998  M Narahashi and E Suzuki Subspace clustering based on compressibility In Discovery Science Lecture Notes in Computer Science 2534 DS  pages 435Ö440 SpringerVerlag 2002  J Rissanen Stochastic Complexity in Statistical Inquiry  World Scientiìc Singapore 1989  K Y amanishi et al On-line unsupervised outlier detection using nite mixtures with discounting learning algorithms In Proc Sixth ACM Intêl Conf on Knowledge Discovery and Data Mining KDD  pages 320Ö324 2000  T  Zhang R Ramakrishnan and M Li vn y  BIRCH An efìcient data clustering method for very large databases In Proc 1996 ACM SIGMOD Intêl Conf on Management of Data SIGMOD  pages 103Ö114 1996 Proceedings of the IEEE/WIC International Conference on Web Intelligence \(WIê03 0-7695-1932-6/03 $17.00 © 2003 IEEE 


Table 2 Classification time and memory usage of L this level forces a priority ordering among rules that avoids considering a medium quality rule belonging to the sec ond level when a better rule is available for classification in the first level Hence. considering second level rules can never cause a reduction in precision since cases classified by these rules would he otherwise unclassified or assigned to the default class The lazy pruning approach allowed us to significantly increase average classification precision over previous ap proaches In particular the experiments show that the pres ence of the second level provides a major enhancement in the classification quality without reducing the efficiency of the classification activity References I Illp;//wvwl,icr.uci.edu ml~~m/MLRepo.~iru~.hrml UCI Machine Leornin Reposirory Unrversiw of CaliJomio Imine 121 R Agrawal and R Srikant Fast algorithm for mining asso ciation rules In VLDB'Y4 Sonriujio Chile Sept 1994 3 G Dong X Zhang L Wong and 1 1.i CAEP Classifi cation by aggregating emerging piter In lnlemnrionrrl Conference on Discovery Science To!qo Japan Dec 1999 141 U Fayyad and K lrani Multi-interval dircrriization of continuos-valued attributes for classification learning In IJ CAl'Y3 1993 151 J Han J Pei and Y Yin Mining frequent patterns with out candidate generation In SIGMOD'W Dollns 7X May 2ooO 6 W Li J Han and J Pei CMAR Accurate and efficient classification based on multiple class-association ruler In ICDM'OI Son Jose CA November 2001 71 B Liu W Hsu and Y Ma Integrating classification and association rule mining In KDD'Y8 New York NY August 1998 8 B Liu Y Ma and K Wong Improving an association rule based classifier In PKDD'oO Lyon France Sept 2wO 191 J Ouinlan C4.5 ru~rarn forclossificarion learninz Mor   gan Kaufmann 1992 IO K Wang Y He D W Cheung and F Y L Chin Mining confident rules without SUDDO reauirement In CIKM'OI  Allanro GA November 2001 I I K Wang S Zhou and Y He Growing decision trees on 42 


a Computing view b query graph t f ivot method Figure 5 pivot method query graphs re\337ecting the produced columns of the select clause contributes with a value of pc 004 8 to the overall cost resulting in 14 004 pc 6 004 pt  5.2 Comparing different Methods The cost of computing the nearest neighbour using the t method may be split into the computation of the cost for the view de\336nition to perform the t of the prototype information and the core select statement using the t view As already mentioned the pt 212 1 ary join of the prototype table is easily optimized by the existence of an index of the prototype ID In this case each join partner has the size of 1 004 1 d  yielding pt 004 1 d  as the overall cost computing the view without any return operator The query itself joins the view with the PConformations table Reading the result of the view corresponds to one single tuple with pt 004 1  d  columns The access of the PConformations table has the size of pc 004 1  d   The outer query adds to the overall costs with reading the result of the inner query  pc 004 1  d  004  pt 1  and accessing the Prototypes table  pt 004 1  d   The return operator 336nally consumes a data stream of cardinality pc with d 3 attributes The cost of discretization using the self-join method can be computed in three steps The 336rst step considers the computation of the view P rototypeAssignment which requires the access to the two tables P roteinConf ormation  pc 004 1  d   and P rototypes  pt 004 1  d   The resulting data stream which has to be read for further processing yields due to the semantics of the Cartesian product to  pc 004 pt  004 1  d 2  The second step addresses the inner query consuming exactly the size of the view and producing a data stream of pc 004 2 for only two columns The outer query reads the result of the inner query and the result of the view   pc 004 pt  004 3  d   At last the return operator has costs the size of the join operator yielding pc 004  d 3  In a similar y the cost for the minpos  method can be computed Instead of the join with the P rototypeAssigment view in the outer query the P roteinConf ormation table with a cost of pc 004  d 1 a Computing view b query graph PrototypeAssignment of self join method Figure 6 self-join method query graphs Figure 7 cost reduction scenario is referenced Additionally the view must be executed only once further reducing the cution costs Table 1 summarizes the partial and total cost for all different methods computing the prototype for each amino acid residue Since this tabular and formular-based representation does not give any hint about the best strategy 336gure 7 gives a scenario with four different dimensions i.e d 1  4  7  10  and 5000 amino acid residues The scenario shows the resulting performance gain of the pivot method compared to minpos  and selfjoin method with 25 100 1000 and 2500 prototypes Within the proposed cost model the t method yields a slightly lower total cost than the minpos  method because the pt 212 1 ary selfjoin is considered extremely cheap r due to the dependency of the total cost from the number of prototypes the t method can not be considered a feasible solution for real applications with a reasonable high number of prototypes Compared to the self-join method the minpos  method yields a substantial cost reduction 6 Summary and Conclusion This paper introduces the problem of 336nding frequent substructures in protein data sets The analysis process is split into two parts The 336rst step consists in 336nding the Proceedings of the 15th International Conference on Sci entific and Statistical Database Management \(SSDBM\22203 1099-3371/03 $17.00 \251 2003 IEEE  


 t method view execution pt 001 1  d   f database operations inner query pt 001 1  d  pc 001 1  d   pt 1 joins outer query pc 001 1  d  001  pt 1 pt 001 1  d  pc 001  d 3 total cost pt 001 3  5 d  pc 001 5  3 d  pc 001 pt 001 1  d  self-join method view execution 2 001  pc 001 1  d  pt 001 1  d   f database operations inner query  pc 001 pt  001 3  d  1 join outer query pc 001 2 pc 001 pt  001 3  d  pc 001 3  d  2 cross product total cost pt 001 2  2 d  pc 001 7  3 d  pc 001 pt 001 6  2 d  1 group by minpos method view execution pc 001 1  d  pt 001 1  d   f database operations inner query  pc 001 pt  001 3  d  1 join outer query pc 001 2 pc 001 1  d  pc 001 3  d  1 cross product total cost pt 001 1  d  pc 001 7  3 d   pc 001 pt  001 3  d  1 group by Table 1 Cost comparison of the different methods nearest prototype in the multi-dimensional dihedral angle space To accomplish this task the data sets are brought into a relational schema and a method is proposed to compute the minimal distance considering the wrap-around effect in the angle space Three different methods to 336nd an associated prototype inside the database systems are compared A minimal SQL extension  minpos  maxpos  function results in much more ef\336cient query execution plans The second step of generating frequent item sets to detect frequent substructures within the amino acid sequences requires substantial SQL extension A ew operator as a new member of the OLAP grouping function operators is introduced This operator is a generic tool and may be ploited by a huge set of data mining applications To summarize a database system used to ef\336ciently analyse huge data volumes requires additional support from the technology  The required extension range from minimal UDFs like our proposed minpos  maxpos functions to more complex operators like our proposed grouping combinations operator f and only if the database community provides this kind of functionality the acceptance of database systems in the biotechnology community will increase in the near future References  R Agra w al T  Imielinski and A N Sw ami Mining association rules between sets of items in large databases In Proceedings of the International Conference on Management of Data  pages 207\320216 ACM Press 1993  S Bohl M Dink elack er  J  Griese and S Schrader  Highly adaptable amino acid side chain rotamer library in pdb coordinates In Workshop in Computational Biology at the Plant Biochemistry Department of the Albert-Ludwigs-Universitt Freiburg Germany  2002  M Bo wer  F  Cohen and R Dunbrack Homology modeling with a backbone-dependent rotamer library J Mol Biol  267:1268\3201282 1997  R Chandrasekaran and G Ramachandran Studies on the conformation of amino acids xi analysis of the observed side group conformations in proteins Int J Pept Prot Res  2:223\320233 1970  R Dunbrack and F  Cohen Bayesian statistical analysis of protein side-chain rotamer preferences Protein Sci  6:1661\320 1681 1997  J Gray  A  Bosw orth A Layman and H Pirahesh Data cube A relational aggregation operator generalizing groupby cross-tab and sub-total In Proceedings of the Twelfth International Conference on Data Engineering  pages 152\320 159 IEEE Computer Society 1996  A Hinneb ur g M Fischer  and F  Bahner  Finding frequent substructures in 3d-protein databases In Workshop on Bioinformatics at the 19th International Conference on Data Engineering  IEEE Computer Society 2003  M James and A Sielecki Structure and re\336nement of penicillo-pepsin at 1.8 a resolution J Mol Biol  125:299\320 361 1983  J K usze wski A Gronenborn and G Clore Impro ving the quality of nmr and crystallographic protein structures by means of conformational database potential derived from structure databases Protein Sci  5:1067\3201080 1996  S C Lo v ell J M W ord J S Richardson and D C Richardson The penultimate rotamer library Proteins Struct Funct Genet  40:389\320408 2000  M MCGre gor  S  Islam and M Sternber g Analysis of the relationship between side-chain conformation and secondary structure in globular proteins J Mol Biol  198:295\320310 1987  R Srikant and R Agra w al Mining generalized association rules In VLDB\32595 Proceedings of 21th International Conference on Very Large Data Bases Switzerland  pages 407\320 419 Morgan Kaufmann 1995  M J Zaki Ef 336cient enumeration of frequent sequences In Proceedings of the 1998 ACM CIKM International Conference on Information and Knowledge Management Bethesda Maryland USA November 3-7 1998  pages 68\32075 ACM 1998  M Zhang B Kao C L Y ip and D  W L Cheung Ffs an i/o-ef\336cient algorithm for mining frequent sequences In Knowledge Discovery and Data Mining PAKDD 2001 5th Paci\336c-Asia Conference Hong Kong China April 16-18 2001 Proceedings  volume 2035 of Lecture Notes in Computer Science  pages 294\320305 Springer 2001 Proceedings of the 15th International Conference on Sci entific and Statistical Database Management \(SSDBM\22203 1099-3371/03 $17.00 \251 2003 IEEE  


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


