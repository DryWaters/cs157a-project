The Application of Improved Apriori Algorithm in Continuous Speech Recognition Dexin Zhou, Jiacang Kang, Zhicheng Fan, Wenlin Zhang Aeronautical Automation College Civil Aviation University of China, CAUC Tianjin, China dxzhou@cauc.edu.cn   Abstract 227In continuous Speech Recognition ,for the problem of recognizing speaker's voice completely and accurately is difficult ,and could not understand the meaning even if the machine could identify the voice completely .The improved algorithm is proposed innovatively based on association logic 
Apriori algorithm .The algorithm divide database into correlated partitions and locate the voice condition in certain association rules specifically according to the character of input voice signal ,thus the efficiency of continuous voice identification and the humanistic of machine is improved ,the drawback of research only on acoustic model is broken Keywords-data mining; speech recognition;association algorithm; imprved apriori algorithm; association rules I  I NTRODUCTION  In recent three decades, speech recognition technology has 
significant progress, including DTW \(Dynamic Time Hidden Markov Model linear prediction residual principle, VQ \(Vector Quantization\ and other technical reference to the speech recognition field. The continuous speech recognition has been rapid development. a variety of algorithms for dealing with noise, voice range breakpoint detection, voice analysis parameter, extraction and matching. But this is only for the acoustic model, no link to the specific environment which include the speaker characteristics and professional knowledge, machine can not like people to divergent thinking based on the content of speech In the practical application of the database, data mining extract hidden information in them which people do not know 
but is useful. Data mining can highly automated analysis of existing data and make inductive reasoning. Data mining divided into problem definition, target description, data selection and preprocessing, model selection algorithm and interpretation of the results of assessment to strike. Data mining algorithm selection is the core of Data mining. All algorithms in data mining, association algorithm is one  , one of the role of the algorithm is to use what is known to predict other association things. Association algorithm is the most classic Apriori algorithm, the paper give the process of the classical Apriori algorith m and propose improved Apriori algorithm basing on the shortcomings of the classical 
algorithm. The improved algorithm used in speech recognition. Depend on data mining database in which There are common sense of people or the speaker's resume or experience, to draw some useful things in the set, according to the input voice information, finding the set related to speaker which enhance the machine to further understanding on the speaker and to improve continuous speech recognition accuracy. These sets to help the machine to recognize continuous speech II  A LGORITHM  A  Association Algorithm The algorithm is a response to the relationship between things and other things, if the relationship among things, it is 
known from one of the things th at could predict other things revealing the relationship among things is the core of the association algorithm. The algorithm is as follows 123    n Sssss 
003=\004  1 
  is the complete set 123    n K kk k k  is the item database AS 002   AKBSBK 
002\002\002 And AB 
 If the probability of supporting {A,B} in the item database entries is t%, then {A,B}\222s support in the item database K is t , all over the set\221s subset is bigger than a given set of minimum support is called frequent set 2  the set include all of the item A, the probability of{A B} in this set is bigger than n%, then the credibility degree of the frequent set {A,B} in item databases K is n Set all of the item is a subset of S Set x w Is a subset of the item database, according to the formula \(2\eek the set of meeting support degree 
  
 x pw t pK 
002\005\005  2 y w Is frequent set, according to the formula \(3\ the association rules of meeting credible degree CAUC2009ZD0102. MHRD201004 756 978-1-4244-9439-2/11/$26.00 \2512011 IEEE 
  1  x wK xm 


   y x pw n pw    yx wwyx 002  3  B  Apriori Algorithm   Apriori algorithm is a classical algorithm for extracting association rules .The way will be divided into two steps 1\In a item database, find all the item set whose the support is greater than t 2\asing on \(1\ , find the association rules whose credibility degree is greater than n Apriori algorithm is an iterative method, first find the item that contains a set of items denoted by p1, according to p1 to find the items which contains two items denoted as p2.Calculating in accordance with this rule until no frequent sets generated. in K times cycle, do \(k-2\ connection between two different sets which includes frequent k-1 items and only one item is different .this operation have produced a frequent set containing k items. When n is greater than 1 a program flow chart of the extraction of frequent item is shown in Figure 1 Database a I=0 start I=I+1 I k Meet support or no Put into a All the data in b are calculated or no n-1 n>1 end no Delete from a yes no no yes no yes yes no yes B make connection Put the data that is from a into  b Drop data  Figure 1.  Extraction procedure of including n frequent items set C  The improved Apriori algorithm Base on the fault of many same project when connecting program, making the following improvements.After getting a candidate set G1, which contains the transaction number and the total number of items, where using cross-table structure and its structure: node header \(support count, point belongs to the set of Services\, node \(item number, point to the next respective item otherwise empty\, and then collected count of items O2 \(item contains two items\rom the G1. By connecting Ok-1 and itself to generate candidate k item sets called Gk, according to the relevant concepts to obtain the count of Gk. the improved Algorithm is showed in Figure2 start Input database and support Create cross-table structure Create frequent se which contain one iterm Create k frequent set From k-1 frequent set K=2 K  frequent set=null Merge all the frequent set end yes no K+1  Figure 2. The flow chart of improved Apriori Algorithm Compared with Apriori algorithm, the improved Apriori algorithm improve the processing speed, shorten the time of the algorithm and mine more association rules .For mining database which contains large amounts of data and in a wide variety ways providing favorable conditions. This is helpful for speech recognition III  T HE APPLICATION OF DATA MINING IN SPEECH RECOGNITION  Continuous speech recognizing  purely rely on grammatical structure or recklessly understanding several words combinations, that is Only a blunt mechanical understanding In the natural community, how to understand each other's meaning by the exchange of people? Not only voice heard by the ears, but relationship between the contents of hear and their environment, or their knowledge or their experiences together For example, a dialogue among professionals, because not understanding of their area of expertise and not knowing the work of them, an outsider is difficult to understand what they mean. Therefore only under related background with the content of speech to understand the speaker to express content A  Association rules derived in speech recognition    There is a database, the database is to address people's lives that may be a specific aspect of emotional, may be sports and so on. And the elements of this database should be specific to a particular thing or something, and contains the full side of things as much as possible. Depending on the application to set support degree, using improved Apriori 757 


algorithm to generate frequent sets. According to the voice input signal and the credibility to found the association rules to meet the requirements. May sometimes get a lot of association rules , how to do Because people in a specific period of time during which the content what people say is a fixed thing or fixed events Therefore, according to the input signal and the next set of credibility, found the association rules match the first request and then find the intersection of this association Rules and association rules obtained last time to get association rules which close to the speaking environment. If the association rule can be sure, we can determine what the speaker say or discussed within the association rule in quite long time,. If the association rule can not be fixed, then still need the next set of voice signals and credibility. The process is shown in Figure 3  Figure 3.  The process of speech recognition derived association rules B  Application examples Once finding the association rules, in quite long time, a large probability of content of speech is within the rules of the association rules, even it is out of association rules. Association rules will be in the vicinity of the association rules For example, the first input voice signal is Jordan, Apriori algorithm based on association rules derived that there are the association rules O {Jordan, Adidas, Nike, ...} \(the rule is concerned with the sports brand\ and the association rules C Bulls, NBA , basketball, Michael Jordan ...} \(the rules relate sport\ ,C and O meet set credibility degree, and the second voice signal derive association rules C {Basketball Bulls NBA, basketball, Michael Jordan ...}, etc. But does not include O association rules ,.Intersected association rules which derived from the two input speech signal ,the result of this is removing the association rule O, left both contain the C association rules. when If the input speech signal properties is a bull, then the machine will look for rules in the relationship you will find with the Bulls in the C rules. The machine will know that this bull is basketball association, the thing is about the NBA , rather than grass, or food-association things IV  CONCLUSION    In this paper, the classical Apriori algorithm and its improved Apriori algorithm is elaborated, and using this algorithm to calculate association rules association to the specific environment, to same extent, simulate the natural communication between people in society environment. Help machines to understand the speaker, helps to improve the efficiency and continuous speech recognition accuracy However, the database based on association rule extraction is directly association to the comprehensiveness and effectiveness of the establishment of data warehouse will be the next focus of the study  A CKNOWLEDGMENT  Thanks to the support from Foundation item \(Research highlight in Civil Aviation University of China CAUC2009ZD0102\AAC science and technology foundation \(MHRD201004 R EFERENCES   1  Wang Yun, Su Tao, \223Association Rule Mining in Traffic Accident Analysis ,\224 Science Technology and Engineering ,2008.4:1824-1826 2  Su Xinning, Yang Jianlin,Jiang Niannan. \223Data warehouse and data mining,\224 Beijing: Tsinghua University Press, 2006 3  Han Jiawei, \223Kamber M. Data ming concepts and techniques,\224 Organ Kaufmann Pubilisher, 2001 4  Wang Anna, Wang Qinwan ,Liu Junfang ,\223 Improved Speech feature extraction method and its application,\224 Computer Engineering, 2008,34 5\96-200 5  Cheng an,cheng ning, \223Data Mining and Applications,\224Beijing:Science Press.2006 6  Lu Fei.Cheng Ming.Ge Wei 223 ARM-based speech recognition system design and implementation,\224 2008 \(9 7  Su Mingwu, \223DSP-based speech recognition technology research and implementation,\224 Harbin:Harbin Engineering University, 2005 8  Wang Bingxi, Qu Dan ,"Practical speech recognition based ,\224 Beijing Tsinghua University Press, 2003 758 


3422 


3423 


3424 


3425 


This discretization is commonly completed before mining 1]. So we can easily know that [0,20k], [21k, 30k] and the other intervals which belong to one dimension cant exist at the same time [4]. But there are still some special situations such as year which is parted by quarter level\(Q1,Q2,Q3,Q4 problem what about the sales in both quarter Q1 and Q2, so when we improve the algorithm we must think about it too First of all, it is obviously that when mining multi-dimension association rules, one predicate set contains no more than one different level from the same dimension [5] without special requirements of users Combine this point with Apriori_Cube algorithm can help reduce the amount of useless predicate set, especially with the great growing of the amount Example 1 There are 4 dimensions of the database: A, B, C D, when the discretization of them has been complete, we can built a table of them like Table 1 A B C D a1 b1 c1 d1 a2 b2 c2 d2 Table 1:Examp1       Graphic1: Connection of example 1 When the 3-itemset come out, itemset {a1, b1, b2 must be contained, according we discussed before it is obviously unsubstantiated. We will use one predicate set contains no more than one different level from the same dimension to do the first pruning, so that this condition will not appear ever Second, when users make special requirements, we also should do some improvements. Such as the example before, when the algorithm discovered that one predicate set contains different levels from the same dimension as {a1 b1, b2} and checked out that this is what the users are interested in, so we will calculate the sum of the counts of a1, b1} and {a1, b2}, then assign it to {a1, b1, b2} and keep it in the frequent 3-itemset. We call this situation SP The algorithm improved also has steps connection and pruning, but the difference is that we use one predicate set contains no more than one different level from the same dimension and the SP to do the first pruning in function gen_candidate \( Lk second pruning by using minsup 


o  a1, b1}, {a1, b2}, {a1, c1 a1, c2}, {a1, d1}, {a1, d2 a2, b1}, {a2, b2}, {a2, c1 a2, c2}, {a2, d1}, {a2, d2 2-itemset a1,b1,c1},{a1,b1,b2 a1,b1,d1},{a1,b1,c1 a1,b1,c2},{a1,b1,d1 a1,b1,c1},{a1,b1,c2 3-itemset computer 3-predicate set. We sign frequent predicate set in the same way of frequent itemset. Just like using Lk to represent the set of frequent k-predicate set, C k to represent the set of Candidate k-predicate set [1 In the former, there existed many algorithms by using the idea of Apriori and its several variations [5]. One of them can mine frequent predicate set from data cube, we call it Apriori_Cube. The difference between them is calculating the degree of predicate support instead of itemset. When mining multi-dimension association rules from data cube, one predicate set is the very set of dimension members from different dimensions in data cube d1  d n | count age-1, business-1, buys-1 the supporting number of predicate set is the count storage in the cell of data cube 2.3 The Formation Of Multi-Dimension Rules With the frequent predicate set having been found out, it is easy to pick up association rules [1 1 subsets 2 set, calculate the confident degree of rules s? \(I-s word, confidence = the confident number of I/the confident number s, if confidence ? PLQFRQI WKHQ WKH rules s? \(I-s 3. The Improve Of Apriori_Cube Algorithm 3.1 The Main Idea Of Improvements First using OLAP to simplify the data cube; second improving function gen_candidate \( Lk 


do 1. The dimensions and levels of mining task are determined by users requiring when data cube is built up. Sometimes it cannot certainly find out strong association rules or it may mine many rules which users arent interested in based on the levels of dimensions. Such as dimension area, when we talk about the problems of the world, the country level will be more useful than the province level. One solution of this situation is that adjusting the levels based to the amount and proximity of the rules which have been mined. But it means mining the data cube once again and while mining multi-dimension association rules, it also cant determine the dimensions which need to be adjusted accurately at the same time. So we think that analyzing the levels of dimensions at the same time of data mining, then using operations roll-up and drill-down of OLAP to adjust the levels, this is called On-Line Analysis Mining, at last carrying on mining process which will be more propriety and efficient Assume that users make an n-dimensions data cube and always hope the multi-dimension association rules mined can contain these n dimensions, on the contrary they arent interested in the rules containing only \(n-1 dimensions or even less [6]. In a word, it must find out the frequent n-predicate set. According to Apriori, every 1 subset of frequent n-predicate set must be frequent 1-set That means every dimension has a frequent 1-predicate set Obviously saying, once if some dimension doesnt contain a frequent 1-predicate set, the frequent n-predicate set must not exist So we check the frequent 1-predicate set after mining them from every dimension by algorithm: we can know that it shows the partition level of dimension we made is too low if some dimension doesnt have frequent 1-predicate set so we should raise the level of this dimension by roll-up; in the opposite, if every 1-predicate set of some dimension is frequent 1-predicate set, we should drill-down to drop the level of this dimension [3]. All of these adjustments will be embedded in mining process in order to strengthen the flexibility and targeting 2. In recent years, we also meet many problems like this when mining rules, most quantitative dimensions can be 


scattered, for example: dimension deposit can be parted in the form of interval as [0, 20K], [21k, 30k]and so on This discretization is commonly completed before mining 1]. So we can easily know that [0,20k], [21k, 30k] and the other intervals which belong to one dimension cant exist at the same time [4]. But there are still some special situations such as year which is parted by quarter level\(Q1,Q2,Q3,Q4 problem what about the sales in both quarter Q1 and Q2, so when we improve the algorithm we must think about it too First of all, it is obviously that when mining multi-dimension association rules, one predicate set contains no more than one different level from the same dimension [5] without special requirements of users Combine this point with Apriori_Cube algorithm can help reduce the amount of useless predicate set, especially with the great growing of the amount Example 1 There are 4 dimensions of the database: A, B, C D, when the discretization of them has been complete, we can built a table of them like Table 1 A B C D a1 b1 c1 d1 a2 b2 c2 d2 Table 1:Examp1       Graphic1: Connection of example 1 When the 3-itemset come out, itemset {a1, b1, b2 must be contained, according we discussed before it is obviously unsubstantiated. We will use one predicate set contains no more than one different level from the same dimension to do the first pruning, so that this condition will not appear ever Second, when users make special requirements, we also should do some improvements. Such as the example before, when the algorithm discovered that one predicate set contains different levels from the same dimension as {a1 b1, b2} and checked out that this is what the users are interested in, so we will calculate the sum of the counts of a1, b1} and {a1, b2}, then assign it to {a1, b1, b2} and keep it in the frequent 3-itemset. We call this situation SP The algorithm improved also has steps connection and pruning, but the difference is that we use one predicate set contains no more than one different level from the same dimension and the SP to do the first pruning in function 


gen_candidate \( Lk second pruning by using minsup o  B4 B3 B2 B1 To sum up the above arguments, we put forward the improved algorithm 3.2 Apriori_Cube_Improved Algorithm Algorithm: Apriori_Cube_Improved Input: N-Dimension Data Cube \( d1  d n | count support: minsup Output: The set of frequent predicate set L Process 1 2 i=1n up the candidate 1-predicate set and we use symbol C1 to represent the set of them 3 C1 4 1-predicate set of every one dimension 5 1 2 3 6 1 2 2 3 4 5 the new improvement of step 1 7 8 k ?Q\fDQG Lk ??\f'R 1 Lk 1 The new function will be expanded below 2 C k 3 4 9 New Function gen_candidate_improved \( Lk 1 1 2 


Lk 1 Do 1 there exists k-2 items in the same between I1 and I 2 a If there exits requirements of the users Then sup_count of C go to b b C k = C k ?c; Else delete c 3 Function gen_frequent \( C k 1 2 I1  I k come from the set do 1 n-dimension data cube\( I1  I k Support=sup_count/total_count 2 3 4. Performance Analysis Example 2 Lets talk about a practical problem just like the status of sales. Assume that we will mine the association rules involved 4 dimension attributes of sales, the minsup=25%. First of all, using OLAP technology to build a 4-D data cube and the 4 dimension attributes are: time location, item, and supplier. For location dimension which contains area, country and so on, we choose province level We use brand level for item dimension, company level for supplier dimension. Time dimension can be divided as Q1 1-3 4-6 7-9 10-12 location\(P1,P2,L1,L2 York, item\(B1,B2,B3,B4 C1,C2,C3 sales data cube can be generalized like this Graphic 2: The 4-Dimension Data Cube of Sales The details of this sales data cube are in the table follow: The amount of cells is 100 Location Time Item Supplier Count Cell-1 P1 Q1 B1 C2 5 Cell-2 P1 Q3 B1 C1 2 Cell-99 L1 Q3 B1 C1 3 Cell-100 L2 Q4 B1 C3 11 Table 2: The details data table of sales data cube We use original Apriori_Cube Algorithm to find 


frequent predicate set with minsup_num= 25%*100=25 According the data table we calculate that sup_count of every member of dimension L is \(P1:8, P2:5, L1:1, L2:24 and also T, I, S. So there is the process Graphic 3: The processes of old algorithm As we know, through comparing with the minsup_num dimension location has no one frequent 1-predicate set, so that there have no frequent 4-predicate set in the output by the original algorithm. But users are interested in the Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate set T I S Q1 Q3 B2 B3 C1 C2 I1 I 2 C1              C2                 C3 2 12 1 3 5 1 


6 2 12 8 11 Q 1  Q 2  Q 3  Q 4 P 1 P 2 L 1 L 2 Candidate 2-Predicate set Q1,B2},{Q1,B3 Q1,C1},{Q1,C2 Q3,B2},{Q3,B3 Q3,C1},{Q3,C2 Candidate 3-Predicate set Q1,B2,B3}{Q 1,Q3,B2},{Q3 C1,B2 Frequent 2-Predicate set Q1,B2}{Q1,B 3},{Q3,B2 Q3,C1 Output: 1L U 2L U 3L Frequent 3-Predicate set Q1,B2,C1},{Q1,B3,B2 Q1,B2,B3  B4 B3 B2 B1 To sum up the above arguments, we put forward the improved algorithm 3.2 Apriori_Cube_Improved Algorithm Algorithm: Apriori_Cube_Improved Input: N-Dimension Data Cube \( d1  d n | count support: minsup 


Output: The set of frequent predicate set L Process 1 2 i=1n up the candidate 1-predicate set and we use symbol C1 to represent the set of them 3 C1 4 1-predicate set of every one dimension 5 1 2 3 6 1 2 2 3 4 5 the new improvement of step 1 7 8 k ?Q\fDQG Lk ??\f'R 1 Lk 1 The new function will be expanded below 2 C k 3 4 9 New Function gen_candidate_improved \( Lk 1 1 2 Lk 1 Do 1 there exists k-2 items in the same between I1 and I 2 a If there exits requirements of the users Then sup_count of C go to b b C k = C k ?c; Else delete c 3 Function gen_frequent \( C k 1 2 I1  I k come from the set do 1 


n-dimension data cube\( I1  I k Support=sup_count/total_count 2 3 4. Performance Analysis Example 2 Lets talk about a practical problem just like the status of sales. Assume that we will mine the association rules involved 4 dimension attributes of sales, the minsup=25%. First of all, using OLAP technology to build a 4-D data cube and the 4 dimension attributes are: time location, item, and supplier. For location dimension which contains area, country and so on, we choose province level We use brand level for item dimension, company level for supplier dimension. Time dimension can be divided as Q1 1-3 4-6 7-9 10-12 location\(P1,P2,L1,L2 York, item\(B1,B2,B3,B4 C1,C2,C3 sales data cube can be generalized like this Graphic 2: The 4-Dimension Data Cube of Sales The details of this sales data cube are in the table follow: The amount of cells is 100 Location Time Item Supplier Count Cell-1 P1 Q1 B1 C2 5 Cell-2 P1 Q3 B1 C1 2 Cell-99 L1 Q3 B1 C1 3 Cell-100 L2 Q4 B1 C3 11 Table 2: The details data table of sales data cube We use original Apriori_Cube Algorithm to find frequent predicate set with minsup_num= 25%*100=25 According the data table we calculate that sup_count of every member of dimension L is \(P1:8, P2:5, L1:1, L2:24 and also T, I, S. So there is the process Graphic 3: The processes of old algorithm As we know, through comparing with the minsup_num dimension location has no one frequent 1-predicate set, so that there have no frequent 4-predicate set in the output by the original algorithm. But users are interested in the Candidate 1-Predicate set L T I S P1 P2 L1 


L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate set T I S Q1 Q3 B2 B3 C1 C2 I1 I 2 C1              C2                 C3 2 12 1 3 5 1 6 2 12 8 11 Q 1  Q 2  Q 3  Q 4 P 1 P 2 L 1 L 2 Candidate 2-Predicate set Q1,B2},{Q1,B3 Q1,C1},{Q1,C2 Q3,B2},{Q3,B3 


Q3,C1},{Q3,C2 Candidate 3-Predicate set Q1,B2,B3}{Q 1,Q3,B2},{Q3 C1,B2 Frequent 2-Predicate set Q1,B2}{Q1,B 3},{Q3,B2 Q3,C1 Output: 1L U 2L U 3L Frequent 3-Predicate set Q1,B2,C1},{Q1,B3,B2 Q1,B2,B3  6 level from the same dimension and SP to pruning Candidate 3 Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special requirement on location B by users 7 1 max_support of every dimension 1 frequent 4-predicate set which contains location dimension And the candidate 3-predicate set {Q1,Q3,B2} is useless because when we mine multi-dimension association rules we should follow that one predicate set contains no more than one different level from the same dimension even the sup_count >minsup_num when there are no requirements from users. But this time, users require that different members from dimension item can exist at the same time So we redo this mining process by the improved Algorithm Graphic 4: The processes of improved algorithm At last, we got the frequent 4-predicate set which the users are interested in, and then can format the rules 5. Conclusion In this paper, we have studied issues and methods on efficient mining of multi-dimension association rules based on data cube. With the development of technology, the size 


of database has become much huge than ever before unimaginable. So the multi-dimension model of database based on data cube has become useful and popular relatively how to mine useful multi-dimension association rules based on this model has been important too. Among the algorithms for the problems we choose the most classify one to improve, to make it more efficient and useful An efficient algorithm, Apriori_Cube_Improved, has been developed which explores the multi-dimension association rules. First, we use max_support and min_support of every frequent 1-predicate set to check the levels of dimensions, and then adjust the data cube by roll-up and drill-down operation immediately. This step is embed in the process of mining association rules, so it makes the rules much more useful and flexible; second applying one predicate set contains no more than one different level from the same dimension and SP pruning can help reduce the amount of predicate set, especially with the great growing of the number. So the speed of algorithm improved is faster than ever At last, there are also many other interesting issues and flaws of the algorithm which call for further study including efficient mining multi-dimension association rules of complex measures and so on. Finally, I should thank all the people who have give me help for this paper Reference 1] Jiawei Han, Micheline Kamber. Data Mining Concepts and Techniques. China Machine Press, 2007 2] Agrawal R, Imielinski T and Swami A. Mining association rules between sets of items in large database. Proc. of the ACM SIGMOD Conf., Washington DC, 1993 3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional Association Rule Mining Method Based on Data Cube Computer Engineering, China,2003 4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research Multi-dimensional Association Rule Ming Based on Apriori Science Technology and Engineering, China, 2009 5] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proc. of the 20th Int.Conf. on VLDB Santiago, Chile, 1994 6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang Mining Multi-Dimension Constrained Gradients in Data 


Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy 2001 7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE Transactions on Knowledge and Data Engineering, 2000 Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Candidate 1-Predicate set L T I S P L Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate L T I S P L 


Q1 Q3 B2 B3 C1 C2 Frequent 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3 Output L1 U L2 U L3 U L4 The Dimension need to be adjust Dimension: Location Operation: Roll-up to increase the level of this dimension Result P\(P1,P2 L1,L2 2 3 2 3 The Features Of Every 1-Predicate Set Location: min_support=13 max_support=25 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 4 5 4 max_support of every dimension 5 minsup_num=25, and no one has to be adjusted Frequent 3-Predicate set Q3,C1,B2},{Q1,B2,L Q1,B3,L},{Q3,P,B2 Candidate 4-Predicate set 


Q3,C1,B2,P},{Q1,B2,B3,L}\(7 The Features of Every 1-Predicate Set Location: min_support=1 max_support=24 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 Candidate 2-Predicate set Q1,B2},{Q1,B3},{Q1,C1 Q1,C2},{Q3,B2},{Q3,B3 Q3,C1},{Q3,C2},{P,Q1 P,Q3},{P,B2},{P,B3},{P,C1 P,C2},{L,Q1},{L,Q3 L,B2},{L,B3},{L,C1},{L,C2 Frequent 2-Predicate set Q1,B2}{Q1,B3},{Q3,B2 Q3,C1},{P,Q3},{L,Q1 6 Candidate 3 -Predicate set Q3,C1,B2},{Q3,C1,P},{Q1,B 2,L},{Q1,B3,L},{Q3,P,B2},{Q 1,B2,B3  6 level from the same dimension and SP to pruning Candidate 3 Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special requirement on location B by users 7 1 max_support of every dimension 1 frequent 4-predicate set which contains location dimension And the candidate 3-predicate set {Q1,Q3,B2} is useless because when we mine multi-dimension association rules we should follow that one predicate set contains no more than one different level from the same dimension even the sup_count >minsup_num when there are no requirements from users. But this time, users require that different 


members from dimension item can exist at the same time So we redo this mining process by the improved Algorithm Graphic 4: The processes of improved algorithm At last, we got the frequent 4-predicate set which the users are interested in, and then can format the rules 5. Conclusion In this paper, we have studied issues and methods on efficient mining of multi-dimension association rules based on data cube. With the development of technology, the size of database has become much huge than ever before unimaginable. So the multi-dimension model of database based on data cube has become useful and popular relatively how to mine useful multi-dimension association rules based on this model has been important too. Among the algorithms for the problems we choose the most classify one to improve, to make it more efficient and useful An efficient algorithm, Apriori_Cube_Improved, has been developed which explores the multi-dimension association rules. First, we use max_support and min_support of every frequent 1-predicate set to check the levels of dimensions, and then adjust the data cube by roll-up and drill-down operation immediately. This step is embed in the process of mining association rules, so it makes the rules much more useful and flexible; second applying one predicate set contains no more than one different level from the same dimension and SP pruning can help reduce the amount of predicate set, especially with the great growing of the number. So the speed of algorithm improved is faster than ever At last, there are also many other interesting issues and flaws of the algorithm which call for further study including efficient mining multi-dimension association rules of complex measures and so on. Finally, I should thank all the people who have give me help for this paper Reference 1] Jiawei Han, Micheline Kamber. Data Mining Concepts and Techniques. China Machine Press, 2007 2] Agrawal R, Imielinski T and Swami A. Mining association rules between sets of items in large database. Proc. of the ACM SIGMOD Conf., Washington DC, 1993 3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional Association Rule Mining Method Based on Data Cube 


Computer Engineering, China,2003 4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research Multi-dimensional Association Rule Ming Based on Apriori Science Technology and Engineering, China, 2009 5] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proc. of the 20th Int.Conf. on VLDB Santiago, Chile, 1994 6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang Mining Multi-Dimension Constrained Gradients in Data Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy 2001 7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE Transactions on Knowledge and Data Engineering, 2000 Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Candidate 1-Predicate set L T I S P L Q1 Q2 Q3 Q4 B1 B2 


B3 B4 C1 C2 C3 Frequent 1-Predicate L T I S P L Q1 Q3 B2 B3 C1 C2 Frequent 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3 Output L1 U L2 U L3 U L4 The Dimension need to be adjust Dimension: Location Operation: Roll-up to increase the level of this dimension Result P\(P1,P2 L1,L2 2 3 2 3 The Features Of Every 1-Predicate Set Location: min_support=13 max_support=25 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 4 5 


4 max_support of every dimension 5 minsup_num=25, and no one has to be adjusted Frequent 3-Predicate set Q3,C1,B2},{Q1,B2,L Q1,B3,L},{Q3,P,B2 Candidate 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3,L}\(7 The Features of Every 1-Predicate Set Location: min_support=1 max_support=24 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 Candidate 2-Predicate set Q1,B2},{Q1,B3},{Q1,C1 Q1,C2},{Q3,B2},{Q3,B3 Q3,C1},{Q3,C2},{P,Q1 P,Q3},{P,B2},{P,B3},{P,C1 P,C2},{L,Q1},{L,Q3 L,B2},{L,B3},{L,C1},{L,C2 Frequent 2-Predicate set Q1,B2}{Q1,B3},{Q3,B2 Q3,C1},{P,Q3},{L,Q1 6 Candidate 3 -Predicate set Q3,C1,B2},{Q3,C1,P},{Q1,B 2,L},{Q1,B3,L},{Q3,P,B2},{Q 1,B2,B3  


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


