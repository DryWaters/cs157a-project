SmartMiner: A Depth First Algorithm Guided by Tail Information for Mining Maximal Frequent Itemsets Qinghua Zou Wesley W Chu Baojing Lu Computer Science Department Computer Science Department Computer Science Department University of California-LA University of California-LA North Dakota State Universiw zou@cs ucla. edu wwc@s.ucla edu baojing. lu@ndsu noduk edu Abstract Maximal frequent itemsets MFI are crucial to many tasks in data mining Since the MaxMiner algorithmfirst introduced enumeration trees for mining MFI in 1998 several methods 
have been proposed to use depth first search to improve performance To further improve the performance of mining MF we proposed a technique that takes advantage of the information gathered from previous steps to discover new MFI More speci$cally our algorithm called SmartMiner gathers and passes tail information and uses a heuristic select function which uses the tail information to select the next node to explore Compared with Mafia and GenMax SmartMiner generates a smaller search tree 
requires a smaller number of support counting and does not require superset checking Using the datasets Mushroom and Connect, our experimental study reveals that SmartMiner generates the same MFI as Mafia and GenMax, but yields an order of magnitude improvement in speed 1 Introduction Mining frequent itemsets in large datasets is an important problem since it enables essential data mining tasks such as discovering association rules data correlations sequential patterns etc Let I be a set of items and D be a set of transactions where a transaction is an itemset 
The support of an itemset is the number of transactions containing the itemset An itemset is frequent if its support is at least a user specified minimum support value minSup Let FI denote the set of all frequent itemsets An itemset is closed if there is no superset that has the same support Let FCI be the set of all frequent closed itemsets A frequent itemset is called maximal if it is not a subset of any other frequent itemset We denote MFI as the set of all maximal frequent itemsets Any maximal frequent itemset X is 
a frequent closed itemset since no nontrivial superset ofX is frequent Thus we have MFIcFCILFI There are three different approaches for generating FI First the candidate set generate-and-test approach I 1,14,8,12,7 most previous algorithms belong to this group The basic idea is to generate and then test the candidate set This process is repeated in a bottom up fashion until no candidate set can be formed Second, the sampling approach it selects samples of a dataset to form the candidate set The candidate set is tested in the entire dataset to identify frequent itemsets Sampling reduces computation complexity but yelds incomplete result Third data 
transformation approach it transforms a dataset for efficient mining For example the FP-tree builds up a compressed data representation called FP-tree from a dataset and then mines FI directly from the FP-tree Another example is the pattern decomposition algorithm PDA 16,17 which decomposes transactions and shrinks the dataset in each pass When the frequent patterns are long mining FI is infeasible because of the exponential number of frequent patterns Thus algorithms mining FCI 9,15,10 are proposed since FCI is enough to generate association rules However FCI could also be as large as the FI As 
a result researchers now turn to find MFI Given the set of MFI it is easy to analyze many interesting properties of the dataset such as the longest pattern the overlap of the MFI etc Moreover we can focus on part of the MFI via supervised data mining  i CreatingBz before i,-/exploring B  I iCreating Wafter i.;./exploring B   a Dynamic reordering b SmartMiner Strategy  Figure I SmartMiner takes advantages of the information from previous steps 
The current MFI mining uses depth first search with dynamic reordering as in Mafia[l and GenMax[5 Those methods are significantly faster than previous approaches However they do not use the information from previous steps for exploring next nodes and requires the traverse of a larger search space than necessary As shown in Figure 1 the dynamic reordering technique creates a set of sub nodes B,-B before exploring B In contrast SmartMiner takes full advantage of the information from previous steps and explores B before selecting the node B 0-7655-1754-4102 17.00 Q 2002 IEEE 570 


Using tail information has many benefits it does not require superset checking reduces the computation for counting support, and yields a small search tree In this paper we first discussed the limitations of current MFI algorithm then introduced the partition and pruning properties used in the SmartMiner The SmartMiner strategy and implementation are then presented Finally the experimental performance comparison of SmartMiner with Mafia and GenMax are included 1.1 Related works We first introduce an enumeration tree for an itemset 1 Assume there is a total ordering over I which can be used to enumerate the search space. Each node has a head and a tail representing a state. The head is a candidate while the tail contains items to form new heads For example Figure 2 shows a complete enumeration tree over five items abcde with the ordering a.b,c.d.e Each node is written as head:tail For an item ai in the tail of a node X;Y a sub-node is created with Xai as its head and the items after ai in Y as its tail For instance the head of abcde is empty and its tail is abcde abcde a:bcdr k<u-du  ab:ede acde ade ae bcde M:e be cde ce de abcde aM:e abe acde ace j ade:j bcde bce i Me cde abde i ahccabde acde abcde z  pg=p  y   w Cut  b.~d.e I  Figure 2 An enumeration tree for abcde for the given order of a b c d e The problem of mining frequent itemsets is to find a cut through this lattice such that all itemsets above the cut are frequent and those helow the cut are infrequent see Figure 2 14 Using the enumeration tree as shown in Figure 2 we can describe recent approaches to the problem of mining MFI MaxMiner 3 uses a breadth-first search and performs look-ahead pruning which prunes a whole tree if the head and tail together is frequent MaxMiner also first uses dynamic reordering which reorder the tail items in the increasing order of their supports In general, however superset pruning works better with a depth-first approach 2 since many long frequent itemsets may already have been discovered But MaxMiner uses a breadth-first approach to limit the number of passes over the database Since large main memory size is available in Gigabytes current MFI mining uses depth first search to improve performance to find long patterns  uses depth first search on a lexicographic tree of itemsets to find MFI and projects transactions database on the current node to speed up the counting for support Depthproject also uses look-ahead pruning and dynamic reordering With dynamic reordering infrequent item at the current node can be deleted from the tail so that the size of the search space can be greatly reduced Mafia proposes parent equivalence pruning PEP and differentiates superset pruning into two classes FHUT and HUTMFI For a given node X:aY the idea of PEP is that if sup\(Xj=sup\(Xaj i.e every transaction containing X also contains the item a then the node can simply he replaced by Xa:Y The FHUT is to use the leftmost tree to prune its sister if the entire tree with root XatY is frequent then we do not need to explore the sisters of the node Xa:Y The HUTMFI is to use the known MFI set to prune a node. That is if an itemset ofXaY is subsumed by some itemset in the MFI set, the node Xa:Y can be pruned Mafia also uses dynamic reordering The results show that PEP has the largest effect of the above pruning methods PEP FHUT and HUTMFI and dynamically reordering also has significant savings in computation Both Depthproject and Mafia mine a superset of the MFI, and require a post-pruning to eliminate non-maximal patterns SI GenMax 5 integrates pruning with mining and returns the exact First just like the transaction database is projected on the current node, the known MFI can also be projected on the node and thus yields fast superset checking Second GenMax uses Diffset propagation to perform fast frequency computation Experimental results show that GenMax has comparable performance with Mafia 1.2 Limitations of previous approaches The algorithms discussed above do not take full advantage of previous searching. Let us use Mafia as an example to illustrate that limitations exist in previous approaches For the example in Figure 2 Mafia will generate a search tree as in Figure 3 assuming that frequent itemsets have different support and the nodes are already'sorted in the order of increasing support In the figure the shaded nodes will be removed by superset pruning The node abcde in the dotted box is not in the search The nodes with crossing lines are tested and found to be infrequent ahcde xbcde 6  c:de de e  abcde ac:de ade ae bcde bde be cde ce de 1 Y abc:de wy abde abea y    _ abcda i ab  e 86$b I    cut l Figure 3 The search tree for Mifia First the size of the tree is too large and can be reduced Although the shaded nodes can be pruned away 571 


a more efficient strategy is not to generate those nodes in the search tree As shown in Figure 3 Mafia traverses 31 nodes SmartMiner uses such a strategy and traverses only 9 nodes \(see section 3.2 for the same example Second there is too much counting for determining the frequency of tail items Figure 4 shows the counting tree for Figure 2 Let X be an itemset and T\(X be the set of transactions that contains X For the root node at the top level the transaction set is T\(4 since the head of the node is empty 4 For the node the supports of a,b,c,d,e are counted and found to be above minsup In the transaction set T\(a we found b.c,d,e frequent Likewise, items c,d,e are frequent in T\(ab and item d is frequent and e infrequent in T\(abc Mafia requires a total of 30 frequency tests Using tail information and a heuristic select function, SmartMiner needs only 23 such tests a-bLh7e hC I I b dec dedeu c d e d e+d e d U I  I Figure 4 The Mafia counting tree Finally all previous approaches require superset checking for two purposes pruning nodes and removing non-maximal itemsets in MFI If the set of MFI is large as in most real datasets superset checking can be very expensive In the above example Mafia performs 30 superset checks As will be discussed later SmartMiner does not require any superset checking 2 Partition and Pruning Properties 2.1 Partitioning a search space Let N=XY be a node where Xis the head of N and Y is the tail of N The power set of Y is the set of all possible subsets of K denoted by P Definition 1 For a node N=X:Y the set of all itemsets obtained by concatenating X with the members in P\(Y is called the search space of N denoted as XY That is XY   XUYl YEP For example the search space b:cd includes four itemsets b bc bd and bcd Likewise the search space abcde includes all subsets of abcde By definition 1 we have X;Y}={XZ where Z=Y-X Thus we will assume the X and Y share no item when X:Y is used in this paper Definition 2 Let S SI and SI he search spaces The set SI SI is apartition of S if and only if S SI U S and SlnS The relationship is denoted by S=Sl+S2 or SI S-S or S SS We say S is partitioned into SI and S Similarly a set SI SI  S is a partition of S if and only if S SI u S U  US and Sin q for i,;\200 I..k and it We denote it asS=Sl+S2  S Let a be an item then is an itemset by adding a to X Theorem 1 For aP X Y the search space of X:aY can he partitioned into Xa:Y and X:Y by item a that is Xa Y Xa Y X U It follows from the fact that each itemset of X:aY either contains a or does not contain a Proof For example we have b:ad In general al,a2  ak can he distinct items and Theorem 2 Partition search space the search space of ala  akY forman itemset X ala  akY can he partitioned into x x{Xai  a  a Y X  Y where a,rX Y Proof It follows by partitioning the search space via items al.a  at sequentially as in theorem 1 For example we have b:cd and  obcde  ab:cde\+\(ac:de  Let X:Y he a search space and Z be a known frequent itemset Since Z is frequent all subsets of Z will be frequent i.e every itemset of Z is frequent Theorem 3 Pruning search space if Z does not contain the head X the space XY can not he pruned by Z i.e X;Y}-{:Z Otherwise the space can be pruned as I k xyi-{:zi C{xai  u8  a Y n z  o,o~...ok=~-z i=l Pro08 If Z does not contain X no itemset in X Y is subsumed by Z Therefore knowing Z is frequent can not prune away any part of the search space X:Y Otherwise Xis a subset of Z Thus we have X:Y a  a,V}+X V,m,here Y=YnZ The head in the first part is Xai Since Z does not contain ai the first part can not he pruned For the second part we have XV z-X Since XnY we have YGZ-X Therefore X:V can be pruned away entirely For example we have  bcde abcd\:bcde  bcd e:bcd Likewise e:bcd}-{ :abe be  e:bcd    ectbd ed:b 2.2 Evaluating Tail Information Definition 3 Let M he the known frequent itemsets and N=XY be a node The tail information of M for N Tlnf NIM is the tail parts of the frequent itemsets in XY that can be obtained from M i.e k i.1 T'nf\(N I M  Y nZ I VZG M,X c Z For example TInf\(e:bcd/{abcd,abe,ace b,c which means that eh and er are frequent given abdabe ace is frequent Likewise lnf\(e:bcd/{abcd.abe,ace bce  b,c,bc For simplicity we refer to tail information as information 572 


Definition 4 Let W be tail information and Z be a The value of tail information W is the member of W union of the power set of 2 That is VTI  U Pp ZE W For example VTl\(\(b,c,bc 4 b,c.bc bcJ Notice that removing non-maximal itemsets from tail information does not decrease its value Therefore a non maximal itemset in the information set can he deleted 3 The SmartMiner 3.1 Information guided depth-first search Since the tail of a node contains many infrequent items pure depth-first search is inefficient Hence current approache uses dynamic reordering to prune away infrequent items from the tail of a node before exploring its sub nodes Input Output Transaction set T=T\(X A UpdaledGlnf Tail of the nade S=Y Global information Figure 5 Search strategy at the node Ni=X:Y In contrast, SmartMiner uses tail information to guide depth-first search to improve search efficiency We illustrate the strategy for a given node N;=X Y as shown in Figure 5 The purpose of the node Ni=XY is to compute maximal frequent itemsets in the transaction set T\(X The input for node N,=X:Y is transaction set T\(X the tail Y and the tail information for N known so far Gini is called global tail information for node N The output of the node is the updated Gin and discovered maximal frequent itemsets Mfi Upon calling the node N we count the supports for the items in the tail Y Yo is obtained by removing infrequent items from Y The time sequence at node N in Figure 5 is tbti  tn At the moment to item a is selected from Yo to be the head of next state Si and Y Y asis the tail of SI The tail information In is computed by Inf\(aO:Y IGlnfi We then create node Ni+,=Xa,:YI The call for node Ni+l returns Mfin and updated In in which the members survive in the node Ni are returned i.e those subsumed by Mfo are deleted no superset checking At ti we calculate the tail information In for Yl from In infio and Mfo The information from Inf and Infli-n is updated global information The information from Mfio is local information Using information In item a is selected from Yl to be the head of the next state Si and Yz U a is the tail of Si Then node Nj+2=Xaz:Yz is created and called to compute maximal frequent itemsets in transaction sets t\(Xa 3 This process continues untill In where no item can be selected as head of SI The returned maximal frequent itemsets Mf U a Mfit iE O..n-I the updated Glnfare these itemsets in the original GInfwhich have not been marked as deleted SmartMiner uses tail information to guide the depth first search which is different from dynamic reordering depth-first strategies DFS First SmartMiner defers creating a node untill its preceding nodes are visited while DFS creates nodes for each item in the tail of a node in the increasing order of their supports DFS creates as many sub trees as the number of frequent items in the tail Second SmartMiner uses a heuristic select function with consideration of the tail information and the frequency about each item see section 4.3 Using this heuristic SmartMiner creates far fewer sub trees than dynamic reordering Finally by passing tail information SmartMiner does not require the time for superset checking that is required for DFS 3.2 An example We now use an example to illustrate how SmartMiner finds the same MFI for the example in Figure 3 As shown in Figure 6 there are nine nodes No NI   Ns in the search tree For a given node the columns to 11  t represent sequential time points The row Sn represents the initial state and the Info is the tail information for So The row SI is the next state to explore and the relevant information is on the row In Note here Inf also called the global information as input for the next state and will be updated The row Mfi is the returned mfi after exploring the state Si On top of each node we give the transaction set for the node For example the transaction set for No is the entire dataset T the transaction set for NI is T\(a which represents all the transactions containing itema SmartMiner begins at the node No at to No\(td where Sn=:abcde and Info is empty At this point item a is selected and thus the next state Sl=a:bcde Here Infi is empty since Info is empty Next SmartMiner creates the node NI for the state SI=a:bcde by setting its transaction set T\(a and its initial set So=:bcde When SmartMiner calls the new node NI each item in the tail So=:bcde will be sorted in the increasing order of their support in T\(a and the infrequent items will be dropped The process continues to N and then to N3\(td where de and e is dropped since it is infrequent in T\(abc This yields So=:d SmanMiner returns d as mfi to N&j which will be added into Info at N,\(tJ Thus at N In d SmartMiner then selects Sl=e:d for the next node N 573 


Figure 7 shows the tree for counting support using SmartMiner At node No SmartMiner counts the supports for a.b,c,d.e and finds they are frequent At node NI items b.c,d,e are found to be frequent in T\(a It is shown that there are a total of 23 times to count for support 4 Implementation of SmartMiner 4.1 Object model design Our data mining system is implemented in Java rather than C because Java has better portability Figure 8 shows the three classes in our system whose data types are specified using Java language The class VDatu is the vertical data model for a transaction dataset It loads data from a given fileNume and builds up a BitSet for each frequent item The Tlnf class manages the tail information for a given node The Miner class uses the proposed tail information based on depth-first search to recursively discover all MFI An instance of Miner has exactly one object of VData and will dynamically create one object of Tlnffor a node when the mining starts More details are given in the following sections Input MR=obcd,obe,oce,ebc.ed  VData data BitSa  mi"sJp in1  VData\(String tileName float rm"S"P  tan\(%orts lail   ealSup\(int bare, Shorts tail   etBardim base shon itm   uilSup\(int base shon item  loadDala\(S1ring fileName Info nil lnf nil Mfi d Tlnf  t snledShotts  mfi Vector tail Sons  infs Hashtable pep short Tlnf\(Vcclor@nf,shotl pep Shons tail  Addlnfdvector newLn0:void  AddMfilnf\(Venormfi1  Doltan\(short itm  rc1cctO:shan  maxlenO:shon   tAnobe Info nil l"f nil Mfi M"er Data VData mfi:vector 1  j  min\(Stingargv void  i~er\(strin~fii noat minsup  minin void  inM fi\(inl base Sholls tails Venor no Venor  output0:void I  jT=T\(d S=e:bed   Glnf=b c I t butput  1 Glnf e  1 Mfi=be d  Figure 6 An SmartMiner Example The entire search route will be No N&J N2\(td Ndtd Nz\(f3 Ndd Nz\(t.d Ndtd Ndtd NV3 No\(t3 N N,\(t,J N6\(tl N Nti\(tJ and No@J As shown in the figure 6 at Nl,\(tl Info=bcd,be,ce Sl=e:bcd and the two itemsets be,ce contain e By removing e from be,ce we get Inf,=b.c When calling No global information Ginf=b,c is passed from NOPI to N Upon completing exploring the node Nti bc,d are found to be mfi and Ginf=b,c will be updated to be empty since they are dropped respectively at Nnlta to No\(tl and at Nti\(tl to N6\(tJ When it returns from N6 the Inf at No will be empty By collecting Mfi Inf and unselected In at No\(rl we have Inf,=bcd,bc,d at No\(tJ The search terminates at Nn@J since the tail of So=:bcd is in the Inf b cd bcd cde c4 d bs m R In hl d Figure 7 The counting tree for SmartMiner 574 


in methods caCSup and getBase is an array of transaction IDS The base of a node represents the transaction set T\(X where Xis the head of the node The private method calSup\(int base short item is to calculate the support of the item in the given base The VData provides three methods for data mining First the method getSfart\(Shorts tail returns the set of items that occur in every transaction It also passes other items by Shorts tail in the order of increasing support The getstart is called at a root node Second the public method calSup\(int base Shorts tail the set of items in every transaction of the base and passes other frequent items at the base in the order of increasing support Finally the method  base short item simply returns a new base which is the subset of the base whose corresponding transactions contain the item Note that when calculating support of an item in a base the VData needs to test as many bits as the size of the base It is slower than the Bitmap model where supports can be calculated a byte 8 bits at a time Our VData model is also slower than the difset model of GenMax  However the VData keeps only one copy of data and thus needs less memory than the other two models In other words both Mafia and GenMax need to build up new datasets for the mining of sub nodes Moreover the VData is easy to implement and is fair to use as a common data model to compare different search strategies of SmartMiner MaJk and CenMax 4.3 Tail information class: TInf For a given node an instance of the Tinf class is created to manage the tail information The ginf is passed from its parent node and the mfi is the local maximal frequent itemsets for the node The itemsets to be explored is stored in the tail whose information is stored in the hash table inf The pep is the set of items occumng in every transaction of the node The constructor method accepts ginf pep and rail to create a new instance The public methods Addlnfo and AddMfilnf calculate relevant information of the newinj and the mfil on tail respectively and then hash them into the hash table infs The method Doltem separates the members in the inf into two groups one mentions the item another does not The first group will he removed from the hash table and returned as a vector after dropping the item from every itemset The second group remains in the table The method also removes the item from the tail For every item in the tail the private method maxLen is to find the maximal length of itemsets in inf that contains the item. Note that in our experiment we use a simplified marLen that returns an array of value either 0 or the maximal length More specifically the maxLen first finds the longest itemset V in the infs and then set the lengths of items in Vto Iy and the lengths of others to 0 I  Select an item to build a sub node  return O if success 1 if no next items I public short select0 1 if ltail.sizeO<=l 2 if tail in infs then mfi=null else mfi=tail 3 return 1 4 short11 len  maxLen0 5 find the min nax psition mi  in len 6 if llen[maxpl==tail.sizeO 7 u-te the ginf info 8 return 1 9 return tail.get\(minp Figure 9 The selection method a heuristic to select an item for oartitionino the search soace Figure 9 describes the heuristic method to select an item to partition the search space In dynamic reordering the item of the least support is chosen to explore first Such heuristic has been shown to be effective Our heuristic select function considers both tail information and the supports The observation is that if an item contained by an itemset of size k in the inf there are 2 itemsets that are known to he frequent and can be pruned away from the search space Therefore our heuristic chooses an item of the smallest known space If the size of current tail is less than 2 the search space is immediately solvable as shown in line 1-3 Line 4 calls the method maLen Line 5 is to find the positions of the minimal and maximal values in fen Note that if several items have the minimal value we will choose the one having the least support If an itemset in the inf has the size of the tail this means the whole search space of the tail is frequent and thus there is no need to build a sub node as shown in lines 6-8 other itemsets in the infs are non-maximal and those originated from ginf will be deleted Note that no superset checking is used to eliminate non-maximal itemsets Line 9 returns the selected item 4.4 Data mining class: Miner The Miner class has two attributes and five methods as shown in Figure 8 The vData stores transaction data in vertical format The mfi is a vector of maximal frequent itemsets The main reads filename and minSup from command line and calls Miner mining and output sequentially The Miner initializes vData and the output stores the mfi into a file The mining method is to mine the vData Now we present the information guided depth first algorithm which returns local MFI as in Figure 10 The parameter base is the transaction set for the node head The globe information ginf will he updated upon return Line I calls vData.calSup to get the pep and an updated tails sorted in the increasing order of supports Line 2 creates an instance of the Information class for this node 575 


Lines 3-9 loop selects an item for the next node and solve it More specifically it selects an item ifm for next node as show in line 3 If there is no node selected it goes to line 10 Otherwise it enters the loop body A new base is calculated at line 4 the infDoIfem method is called and the new-tail is set Then line 7 solves the sub node Upon returning from the sub node it adds the updated newsinfinto the infat line 8 and also saves the new-mf by method AddMf;/nf at line 9 It returns the mfi of the node at line IO I  Recursively find mfi  param base The tidSet for Current head  param tail The possible extension of the head  Bparam ginf The global information return The local maximal frequent itemsets I private Vector infMfi\(int[l base Shorts tails Vector ginfl 1  pep  voata.calSuglbase.tails 2 TInf inf  new TInfrainf oeo tails  3 while\(\(itm=inf.selectill~=Ol 4 int[l newbase  vOata.getBaSeiba5e.itml 5 Vector newginf=inf.DoItem\(itml 6 Shorts newtail=new Shorts\(inf.tail1 7 Vector newmfi=infMfiinewbase,newtail,newginfl 8 inf AddInfa\(newginfl  9 inf AddMfiInf newmfil  10 return inf.mfi Figure 10 The infMfi method For the node at the level 0 the local new-mf is actually maximal frequent itemsets and can output directly into a file Since its information for future searching is saved by the method inJAddMfilnf in line 9 there is no need to keep the new-mfi and the memory of new-mfi can he released 5 Experimental Results 10 1 0.1 0.01 Mnirmrn Support  Figure 11 Running time on Mushroom We compare SmartMiner with Mafia and GenMax All of them are implemented in Java JDK1.3 For fair comparison the three methods use the same vertical data model VData As we discussed before there are many ways to implement the vertical data model In this paper our purpose is to study the efficiency of different search strategies We choose VData because it takes less memory and is easy to implement The experiment was done on a lGhr Celeron with 512 MB of memory A detailed comparison of SmartMiner with Mafia and GenMax was conducted on two datasets Connect-4 and Mushroom Figure 11 shows the performance comparison of the three methods on Mushroom All three methods use the PEP pruning technique Our running time does not include the input time but does include the output time The horizontal axis shows minimum support in percentage The vertical axis is the running time in seconds In general SmartMiner is one order of magnitude faster than both Mafia and Genmax When minimal support is high Mafia is faster than Genmax Low minimal support increase the number of MFI, then Genmax performs better than Mafia 1000 10 1 0.1 0.01 Mnirmrn Support  Figure 12 Search tree size on Mushroom Figure 12 compares the sizes number of nodes in a tree of the search trees for the three methods From the figure we noticed that Genmax generates 10 times more nodes than SmartMiner and also much more than Mafia This indicates that the static ordering in GenMax is not as efficient as the dynamic reordering used by both SmartMiner and Mafia Moreover we noticed that SmartMiner generates less nodes than Mafia which is due to the heuristic select function used the SmarMiner 10000 I 1 5 1000   a 100 0 10 1 0.1 0.01 Minimum Support  Figure 13 the  of counting on Mushroom Figure 13 compares the number of support counti.ng which shows the number of times that the private method  base short item in VData is called As shown in Figure 13, Genmax calls the calSup methods significantly more than both SmartMiner and Mafia 576 


Further SmartMiner needs less number of support counting than Mafia Since GenMax introduces a fast superset checking algorithm the performance gain of dynamic reordering of Mafia is mitigated by the increasing time for superset checking when the set of MFI becomes large This is the reason we see in Figure 10 and Figure 13 that Mafia is better than Genmax when minimal support is high and the reverse when minimal support is low 10000 P I    smrtMinx 95 90 80 70 60 50 40 30 20 10 Mnirmm Support X Figure la Running time on Connect Figure 14 shows the performance comparison of the three methods for the Connect dataset Again we noticed the significant performance improvements of SmartMiner than Mafia and GenMax 6 Conclusion In this paper we propose the SmartMiner algorithm to find exact maximal frequent itemsets for large datasets The SmartMiner algorithm is able to take advantage of the information gathered from previous steps to search for MFI First it gathers global and local tail information and uses an heuristic select function to reduce the search tree Second the passing oftail information eliminates the need of known MFllfor supersel checking Smartminer does not require superset checking which can be very expensive Finally SmartMiner also reduces the number of support counting for determining the frequency of tail items and thus greatly saves counting time Our experiments reveal that the SmartMiner algorithm yields an order of magnitude improvement over Mafia and GenMax in generating the MFI on the two datasets Acknowledgements The authors wish to thank Professor Mohammed J Zaki for stimulating discussion in the performance study References I R Agrawal and R Srikant Fast algorithms for mining association des In Proceedings of the 20th VLDB Conference Santiago, Chile, 1994 2 R Agarwal C Agganval and V Prasad A tree projection algorithm for generation of frequent itemsets. Journal of Parallel and Distributed Computing 2001 3 Roberta Bayardo Efficiently mining long patterns from databases In ACM SIGMOD Conference 1998 4 D Burdick M. Calimlim, and J Gehrke MAFIA a maximal frequent itemset algorithm for transactional databases In Intl. Conf on Data Engineering Apr 2001 5 K Gouda and M J Zaki Efftciently Mining Maximal Frequent Itemsets Proc of the IEEE Int Conference on Data Mining, San Jose 2001 6 J Han I Pei and Y Yin Mining Frequent Patterns without Candidate Generation, Proc 2000 ACM-SIGMOD Int Conf on Management of Data SIGMODOO Dallas TX May 2000 7 Heikki Mannila Hannu Toivonen and A Inken Verkanw Eflicient algorithms for discovering association rules In KDD 94 AAAl Workshop on Knowledge Discovery in Databases pages 181-192, Seattle Washington July 1994 8 I S Park M Chen, and P S Yu An effective hash based algorithm for mining association rules In Proc ACM SIGMOD Intl Conf Management of Data May 1995 9 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules In 7th Intl Conf on Database Theory January 1999 IO 1 Pei, J Han, and R Mao Closet An efficient algorithm for mining frequent closed itemsets In SIGMOD Int'l Workshop on Data Mining and Knowledge Discovery May 2000 ll]Brin,S.;Mohvani,R.;Ullman,J.;andTsur,S 1997 Dynamic Itermet Counting and Implication Rules for Market Basket Data In Proc of the 1997 ACM-SIGMOD Conf On Management of Data 255-264  121 Ashok Sarasere Mward Omiecinsky, and Shamkant Navathe An eflicient algorithm for mining association rules in large databases In 21st Int'l Conf on Very Large Databases VLDB ZTrich, Switzerland, Sept. 1995  131 Hmu Toivonen. Sampling large databases for association rules In Proc of the VLDB Conference Bombay India September 1996 14]M I Zaki S Parthasarathy M Ogihara and W Li New algorithms for fast discovery of association rules In 3rd Intl Cod on Knowledge Discovery and Data Mining August 1997  151 M J Zaki and C. Hsiao Charm An efficient algorithm for closed association rule mining In Technical Report 99-1 0 Computer Science, Rensselaer Polytechnic Institute, 1999  161 Q Zou W Chu D Johnson and H Chiu A Panern Decomposition \(PD\Algorithm for Finding All Frequent Panems in Large Datasets Proc of the IEEE Int. Conference on Data Mining, San lose 2001 17]Q Zou W Chu D Johnson and H Chiu Panern Decomposition Algorithm for Data Mining of Frequent Patterns Joumal of Knowledge and Information System Volume 4 page 466482.2002 577 





BAT841 CAR881 I COD701 COD721 COL891 871 LAM89 J LEC88 MAC851 W861 Alashqur A.M Su S.Y.W and Lam H A Rule-based Language for Deductive Object Oriented Databases Proceedings of the 6th Inter national Conference on Data Engineering Los Angeles CA Feb 5-9 1990 Bancilhon F et al FAD a Powerful and Si ple Database Language Proceedings of the 13th VLDB Conference Brighton 1987 Banerjee J et al Queries in 8ject-oriented Databases Proceedings of the 4th Intl Confer ence on Data Encrineerine Los Angeles CA  97-105 I I  1988 pp 31-38 Batory D.S and Buchmann A.P Molecular Obiects Abstract Abstract Data Tvws and Data M6deIsi A Framework Proceedings Intl Confer ence on VLDB 1984, pp 172-184 Carey M J et al A Data Model and Query Language for EXODUS ACM-SIGMOD Confer ence 1988, pp. 413-423 Chuang H S Operational Role Processing in a Prototype OSAM KBMS Master's thesis University of Florida 1990 Codd E A Relational Model of Data for Large Shared Data Bank CACM Vol 13 No 6 1970 pp 377-387 Codd E R:lational Completeness of Database Sublanguages in Data Base Systems Rustin R ed Prentice-Hall Inc Englewook Cliffs NJ Colby L S A Recursive Algebra and Query Optimization for Nested Relations ACM SIGMOD Conference 1989 pp 273-283 Fishman D H et al Iris An Object-Oriented Database Management System ACM Transaction on Office Infomation Systems 51 1987 pp49 69 Goldberg A Introducing the Smalltalk-80 Sys tem Byte Aug 1981. pp.14-26 Hammer M and Mcleod D Database Descrip tion with SDM A Semantic Association Model Kim W et al Composite Object Support in an Object-oriented Database System Proceedings of King R Sembase A Semantic DBMS the Proceedings of the First Intemational Workshop on Expert Database Systems Oct 1984, pp.151 171 Lam H et al Prototype Implementation of an Object-oriented Knowledge Base Management System Proceedings of PROCIEM 89 Orlando Lecluse C Richard P and Velez F 02 an Object-Oriented Data Model ACM-SIGMOD Conference 1988, pp 425-433 MacGregor R ARTEL--A Semantic Front-End to Relational DBMSs Proceedings of VLDB 85 Atlanta GA April 1988 Maier D et al Development of an Object oriented DBMS Proc of OOPSLA 86 Confer ence Sept 29  Oct 2 Portland Oregon pp 472-482 1971 pp.65-98 ACM TODS Vol 6 NO 3 1981 pp 351-368 OOPSLA, Oct 4-8 1987 FL pp 118-125 FL NOV 13-15, 1989 SU86 SUSS SU89 ZD087 Manola F and Dayal U PDM An Object Oriented Model Int'l Workshop On Object Oriented Database Systems 1986, pp 18-25 Pant S An Intelligent Schema Design Tool for OsAM Master's thesis University of Florida 1990 Rowe L A and Stonebraker M R The POSTGRES Data Model Proceedings of the 13th VLDB Conference Brighton 1987 pp 83-96 Shaw G M and ZdoNc S B A Query Alge bra for Object-Oriented Databases IEEE Trans on Data Engineering 12\(3 pp 154-162 Feb 1990 Shipan D The Functional Data Model and the Data Language DAPLAX ACM Trans Database System ql March 1918 Singh M Transaction Oriented Rule Processing in an Object-Oriented Knowledge Base Manage ment System Master's thesis University of Florida 1990 Su S.Y.W Modeling Integrated Manufacturing Data With SAM IEEE Computer January Su S.Y.W et al An Object-oriented Comput ing Environment for Productivity Jmprovement in Automated Design and Manufacturing Project Summary PROCIEM 88,Orlandq E Nov 14-15 1988 Su S.Y.W Kxishnamurthy V and Lam H An Object-oriented Semantic Association Model OsAM AI in Industrial Engineering and Manufacturing Theoretical Issues and Applica tions Kumara S Soyster A.L and Kashyap R.L eds American Institute of Industrial Engineering 1989 Su S.Y.W Guo M and Lam H Association Algebra A Mathematical Foundation for Object Oriented Databases to be submitted to Trans on Knowledge and Data Engineering Tsurt S and Zaniolo C An Implementation of GEM  Supporting a Semantic Data Model on a Relational Back End Proceedings of the ACM SIGMOD Jntl Conference on the Management of Data 1984 pp 286-295 Frederick Ty The Design and Implementation of a Graphics Interface for an Object-oriented Language Master's thesis University of Florida 1988 Zaniolo C The Database language GEM Proceedings of the ACM SIGMOD Intl Confer ence on the Management of Data 1983 Zdonik S B Skarra A H and Reiss S P An Object Server for an Object-oriented Database System Intemational Workshop on Object oriented Database Systems Pacific Grove CA Sept 1986 Zdonik S.B The implementation of a Shared Clustered Memory System for an 0-0 Database System ACM Trans on Office Information Sys tems Apr 1987 1986 pp.34-49 32 I 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


