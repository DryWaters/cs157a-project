2001 Proceedings of the 23rd Annual EMBS International Conference October 25-28 Pstanbul Turkey KNOWLEDGE DISCOVERY FROM DISTRIBUTED CLINICAL DATA SOURCES: THE ERA FOR INTERNET-BASED EPIDEMIOLOGY G A Potamias'92 V S Mo~stakis Institute of Computer Science Foundation for Research  Technology  Hellas \(FORTH\Heraklion, Crete, Greece Department of Computer Science, University of Crete, Heraklion, Crete, Greece 3Department of Production Engineering and Management Technical University of Crete, Chania, Greece Abstract-A methodology and the operational framework for knowledge discovery from distributed and heterogeneous clinical data sources are presented The methodology follows a 
multi-phase process for the integration homogenization and intelligent processing of the distributed and heterogeneous data Its realization is based on the coupling of multi disciplinary technologies ranging from CORBA-based seamless access to distributed data to semantic data homogenization Operations and to advanced DTDlXML operations These operations coupled with advanced and effective data representation models forms a framework in which effective knowledge discovery KDD operations are performed The fundamental contribution of our work is the incorporation and customization of Association Rule Mining ARM operations on top of appropriately generated XML documents. Based on the argument that future databases will use XML-like structures in order to store and retrieve data then our work presents a promising direction towards 
internet-based epidemiology as realized by the respective knowledge-discovery from distributed clinical data sources operations Keywords  Telemedicine epidemiology knowledge discovery association rules distributed databases INTRODUCTION Epidemiology refers to the study of the distribution and determinants of health-related states or events in specified populations and the application of this study to control of health problems The key-objective of epidemiological studies is to analyze clinical data collected fiom different geographical locations The ultimate goal is health prevention and health prevention is highly dependent on information transfer The convergence of computing and communications will allow for greater efficiency 
and accuracy in conducting epidemiological and public health research projects In this context some of the key-features that determine the hture of epidemiological methodologies include data collection jom distributed and heterogeneous information immediate access to summary data and increased communication between researchers and between researchers and participants With the current explosion of data the problem of how to combine distributed and heterogeneous D&H information sources becomes more and more critical Besides collecting enormous amount of data it is very important to consider the general need of semantic integration and knowledge discovery from these sources 
The main differences here and consequently the grand challenges with respect to single static and homogeneous information sources is the raising need for integrating multiple knowledge representations e.g domain ontologies and data-models If the distributed nature of data has a more-or-less clear definition heterogeneity is a more complex concept. Take as an example the Integrated Electronic Health Care Record I-EHCR environment 161 8  The real issue here is not only how to access specific information systems that maintain EHCR segments but also how to identify and index the essential information in them A promising approach to this integration problem is to gain control of 
the organization's information resources at a meta-data level while allowing autonomy of individual systems at the data instance level However achieving integration at the semantic level is a challenging problem mainly because the logic knowledge and data structures used in various systems are complex and often incompatible  121 A realistic solution should hide heterogeneity at the top level while making the individual sources of information appear to end users as a large collection of objects that behave uniformly In this paper we present our work towards knowledge discovery form D&H clinical data sources In particular we tackle the problem of inducing interesting associations 
between data items stored in remote clinical information systems With the appropriate customization, the discovered knowledge may reflect potential health-indicators The test bed environment of our approach is the HYGEIAnet The Integrated Health Care Network of Crete 9 13 One of the basic healthcare services offered within the HYGEIAnet network is the access to patients' clinical information stored in autonomous legacy clinical information systems 131 METHODOLOGY To tackle the problem of mining H&D data sources a multi-phase data integration procedure should be followed A rational approach should cfficiently confront and cope with i efficient access to 
structured and distributed data sources ii reliable homogenization and integration of heterogeneous data with a dedicated domain ontology and respective ontological operations playing an important role iii effective and reliable dala processing operations e.g traditional statistical analysis data mining etc and iv presentation of results e.g visualization operations Fig 1 shows our view cin the general architecture for knowledge-discovery in an environment of distributed and heterogeneous data sources The access operations lying between the autonomous clinical information systems and 0-7803-721 1-5/01/$17.00 0 2001 IEEE 3638 


the mediator are already in place and operational within the integrated health Telematic network of the Crete region Our work expands the architecture by adding a the semantic indexing operations b the DTD/XML generation and parsing operations c the object-oriented data representation schemas and operations and d the adaptation of KDD operations as realized by a special devised Associations Rule Mining ARM algorithm that operates on top of XML documents Data Access and Integration The Patient Clinical Data Directory PCDD lo is an implementation of the I-EHCR in the Crete region Via PCDD the various systems forms a federation of autonomous distributed and heterogeneous clinical information systems What PCDD offers is a meta-data abstraction of the distributed data sources and of the respective data items In other words it provides the necessary information about where to search for The semantic mapping of the pointers i.e URL links kept by PCDD to the respective patients clinical data is realized via specially devised wrappers or gateways for the various clinical information systems in the federation The underlying systems communication is based on a CORBA infrastiucture and respective IDL interfaces 5 a ii Fig 1 General architecture of an integrated environment for Knowledge Discovery from Distributed Data Sources Unifbrm Data Representation Semantic Homogenization While CORBA makes it possible for developers to independently contribute to a library of components across platforms and languages it offers little or no help with the knowledge-level task of ensuring that particular components actually can work together The inclusion of semantics would provide what is currently missing from IDLs information about the meaning of a component and information about what the component will accomplish 7 This is a task to be accomplished by a the introduction and utilization of a domain data-model and b the incorporation of respective domain ontology Clinical data-model To this end we have adopted a standard IDL interface that copes with the access to clinical data sources operations namely the COAS Clinical Object Access Service interface 4 The basic service offered by COAS is a hierarchical organization of clinical information Medical ontology The success of an infomiation brokering service that access and retrieve distributed data stores depends heavily on its ability to cope with the heterogeneous nature of the stored data making the incorporation of a domain specific ontology more than a prerequisite. Towards this goal we have developed a service for the storage and retrieval of common and universally accepted names and codes of medical tenns the Common Clinical Term Reference service CCTR The service exploits and utilizes terms and relations from the UMLS Unified Medical Language System  and from the International Codingfor Diseases ICD\standards Having accessed and uniformly organized the distributed clinical data the problem is how to represent them. The XML standardized infrastructure 15 serves this need Towards this end we have developed and implemented a COAS-compliant DTD grammar in order to automate the generation of XML documents the content of which corresponds to the remotely accessed and retrieved clinical data Information processing takes place exclusivelv on top of the XML documents. For this purpose a special XML parser was devised The parser i reads parse the XML document ii identifies composite/atomic observations and their corresponding values and iii constructs tree-like structures for storing and retrieving XML documents. Each tree corresponds to a specific composite observation starting from the root of the tree e.g ObsNAME SYSTOLICPRESSURE1l  ValueObsNAME 112011 for clinical examinations and symptoms We argue that future databases will use XML-like structures in order to store and retrieve data Using these structures it is easy to develop and implement appropriately customized knowledge-discovery operations on top of XML documents MINING DISTRIBUTED AND HETEROGENEOUS CLINICA DATA SOURCES We may now proceed on the specifics of knowledge discovery processes. In particular we are interested on the discovery of interesting associations i.e association rules between the recorded patients clinical data items Under  3639 


suitable assumptions these associations may be linked with indicative epidemiological and health-indicators Association Rule Mining Association Rule Mining ARM is among the most advanced and interesting methods introduced by machine learning and data mining research l 2 ll The definition of an ARM problem has as follows: Let I  i i2   i be a set of literals called items Let D be a set of transactions, where each transaction T is a set of items such that T c I We say that a transaction T contains X a set of some items in I if X c T An association rule is an implication of the form X Y  where X c I Yc I and X n Y  0 The rule X Y holds in the transaction set D with confidence c if c of transactions in D that contain X also contains Y The rule X Y has support s in the transaction set D ifs of transactions in D contains X uY Given a set of transactions D the ARM problem is to discover identify and form the associations that exhibit support and confidence values higher that the user specified minimum support minsup and minimum confidence minconf levels respectively Note that the exploration for association rules does not confined by the representation of D being a plane ascii file or a relational database hour case we rely on the tree-like data structures that correspond to respective XML-documents ARM in the CIinical Domain The clinical validity as well as the reliability of ARM operations in a distributed clinical information systems environment demands some assumptions and conventions  Each fransacfion corresponds to a specific patient encounter i.e identifiable visits of patients in a healthcare unit of the federation Each encounter is uniquely identified by reference to four attributes namely Patient-Id Information-System Visit-Id and Date Note that clinical-data are recalled anonymously and retrieved in a secure-manner services offered by the respective PCDD security-server  Each item is represented by the triplet Atomic-Observation Value Interval where Atomic-Observation represents a clinical observation e.g cholesterol Value the value recorded for a specific atomic observation e.g 251 as a value for cholesterol and Interval the interval in which Value belongs e.g 120-2001  Normal In the medical domain most of the items are numeric In order to discover associations between items we have to transform these numeric values into nominal ones i.e intervals at least for the association rule discovery procedures that cope only with nominal attributes These intervals may come from expert medical advice and/ or form established and universally accepted clinical protocols and guidelines Discovery of Interesting Associlitions The ARM techniques that we have implemented rely on the principles of the Apriori algorithm I 2 Taking advantage of the employed dynamic tree-like data structures we have added some extra featwes to these algorithms. With these revisions our ARM algorithm, called ApriorXML is enhanced with object-oriented search operations able to work on top of XML-structured data and respective representation formalisms One of the fundamental operations of Apriori-like algorithms is the generation of large itemsets This is achieved by making multiple scans on the input data The AprioriXML algorithm follows a similar but more economic operation After each scan it prunes the parts of the tree branches that are not needed in the next scans So the database is successively reduced with a consequent reduction into the needed computational space and time see table I RESULTS A full scenario for the activation of the presented ARM KDD operations includes the following steps i the user via the patient clinical data directory service posts a specific query For example helshe may be interested for all patients encounters present in the federation with pre-specified values for clinical findings; for laboratory results and for recorded diagnoses values ii the respective autonomous clinical information systems are accessed and the details of patients encounters that match the pre-specified values are retrieved and recalled iii the DTD/XML generation operations are activated and the respective query-specific XML document is generated iv the generated XML is parsed and semantically homogenized and the respective tree-like structures are generated v the KDD/ARM operations as realized by the AprioriXML algorithm\are activated in order to discover and form interesting associations The Data Patients Visits and Transactions In order to make transparent and easy to inspect the involved operations we rely on a small database of ten 1 0 patients visits from a total of about 200 recorded in two remote healthcare centers of Crete The posted query was in natural language access and retrieve all hematological examinations for all recordedpatient visits The numeric-valued observations were passed through a discretisation routine and all values are grouped into three intervals reflecting the IOW  the normal and the high status of the specific lab-exams measurements Table I shows a subset of thle selected transactions In the same table the resulted database after the pruning operation is activated is shown a reduction of 55 in size is achieved after the Td scan 3640 


TABLE I ORIGINAL AND REDUCED AFTER 2\223 SCAN\DATABASE TRANSACTIONS REFERENCES Original After 2\224d Scan Status  I 4 8 12 15 16  1 4,8,16 Reduced 3,5 Eliminated  1  5 8 10 13 18 19  1 8 IO 19 Reduced  1 4 7 IO 14 16 19,23  1 4 IO 16 19,23 Reduced 3 6,9 11 18,21,22,25 Eliminated c 1 4 8 IO 13 16 21 24,27  1 4 8 IO 16 Reduced I 4 8 11 14 17.19 23,26,30  1 4 8 19,23 Reduced  1 4 8 IO 15 16, 19 23 25  1 4,8 IO 16 19,23 Reduced  1 4 8 11 15 16.23 25 1,4,8,16,23 Reduced  2.4 8 IO 14 16 20 23  4,8 IO 16,232 Reduced Table I1 shows a sample of the discovered association rules that meet the following threshold requirements ininsup  60 and minconf   80 TABLE 11 THE DISCOVERED ASSOCIATION RULES minsuc=60 rninconHO Association Rule support Confidence   GMN[LOV  WBC[LOv 74.2 82.1 RBC[NORMAL  GRAN[LOW 77.4 92.3 100 100 PCT[NORMAL  UPV[NORML  PDV[NORUAL 64.5 83.3 WBC[LOv  MPV[NORUAL LEMFO[NORUAL  RBC[NORMAL RBC[NORML  UCV[NORMAL UCH[NORML  PCT[NORMAL MPV[NORUAL  PDV[NORMAL  LEUFO[NORMAL 45.2 100 CONCLUSSION AND FUTURE WORK We have presented a methodology its respective architectural setting and operational framework for mining structured distributed and heterogeneous data sources The approach is realized by the coupling of multi-disciplinary technologies ranging from CORBA based seamless access to distributed data to semantic data homogenization operations- based on the appropriate utilization of a domain specific ontology, and to advanced DTD/XML operations The hndamental contribution of our work is the incorporation and customization of KDD/ARM operations on top of appropriately generated DTD/XML documents Based on the argument that future databases will use Xh4L like structures in order to store and retrieve data then our work shows the effectiveness of respective distributed knowledge-discovery operations Furthermore with the natural assumption that the discovered knowledge constructs may point to potential health-indicators our work presents a promising direction towards internet-based epidemiology Our hture research and development plans include a large scale experiments in order to test the scalability of our approach b design and development of appropriate human computer interfaces accompanied with user-profiling capabilities for personalized delivery of KDD results and c customization of other KDD data analysis methods e.g clustering, decision trees etc  I R Agrawal T Imielinski and A Swami 223Mining association rules between sets of items in large databases\224 in SIGMOD, Washington D.C pp 207-2 16 May 1993 2 R Agrawal and R Srikant, \223Fast algorithms for mining association rules\224 Proc of the 20\221h Int\221l Conference on Very Large Databases Santiago, Chile, Sept. 1994 3 W.M.Q Baldonado and S.B Cousins 223Addressing heterogeneity in the networked information environment\224 Technical Report Computer Science Department, Stanford University December 1996 4 COAS 223Clinical Observations Access Service 224 COAS Final Submission OMG Document corbamed99-03-25 1999 5 CORBA Web site http://www.corba.org 6 D. Forslund, and D. Kilman, \223The virtual patient record a key to distributed healthcare and telemedicine\224 Los Alamos National Laboratory February 29 1996 http www.acl.lanl.gov TeleMed Papers/ virtual.html 7 J.H Gennari A.R Stein and M.A Musen, \223Reuse for knowledge-based systems and CORBA components.\224 in Proceedings of I dh Knowledge Acquisition Workshop Banff, Alberta, Canada 1996 8 W Grimson D Berry J Grimson G Stephens E Felton P Given and R OMoore 223Federated healthcare record server  the synapses paradigm\224 Web document hi www.cs.tcd.ie synapsed public html technicaldescription.html 1997 9 HYGEIAnet Web site 223Integrated health care network of Crete\224 http://www.hygeianet.gr  101 InterCare 223InterCare end-user applications\224 Deliverable 04 I Health Telematics program Europe HC 401 I project November 1999 I 11 A Mueller 223Fast sequential and parallel algorithms for association rule mining a comparision\224 Technical report CS-TR-35 15 Dept of Computer Science University of Maryland Vollege Park MD, August 1995 12 E Sciore M Siegel and A Rosenthal 223Using semantic values to facilitate interoperability among heterogeneous information systems\224 ACM Transactions on Database Systems Vol 19 No 2 pp. 254-290, June 1994 13 M Tsiknakis C.E Chronaki S Kapidakis C Nikolaou and S.C Orphanoudakis 223An integrated architecture for the provision of health telematic services based on digital library technologies\224 International Journal on Digital Libraries Special Issue on 223Digital Libraries in Medicine\223 vol 1 3 pp 257-277 1997  UMLS UMLS 2000 documentation Web document http://www.nlm.nih.gov research/umls/UMLSDOC.HTML  XML Web site 223W3C main XML document\224 http://www.w3 .org/XML 3641 


by node-link to a queue The head of each queue is stored in a Header table To store the original rule set 13 cells are needed for the left hand sides of the rules Using CR-tree only 9 nodes are needed As can be seen from the above example the CR-tree structure has some advantages as follows CR-tree is a compact structure It explores potential sharing among rules and thus can save a lot of space on storing rules Our experimental results show that in many cases about 50-6076 of space can be saved using CR-tree CR-tree itself is an index for rules For example if we want to retrieve all the rules having attribute value b and d in the set of rules in Example 2 we only need to traverse node-links of d which starts at the header table, and keep looking upward for b Once a CR-tree is built rule retrieval becomes efficient That facilitates the pruning of rules and using rules for classification dramatically 3.3 Pruning Rules The number of rules generated by class-association rule mining can be huge To make the classification effective and also efficient we need to prune rules to delete redun dant and noisy information According to the facility of rules on classification a global order of rules is composed Given two rules R1 and Rz RI is said having higher rank than Rz denoted as R1  Rz if and only if 1 conf\(R1  conf\(R2 2 conf\(R1  conf\(R2 but sup\(R1  sup\(R2 or 3 conf\(R1  conf\(R sup\(R1  sup\(R2 but RI has fewer attribute values in its left hand side than R2 does In addition a rule RI  P  c is said a general rule w.r.t rule R2  P  c if and only if P is a subset of PI CMAR employs the following methods for rule pruning First using general and high-confidence rule to prune more specijic and lower confidence ones Given two rules R1 and R2 where R1 is a general rule w.r.t R2 CMAR prunes R2 if R1 also has higher rank than R2 The ratio nale is that we only need to consider general rules with high confidence and thus more specific rules with low confidence should be pruned This pruning is pursued when the rule is inserted into the CR-tree When a rule is inserted into the tree, retrieval over the tree is triggered to check if the rule can be pruned or it can prune other rules that are already inserted Our experimental results show that this pruning is effective Second selecting only positively correlated rules For each rule R  P  c we test whether P is positively cor related with c by x2 testing Only the rules that are posi tively correlated i.e those with x2 value passing a signif icance level threshold, are used for later classification All the other rules are pruned Algorithm 1 Selecting rules based on database coverage Input a set of rules and a coverage threshold 6 Output a subset of rules for classification Method 1 Sort rules in the rank descending order 2 For each data object in the training data set, set its cover-count to 0 3 While both the training data set and rule set are not empty, for each rule R in rank descending order find all data objects matching rule R If R can correctly classify at least one object then select R and increase the cover-count of those objects matching R by 1 A data object is removed if its cover-count passes cov erage threshold 6 Figure 3 Selecting rules based on database coverage The rationale of this pruning is that we use the rules re flecting strong implications to do classification. By remov ing those rules not positively correlated, we prune noise This pruning happens when a rule is found Since the distribution of class labels w.r.t frequent patterns is kept track during the rule mining, the x2 testing is done almost for free Third pruning rules based on database coverage CMAR selects a subset of high quality rules for classifica tion That is achieved by pruning rules based on database coverage CMAR uses a coverage threshold 7 to select database coverage as shown in Figure 3 The database coverage method used in CMAR IS simi lar to that in CBA The major difference is that, instead of removing one data object from the training data set imme diately after it is covered by some selected rule we let it stay there until it is covered by at least 6 rules That allows more selected rules When classifying a new data object it may have more rules to consult and may have better chance to be accurately predicted This pruning is pursued when the rule mining process finishes. It is the last pruning of rules 4 Classification Based on Multiple Rules After a set of rules is selected for classification as dis cussed in Section 3 CMAR is ready to classify new ob jects Given a new data object CMAR collects the subset of rules matching the new object from the set of rules for classification In this section, we discuss how to determine the class label based on the subset of rules Trivially if all the rules matching the new object have the same class label CMAR just simply assigns that label to the new object 373 


If the rules are not consistent in class labels CMAR di vides the rules into groups according to class labels All rules in a group share the same class label and each group has a distinct label CMAR compares the effects of the groups and yields to the strongest group To compare the strength of groups we need to mea sure the 223combined effect\224 of each group Intuitively if the rules in a group are highly positively correlated and have good support, the group should have strong effect There are many possible ways to measure the combined effect of a group of rules For example, one can use the strongest rule as a representative That is the rule with highest x2 value is selected However simply choosing the rule with highest x2 value may be favorable to minority classes, as illustrated in the following example RZ ed=univ edfuniv Example3 In a credit card application approval case there are two rules RI  job  no  rejected\(support  R2  education  university  approved\(sup  The observed and expected contingencies are shown in confidence  SO and 200 confidence  99.5 Figure 4 I RI 1 amroved I reiected I total 1 approved rejected total 199 1 200 251 49 300 L  I job=yes I 438 I 32 I 470 iob=no I 12 I 18 I 30 total I 450 I 50 I 500 The observed contingency of rule RI I I I total 1 450 I 50 I 500 The observed contingency of rule Rz  RI I approved I rejected I total I I iob=ves I 423 I 47 I 470 I _.I job=no I 27 1 31 30 total I 450 I 50 I 500 customer having no job and with university education we may predict her application would be rejected using rule RI if the choice of rules is based on only x2 values However rule R2 is intuitively much better than RI since Rz has much higher support and confidence Another alternative is to use the compound of correla tion of rules as measure For example we can sum up x2 values in a group as the measure of the group However this method suffers from the same problem that it may fa vors minority too much A better way is to integrate both information of cor relation and popularity into the measure After empirical verification CMAR adopts a weighted x2 measure 7 as follows For each rule R  P  c let sup\(c be the number of data objects in training data set that associated with class label c and IT1 the number of data objects in the training data set We define maxX2 for rule R as follows where rnaxX2 computes the upper bound of x2 value of the rule w.r.t other settings are fixed Then, for each group of rules the weighted x2 measure of the group is defined as As can be seen we use the ratio of x2 value against its upper bound i.e rnaxx\222 to overcome the bias of x2 value favoring minority class Please note that theoreti cally, it is hard to verify the soundness or effect of mea sures on strength of groups of rules Instead we explore the effect of measures empirically, and according to our experimental results weighted x2 value is the best from among a good set of candidate measure formulas that can be worked out by us E The expected contingency of rule RI I R2 I approved I rejected I total I I ed=univ I 180 I 20 I 200 1 edfuniv I 270 I 30 I 300 total I 450 1 50 1 500 The expected contingency of rule Rz Figure 4 Observed and expected contingen cies for rules Based on the observed and expected values, the x2 val ues for R1 and Rz are 88.4 and 33.6 respectively For a 5 Experimental Results and Performance Study To evaluate the accuracy efficiency and scalability of CMAR we have performed an extensive performance study In this section we report our experimental results on comparing CMAR against two popular classification meth ods CBA 9 and C4.5 lo It shows that CMAR outper forms both CBA and C4.5 in terms of average accuracy efficiency and scalability All the experiments are performed on a 600MHz Pen tium PC with 128M main memory running Microsoft Windows/NT CBA and C4.5 were implemented by their authors, respectively In the experiments, the parameters of the three methods are set as follows 374 


All C4.5 parameters are default values We test both C4.5 decision tree method and rule method. Since the rule method has better accuracy we only report the accuracy for rule method For CBA we set support threshold to 1 and confi dence threshold to 50 and disable the limit on number of rules Other parameters remain default For CMAR the support and confidence thresholds are set as same as CBA. The database coverage threshold is set to 4 and the confidence difference threshold to 20 All reports of the runtime include both CPU time and VO time We test 26 data sets from UCI Machine Learning Repository We use C4.5 shufle utility to shuffle the data sets Also we adopt the same method used by CBA to discretize continuous attributes I Data set I  attr I CIS I  rec I C4.5 I CBA I CMAR I I Anneal I 38 I 6 I 898 I 94.8 1 97.9 I 97.3 I I I I I I I Austral I 14 I 2 I 690 I 84.7 I 84.9 I 86.1 Auto I 25 I 7 I 205 I 80.1 1 78.3 I 78.1 I I I I 1 Breast I 10 I 2 I 699 I 95 I 96.3 I 96.4 Cleve I 13 I 2 I 303 1 78.2 I 82.8 I 82.2 Table 3 The comparison of C4.5 CBA and CMAR on accuracy As can be seen from the table CMAR outperforms both C4.5 and CBA on accuracy Furthermore out of the 26 data sets CMAR achieves the best accuracy in 13 ones In another word CMAR wins in 50 of test data sets In some data sets, e.g Lymph CMAR wins the second place over 5 in accuracy There are two important parameters database coverage threshold and confidence difference threshold in CMAR As discussed before, these two thresholds control the num ber of rules selected for classification In general if the set of rules is too small some effective rules may be missed On the other hand if the rule set is too large, the training data set may be overfit Thus we need to test the sensitivities of the two thresholds w.r.t classification accuracy As an example we test different database coverage threshold values on the Sonar data set from UCI Machine Learning Database Repository The results are shown in Figure 5 where the confidence difference threshold is set to 0 On the other hand we test different confidence differ ence threshold values on the Sonar data set The results are shown in Figure 6 where the database coverage threshold is set to 1 I"""'1 1 1.5 2 2.5 3 3.5 4 4.5 5 Database coverage threshold Figure 5 The effect of coverage threshold on accuracy 80 75 0 0.02 0.04 0.06 0.08 0.1 0.12 Confidence difference threshold Figure 6 The effect of confidence difference threshold on accuracy From the figures one can see that the peaks of accu racy are achieved at the middle of both curves That is there are optimal settings for both thresholds However according to our experimental results, there seems no way to pre-determine the best threshold values Fortunately both curves are quite plain That means the accuracy is not very sensitive to the two thresholds values CR-tree is a compact structure to store rules To test the effectiveness of CR-tree we compare the main mem ory usage of CBA and CMAR on large test data sets The 375 


results are shown in Table 4 6 Conclusions mem \(M mem M associative classification 1 efficiency at handling huge Table 4 The comparison of CBA and CMAR on main memory usage Dataset Auto Hypo Ion0 Sick Please note that in this experiment we disable the lim itation of number of rules in CBA In such a setting CBA and CMAR generate all the rules necessary for classifica tion and thus are compared in a fair base From the table one can see that on average CMAR achieves 77.12 sav ing on main memory usage The saving in main memory usage can be explained from two apsects First CMAR uses CR-tree The compactness of CR-tree brings significant gain in storing a large set of rules where many items in the rules can be shared On the other hand CR-free is also an index structure of rules Before a rule is inserted into a CR-tree CMAR checks if there is a general rule or some more specific rules in the tree If so related pruning is pursued immediately Such a pruning techique also contributes to the saving of main memory To test the scalability of CMAR we compare the run time of CBA and CMAR on six data sets The results are shown in Figure 5 Again we disable the limit on number of rules in CBA In the experiments CBA spends a large portion of runtime on YO  attr  cls  rec CBA runtime CMAR runtime 25 7 205 612s 408s 25 2 3163 92s 19s 34 2 351 150s 89s 29 2 2800 74s 13s Sonar I 60 I 2  208 1 226s 145s Table 5 The runtime of CBA and CMAR As can be seen from the table CMAR is faster than CBA in many cases Please be note that the machine we use for testing is with relatively small size of main memory 128M Both CBA and CMAR can be expected running significantly faster if more main memory is available racy 2 it prunes rules effectively based on confidence correlation and database coverage and 3 its efficiency is achieved by extension of an efficient frequent pat tern mining method FP-growth construction of a class distribution-associated FP-tree and applying a CR-tree structure to store and retrieve mined association rules effi ciently Our experiments on 26 databases in UCI machine learning database repository show that CMAR is consis tent highly effective at classification of various kinds of databases and has better average classification accuracy in comparison with CBA and C4.5 and is more efficient and scalable than other associative classification methods References I R Agrawal and R Srikant Fast algorithms for mining as 2 P Clark and T Niblett The CN2 induction algorithm Ma 3 G Dong X Zhang L Wong and J Li Caep Classifi sociation rules In VLDB\22294 Chile Sept 1994 chine Learning 3:261-283,1989 cation by aggregating emerging patterns In DS\22299 LNCS I72 I Japan Dec 1999 4 R Duda and P Hart Pattern Classification and Scene Anal ysis John Wiley  Sons 1973 5 J Han J Pei and Y Yin Mining frequent patterns without candidate generation In SIGMOD\222OO Dallas TX May 2000 6 B Lent A Swami, and J Widom Clustering association rules In ICDE\22297 England April 1997 7 W Li Classification based on multiple association rules M.Sc Thesis Simon Fraser University April 2001 8 T.-S Lim W.-Y Loh and Y.-S Shih A comparison of prediction accuracy complexity and training time of thirty-three old and new classification algorithms Machine Learning 39,2000 9 B Liu W Hsu and Y Ma Integrating classification and association rule mining In KDD\22298 New York NY Aug 1998 IO J R Quinlan C4.5 Programs forkfachine Learning Mor gan Kaufmann 1993 I I K. Wang S Zhou and Y He Growing decision tree on support-less association rules In KDD\222OO Boston MA Aug 2000 376 


of the query expression without ha ving the global view of the in ten tion There is a big c hance that the enco ded pro cedure ma y not b e the b est w a y to compute the rules dep ending on the database instance F urthermore as w e understand it their prop osals require p oten tially large n um ber of name generation for relations and attributes The names that are needed are usually database dep enden t and th us p ossibly cannot b e gathered at query time An additional pro cess needs to b e completed to gather those v ariables b efore actual computations can b egin 5  9 Optimization Issues While it w as in tellectually c hallenging to dev elop a declarativ e expression for asso ciation rule mining from deductiv e databases there are sev eral op en issues with great promises for resolution In the w orst case the least xp oin tneedsto generate n 2 tuples in the rst pass alone when the database size is n  Theoretically  this can happ en only when eac h transaction in the database pro duces an in tersection no de and when they are not related b y subset-sup erset relationship In the second pass w e need to do n 4 computations and so on The question no w is can w e a v oid generating and p erhaps scanning some of these com binations as they will not lead to useful in tersections F or example the no de b 0 3 in gure 11 is redundan t A signican t dierence with apriori lik e systems is that our system generates all the item sets top do wn in the lattice without taking their candidacy as a large item set in to consideration Apriori on the other hand do es not generate an y no de if their subsets are not large item sets themselv es and thereb y prunes a large set of no des Optimization tec hniques that exploit this so called an ti-monotonicit y prop ert y of item set lattices similar to apriori could mak e all the dierence in our setup The k ey issue w ould b e ho ww e push the selection threshold minim um supp ort inside the top do wn computation of the no des in the lattice in our metho d F or the momen t and for the sak e of this discussion let us consider a higher supp ort threshold of 60 for the database T of gure 9 No w the l-en v elop e will b e the one sho wn in ligh ter dashed lines in gure 11 and the no des under this line will b e the large item sets Notice that no ww eha v eto discard no des ad 2 0 and d 0 2 to o This raises the question is it p ossible to utilize the supp ort and condence thresholds pro vided in the query and prune candidates for in tersection an y further Ideas similar to magic sets transformation 3  24 ma y be b orro w ed to address this issue The only problem is that pruning of an y no de dep ends on its supp ort coun t whic h ma y come at a later stage By then all no des ma y already ha v e b een computed and th us pushing selection conditions inside aggregate op erator ma y b ecome non-trivial Sp ecial data structures and indexes ma y also aid in dev eloping faster metho ds to compute ecien t interse ction joins that w e ha v e utilized in this pap er W e lea v e these questions as op en issues that should be tak en up in the future F ortunately though there has been a v ast b o dy of researc h in optimizing Datalog programs including recursiv e programs suc h as the one w e ha v e used in this pap er and hence the new questions and researc h 5 Recall that their prop osal requires one to express the mining problem to the system using sev eral queries and up date statemen ts that utilizes information ab out the database con ten ts to ac hiev e its functionalit y  c hallenges that this prop osal raises for declarativ e mining ma y exploit some of these adv ances Needless to emphasize a declarativ e metho d preferably a formal one is desirable b ecause once w e understand the functioning of the system w e will then be able to select appropriate pro cedures dep ending on the instances to compute the seman tics of the program whic hw e kno wis in tended once w e establish the equiv alence of declarativ e and pro cedural seman tics of the system F ortunately  w e ha v e n umerous pro cedural metho ds for computing asso ciation rules whic h complemen t eac h other in terms of sp eed and database instances In fact that is what declarativ e systems or declarativit y buy us  a c hoice for the most ecien t and accurate pro cessing p ossible 10 Conclusion Our primary goal for this pap er has b een to demonstrate that mining asso ciation rules from an y rst-order kno wledge base is p ossible in a declarativ ew a y  without help from an y sp ecial to ols or mac hinery  and that w e can no wha v ea v ery in tuitiv e and simple program to do so W eha v esho wn that it is indeed p ossible to mine declarativ ekno wledge b y exploiting the existing mac hinery supp orted b ycon temp orary inference engines in programming languages e.g Prolog or kno wledge base systems e.g RelationLog XSB LD L  CORAL All w e require is that the engine b e able to supp ort set v alued terms grouping aggregate functions and set relational op erators for comparison functionalities whic hmostofthesesystemscurren tly supp ort W e ha v e also demonstrated that our formalism is grounded on a more mathematical foundation with formal prop erties on whic h the seman tics of the R ULES system rely  W e ha v e also raised sev eral op en issues related to eciency and query optimization whic h should b e our next order of business As future researc h w e plan to dev elop optimization tec hniques for mining queries that require non-trivial lo ok ahead and pruning tec hniques in aggregate functions The dev elopmen ts presen ted here also ha v e other signican t implications F or example it is no w p ossible to compute c hi square rules 4 using the building blo c ks pro vided b y our system Declarativ e computation of c hi square rules to our kno wledge has nev er b een attempted for the man y pro cedural concepts the computation of c hi square metho d relies on In a separate w ork 2 w e sho w that the coun ting metho d prop osed in this pap er can be eectiv ely utilized to generate the exp ectations needed in order to compute suc h rules rather easily  These are some of the issues w e plan to address in the near future The motiv ation imp ortance and the need for in tegrating data mining tec hnology with relational databases has b een addressed in sev eral articles suc h as 12  13 They con vincingly argue that without suc h in tegration data mining tec hnology ma y not nd itself in a viable p osition in the y ears to come T o b e a successful and feasible to ol for the analysis of business data in relational databases suc htec hnology m ust b e made a v ailable as part of database engines and as part of its declarativ e query language Our prop osal for declarativ e mining bears merit since it sheds ligh t on ho w rst order databases can be mined in a declarativ e and pro cedure indep enden t w a y so that the optimization issues can b e delegated to the underlying database engine Once suc h argumen ts are accepted sev eral systems 9 


related issues b ecome prime candidates for immediate atten tion F or example traditionally database systems supp orted declarativ e querying without the necessit y to care ab out the pro ceduralit y of the queries In this pap er w eha v e actually demonstrated that asso ciation rule mining can b e view ed as a Datalog query  It is immediate that a direct mapping from the Datalog expressions presen ted in this pap er to SQL can be dev elop ed with no problem at all W e can then rely on ecien t database pro cessing of the query in an optimized fashion Hence w ecomeclose to the essence of the visions expressed b y the leading database researc hers and practioners 12  References 1 Rak esh Agra w al and Ramakrishnan Srik an t F ast algorithms for mining asso ciation rules in large databases In VLDB  pages 487{499 1994 2 Anon ymous A declarativ e metho d for mining c hisquare rules from deductiv e databases T ec hnical rep ort Departmen t of Computer Science Anon ymous Univ ersit y USA F ebruary 2001 3 C Beeri and R Ramakrishnan On the po w er of magic In Pr o c e e dings of the 6th A CM Symp osium on Principles of Datab ase Systems  pages 269{283 1987 4 Sergey Brin Ra jeev Mot w ani and Craig Silv erstein Bey ond mark et bask ets Generalizing asso ciation rules to correlations In Pr o c A CM SIGMOD  pages 265 276 1997 5 D Chimen ti et al The LD L system protot yp e IEEE Journal on Data and Know le dge Engine ering  2\(1 90 1990 6 Jia w ei Han Jian P ei and Yiw en Yin Mining frequen t patterns without candidate generation In Pr o c A CM SIGMOD  pages 1{12 2000 7 Marcel Holsheimer Martin L Kersten Heikki Mannila and Hann uT oiv onen A p ersp ectiv e on databases and data mining In Pr o c of the sixth A CM SIGKDD Intl Conf  pages 150{155 Mon treal Queb ec 1995 8 Flip Korn Alexandros Labrinidis Y annis Kotidis and Christos F aloutsos Ratio rules A new paradigm for fast quan tiable data mining In Pr o c of 24th VLDB  pages 582{593 1998 9 Brian Len t Arun N Sw ami and Jennifer Widom Clustering asso ciation rules In Pr o c of the 3th ICDE  pages 220{231 1997 10 Mengc hi Liu Relationlog At yp ed extension to datalog with sets and tuples In John Llo yd editor Pr oc e e dings of the 12th International L o gic Pr o gr amming Symp osium  pages 83{97 P ortland Oregon Decem ber 1995 MIT Press 11 Rosa Meo Giusepp e Psaila and Stefano Ceri An extension to SQL for mining asso ciation rules Data Mining and Know le dge Disc overy  2\(2 1998 12 Amir Netz Sura jit Chaudh uri Je Bernhardt and Usama M F a yy ad In tegration of data mining with database tec hnology  In Pr o c e e dings of 26th VLDB  pages 719{722 2000 13 Amir Netz Sura jit Chaudh uri Usama M F a yy ad and Je Bernhardt In tegrating data mining with SQL databases In IEEE ICDE  2001 14 Ra ymond T Ng Laks V S Lakshmanan Jia w ei Han and Alex P ang Exploratory mining and pruning optimizations of constrained asso ciation rules In Pr o c A CM SIGMOD  pages 13{24 1998 15 Jong So o P ark Ming-Sy an Chen and Philip S Y u An eectiv e hash based algorithm for mining asso ciation rules In Pr o c A CM SIGMOD  pages 175{186 1995 16 Karthic k Ra jamani Alan Co x Bala Iy er and A tul Chadha Ecien t mining for asso ciation rules with relational database systems In Pr o c e e dings of the International Datab ase Engine ering and Applic ations Symp osium  pages 148{155 1999 17 R Ramakrishnan D Sriv asta v a and S Sudarshan CORAL  Con trol Relations and Logic In Pr o c of 18th VLDB Confer enc e  pages 238{250 1992 18 Konstan tinos F Sagonas T errance Swift and Da vid Scott W arren XSB as an ecien t deductiv e database engine In Pr o c of the A CM SIGMOD Intl Conf  pages 442{453 1994 19 Sunita Sara w agi Shib y Thomas and Rak esh Agra w al In tegrating mining with relational database systems Alternativ es and implications In Pr o c A CM SIGMOD  pages 343{354 1998 20 Ashok a Sa v asere Edw ard Omiecinski and Shamk an tB Nav athe An ecien t algorithm for mining asso ciation rules in large databases In Pr o c of 21th VLDB  pages 432{444 1995 21 Pradeep Sheno y  Ja y an t R Haritsa S Sudarshan Gaura v Bhalotia Ma y ank Ba w a and Dev a vrat Shah T urb o-c harging v ertical mining of large databases In A CM SIGMOD  pages 22{33 2000 22 Abraham Silb ersc hatz Henry F Korth and S Sudarshan Datab ase System Conc epts  McGra w-Hill third edition 1996 23 Shib y Thomas and Sunita Sara w agi Mining generalized asso ciation rules and sequen tial patterns using SQL queries In KDD  pages 344{348 1998 24 J D Ullman Principles of Datab ase and Know le dgeb ase Systems Part I II  Computer Science Press 1988 25 Mohammed J Zaki Generating non-redundan t association rules In Pr o c of the 6th A CM SIGKDD Intl Conf  Boston MA August 2000 1 0 


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


