Descriptive Data Mining A Granular Model Anita Wasilewska Department of Computer Science Stony Brook University Stony Brook NY USA e-mail anita@cs.sunysb.edu Abstract  We present here a formal syntax and semantics for descriptive data mining We do so in terms of three abstract models Descriptive Semantic and Granular The Descriptive Model formalizes the syntactical concepts and properties of the data mining process Semantic Model formalizes its semantical properties The Granular Model establishes a relationship between the Descriptive and Semantic models in terms of a formal satisfaction relation I I NTRODUCTION One of the main goals of Data Mining is to 
provide comprehensible descriptions of information extracted from the data bases We are hence interested in building models for a descriptive data mining i.e the data mining which main goal is to produce a set of descriptions in a language easily comprehensible to the user These descriptions come in different forms In case of classiﬁcation problems it might be a set of characteristic or discriminant rules it might be a decision tree or a neural network with xed set of weights In case of association analysis it is a set of associations frequent item sets or association rules with accuracy parameters In case of cluster analysis it is a set of clusters each of which has its own description and a cluster name In case of approximate classiﬁcation by the Rough Set analysis it is usually a set of 
discriminant or characteristic rules with or without accuracy parameters or a set of decision tables Data Mining results are usually presented to the user in their descriptive i.e syntactic form as it is the most natural form of communication But the Data Mining process is deeply semantical in its nature We hence build our Granular Model on two levels syntactic and semantic We understand by syntax or syntactical concepts simple relations among symbols and expressions of a formal symbolic languages A symbolic language is a pair L  A  E   where A is an alphabet and E is the set of expressions of L The expressions of formal languages even if created with a speciﬁc meaning in mind do not carry themselves any meaning they 
are just nite sequences of certain symbols The meaning is being assigned to them by establishing a proper semantics Semantics for as given symbolic language L assigns a speciﬁc interpretation in some domain to all symbols and expressions of the language It also involves related ideas such as truth and model They are called semantical concepts to distinguish them from the syntactical ones The word model is used in many situations and has many meanings but they all reﬂect some parts if not all of its following formal meaning A structure M  called also an interpretation is a model for a set E 0 E of expressions of a formal language L if and only if every expression 
E E 0 it true in M  We present here three abstract models Descriptive Semantic and Granular All of them are abstract structures that allow us to formalize some general properties of Data Mining process and address the semantics-syntax duality inherent to any Data Mining process Descriptive Model formalizes the syntactical concepts and properties of the process Semantic Model formalizes its semantical properties Finally they form components of the Granular Model in which we establish a relationship between the Descriptive and Semantic models by providing a formal semantics for syntactical expressions of the language L of the Descriptive Model thus justifying a use of the word model Moreover within the Granular Model it is possible to 
provide a formal deﬁnition of Data Mining as the process of information generalization The notion of generalization is deﬁned in terms of granularity of steps of the process In the model the data is represented in a form of Knowledge Systems Each Knowledge System has a granularity associated with it and the process changes or not its granularity The notion of granularity is crucial for deﬁning some notions and components of the model hence the Granular Model name II S EMANTIC M ODEL The deﬁnition of a semantic model presented here is a simpler and more comprehensible version of  18  21  T he initial i n v estig ations of the s ubject appeared also in 10  3  8 T he notion o f a Kno w ledge System is central to the deﬁnition and development of the semantic 
model We deﬁne it formally in the next section We also use it to formalize a notion of a granule and granularity deﬁnition 2.3 that is central to our Granular Model A Knowledge Systems and Granularity The formal deﬁnitions of Information System Knowledge and Target Knowledge Systems and their granularity are as follows Deﬁnition 2.1 A Pawlak’s Information System is a system I  U A V A f   where U    is called a set of objects  A     V A    are called the set of 
attributes and values of of attributes  respectively f  U  A  V A is called an information function Deﬁnition 2.2 A Knowledge System based on I   U A V A f  is a system K  K  U  A,E,V A V E g  where 
978-1-4244-2352-1/08/$25.00 ©2008 IEEE 


the universe K  U  of K is a subset of the set P  U  of all subsets of the universe U of I  i.e K  U   P  U   E is a nite set of knowledge attributes k-attributes such that A  E    V E is a nite set of values of kattributes  g is a partial function called a knowledge function k-function g  P  A  E    V A  V E  is such that i g    x  U  x  A  f  ii  S P   a  A  S a   dom  g   g  S a   V A   iii  S P   e  E  S e   dom  g   g  S e   V E   We use the above notion of knowledge system to deﬁne the granules of the universe and the granularity of the system and hence when is needed the granularity of the data mining process Deﬁnition 2.3 Any set S P  U  i.e S  U is called a granule of U  The cardinality  S  of S is called a granularity of S Theset Gr K   S P  012 b   E  A   S b   dom  g   is called a granule universe of K  A number gr K  max  S   S  Gr K  is called a granularity of K  Observe that by conditions ii iii of deﬁnition 2.2 Gr K  K  U  and it justiﬁes its name The condition i of deﬁnition 2.2 says that when E    the k-function g is total on the set  x   x  U  A and  x  U  a  A  g   x  a  f  x a   We denote P 1  U   x   x  U   Let I  U A V A f   Any system K 1   P 1  U  A  V A   g   P 1  U  A,V A g   P 1  U  A,V A g  is called a target knowledge system based on I  In our Model we view data mining algorithms as certain operators We put all the above observations and deﬁnitions into a formal notion of a semantic model Deﬁnition 2.4 A Semantic Model is a system S M   P  U   K  G   where U    is the universe  K    is a set of knowledge systems called also data mining process states G    is the set of operators  Each operator p G is a partial function on the set of all data mining process states i.e p  KäK  The semantic model is always being built for a given application The target data is represented rst in a form the target database represented by the information system with the universe U  and then in the form of target knowledge system III D ESCRIPTIVE M ODEL Given a Semantic Model S M  P  U   K  G  We associate with it its descriptive counterpart deﬁned below Deﬁnition 3.1 A Descriptive Model is a system D M   L  E  DK  where L  A  E  is called a descriptive language  A is a countably inﬁnite set called the alphabet  E    and EA  is the set of descriptive expressions of L  DK    and DK  P  E  is a set of descriptions of knowledge states  As in a case of semantic model we build the descriptive model for a given application We deﬁne here only a general form of the model We assume however that whatever is the application the descriptions are always build in terms of attributes and values of the attributes some logical connectives some predicates and some extra parameters if needed For example a neural network with its nodes and weights can be seen as a formal description in an appropriate descriptive language and the knowledge states would represent changes in parameters during the neural network training process or a nal converged network We also identify for example a decision tree constructed by the classiﬁcation by Decision Tree algorithm with the set of discriminant rules obtained from the tree The commonly used descriptions have the form  a  v  to denote that the attribute a has a value v  but one might also use as it is often done a predicate form a  v  or a  x v  instead We deﬁne the components of our Descriptive Model D M in the following stages Stage1 For each K K  we deﬁne section III-A its own descriptive language L K  A K  E K   Stage2 For each K K  and descriptive expression F  E K  we deﬁne what does it mean that D satisﬁed in K  i.e we deﬁne deﬁnition 3.5 a satisfaction relation   K  Stage3 For each K K  and descriptive expression F  E K  we deﬁne what does it mean that D is true K  i.e   K D deﬁnition 3.8 Stage4 We use the satisfaction relation   K to deﬁne for each K K theset D K P  E K  of descriptions of its own knowledge deﬁnition 3.6 Stage5 We use the languages L K to deﬁne the descriptive language L deﬁnition 3.9 Stage6 We use the descriptive expressions E K of L K to deﬁne the set E of descriptive expressions of L deﬁnition 3.10 Stage7 We use the satisfaction relations   K to deﬁne the satisfaction relation   of our Granular Model GM deﬁnitions 4.1 and 4.2 respectively A K-Descriptive Language As the stage one of our construction of the Descriptive Model we deﬁne for each K K  its own description language L K  The language depends on the semantic model and the goal of the data mining process As we have said the descriptions produced by a data ming algorithms come is a different forms We build here a model for a descriptive data mining  i.e we assume that the descriptions are build from attributes and values of attributes and two logical connectives conjunction and implication The implication connective is needed to model the different kind of rules that 


are being mined by data mining algorithms discriminant and characteristic rules in classiﬁcation analysis association rules by association analysis or other rules obtained by hybrid systems Deﬁnition 3.2 For any K K of S M  K   P  U  A,E,V A V E g   we deﬁne the Descriptive Language of K  L K as L K  A K  E K   where A K is called an alphabet  E K the set of descriptive expressions such that E K  D K F K  for D K the set of descriptive formulas and F K the set of formulas of L K deﬁnition 3.3 The alphabet A K  VAR K           where    are logical connectives of L K  The set VAR K of variables of K is also called its set of atomic descriptions  We put VAR K  D A D E  where D A    a  v  a  A v  V A   D E    a  v  a  E v  V E   Atomic descriptions i.e the elements of VAR K  D A  D E represent minimal blocks of semantical description Elements of D A are atomic descriptions of minimal blocks build with use of the attributes of the initial target database Elements of D E are atomic descriptions of minimal blocks build with use of knowledge attributes used if any during the process of data mining We use them to deﬁne the sets of descriptions and formulas as follows Deﬁnition 3.3 The set D K of all descriptive formulas of L K is the set D K  A D K  E D K F K  where A D K E D K  F K are deﬁned as below The set A D K A K  is called the set of data attribute descriptions and is the smallest set such that the following conditions hold 1 D A  A D K data attribute atomic description 2 If D 1 D 2  A D K  then D 1  D 2  A D K  The set E D K A K  is called the set of knowledge attribute descriptions and is the smallest set such that the following conditions hold 1 D E  E D K knowledge attribute atomic description 2 If D 1 D 2  E D K  then D 1  D 2  E D K  We distinguish two categories of formulas one F A describes a certain knowledge the other F E  the uncertain or approximate knowledge The certain knowledge is expressed in our model in terms of attributes and values of attributes of the initial target data only The description of the approximate knowledge includes the extra parameters,describing properties of granules as deﬁned by the the knowledge attributes of K deﬁnition 2.2 Deﬁnition 3.4 The set F K of formulas of L K is a union of two sets of formulas F A  F E  i.e F K  F A F E  where F A  A D K  A R K and F E  AE D K  E R K are deﬁned below The set AE D K   D 1  D 2  D 1  A D K D 2  E D K  is the set of knowledge description formulas  with knowledge attribute descriptions depicting uncertainty measures The set A R K    D 1  D 2  D 1 D 2 D A  is the set of attribute rule-formulas depicting certain rules obtained during the data mining process The set E R K    D 1  D 2   D 3  D 1 D 2 D A D 3  D E  is the set of knowledge rule-formulas depicting rules with uncertainty measures described obtained by descriptions from D E  The formulas of the language L K are not yet the data mining rules  They only describe a syntactical form of the rules appropriate for a given application Formulas from F K become rules determined by K only when they do relate semantically to K  i.e reﬂect the properties of our initial target database In this case we say that they are true or true under the measures described by descriptions from D E  We deﬁne in the next section the notion of truthfulness as we always do via notion of satisﬁability B KSatisfaction and KTruth In this section we deﬁne deﬁnition 3.5 a satisfaction relation   K P  U  E K that establishes the relationship between descriptive expressions of the L K and what they semantically represent in K  For any  S F    K we say that S satisﬁes F in K and write it symbolically as S   K F We call our satisfaction relation a k-satisfaction as it represents a satisfaction relation relative to the system K Asthe next step we deﬁne the notion of K-truth deﬁnition 3.8 i.e we deﬁne what does it mean that a descriptive expression F is true in K  symbolically expressed as   K F Let K  K  U  A,E,V A V E g  and let L K  A K  E K  be the description language deﬁned by K deﬁnition 3.2 where E K  D K F K  Deﬁnition 3.5 A ksatisfaction relation   K P  U   E K is deﬁned by induction over level of complexity of any descriptive expression F E K as follows 1 F D K  i Let  a  v  D A be an atomic attribute description We deﬁne for any S P  U   for any a  A v  V A  S   K  a  v  if and only if  S a   dom  g  and g  S a  v  ii Let  e  v  D E is an atomic knowledge attribute description We deﬁne for any S P  U   for any e  E v  V E  S   K  a  v  if and only if  S e   dom  g  and g  S e  v  2 F F K  We extend   K to the set F A  A D K  A R K 


iii Let D  A D K and D  D 1    D n  We deﬁne for any S P  U   S   K D if and only if  1  i  n   S   K D i  iv Let  D 1  D 2   A R K  i.e D 1 D 2  A D K We deﬁne for any S P  U   S   K  D 1  D 2  if and only if S   K D 1 and S   K D 2  v Let  D 1  D 2   D 3  E R K  i.e  D 1  D 2   A R K and D 3 D E  We deﬁne for any S P  U   S   K  D 1  D 2   D 3 if and only if S   K  D 1  D 2  and S   K D 3  This ends the stage 2 of the deﬁnition of the Descriptive Model and we are ready for the stage 3 i.e to deﬁne the set of all descriptions of knowledge states Deﬁnition 3.6 Let K K theset D K P  E K  of descriptions of knowledge states of K is deﬁned as follows D K   F E K  012 S  Gr K  S   K F    Now we are ready to deﬁne for any formula F F K  a notion of F is true in K  We express it symbolically by   K F  This notion relates the satisfaction relation   K  i.e satisﬁability of formulas in the system K with the initial target data as out ultimate point of reference This connection is being established by the notion of the truth set deﬁned below Deﬁnition 3.7 For any attribute data description D  A D K  E D K the set S  D   x  U  D  is called a truth set for D  We deﬁne the notion   K F  F is true in K  by induction over level of complexity of any descriptive expression F F K as follows Deﬁnition 3.8 1 F  A D K  i For any any attribute data description D  A D K we deﬁne   K D if and only if S  D    K D  ii For any attribute description formula D  A D K we put   K D if and only if S  D    K D  2 F F A  iii For an attribute rule-formula  D 1  D 2   A R K  we deﬁne   K  D 1  D 2  if and only if S  D 1    K D 1 S  D 1    K D 2  and S  D 1   S  D 2  or  D 1   S  D 2      3 F  AE D K  iv For any any knowledge attribute description D 1  D 2  AE D K  D 1  A D K  D 2  E D K we deﬁne   K D 1  D 2 if and only if S  D 1   S  D 2    K D 1  4 F F E  v For an attribute rule-formula  D 1  D 2   D 3  AE R K  we deﬁne   K  D 1  D 2   D 3 if and only if S  D 1   S  D 3    K D 1  S  D 1   S  D 3    K D 2  and S  D 1   S  D 3   S  D 2   S  D 3  or  D 1   S  D 2   S  D 3      In case when F F E and   K F we say that F is true in K under the measures D measures deﬁned by conditions D  for a certain D  AE D K  If   K  D 1  D 2  and the condition S  D 1   S  D 2  holds then we call the formula  D 1  D 2  Ctrue in K or Ctrue under the measures D if   K  D 1  D 2   D  If   K  D 1  D 2  and the condition  D 1   S  D 2     holds then we call the formula  D 1  D 2  Dtrue in K  or D true under the measures D if   K  D 1  D 2   D  The notion of D-truth reﬂex the semantics needed to deﬁne the discriminant rules for the classiﬁcation analysis and Ctruth is needed for the characteristic rules hence the names As we have said before the knowledge attributes from the set E describe uncertainty measures for the granules S  K  U  The formulas that incorporate the knowledge attribute descriptions D  E D K can be only true in K under some uncertainty measures C Descriptive Language and Descriptive Model We have already constructed sections III-C 3.5 all subcomponents of the deﬁnition 3.1 of the Descriptive Model D M and now we are ready to deﬁne its two main components Deﬁnition 3.9 We deﬁne the language of D M as L  A  E   where A   A K  K K  and E   E K  K K for A K  E K deﬁned by deﬁnition 3.2 The set DK of all descriptions of knowledge states of the Semantic Model S M  P  U   K  G  is the following Deﬁnition 3.10 Let D K be the set of descriptions of knowledge states of K deﬁnition 3.6 We deﬁne the set DK of descriptions of knowledge states of S M as follows DK   D K  K K  This completes the deﬁnition of the Descriptive Model as asystem D M  L  E  DK  where L  A  E  is a descriptive language A an alphabet and E is the set of descriptive expressions deﬁnition 3.9 DK is a set of descriptions of knowledge states deﬁnition 3.10 IV G RANULAR M ODEL S ATISFACTION AND T RUTH We deﬁne our Granular Model as follows Deﬁnition 4.1 A Granular Model is a system GM  S M  D M     where S M is a Semantic Model deﬁnition 2.4 D M is a Descriptive Model deﬁnition 3.1   P  U  E is a satisfaction relation deﬁnition 4.2 U is the universe of S M and E is the set of descriptions deﬁned by the D M  The satisfaction relation   establishes the relationship between expressions of the Semantic and Descriptive Models We have hence established formally the syntax-semantics duality of the data mining process All the components of GM except the satisfaction relation have already been deﬁned As 


the last step we deﬁne the satisfaction relation and the notion of truth in GM as follows Deﬁnition 4.2 For any S P  U  and for any F E  S   F if and only if 012 K K  S   K F   Deﬁnition 4.3 We say that F E is true in GM symbolically   F  if and only if 012 K K    K F   V F UTURE D IRECTIONS The models presented here set a general framework for future foundational investigations They can be carried on three levels One the most general would deal with further developments within the Granular Model The second a more speciﬁc one would deal with applications of the methodology developed within the Granular Model to speciﬁc domains On this level one would build Semantic Descriptive and a Granular models for different Data Mining domains i.e build and examine speciﬁc models for classiﬁcation clustering and for association analysis Finally the most speciﬁc third level would deal with building models for basic descriptive data mining algorithms decision trees and rough sets classiﬁcation association and classiﬁcation by association and clustering The general methodology of analysis of the data mining process developed for the Granular Model can hence serve as unifying language in which different data mining domains and algorithms can be studied discussed and compared VI B IBLIOGRAPHY R EFERENCES  P ete Chapman et al CRISP-DM 1.0 Step-by-step Data Mining CRISPDM Consortium:1.0 August 2000  S alv atore Greco B enedetto Matarazzo R oman Slo w inski Jerzy S tefanowski Importance and Interaction of Conditions in Decision Rules Proceedings of Third International RSCTC’02 Conference Malvern PA USA October 2002 pp 255-262 Springer Lecture Notes in Artiﬁcial Intelligence  M  H adjimichael A W a sile wska A Hierarchical Model for Information Generalization Proceedings of the 4th Joint Conference on Information Sciences Rough Sets Data Mining and Granual Computing RSDMGrC’98 North Carolina USA vol.II pp 306–309 4 J H a n M K a m b e r  Data Mining Concepts and Techniques Morgan Kauffman 2000  M  I nuiguchi T  T anino Classiﬁcation versus Approximation oriented Generalization of Rough Sets Bulletin of International Rough Set Society Volume 7 No 1/2 2003  J  K omoro w ski Modelling Biological Phenomena with Rough Sets Proceedings of Third International Conference RSCTC’02  Malvern PA USA October 2002 p13 Springer Lecture Notes in Artiﬁcial Intelligence  T Y  L in Database Mining on Derived Attributes Proceedings of Third International Conference RSCTC’02  Malvern PA USA October 2002 pp 14 32 Springer Lecture Notes in Artiﬁcial Intelligence  J uan F Martinez Ernestina M enasalv as A nita W a sile wska C o v adonga Fern  andez M Hadjimichael Extension of Relational Management System with Data Mining Capabilities Proceedings of Third International Conference RSCTC’02 Malvern PA USA October 2002 pp 421424 Springer Lecture Notes in Artiﬁcial Intelligence  E rnestina M enasalv as A nita W a sile wska C o v adonga F ern  andez The lattice structure of the KDD process Mathematical expression of the model and its operators Fundamenta Informaticae special issue with International Journal of Information Systems 2001 pp 48 62  Ernestina M enasalv as A nita W a sile wska C o v adonga F ern  andez Juan F Martinez Data MiningA Semantical Model Proceedings of 2002 World Congres on Computational Intelligence Honolulu Hawai USA May 1117 2002 pp 435 441  P a wlak Z  Information systems theoretical foundations Information systems 6 1981 pp 205-218  P a wlak Z  Rough Setstheoretical Aspects Reasoning About Data Kluwer Academic Publishers 1991  F a yyad Piatetsk y Shapiro S myth From Data Mining to Knowledge Discovery An Overview  Advances in Knowledge Discovery and Data Mining Fayyad Piatetsky-Shapiro Smyth Uthurusamy editors AAAI Press  The MIT Press Menlo Park CA 1996 pp 1-34  Polk o w ski L ech Rough SetsMathematical Foundations PhysicaVerlag A SpringerVerlag Company 2002  Colin Shearer  The CRISP-DM Model The New Blueprint for Data Mining  Journal of Data Warehousing Volume 5 Number 4 Fall 2000 pp 13-22  Sk o w ron A Data Filtration A Rough Set Approach Proceedings de Rough Sets Fuzzy Sets and Knowledge Discovery 1993 pp 108-118  A W a sile wska E rnestina M enasalv as Ruiz Mar  a C Fern  andez-Baizan A Model for RSMD Implementation 1st International Conference on Rough Sets and Current Trends in Computing RSCTC’98 June 22 26 1998 Warsaw Poland pp 186193  Anita W a sile wska E rnestina M enasalv as Data Preprocessing and Data Mining as Generalization Process Proceedings of ICDM’04 The Fourth IEEE International Conference on Data Mining Brighton UK Nov 1-4 2004 pp 133-137  Anita W a sile wska E rnestina M enasalv as Data Mining Operators Proceedings of ICDM’04 The Fourth IEEE International Conference on Data Mining Brighton UK Nov 1-4 2004 pp 209-214  Anita W a sile wska E rnestina M enasalv as C hristelle Scharf f Uniform Model for Data Mining Proceedings of FDM05 Foundations of Data Mining in ICDM2005 Fifth IEEE International Conference on Data Mining Austin Texas Nov 27-29 2005 pp 19-27  A W a sile wska E rnestina M enasalv as Ruiz Data Mining as Generalization A Formal Model  book chapter in Foundations and Novel Approaches in Data Mining T.Y Lin S Ohsuga C J Liau and X Hu  editors Springer 2006 Studies in Computational intelligence 9 pp 99-126  A W a sile wska E rnestina M enasalv as Ruiz Data Preprocessing and Data Mining as Generalization  book chapter in Data Mining Foundations and Practice  Tsau Young Lin Ying Xie Anita Wasilewska ChurnJung Liau editors Springer Studies in Computational Intelligence to appear  A W a sile wska E rnestina Menasalv as Ruiz A Classiﬁcation Model Syntax and Semantics for Classiﬁcation Proceedings of 10thInternational RSFDGrC2005 Conference Regina Canada August/September 2005 vol 2 pp 59-68 Springer Lecture Notes in Artiﬁcial Intelligence  W o jciech Ziark o  X ue Fei VPRSM Approach to WEB Searching Proceedings of Third International RSCTC’02 Conference Malvern PA USA October 2002 pp 514522 Springer Lecture Notes in Artiﬁcial Intelligence  W o jciech Ziark o  Variable Precision Rough Set Model Journal of Computer and Systen Sciences Vol.46 No.1 pp 39-59 1993  J.T  Y a o Y  Y  Y a o Induction of Classiﬁcation Rules by Granular Computing Proceedings of Third International RSCTC’02 Conference Malvern PA USA October 2002 pp 331-338 Springer Lecture Notes in Artiﬁcial Intelligence 


0 0.05 0.1 0.15 0.2 0.25 0.3 0 20 40 60 80 100 Error k Real data set Chess  Mushroom   Votes  0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0 100 200 300 400 500 600 700 800 Error k Synthetic data set tau=0.10  tau=0.05  Figure 5 Support estimation error increasing k  high when k is low We now turn our attention to the synthetic data set where we built models with much higher k  In this case error shows a slower rate of decrease The trend also seems asymptotic We can also see there is a clear gap between 002 0  05 and 002 0  10  error roughly grows 100 when 002 decreases by 50 Interestingly enough results are much better for real data sets than for the synthetic data set Error decreasing minimum support Figure 6 shows error growth as 002 decreases The left graph analyzes error for real data sets We can see error growth shows different behavior for each data set Error growth is slow for the Votes data set Error grows almost linearly for the Chess data set Error grows fast for the Mushroom data set The trend indicates we need to build more accurate models with higher k for Mushroom probably not for Chess and not necessary for the Votes data set The right graph analyzes error growth for the synthetic data set Theﬁrstmodelat k  800 is twice as accurate as k  400  We can see error grows linearly for high 002 values but it start growing faster at 002 0  1  The trend indicates the growth is not linear but it does not seem exponential 4.4 Comparing Speed and Scalability We rst compare our proposal versus the standard algorithm to mine association rules We then study time complexity and scalability Comparing clustering and A-priori Table 2 compares the efﬁci ency of the model with the standard A-priori algorithm We must stress our proposal does not intend to substitute fast association rule algorithms Table 2 Comparing model and A-priori 002 from model clustering+model A-priori 0.20 1 1672 24 0.15 1 1672 43 0.10 1 1672 156 0.05 3 1674 645 0.02 11 1683 3347 0.01 36 1708 14806 32 16 b u t w e i ncl ude t h es e c ompari s ons t o pro v i d e a rel ative performance benchmark We used the synthetic data with n 1 M  The clustering model used had the number of clusters set to k  100  These times include the time to compute exact support on a nal pass The rst column varies 002 the minimum support threshold The second column shows the time to discover frequent itemsets from the model excluding the time to compute the model The third column adds the time to compute the clustering model and the time to mine frequent itemsets Finally the fourth column shows the time for the traditional algorithm As can be seen the clustering model r epresents an efﬁcient mechanism to produce all frequent itemsets assuming the model is already tuned and stored That is we assume the model is computed a few times or even once The second column shows clustering the data set takes most of the time In this case the standard algorithm is faster at high support levels but the model becomes faster at low support levels Notice the third column represents a pessimistic case in which the model is recomputed every time Finally we can see the A-priori algorithm suffers scalability problems due to the exponential growth of patterns The basic reason the model is faster is because it uncovers long itemsets and it is efﬁciently manipulated in main memory 
614 
614 


0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0 0.1 0.2 0.3 0.4 0.5 0.6 Error tau Real data set Chess  Mushroom   Votes  0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0 0.05 0.1 0.15 0.2 Error tau Synthetic data set k=400  k=800  Figure 6 Support estimation error decreasing 002  0 100 200 300 400 500 600 700 0 200 400 600 800 1000 1200 1400 1600 Time in seconds n x 1000 Data set size d= 100  d=1000  0 50 100 150 200 250 300 0 20 40 60 80 100 120 140 160 Time in seconds k Number of clusters d= 100 n=100k  d=1000 n=100k  Figure 7 Time complexity for clustering large data sets with K-means 
615 
615 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


