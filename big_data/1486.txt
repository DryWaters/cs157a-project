Big Data: Issues and Challenges Moving Forward \nStephen Kaisler \ni_SW Corporation \nskaisler1@comcast.net\nFrank Armour \nAmerican University \nfjarmour@gmail.com\nJ. Alberto Espinosa \nAmerican University nalberto@american.edu\nWilliam Money, \nGeorge Washington \nUniversity\nwmoney@gwu.edu\nAbstract\nBig data refers to data volumes in the range of \nexabytes \(1018 storage systems and \nprocessing systems. Data, information, and knowledge \nare being created and collected at a rate that is \nrapidly approaching the exabyte/year range. But, its \ncreation and aggregation are accelerating and will \napproach the zettabyte/year range within a few years. \nVolume is only one aspect of big data; other attributes \nare variety, velocity, value, and complexity. Storage \nand data transport are technology issues, which seem \nto be solvable in the near-term, but represent long-\nterm challenges that require research and new \nparadigms. We analyze the issues and challenges as \nwe begin a collaborative research program into \nmethodologies for big data analysis and design.\n1. Introduction\n The concept of big data has been endemic within \ncomputer science since the earliest days of computing. \n“Big Data” originally meant the volume of data that \ncould not be processed \(efficiently tools. Each time a new storage \nmedium was invented, the amount of data accessible \nexploded because it could be easily accessed. The \noriginal definition focused on structured data, but most \nresearchers and practitioners have come to realize that \nmost of the world’s information resides in massive, \nunstructured information, largely in the form of text \nand imagery. The explosion of data has not been \naccompanied by a corresponding new storage medium. \n We define “Big Data” as the amount of data just \nbeyond technology’s capability to store, manage and \nprocess efficiently. These imitations are only \ndiscovered by a robust analysis of the data itself, \nexplicit processing needs, and the capabilities of the \ntools \(hardware software, and methods lead to a recommendation that \nnew tools need to be forged to perform the new tasks. \nAs little as 5 years ago, we were only thinking of tens \nto hundreds of gigabytes of storage for our personal \ncomputers. Today, we are thinking in tens to hundreds \nof terabytes. Thus, big data is a moving target. Put  \nanother way, it is that amount of data that is just \nbeyond our immediate grasp, e.g., we have to work \nhard to store it, access it, manage it, and process it.  \n The current growth rate in the amount of data \ncollected is staggering. A major challenge for IT \nresearchers and practitioners is that this growth rate is \nfast exceeding our ability to both: \(1 2 relevant meaning for \ndecision making. In this paper we identify critical \nissues associated with data storage, management, and \nprocessing. To the best of our knowledge, the research \nliterature has not effectively addressed these issues,  \n1.1 Importance of Big Data \n In August 2010, the White House, OMB, and nOSTP proclaimed that Big Data is a national challenge \nand priority along with healthcare and national nsecurity [1]. The National Science Foundation, the \nNational Institutes of Health, the U.S. Geological nSurvey, the Departments of Defense and Energy, and \nthe Defense Advanced Research Projects Agency \nannounced a joint R&D initiative in March 2012 that \nwill invest more than $200 million to develop new big \ndata tools and techniques. Its goal is to advance our \n“…understanding of the technologies needed to \nmanipulate and mine massive amounts of information; \napply that knowledge to other scientific fields “as well \nas address the national goals in the areas of health \nenergy defense, education and researcher” [14]. \n The government’s emphasis is on how big data \ncreates “value” – both within and across disciplines \nand domains. Value arises from the ability to analyze \nthe data to develop actionable information. Our survey \nof the technical literature suggests five generic ways \nthat big data can support value creation for \norganizations \(see Table 1 10.1109/HICSS.2013.645\n9945\nTable 1. Value Created from Big Data \nCreating transparency by making big data openly \navailable for business and functional analysis \(quality, \nlower costs, reduce time to market, etc nSupporting experimental analysis in individual locations \nthat can test decisions or approaches, such as specific \nmarket programs \nAssisting, based on customer information, in defining \nmarket segmentation at more narrow levels  \nSupporting Real-time analysis and decisions based on \nsophisticated analytics applied to data sets from \ncustomers and embedded sensors \nFacilitating computer-assisted innovation in products \nbased on embedded product sensors indicating customer \nresponses\nWhile the government seems to assume that big \ndata users will be more successful, more productive, \nand have differential impacts across many industries, \ntheir underlying concern seems to be a lack of tools \nand a lack of trained personnel to properly work with \nbig data. Others suggest that the analysis of generic \nsequences, social media interactions, health records nphone logs, and government records, will not create \nbetter tools and services, but may create a new set of nprivacy incursions and invasive and unwanted \nmarketing.[3] these conflicting concerns drive \ncompeting 


visions of how to deal with big data. \n An example from the medical field illustrates how \nand why big data and new analytics may be truly \nbeneficial. Fox [6] describes how current data in a \npatient’s medical record and current health situation is \nused to plan and target patient participation in wellness \nand disease management programs. Fox asserts that \ndoctors \(and insurance companies than the disease\(s impacts a patient’s choice to \nparticipate, level of engagement, and appropriateness \nfrom public data associating behavior and health data \n– beyond that solely related to a patient’s medical \ncondition”. Thus programs may determine how to \nbetter target, retain, and treat people in their programs \nby leveraging predictive models that could assist \ndoctors and case managers who seek to positively \nimpact the behavior of patients with chronic health \ndisease.\n1.2 Big Data Characteristics \n One view, espoused by Gartner’s Doug Laney \ndescribes Big Data as having three dimensions: \nvolume, variety, and velocity. Thus, IDC defined it n“Big data technologies describe a new generation of \ntechnologies and architectures designed to \neconomically extract value from very large volumes of \na wide variety of data, by enabling high-velocity \ncapture discovery, and/or analysis.” [8] Two other \ncharacteristics seem relevant: value and complexity.  \nWe summarize these characteristics in table 2. \n1.3 Big Data – Where is it? \n Big data surrounds us, although we may not \nimmediately realize it \(see Table 3 most \nof us don’t deal with large amounts of data in our \neveryday lives. Lacking this immediate experience we \noften fail to understand both opportunities as well \nchallenges  presented by big data. Because of these ncharacteristics,there are currently a number of issues \nand challenges in addressing these characteristics ngoing forward. \n1.4  Issues\n We suggest there are three fundamental issue \nareas that need to be addressed in dealing with big \ndata: storage issues, management issues, and \nprocessing issues. Each of these represents a large set \nof technical research problems in its own right. \nTable 2. Big Data Characteristics \nData Volume Data volume measures the \namount of data available to an organization, \nwhich does not necessarily have to own all of \nit as long as it can access it. As data volume \nincreases, the value of different data records \nwill decrease in proportion to age, type, \nrichness, and quantity among other factors. \nData Velocity: Data velocity measures the \nspeed of data creation, streaming, and \naggregation. eCommerce has rapidly \nincreased the speed and richness of data used \nfor different business transactions \(for \nexample, web-site clicks velocity \nmanagement is much more than a bandwidth \nissue; it is also an ingest issue extract-\ntransform-load.  \nData Variety: Data variety is a measure of the \nrichness of the data representation – text, \nimages video, audio, etc. From an analytic \nperspective, it is probably the biggest obstacle \nto effectively using large volumes of data. \nIncompatible data formats, non-aligned data nstructures, and inconsistent data semantics \nrepresents significant challenges that can lead \nto analytic sprawl.  \n9956\nData Value: Data value measures the \nusefulness of data in making decisions. It has \nbeen noted that “the purpose of computing is \ninsight, not numbers”. Data science is \nexploratory and useful in getting to know the \ndata, but “analytic science” encompasses the \npredictive power of big data. \nComplexity Complexity measures the degree \nof interconnectedness \(possibly very large structures \nsuch that a small change \(or combination of \nsmall changes very large changes or a small change \nthat ripple across or cascade through the \nsystem and substantially affect its behavior, or \nno change at all.  \n1.4.1 Storage and Transport Issues \n The quantity of data has exploded each time we \nhave invented a new storage medium. What is \ndifferent about the most recent explosion due largely \nto social media – is that there has been no new storage \nmedium. Moreover, data is being created by everyone \nand everything \(e.g., devices, etc scientist, \njournalists, writers, etc.  \nTable 3. Some Examples of Big Data \nData Set/Domain Description nLarge Hadron \nCollider/Particle \nPhysics \(CERN n\(Cisco 144 \nper tweet. \nHuman Digital \nUniverse \n1.7 Zbytes \(2011 Gantz \nand Reinsel 2011 nBritish Library UK \nWebsite Crawl \n~ 110 TBytes per domain \ncrawl to be archived \nOther RFIDS, smart electric nmeters, 4.6 billion \ncamera phones w/ GPS \n Current disk technology limits are about 4 \nterabytes per disk So, 1 exabyte would require 25,000 \ndisks. Even if an exabyte of data could be processed \non a single computer system, it would be unable to \ndirectly attach the requisite number of disks. Access to \nthat data would overwhelm current communication \nnetworks. Assuming that a 1 gigabyte per second \nnetwork has an effective sustainable transfer rate of \n80%, the sustainable bandwidth is about 100 \nmegabytes. Thus, transferring an exabyte would take \nabout 2800 hours, if we assume that a sustained \ntransfer could be maintained. It would take longer to \ntransmit the data from a collection or storage point to a \nprocessing point than it would to 


actually process it! \n Two solutions manifest themselves. First, process \nthe data “in place” and transmit only the resulting \ninformation. In other words, “bring the code to the \ndata”, vs. the traditional method of bring the data to \nthe code.” Second, perform triage on the data and \ntransmit only that data which is critical to downstream \nanalysis. In either case, integrity and provenance \nmetadata should be transmitted along with the actual \ndata. \n1.4.2 Management Issues \n Management will, perhaps, be the most difficult nproblem to address with big data. This problem first \nsurfaced a decade ago in the UK eScience initiatives nwhere data was distributed geographically and \n“owned” and “managed” by multiple entities.  \nResolving issues of access, metadata, utilization, \nupdating, governance, and reference \(in publications major stumbling blocks. \n Unlike the collection of data by manual methods, \nwhere rigorous protocols are often followed in order to \nensure accuracy and validity, digital data collection is \nmuch more relaxed. The richness of digital data \nrepresentation prohibits a bespoke methodology for \ndata collection. Data qualification often focuses more \non missing data or outliers than trying to validate \nevery item. Data is often very fine-grained such as \nclickstream or metering data. Given the volume, it is \nimpractical to validate every data item: new \napproaches to data qualification and validation are \nneeded. \n The sources of this data are varied - both \ntemporally and spatially, by format, and by method of \ncollection. Individuals contribute digital data in \nmediums comfortable to them: documents, drawings, \npictures, sound and video recordings, models, software \nbehaviors, user interface designs, etc – with or without \nadequate metadata describing what, when, where, \nwho, why and how it was collected and its \nprovenance. Yet, all this data is readily available for \ninspection and analysis. \n Going forward, data and information provenance \nwill become a critical issue. JASON has noted [10] \nthat “there is no universally accepted way to store raw \ndata reduced data, and … the code and parameter \nchoices that produced the data.” Further, they note: \n9967\n“We are unaware of any robust, open source, platform-\nindependent solution to this problem.” As far as we \nknow, this remains true today. To summarize, there is \nno perfect big data management solution yet. This \nrepresents an important gap in the research literature \non big data that needs to be filled. \n1.4.3 Processing Issues \n Assume that an exabyte of data needs to be \nprocessed in its entirety. For simplicity, assume the \ndata is chunked into blocks of 8 words, so 1 exabyte = \n1K petabytes. Assuming  a processor expends 100 \ninstructions on one block at 5 gigahertz, the time \nrequired for end-to-end processing would be 20 \nnanoseconds. To process 1K petabytes would require a \ntotal end-to-end processing time of roughly 635 years. \nThus, effective processing of exabytes of data will \nrequire extensive parallel processing and new analytics \nalgorithms in order to provide timely and actionable \ninformation.\n2. Dynamic Design Challenges \n There are numerous challenges requiring long-\nterm research to working with big data. Stonebreaker \nand Hong [18] argue that the design for the systems \nand components that work with big data will require \nan understanding of both the needs of the users and the \ntechnologies that can be used to solve the problem \nbeing investigated – i.e., not all big data and its \nrequirements are the same. In this instance, since the \ndata that is newly created envisioned and collected interfaces, graphics, and icons; \napplication organization; and conceptual models, \nmetaphors, and functionality. Because the end users \nwill not often be the system designers, this presents an \nadditional design challenge. \n  There are unknown challenges that will arise with \neach increase in scale and development of new \nanalytics. Some of these challenges will be \nintractactable with the tools and techniques at hand. \nWe believe these challenges to be just “over the \nhorizon” with the next jump to zettabyte-size data sets. \n2.1 Data Input and Output Processes \n A major issue raised in big data design is the \noutput process. Jacobs [9 summarized the issue very \nsuccinctly – “…its easier to get the data in than out.” \nHis work shows that data entry and storage can be \nhandled with processes currently used for relational \ndatabases. But, the tools designed for transaction \nprocessing that add, update, search for, and retrieve \nsmall to large amounts of data are not capable of \nextracting the huge volumes and cannot be executed in \nseconds or a few minutes.  \n How to access very large quantities of semi- or \nunstructured data, and how to utilize as yet unknown \ntool designs is not known. It is clear the problem may \nneither be solved by dimensional modeling and online nanalytical processing \(OLAP nthe data into memory. Technical considerations that \nmust be factored into the design include the ratio of nthe speed of sequential disk reads to the speed of \nrandom memory access. The current technology shows \nthat random access to memory is 150,000 times slower \nthan sequential access.  Joined tables, an assumed nrequirement of associating large volumes of disparate \nbut somehow related data, perhaps by observations nover time alone, will come at further huge \nperformance costs. \(Jacobs, 2009 An emerging challenge for big data users is \n“quantity vs. quality”. As users acquire and have \naccess to more 


data \(quantity Perhaps, because they believe \nthat with enough data, they will be able to perfectly \nexplain whatever phenomenon they are interested in.  \n Conversely, a big data user may focus on quality \nwhich means not having all the data available, but \nhaving a \(very precise and high-valued \nconclusions. \(see Table 3 nthe level of precision that the user requires?  For \nexample, trend analysis may not require the precision nthat traditional DB systems provide, but which \nrequires massive processing in a Big Data \nenvironment. This problem also manifests itself in the \n“speed versus scale” challenge discussed below. \nTable 4. Some Quantity and Quality Challenges \nHow do we decide which data is irrelevant \nversus selecting the most relevant data nHow do we ensure that all data of a given type \nis reliable and accurate? Or, maybe just \napproximately accurate? \nHow much data is enough to make an estimate \nor prediction of the specific probability and naccuracy of a given event? \nHow do we assess the “value” of data in \ndecision making? Is more necessarily better? \n9978\n2.3 Data Growth versus Data Expansion \n Most organizations expect their data to grow over \ntheir lifetime as the organization increases its services, \nits business and business partners and clients, its nprojects and facilities, and its employees.  Few \nbusinesses adequately consider data expansion, which noccurs when the data records grow in richness, when \nthey evolve over time with additional information as nnew techniques, processes and information demands \nevolve. Most data is time-varying – the same data \nitems can be collected over and over with different \nvalues based on a timestamp. Much of this data is \nrequired for retrospective analysis – particularly that \nwhich is used in estimative and predictive analytics.\n2.4  Speed versus Scale \n As the volume of data grows, the “big” may \nmorph from the scale of the data warehouse to the namount of data that can be processed in a given \ninterval, say 24 hours. Gaining insight into the \nproblem being analyzed is often more important than \nprocessing all of the data. Time-to-information is \ncritical when one considers \(near identifiers \(RFIDs – used to read electronic \ndata wirelessly, such as with EZPass tags sensors. An organization must determine how \nmuch data is enough in setting its processing interval \nbecause this will drive the processing system \narchitecture, the characteristics of the computational \nengines, and the algorithm structure and \nimplementation. \n That said, another major challenge is data \ndissemination. The bottleneck is the communications \nmiddleware. While communication hardware speeds \nare increasing with new technologies, message \nhandling speeds are decreasing only slowly. The \ncomputation versus communication dichotomy has not\nbeen fully resolved by large data store systems such as \nHDFS or Accumulo for exabyte-sized data sets. \n2.5 Structured versus Unstructured Data\n Translation between structured data with well-\ndefined data definitions \(often in tables e.g., free ntext, graphics, multi-media, etc nemergence of non-relational, distributed, analytics-\noriented databases such as NoSQL, MongoDB, SciDB \nand linked data DBs provides dynamic flexibility in \nrepresenting and organizing information.    \n Unlike a data set, a data source has no beginning \nand no end. One begins collecting and continues to do \nso until one has enough data or runs out of patience or \nmoney or both. The data streams in with varied speed, \nfrequency volume, and complexity. The data stream \nmay dynamically change in two ways: \(1 necessitating changes in the way the \nanalytics process the data, or \(2 nnecessitating different analytics to process it. A \ncomplicating factor is the implicit assumption that the ndata streams are well-behaved and that the data arrive \nmore or less in order. In reality, data streams are not so \nwell-behaved and often experience disruptions and \nmixed-in data, possibly unrelated, to the primary data \nof interest. There is a need to rethink data stream \nprocessing to, perhaps, emphasize continuous nanalytics over discontinuous and distributed data \nstreams. \n2.6 Data Ownership \n Data ownership presents a critical and ongoing \nchallenge, particularly in the social media arena. \nWhile petabytes of social media data reside on the \nservers of Facebook, MySpace, and Twitter, it is not \nreally owned by them \(although they may contend so \nbecause of residency data. This \ndichotomy will have to be resolved in court. Kaisler, \nMoney and Cohen [12] addressed this issue with \nrespect to cloud computing as well as other legal \naspects that we will not delve into here. \n With ownership comes a modicum of \nresponsibility for ensuring its accuracy. This may not \nbe required of individuals, but almost certainly is so of \nbusinesses and public organizations. However, \nenforcement of such an assumption \(much less a \npolicy social media purveyor has the \nresources to check every data item on its servers.  \n With the advent of numerous social media sites, \nthere is a trend in big data analytics towards mixing of \nfirst-party 


reasonably verified data, with public and \nthird-part external data, which has largely not been \nvalidated and verified by any formal methodology. \nThe addition of unverified data: compromises the \nfidelity of the dataset; may introduce non-relevant \nentities; and may lead to erroneous linkages among \nentities. As a result, the accuracy of conclusions drawn \nfrom processing this mixed data varies widely.  \n9989\nTable 5. Some Big Data Ownership Challenges \nWhen does the validity of \(publicly available is expired, should the data be \nremoved from public-facing websites or data sets? \nWhere and how do we archive expired data? \nShould we archive it? \nWho has responsibility for the fidelity and \naccuracy of the data? Or it a case of user beware? \n2.7 Compliance and Security \n In certain domains, such as social media and \nhealth information, as more data is accumulated about \nindividuals , there is a fear that certain organizations \nwill know too much about individuals. For example, \ndata collected in electronic health record systems in naccordance with HIPAA/HITECH provisions is \nalready raising concerns about violations of one’s \nprivacy Developing algorithms that randomize \npersonal data among a large data set enough to ensure \nprivacy is a key research problem. \n Perhaps the biggest threat to personal security is \nthe unregulated accumulation of data by numerous \nsocial media companies. This data represents a severe \nsecurity concern, especially when many individuals so \nwillingly surrender such information. Questions of \naccuracy, dissemination, expiration, and access \nabound. For example, the State of Maryland became \nthe first state to prohibit by law employers asking for \nFacebook and other social media passwords during \nemployment interviews and afterwards. \n International Data Corporation \(IDC individual which has been collected, \norganized, and perhaps analyzed, to form an aggregate \n“picture” of the individual. It is the information about \nyou that is much greater than the information you \ncreate and/or release about yourself. A key problem is \nhow much of this information – either original or \nderived – do we want to remain private? \n Clearly, some big data must be secured with \nrespect to privacy and security laws and regulations. \nIDC suggested five levels of increasing security [8]: \nprivacy, compliance-driven custodial, confidential, \nand lockdown. Further research is required to clearly \ndefine these security levels and map them against both \ncurrent law and current analytics. For example, in \nFacebook, one can restrict pages to ‘friends’. But, if \nFacebook runs an analytic over its databases to extract \nall the friend’s linkages in an expanding graph, at what \nsecurity level should that analytic operate? e.g., how \nmany of an individual’s friends should be revealed by \nsuch an analytic at a given level if the individual \(has \nthe ability to and All \nData”\n Not all data is created equal; some data is more \nvaluable than other data – temporally spatially, \ncontextually, etc. Previously, storage limitations \nrequired data filtering and deciding what data to keep. \nHistorically, we converted what we could and threw \nthe rest away \(figuratively, and often literally combining data from multiple \nsources about individuals into a single \nrepository?  \nDo compliance laws \(such as HIPAA and regulations should exist for \nprohibiting the collection and storage of data \nabout individuals – either centralized or \ndistributed? \nShould an aggregation of data be secured at a \nhigher level than its constituent elements? \nGiven IDC’s security categorization, what \npercentage of data should reside in each \ncategory? What mechanisms will allow data to \nmove between categories? \n The concept of “quantitative qualitative ncomputation” suggests that we need new mechanisms \nfor converting latent, unstructured text, image or audio ninformation into numerical indicators to make them \ncomputationally tractable. With big data and our nenhanced analytical capabilities, the trend is towards \nkeeping everything with the assumption that analytical \nsignificance will emerge over time. However, at any \npoint in time the amount of data we need to analyze \nfor specific decisions represents only a very small \nfraction of all the data available in a data source and \nmost data will go un-analyzed. \n2.9 Distributed Data and Distributed \nProcessing\n The allure of hardware replication and system \nexpandability as represented by cloud computing \nalong with the MapReduce and Message Passing \nInterface \(MPI utilizing a \ndistributed approach. Even with this approach, \nsignificant performance degradation can still occur \n9991000\nbecause of the need for communication between the \nnodes. \nTable 7. Some Data Value Challenges nFor a given problem domain, what is the minimum \ndata volume required for descriptive, estimative npredictive and prescriptive analytics and decision \nmodeling with a specified accuracy? \nFor a given data velocity, how do we update our \ndata volume to ensure continued accuracy and \nsupport \(near processing? \nFor a given problem domain, what constitutes an \nanalytic science for non-numerical data? \n“What if we know everything?” – What do we do \nnext? \n An open research question is, which big data \nproblems are 


MapReducible”? Specialized distributed \nalgorithms, not necessarily based on the MapReduce \nor MPI paradigms may be required to complete tasks \nto minimize the communication needed between the \nnodes. Finally, if distributed processing is viewed as \nan alternative, the overall reliability of the system will \nneed to be increased to assure that no simple node or \nrequired communication fails. Both Google’s and \nHadoop’s MapReduce systems have taken initial steps \nto ensure fault-tolerance for hardware, system \nsoftware, and to some extent, for the algorithms in \nuser application software. \n3. Processing Big Data: Analytics \nChallenges n Processing big data is a major challenge, perhaps \nmore so than the storage or management problem. \nThere are many types of analytics: descriptive, \nestimative, predictive, and prescriptive, leading to \nvarious types of decision and optimization models. \nSome common business analytics are depicted in \nFigure 1. Kaisler [11 presents another decomposition \nof analytics into 16 categories based on the types of \nproblems to be addressed, including econometric \nmodels, game theory, control theory, evolutionary \ncomputation, and simulation models. The new normal \nis agile, advanced, predictive analytics that adapt \nreadily to changing data sets and streams and yield \ninformation and knowledge to improve services and \noperations across academia, industry, and government.  \n3.1 Scaling \n A critical issue is whether or not an analytic \nprocess scales as the data set increases by orders of \nmagnitude. Every algorithm has a “knee” – the point at \nwhich the algorithm’s performance ceases to increase \nlinearly with increasing computational resources and \nstarts to plateau or, worse yet, peak, turn over, and \nstart decreasing. Solving this problem requires a new nalgorithm for the problem, or rewriting the current \nalgorithm to “translate” the knee farther up the scale nAn open research question is whether for any given \nalgorithm, there is a fundamental limit to its nscalability. These limits are known for specific \nalgorithms with specific implementations on specific nmachines at specific scales. General computational \nsolutions, particularly using unstructured data, are not nyet known. Table 8 gives some examples of analytic \napproaches that may not scale linearly. Simplistically nthe processing of big data can be characterized in one \nof three ways as shown in table 9. \nFigure 1 Examples of Types of Analytics \nTable 8. Examples of Analytics That May Not Scale \nto Zettabytes \nMachine Learning Techniques \nUnstructured Text/Image/Video Analytics \nVisualization \nCloud Computing \nData Mining nGraph and Mesh Algorithms \nJoining Algorithms Across Structured Data \nSets\nFigure 2. The Needle in the Haystack \n10001\nTable 9. Big Data Processing \n1. Finding the \nneedle in the \nhaystack\nThe objective is to discover and \nextract the critical piece of \ninformation that provides the user \nwith leverage in some situation \n2. Turning \nstraw into \ngold \nThere is no point solution, but a \nmyriad of solutions depending on nhow the problem is presented. \nThe objective is to select the best, \nbut not necessarily the optimal nsolution. \n3. A hybrid of \ntechniques\nThe objective is to converge on \nthe answers at the same time we nconverge on the question\(s the Needle in the Haystack \n This challenge focuses on finding the key data \nthat provides leverage for decision-making within a \nproblem space. A needle-in-a-haystack problem is one \nin which the right answer is very difficult to determine \nin advance, but very easy to verify once you know \nwhere the needle is [5 Suppose we characterize it as \nfinding the one right answer within a pool of \n1,000,000 wrong answers. If the decision process is \nwrong 0.1% of the time, then of the 100 answers \nproposed, there are 101 answers, only one of which is \n‘correct”. As Felten notes, any research area \ndepending on this approach will suffer this problem, \nespecially if it relies on statistical analysis. There are \nonly two ways out of this problem reduce the size of \nthe haystack, or improve our search, analysis and \ndecision-making procedures. \n3.3 Turning Straw into Gold \n This challenge focuses on processing a large set \nof discrete data points into high-valued data. Consider \nfigure 3 below – a data visualization of Kenneth \nFreeman’s Facebook Friends in December 2011 [7]. It \nrepresents a small subset of the hundreds of millions \nor so people using Facebook.  As the number of edges \nemanating from “central” nodes increases, the overall \nmesh complexity increases nonlinearly. Finding \nsubgraphs within graphs with particular sets of \nfeatures may not be a linearly computationally \ntractable problem using standard graph-traversal and \nanalysis algorithms. \n One approach to solving the representation \nproblem is to parse semistructured text and convert it \nto linked data using the Resource Description \nFramework \(RDF 10:1 due to the \nuse of RDF tags to identify components of an RDF \nstructure. This translates the problem from processing \nsemistructured text to finding relationships over a very \nlarge, real-world, partially connected mesh. Extracting \nmesh structural features is critical to identifying \npatterns and anomalies. Inference across mesh \nsubstructures is akin to ‘guilt by association”, e.g., if a \nperson is a drug abuser, it is likely his friends are as \nwell. Beliefs are propagated across the mesh resulting \nin a further explosion of data. [13]. \n Another challenge is the time-varying nature of \nvery large graphs. Freeman’s graph is a 


snapshot. \nDetermining the change between two snapshots – \neither statically or continuously for a given interval – \nis a computationally explosive problem. This type of \nproblem occurs frequently, but requires more intensive \ncomputation and new algorithms when applied in \nnear-real-time analytics such as network attack nmonitoring. \n To us, it is clear: Gold Mining is not equal to \nData Mining! Different algorithms with greater nreliance on reasoning \(machine learning – symbolic, \nnot statistical ndomain-based analysis are essential to seeing the “big \npicture” in order to interpret and extract actionable npatterns of behavior, meaning and nuggets of \nintelligence for informed decision-making. \n3.4 A Hybrid of Techniques\n Given a very large heterogeneous data set, a major \nchallenge is to figure out what data one has and how to \nanalyze it. Unlike the previous two sections, hybrid \ndata sets combined from many other data sets hold \nmore surprises than immediate answers. To analyze \nthese data will require adapting and integrating nmultiple analytic techniques to “see around corners”, \ne.g., to realize that new knowledge is likely to emerge \nin a non-linear way. It is not clear that statistical \nanalysis methods, as Ayres [2] argues, are or can be \nthe whole answer. \n Nassim Taleb [19] addressed the potential for \nundirected and unpredicted effects arising from events \nthat are outliers that lie outside the realm of regular \nexpectations, because nothing in the past can \nconvincingly point to their possibility. Such events \noften have an extreme impact – a “shock to the \nsystem that can force new behaviors. Because we do \nnot expect it, we cannot predict it. Thus, we can only \ntry to explain what happened retrospectively. With \nmore data, the likelihood of identifying such events nrises and will force us to re-evaluate our estimative \nand predictive analytical tools.  \n10012\nFigure 3 Freeman’s Facebook friends \n3.5 Know the World \n With the \(over research question is: can we model world \nsystems, say, on the order of our desire to model and \npredict the weather? For example, can we forecast \nglobal political and/or economic stability at a given \ntemporal interval? Underlying this challenge are \nquestions of modeling natural science, social and \ncultural interactions at different scales; understanding \nhow societies function and the causes of global unrest; \nand understanding how human societies produce and \nconsume resources and the resource flows around the \nworld Such questions are important to transnational \nand global businesses determining where to allocate \nresources and invest in infrastructure. \n Creating models in a computer is standard \nscience. But, creating world-encompassing models \(or, \neven, domain-encompassing challenge is to begin \nbuilding such models that will allow us to comprehend \nsystems at both the scope and granularity necessary to \nanswer fundamental questions of cause and effect. \n Consider the effects of natural disasters \(such as \nthe tsunami affecting Japan’s decisions regarding \nnuclear power the US \nhousing and banking crisis ngovernments, or technological innovation \(such as the \nrise of social media ndecision makers – business, academia, government – \nis, “what does it all mean?” followed by “what is \nlikely to happen next?” \n These are all “wicked problems” as defined by \nRitchey [16]. A wicked problem is one which has \nincomplete, contradictory and often changing \nrequirements [17]. Because of the complex ninterdependencies of their elements, it is often difficult \nto recognize that one has achieved even a partial nsolution. Moreover, while attempting to solve a \nwicked problem, the partial solution often reveals or ncreates even more complex problems. The underlying \nsystems are emergent, adaptive systems meaning that \nthe system dynamically changes its behavior and its \nability to adapt to new situations. Modeling these \ntypes of systems must continually evolve in order to \nsupport the decision-maker’s wide area situation \nawareness. \n4 Conclusions and Future Work \n Big data is the “new” business and social science \nfrontier. The amount of information and knowledge \nthat can be extracted from the digital universe is \ncontinuing to expand as users come up with new ways \nto massage and process data. Moreover, it has become \nclear that “more data is not just more data”, but that \n“more data is different”. \n “Big data” is just the beginning of the problem. \nTechnology evolution and placement guarantee that in \na few years more data will be available in a year than \nhas been collected since the dawn of man. If Facebook \nand Twitter are producing, collectively, around 50 \ngigabytes of data per day, and tripling every year, \nwithin a few years \(perhaps 3-5 of “big data becoming really big data”. \n We – as a global society – are evolving from a \ndata-centric to a knowledge-centric community. Our \nknowledge is widely distributed and equally widely \naccessible. One program that is addressing this \nproblem is The Federal Semantic Interoperability \nCommunity of Practice \(SICoP supports an \nevolving model: Citizen-Centric Government – \nSystems That Know; Advanced Analytics – Systems nThat Learn; and Smart Operations – Systems That \nReason. These systems will require big data. The data \nwill not be stored in one or even a few locations; it \nwill not be just one or even a few types and formats; it nwill not be amenable to analysis by just one or a few \nanalytics; and there will not be just one or a few 


cross-\nlinkages among different data elements. Thus, it is an \nexemplar of some of the issues we have addressed in \nthis paper. Solving the issues and challenges addressed \nin this paper will require a concerted research effort – \none which we expect to evolve over the next several \nyears\n This paper initiates a collaborative research effort \nto begin examining big data issues and challenges. We \nidentified some of the major issues in big data storage, \nmanagement, and processing. We also identified some \nof the major challenges – going forward – that we \nbelieve must be addressed within the next decade and \n10023\nwhich will establish a framework for our Big Data \nminitrack in future HICSS sessions. Our future \nresearch will concentrate on developing a more \ncomplete understanding of the issues associated with \nbig data, and those factors that may contribute to a \nneed for a big data analysis and design methodology. \nWe will begin to explore solutions to some of the \nissues that we have raised in this paper through our \ncollaborative research effort. \n5. References \n[1] American Institute of Physics \(AIP http://www.aip.org/fyi/2010   and K. Craford. 2011. “Six Provocations \nfor Big Data”, Oxford Internet Institute’s “A Decade \nin Internet Time: Symposium on the Dynamics of the \nInternet and Society” \n[4] The Economist. 2010. “Data, Data Everywhere”, \n\(online edition, February 28  Needle in a Haystack Problems”, \nhttps://freedom-to-tinker.com/blog/felten/needle-haystack-\nproblems/\n[6 Fox, B. 2011. “Leveraging Big Data for Big \nImpact”, Health Management Technology nhttp://www.healthmgttech.com/\n[7] Freeman, K. 2011 nhttp://en.wikipedia.org/wiki/File:Kencf0618Facebook\nNetwork.jpg \n[8] Gantz, J. and E. Reinsel. 2011 Extracting Value from \nChaos”, IDC’s Digital Universe Study, sponsored by EMC \n[9] Jacobs, A. 2009 Pathologies of Big Data”, \nCommunications of the ACM, 52\(8  Challenges”, The \nMitre Corporation, McLean, VA, JSR-08-142 \n[11] Kaisler, S. 2012. “Advanced Analytics nCATALYST Technical Report, i_SW Corporation, \nArlington, VA \n[12] Kaisler, S., W. Money, and S. J. Cohen 2012. “A \nDecision Framework for Cloud Computing”, 45th\nHawaii International Conference on System Sciences,\nGrand Wailea, Maui, HI, Jan 4-7, 2012 \n[13] Kang, U. 2012. “Mining Tera-scale Graphs with nMapReduce: Theory, Engineering, and Discoveries”, \nPhD. Thesis, Computer Science, Carnegie-Mellon nUniversity, Pittsburgh, PA \n [14] Mervis, J. 2012. “Agencies Rally to Tackle Big \nData”, Science, 336\(4 June 6, 2012 \n[15] Popp, R., S. Kaisler, et al. 2006. “Assessing \nNation-State Fragility and Instability IEEE \nAerospace Conference, 2006, Big Sky, MT \n[16] Ritchey, T. 2005. "Wicked Problems: Structuring \nSocial Messes with Morphological Analysis", Swedish \nMorphological Society, \nhttp://www.swemorph.com/wp.html\n[17 Rittel, H. and M. Webber. 1973. “Dilemmas in a \nGeneral theory of Planning”, in Policy Sciences, Vol. \n4 Elsevier Scientific, Amsterdam, the Netherlands, \npp. 155-169 \n[18] Stonebraker, M. and J. Hong. 2012 Researchers' \nBig Data Crisis; Understanding Design and \nFunctionality”, Communications of the ACM n55\(2  York, \nNY\n10034\n 


contact.\nIn order to accommodate all the expected customers, a re-\nlay network has to determine how to optimally allocate its\nresources. This can be done through a scheduling algorithm\nthat indicates what users and services will be supported at\na given moment in time. This schedule is built taking into\nconsideration at least three parameters:\n• The network topology, i.e. what links are available at each\nmoment of time. This information has already been computed\nand is stored in the matrix NT.\n• The user and service priorities. They will indicate what\nusers and services are more important and therefore must be\nscheduled first. This information is an input to the model.\n• The concept of operations for a particular user and service:\nnumber of contacts, data volume to return per contact, mini-\nmum time between contacts. This information is an input to\nthe model.\nThe current performance model has an especially built-in\nheuristic algorithm to emulate the scheduling process. Its\nhigh level structure is shown in figure 2.\nIt is assumed that a sorted list of required contacts is available\nat the start of the algorithm. This list is indexed through k and\nhas the high priority jobs in the first positions. Therefore, if\nthe scheduler tries to iteratively satisfy jobs in a descending\npriority order \(k=k+1 snapshot t the network status must be\ncomputed. This is done by determining the amount of time\nthat all links will be available and multiplying it by their\ncurrent capacities. The result of this computation is a matrix\nDV capturing the data volume that can be returned given\nthe current topology of the network. The link capacity\nused for this computations is not the nominal value but the\nremaining data rate given the connections that are already\nbeing supported. In other words, links that are physically\nviable but cannot support more services because they are\nalready saturated are automatically neglected.\nOnce the network status has been computed, the next step is to\ntry to schedule as many jobs as possible. The algorithm will\nuse a shortest path algorithm to find a viable path between\nthe user being serviced and a ground station of the network.\nIn order to do so, a metric for costing a link Lij must be\nspecified.\nCost\(Lij 0} \(4 result, a telemetry\nservice will always be routed through a set of low data rate\nlinks while a science return service will use a high data rate\ncommunication path. On the other hand, if a link does not\nhave enough capacity to support a contact \(Cost\(Lij Cost\(Lij Heuristic scheduling algorithm\nWith this approach, a service with a viable path from the user\nto a ground station is automatically selected for schedule.\nWhen that happens, three main actions take place:\n Information regarding the instant times during which a\nparticular job has been served gets stored. This information\ncan be later processed to graphically represent the obtained\nschedule for any user of the network.\n• The capacities of the links on the selected path decrease\nby the service data rate. It is assumed that there is no chan-\nnelization of the link bandwidth and therefore the amount of\nretractable data rates is not discretized.\n• Links that were previously viable become unfeasible due\nto beam pointing constraints. These links are only invalidated\nfor the duration of the scheduled contact.\nModeling the network traffic— The current performance\nmodel neglects the burstiness of the traffic that flows through\n9\nthe network. In fact data rates to support a service are always\ncomputed as averages in this model.\nRb =\ndata volume\ncontact time\n\(5 rate \(CBR information.\nA more realistic approach would consider that each service\nhas different traffic characteristics 21] presents a com-\nprehensive study on video, voice, telemetry and command\nservices on past NASA missions Its results indicate that\nthe data rate for supporting these services can in fact be\nmodeled through gamma distributions. A similar conclusion\nis presented in [22] for IP traffic over space networks. It\nalso presents an analytic solution for determining the gain\nof using packet-based architectures that statistically multiplex\nthe incoming traffic.\nIncorporating these result to the performance model will be\ndone in future versions of the tool. They will allow to increase\nthe accuracy of the heuristic scheduler and ensure that link do\nnot get overloaded due to the traffic burstiness. They will also\nallow to numerically assess the differences between packet\nand circuit-switched architectures.\nComputing the architecture benefit—Once the network sched-\nule has been computed, determining the benefit of the archi-\ntecture can be easily done through a two step process.\n• The fraction of successfully scheduled contacts is com-\npared to the expectations of each mission through the require-\nment satisfaction rules. They encode the satisfaction that the\nuser has given the performance of the system. As an example,\nan Earth Observation mission that can only schedule 50 of\nits desired contacts might be completely unsatisfied because\nhalf of the collected data cannot be returned.\n• The satisfaction of all missions are aggregated for the\nobjectives and stakeholders through the value aggregation\nrules. They compute the weighted sum of the user, objective\nand stakeholder satisfaction in 


order to assess the benefit of\nthe whole architecture. The relative importance or weights\nfor the value aggregation rules must be known beforehand\nand are an input to the model.\nCost model\nThe goal of the cost model is to provide an estimate of the\nlifecycle cost of an architecture \(i.e., a set of constellations\nand ground stations particular differ-\nentiating between different contract modalities, \(procurement\nvs hosted payloads vs 100 commercial cost, operations cost, and\nprogram overhead. Some of these are further divided into\nnon-recurring and recurring costs, as illustrated in Fig. 3.\nPayload Cost—Payload cost is only incurred when the con-\ntract modality is procurement or hosted payloads. If a 100%\ncommercial approach is taken, payload cost is set to zero,\nas it is included in the service fee charged to NASA by the\nprovider. Payload cost is the sum of a non-recurring cost\nand a recurring cost. When these values are not provided by\nFigure 3. Lifecycle cost breakdown\nthe user, they are estimated using CERs that utilize payload\nmass m and number of channels n as independent variables.\nThe CERs are taken from the USCM8 model [23] and are\nprovided below. All values are in FY2010$k.\nCpayl,NR = 339m+ 5127n \(6 7 cost\nof fabricating a qualification unit and Cpayl,R is the cost of\nfabricating the first flight unit. The standard error of the\nestimate \(SEE SEE of Equation 7 inside\nthe domain 38-928kg is 28%. All SEEs are corrected for the\nnumber of degrees of freedom. Total cost for development\nand fabrication of N identical payloads is thus given by:\nCpayl = Cpayl,NR N\nbCpayl,R \(8 is only incurred when the contract\nmodality is procurement. If a 100% commercial approach\nor hosted payloads approach are taken, bus cost is set to zero,\nas it is included in the service fee charged to NASA by the\nprovider. Bus cost is the sum of a non-recurring cost and a\nrecurring cost. When these values are not provided by the\n10\nuser, they are estimated using CERs that utilize subsystem\nmass as the independent variable. The CERs are taken from\nthe USCM8 model [23] and are provided below. All values\nare in FY2010$k.\nCbus,NR = 110.2mdry \(9 10 is the devel-\nopment cost including the cost of fabricating a qualification\nunit and Cpayl,R is the cost of fabricating the first flight\nunit. The standard error of the estimate \(SEE 114-5127kg is 47%. The SEE of Equation\n10 inside the domain 288-7398kg is 21%. Total cost for\ndevelopment and fabrication of N identical buses is computed\nas illustrated in Equation 8, with a learning factor of 95%.\nNote that the computation of bus cost depends on the dry\nmass of the spacecraft. This can be provided by the user,\nor it can be estimated by the spacecraft design module, which\nis described later in this section.\nLaunch Cost—Launch cost is only incurred when the contract\nmodality is procurement. If a 100% commercial approach\nor hosted payloads approach are taken, launch cost is set to\nzero, as it is included in the service fee charged to NASA by\nthe provider. Launch cost is given by the sum of the costs of\nlaunching all constellations in the architecture. Computation\nof launch cost for a constellation is based on the assumption\nthat, given a constellation of P planes and S satellites per\nplane, P < NL < PS launches are necessary to launch\nthe constellation, i.e., at least one launch per plane \(in\nother words, satellite engines are not sized to do inclination\nchanges after injection by taking into account both\nperformance and geometrical considerations.\nIn particular, a database of launchers is available to each\nconstellation. This database is shown in Table 5. Note that\nthe data concerning performance is provided in terms of the\n3 coefficients of a quadratic function of the orbit altitude.\nIn other words, if the entry of the table for a certain orbit\ntype \(e.g., LEO polar  then the\nperformance at altitude h can be computed as :\nperf\(h 11 computed as follows:\nNL = max{NL,mass, NL,vol, NL,dim} \(12 required\ngiven the total spacecraft mass and the performance of the\nlaunch vehicle to the desired orbit NL,vol is the minimum\nnumber of launches required given the total spacecraft vol-\nume and the volume of the launch vehicle; NL,dim is the\nminimum number of launches required given the sum of the\nmaximum dimension of all spacecraft and the height of the\nlaunch vehicle:\nNL,mass = d\n?NS/C\ni=1 mwet,i\nperf\(lv, orbit 13 14 15 spacecraft in the constella-\ntion, mwet,i, voli, and dmaxi are the wet mass, volume,\nand maximum dimension respectively of spacecraft i, and\nperf\(lv, orbit the volume, and the height respectively of\nthe launch vehicle. Once the number of launches has been\ncomputed launch cost is simply given by the product of\nnumber of launches and launch cost.\nGround segment cost—Ground segment cost is the sum of a\nnon-recurring cost and a recurring cost. When these values\nare not provided by 


the user, they are estimated using CERs\nthat utilize location of the facility, the number of spacecraft,\nand spacecraft lifetime as independent variables. The CERs\nare taken from [23] and are provided below. All values are in\nFY2010$k.\nCground,R = Cground,NR + Cground,Rt\(yr loc n$\nm2\n m2 0.5\n$M\nS/C/yr\nNS/Ct\(yr 16 loc construction cost in different locations, A is\nthe floor area of the facility in m2, and t\(yr lifetime\nin years. The values of the adjustment factor are taken from\n[23].\nService fees— Service fees are only applicable for hosted\npayloads and 100% commercial approaches. In these cases,\nthey can replace payload bus, launch, or ground segment\ncost. Service fees for the hosted payloads approach are\ncomputed as a fix quantity \(10$M resources such as volume, mass, power, data rate, or a\ncombination thereof. Service fees for the 100 commercial\napproach are computed as a fixed quantity \($2k spacecraft design module can be by-\npassed by choosing to assign a commercially available bus\ninstead of designing a bus. While the standard bus approach is\ncloser to reality, the spacecraft design module provides more\ndistinction in lifecycle cost between different architectures,\nand therefore it is chosen as the primary operating mode of\nthe tool. The spacecraft design module is an iterative module\nthat provides a subsystem-level design of the spacecraft bus\nincluding a very rough configuration of the spacecraft from\nthe payload requirements.Assumptions concerning the differ-\nent sub-modules in the spacecraft design module namely the\nbus selection module and the four steps of the process shown\n11\nTable 5. Extract of launch vehicle database\nAtlas-V Delta-7920 Taurus-XL\nPayload GTO [104,0,0] [5 · 102,0,0] [0,0,0]\nPayload LEO polar [15 103,?4 · 10?2,0] [4 · 103,?1 · 10?2,7 · 10?5] [1.2 · 103,?4.4 · 10?2,0]\nDiameter \(m m 10.0 7.53 5.71\nCost \(FY2010$M detail in the rest of this section.\nElectrical power subsystem design— The Electrical Power\nSubsystem \(EPS designed based on a very rough power\nbudget. The mass of the EPS is given by:\nmEPS = mSA +mbatt +mother 17 the other\ncomponents. The solar array is designed to provide enough\npower at end-of-life, assuming a certain yearly degradation\n?\(%/yr W/kg W nPe\nTe\nXe\n+ Pd\nTd\nXd\nTd\nWBOL\(\nW\nm2\n m\n2 1 kg 18 Te\(s s Te,\nT \(s W W requirements during\ndaylight, Xd and Xe are the energetic efficiencies between\nthe solar array and the power bus \(through the batteries in\ncase of eclipse W/m2 technology, Id is an efficiency, ? is the Sun\nangle, WBOL\( Wm2 at\nBOL, and ASA\(m2 capacity\nassuming a certain specific energy ?e\(Wh/kg Wh kg 19 is the depth of discharge \(which depends on the\norbital parameters batteries\nto the load. In particular, the DOD is assumed to be 0.8 for\nGEO, 0.6 for dawn-dusk SSO, and 0.4 for all other orbits.\nThe mass of the rest of components \(regulators, converters,\nand wiring function of the power at be-\nginning of life PBOL and the spacecraft dry mass mdry as\nsuggested in 24]:\nmother = ?PBOL + ?mdry \(20 power, PBOL = WBOLASA is the power\navailable at BOL, ? accounts for wiring, and mdry is the\nspacecraft dry mass.\nDelta-V and propellant mass budgets— The design of the\nADCS and propulsion subsystems is based on a rough ?V\nbudget of the spacecraft, which consists of four components:\ninjection, drag compensation, ADCS, and deorbiting:\n?V = ?Vinj + ?Vdrag + ?VADCS + ?Vdeorbit \(21 is in-\njected into a transfer orbit that has the perigee at 150km and\nthe apogee at the final orbit altitude:\n12\nTable 6. ?V required to compensate drag for different\norbits\nOrbit ?Vdrag\(m/s/yr h < 500km 12\nLEO\(500km < h < 600km 600km < h < 1000km configuration ?VADCS\(m/s/yr rp, ra, r n1\nr\n? 1\nrp ra\n rp1, ra1, rp2, ra2, r rp2, ra2, r rp1, ra1, r RE + 150km, r, r, r, r 22 rp1, ra1, rp2, ra2, r orbit \(rp1, ra1 rp2, ra2 to compensate drag is strongly depen-\ndent on orbit altitude. The values shown in Table 6 were taken\nfrom 25]:\nThe ?VADCS required for ADCS depends on the ADCS\nconfiguration, as shown in Table 7. These values were\nadapted from [25].\nThe ?Vdeorbit is computed assuming that LEO spacecraft are\ndeorbited using atmospheric drag, and all other spacecraft\nare deorbited using solar radiation presure. For drag-based\ndeorbiting 


the?Vdeorbit is computed based on a change of\nsemimajor axis from the current circular orbit to an elliptical\norbit that has the perigee at 0km and the apogee at the orbit\naltitude:\n?Vdeorbit,drag = ?V \(r, r RE , r, r 23 semimajor axis from the\ncurrent circular orbit to an elliptical orbit that has the same\nperigee and a slightly higher apogee:\n?Vdeorbit,SRP = ?V \(r, r, r, r + ?h, r 24 are due to the GEO restricted zone, the\n35km are to allow for gravitational perturbations, and the\nremaining margin depends on the magnitude of the effect of\nsolar radiation pressure on the spacecraft \(the larger the ef-\nfect, the larger the margin the spacecraft.\nOnce the ?V has been calculated, it is possible to compute\nthe propellant mass required to satisfy this ?V budget. The\ntool assumes that ?Vinj is performed by the apogee kick\nmotor \(AKM other ?V are performed by the\nADCS subsystem. For each of these propulsion systems, the\npropellant mass can be computed using the rocket equation:\n?Vj = gIsp,j log\nmi\nmf\n\(25 which can be\ndifferent for the AKM and the ADCS subsystem, mi is the\ninitial mass with propellant and mf is the final mass without\nthe propellant.\nAttitude Determination and Control and Propulsion Subsys-\ntem—The mass of the ADCS is mostly given by the mass\nof the sensors and the mass of the actuators. The mass\nof the sensors is driven by the attitude knowledge accuracy\nrequirement acc \(Equation 26 satisfy by the momentum storage h required \(Equation 27 26 27 that acc can vary depending on the architecture, as the\npointing requirements of a high gain antenna, or an optical\npayload, are very different from those of a low gain antenna.\nConcerning the momentum storage h, it is assumed to be\nsized to counter the different disturbance torques produced by\natmospheric drag, gravity gradient, solar radiation pressure,\nor the Earth’s magnetic field. Expressions for these distur-\nbance torques were taken from [25].\nIn addition to sensors and actuators, the ADCS has additional\nmass that can be estimated as a fix fraction of the spacecraft\ndry mass:\nmADCS = 3msen + 4mact + 0.01mdry \(28 subsystem, the mass of the AKM\ncan be estimated from its propellant mass assuming a certain\nmass fraction:\nmAKM =\n\(1 29 structure subsystem\nSubsystem k\nThermal 0.0607\nAvionics 0.0983\nStructure 0.5462\nThermal, avionics, and structure subsystems—The thermal,\navionics, and structure subystems are designed using simple\nparametrics of the form msubsystem = kmpayload. The\nconstans k that are used for each subsystem are summarized\nin Table 8.\nThe mass of the launch adapter mLA = 0.01mdry is added\nto the mass of the spacecraft.\nUpdate spacecraft mass and dimensions—After the first iter-\nation, the dry and wet mass of the spacecraft are updated.\nDimensions are estimated assuming a perfect cube of 100\nkg/m3. The mass and dimensions of the solar panels are\ntaken into account to update the inertial properties of the\nspacecraft, as illustrated in Equation 30:\nLA = 1.5s+ 0.5\n?\nAa\n2\nIz = 0.01mdry\nIx = Iy = Iz + L\n2\naMa \(30 design algorithm is\niterative because several feedback loops appear in the N2\nmatrix showing the dependences between different modules\nin the algorithm. For instance, the mass of the ADCS depends\non the mass of the spacecraft, which obviously depends on the\nmass of the ADCS. Thus, a set of convergence criteria need\nto be defined. The convergence criteria used by the tool are\ndescribed in Equation 31:\n|mdry,i+1 ?mdry,i| < 10kg 31 the current status of the MIT archi-\ntecture study for the SCaN system. The study consists of a\nstakeholder analysis to identify the primary stakeholders and\ntheir needs, and the development of a computational tool to\nexplore the architectural tradespace.\nSeveral interviews have been conducted with experts at\nNASA to elicit the potential requirements on SCaN from\ndifferent user communities.\nThe major architectural decisions to be made by the SCaN\nprogram have been identified and encoded in a mathematical\nmodel. A computational tool has been developed that can\nautomatically enumerate and evaluate thousands of different\nSCaN architectures. This tool contains both a performance\nand a cost model.\nNext Steps\nThe next steps include calibration of the optical link budget\ncalculations, comparisons of the network scheduling calcula-\ntions with historical TDRSS load data, and validation of the\nspacecraft sizing algorithm with real TDRS data. Following\nthe completion of the stakeholder analysis, the tool will be\nbe used to explore the architectural tradespace and identify\na subset of preferred architectures worth studying in more\ndetail. These architectures could then be analyzed in NASA’s\nArchitecture Development Lab \(ADL NNX11AR70G.\nThe authors would also like to thank the Centre de Formacio\nInterdisciplina`ria Superior and the Cellex Foundation for\npartially funding this project.\nREFERENCES\n[1] S. Tsiao, Read you loud and clear! The story of NASA’s\nspaceflight tracking and data network. Washington\nDC: Library of Congress, 2007.\n[2] G. Maral Satellite Communication Systems: systems,\ntechniques and technology, 2009.\n[3] K. Y. Jo, “Satellite 


communications with Internet Pro-\ntocol \(IP Conference, pp. 1–7, Oct. 2009.\n[4] E. Jennings and D. Heckman, “Architecture Modeling\nand Performance Characterization of Space Communi-\ncations and Navigation \( SCaN  R. Borgen, S. Nguyen, J. Segui, T. Stoe-\nnescu, S.-y. Wang, and S. Woo, “Space Communica-\ntions and Navigation SCaN Astronautics, no. August, pp. 1–11,\n2009.\n[6] E. Jennings and D. Heckman, “Performance Charac-\nterization of Space Communications and Navigation\n\(SCaN Mar. 2008.\n[7] J. Alonso and K. Fall, “A Linear Programming For-\nmulation of Flows over Time with Piecewise Constant\nCapacity and Transit Times piecewise constant capacity\nand transit times,” 2003.\n[8] B. L. Murphy High Resolution Satellite Communica-\ntion Simulation,” 2000.\n[9] M. Werner, A. Jahn, E. Lutz, and A Bottcher, “Analysis\nof System Parameters for LEO/ICO-Satellite Commu-\nnication Networks,” 1995.\n[10] T Weilkiens, Systems engineering with SysML/UML:\nmodeling, analysis, design. Heidelberg, Germany: The\nMorgan Kaufmann/OMG Press, 2006.\n[11] M. Rao, S. Ramakrishnan, and C. Dagli, “Modeling and\nSimulation of Net Centric System of Systems Using\nSystems Modeling Language and Colored Petri-nets :\nA Demonstration Using the Global Earth Observation\n14\nSystem of Systems,” Systems Engineering, vol. 11,\nno. 3, pp. 203–220, 2008.\n[12] B. H. Y Koo, W. L. Simmons, and E. F. Crawley, “Al-\ngebra of Systems: A Metalanguage for Model Synthesis\nand Evaluation,” IEEE Transactions on Systems, Man,\nand Cybernetics - Part A: Systems and Humans, vol. 39,\nno. 3 pp. 501–513, May 2009.\n[13] M. Ehrgott and X. Gandibleux, “A Survey and An-\nnotated Bibliography of Multiobjective Combinatorial\nOptimization,” OR Spectrum, vol. 22, no. 4, pp. 425–\n460, Nov. 2000.\n[14] D Selva, “Rule-based system architecting of Earth\nobservation satellite systems,” PhD dissertation Mas-\nsachusetts Institute of Technology, 2012.\n[15] D. Selva and E. F. Crawley, “VASSAR: Value Assess-\nment of System Architectures using Rules,” in Proceed-\nings of the 2013 IEEE Aerospace Conference, Big Sky,\nMontana 2013.\n[16] T. Sutherland, B. Cameron, and E. Crawley, “Program\ngoals for the nasa/noaa earth observation program de-\nrived from a stakeholder value network analysis,” 2012.\n[17] B. Cameron, E. Crawley, G. Loureiro and E. Reben-\ntisch, “Value flow mapping: Using networks to inform\nstakeholder analysis,” Acta Astronautica vol. 62, pp.\n324–333, 2008.\n[18] B. Cameron and Crawley, “Goals for space exploration\nbased on stakeholder network value considerations,”\nActa Astronautica, vol. 68, pp. 2088–2097, 2011.\n[19] D. Selva and E. F Crawley, “Integrated Assessment of\nPackaging Architectures in Earth Observing Programs,”\nin Proceedings of the 2011 IEEE Aerospace Confer-\nence, Big Sky, Montana, 2010.\n[20] O. P. Gupta and C. S. Fish, “Iridium NEXT: A Global\naccess for your sensor needs,” in Proceedings of the\n2010 American Geophysical Union Fall Meeting San\nFrancisco, CA, 2010.\n[21] T. Stoenescu and L. Clare, “Traffic Modeling for\nNASA’s Space Communications and Navigation\n\(SCaN  communications with Internet Pro-\ntocol \(IP Conference, pp. 1–7, Oct. 2009.\n[23] H. Apgar, “Cost Estimating,” in Space Mission Engi-\nneering: The new SMAD. Hawthorne, CA: Microcosm,\n2011, ch. 11.\n[24] R. S. Bokulic, C. C. DeBoy, S. W. Enger, J. P. Schnei-\nder and J. K. McDermott, “Spacecraft Subsystems IV\nCommunications and Power,” in Space Mission Engi-\nneering: The new SMAD. Hawthorne, CA: Microcosm,\n2011, ch. 21.\n[25] P. Springmann and O. de Weck, “Parametric scaling\nmodel for nongeosynchronous communications satel-\nlites,” Journal of spacecraft and rockets, vol. 41, no. 3,\npp 472–477, 2004.\nBIOGRAPHY[\nMarc Sanchez is a senior student from\nUniversitat Politecnica de Catalunya\n\(Barcelona, Spain Telecommunications En-\ngineering. His is currently a Visiting\nStudent at the Space System Architecture\nGroup of MIT, focusing his interests in\nrule-based expert systems and how they\ncan be applied to space communications\nnetworks. Prior to his work at MIT, Marc has been a\nsoftware engineer at Sener Ingenieria y Sistemas involved in\nthe development of commercial software FORAN CAD/CAM.\nDr. Daniel Selva received a PhD in\nSpace Systems from MIT in 2012 and\nhe is currently a post-doctoral associate\nin the department of Aeronautics and\nAstronautics at MIT. His research inter-\nests focus on the application of multi-\ndisciplinary optimization and artificial\nintelligence techniques to space systems\nengineering and architecture, in partic-\nular in the context of Earth observa-\ntion missions. Prior to MIT, Daniel worked for four years\nin Kourou \(French Guiana in\noperations concerning the guidance, navigation and control\nsubsystem, and the avionics and ground systems Daniel has\na dual background in electrical engineering and aeronautical\nengineering, with degrees from Universitat Politecnica de\nCatalunya in Barcelona, Spain, and Supaero in Toulouse,\nFrance. He is a 2007 la Caixa fellow, and received the Nortel\nNetworks prize for academic excellence in 2002.\nDr. Bruce Cameron is a Lecturer\nin Engineering Systems at MIT and a\nconsultant on platform strategies. At\nMIT, Dr. Cameron ran the 


MIT Com-\nmonality study, a 16 firm investigation\nof platforming returns. Dr. Cameron’s\ncurrent clients include Fortune 500 firms\nin high tech, aerospace, transportation,\nand consumer goods. Prior to MIT,\nBruce worked as an engagement man-\nager at a management consultancy and as a system engineer\nat MDA Space Systems, and has built hardware currently in\norbit. Dr. Cameron received his undergraduate degree from\nthe University of Toronto, and graduate degrees from MIT.\nDr. Edward F. Crawley received an\nSc.D. in Aerospace Structures from MIT\nin 1981. His early research interests\ncentered on structural dynamics, aeroe-\nlasticity, and the development of actively\ncontrolled and intelligent structures. Re-\ncently, Dr. Crawleys research has fo-\ncused on the domain of the architecture\nand design of complex systems. From\n1996 to 2003 he served as the Depart-\nment Head of Aeronautics and Astronautics at MIT, leading\nthe strategic realignment of the department Dr. Crawley is a\nFellow of the AIAA and the Royal Aeronautical Society \(UK academies of engineering.\n15\nHe is the author of numerous journal publications in the\nAIAA Journal, the ASME Journal, the Journal of Composite\nMaterials, and Acta Astronautica. He received the NASA\nPublic Service Medal Recently, Prof Crawley was one of\nthe ten members of the presidential committee led by Norman\nAugustine to study the future of human spaceflight in the US.\nBernard D. Seery is the Assistant Di-\nrector for Advanced Concepts in the Of-\nfice of the Director at NASA’s Goddard\nSpace Flight Center \(GSFC include assisting the Deputy\nDirector for Science and Technology\nwith development of new mission and\nmeasurement concepts, strategic analy-\nsis, strategy development and investment\nresources prioritization Prior assign-\nments at NASA Headquarters included Deputy for Advanced\nPlanning and Director of the Advanced Planning and In-\ntegration Office \(APIO and Evaluation \(PA&E DAA and Physical Research \(OBPR Directorate, Code 600, at \(GSFC bachelors of science in physics, with emphasis in\nnuclear physics. He then attended the University of Ari-\nzona’s School of Optical Sciences, and obtained a masters\ndegree in Optical Sciences, specializing in nonlinear optical\napproaches to automated alignment and wavefront control\nof a large, electrically-pumped CO2 laser fusion driver. He\ncompleted all the course work for a PhD in Optical Sciences\nin 1979, with emphasis in laser physics and spectroscopy. He\nhas been a staff member in the Laser Fusion Division \(L-\n1 Alamos National Laboratories \(LANL working on innovative infrared laser auto-alignment\nsystems and infrared interferometry for target alignment for\nthe HELIOS 10 kilojoule, eight-beam carbon dioxide laser\nfusion system. In 1979 he joined TRW’s Space and Defense\norganization in Redondo Beach, CA and designed and de-\nveloped several high-power space lasers and sophisticated\nspacecraft electro-optics payloads. He received the TRW\nPrincipal Investigators award for 8 consecutive years.\nDr. Antonios A. Seas is a Study Man-\nager at the Advanced Concept and For-\nmulation Office ACFO Electro-Optics branch where\nhe focused on optical communications\nand the development of laser systems\nfor space applications. Prior to joining\nNASA in 2005 he spent several years in\nthe telecommunication industry developing long haul sub-\nmarine fiber optics systems, and as an Assistant Professor\nat the Bronx Community College. Antonios received his\nundergraduate and graduate degrees from the City College of\nNew York, and his doctoral degree from the Graduate Center\nof the City University of New York. He is also a certified\nProject Management Professional.\n16\n 


1483\nSUB-NYQUIST SAMPLING RATES\nMustafa Al-Ani, University of Westminster, United Kingdom; Bashar Ahmad, University of Cambridge, \nUnited Kingdom; Andrzej Tarczynski University of Westminster, United Kingdom\nTPa-8.10: OPPORTUNISTIC TRANSMITTER SELECTION FOR SELFLESS 1488\nOVERLAY COGNITIVE RADIOS\nMohammad Shaqfeh, Texas A&M University at Qatar, Qatar; Ammar Zafar, King Abdullah University \nof Science and Technology, Saudi Arabia Hussein Alnuweiri, Texas A&M University at Qatar, Qatar; \nMohamed-Slim Alouini, King Abdullah University of Science and Technology, Saudi Arabia\nTPa-8.11: A GAME THEORETIC POWER CONTROL FRAMEWORK FOR 1493\nSPECTRUM SHARING IN COMPETITIVE ENVIRONMENTS\nRaghed El-Bardan, Swastik Brahma, Pramod K. Varshney, Syracuse University, United States\nTPa-8.12: COGNITIVE RADIO TRANSMISSION STRATEGIES FOR PRIMARY ...........................................1498\nERASURE CHANNELS\nAhmed ElSamadouny, University of Texas at Dallas, United States; Mohammed Nafie, Ahmed Sultan, \nNile University Egypt\nTPa-8: RELAYS IN COMMUNICATIONS\nTPa-8.1: OPTIMIZED RECEIVER DESIGN FOR DECODE-AND-FORWARD 1535\nRELAYS USING HIERARCHICAL MODULATION\nTu Nguyen, Broadcom Corporation, United States; Pamela Cosman, Laurence Milstein, University of \nCalifornia, San Diego, United States\nTPa-8.2: OPTIMAL LINEAR-COMBINING RECEIVER FOR 1540\nDECODE-AND-FORWARD RELAYS USING SUPERPOSITION CODING\nTu Nguyen, Broadcom Corporation, United States; Laurence Milstein, University of California, San \nDiego, United States\nTPa-8.3: ALTERNATE RELAYING AND THE DEGREES OF FREEDOM OF 1545\nONE-WAY CELLULAR RELAY NETWORKS\nAya Salah, Amr El-Keyi Mohammed Nafie, Nile University, Egypt\nTPa-8.4: DISTRIBUTED AF BEAMFORMING RELAY NETWORKS UNDER 1550\nTRANSMIT POWER CONSTRAINT\nKanghee Lee, Hyuck M. Kwon Edwin M. Sawan, Wichita State University, United States; Hyuncheol \nPark, Korea Advanced Institute of Science and Technology, Republic of Korea\nTPa-8.5: JOINT TRANSMIT DESIGN AND NODE SELECTION FOR 1555\nONE-WAY AND TWO-WAY UNTRUSTED RELAY CHANNELS\nJing Huang, A. Lee Swindlehurst, University of California, Irvine, United States\nTPa-8.6: WIRELESS PHYSICAL LAYER SECURITY ENHANCEMENT WITH  ..............................................1560\nBUFFER-AIDED RELAYING\nJing Huang, A. Lee Swindlehurst, University of California, Irvine, United States\nTPa-8.7: TRAINING SLOT ALLOCATION FOR MITIGATING ESTIMATION  ................................................1565\nERROR PROPAGATION IN A TWO-HOP RELAYING SYSTEM\nQian Gao, Gang Chen, Yingbo Hua, University of California, Riverside United States\nxxv\nTPa-8.8: TRANSMIT OUTAGE PRE-EQUALIZATION FOR 1570\nAMPLIFY-AND-FORWARD RELAY CHANNELS\nFernando Sanchez, Gerald Matz, Vienna University of Technology, Austria\nTPa-8: ADAPTIVE FILTERING\nTPa-8.1: A GRADIENT-CONTROLLED IMPROVED PROPORTIONATE 1505\nMULTI-DELAY FILTER\nJie Yang, Texas Instruments United States; Gerald Sobelman, University of Minnesota, United States\nTPa-8.2: COMPLEX PROPORTIONATE-TYPE AFFINE PROJECTION  ...........................................................1510\nALGORITHMS\nKevin Wagner Naval Research Laboratory, United States; Miloš Doroslovacki, George Washington \nUniversity, United States\nTPa-8.3: RADAR WAVEFORM DESIGN IN ACTIVE COMMUNICATIONS 1515\nCHANNEL\nKevin Shepherd, Ric Romero, Naval Postgraduate School, United States\nTPa-8.4: THE LEAKY LEAST MEAN MIXED NORM ALGORITHM................................................................1520\nMohammed Abdul Nasar, Azzedine Zerguine, King Fahd University of Petroleum & Minerals, Saudi \nArabia\nTPa-8.5: A NEW VARIABLE STEP-SIZE ZERO-POINT ATTRACTING  ...........................................................1524\nPROJECTION ALGORITHM\nJianming Liu, Steven Grant, Missouri University of Science and Technology, United States\nTPa-8.6 RECURSIVE LEAST SQUARES FILTERING UNDER STOCHASTIC 1529\nCOMPUTATIONAL ERRORS\nChandrasekhar Radhakrishnan, Andrew Singer, University of Illinois at Urbana-Champaign, United \nStates\nTPa-8: CELLULAR AND HETEROGENEOUS NETWORKS\nTPa-8.1: DOWNLINK COVERAGE ANALYSIS OF N-TIER 1577\nHETEROGENEOUS CELLULAR NETWORKS BASED ON CLUSTERED STOCHASTIC \nGEOMETRY\nChunlin Chen, Robert Elliott, Witold Krzymien, University of Alberta / Telecommunications Research \nLaboratories, Canada\nTPa-8.2: SYSTEM-LEVEL PERFORMANCE OF THE MIMO-OFDM 1582\nDOWNLINK WITH DENSE SMALL CELL OVERLAYS\nThomas Wirth, Bernd Hofeld, Fraunhofer Heinrich Hertz Institute, Germany\nTPa-8.3: ADAPTIVE HARQ AND SCHEDULING FOR VIDEO OVER LTE......................................................1584\nAvi Rapaport, Weimin 


Liu, Liangping Ma, Gregory S. Sternberg, Ariela J. Zeira, Anantharaman \nBalasubramanian, InterDigital, United States\nTPa-8.4: NOVEL PARTIAL FEEDBACK SCHEMES AND THEIR EVALUATION 1589\nIN AN OFDMA SYSTEM WITH CDF BASED SCHEDULING\nAnh Nguyen University of California, San Diego, United States; Yichao Huang, Qualcomm \nTechnologies, Inc., United States Bhaskar D. Rao, University of California, San Diego, United States\nTPa-8.5: OPPORTUNISTIC THIRD-PARTY BACKHAUL FOR CELLULAR  ...................................................1594\nWIRELESS NETWORKS\nRussell Ford, Changkyu Kim, Sundeep Rangan, Polytechnic Institute of New York University, United \nStates\nTPa-8.6: PROACTIVE USER ASSOCIATION IN WIRELESS SMALL CELL  ...................................................1601\nNETWORKS VIA COLLABORATIVE FILTERING\nFrancesco Pantisano, Joint Research Center, Italy; Mehdi Bennis, University of Oulu Finland; Walid \nSaad, University of Miami, United States; Stefan Valentin, Bell Labs, Alcatel-Lucent, Germany nMérouane Debbah, Supélec, France; Alessio Zappone, Technische Universität Dresden, Germany\nTPa-8.7 INTERFERENCE ANALYSIS OF MULTI-HOP CELLULAR SENSOR 1606\nNETWORKS\nYeashfi Hasan, R. Michael Buehrer, Virginia Polytechnic Institute and State University, United States\nxxvi\nTPb-1: FULL-DUPLEX MIMO COMMUNICATIONS II\nTPb-1.1: DIVERSITY-MULTIPLEXING TRADEOFF ANALYSIS OF MIMO 1613\nRELAY NETWORKS WITH FULL-DUPLEX RELAYS\nQiang Xue University of Oulu, Finland; Anna Pantelidou, Renesas Mobile Europe, Finland; Behnaam \nAazhang, Rice University, United States\nTPb-1.2: ERGODIC MUTUAL INFORMATION OF FULL-DUPLEX MIMO 1618\nRADIOS WITH RESIDUAL SELF-INTERFERENCE\nAli Cagatay Cirik, University of California, Riverside, United States; Yue Rong, Curtin University, \nAustralia; Yingbo Hua, University of California, Riverside, United States\nTPb-1.3: FULL-DUPLEX IN LARGE-SCALE WIRELESS SYSTEMS 1623\nBei Yin, Michael Wu, Christoph Studer Joseph R. Cavallaro, Rice University, United States; Jorma \nLilleberg, Broadcom, United States\nTPb-1.4 FULL-DUPLEX COMMUNICATION VIA ADAPTIVE NULLING......................................................1628\nScott Johnston, Paul Fiore, Massachusetts Institute of Technology, United States\nTPb-1.5: WEIGHTED-SUM-RATE MAXIMIZATION FOR BI-DIRECTIONAL  ...............................................1632\nFULL-DUPLEX MIMO SYSTEMS\nAli Cagatay Cirik, University of California, Riverside, United States; Rui Wang, The Chinese nUniversity of Hong Kong, Hong Kong SAR of China; Yingbo Hua, University of California, Riverside, \nUnited States\nTPb-2: PHY PERFORMANCE ABSTRACTION TECHNIQUES\nTPb-2.1: STOCHASTIC DYNAMIC MODELS IN PHY ABSTRACTION 1639\nFrancesc Rey, Josep Sala-Alvarez, Technical University of Catalonia, Spain\nTPb-2.2: ON SCALABILITY, ROBUSTNESS AND ACCURACY OF PHYSICAL 1644\nLAYER ABSTRACTION FOR LARGE-SCALE SYSTEM LEVEL EVALUATIONS OF LTE \nNETWORKS\nFlorian Kaltenberger, Imran Latif, Raymond Knopp, Eurecom, France\nTPb-2.3: LINK ADAPTATION IN MIMO-OFDM WITH PRACTICAL 1649\nIMPAIRMENTS\nAlberto Rico-Alvarino University of Vigo, Spain; Robert W. Heath, Jr., University of Texas at Austin, \nUnited States\nTPb-2.4 DIGITAL PRE-DISTORTION OF RADIO FREQUENCY 1654\nFRONT-END IMPAIRMENTS IN THE DESIGN OF SPECTRALLY AGILE MULTICARRIER \nTRANSMISSION \nZhu Fu, Alexander Wyglinski, Worcester Polytechnic Institute United States\nTPb-2.5: SYSTEM-LEVEL INTERFACES AND PERFORMANCE EVALUATION 1659\nMETHODOLOGY FOR 5G PHYSICAL LAYER BASED ON NON-ORTHOGONAL nWAVEFORMS\nGerhard Wunder, Martin Kasparick, Fraunhofer Heinrich Hertz Institute, Germany; Stephan Ten \nBrink University of Stuttgart, Germany; Frank Schaich, Thorsten Wild, Yejian Chen, Bell Labs, \nAlcatel-Lucent Germany; Ivan Gaspar, Nicola Michailow, Gerhard Fettweis, Technische Universität \nDresden, Germany; Dimitri Ktenas, Nicolas Cassiau, Commissariat à l’énergie atomique et aux \nénergies alternatives, France; Marcin Dryjanski, Kamil Sorokosz, Slawomir Pietrzyk, IS-Wireless, \nPoland; Bertalan Eged, National Instruments Hungary\nTPb-3: LOW-DIMENSIONAL SIGNAL MODELS\nTPb-3.1: NEAREST SUBSPACE CLASSIFICATION WITH MISSING DATA 1667\nYuejie Chi, The Ohio State University, United States\nTPb-3.2: REFLECTIONS ON SAMPLING-FILTERS FOR COMPRESSIVE 1672\nSENSING AND FINITE-INNOVATIONS-RATE MODELS\nP. P Vaidyanathan, Srikanth Tenneti, California Institute of Technology, United States\nTPb-3.3: IDENTIFIABILITY BOUNDS FOR BILINEAR INVERSE 1677\nPROBLEMS\nSunav Choudhary, Urbashi Mitra, University of Southern California, United States\nTPb-3.4: LOAD FORECASTING VIA LOW RANK AND SPARSE 


MATRIX  ...................................................1682\nFACTORIZATION\nSeung-Jun Kim, Georgios B Giannakis, University of Minnesota, United States\nxxvii\nTPb-3.5: SEMI-BLIND SOURCE SEPARATION VIA SPARSE 1687\nREPRESENTATIONS AND ONLINE DICTIONARY LEARNING\nSirisha Rambhatla, Jarvis Haupt, University of Minnesota - Twin Cities, United States\nTPb-4: LOCATION-AWARE NETWORKING\nTPb-4.1: ROBUST LINK SCHEDULING WITH CHANNEL ESTIMATION 1695\nAND LOCATION INFORMATION\nSrikar Muppirisetty, Rocco Di Taranto, Henk Wymeersch, Chalmers University of Technology, Sweden\nTPb-4.2: SIMULTANEOUS ROUTING AND POWER ALLOCATION USING  .................................................1700\nLOCATION INFORMATION\nRocco Di Taranto Henk Wymeersch, Chalmers University of Technology, Sweden\nTPb-4.3: LOCATION AWARE TRAINING SCHEME FOR D2D NETWORKS ..................................................1705\nDaoud Burghal, Andreas F. Molisch, University of Southern California, United States\nTPb-4.4: A COOPERATIVE HIGH-ACCURACY LOCALIZATION ALGORITHM 1709\nFOR IMPROVED ROAD WORKERS’ SAFETY\nSankalp Dayal, Adam Mortazavi, Khanh H. Huynh, University of California, Santa Barbara, United \nStates; Ramez L. Gerges California Department of Transportation, United States; John J. Shynk, \nUniversity of California, Santa Barbara, United States\nTPb-4.5: REAL-TIME ENERGY STORAGE MANAGEMENT WITH 1714\nRENEWABLE ENERGY OF ARBITRARY GENERATION DYNAMICS\nTianyi Li, Min Dong, University of Ontario Institute of Technology, Canada\nTPb-5: ANALYSIS OF COMPLEX BIOLOGICAL SYSTEMS AND OMICS DATA II\nTPb-5.2: STATISTICAL VALIDATION OF PARAMETRIC APPROXIMATIONS TO 1721\nTHE MASTER EQUATION\nGarrett Jenkinson, John Goutsias, The Johns Hopkins University, United States\nTPb-5.4: A MESSAGE-PASSING ALGORITHM FOR HAPLOTYPE ASSEMBLY 1726\nZrinka Puljiz, Haris Vikalo, University of Texas at Austin United States\nTPb-6: TARGET TRACKING I\nTPb-6.1: TRACK STATE AUGMENTATION FOR ESTIMATION OF 1733\nPROBABILITY OF DETECTION IN MULTISTATIC SONAR DATA\nEvan Hanusa, David Krout, University of Washington, United States\nTPb-6.2: HYPOTHESIS STRUCTURE IN ENHANCED 1738\nMULTIPLE-HYPOTHESIS TRACKING\nStefano Coraluppi, Craig Carthel, Compunetix Inc., United States; Marco Guerriero, SAIRA/FAR nAMERICAS Inc., United States\nTPb-6.3: SPLINE PROBABILITY HYPOTHESIS DENSITY FILTER FOR 1743\nNONLINEAR MANEUVERING TARGET TRACKING\nRajiv Sithravel, Xin Chen, McMaster University, Canada; Mike McDonald, Defence Research and \nDevelopment Canada Canada; Thia Kirubarajan, McMaster University, Canada\nTPb-6.4: PERFORMANCE ANALYSIS OF THE CONVERTED RANGE RATE  ...............................................1751\nAND POSITION LINEAR KALMAN FILTER\nSteven Bordonaro Naval Undersea Research Center, United States; Peter Willett, Yaakov Bar-Shalom, \nUniversity of Connecticut United States\nTPb-6.5: MAP-PF MULTITARGET TRACKING WITH PROPAGATION 1756\nMODELING UNCERTAINTIES\nKristine Bell, Robert Zarnich, Metron, United States\nTPb-7: MACHINE LEARNING AND STATISTICAL SIGNAL PROCESSING II\nTPb-7.1 FORWARD/BACKWARD STATE AND MODEL PARAMETER 1763\nESTIMATION FOR CONTINUUM-STATE HIDDEN MARKOV MODELS \(CHMM States\nxxviii\nTPb-7.2: LOW-RANK KERNEL LEARNING FOR ELECTRICITY MARKET 1768\nINFERENCE\nVassilis Kekatos, Yu Zhang, Georgios B Giannakis, University of Minnesota, United States\nTPb-7.3: HIERARCHICAL CLUSTERING METHODS AND ALGORITHMS 1773\nFOR ASYMMETRIC NETWORKS\nGunnar Carlsson, Stanford University, United States; Facundo Mémoli, University of Adelaide, \nAustralia; Alejandro Ribeiro, Santiago Segarra, University of Pennsylvania, United States\nTPb-7.5: ACHIEVING COMPLETE LEARNING IN MULTI-ARMED BANDIT 1778\nPROBLEMS\nSattar Vakili, Qing Zhao, University of California, Davis, United States\nTPb-8: DESIGN AUTOMATION\nTPb-8.1: MPMAP: A HIGH LEVEL SYNTHESIS AND MAPPING TOOL FOR  ................................................1785\nMPSOCS\nAmr Hussien, Ahmed M. Eltawil University of California, Irvine, United States; Rahul Amin, Jim \nMartin, Clemson University, United States\nTPb-8.2: SOFTWARE TOOL FOR FPGA BASED MIMO RADAR APPLICATIONS 1792\nAmin Jarrah, Mohsin M. Jamali, University of Toledo, United States\nTPb-8.3: MULTI-CLOCK DOMAIN OPTIMIZATION FOR 1796\nRECONFIGURABLE ARCHITECTURES IN HIGH-LEVEL DATAFLOW APPLICATIONS\nSimone Casale-Brunet, Endri Bezati, Claudio Alberti, Marco 


Mattavelli, École Polytechnique Fédérale \nde Lausanne \(EPFL Milano, Italy; Jörn Janneck, Lund \nUniversity, Sweden\nTPb-8.4: ACTOR CLASSIFICATION USING ACTOR MACHINES 1801\nGustav Cedersjö, Jörn Janneck, Lund University, Sweden\nTPb-8.5: SYSTEMS DESIGN SPACE EXPLORATION BY SERIAL DATAFLOW 1805\nPROGRAM EXECUTIONS\nSimone Casale-Brunet, Marco Mattavelli Claudio Alberti, École Polytechnique Fédérale de Lausanne \n\(EPFL Sweden\nTPb-8.7: REAL-TIME RADAR SIGNAL PROCESSING ON MASSIVELY 1810\nPARALLEL PROCESSOR ARRAYS\nZain Ul-Abdin, Halmstad University, Sweden; Anders Åhlander, Saab AB, Sweden; Bertil Svensson, \nHalmstad University, Sweden\nTPb-8.8 ALGORITHM AND ARCHITECTURE CO-DESIGN OF MIXTURE  ..................................................1815\nOF GAUSSIAN \(MOG States; Robert Bushey, Analog Devices Inc., \nUnited States; Gunar Schirner Schirner, Northeastern University United States\nTPb-8: MULTIUSER MIMO SYSTEMS\nTPb-8.1: MULTI-USER MIMO SCHEDULING IN THE FOURTH 1855\nGENERATION CELLULAR UPLINK\nNarayan Prasad, NEC Laboratories America, Inc., United States; Honghai Zhang, Google, United \nStates; Hao Zhu University of Illinois at Urbana-Champaign, United States; Sampath Rangarajan, \nNEC Laboratories America Inc., United States\nTPb-8.2: OPTIMAL DOF REGION OF THE TWO-USER MISO-BC WITH 1860\nGENERAL ALTERNATING CSIT\nJinyuan Chen, Petros Elia Eurecom, France\nTPb-8.3: EXPLOITING SPATIAL SPECTRUM HOLES IN MULTIUSER 1865\nMIMO SYSTEMS\nFeeby Salib, Karim Seddik, American University in Cairo, Egypt\nTPb-8.4: DEGREES OF FREEDOM ACHIEVED USING SUBSPACE 1869\nALIGNMENT CHAINS FOR THREE-CELL NETWORKS\nGokul Sridharan, Wei Yu, University of Toronto, Canada\nTPb-8.5: INTERFERENCE ALIGNMENT FOR MISO BROADCAST  ...............................................................1875\nCHANNELS UNDER JAMMING ATTACKS\nSaiDhiraj Amuru, Ravi Tandon, R. Michael Buehrer, T. Charles Clancy, Virginia Tech, United States\nxxix\nTPb-8.6: PERFORMANCE STUDY OF MRC AND IRC WEIGHTS IN 1880\nLTE/LTE-A SYSTEMS WITH INTERFERENCE MANAGEMENT\nThomas Svantesson, ArrayComm, United States\nTPb-8.8: A SYSTEM-LEVEL STUDY ON MULTI-USER MIMO 1885\nTRANSMISSION FOR DENSE FDD NETWORKS\nLars Thiele, Martin Kurras, Kai Börner, Thomas Haustein, Fraunhofer HHI, Germany\nTPb-8.9 DIVERSITY-MULTIPLEXING TRADEOFF OF MIMO LINEAR 1890\nPRECODING\nAhmed Mehana, Samsung Electronics, Co Ltd., United States; Aria Nosratinia, University of Texas at \nDallas, United States\nTPb-8: ELECTROPHYSIOLOGY AND BRAIN IMAGING\nTPb-8.1: JOINT COMPRESSION OF NEURAL ACTION POTENTIALS AND 1823\nLOCAL FIELD POTENTIALS\nSebastian Schmale, Benjamin Knoop, Janpeter Hoeffmann, Dagmar Peters-Drolshagen, Steffen Paul, \nUniversity of Bremen, Germany\nTPb-8.2 REDUCING THE EFFECT OF CORRELATED BRAIN SOURCES IN  ...............................................1828\nMEG USING A LINEARLY CONSTRAINED SPATIAL FILTER BASED ON MINIMUM \nNORM\nJosé Alfonso Sánchez De Lucio, David M Halliday, University of York, United Kingdom\nTPb-8.3: ONLINE BAYESIAN CHANGE POINT DETECTION ALGORITHMS 1833\nFOR SEGMENTATION OF EPILEPTIC ACTIVITY\nRakesh Malladi, Rice Unviersity, United States; Giridhar P Kalamangalam, University of Texas Health \nScience Center, United States Behnaam Aazhang, Rice Unviersity, United States\nTPb-8.4: SPIKING NEURAL NETWORKS BASED ON LIF WITH LATENCY 1838\nSIMULATION AND SYNCHRONIZATION EFFECTS\nGian Carlo Cardarilli, Alessandro Cristini, Marco Re, Mario Salerno, Gianluca Susi, University of \nRome Tor Vergata Italy\nTPb-8.5: TIME-FREQUENCY ANALYSIS OF BRAIN ELECTRICAL SIGNALS 1843\nFOR BEHAVIOUR RECOGNITION IN PATIENTS WITH PARKINSON’S DISEASE\nHuaiguang Jiang, Jun Jason Zhang, University of Denver, United States; Adam Hebb, Colorado nNeurological Institute, United States; Mohammad H. Mahoor, University of Denver, United States\nTPb-8.7: A MEASURE OF CONNECTIVITY IN THE PRESENCE OF 1848\nCROSSTALK\nSergul Aydore, Syed Ashrafulla Anand Joshi, Richard M Leahy, University of Southern California, \nUnited States\nWAa-1: MIMO INTERFERENCE MANAGEMENT\nWAa-1.1: DEGREES OF FREEDOM FOR THE CONSTANT MIMO 1897\nINTERFERENCE CHANNEL WITH COMP TRANSMISSION\nCraig Wilson, Venugopal V. Veeravalli, University of Illinois at Urbana-Champaign, United 


States\nWAa-1.2: DYNAMIC INTERFERENCE MANAGEMENT 1902\nAly El Gamal Venugopal V. Veeravalli, University of Illinois at Urbana-Champaign, United States\nWAa-1.3: A MUD/RATE SELECTION TOOL FOR COGNITIVE RADIOS IN  ..................................................1907\nPACKET BASED ASYNCHRONOUS GAUSSIAN MULTIPLE ACCESS CHANNELS\nPrabahan Basu, Rachel Learned, MIT Lincoln Laboratory, United States\nWAa-1.4: PRECODER DESIGN FOR FRACTIONAL INTERFERENCE 1912\nALIGNMENT\nHari Ram Balakrishnan, Giridhar K Indian Institute of Technology Madras, India\nWAa-2: OFDM\nWAa-2.1: MIMO-OFDM OUTAGE CHANNEL CAPACITY WITH PRACTICAL  .............................................1919\nIMPERFECT CSI\nMarko Kocic, MIT Lincoln Laboratory, United States; Nicholas Chang, Applied Communication \nSciences, United States; Matthew Ferreira MIT Lincoln Laboratory, United States\nxxx\nWAa-2.2: BIASED ESTIMATION OF SYMBOL TIMING OFFSET IN OFDM 1924\nSYSTEMS\nRohan Ramlall, University of California, Irvine United States\nWAa-2.3: A FACTOR-GRAPH APPROACH TO JOINT OFDM CHANNEL 1929\nESTIMATION AND DECODING IN IMPULSIVE NOISE CHANNELS\nMarcel Nassar, University of Texas at Austin, United States; Philip Schniter, The Ohio State University, \nUnited States; Brian Evans, University of Texas at Austin, United States\nWAa-2.4: WIDELY LINEAR DATA ESTIMATION FOR UNIQUE WORD  ........................................................1934\nOFDM\nMario Huemer, Alexander Onic, Christian Hofbauer, Stefan Trampitsch, Johannes Kepler University \nLinz Austria\nWAa-3: ADAPTIVE FILTERING\nWAa-3.1: A GRADIENT-CONTROLLED PROPORTIONATE TECHNIQUE FOR 1941\nACOUSTIC ECHO CANCELLATION\nJie Yang, Texas Instruments United States; Gerald Sobelman, University of Minnesota, United States\nWAa-3.2: INTERFERENCE IDENTIFICATION IN CELLULAR NETWORKS  .................................................1946\nVIA ADAPTIVE PROJECTED SUBGRADIENT METHODS\nKonstantin Oltmann, Renato L. G. Cavalcante, Slawomir Stanczak, Martin Kasparick, Fraunhofer \nHeirinch Hertz Institute, Germany\nWAa-3.3: A RECONSIDERATION OF IMPROVED PNLMS ALGORITHM 1951\nFROM METRIC COMBINING VIEWPOINT\nOsamu Toda, Masahiro Yukawa, Keio University, Japan\nWAa-3.4: DETECTION PERFORMANCE OF MATCHED TRANSMIT 1956\nWAVEFORM FOR MOVING EXTENDED TARGETS\nRic Romero, Naval Postgraduate School, United States\nWAa-4: RELAYING AND COOPERATION\nWAa-4.1: TWO-WAY AMPLIFY-AND-FORWARD RELAY STRATEGIES  .......................................................1963\nUNDER RELAY POWER CONSTRAINT\nKanghee Lee, Hyuck M. Kwon, Edwin M. Sawan, Wichita State University, United States Hyuncheol \nPark, Korea Advanced Institute of Science and Technology, Republic of Korea\nWAa-4.2: GAUSSIAN INTERFERING RELAY CHANNELS...............................................................................1968\nHieu T. Do, Tobias J. Oechtering, Mikael Skoglund, KTH Royal Institute of Technology, Sweden; Mai \nVu, Tufts University, United States\nWAa-4.3: THROUGHPUT IMPROVEMENTS FOR CELLULAR SYSTEMS 1973\nWITH DEVICE-TO-DEVICE COMMUNICATIONS\nPhuongBang Nguyen, Bhaskar D. Rao, University of California, San Diego, United States\nWAa-4.4: COOPERATIVE SIMULTANEOUS LOCALIZATION AND  ...............................................................1978\nSYNCHRONIZATION: A DISTRIBUTED HYBRID MESSAGE PASSING ALGORITHM\nBernhard Etzlinger, Johannes Kepler University, Austria; Florian Meyer, Vienna University of \nTechnology, Austria; Andreas Springer, Johannes Kepler University, Austria; Franz Hlawatsch, Vienna \nUniversity of Technology, Austria; Henk Wymeersch, Chalmers University of Technology Sweden\nWAa-5: IMAGE ANALYSIS AND PROCESSING\nWAa-5.1: MULTISCALE AM-FM IMAGE RECONSTRUCTIONS BASED ON 1985\nELASTIC NET REGRESSION AND GABOR FILTERBANKS\nIoannis Constantinou, University of Cyprus, Cyprus; Marios Pattichis, University of New Mexico, \nUnited States Constantinos Pattichis, University of Cyprus, Cyprus\nWAa-5.2: COLORIZATION BASED ON PIECEWISE AUTOREGRESSIVE 1990\nMODEL\nYasuhiro Nakajima, Takashi Ueno, Taichi Yoshida, Masaaki Ikehara, Keio University, Japan\nWAa-5.3: IMAGE DENOISING BY ADAPTIVE DIRECTIONAL 1995\nLIFTING-BASED DISCRETE WAVELET TRANSFORM AND QUANTIZATION\nNaoki Furuhashi, Azusa Oota, Taichi Yoshida, Masaaki Ikehara, Keio University Japan\nxxxi\nWAa-5.4: INTRODUCING DIVERSITY TO NORMALIZED CROSS 2000\nCORRELATION FOR DENSE IMAGE REGISTRATION\nNafise Barzigar, Aminmohammad Roozgard, Pramode Verma, Samuel Cheng, University of Oklahoma nUnited States\nWAa-6: MULTI-SENSOR SIGNAL PROCESSING\nWAa-6.1: WHY DOES DIRECT-MUSIC ON SPARSE-ARRAYS WORK 2007\nP. P Vaidyanathan, Piya Pal, California 


Institute of Technology, United States\nWAa-6.2: ASYMPTOTICALLY OPTIMAL TRUNCATED HYPOTHESIS TEST 2012\nFOR A LARGE SENSOR NETWORK DESCRIBED BY A MULTIVARIATE GAUSSIAN \nDISTRIBUTION\nJiangfan Zhang, Rick Blum, Lehigh University, United States\nWAa-6.3: A JOINT LOCALIZATION AND SYNCHRONIZATION TECHNIQUE  ............................................2017\nUSING TIME OF ARRIVAL AT MULTIPLE ANTENNA RECEIVERS\nSiamak Yousefi, Xiao-Wen Chang, Benoit Champagne, McGill University Canada\nWAa-6.4: REDUCING THE FRACTIONAL RANK OF INTERFERENCE WITH 2022\nSPACE-TIME-FREQUENCY ADAPTIVE BEAMFORMING\nShawn Kraut, Adam R. Margetts, MIT Lincoln Laboratory, United States; Daniel Bliss, Arizona State \nUniversity, United States\nWAa-7: COMMUNICATION SYSTEM DESIGN\nWAa-7.1: IMPLEMENTATION OF SELECTIVE PACKET DESTRUCTION ON 2029\nWIRELESS OPEN-ACCESS RESEARCH PLATFORM\nStephen Hughes Bosheng Zhou, Roger Woods, Queen’s University Belfast, United Kingdom; Alan \nMarshall, Unievrsity of Liverpool, United Kingdom\nWAa-7.2: EFFICIENT ERROR-AWARE POWER MANAGEMENT FOR 2034\nMEMORY DOMINATED OFDM SYSTEMS\nMuhammad S Khairy, Ahmed M. Eltawil, Fadi J. Kurdahi, University of California, Irvine, United \nStates; Amin Khajeh Intel labs, United States\nWAa-7.3: FPGA IMPLEMENTATION OF A MESSAGE-PASSING OFDM 2041\nRECEIVER FOR IMPULSIVE NOISE CHANNELS\nKarl Nieman, University of Texas at Austin, United States; Marcel Nassar, Samsung Information \nSystems America United States; Jing Lin, Brian Evans, University of Texas at Austin, United States\nWAa-7.4: MOBILE TRANSMITTER DIGITAL PREDISTORTION:  ...................................................................2046\nFEASIBILITY ANALYSIS, ALGORITHMS AND DESIGN EXPLORATION\nMahmoud Abdelaziz, Tampere University of Technology, Finland Amanullah Ghazi, University of \nOulu, Finland; Lauri Anttila, Tampere University of Technology, Finland; Jani Boutellier, University of \nOulu, Finland; Toni Lähteensuo, Tampere University of Technology, Finland; Xiaojia Lu, University of \nOulu, Finland; Joseph R. Cavallaro, Rice University, United States; Shuvra Bhattacharyya University \nof Maryland, United States; Markku Juntti, University of Oulu, Finland; Mikko Valkama, Tampere nUniversity of Technology, Finland\nWAb-1: MIMO PROCESSING\nWAb-1.1: MMSE RECEIVE FILTERING FOR PRECODED MIMO SYSTEMS .................................................2057\nAhmed Mehana, Samsung Electronics, Co., Ltd United States; Aria Nosratinia, University of Texas at \nDallas, United States\nWAb-1.2: COVERAGE IN DENSE MILLIMETER WAVE CELLULAR  ............................................................2062\nNETWORKS\nTianyang Bai, Robert W. Heath, Jr., The University of Texas at Austin, United States\nWAb-1.3: LINEAR PRECODING FOR MIMO WITH LDPC CODING AND  .....................................................2067\nREDUCED RECEIVER COMPLEXITY\nThomas Ketseoglou, California State University, Pomona, United States; Ender Ayanoglu, University nof California, Irvine, United States\nxxxii\nWAb-1.4: OPTIMAL PILOT BEAM PATTERN DESIGN FOR MASSIVE MIMO 2072\nSYSTEMS\nSong Noh, Michael D. Zoltowski, Purdue University United States; Youngchul Sung, Korea Advanced \nInstitute of Science and Technology, Republic of Korea; David J. Love, Purdue University, United \nStates\nWAb-2: ADVANCES IN CODING AND DECODING\nWAb-2.1: EFFICIENTLY ENCODABLE NON-BINARY GENERALIZED LDPC  .............................................2079\nCODES\nNicholas Chang Applied Communication Sciences, United States; Marko Kocic, MIT Lincoln \nLaboratory, United States\nWAb-2.2 PRACTICAL NON-BINARY RATELESS CODES FOR WIRELESS 2084\nCHANNELS\nDavid Romero, Massachusetts Institute of Technology, United States; Nicholas Chang, Applied \nCommunication Sciences, United States; Adam R. Margetts Massachusetts Institute of Technology, \nUnited States\nWAb-2.3: ON THE OPTIMALITY OF POLAR CODES FOR THE 2089\nDETERMINISTIC WIRETAP CHANNE\nAli Fakoorian, A. Lee Swindlehurst, University of California, Irvine, United States\nWAb-2.4: DELAY-OPTIMAL STREAMING CODES UNDER 2094\nSOURCE-CHANNEL RATE MISMATCH\nPratik Patil, Ahmed Badr, Ashish Khisti, University of Toronto, Canada; Wai-Tian Tan Hewlett-\nPackard Labs, United States\nWAb-3: DETECTION\nWAb-3.1: ASYNCHRONOUS SIGNAL DETECTION IN 2103\nFREQUENCY-SELECTIVE NON-GAUSSIAN CHANNELS\nSaiDhiraj Amuru, Daniel Jakubisin, R. Michael Buehrer, Virginia Tech, United States Claudio da \nSilva, Samsung Electronics, Co., Ltd., United States\nWAb-3.2: AN INFORMATION THEORETIC CHARACTERIZATION OF THE  ................................................2108\nCHANNEL SHORTENING RECEIVER\nFredrik Rusek, Ove Edfors, Lund University, Sweden\nWAb-3.3: ITERATIVE MMSE-SIC RECEIVER WITH LOW-COMPLEXITY  ...................................................2113\nSOFT SYMBOL AND RESIDUAL INTERFERENCE ESTIMATIONS\nGuosen Yue, Narayan Prasad, Sampath Rangarajan, NEC Laboratories America, Inc., United 


States\nWAb-3.4: NEW RESULTS IN THE ANALYSIS OF DECISION-FEEDBACK 2118\nEQUALIZERS\nAhmed Mehana, Samsung Electronics, Co Ltd., United States; Aria Nosratinia, University of Texas at \nDallas, United States\nWAb-5: TARGET TRACKING II\nWAb-5.1: POSTERIOR DISTRIBUTION PREPROCESSING FOR PASSIVE 2125\nDTV RADAR TRACKING: SIMULATED AND REAL DATA\nEvan Hanusa, Laura Vertatschitsch, David Krout, University of Washington, United States\nWAb-5.2: DEPTH-BASED PASSIVE TRACKING OF SUBMERGED SOURCES  ............................................2130\nIN THE DEEP OCEAN USING A VERTICAL LINE ARRAY\nLisa Zurk, John K. Boyle, Jordan Shibley, Portland State University, United States\nWAb-5.3: GENERALIZED LINEAR MINIMUM MEAN-SQUARE ERROR 2133\nESTIMATION WITH APPLICATION TO SPACE-OBJECT TRACKING\nYu Liu, X. Rong Li, Huimin Chen, University of New Orleans, United States\nWAb-5.4: FEATURE-AIDED INITIATION AND TRACKING VIA TREE SEARCH ..........................................2138\nHossein Roufarshbaf Jill Nelson, George Mason University, United States\nxxxiii\nWAb-6: DIRECTION OF ARRIVAL ESTIMATION\nWAb-6.1: A SELF-CALIBRATION TECHNIQUE FOR DIRECTION 2145\nESTIMATION WITH DIVERSELY POLARIZED ARRAYS\nBenjamin Friedlander, University of California, Santa Cruz, United States\nWAb-6.2: CRAMER-RAO PERFORMANCE BOUNDS FOR SIMULTANEOUS  ..............................................2150\nTARGET AND MULTIPATH POSITIONING\nLi Li, Jeff Krolik, Duke University, United States\nWAb-6.3: COPY CORRELATION DIRECTION-OF-ARRIVAL ESTIMATION  .................................................2155\nPERFORMANCE WITH A STOCHASTIC WEIGHT VECTOR\nChrist Richmond, Keith Forsythe, MIT Lincoln Laboratory, United States; Christopher Flynn, Stevens nInstitute of Technology, United States\nWAb-6.4: LOCATING CLOSELY SPACED COHERENT EMITTERS USING 2160\nTDOA TECHNIQUES\nJack Reale, Air Force Research Laboratory / Binghamton University, United States; Lauren Huie, Air \nForce Research Laboratory, United States Mark Fowler, State University of New York at Binghamton, \nUnited States\nWAb-7: ENERGY- AND RELIABILITY-AWARE DESIGN\nWAb-7.1: LOW-ENERGY ARCHITECTURES FOR SUPPORT VECTOR 2167\nMACHINE COMPUTATION\nManohar Ayinala, Keshab K Parhi, University of Minnesota, United States\nWAb-7.2: TRUNCATED MULTIPLIERS THROUGH POWER-GATING FOR 2172\nDEGRADING PRECISION ARITHMETIC\nPietro Albicocco, Gian Carlo Cardarilli, University of Rome Tor Vergata, Italy; Alberto Nannarelli, \nTechnical University of Denmark Denmark; Massimo Petricca, Politecnico di Torino, Italy; Marco Re, \nUniversity of Rome Tor Vergata Italy\nWAb-7.3: A LOGARITHMIC APPROACH TO ENERGY-EFFICIENT GPU 2177\nARITHMETIC FOR MOBILE DEVICES\nMiguel Lastras Behrooz Parhami, University of California, Santa Barbara, United States\nWAb-7.4: ON SEPARABLE ERROR DETECTION FOR ADDITION ..................................................................2181\nMichael Sullivan, Earl Swartzlander, University of Texas at Austin, United States\nWPb-1: PAPERS PRESENTED IN 2012\nWPb-1.1 DYNAMICALLY RECONFIGURABLE AVC DEBLOCKING FILTER  .............................................2189\nWITH POWER AND PERFORMANCE CONSTRAINTS\nYuebing Jiang, Marios Pattichis, University of New Mexico\nxxxiv\n 


on science teams for numerous planetary missions including Magellan, Mars Observer, Mars Global Surveyor and Rosetta. He was the US Project Scientist for the international Mars NetLander mission, for which he was also principal investigator of the Short-Period Seismometer experiment, and is currently the Project Scientist for the Mars Exploration Rovers. He led the Geophysics and Planetary Geology group at JPL from 1993-2005, and is the JPL Discipline Program Manager for Planetary Geosciences. He has held several visiting appointments at the Institut de Physique du Globe de Paris. He has a BS in physics and a PhD in geophysics from the University of Southern California  David Hansen is a member of the technical staff in the Communications Systems and Operations Group at the Jet Propulsion Laboratory. Current work includes the development of the telecom subsystem for the Juno project. David received a B.S. in Electrical Engineering from Cornell University and an M.S. in Electrical Engineering from Stanford University  Robert Miyake is a member of the technical staff in the Mission and Technology Development Group at the Jet Propulsion Laboratory. Current work includes the development of thermal control subsystems for interplanetary flagship missions to Jupiter and Saturn missions to Mars and the Earth Moon, and is the lead Thermal Chair for the Advanced Project Design Team Robert graduated with a B. S. from San Jose State University, with extensive graduate studies at UCLA University of Washington, and University of Santa Clara  Steve Kondos is a consultant to the Structures and Mechanisms group at the Jet Propulsion Laboratory. He currently is generating the mechanical concepts for small Lunar Landers and Lunar Science Instrument packages in support of various Lunar mission initiatives. He also provides conceptual design, mass and cost estimating support for various Team X studies as the lead for the Mechanical Subsystem Chair. Steve is also involved with various other studies and proposals and provides mentoring to several young mechanical and system engineers. He graduated with a B.S. in Mechanical Engineering from the University of California, Davis and has 28 years of experience in the aerospace field ranging from detail part design to system of systems architecture development. He has worked both in industry and in government in defense, intelligence commercial and civil activities that range from ocean and land based systems to airborne and space systems. Steve has received various NASA, Air Force, Department of Defense and other agency awards for his work on such projects as the NASA Solar Array Flight Experiment, Talon Gold, MILSTAR, Iridium, SBIRS, Mars Exploration Rovers ATFLIR, Glory Aerosol Polarimeter System and several Restricted Programs  Paul Timmerman is a senior member of technical staff in the Power Systems Group at the Jet Propulsion Laboratory Twenty-five years of experience in spacecraft design including 22 at JPL, over 250 studies in Team-X, and numerous proposals. Current assignments include a wide variety of planetary mission concepts, covering all targets within the solar system and all mission classes. Paul graduated from Loras College with a B.S. in Chemistry in 1983  Vincent Randolph is a senior engineer in the Advanced Computer Systems and 


the Advanced Computer Systems and Technologies Group at the Jet Propulsion Laboratory. Current work includes generating Command and Data Handling Subsystem conceptual designs for various proposals and Team X.  He also supports Articulation Control and Electronics design activities for the Advanced Mirror Development project. Vincent graduated from the University of California at Berkeley with a B.S. in Electrical Engineering 18  pre></body></html 


i models into time and covariate dependent dynamic counterparts  ii models and reliability analysis in a more realistic manner  iii level  whether or not functional components \(loyal generals diagnose correctly and take proper actions such as fault mask of failed components \(traitors asymmetric  iv survivability analysis. Evolutionary game modeling can derive sustainable or survivable strategies \(mapped from the ESS in EGT such as node failures such as security compromise level modeling in the so-called three-layer survivability analysis developed in Ma \(2008a this article  v offer an integrated architecture that unite reliability survivability, and fault tolerance, and the modeling approaches with survival analysis and evolutionary game theory implement this architecture. Finally, the dynamic hybrid fault models, when utilized to describe the survival of players in EGT, enhance the EGT's flexibility and power in modeling the survival and behaviors of the game players which should also be applicable to other problem domains where EGT is applicable  5. OPERATIONAL LEVEL MODELING AND DECISION-MAKING  5.1. Highlights of the Tactical and Strategic Levels  Let's first summarize what are obtainable at both tactical and strategic levels. The results at both tactical and strategic levels are precisely obtainable either via analytic or simulation optimization. With the term precisely, we mean that there is no need to assign subjective probabilities to UUUR events. This is possible because we try to assess the consequences of UUUR events \(tactical level ESS strategies \(strategic level time prediction of survivability. The following is a list of specific points. I use an assumed Wireless Sensor Network WSN  i of UUUR events: \(a actions which can be treated as censored events; \(b Cont' of Box 4.2 It can be shown that the replicator differential equations are equivalent to the classical population dynamics models such as Logistic differential equation and LotkaVolterra equation \(e.g., Kot 2001 Logistic equation, or the limited per capital growth rate is similar to the change rate of the fitness  xfxfi which can be represented with the hazard function or survivor functions introduced in the previous section on survival analysis.  This essentially connects the previous survival analysis modeling for lifetime and reliability with the EGT modeling. However, EGT provides additional modeling power beyond population dynamics or survival analysis approaches introduced in the previous section. The introduction of evolutionary theory makes the games played by a population evolvable. In other words, each player \(individual 


other words, each player \(individual agent and players interact with each other to evolve an optimized system Box 4.3. Additional Comments on DHF Models  The above introduced EGT models are very general given they are the system of ordinary differential equations. Furthermore, the choice of fitness function f\(x complexity to the differential equation system.  The system can easily be turned into system of nonlinear differential equations. The analytical solution to the models may be unobtainable when nonlinear differential equations are involved and simulation and/or numerical computation are often required  In the EGT modeling, Byzantine generals are the game players, and hybrid fault models are conveniently expressed as the strategies of players; the players may have different failure or communication behaviors Furthermore, players can be further divided into groups or subpopulations to formulate more complex network organizations. In the EGT modeling, reliability can be represented as the payoff \(fitness, the native term in EGT of the game. Because reliability function can be replaced by survivor function, survival analysis is seamlessly integrated into the EGT modeling. That is, let Byzantine generals play evolutionary games and their fitness reliability function  The evolutionary stable strategy \(ESS counterpart of Nash equilibrium in traditional games ESS corresponds to sustainable strategies, which are resistant to both internal mutations \(such as turning into treason generals or nodes such as security compromises represent survivable strategies and survivability in survivability analysis. Therefore, dynamic hybrid fault models, after the extension with EGT modeling, can be used to study both reliability and survivability 13 risks such as competing risks which can be described with CRA; \(c captured with the shard frailty.  We believe that these UUUR events are sufficiently general to capture the major factors/events in reliability, security and survivability whose occurrence probabilities are hard or impossible to obtain  Instead of trying to obtain the probabilities for these events which are infeasible in most occasions, we focus on analyzing the consequences of the events.  With survival analysis, it is possible to analyze the effects of these types of events on survivor functions. In addition, spatial frailty modeling can be utilized to capture the heterogeneity of risks in space, or the spatial distribution of risks \(Ma 2008a d UUUR events introduced previously. These approaches and models that deal with the effects of UUUR events form the core of tactical level modeling  To take advantage of the tactical level modeling approaches it is obviously necessary to stick to the survivor functions or hazard functions models. In other words, survival analysis can deal with UUUR events and offer every features reliability function provides, but reliability function cannot deal with UUUR events although survivor function and reliability function have the exactly same mathematical definition. This is the junction that survival analysis plays critical role in survivability analysis at tactical level. However, we 


recognize that it is infeasible to get a simple metric for survivability similar to reliability with tactical level modeling alone. Actually, up to this point, we are still vague for the measurement of survivability or a metric for survivability. We have not answered the question: what is our metric for survivability? We think that a precise or rigorous definition of survivability at tactical level is not feasible, due to the same reason we cited previously  the inability to determine the probabilities of UUUR events However, we consider it is very helpful to define a work definition for survivability at the tactical level  We therefore define the survivability at tactical level as a metric, Su\(t t function or reliability function with UUUR events considered. In the framework of three-layer survivability analysis, this metric is what we mean with the term survivability. The "metric" per se is not the focus of the three-layer survivability analysis. It is not very informative without the supports from the next two levels  strategic and operational models.  However, it is obvious that this metric sets a foundation to incorporate UUUR effects in the modeling at the next two levels  Due to the inadequacy of tactical level modeling, we proposed the next level approach  strategic level modeling for survivability. As expected, the tactical level is one foundation of strategic level modeling ii objectives: \(a affect survivability which survival analysis alone is not adequate to deal with; \(b survivability at tactical level is necessary but not sufficient for modeling survivability, we need to define what is meant with the term survivability at strategic level  With regard to \(a behaviors or modes which have very different consequences. These failure behaviors can be captured with hybrid fault models. However, the existing hybrid fault models in fault tolerance field are not adequate for applying to survivability analysis. There are two issues involved: one is the lack of real time notion in the constraints for hybrid fault models \(e.g., N&gt;3m+1 for Byzantine Generals problem synthesize the models after the real-time notions are introduced. The solution we proposed for the first issue is the dynamic hybrid fault models, which integrate survivor functions with traditional hybrid fault models. The solution we proposed for the second issue is the introduction of EGT modeling  With regard to \(b modeling our problem at strategic level, EGT modeling is essentially a powerful optimization algorithm.  One of the most important results from EGT modeling is the so-called evolutionary stable strategies \(ESS We map the ESS in EGT to survivable strategies in survivability analysis.   Therefore, at the strategic level, our work definition for survivability refers to the survivable strategies or sustainable strategies in the native term of EGT, which can be quantified with ESS  In addition to integrating dynamic hybrid fault models another advantage for introducing EGT modeling at strategic level is the flexibility for incorporating other node behaviors \(such as cooperative vs. non-cooperative those behaviors specified in standard hybrid fault models, as well as anthropocentric factors such as costs constraints  Without UUUR events, both tactical and strategic level 


Without UUUR events, both tactical and strategic level models default to regular reliability models. This implies that, in the absence of UUUR events, reliable strategies are sustainable or survivable.  This also implies that three-layer survivability analysis defaults to reliability analysis however, the three-layer approach does offer some significant advantages over traditional reliability analysis, as discussed in previous sections. Nevertheless, when UUUR events exist, reliable strategies and survivable strategies are different. This necessitates the next operational level modeling  5.2. Operational Level Modeling and Decision-Making  When UUUR events are involved, we cannot make real time predictions of survivability at tactical and strategic levels This implies that the implementations of survivable 14 strategies need additional measures that we develop in this section.  Box 5.1 explains the ideas involved with possibly the simplest example  Figure 4 is a diagram showing a simplified relationship between action threshold survivability \(TS survivability \(ES view since both TS and ES are multidimensional and dynamic in practice. Therefore, the sole purpose of the diagram is to illustrate the major concepts discussed above The blue curve is the survivability when survivable strategies specified by ESS are implemented at some point before time s.  The system is then guaranteed to hold survivability above ES. In contrary, if no ESS implemented before time s, then the system quickly falls below to the survivable level at around 40 time units  T i m e 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 1 0 0 Su rv iv ab ili ty M et ric S u t 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 E S S  i s  I m p lm e n t e d N o  E S S  is  I m p lm e n t e d ts E S T S  Figure 4. A Diagram Showing the Relationship Between TS and ES, as well as timing of s and t, with s &lt; t  6. SUMMARY  The previous sections discussed the major building blocks 


The previous sections discussed the major building blocks for the new life-system inspired PHM architecture. This section first identifies a few minor aspects that have not been discussed explicitly but are necessary for the implementation of the architecture, and then we summarize the major building blocks in a diagram  6.1. Missing Components and Links  Optimization Objectives  Lifetime, reliability, fault tolerance, and survivability, especially the latter two, are application dependent. Generally, the optimization of reliability and survivability are consistent; in that maximization of reliability also implies maximization of survivability. However, when application detail is considered, optimization of lifetime is not necessarily consistent with the optimization of reliability. Consider the case of the monitoring sensor network as an example. The network reliability is also dependent on connectivity coverage, etc, besides network lifetime. What may be further complicated is the time factor. All of the network metrics are time-dependent. A paradoxical situation between lifetime and reliability could be that nodes never 'sleep                                                   


          Box 5.1 Operational Level Modeling  Assuming that the ESS solution for a monitoring sensor network can be expressed with the following simple algebraic conditions: survivability metric at tactical level SU = 0.7, Router-Nodes in the WSN &gt; 10%, Selfish Nodes &lt; 40%. Even with this extremely simplified scenario, the ESS strategies cannot be implemented because we do not know when the actions should be taken to warrant a sustainable system.  These conditions lack a correlation with real time  The inability to implement ESS is rooted in our inability to assign definite probabilities to UUUR events, which implies that we cannot predict when something sufficiently bad will jeopardize the system survivability What we need at the operational level is a scheme to ensure ESS strategy is in place in advance  The fundamental idea we use to implement the ESS strategy is to hedge against the UUUR events. The similar idea has been used in financial engineering and also in integrated pest management in entomology. This can be implemented with the following scheme  Let us define a pair of survivability metrics: one is the expected survivability \(ES threshold survivability or simply threshold survivability \(TS ES is equivalent to the survivability metric at tactical level. ES corresponds to ESS at strategic level, but they are not equivalent since ESS is strategy and ES is survivability. TS is the survivability metric value \(at tactical level and TS can be obtained from strategic level models. For example, TS = SU\(s t condition for the implementation of ESS. In other words, the implementation of strategies that ensures TS at time s will guarantee the future ES level at time t.  To make the implementation more reliable and convenient multiple dynamic TSs can be computed at time s1, s2 sk, with si &lt; t for all i.  These TS at times s1, s2, ..., sk should be monitored by some evaluation systems  Unlike tactical and strategic levels, the operational level modeling is approximate. The term "approximate means that we cannot predict the real time survivability or we do not know the exact time an action should be taken. Instead, the action is triggered when the monitored survivability metric SU\(r survivability \(TS scheme of TS and ES, we ensure the ES by taking preventative actions \(prescribed by ESS and triggered by the TS consequences of UUUR events  Figure 4 is a diagram showing the above concepts and the decision-making process involved 15 This wakefulness \(never 'sleep short period but at the expense of network lifetime. Of course, when the network is running out of lifetime, network reliability ultimately crashes. This example reminds us that 


reliability ultimately crashes. This example reminds us that multi-objective optimization should be the norm rather than exception  Constraints and Extensions  Many application specific factors and constraints are ignored in this article. For example, we mentioned about spatial heterogeneity of environment, but never present a mathematical description The spatial heterogeneity can be modeled with the so-called spatial frailty in multivariate survival analysis \(Ma 2008a  Evolutionary Algorithm  Evolutionary game modeling when implemented in simulation, can be conveniently implemented with an algorithm similar to Genetic Algorithms \(GA ESS in the evolutionary game model with simulation is very similar to GA. Dynamic populations, in which population size varies from generation to generation \(Ma &amp; Krings 2008f of node failures. Another issue to be addressed is the synchronous vs. asynchronous updating when topology is considered in the simulation. This update scheme can have profound influences on the results of the simulation. Results from cellular automata computing should be very useful for getting insights on the update issue  6.2. Summary and Perspective  To recapture the major points of the article, let us revisit Figure 3, which summarizes the principal modules of the proposed life-system inspired PHM architecture. The main inspiration from life systems is the notion of individuals and their assemblage, the population. Population is an emergent entity at the next level and it has emergent properties which we are often more concerned with. Survival analysis, which has become a de facto standard in biomedicine, is particularly suitable for modeling population, although it is equally appropriate at individual level. Therefore, survival analysis \(including competing risks analysis and multivariate survival analysis comprehensively in the context of PHM in a series of four papers presented at IEEE AeroSpace 2008 \(Ma &amp; Krings 2008a, b, c, &amp; d proposed architecture. Survival analysis constitutes the major mathematical tools for analyzing lifetime and reliability, and also forms the tactical level of the three-layer survivability analysis  Besides lifetime and reliability, two other major modules in Figure 3 are fault tolerance and survivability. To integrate fault tolerance into the PHM system, Dynamic Hybrid Fault DHF 2008e, Ma 2008a make real-time prediction of reliability more realistic and make real-time prediction of fault tolerance level possible DHF models also unite lifetime, reliability and fault tolerance under a unified modeling framework that consists of survival analysis and evolutionary game theory modeling  DHG models also form the partial foundation, or strategic level, for the three-layer survivability analysis. At the strategic level, the Evolutionary Stable Strategies \(ESS which is mapped to survivable or sustainable strategies, can be obtained from the evolutionary game theory based DHF models. When there is not any UUUR event involved reliability and survivability are consistent, and reliable strategies are survivable. In this case, the strategic level modeling up to this point is sufficient for the whole PHM system modeling, and there is no need for the next level  operational level modeling  When there are UUUR events in a PHM system, the 


When there are UUUR events in a PHM system, the inability to determine the occurrence probabilities of UUUR events makes the operational level modeling necessary Then the principle of hedging must be utilized to deal with the "hanging" uncertainty from UUUR events. In this case reliability strategies are not necessarily survivable strategies At the operational level modeling, a duo of survivability metrics, expected survivability \(ES survivability \(TS the survivable strategies \(ESS level are promptly implemented based on the decisionmaking rules specified with the duo of survivability metrics then the PHM system should be able to endure the consequences of potentially catastrophic UUUR events. Of course, to endure such catastrophic events, the cost may be prohibitively high, but the PHM system will, at least, warn decision-makers for the potentially huge costs.  It might be cheap to just let it fail  Figure 3 also shows several other modules, such as security safety, application systems \(such as Automatic Logistics CBM+, RCM, Life cycle cost management, Real-time warning and alert systems architectures, but we do not discuss in this paper. Generally the new architecture should be fully compatible with existing ones in incorporating these additional modules. One point we stressed is that PHM system can be an ideal place to enforce security policies. Enforcing security policies can be mandatory for PHM systems that demand high security and safety such as weapon systems or nuclear plant facilities.  This is because maintenance, even without human-initiated security breaches, can break the security policies if the maintenance is not planned and performed properly  In perspective, although I did not discuss software issues in this paper, the introduced approaches and models should provide sufficient tools for modeling software reliability and survivability with some additional extension. Given the critical importance of software to modern PHM systems, we present the following discussion on the potential extension to software domain. Specifically, two points should be noted: \(1 architecture to software should be a metric which can 16 replace the time notion in software reliability; I suggest that the Kolmogorov complexity \(e.g., Li and Vitanyi 1997 be a promising candidate \(Ma 2008a change is because software does not wear and calendar time for software reliability usually does not make much sense 2 software reliability modeling.  Extending to general survivability analysis is not a problem either. In this article I implicitly assume that reliability and survivability are positively correlated, or reliability is the foundation of survivability. This positive correlation does not have to be the case. A simplified example that illustrates this point is the 'limit order' in online stock trading, in which limit order can be used in either direction: that stock price is rising or falling.  The solution to allow negative or uncorrelated relationships between reliability and survivability are very straightforward, and the solutions are already identified in previous discussions. Specifically, multiple G-functions and multi-stage G-functions by Vincent and Brown \(2005 very feasible solution, because lifetime, reliability and survivability may simply be represented with multiple Gfunctions. Another potential solution is the accommodation of the potential conflicts between reliability and survivability with multi-objective GA algorithms, which I previously suggested to be used as updating algorithms in the optimization of evolutionary games  


 The integration of dynamic hybrid fault models with evolutionary game modeling allows one to incorporate more realistic and detailed failure \(or survival individual players in an evolutionary game. This is because dynamic hybrid fault models are supported by survival analysis modeling, e.g., time and covariate dependent hazard or survivor functions for individual players. If necessary, more complex survival analysis modeling including competing risks analysis and multivariate survival analysis, can be introduced.  Therefore, any field to which evolutionary game theory is applicable may benefit from the increased flexibility in modeling individual players.  Two particularly interesting fields are system biology and ecological modeling.  In the former field, dynamic hybrid fault models may find important applications in the study of biological networks \(such as gene, molecular, and cell networks 2008g conjecture that explains the redundancy in the universal genetic code with Byzantine general algorithm. In addition they conducted a comparative analysis of bio-robustness with engineering fault tolerance, for example, the strong similarity between network survivability and ecological stability \(Ma &amp; Krings 2008g survivability analysis can be applied for the study of survivals or extinctions of biological species under global climate changes \(Ma 2008b  In this paper, I have to ignore much of the details related to the implementation issues to present the overall architecture and major approaches clearly and concisely. To deal with the potential devils in the implementation details, a well funded research and development team is necessary to take advantages of the ideas presented here. On the positive side I do see the great potential to build an enterprise PHM software product if there is sufficient resource to complete the implementation. Given the enormous complexity associated with the PHM practice in modern engineering fields, it is nearly impossible to realize or even demonstrate the benefits of the architecture without the software implementation. The critical importance of PHM to mission critical engineering fields such as aerospace engineering, in turn, dictates the great value of such kind software product  6.3. Beyond PHM  Finally, I would like to raise two questions that may be interested in by researchers and engineers beyond PHM community. The first question is: what can PHM offer to other engineering disciplines? The second question is: what kinds of engineering fields benefit most from PHM? Here, I use the term PHM with the definition proposed by IEEE which is quoted in the introduction section of the paper  As to the first question, I suggest software engineering and survivability analysis are two fields where PHM can play significant roles. With software engineering, I refer to applying PHM principles and approaches for dealing with software reliability, quality assurance, and even software process management, rather than building PHM software mentioned in the previous subsection. For survivability analysis, borrowing the procedures and practices of PHM should be particularly helpful for expanding its role beyond its originating domain \(network systems that control critical national infrastructures is a strong advocate for the expansion of survivability analysis to PHM. Therefore, the interaction between PHM and survivability analysis should be bidirectional. Indeed, I see the close relationships between PHM, software engineering, and survivability as well-justified because they all share some critical issues including reliability survivability, security, and dependability  


 The answer to the second question is much more elusive and I cannot present a full answer without comparative analysis of several engineering fields where PHM has been actively practiced. Of course, it is obvious that fields which demand mission critical reliability and dependability also demand better PHM solutions. One additional observation I would like to make is that PHM seems to play more crucial roles for engineering practices that depend on the systematic records of 'historical' data, such as reliability data in airplane engine manufacturing, rather than on the information from ad hoc events.  This may explain the critical importance of PHM in aerospace engineering particularly in commercial airplane design and manufacturing.  For example, comparing the tasks to design and build a space shuttle vs. to design and manufacture commercial jumbo jets, PHM should be more critical in the latter task  17    Figure 2. States of a monitoring sensor node and its failure modes \(after Ma &amp; Krings 2008e     Figure 3. Core Modules and their Relationships of the Life System Inspired PHM Architecture    REFERENCES  Adamides, E. D., Y. A. Stamboulis, A. G. Varelis. 2004 Model-Based Assessment of Military Aircraft Engine Maintenance Systems Model-Based Assessment of Military Aircraft Engine Maintenance Systems. Journal of the Operational Research Society, Vol. 55, No. 9:957-967  Anderson, R. 2001. Security Engineering. Wiley  Anderson, R. 2008. Security Engineering. 2nd ed. Wiley  Bird, J. W., Hess, A. 2007.   Propulsion System Prognostics R&amp;D Through the Technical Cooperation Program Aerospace Conference, 2007 IEEE, 3-10 March 2007, 8pp  Bock, J. R., Brotherton, T., W., Gass, D. 2005. Ontogenetic reasoning system for autonomic logistics. Aerospace Conference, 2005 IEEE 5-12 March 2005.Digital Object Identifier 10.1109/AERO.2005.1559677  Brotherton, T., P. Grabill, D. Wroblewski, R. Friend, B Sotomayer, and J. Berry. 2002. A Testbed for Data Fusion for Engine Diagnostics and Prognostics. Proceedings of the 2002 IEEE Aerospace Conference  Brotherton, T.; Grabill, P.; Friend, R.; Sotomayer, B.; Berry J. 2003. A testbed for data fusion for helicopter diagnostics and prognostics. Aerospace Conference, 2003. Proceedings 2003 IEEE  Brown, E. R., N. N. McCollom, E-E. Moore, A. Hess. 2007 Prognostics and Health Management A Data-Driven Approach to Supporting the F-35 Lightning II. 2007 IEEE AeroSpace Conference  Byington, C.S.; Watson, M.J.; Bharadwaj, S.P. 2008 Automated Health Management for Gas Turbine Engine Accessory System Components. Aerospace Conference 2008 IEEE, DOI:10.1109/AERO.2008.4526610 


2008 IEEE, DOI:10.1109/AERO.2008.4526610 Environment Covariates &amp; Spatial Frailty Applications: AL; Life Cycle Mgmt; Real-Time Alerts CBM+, RCM, TLCSM; Secret Sharing and Shared Control 18 Chen, Y. Q., S. Cheng. 2005. Semi-parametric regression analysis of mean residual life with censored survival data Biometrika \(2005  29  Commenges, D. 1999. Multi-state models in Epidemiology Lifetime Data Analysis. 5:315-327  Cook, J. 2004. Contrasting Approaches to the Validation of Helicopter HUMS  A Military User  s Perspective Aerospace Conference, 2004 IEEE  Cook, J. 2007. Reducing Military Helicopter Maintenance Through Prognostics. Aerospace Conference, 2007 IEEE Digital Object Identifier 10.1109/AERO.2007.352830  Cox, D. R. 1972. Regression models and life tables.  J. R Stat. Soc. Ser. B. 34:184-220  Crowder, M. J.  2001. Classical Competing Risks. Chapman amp; Hall. 200pp  David, H. A. &amp; M. L. Moeschberger. 1978. The theory of competing risks. Macmillan Publishing, 103pp  Ellison, E., L. Linger, and M. Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013  Hanski, I. 1999. Metapopulation Ecology. Oxford University Press  Hallam, T. G. and S. A. Levin. 1986. Mathematical Ecology. Biomathematics. Volume 17. Springer. 457pp  Hess, A., Fila, L. 2002.  The Joint Strike Fighter \(JSF concept: Potential impact on aging aircraft problems Aerospace Conference Proceedings, 2002. IEEE. Digital Object Identifier: 10.1109/AERO.2002.1036144  Hess, A., Calvello, G., T. Dabney. 2004. PHM a Key Enabler for the JSF Autonomic Logistics Support Concept. Aerospace Conference Proceedings, 2004. IEEE  Hofbauer, J. and K. Sigmund. 1998. Evolutionary Games and Population Dynamics. Cambridge University Press 323pp  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Huzurbazar, A. V. 2006. Flow-graph model for multi-state time-to-event data. Wiley InterScience  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis. Springer. 481pp  Kacprzynski, G. J., Roemer, M. J., Hess, A. J. 2002. Health management system design: Development, simulation and cost/benefit optimization. IEEE Aerospace Conference Proceedings, 2002. DOI:10.1109/AERO.2002.1036148  Kalbfleisch, J. D., and R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data. Wiley-InterScience, 2nd ed  Kalgren, P. W., Byington, C. S.   Roemer, M. J.  2006 Defining PHM, A Lexical Evolution of Maintenance and Logistics. Systems Readiness Technology Conference 


Logistics. Systems Readiness Technology Conference IEEE. DOI: 10.1109/AUTEST.2006.283685  Keller, K.; Baldwin, A.; Ofsthun, S.; Swearingen, K.; Vian J.; Wilmering, T.; Williams, Z. 2007. Health Management Engineering Environment and Open Integration Platform Aerospace Conference, 2007 IEEE, Digital Object Identifier 10.1109/AERO.2007.352919  Keller, K.; Sheahan, J.; Roach, J.; Casey, L.; Davis, G Flynn, F.; Perkinson, J.; Prestero, M. 2008. Power Conversion Prognostic Controller Implementation for Aeronautical Motor Drives. Aerospace Conference, 2008 IEEE. DOI:10.1109/AERO.2008.4526630  Klein, J. P. and M. L. Moeschberger. 2003. Survival analysis techniques for censored and truncated data Springer  Kingsland, S. E. 1995. Modeling Nature: Episodes in the History of Population Ecology. 2nd ed., University of Chicago Press, 315pp  Kot, M. 2001. Elements of Mathematical Ecology Cambridge University Press. 453pp  Krings, A. W. and Z. S. Ma. 2006. Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks Military Communications Conference, 23-25 October, 7 pages, 2006  Lamport, L., R. Shostak and M. Pease. 1982. The Byzantine Generals Problem. ACM Transactions on Programming Languages and Systems, 4\(3  Lawless, J. F. 2003. Statistical models and methods for lifetime data. John Wiley &amp; Sons. 2nd ed  Line, J. K., Iyer, A. 2007. Electronic Prognostics Through Advanced Modeling Techniques. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352906  Lisnianski, A., Levitin, G. 2003. Multi-State System Reliability: Assessment, Optimization and Applications World Scientific  Liu, Y., and K. S. Trivedi. 2006. Survivability Quantification: The Analytical Modeling Approach, Int. J of Performability Engineering, Vol. 2, No 1, pp. 29-44  19 Luchinsky, D.G.; Osipov, V.V.; Smelyanskiy, V.N Timucin, D.A.; Uckun, S. 2008. Model Based IVHM System for the Solid Rocket Booster. Aerospace Conference, 2008 IEEE.DOI:10.1109/AERO.2008.4526644  Lynch, N. 1997. Distributed Algorithms. Morgan Kaufmann Press  Ma, Z. S. 1997. Demography and survival analysis of Russian wheat aphid. Ph.D. dissertation, Univ. of Idaho 306pp  Ma, Z. S. 2008a. New Approaches to Reliability and Survivability with Survival Analysis, Dynamic Hybrid Fault Models, and Evolutionary  Game Theory. Ph.D. dissertation Univ. of Idaho. 177pp  Ma, Z. S. 2008b. Survivability Analysis of Biological Species under Global Climate Changes: A New Distributed and Agent-based Simulation Architecture with Survival Analysis and Evolutionary Game Theory. The Sixth 


International Conference on Ecological Informatics. Dec 25, 2008. Cancun, Mexico  Ma, Z. S. and E. J. Bechinski. 2008. A Survival-Analysis based  Simulation Model for Russian Wheat Aphid Population Dynamics. Ecological Modeling, 216\(2 332  Ma, Z. S. and A. W. Krings. 2008a.  Survival Analysis Approach to Reliability Analysis and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT, 20pp  Ma, Z. S. and A. W. Krings. 2008b. Competing Risks Analysis of Reliability, Survivability, and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008.  Big Sky, MT. 20pp  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(I Dependence Modeling", Proc. IEEE  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT. 21pp  Ma, Z. S. and A. W. Krings., R. E. Hiromoto. 2008d Multivariate Survival Analysis \(II State Models in Biomedicine and Engineering Reliability IEEE International Conference of Biomedical Engineering and Informatics, BMEI 2008.  6 Pages  Ma, Z. S. and A. W. Krings. 2008e. Dynamic Hybrid Fault Models and their Applications to Wireless Sensor Networks WSNs Modeling, Analysis and Simulation of Wireless and Mobile Systems. \(ACM MSWiM 2008 Vancouver, Canada  Ma, Z. S. &amp; A. W. Krings. 2008f. Dynamic Populations in Genetic Algorithms. SIGAPP, the 23rd Annual ACM Symposium on Applied Computing, Ceara, Brazil, March 16-20, 2008. 5 Pages  Ma, Z. S. &amp; A. W. Krings. 2008g. Bio-Robustness and Fault Tolerance: A New Perspective on Reliable, Survivable and Evolvable Network Systems, Proc. IEEE  AIAA AeroSpace Conference, March 1-8, Big Sky, MT, 2008. 20 Pages  Ma, Z. S.  and A. W. Krings. 2009. Insect Sensory Systems Inspired Computing and Communications.  Ad Hoc Networks 7\(4  MacConnell, J.H. 2008. Structural Health Management and Structural Design: An Unbridgeable Gap? 2008 IEEE Aerospace Conference, DOI:10.1109/AERO.2008.4526613  MacConnell, J.H. 2007. ISHM &amp; Design: A review of the benefits of the ideal ISHM system. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352834  Marshall A. W., I. Olkin. 1967. A Multivariate Exponential Distribution. Journal of the American Statistical Association, 62\(317 Mar., 1967  Martinussen, T. and T. H. Scheike. 2006. Dynamic Regression Models for Survival Data. Springer. 466pp  Mazzuchi, T. A., R. Soyer., and R. V. Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Millar, R.C., Mazzuchi, T.A. &amp; Sarkani, S., 2007. A Survey of Advanced Methods for Analysis and Modeling of 


of Advanced Methods for Analysis and Modeling of Propulsion System", GT2007-27218, ASME Turbo Expo 2007, May 14-17, Montreal, Canada  Millar, Richard C., "Non-parametric Analysis of a Complex Propulsion System Data Base", Ph.D. Dissertation, George Washington University, June 2007  Millar, R. C. 2007. A Systems Engineering Approach to PHM for Military Aircraft Propulsion Systems. Aerospace Conference, 2007 IEEE. DOI:10.1109/AERO.2007.352840  Millar, R. C. 2008.  The Role of Reliability Data Bases in Deploying CBM+, RCM and PHM with TLCSM Aerospace Conference, 2008 IEEE, 1-8 March 2008. Digital Object Identifier: 10.1109/AERO.2008.4526633  Nowak, M. 2006. Evolutionary Dynamics: Exploring the Equations of Life. Harvard University Press. 363pp  Oakes, D. &amp; Dasu, T. 1990. A note on residual life Biometrika 77, 409  10  Pintilie, M. 2006. Competing Risks: A Practical Perspective.  Wiley. 224pp  20 Smith, M. J., C. S. Byington. 2006. Layered Classification for Improved Diagnostic Isolation in Drivetrain Components. 2006 IEEE AeroSpace Conference  Therneau, T. and P. Grambsch. 2000. Modeling Survival Data: Extending the Cox Model. Springer  Vincent, T. L. and J. L. Brown. 2005. Evolutionary Game Theory, Natural Selection and Darwinian Dynamics Cambridge University Press. 382pp  Wang. J., T. Yu, W. Wang. 2008. Research on Prognostic Health Management \(PHM on Flight Data. 2008 Int. Conf. on Condition Monitoring and Diagnosis, Beijing, China, April 21-24, 2008. 5pp  Zhang, S., R. Kang, X. He, and M. G. Pecht. 2008. China  s Efforts in Prognostics and Health Management. IEEE Trans. on Components and Packaging Technologies 31\(2             BIOGRAPHY  Zhanshan \(Sam scientist and earned the terminal degrees in both fields in 1997 and 2008, respectively. He has published more than 60 peer-refereed journal and conference papers, among which approximately 40 are journal papers and more than a third are in computer science.  Prior to his recent return to academia, he worked as senior network/software engineers in semiconductor and software industry. His current research interests include: reliability, dependability and fault tolerance of distributed and software systems behavioral and cognitive ecology inspired pervasive and 


behavioral and cognitive ecology inspired pervasive and resilient computing; evolutionary &amp; rendezvous search games; evolutionary computation &amp; machine learning bioinformatics &amp; ecoinformatics                 pre></body></html 


