Enterprise Interaction Ontology for Change Impact Analysis of Complex Systems  Aman Kumar  Preethi Raghavan  Jay Ramanathan  Rajiv Ramnath   Ohio State University/ Computer Science and Engineering, Columbus, Ohio  kumaram@cse.ohio-state.edu, raghavap@cse.ohi o-state.edu, jayram@cse.ohio-state.edu ramnath@cse.ohio-state.edu    Abstract  Reasoning about the impact of change is critical throughout the Information Technology \(IT\ architecture lifecycle management processes and this is especially challenging because installed architectures are complex evolve constantly, and most changes have some global impact.  We present an enterprise-interaction ontology for integrated query, analysis, and monitoring that supports features to allow architects and engineers pinpoint the impact of change to the installed architecture before implementation.  The ontology represents select associations between the enterprise’s business processes services and infrastructure so that significant consequences of a change are propagated to affected areas based on underlying rules.  Thus interdependencies and relationships that are not obvious are identified and the impact is quantified.  This allows the architect to know the complete scope of modifications required in order to accomplish a change in a manner consistent with best practices \(like ITIL version 3\.   We illustrate - 1\ules and taxonomy relationships that give us the ability to pro pagate changes and determine the impact, and 2\ how actual questions and decisionmaking during the architecture management processes can be better supported using a more precise and factual understanding.  Not only does the interaction methodology help analyze the potential impact of adding a new component, a change due to an incident, or the deletion of an existing component from the architecture it also supports business-IT alignment processes like chargeback, capacity management and disaster recovery  1  Introduction  1.1  Business Problem  The business problem is to support the Architecture Life-Cycle Management Processes.  The related processes are typically kicked off in response to a new initiative, a new project, or a request for change.  One objective during these processes is to ensure that the plans and implementations do not degrade the performance of ongoing operations and the non-functional attributes of the existing systems.  Through our investigations we concluded that the state of practice is such that important insights from the installed enterprise architecture are not available to inform its evolution.  The feedback from deployment is difficult to obtain and validate against the initial design to see how well both are aligned.  To achieve this continuous improvement, the paper proposes both an ontology and ontology-based methodology to analyze the impact of introducing a change in the existing enterprise architecture.  The methodology involves creating a logical enterprise model \(taxonomical representation\ the entities – business goals, processes, services, and infrastructure components – within the enterprise and the associations between them.  In addition, there are intrinsic and extrinsic non-functional attributes associated with the entities, describing their service and operating levels Using this representation, we can analyze the impact along associations and derive specific values for decision making.  The impact is in terms of changed request throughput and availability  1.2  Business Scenarios  By examining Architecture Lifecycle Management practices within industry we have identified important use cases that pose a challenge.  Typically they were related to shared services and thus the ones selected for discussion here include Scenario 1: Retirement of a software service and determining the increased availability of shared infrastructure capacity Scenario 2: Consolidation of a service and the decreased availability of shared infrastructure capacity Scenario 3: A surge in requests and consequent increase in demand for services due to an unforeseen event \(like Hurricane Katrina Scenario 4: Support of new business goals through a new process transaction and associations to existing shared resources within the existing enterprise taxonomy We found that while many questions are asked during the architecture review process, the macro-level cases of the type identified above proved to the most challenging because they required insights not only of the installed architecture, but also collaboration across the IT organizations.  Further, due to space limitations, we are not selecting the micro-level use cases, such as response time, as a performance measure here since typically they can be addressed by increasing capacity or by using the re-architecting methods at the macro-level 
2008 IEEE Asia-Pacific Services Computing Conference 978-0-7695-3473-2/08 $25.00 © 2008 IEEE DOI 10.1109/APSCC.2008.200 303 
2008 IEEE Asia-Pacific Services Computing Conference 978-0-7695-3473-2/08 $25.00 © 2008 IEEE DOI 10.1109/APSCC.2008.200 303 


2  Background in Existing Research and Best Practices  The enterprise ontology is the basis of the enterprise taxonomy or model that represents select interaction relationships between entities such as business goals processes, software component services, and infrastructure components \(such as hardware and networks\; thus providing a logical view of the architecture.  This is envisioned as a layer on the enterprise’s configuration management data base of items \(Figure 1\.  Our approach demonstrates that non-functional attributes like space processing speed, and availability can be propagated and calculated for component infrastructures and attributes such as throughput can be calculated for functional services based on the interactions.  Propagation rules for reasoning about capacity and availability of infrastructures and services in terms of these functional and non functional attributes facilitates intelligent decision-making during the Architecture Life-Cycle Management process Before proceeding, we briefly review existing research and best practice literature   Figure 1:  Architecture Life Cycle Management using a Logical Enterprise Taxonomy 2.1  ITIL version 3  ITIL’s [1 nfi gur at i o n M a na ge m e nt D a t a  B a s e  CMDB\ focuses on the configuration items \(CIs\ the installed IT architecture and associations between them In addition to this, most practitioners agree that a CMDB should have four key capabilities – visualization federation, reconciliation and active monitoring.  First you need to be able to visualize individual infrastructure components such as an application running on a server networking or storage devices.  A second capability is federation, which is the ability to integrate data from tools outside of a single vendor's software suite.  Most IT managers are leaning toward suite-based approaches where they might buy their problem incident, application management and CMDB from the same vendor.  But the suite must be able to accept data from other tools.  A third CMDB feature is the ability to reconcile data.  Different applications might monitor, store and report assets in different ways.  We need to consolidate this information from different application data sources and correlate it with existing knowledge present in the CMDB.  Next, it needs to provide active monitoring capabilities.  As the state of CIs or assets change in our infrastructure, CMDB should be able to reflect that state instantaneously.  Also the relationships between the affected CI/assets with other items in the infrastructure should be revalidated and updated. This is critical as an outdated representation of IT infrastructure is no better than an incorrect representation Our focus here is to build upon the CMDB and support decision-making at a macro level.  In general the data managed in the CMDB is at a very detailed level [3, 4    We introduce a way to extend the capabilities of the CMDB effectively using an enterprise-specific taxonomy An underlying ontology represents complex logical relationships are not maintained in the traditional CMDB This allows the application of rules that determine the impact of change between CIs/assets, services, business processes and business goals  2.2  Goal-to-Architecture Modeling  Several goal modeling and specification techniques such as KAOS [9 o alB a s ed R e qu i r em en ts A n al y s is  Method  i   11  a n d t h e  N F R Fr a m e w o r k [1 2 ha ve  been proposed in support of requirements engineering and related activities such as elaboration, consistency and completeness checking, evaluation of alternatives, and evolution.  The NFR Framework initially focused on system requirements, but has been extended [1 t o  connect enterprise architecture goals to system architecture goals, and can potentially be extended to connect enterprise strategy goals.  The ATAM method [2  is an accepted best-practice for analyzing architectures to validate how specific quality goals are met by the architectural elements of the system Of special relevance are [15 w h ich pr o pos es an  integrated top-down and bottom-up approach to identifying misaligned goals and planning organizational change, [14 w h ich des c ri be s a m e th od f o r as s o ci atin g  goals with justifications, and [1 h i c h use s go a l m o d e l s  to guide the adaptation of pervasive systems to changes in the environment This considerable body of work, however, needs to be extended for use in the dynamic, sense-and-respond environment in which service-oriented organizations exist This implies the linking of goals to and from the dynamic behavior of the organization, including the linking of ongoing business processes, their execution by resources and the performance of the business transactions.  This is the gap that we address here  2.3  Architecture Change Management  There also has been previous work in the area of architecture change impact analysis. Feng and Maletic [5  
304 
304 


propose a component interaction trace-based approach to support dynamic change impact analysis at software architecture level.  Given an architectural change, their approach determines the architecture elements causing the change and impacted by the change.  But, they do not look at the impact on services, business processes and business goals; neither do they analyze the impact of a change introduced at these levels.  Also, [8 d d r es s e s ch an g e  impact analysis of enterprise architectures from a semantic perspective by using semantic classification of relationships present in the modeling language ArchiMate and applying different kinds of atomic changes that could affect concepts in an enterprise.  They also look into the    Figure 2:  Enterprise interaction ontology identifying relationships between entities  Problem of change impact analysis in order to identify potential impacts of a change before it actually takes place by defining certain impact rules for analysis on the level of the conceptual structure of an enterprise  Our approach to change impact analysis spans across multiple levels, from business goals and services to software services and infrastructure components Moreover, our methodology defines propagation rules using certain attributes at a fine-grained level  2.4  Contributions towards an Interaction-based Enterprise Modeling Environment  In this paper we propose a logical, active monitoring layer to manage the changes to the enterprise architecture and better align Business and IT \(Figure 1\.  IT operations management practitioners have been proponents of relating business predictions of needed business capacity to underlying service and infrastructure capacity However, implementation techniques to achieve this are not developed [3, 4  W e  a d dr es s  t h e i s s u es by  introducing a logical layer on the CMDB to reason about dynamically changing relationships between business goals and demands, services demand and infrastructure use.  We uniquely propose a modeling environment that allows us to reason about each entity in the enterprise throughout its life-cycle from inception-to-change-toretirement The solution proposed has the following components 1\ A logical taxonomy layer on a CMDB, 2\ An Enterprise Interaction Ontology with propagation rules and analysis methods, and 3\ Integration of monitored performance for impact analysis and continuous improvement of overall performance of the complex system by considering the impact of the individual entities and their life cycles  3  Enterprise Modeling  Figure 2 illustrates the four types of entities that we assume in any enterprise system – business goals, process transactions, service components, and infrastructure components.  Progressing from the bottom up, their roles in the enterprise are as follows 1  Infrastructure components are primarily the physical machines, routers, etc. These components also include essential operating software that ‘runs on’ the hardware 2  Service components ‘run on’ the infrastructure Their space and processing speed needs are thus provisioned by the infrastructure. Service components also ‘provide’ services to other service components 3  Process transactions are abstractions that represent the end-to-end business processes and subprocesses ‘executed’ by service components or sub-transactions to fulfill business-requests.  The process transactions and sub-transactions reflect the external request-driven use of the service components 4  Business goals reflect the interests of stakeholders There can be many goals and stakeholders Process transactions execute and in turn ‘deliver on business goals  3.1  Entities, Attributes and Service Levels  Each entity identified above also has specific attributes as discussed next 1  Infrastructure component \(or just infrastructure attributes include a  Let S-S be defined as the collection of space and processing speed attributes.  This collection includes derived attributes such as current available capacity, availability, cost, monitored actual projected future etc.  We shall often refer to these as the S-S non-functional attributes.  An infrastructure component’s processing speed 
305 
305 


determines the request throughput of service components that run on it 2  Service component \(or just service\ attributes include a  For a service component, these S-S attributes are also of two types - intrinsic and extrinsic S-S attributes.  The intrinsic S-S attributes are determined by the characteristics of the software component.  For example, like those determined by running a database service component in a test infrastructure machine environment, on benchmarks and in isolation Extrinsic attributes are those S-S attributes that are affected by business requests from other entities along associations that we will explore later.  We will henceforth use the notation  S-S x to reflect the intrinsic and extrinsic S-S values of a component -X b  RR or Input Request Rate:  Number of input requests queued per time period c  SR or  Serviced Request Rate:  Number of requests serviced per time period What do S-S attributes convey differently for infrastructure components and service components To answer this let us first think of service component X that runs on an infrastructure component or machine.  In its essence the hardware has a certain available space and is able to process at a certain speed.  However, the machine can accommodate a component such as a database only if the machine’s available S-S is greater than the database’s own needs or \(S-S database The needed database S-S also increases with the number of requests serviced from different process transactions.  To accommodate this increase the hardware must in turn have the extra needed S-S available.  That is, more generally the software has a certain fluctuating \(S-S\ footprint due to extrinsic circumstances that it needs from the hardware.    The process transaction entity and its attributes introduced next allow us to compute the needed extrinsic S-S values due to external requests 3  Process transaction attributes include a  The number of users \(#Users b  RR and SR.  Here RRs are business requests distinguished from the request throughput SR by the fact that the former is initiated and queued by the customer while the latter is the actual number processed \(and could include many internally generated requests serviced by the lower level entities 4  Business goal attributes are RR and SR costs Note:  In addition to the essential attributes identified above there are additional attributes that reflect target as well as actual values for costs, resource costs of S-S for processing a request, and so on We can now also introduce specific Service Levels \(SL and Operating Levels \(OL\.  The SL of a process transaction is reflected by its request throughput - SR.  As we shall see, this is provisioned by the service components and their S-S attributes at certain Operating Levels \(OLs\chieve process transaction and business SR targets  3.2  Associations and Types of Rules  Next we elaborate on the general types of rules related to the associations of the ontology in Figure 2.  These allow us to propagate information along the enterprise model and relate business, functional and non-functional attributes of entities.  Thus these help answer questions and address scenarios that arise during architecture life cycle management.  Each of these types of rules is introduced next \(more specific rules with the business scenarios are presented later Rules for the ‘runs on’ association:  Each service component X reduces the available S-S of the infrastructure component that it runs on Rules for ‘provides’ association:  These adjust the S-S footprint of a service component due to provisioning other service components and handling requests for execution from process transactions.  The S-S values of each service component X are determined by F \(\(S-S x RRs from all other Y components provisioned by X\, where F is a function that adjusts the intrinsic values based on the extrinsic values to get the total footprint.  In our illustrative examples we will assume F to be a function simply dependent on the incoming requests Rules for ‘executing’ process transactions with component services:  Usually many service requests are executed by each process transaction upon the arrival of a customer request.  The process transaction \(which in most cases may itself be implemented as business processes within component services\ is an abstraction that keeps track of customer-generated RRs separate from the responding internally-generated service RRs and SRs The services are provided by the services components that run on infrastructures and their S-S attributes are adjusted as in the above rule type Rules for synthesizing process transaction’s ‘execution costs:  Each process transaction has an associated number of users and RRs that use the services and infrastructures of the underlying components.  The costs of these components are synthesized for each process transaction instance Rules for process transactions and delivery of business goals:  The ‘delivers’ association between the process transaction and the business goal reflects the requests executed.  The synthesized cost of the services for each process transaction can be distributed over the number of business requests allowing us to compute the cost for servicing a request.  This cost can be compared to the value delivered towards meeting the business goal  3.3  Enterprise Taxonomy and Propagation  The ontology introduced in Figure 2 is used to represent an enterprise-specific taxonomy based on the shared Proteomics facility for cancer research.  The facility provides various ways to analyze actual experimental patient data.  As illustrated, the business goal of establishing patient benefits through experimentation is addressed by layers of process transactions, service components, and infrastructure components respectively We next identify the how ontology guides the enterprise specific representation 
306 
306 


First, identify business goals and stakeholders and the process transactions that meet the goals.  Next for the process transactions \(data analysis 2\reflect any subtransactions \(e.g. validation\ine the attributes such as numbers of users and RR.  And next represent service e.g. Protein Analysis Tool 1.2\ and infrastructure \(e.g File system\mponents with their attributes, such as S-S A given service might in turn provide services to multiple other services, identify these.  For example, note that the File system is shared in the implementation of Oracle and this is captured by the ‘provides’ association.  Only shared infrastructure components are represented in order to keep the models from getting complex with details that are not relevant.  Thus all the associations between entities are identified based on the ontology introduced above Finally, note that if we model our relationships carefully we shift from a functional to non-functional realm as we move from the top layers to the lower ones.  That is, the RRs for a process transaction are related to the provisioning components’ non-functional attributes For each entity in the resulting enterprise model we can now make the following observations 1  The propagation rules introduced previously determine each entity’s inherited and synthesized values from the associated entities and their attributes The algorithms for propagation are available in literature [17     2  Attributes are recomputed based on instrumentation and monitoring of actual performance of the underlying components, as captured in the CMDB From a continuous improvement perspective, it is expected the S-S attributes are computed and updated frequently.  This can be done with a variety of monitoring software typical in any enterprise.  This monitoring is particularly important as execution-time information allows us to better identify the extrinsic S-S values and interference between components We next illustrate the result of applying propagation rules to specific scenarios and related decision-making  4  Enterprise Modeling for Decision Making  4.1  Scenario 1 - Removing of a service component  Removal of a component from the architecture changes the S-S availability attribute values of all services and infrastructures that are associated by ‘provides’ and ‘runs on’.   Consider the case where “Protein Analysis Tool” is removed from the infrastructure 1  A decreased RR from the Data Analysis process transaction to the Protein Analysis, and consequently a decreased RR passed through to underlying infrastructure \(machine 6 and router 2  Increased availability of infrastructure - machine 6 This is determined by \(S-S machine  6 S-S machine  6 F\(\(S-S mxml S-S oracle SRs\ due to data analysis 1 and storage 
307 
307 


3  The increased S-S availability attribute values are synthesized and propagated to the process transactions and the business goals where it is possible to reason about these attributes in terms of number of users’ and the number of ‘incoming requests over a period of time’.  This in turn enables us to determine the accurate cost of removing a component from the architecture and verify if it meets the desired business goal  4.2  Scenario 2 - Consolidation of services  Change in process transactions may require service consolidation that involves combining multiple services into a consolidated operational capability Consider the case when we want to delegate the functionalities provided by LDAP to Oracle 12g. This causes the RR of Oracle 12g to increase that would in turn increase the RR of Machine 6. Thus, the new value of required capacity S-S could exceed the total capacity. This estimation would enable us to take an informed decision about whether to merge software services.  To calculate the target availability of the infrastructure component Machine 6’, taking into consideration the increase in the number of requests we use the formula S-S machine 6 F\(\(S-S mxml S-S mxml S-S Oracle12g  SRs During this analysis, we may discover that the desired target availability exceeds the current availability in terms of S-S attributes of Machine 6 thus making it difficult to maintain the desired SL of the new Data Analysis process transaction and OL levels of Machine 6.  This scenario also illustrates another potential benefit of visualizing apriori, the points of failure as a result of the consolidation For example, a failure in certain critical components Machine 6 in this case\ill now have wider impact on process transaction service levels  4.3  Scenario 3 - A surge of requests  In this scenario, we consider the case when there is an unexpected surge in the number of RRs and we want to see what potential target SR values can be achieved Specifically, consider when the Medical Service starts getting very high number of input requests due to a new regulation or lawsuit which requires certain data to be processed in a limited amount of time.  So based on its new RR we can calculate RR values for Data Analysis 1 Data Analysis 2, Storage and then for Mxml Analysis Tool, Protein Analysis Tool and Oracle.  This can be done in many dynamic ways – for example by relating the RR at the parent with RR traffic generated at the child applications For, the infrastructure components e.g. Machine 6, we can also calculate available S-S by using the needed footprint - F\(S-S\ value - of its parent components \(Oracle and Protein Analysis Tool\The new target SR value for Oracle can be easily calculated based on 1\ its own pristine|intrinsic S-S value, 2\ the new target values calculated for its child entities, and 3\ the increment of SS needed due to the extrinsic effect of SRs.  Now, target SR values can be propagated up, to calculate the new estimated SR targets for the Medical Service D. Scenario 4:  Adding a new process transaction and its sub network  The final scenario is how an enterprise model changes with the addition of new process transactions.  Referring to the figure 3, before adding new Automated Medical Service’, that provides workflow support for more effective data management and pipelining, we can calculate its effect on the overall capacity of the existing infrastructure by using the previously mentioned attributes and propagation rules.  As we would typically migrate some of the users from old medical service to the new automated service would have new decreased SR for old service and a newly estimated SR for new service.  Based on these values and the new associations \(dashed lines\ shown in Figure 3, we can recalculate the SR values of the services and infrastructure components being used by these two impacted services As described in Scenario 1, these values can now be calculated till the lowest level entity and the propagated up back to calculate the new throughput SR.  We can thus reason about whether the new automated services would improve the overall throughput of the system and more importantly if the existing infrastructure can support the new system components.  Also, we can put in new increased/decreased S-S attribute values for components to judge its effect on the overall performance and can, in this way find and eliminate bottlenecks Before adding new Automated Medical Service, that provided more effective data management and pipelining support, we can calculate its effect on the overall capacity of the existing infrastructure by using the above mentioned attributes and propagation rules.  As we might choose to migrate some users from the old medical service to the new automated service, we would obtain decreased RR value for old service and a newly estimated RR for new service. Based on these values and the new associations as shown in Figure 3, we can recalculate the RR values of the services and components being used by these two impacted services.  As described in Scenario 1 these values can now be calculated till the lowest level entity and propagated up to calculate the new throughput RT\or the services.  We can thus reason if the new automated services would improve the overall throughput of the system and more importantly if the existing infrastructure can support the new system components Also, we can put in new increased/decreased \(S-S attribute values for components to estimate its effect on the overall performance and can in this way find and eliminate bottlenecks  5  Conclusions  We have shown how a logical view of the enterprise entities and associations provides a basis for propagation rules that allow us to determine and more precisely quantify the impact of a change.  In particular, because the impact is across entities, the typical organization silos make this difficult to determine.  The enterprise interaction ontology and methodology provides a precise communication scheme for more effective collaboration during architecture evolution.  The propagation rules were applied to four scenarios typical during the 
308 
308 


architecture life-cycle management process.  The precise enterprise interaction ontology and methodology presented in this paper allows us to a  Establish a precise vocabulary that associates the business, functional and non-functional entities and attributes of an enterprise with rules of propagation and communications b  Facilitates a precise framework for implementation of a logical architecture management layer on a CMDB to manage the life-cycle of individual components by more precisely determining the impact of change c  Leverages monitoring of individual components and propagation rules to support more proactive strategies at the macro-level The S-S attributes of this framework also have other wide-ranging applications like determining the chargeback for services and even the use of power.  The important element addressed here is to ensure that the cost of resources delivering on a customer initiated request is evaluated accurately.  Our ongoing next steps in the project are to define and implement the feedback policies for better visualization and decision making  6  Acknowledgements  This research has been conducted within the National Science Foundation funded Center for Experimental Research in Computer Science at Georgia Tech and The Ohio State University.  We would like to thank industry sponsors and affiliates especially those at Nationwide and The OSU Medical Center.  Some of the early work was funded by IBM Faculty Grant  7  References  1  ITIL version 3, http://www.itilofficialsite.com/home/home.asp 2  Rick Kazman, Mark Klein, Paul Clements  ATAM:SM Method for Architecture Evaluation www.sei.cmu.edu/pub/documents/00.reports/pdf/00tr004.p df 3  Microsoft Operations Framework  Capacity Management Service Management Function, Microsoft Corp, January 2005 4  Ron J. Colville, Gartner RAS Core Research Note G00137125 Gartner on CMDB March 13, 2006 5  Tie Feng, Jonathan I. Maletic.,  “Applying Dynamic Change Impact Analysis in Component-based Architecture Design”, Proceedings of the Seventh ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing  SNPD’06\006 6  Shy Cohen, “Ontology and Taxonomy of Services in a Service Oriented Architecture Microsoft Architect Journal  7  BCS, British Computer Society Configuration Management Specialist Group ,  www.bcs-cmsg.org.uk 8  de Boer, F.S.   Bonsangue, M.M.   Groenewegen, L.P.J Stam, A.W.   Stevens, S.   van der Torre, L. , “Change impact analysis of enterprise architectures” ,  CWI Information Reuse and Integration, IEEE International Conference, Amsterdam, Netherlands, Conf, 2005 9  Dardenne, A., van Lamsweerde, A., Fickas, S., "Goaldirected requirements acquisition Science of Computer Programming 1993   Anton, A.I., "Goal-based requirements analysis Proceedings of the 2nd International Conference on Requirements Engineering \(ICRE ’96\, 1996   Yu, E.S.K, Mylopoulos, J. “An actor dependency model of organizational work: with application to business process reengineering”, Proceedings of the Conference on Organizational Computing systems, December 1993   Chung, L., Nixon, B. A., Yu, E. and Mylopoulos, J. “NonFunctional Requirements in Software Engineering Kluwer Academic Publishers, Boston, 2000   Subramanian, N., Chung, L., Yeong-tae Song , “An NFRBased Framework for Establishing Traceability between Enterprise Architectures and System Architectures Seventh ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, 2006   Jureta, I.J., Faulkner, S., Schobbens, P., "Justifying Goal Models," 14th IEEE International Requirements Engineering Conference \(RE'06\, 2006   Kavakli, E., Loucopoulos, P., "Experiences With GoalOriented Modeling of Organizational Change", IEEE Transactions On Systems, Man, And Cybernetics Part C Applications And Reviews, Vol. 36, No. 2, March 2006   Mei, L., Easterbrook, S., "Evaluating User-centric Adaptation with Goal Models, First International Workshop on Software Engineering for Pervasive Computing Applications, Systems, and Environments May 2007   Fundamentals of Computer Algorithms by Ellis; Sahni Horowitz,  Publisher: W. H. Freeman & Company, 1978  
309 
309 


to perfect their approaches Our aim is to ensure further continuity through holding a series of future competitions using gradual re\002nements of the rules and software used in the current competition The following improvement will be made to the software in time for the CIG competition 017 The installation process will be streamlined 017 Reliability will be improved 017 Support for multi-car and multi-track training will be added making it easier to apply co-evolution and incremental evolution 017 More sample controllers and trainers e.g temporal difference learning trainers will be supplied An amusing illustration of the need to improve reliability is that in an early version of the software it was possible to achieve the 002tness value of driving a whole lap simply by slowly driving up to and passing the start line the car starts 100 meters before that line then turning and passing the line again This 003aw is inherent in TORCS presumably because its developers never thought of anyone doing something so bizarre Evolutionary algorithms however are good at coming up with bizarre solutions and Matt Simmerson's algorithm quickly evolved a controller that exploited this bug A patch for this bug is now part of the software package A reviewer of the paper summarizing the previous car racing competitions pointed out that in its current form the competition is not only about learning algorithms It is certainly possible to hand-code a non-learning controller that outperforms the best CI-based controllers Indeed the best controllers that come with the TORCS game developed by the TORCS developers are non-learning and by far outperform all the controllers submitted to this competition so far though they often access information state information that is not directly available through the competition API Of course we hope that future editions of this competition will see CI-based contributions that perform better than the best hand-coded ones and there are no reasons why this should not happen Still it would be interesting to run a version of the competition that compared only the quality of the learning algorithm One way could be to de\002ne a standard e.g neural network-based controller architecture and then provide an interface for a learning algorithm to set the parameters for this controller optimally given a certain numbers of laps around an unknown track The participants would then submit an algorithm rather than a controller to be run and evaluated by the organizers of the competition Another interesting version of the competition would be one where the controllers where presented with a richer but more primitive state description in particular visual data This could come in the form of the full rendered 3D view through the controlled car's windscreen or a part of it Such a state description would ultimately give the controllers more information and thus allow for better driving but would also require more complex controllers Given the various interesting variations on the car racing concept that are possible our plan is to organize editions of the car racing competition in conjunction with several international conferences and at each conference hold both the competition in its original form and some variation on the concept like the ones suggested above VI C ONCLUSION We have described the organization rules and software of the car racing competition in the form it was organized in conjunction with IEEE WCCI 2008 Four out of 002ve participating teams described the architecture and training of their controllers We have also reported the scoring procedure and results of the competition and plans for future competitions We hope that this paper in addition to serving as a record of the competition will provide organizers of similar competitions with inspiration and insights and that the descriptions of the controllers will be useful for researchers working on learning vehicle control in general and for participants in future car racing competitions in particular R EFERENCES  J T ogelius S M Lucas H Duc Thang J M Garibaldi T Nakashima C H Tan I Elhanany S Berant P Hingston R M MacCallum T Haferlach A Gowrisankar and P Burrow 223The 2007 ieee cec simulated car racing competition,\224 Genetic Programming and Evolvable Machines  2008 A v ailable http://dx.doi.org/10.1007/s10710-008-9063-0  223The open raci ng car simulator  224 Online A v ailable http://torcs.sourceforge.net  223Softw are manual of the car racing competition 224 WCCI-2008 A v ailable http://cig.dei.polimi.it/wpcontent/uploads/2008/04/manual  v03.pdf  R G Re ynolds and M Z Ali 223Computing with the social f abric The evolution of social intelligence within a cultural framework,\224 IEEE Computational Intelligence Magazine  vol 3 no 1 pp 18\22630 2008  R G Re ynolds M Z Ali and T  Jayyouzi 223Mining the social f abric of archaic urban centers with cultural algorithms,\224 Computer  vol 41 no 1 pp 64\22672 2008  K O Stanle y  223Ef 002cient e v olution of neural netw orks through complexi\002cation,\224 Ph.D dissertation Department of Computer Sciences University of Texas Austin TX 2004  M Simmerson 223Neat4j homepage 224 2006 Online A v ailable http://neat4j.sourceforge.net  S Baluja and R Caruana 223Remo ving the genetics from the standard genetic algorithm,\224 in Proceedings of the international conference on machine learning ICML  1995  R Sukthankar  S Baluja and J Hancock 223Proto yping intelligent vehicle modules,\224 in Proceedings of the International Conference on Robotics and Automation ICRA  1997  J T ogelius and S M Lucas 223Ev olving controllers for simulated car racing,\224 in Proceedings of the Congress on Evolutionary Computation  2005  227\227 223Ev olving rob ust and specialized car racing skill s 224 in Proceedings of the IEEE Congress on Evolutionary Computation  2006  J Bernard J Gruening and K Hof fmeister  223Ev aluat ion of v ehicle/driver performance using genetic algorithms,\224 Society of Automotive Engineers  1998  D Floreano T  Kato D Marocco and E Sauser  223Coe v olution of active vision and feature selection,\224 Biological Cybernetics  vol 90 pp 218\226228 2004  J T ogelius and S M Lucas 223 Arms races and car races 224 in Proceedings of Parallel Problem Solving from Nature  Springer 2006  223The car racing competition homepage 224 WCCI-2008 Online Available http://cig.dei.polimi.it/?page  id=5 


Table 3 Compressed sizes and number of extracted itemsets f or the itemset selection algorithms Candidate Itemsets S ET P ACK S ET P ACK G REEDY K RIMP Dataset min-sup  sets c  T  c  T  c  T b    sets c  T  c  T  c  T b    sets  bits  sets anneal 175 8837 20777 89.9 103 20781 89.9 69 31196 53 breast 1 9920 5175 63.7 42 5172 63.9 49 4613 30 courses 55 5030 64835 84.9 268 64937 85.1 262 73287 93 mammals 700 7169 65091 83.4 427 65622 84.1 382 124737 125 mushroom 1000 123277 313428 70.9 636 262942 59.5 1225 474240 140 nursery 50 25777 314081 93.0 276 314295 93.1 218 265064 225 pageblocks 1 63599 11961 78.3 92 11967 78.3 95 10911 53 tic–tac–toe 7 34019 23118 92.0 620 23616 94.0 277 28957 159 large candidate family for mushroom  For comparison we use the same candidates for K RIMP  We also compare to S ET P ACK G REEDY  which required 1–12 minutes 7 minutes typically with an exception of 2 1 2 hours for mushroom  Comparing the results of this experiment Table 3 with the results of G REEDY P ACK in the previous experiment we see that the selection process is more strict now even fewer itemsets are regarded as interesting enough Large candidate collections are strongly reduced in number up to three orders of magnitude On the other hand the compression ratios are still very good The reason that G REEDY P ACK produces smaller compression ratios is because it is allowe d to consider any itemset Further the fact alone that even with this very strict selection the compression ratios are generally well below 90 show that these few sets are indeed of high importance to describing the major interactions in the data If we compare the number of selected sets to K RIMP  we see that our method returns in the same order as many itemsets These descriptions require far less bits than tho se found by K RIMP  As such ours are a better approximation of the Kolmogorov complexity of the data Between S ET P ACK and S ET P ACK G REEDY the outcomes are very much alike this goes for both the obtained compression as well as the number of returned itemsets However the greedy search of S ET P ACK G REEDY allows for much shorter running times 8 Discussion The experimentation on our methods validates the quality of the returned models The models correctly detect dependencies in the data while ignoring independencies Only a small number of itemsets is returned which are shown to provide strong compression of the data By the MDL principle we then know these describes all important regularities in the data distribution in detail ef\002ciently and witho ut redundancy This claim is further supported by the high classi\002cation accuracies our models achieve The G REEDY P ACK algorithm generally uses more itemsets and obtains better packing ratios than S ET P ACK  While G REEDY P ACK is allowed to use any itemset S ET P ACK may only use frequent itemsets This suggests that we may able to achieve better ratios if we use different candidates  for example low-entropy sets 16  The running times of the experiments reported in this work range from seconds to hours and depend mainly on the number of attributes and rows of the datasets The exhaustive version S ET P ACK may be slow on very large candidate sets however the greedy version S ET P ACK G REEDY can even handle such families well Considering that our curren t implementation is rather na¨\021ve and the fact that both methods are easily parallelized both G REEDY P ACK and S ET P ACK G REEDY are suited for the analysis of large databases The main outcomes of our models are the itemsets that identify the encoding paths However the decision trees from which these sets are extracted can also be regarded as interesting as these provide an easily interpretable view o n the major interactions in the data Further just consideri ng the attributes used in such a tree as an itemset also allows for simple inspection of the main associations In this work we employ the MDL criterion to identify the optimal model Alternatively one could consider using either BIC or AIC both of which can easily be applied to judge between our decision tree-based models 9 Conclusions In this paper we presented two methods that 002nd compact sets of high quality itemsets Both methods employ compression to select the group of patterns that describe all interactions in the data best That is the data is considere d symmetric and thus both the 0s and 1s are taken into account in these descriptions Experimentation with our methods 
596 
596 


showed that high quality models are returned Their compact size typically tens to thousands of itemsets allow fo r easy further analysis of the found interactions References 1 C  C  A g g a r w a l a n d P  S  Y u  A n e w f r a m e w o r k f o r itemset generation In Proceedings of the ACM SIGACTSIGMOD-SIGART symposium on Principles of Database Systems PODS  pages 18–24 ACM Press 1998 2 R  A g r a w a l  H  M a n n i l a  R  S r i k a n t  H  T o i v o n e n  a n d A  I  Verkamo Fast discovery of association rules In Advances in Knowledge Discovery and Data Mining  pages 307–328 AAAI 1996 3 S  B r i n  R  M o t w a n i  a n d C  S i l v e r s t e i n  B e y o n d m a r k e t baskets Generalizing association rules to correlations In ACM SIGMOD International Conference on Management of Data  pages 265–276 ACM Press 1997 4 S  B r i n  R  M o t w a n i  J  D  U l l m a n  a n d S  T s u r  D y n a m i c itemset counting and implication rules for market basket data In ACM SIGMOD International Conference on Management of Data  pages 255–264 1997 5 B  B r i n g m a n n a n d A  Z i m m e r m a n n  T h e c h o s e n f e w  O n identifying valuable patterns In IEEE International Conference on Data Mining ICDM  pages 63–72 2007 6 T  C a l d e r s a n d B  G o e t h a l s  M i n i n g a l l n o n d e r i v a b l e f r e quent itemsets In Proceedings of the 6th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases  pages 74–85 2002 7 V  C h a n d o l a a n d V  K u m a r  S u m m a r i z a t i o n c o m p r e s s i n g data into an informative representation In Proceedings of the IEEE Conference on Data Mining  pages 98–105 2005 8 F  C o e n e n  T h e L U C S K D D d i s c r e t i s e d  n o r m a l i s e d A R M and CARM data library 2003 9 G  F  C o o p e r a n d E  H e r s k o v i t s  A B a y e s i a n m e t h o d f o r the induction of probabilistic networks from data Machine Learning  9:309–347 1992 10 T  C o v e r a n d J  T h o m a s  Elements of Information Theory 2nd ed John Wiley and Sons 2006 11 W  D u M o u c h e l a n d D  P r e g i b o n  E m p i r i c a l b a y e s s c r e e n i n g for multi-item associations In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 67–76 2001 12 C  F a l o u t s o s a n d V  M e g a l o o i k o n o m o u  O n d a t a m i n i n g  compression and kolmogorov complexity In Data Mining and Knowledge Discovery  volume 15 pages 3–20 Springer 2007 13 P  D  G r  u n w a l d  The Minimum Description Length Principle  MIT Press 2007 14 J  H a n  H  C h e n g  D  X i n  a n d X  Y a n  F r e q u e n t p a t t e r n mining Current status and future directions In Data Mining and Knowledge Discovery  volume 15 Springer 2007 15 J  H a n a n d J  P e i  M i n i n g f r e q u e n t p a t t e r n s b y p a t t e r n growth methodology and implications SIGKDD Explorations Newsletter  2\(2\:14–20 2000 16 H  H e i k i n h e i m o  E  H i n k k a n e n  H  M a n n i l a  T  M i e l i k  a i nen and J K Sepp¨anen Finding low-entropy sets and trees from binary data In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 350–359 2007 17 S  J a r o s z e w i c z a n d T  S c h e f f e r  F a s t d i s c o v e r y o f u n e x p ected patterns in data relative to a bayesian network In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 118–127 2005 18 S  J a r o s z e w i c z a n d D  A  S i m o v i c i  I n t e r e s t i n g n e s s o f frequent itemsets using bayesian networks as background knowledge In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 178–186 2004 19 A  J  K n o b b e a n d E  K  Y  H o  M a x i m a l l y i n f o r m a t i v e k itemsets and their ef\002cient discovery In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 237–244 2006 20 A  J  K n o b b e a n d E  K  Y  H o  P a t t e r n t e a m s  I n Proceedings of the 10th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases  pages 577–584 2006 21 P  K o n t k a n e n a n d P  M y l l y m  a k i  A l i n e a r t i m e a l g o r i t h m for computing the multinomial stochastic complexity Information Processing Letters  103\(6\:227–233 2007 22 M  v a n L e e u w e n  J  V r e e k e n  a n d A  S i e b e s  C o m p r e s s i o n picks the item sets that matter In Proceedings of the 10th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases  pages 585–592 2006 23 M  L i a n d P  V i t  a n y i  An Introduction to Kolmogorov Complexity and its Applications  Springer-Verlag 1993 24 R  M e o  T h e o r y o f d e p e n d e n c e v a l u e s  ACM Trans Database Syst  25\(3\:380–406 2000 25 A  J  M i t c h e l l J o n e s  G  A m o r i  W  B o g d a n o w i c z  B Krystufek P J H Reijnders F Spitzenberger M Stubb e J B M Thissen V Vohralik and J Zima The Atlas of European Mammals  Academic Press 1999 26 K  V  S  M u r t h y  On growing better decision trees from data  PhD thesis Johns Hopkins Univ Baltimore 1996 27 S  N i j s s e n a n d  E Fromont Mining optimal decision trees from itemset lattices In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 530–539 2007 28 N  P a s q u i e r  Y  B a s t i d e  R  T a o u i l  a n d L  L a k h a l  D i s c o vering frequent closed itemsets for association rules Lecture Notes in Computer Science  1540:398–416 1999 29 J  R i s s a n e n  F i s h e r i n f o r m a t i o n a n d s t o c h a s t i c c o m p l e xity IEEE Transactions on Information Theory  42\(1\:40–47 1996 30 A  S i e b e s  J  V r e e k e n  a n d M  v a n L e e u w e n  I t e m s e t s t h a t compress In Proceedings of the SIAM Conference on Data Mining  pages 393–404 2006 31 N  T a t t i  M a x i m u m e n t r o p y b a s e d s i g n i 002 c a n c e o f i t e m s e t s Knowledge and Information Systems KAIS  2008 Accepted for publication 32 N  T a t t i a n d H  H e i k i n h e i m o  D e c o m p o s a b l e f a m i l i e s o f itemsets In Proceedings of the 12th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases  2008 33 J  V r e e k e n  M  v a n L e e u w e n  a n d A  S i e b e s  C h a r a c t e r i s i ng the difference In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 765–774 2007 34 J  V r e e k e n  M  v a n L e e u w e n  a n d A  S i e b e s  P r e s e r v i n g privacy through data generation In Proceedings of the IEEE Conference on Data Mining  pages 685–690 2007 
597 
597 


