Rapid Scalability of Complex and Dynamic Web-Based Systems: Challenges and Recent Approaches to Mitigation Mani Malarvannan Cybelink www.cybelink.com manim@ieee.org S. Ramaswamy Department of Computer Science University of Arkansas at Little Rock AR 72204 srini@ieee.org Abstract In this paper, we summarize and outline some of the big challenges in addressing performance, scalability, and availability of applications with large and complex backend database systems. We present some of the 
limitations imposed by the CAP theorem on distributed systems development and identify some approaches that can be used to effectively address these limitations. We propose the adoption of an effective System-of-Systems approach that pushes adoptio n of principles supported by loosely-coupled SPU base d architecture, which can support incremental developme nt and rapid scalability of such complex and dynamic applicative needs 1. Introduction The rapid evolution of Internet has created technology companies such as Google, Amazon, Facebook 
Twitter, etc., which are unlike trad itional multinational companies. These web-focused companies have one major business requirement that is unique - they need to mange several petabytes of data to efficiently run their daily operations. The existence of these companies depends upon the performance, scalability, and availability of their large and complex backend database systems. Traditional relational database design techniques cannot support these companies manage such massive amounts of data. Furthermore, such voluminous data cannot be stored and managed using a 
single server, or even a cluster of servers The Internet research firm IDC estimates that the digital universe has 281 exabytes of data and it will grow to be  o st of the exponential  growth happening not due to traditional computing and communication equipments, but through mobile and embedded devices. For example, Google is estimated to currently process 20 petabytes of data eve ry day and  this is growing rapidly Amazon with their cloud computing solutions is stori ng petabytes of customer's data in their systems. Twitter gets more than 50 million tweets per day. For 
efficient management and service, they require thousands of servers located all over the world, to provid e access redundancy, so their customers always experience very efficient service performance. Managing such vastly distributed data across thousands of servers is not a simple task and it needs an effective system of systems engineering principles to identify and fix the issues that arise dynamically in real-time. Of course, while most of the data will sit idle in disks an d will only be retrieved when requested, the efficiency of retrieval when needed 
is of critical importance. Moreover, there are still several petabytes of data like email, blogs, web searches, online business transactions, etc. that are read and regularly updated by the internet community Web companies get thousands of concurrent requests either to read or update their data. They cannot store such data in small server farms to effectively serve their customers’ needs. They need to store their data across multiply distributed servers that are often geographically separated in multiple locations. For example if Amazon gets a request for a book from a US host it will read it from one of their data center located in US to the customer's browser. Now consider another 
request comes for the same book from a customer in London it will be retrieved from Amazon data center located in London. Say both the customers add the book into their shopping cart and successfully processed the order. Now Amazon need to reduce the quantity of the book by two and it has to be replicate this update to all their servers. How can Amazon do this seamlessly without any face-losing failures One of the hallmarks of such web-based applications is the ability to handle re quest spikes that come nondeterministically from a ll over the world. Anytime a major event \(like Haiti Earth 
quake ns, the number of tweets to a Twitter site spikes drastically by several millions. During these unexpected events, users following other users send a flurry of requests to find what other users are tweeting about. If one can imagine a user following thousand of other users trying to find the tweets posted, the issue then is about how can one manage these sudden upsurges in web traffic 2010 5th International Conference on System of Systems Engineering 978-1-4244-8196-5/10/$26.00 2010 IEEE DOI 10.1109/SOSE.2010.387 
 


Among several popular Google applications is Google Analytics [3 whic h ser ves a very im porta n t busines s need, especially for small businesses. This application embeds a small code snippet on web sites, thereby allowing companies to monitor and analyze web traffic on their sites. It is a free service offered by Google and millions of web sites are using it to perform real-time analysis of web site traffic. Such a strong dependency by businesses implies Google should provide these services consistently Once data is distributed and stored across multiple servers, companies need to worry about answering all the questions raised above in a cost effective and efficient manner. In such a co mplex system of multiply distributed server farms, single or multiple server failures are to be assumed as a daily occurrence, and companies need to identify and fix such server failures as they occur without causing any service disruption This has become a huge issue for cloud computing environments; environments where companies manage data for other companies, subject to honoring specific service-level agreements \(S LAs\l processes tools and technologies cannot help companies to manage their data stored ov er such a vast array of servers. They need to adopt system of systems engineering principles; specifically with respect to dealing with distributed dat abases, in order to achieve their goals The rest of the paper is organized as follows: Section II briefly presents RDBMS, their properties, various efforts to improve RDBMS fo r rapid scalability and some of their limitations.  Section III presents the CAP theorem, BASE property and its importance to the development of such applications that have rapid exponential-scale’ upswings in load / user driven activity. Section IV…, Section V 2. RDBMS and ACID Properties RDBMS have been a de-facto standard for managing data for several years now. Based on the ACID properties \(Atomicity, C onsistency, Isolation Durability\RDBMS offered all the benefits needed to manage data in a single server or in a rack of servers But RDBMS have failed to offer th e benefits needed for web companies due to following reasons i RDBMS is built on a btree index that works well as long as the index can be stored in memory. Any time a request comes, index in the memory is used to find the location in the hard disk to either read or update the data. By avoiding the disk seeks RDBMS gave good performance to the users. But web based companies need to maintain large volume of data, and these in dexes often cannot fit in the memory. Hence RDBMS need to do disk seeks to find data, an expensive operation that degrades the scalability of the application ii RDBMS vendor solutions to the above problem is to buy bigger servers with more RAM capacity termed vertical scaling.  Though this may work for some instances, it cannot completely address huge user spikes that are experienced by the web companies and ultimately it will be a performance bottleneck to support th e simultaneous requests from a large number web customers While relational database systems are a good fit for applications that follows ACID properties, they are not appropriate for applications th at undergo rapid surges in user activities. For example in a trading application it is a fundamental requirement to inform the customer if a bid is successfully placed or not, an RDBMS may suffice. Any time a transaction fails due to one database instance failure then th e entire transaction is abandoned The data updated at one instance is either available in all the instances or it is not For web-based applications ACID properties are too restrictive to satisfy the associated business needs due to surges in user activity hence a different set of properties must be established to achieve scalab ility and performance 2.1. RDBMS Replication RDBMS has been using data replication techniques to achieve performance and scalability. There are several types of replication available and each has its own set of limitations for web based applications. Different relational databases support different flavors of replication: Master-Slave, Multi-Master, Synchronous and Asynchronous. For read-intensive applications replication gives good scalability and performance but for write-intensive applications it cannot provide the desired scalability. Let us examine each of these in more detail in the sequel 2.1.1. Master-Slave Replication In this configuration one master manages several slaves; while reads can happen from any slave, all writes must go through one Master. This restriction degrades scalability and performance. It additionally, leads to a single point of failure for all write operations 2.1.2. Multi-Master Replication In this configuration more than one master manages several slaves, though write operations may scale, it will lead to manual conflict resolutions wh ich are error-prone and problematic as the number of masters’ increase 2.1.3. Synchronous Multi-Master Replication RDBMS vendors have been using two phase commit 
 


transactions for a long time to guarantee ACID properties to support synchr onous replication. In the two phase commit protocol, first a transaction manager asks each participating database instance to check if it can commit the transaction. If all the database instances agree then the transaction manager asks each database instance to commit the transaction. If any database instance vetoes the commit, then all database instances are asked to roll back their transactions. For web-based applications this cannot work as they maintain their data in multiple servers that could go down without warning, causing write failures and degrading availability of the application 2.1.4. Asynchronous Multi-Master Replication In asynchronous replication, when more than one master gets a write operation for the same data, both will succeed. In this case, the database has to resolve the conflicts based on timesta mps on the data. Current relational databases do not su pport this feature out of the box. Even if they have the ability to support, such support is not automatic and manual intervention is needed to resolve the conflicts 2.2. RDBMS and Sharding Database sharding is a process in which the data is partitioned either horizontally or vertically and kept in different servers to increase performance and scalability. Although this concept has been around for a while, it has been popularized by Google engineers for improving the throughput and overall performance of high-transaction, large database-centric business applications Database ven dors support two types of partitions, vertical partition and horizontal partition. In vertical partition a table is split vertically and kept in different servers and in ho rizontal partition different rows are kept in different database servers. When a read or write operation comes from a client it is routed to the server where the data resides to execute the operation The main problem with relational database partition is performing join queries, without partition all the join queries will execute in the same server. Once the data is shared across multiple serv ers, it is not feasible to perform join queries in a reasonable time that spans multiple servers. For example, if we are creating a partition for a twitter website in a relational database using horizontal partitions one approach is to store the different set of users and their tweets in different servers. When a request comes to display all the twittes for a particular user, it is easy to read the data from one server and send it to the client. Twitter allows a user to follow other users, by following other users you can see their tweets. Now if a request comes from a user to see all the tweets posted by their “following users”, we have to perform join query to join the twittes across all the servers where the “fo llowing users” data resides  Figure 1. Twitter Information flows 3. Distributed Databases and Scalability The Internet pioneers Amazon and Google created distributed database systems that follows Eric Brewer's CAP theorem their own needs, which are now widely used by other internet companies 3.1. CAP Theorem Simply put, the CAP theorem suggests that no distributed database systems can have all of the following pr operties of Consistency Availability and Partition Toleranc e, at the same time 1 Consistency Set of operations performed on distributed databases is perceived as a single operation by the clients. It is similar to “C” in ACID properties. In low volume transactions where the data is stored in single database, it is possible to achieve consistency. But for web based applications where the data is stored in multiple servers, it is not possible to achieve strict consistency 2 Availability The system must be always available to the clients 3 Partition Tolerance All the operations on the distributed database will co mplete even if individual database instances are not available 3.2. BASE Distributed databases which have only two of the properties mentioned in the CAP theorem, are said to satisfy BASE properties, or B asically A vailable S oft State E ventually Consistent. BASE properties suggest that distributed systems are available all the time even if the individual database instances are not consistent and it will become consistent eventually BASE is opposite to ACID, ACID is pessimistic and forces consistency on every operation. BASE, on the other hand, is optimistic and accepts the fact that the data will be in inconsistent state for clients, by relaxing the consistency distri buted databases achieve scalability, availability, and perfo rmance which are the backbone for web applications. Of course BASE is not suitable for all types of app lications, for example it is not possible to design an online trading system using BASE, one will need to us e relational database that supports ACID. But there are web applications where it is acceptable to have BASE properties and for those distributed databases, it is fast becoming the standard 
 


3.3. Distributed Databases Google created Big Table nd Am azon create d Dynam eir own scalability, performance and availability issues. Numerou s open source projects created distributed databas es based on these two technologies that are called in different names like keyvalue db, document store, extended record, etc. While explaining all the projects and their differences is beyond the scope of this paper, we will explore the big two: Big Table and Dynamo to see how they achieve BASE properties 3.3.1. Big Table Google’s BigTable is a multi dimensional map built on top of their propriety Google File System, GFS gTable data is arranged in rows and columns that can be accessed by giving row and column positions. To manage a huge table, it is split across row boundaries called Tablet. By default GFS replicates the data into three servers  Figure 2. BigTable Architecture BigTable has a Master server that manages the Tablet Servers and performs other Meta operations. For read operation BigTable Client sends a request to BigTable Master to get the location of the Primary and secondary Tablet Servers. Once the client gets the locations of the tablet servers, it directly contacts the closed Tablet server to read the data. For write operations once again the client contacts the Master serv er to get the locations of the tablet servers and send s the data directly to the Primary Tablet Server. The Primary Tablet Server appends the new data with its existing data and keeps it in memory. It then no tifies the two secondary servers to perform the same mergin g operation. If everything succeeds the Primary Tablet Server notifies the client the success of the write. If any of the merging operation fails then the operation is repeated until the data is consistent across all the servers. Based on the above, it is clear that BigTable supports C and P properties of the CAP theorem. When the write operation is executed on the BigTable, it is guarant eed that all the data is in consistent state for subsequent read operations. If any of the merging mechanism fails then the write operation fails, by giving importance to Consistency and Partition Tolerance it reduces the availab ility of the BigTable Though it is rare all the Tablet servers will fail but it is still possible 3.3.2. Dynamo Dynamo is Amazon’s distributed keyvalue pair database used by several Amazon applications. Based on CAP theorem Dynamo gives importance to A and P, any time a client puts a value with an associated key, Dynamo adds version to the key-value and replicates it to other nodes within the same cluster Number of nodes that will be replicated is a configuration parameter. In th e figure, Client1 executes a put command with key-value pair to Node6, if the replication configuration parameter is set to three Node6 replicates the key-val ue along with version to Node1, Nod2, and Node3. An y time a Node gets a keyvalue with version, first it checks if it already has the key, if it finds the same key, checks if it is newer to the key it just received. If it is new, deletes the older version with newer version. This automatic reconciliation is called syntactic reconciliation. If more than one client reads a key-value concurrently and writes it back, Dynamo accepts both the write and stores all the updates from the clients. In this case Dynamo has more than one version of the same keyvalue pair and it cannot reconcile automatically. When a read comes for the same key, Dynamo sends all the versions for that key and makes the client to do manual reconciliation which is called semantic reconciliation Dynamo’s architecture is based on eventual consistency model, replication is perform ed asynchronously. Any time a client writes the key-v alue, even before it replicates the data to all th e nodes, the write operation returns to the client. Amazon’s business requires high scalability on read and writes operations, they get millions of concurrent reques ts to view their catalog and write the contents of th e shopping cart. In the CAP properties Dynamo sacrificed C and has A and P   Figure 3. Dynamo Architecture 4. System of Systems Approach Based on BigTable and Dynamo there are many open source projects which are being actively developed and some of them are used in commercial web applications While it is true that the CAP theorem states it is not possible to achieve all three of C, A, and P 
 


simultaneously, a using a sy stem of systems approach where the architecture is dynamically reconfigurable can support all th ree properties adequately. For example, by setting the number of nodes to replicate the data asynchronously, we can have distributed DB which behaves like a RDBMS with strong consistency. Based on the applicative needs, distributed database can be modified from strong to wea k consistency models and vice versa 4.1. Software Processing Unit Architecture Traditional multi layered architecture based approach has been used to architect large-scale complex software systems. In a multi layered architecture approach, the lower layer provides services to the upper layer. Any time a software object moves between layers it has to go through serialization, de serialization and it has to pass through different networks which affect both performance and scalability. Multi layer architecture works well where the load on th e system is predictable and provides vertical scalability. But the complex nature of web application ne ed massive scalability with minimal cost and buying bi gger hardware is expensive and hence we may quickly reach the upper limit on scalability  Figure 4. Proposed SPU Architecture Building on similar concepts proposed by space-based  nothi and adopting a loosely-coupled systems approach, we propose a new architectur al paradigm for web applications design, in wh ich the application is built as stateless software services a nd deployed in its own server with all other software components are contained in a single module called Software Processing Unit SPU. This SPU architecture, as depicted in the figure 4 will operate on top of a distributed database and provides both ACID and CAP properties based on the needs of the SPUs. Each SPU is a self contained unit that has its own cached data messaging, rule engine, UI and other software components needed. Only time an SPU communicates with external resource is when it needs to either read or write data into the distributed database. SPU Request Router routes the incoming requests to appropriate SPU for processing. Once the connection is established be tween the client and an SPU all the communication happens between them directly 4.1. Designing software for SPU Architecture The SPU architecture collapses traditional multi-layer design into single module called SPU. This does not mean that UI, businesses services, data access logic must be designed into one single monolithic code Individual components must be designed with high cohesion and low coupling. Th e only requirement is when deployed all the components for a SPU must collocate in a single physical server. Intra communication between components within a SPU will happen through messaging or local method invocation depending on asynchronous or synchronous nature of the call. Since there are no remote calls and network delays for the components inside a SPU, this design scales rapidly Only time a SPU communicates with external system is when it need to persist th eir data in a permanent store For this purpose all SPUs run on top of a Distributed DB. Based on the needs of an individual SPU the distributed DB can have either weak or strong consistency. All the read operati ons from SPU to the Distributed DB happen with out delay. Based on the need of the individual SPU write operations can happen either through weak or strong consistency 4.2. Cloud Ready SPU Architecture The loosely coupled nature of the SPU architecture allows it to be deployed on a cloud computing platform The cloud allows SPU provisioning and virtualization based on the needs of external loads. The cloud monitors the individual loads on a SPU and either increases or decreases the number of instances of the SPU. The cloud can also monitor the load and switch the Distributed DB from w eak consistency to strong and vice versa 4.3. Limitations of SPU architecture SPU architecture provides massive scalability for read intensive applications. For write intensive applications it provides good scalability co mpared with traditional multi-tier architecture that run s on relational databases but it is limited by the communication between SPU and Distributed DB. Also write intensive applications 
 


need to communicate the state changes between SPU instances so that the cached data will be in sync in all SPU instances 5. Conclusion In this paper, we have su mmarized and outlin ed some of the big challenges in addressing performance scalability, and availability of applications with large and complex backend database systems. We have presented the limitations imposed by the CAP theorem and the approaches that can be used to address such limitations. We have articulated the use of a looselycoupled system of systems approach to scale the software architecture further Our future work will be focused on studies that validate the applicability of the SPU architecture approach with open source Distributed Database on in dustrial strength software systems and issues that arise therein 6. Acknowledgments This work is based in part, upon research supported by the National Science Foundation \(under Grant Nos. CNS 0619069, EPS-0701890, OISE 0729792, CNS-0855248 and EPS-0918970\y opinions, findings, and conclusions or recommendations expressed in this material are those of the author\(s\ reflect the views of the funding agencies  References 1 The Digital Universe Is Still Growing”, EMC / IDC study, 2009. http://www.emc.com/digital_universe 2 J. Dean, S. Ghemwat, “MapReduce: simplified Data Processing on Large Clusters”, Communications of the ACM, 51\(1\ 2008, pp. 107-113 3 M. Tyler, J. Ledford, “Google Analytics”, JWS, 2006 4 http://www.google.com/analytics 5 Dan Pritchett, “BASE: An ACID Alternative”, ACM Queue, July 28 th 2008 6 I. Filip, I. C. Vasar, R. Robu, “Considerations about an Oracle database multi-master replication”, 5th International Symposium on Applied Computational Intelligence and Informatics,  May 2009, pp 147 – 152 7 F. Chang, J. Dean, S. Ghem awat, W. C. Hsieh, D. A Wallach, M. Burrows, T. Chandra, A. Fikes, R. E Gruber, “Bigtable: A Distributed Storage System for Structured Data”, 7th Symposium on Operating System Design and Implementation, OSDI'06, pp 205-218 8 G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati A. Lakshman, A. Pilchin, S. Sivasubramanian, P Vosshall, W. Vogels, ‘Dynamo: Amazon’s Highly Available Key-Value Store”, ACM SIGOPS Operating Systems Review, 41\(6\, December 2007, pp. 205-220 9 Sanjay Ghemawat, Howard Gobioff, Shun-Tak Leung the google File System, ACM SIGOPS Operating Systems Review,  37\(5\43 10. C. Dumont, F. Mourlin, “Space Based Architecture for Numerical Solving” Intnl Conf. on Comp. Intel. for Modelling, Control & Automation, 2008. pp. 309-314 11. J. Lifflancer, A. McDonald, O. Pilskalns, “Clustering Versus Shared Nothing: A Case Study”, 33rd Annual IEEE International Computer Software and Applications Conference, 2009. COMPSAC '09, pp. 116-121 
 


inside B using the B refinement mechanism to derive design specifications and generate implementation The rational for transforming KAOS operations to B while not transforming the rest of the goal graph is that operations sums up all the behaviors that agents need to have to fulfill their requirements, which are the leaf goals in the goal graph. The mechanism of constructing the goal graph shows that high level goals are refined using AND/OR refinement steps until leaf goals are derived meaning that the fulfillment of leaf goals implies the fulfillment of the higher level goals in the goal graph. Therefore, it is safe to only transform KAOS operations used to express behaviors of agents that perform them to fulfill the leaf goals in the goal graph without compromising the completeness and consistency properties of the requirements model  4.3 Transforming Security Requirements to B  The initial B machine obtained from transforming the requirements model obtained in section 3.2 to B includes an abstract representation of each KAOS operation as a B operation. KAOS operations preconditions are mapped to preconditions of their corresponding B operations. Machine invariants in B are constructed from the invariants of the KAOS objects manipulated by KAOS goals. For further details on the transformation scheme, refe e  have employed the B-Toolkit, which is one of the two most famous commercial tools for B development as a tool to develop our B model and refine it to derive design specifications and implementation. We highlight the significant parts of the B machine as follows MACHINE SpyNetwork \(maxSpies The SpyNetwork machine has a set representing all the spies in the network. The machine invariant defines the types of the machine variables that represent the state of the machine. The machine variables include the set of spies as well as their security attributes such as their mailbox passwords, their public and private keys  The KAOS operations are mapped to the following B operations. We use the signRevelation and accessMailbox operations as examples of KAOS operations that are transformed to their B equivalents and refine them in B due to space limitation     4.4 Derivation of Design and Implementation  The abstract machine obtained from transforming the KAOS operations to B is then refined to formally derive design specifications and implementation of the SNS security requirements. Each B refinement step prior to the implementation refinement reflects some design decision\(s\ the refining B machine to the refined B machine until implementation is obtained. The refinement mechanism in B provides means for documenting design decisions and building forward traceability links from requirements to design This is achieved through building a more detailed model from a more abstract one The first refinement step for the SNS focuses on data refinement through making the design decision of representing the pool of spies as an array of spies and we will show how the signRevelation and accessMailbox operations are refined accordingly as examples. From the traceability perspective, we can see that this data refinement step does not address the realization of a specific security requirement in the system. It rather concentrates on building a concrete data structure representing the internal system state in a form realizable by programming languages while implementation is generated  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


  The second refinement step makes a design decision of employing DSA \(Digital Signature Algorithm\ as the signature algorithm to achieve the revelation integrity requirements. This refinement step provides traceability links between integrity requirements and the design that realizes them. The second refinement step is classified as a procedural refinement since it only removes the non-determinism of the digital signature algorithm being used without modifying the state representation of the refined machine. The second refinement step is not described in detail due to space limitation The next step is to refine SpyNetworkR1 \(the second refinement step\e implementation machine      The final step is to generate C code from the implementation machine. The programming language choice is based on the programming languages available in the B tool being used. Almost all the commercial B tools generate code in C and very few of them generate ADA. The security properties should be maintained by the design decisions and the semantics of the B machines rather than by specific security construct in the programming language to which the implementation machine is translated  5. Security Specification Changes  With the introduction of security changes to software systems, more vulnerability might be added to the system as a result of the maintenance activities Current security engineering approaches might not either consider maintenance activities or provide sufficient information to perform accurate impact analysis resulting in introducing security vulnerabilities. In this section, we demonstrate the Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


capability of FADES to structurally handle changes of security specifications by providing sufficient traceability information allowing for more complete and accurate impact analysis of change. According to  e u s e o f a s o f t w a re m odel th at s t ores des i gn decisions and traceability links significantly improve the accuracy and completeness of impact analysis Maintenance activities are classified into four categories according e \(ch a ng es i n t h e  software environment\ective \(new user requirements\e \(fixing errors\d preventive \(prevent future problems\ave chosen an example of a corrective change since corrective changes consume 21% of change requests h e other three categories of maintenance activities will be part of our future work A defective scenario threatening revelation confidentiality is as follows: A spy leaves his team and gets reallocated to another team after a message has been sent. This scenario is not handled by the current encryption/decryption solution used to protect the confidentiality of revelations since the leaving spy would receive a revelation that he is no longer eligible to receive. To correct this defect, the KAOS framework provides a conflict construct that allows the expression of situations that contradict with system requirements. We introduce the following conflict to the RevelationConfidentiality goal   This conflict could be resolved using one of the patterns for conflict resolution y i n trodu ci n g a  new goal to anticipate the conflict   This goal could be assigned to a reliable agent such as the big boss. Analyzing the impact of introducing the new goal shows that the RevelationConfidentiality requirement would be affected by this change. The traceability information provided by the hierarchical structure of the goal graph and the KAOS refinement mechanism direct the change impact analysis to revisit the AND refinement of the RevelationConfidentiality goal. The new goal needs to be added as a subgoal to the refinement of the RevelationConfidentiality goal The goal graph for the RevelationConfidentiality goal would be modified as in Figure 4 \(notice the dark gray subgoal that has been added to the refinement of the RevelationConfidentiality goal  Figure 4:  Accommodating the New Goal Since the new goal is a leaf goal, it will be operationalized using the following operation Operation NotifyRelayWithReallocation Input  Spy{arg relay}, Spy{arg leavingSpy DomPre   relay leavingSpy DomPost  relay, leavingSpy:Spy\ Notified\(relay leavingSpy ReqPreFor  team1, team2:Team\mber\(relay team1\mber\(leavingSpy, team1  Member\(leavingSpy, team2 This operation needs to be transformed to B in order to propagate this corrective change to the derived design and implementation. According to the change impact analysis performed with respect to the transformation of the new operation to B, we discovered that the state representation \(Variables the SpyNetwork machine needs to be complemented with the following variables: team, relaySpies authorizedReceiversFrom, authorizedSendersTo. These variables represent the set of assigned relays of all teams as well as the set of spies authorized to send to or receive from relays. Constraints on these variables need to be added to the invariant of the machine as follows   The definition of the operation notifyRelayWithReallocation is given below while its refinement is not shown here due to space limitation Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


  6. Conclusion and Future Work  This paper presented the FADES approach using the SNS case study. This research addressed a case study that represents a sample of the category of communication systems using the FADES approach This category of systems share a common set of security requirements that have been elaborated in the paper. This assists in verifying the applicability of FADES to communication systems that exhibit high security demands. We have also illustrated how the underlying formality allows for documenting design decisions and trace these decisions to their corresponding security requirements While this paper demonstrated the conceptual framework of FADES, a number of future research steps are ongoing. We have embarked on case studies from other categories of software systems for further refinement and to show its applications. Further FADES is being quantitatively compared to other relevant approaches that derive design from requirements. The comparison will involve the application of both FADES and the other comparable approaches to the same case studies to measure the strength and limitations of FADES as opposed to other relevant approaches Future work might involve the demonstration of the capability of FADES to structurally handle other categories of maintenance activities \(perfective adaptive and preventive  Acknowledgement  This work was supported in part by Science Foundation Ireland grant 03/CE2/I303_1 to Lero-the Irish Software Engineering Research Center www.lero.ie Thanks to Dr. Van Lamsweerde who granted us a free license of the Objectiver software that has facilitated our work in the elicitation and modeling of security requirements using the KAOS framework  References  1  A. van Lamsweerde Elaborating Security Requirements by Construction of Intentional AntiModels  Proc. ICSE 04: 26th Intl. Conf. on Software  Engineering May 2004 2  Wilander, J. & Gustavsson, J Security requirements  A field study of current practice  Symposium on Requirement Engineering for  Information Security SREIS 2005 Paris, France, 2005 3  CERT http://www.cert.org/stats/cert_stats.html  4  A. van Lamsweerde and E. Letier Handling Obstacles in Goal-Oriented Requirements Engineering  IEEE Transactions on Software Engineering Special Issue on Exception Handling, Vol. 26 No. 10, Oct. 2000, 9781005 5  Lehman M. M On understanding Laws, evolution and conversation in the large program lifecycle  Journal of Software & Systems vol. 1, pp. 213 - 221, 1980 6  Cimitile, A.; Fasolino, A.R.; Visaggio, G A Software Model for Impact Analysis: A Validation Experiment  Proceedings of the Sixth Working Conference on Reverse Engineering Oct. 1999, pg. 212-222 7  P.J. Fontaine Goal-Oriented Elaboration of Security Requirements M.S. Thesis, Dept. Computing Science University of Louvain, June 2001 8  Abdel-Moneim, Riham, et. al Goal-Oriented, BBased, Formal Derivation of Security Design Specifications from Security Requirements  Symposium on Requirements Engineering for Information Security Barcelona, Spain, 2008 9  J. Viega and G. McGraw Building Secure Software How to Avoid Security Problems the Right Way Addison-Wesley, 2001 10  Massacci, Fabio, et. al Using a Security Requirements Engineering Methodology in Practice: The Compliance with The Italian Data Protection Legislation  Computer Standards & Interfaces issue 27, pg. 445-455, 2005 11  Cansell, Dominique and M´ery, Dominique Foundations of the B Method  Proceedings of Computing and Informatics vol. 22, pg. 1-31, 2003 12  Sekerinski, Emil, et. al Program Development by Refinement: Case Studies Using the B Method  Springer, 1999 13  Cimitile, A.; Fasolino, A.R.; Visaggio, G A Software Model for Impact Analysis: A Validation Experiment  Proceedings of the Sixth Working Conference on Reverse Engineering Oct. 1999, pg. 212-222 14  Bowen, J., and Hinchey, M Application of Formal Methods Prentice Hall, 1995 15  Stepney, Susan, Cooper, et. al An Electronic Purse Specification, Refinement, and Proof  Programming Research Group, Oxford University Computing Labarotory England, July 2000 16  Abrial, J.-R The B Book: Assigning Programs to Meanings Prentice-Hall  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


  11  Raffi P. Tikidjian received his B.S degree in Computer Science from the University of Cal Poly Pomona in 2003 and went on to receive his M.S. degree in Computer Science speci alized in the area of Software Engineering from the University of Southern California \(USC in 2006.  He is presently a member of technical staff for the Reasoning, Modeling, and Simulation Group at the Jet Propulsion Laboratory and a PhD candidate in the Center for Systems and Software Engineering \(CSSE at the University of Southern California \(USC guidance of Dr. Barry Boehm His interests are in the areas of systems health management, simulation and modeling model based engineering technologies, software methodologies and processes, mobile and web application user interface design Doug Abraham is a Senior Engineer within the Architecture and Strategic Planning Office of JPL\222s Interplanetary Network Directorate.  As the Strategic Forecasting Lead, he oversees efforts to forecast future mission customer requirements and trends, assesses their implications for Deep Space Network evolution, and assists in the development of the roadmaps and plans needed to guide this evolution.  Doug also supports NASA HQ-led study activ ities pertaining to future lunar and Mars communications architectures Prior to his current assignment, Doug worked on the Galileo, Ulysses, and Cassini flight projects.  He has also worked on several pre-project formulation activities including Pluto Fast Flyb y and the \223Fire and Ice\224 collaboration with Russia Doug began his career as a graduate student intern in the International Space Station Program Office \(1988 graduated Magna Cum Laude from Texas A&M University in Physics \(1986\.S. in Technology and Science Policy, with specialization in technology assessment and electrical engi neering, from Georgia Tech 1990 Janet Wu received her Bachelor of Science \(2000\.S. \(2002 degrees in Planetary Science from the Massachusetts Institute of Technology in Cambridge, MA She has been a technical staff member in the Optical Communications group at the Jet Propulsion Laboratory in Pasadena, CA since 2002 


target incoming target incoming target incoming source outgoing Figure 6. EIS System Network View Meta-model operations are those used for service description and must be ultimately decomposed into elementary ones \(i.e. data processing, storing and transferring are estimated through parameter values that are propagated by service invocation parameters to parameters describing application operations constituting the service description which are further propagated to parameters describing elementary operations 4.3. EIS Architecture Design Task Implementation EIS Architecture Design tasks may be supported by existing tools [20]. Systems Modeling Language \(SysML 15] is considered as the most appropriate for EIS System Network model representation and requirement engineering, since it supports the concepts of requirements and resource allocation. As a direct consequence, SysML allows the representation of requirements as model elements which means that requirements are part of the system architecture. For representation purposes, a SysML pro?le for EIS System Network meta-model \(?gure 6 mented as a plugin to MagicDraw modeling tool [2]. In order to facilitate model exchangeability, EIS System Network model is being realized in XML, which is a standard exchangeable format. In order to exchange data with speci?c software tools, model transformations will be accomplished through appropriate XSLTs developed for each tool for example as the one transforming XMI to the EIS System Network document type description \(DTD 5. Case Study In the following we discuss the case of renovating a legacy information system supporting a large-scale public organization based on the proposed concepts. The organization supports more than 350 interconnected regional of?ces and its main purpose is to provide services to the public both citizens and businesses. Regional of?ces are divided into three categories according to their size and information infrastructure requirements \(large, medium and small More than 15.000 employees work in the organization having on-line access to the legacy system. There are more than 300 different services provided to the public, while each citizen is required to register in the one belonging to his/her residential area, called residential of?ce. Some of them require the actual presence of citizens in their residential of?ce Existing system architecture is based on a fat clientserver architecture. All application logic is programmed within the client platform, while data is distributed in local database servers located in each regional of?ce. A Central database is supported in the Datacenter for data synchronization and lookup purposes. The Datacenter and all regional of?ces participate in a private TCP/IP network to facilitate ef?cient data replication. Most data related to a speci?c citizen are maintained as local data in his/her residential of?ce. Client programs access the local database to store data, while they access the central database mostly for lookup purposes. Local data are asynchronously replicated in the central database using a transaction management system \(TMS Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 to facilitate communication with the central database. The central database provides the overall view of each citizen  s record 


record To enhance the level of services provided by the organization, we decided to establish an enhanced e-services environment through an e-government portal. The main target of the portal is to minimize the need for citizen  s presence in regional of?ces and intents to deal with all the drawbacks of the current e-service platform. It provides easy access to citizens and businesses twenty four hours per day, seven days per week. It also promotes the increase of e-services to users, facilitating the organization to accomplish its strategic goals. The portal facilitates on-line transactional services and ensures on-line access to the databases of the legacy information system Provision of transactional e-services re?ects the operation of the legacy system and thus results in its renovation. In order to effectively support both systems \(e.g the portal and the legacy system able to apply the same policies and minimize maintenance cost. Thus, it was decided to explore the renovation of the legacy information system by adopting modern technological trends, such as multi-tiered application architecture server-based computing and light clients. It was decided also to rewrite application code based on J2EE architecture to develop a web interface for the legacy information system in order to support a uni?ed environment for both the legacy system and the portal. This decision affected the legacy system architecture, described in System Network View. Some of issues raised included: \(a database architecture? It is currently distributed. Should it become centralized? What are the implications in the network infrastructure? \(b complished to minimize maintenance cost Though EA was never fully described, the organization had already decided to establish an EA based on Zachman framework a few years ago. RUPmethodology was used for software development, thus application description models were developed within Rational Rose platform. In order to be able to apply the proposed tasks identi?ed in section 4, relative information had to be extracted from the corresponding cells. Application description \(e.g. applications and modules tracted from corresponding Rational Rose ?les. Though the process was not automated, the provision of System Network meta-model, helped architecture designer to identify the information needed to obtain from software designers Detailed service description in terms of load requirements could not be extracted from software description. This was crucial in order to decide upon Intranet and Datacenter architecture. This information was collected by interviewing software developers The new system has to deal with a number of require&lt;&lt;Site&gt;&gt Central Organization Building lt;&lt;ServerProcess&gt;&gt Oracle Central database lt;&lt;Site&gt;&gt Medium Regional Office lt;&lt;Site&gt;&gt Datacenter server room lt;&lt;Site&gt;&gt Large Regional Office lt;&lt;Site&gt;&gt Small Regional Office oracle L.R.O lt;&lt;ServerProcess&gt;&gt tuxido L.R.O lt;&lt;ServerProcess&gt;&gt lt;&lt;Site&gt;&gt L.R.O. department tuxido Central 


tuxido Central lt;&lt;ServerProcess&gt;&gt lt;&lt;Site&gt;&gt R.O. server room lt;&lt;ClientProcess&gt;&gt oracle net lt;&lt;Site&gt;&gt Organization lt;&lt;ClientProcess&gt;&gt tuxido lt;&lt;ClientProcess&gt;&gt application lt;&lt;UserProfile&gt;&gt officer lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;initiate&gt;&gt Figure 7. Topology View - Existing System ments, with security and availability being the most important ones. Security issues have to do with the security of the network, security of data, authentication control, etc. Availability requirements deal with the backup subsystem, the recovery system and high availability UPS. Privacy must be enforced with the use of cryptography and compression techniques. All these requirements were identi?ed during the System Architecture design process and consequently exported in System Motivation cell where all system requirements are gathered using a simpli?ed text-based requirement description method System Architecture design tasks were performed by existing tools already described in [20]. The existence of System Network meta-model and its implementation in XML facilitated tool integration and interoperability. The identi?cation of primary EIS engineering activities served by Zachman matrix rows and columns facilitated a better understanding between software developers, architecture designers and organization management and enhanced discrete methodology integration. Existing and renovated application architecture of the legacy system de?ned by Topology View are presented in ?gures 7 and 8 respectively. The screenshots are from the MagicDraw [2] tool, enhanced with EIS pro?le to provide the appropriate functionality 6. Conclusions &amp; Future Work MB-EISE process based on Zachman framework was explored in the paper. The designer may adjust basic MB-EISE activity model for each cell, formulate a methodology-independent EIS cell-related view, and ?nally identify methods and tools appropriate for implementing each speci?c task. One could argue that in such a case, 36 distinct EIS sub-views should be de?ned, each of them being rather complex, while basic MB-EISE activity should be adjusted 36 times, resulting in a very complicated process Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 lt;&lt;Site&gt;&gt Central Organization Building lt;&lt;ServerProcess&gt;&gt Oracle Application Server lt;&lt;ServerProcess&gt;&gt Oracle Central database lt;&lt;Site&gt;&gt Medium Regional Office lt;&lt;Site&gt;&gt Datacenter server room lt;&lt;Site&gt;&gt Large Regional Office lt;&lt;Site&gt;&gt 


lt;&lt;Site&gt;&gt Small Regional Office lt;&lt;Site&gt;&gt L.R.O. department lt;&lt;Site&gt;&gt Organization lt;&lt;ClientProcess&gt;&gt web browser lt;&lt;ServerProcess&gt;&gt Web Server lt;&lt;UserProfile&gt;&gt officer &lt;&lt;initiate&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt Figure 8. Topology View - Renovated System However, EIS engineering process, as enterprise architecture, is itself complex. The bene?t of the proposed approach is that all aspects \(simple or complex form and modular fashion. Cell-related sub-views and corresponding meta-models, as well as cell-related MB-EISE activity model may be progressively formed according to the designer  s priorities and perspectives Having a black-box view of each Zachman cell, the proposed approach focuses on EIS view integration and interview consistency. The notion of external entities when de?ning EIS cell-related views provides the means for interoperability with external cells, while at the same time facilitates atomicity within the limits of each cell. We are currently emphasizing Business and System rows, and especially Function and Network cells, exploring in parallel Motivation column and the way non-functional requirements are managed Having a white-box view of each Zachman cell, it is evident that the de?nition of a technology neutral metamodel and the identi?cation of basic engineering tasks, corresponding to EIS cell-related views, contributes to the integration of different methodologies and tools. A library of EIS System Network models has been already implemented in XML. Emphasis is given to requirements management and especially requirements derivation References 1] Institute For Enterprise Architecture Developments http://www.enterprise-architecture.info 2] MagicDraw UML. http://www.magicdraw.com 3] A. Aurum and C. Wohlin. Engineering and Managing Software Requirements. Springer, 2005 4] F. S. d. Boer, M. M. Bonsangue, J. Jacob, A. Stam, and L. W N. v. d. Torre. A Logical Viewpoint on Architectures. In EDOC, pages 73  83. IEEE Computer Society, 2004 5] E. R. Byrne. IEEE Standard 830: Recommended Practice for Software Requirements Speci?cations, 1998 6] B. Dave and D. Jim. The new, improved RUP SE Architecture Framework, 2005. IBM Rational Edge 7] D. J. de Villiers. Using the Zachman Framework to assess RUP. Rational Edge, 2001 8] J. A. Estefan. Survey of Model-based Systems Engineering \(MBSE May 2007 9] A. Fatolahi and F. Shams. An investigation into applying UML to the Zachman Framework. Information Systems Frontiers, 8\(2  143, 2006 10] M. Glinz. On non-functional Requirements. 15th IEEE International Requirements Engineering Conference, 2007 11] F. Goethals, W. Lemahieu, M. Snoeck, and J. Vandenbulcke. An overview of enterprise architecture framework deliverables. In Banda RKJ \(ed Introduction. ICFAI University Press., 2006 12] H.-P. Hoffmann. Harmony-SE/SysML Deskbook: ModelBased Systems Engineering with Rhapsody, Rev. 1.51 Telelogic/I-Logix white paper. Telelogic AB, May 2006 


Telelogic/I-Logix white paper. Telelogic AB, May 2006 13] IEEE. IEEE Recommended Practice for Architectural Description for Software-Intensive Systems - Std 1471. Technical report, oct 2000 14] O. M. G. Inc. UML 2.0 Superstructure Speci?cation, October 2004 15] O. M. G. Inc. Systems Modeling Language \(SYSML i?cation. Version 1.0, September 2007 16] INCOSE. INCOSE Handbook SE Process Model, September 2003. http://g2sebok.incose.org 17] Institute for Electrical and Electronic Engineers. IEEE Std 15288 -2004, Systems Engineering -System Life Cycle Processes, June 2005 18] A. v. Lamsweerde. Goal-Oriented Requirements Engineering: A Guided Tour. In Fifth IEEE International Symposium on Requirements Engineering \(RE  01 19] S. Leist and G. Zellner. Evaluation of current architecture frameworks. In H. Haddad, editor, SAC, pages 1546  1553 ACM, 2006 20] M. Nikolaidou and N. Alexopoulou. Enterprise Information System Engineering: A Model-Based Approach Based on the Zachman Framework. In HICSS  08. IEEE Computer Society, 2008 21] M. Nikolaidou and D. Anagnostopoulos. A systematic approach for con?guring web-based information systems Journal of Distributed and Parallel Databases, 17\(3  290, May 2005 22] C. M. Pereira and P. Sousa. A method to de?ne an Enterprise Architecture using the Zachman Framework. In H. Haddad A. Omicini, R. L. Wainwright, and L. M. Liebrock, editors SAC, pages 1366  1371. ACM, 2004 23] K. Pohl and E. Sikora. Supporting the Co-Design of Requirements and Architectural Artifacts. In 15th IEEE International Requirements Engineering Conference \(RE  07 pages 258  261, India Habitat Center, New Delhi, 2007 24] J. Schekkerman. How to Survive in the Jungle of Enterprise Architecture Frameworks: Creating or Choosing an Enterprise Architecture Framework. Trafford, 2003 25] J. F. Sowa and J. A. Zachman. Extending and Formalizing the Framework for Information Systems Architecture. IBM Systems Journal, 31\(3  616, 1992 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


C. \(2005 Implementation and Use in an Existing Clinical Information System. In Connecting Medical Informatics and Bio-Informatics: Proceedings of MIE2005 - The XIXth International Congress of the European Federation for Medical Informatics, 328-333. IOS Press, 2005  4] Fetterman, D. M. Ethnography, 2nd ed. Thousand Oaks CA: Sage, 1997  5] Furukawa, N.  Ikeda, H.  Kato, Y.  Sako, H. D-Pen: a digital pen system for public and business enterprises. In Frontiers in Handwriting Recognition 2004: Proceedings of the Ninth International Workshop on Frontiers in Handwriting Recognition \(IWFHR-9 2004  6] Guimbreti  re, F. 2003. Paper augmented digital documents. In Proceedings of the 16th Annual ACM Symposium on User interface Software and Technology Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 UIST ?03 November 2003  7] Holman, D., Vertegaal, R., Altosaar, M., Troje, N., and Johns, D. 2005. Paper windows: interaction techniques for digital paper. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems \(CHI ?05 Portland, Oregon. New York: ACM, 2005  8] Kiewra, K., DuBois, N., Christian, D., McShane, A Meyerhoffer, M., &amp; Roskelley, D. Note-taking functions and techniques. Journal of Educational Psychology, 83 240-245, 1991  9] Kobayashi, K. Combined effects of note-taking/reviewing on learning and enhancements through interventions: a meta-analytic review. Educational Psychology, 26, 459-477, 2006  10] Liao, C., Guimbreti  re, F., and Hinckley, K. 2005 PapierCraft: a command system for interactive paper. In Proceedings of the 18th Annual ACM Symposium on User interface Software and Technology \(UIST ?05 Seattle, WA. New York: ACM, 2005  11] Livescribe. Pulse Smartpen [electronic device http://www.livescribe.com/, last retrieved May 28, 2008  12] Logitech. io2 Digital Pen [electronic device http://www.logitech.com/index.cfm/mice_pointers/digital_ pen/devices/408&amp;cl=us,en, last retrieved May 28, 2008  13] Norrie, M. C., Signer, B., and Weibel, N. Print-n-link weaving the paper web. In Proceedings of the 2006 ACM Symposium on Document Engineering \(DocEng '06 New York: ACM, 2006  14] Randall, D., Harper, R., and Rouncefield, M Fieldwork for Design: Theory and Practice. London Springer-Verlag, 2007  15] Searle, J. R. Speech Acts: An Essay in the Philosophy of Language. Cambridge: Cambridge Univ. Press, 1969  16] Sellen, A. J. and Harper, R. H. The Myth of the Paperless Office. Cambridge, MA: MIT Press, 2003  17] Signer, B. and Norrie, M. C. 2007. PaperPoint: a paper-based presentation and interactive paper prototyping 


paper-based presentation and interactive paper prototyping tool. In Proceedings of the 1st international Conference on Tangible and Embedded interaction \(TEI ?07 Baton Rouge, Louisiana. New York: ACM, 2007  18] Tanabe, K., Yoshihara, M., Kameya, H., Mori, S Omata, S., Ito, T., Automatic Signature Verification Based on the Dynamic Feature of Pressure. Proceedings of the Sixth International Conference on Document Analysis and Recognition \(ICDAR ?01 Computer Society, 2001   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


The HyspIRI mission utilizes innovative techniques to both reduce the amount of data that must be transmitted to the ground and accommodate the required data volume on the ground The infrastructure and techniques developed by this mission will open the door to future high data volume science missions The designs presented here are the work of the authors and may differ from the current HyspIRI mission baseline A CKNOWLEDGMENTS This research was carried out at the Jet Propulsion Laboratory California Institute of Technology and was sponsored by the Space Grant program and the National Aeronautics and Space Administration R EFERENCES  K W ar\002eld T  V  Houten C Hee g V  Smith S Mobasser B Cox Y He R Jolly C Baker S Barry K Klassen A Nash M Vick S Kondos M Wallace J Wertz Chen R Cowley W Smythe S Klein L Cin-Young D Morabito M Pugh and R Miyake 223Hyspiri-tir mission study 2007-07 002nal report internal jpl document,\224 TeamX 923 Jet Propulsion Laboratory California Institute of Technology 4800 Oak Grove Drive Pasadena CA 91109 July 2007  R O Green 223Hyspiri summer 2008 o v ervie w  224 2008 Information exchanged during presentation  S Hook 2008 Information e xchanged during meeting discussion July 16th  R O Green 223Measuring the earth wi th imaging spectroscopy,\224 2008  223Moore s la w Made real by intel inno v ation 224 http://www.intel.com/technology/mooreslaw/index.htm  T  Doggett R Greele y  S Chein R Castano and B Cichy 223Autonomous detection of cryospheric change with hyperion on-board earth observing-1,\224 Remote Sensing of Environment  vol 101 pp 447\226462 2006  R Castano D Mazzoni N T ang and T  Dogget 223Learning classi\002ers for science event detection in remote sensing imagery,\224 in Proceedings of the ISAIRAS 2005 Conference  2005  S Shif fman 223Cloud detection from satellite imagery A comparison of expert-generated and autmatically-generated decision trees.\224 ti.arc.nasa.gov/m/pub/917/0917 Shiffman  M Griggin H Burk e D Mandl and J Miller  223Cloud cover detection algorithm for eo-1 hyperion imagery,\224 Geoscience and Remote Sensing Symposium 2003 IGARSS 03 Proceedings 2003 IEEE International  vol 1 pp 86\22689 July 2003  V  V apnik Advances in Kernel Methods Support Vector Learning  MIT Press 1999  C Bur ges 223 A tutorial on support v ector machines for pattern recognition,\224 Data Mining and Knowledge Discovery  vol 2 pp 121\226167 1998  M Klemish 223F ast lossless compression of multispectral imagery internal jpl document,\224 October 2007  F  Rizzo 223Lo w-comple xity lossless compression of h yperspectral imagery via linear prediction,\224 p 2 IEEE Signal Processing Letters IEEE 2005  R Roosta 223Nasa jpl Nasa electronic parts and packaging program.\224 http://nepp.nasa.gov/docuploads/3C8F70A32452-4336-B70CDF1C1B08F805/JPL%20RadTolerant%20FPGAs%20for%20Space%20Applications.pdf December 2004  I Xilinx 223Xilinx  Radiation-hardened virtex-4 qpro-v family overview.\224 http://www.xilinx.com/support/documentation data sheets/ds653.pdf March 2008  G S F  Center  223Tdrss o v ervie w  224 http://msp.gsfc.nasa.gov/tdrss/oview.html 7  H Hemmati 07 2008 Information e xchanged during meeting about LaserComm  223W orldvie w-1 224 http://www digitalglobe.com/inde x.php 86/WorldView-1 2008  223Sv albard ground station nor way.\224 http://www.aerospacetechnology.com/projects/svalbard 7 2008  223Satellite tracking ground station 224 http://www.asf.alaska.edu/stgs 2008  R Flaherty  223Sn/gn systems o v ervie w  224 tech rep Goddard Space Flight Center NASA 7 2002  223Geoe ye-1 f act sheet 224 http://launch.geoeye.com/launchsite/about/fact sheet.aspx 2008  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-720 Transmitter  5 2007 PDF Spec Sheet for the T720 Ku-Band TDRSS Transmitter  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-722 X-Band  7 2007 PDF Spec Sheet for the T-722  J Smith 07 2008 Information e xchanged during meeting about GDS  J Carpena-Nunez L Graham C Hartzell D Racek T Tao and C Taylor 223End-to-end data system design for hyspiri mission.\224 Jet Propulsion Laboratory Education Of\002ce 2008  J Behnk e T  W atts B K obler  D Lo we S F ox and R Meyer 223Eosdis petabyte archives Tenth anniversary,\224 Mass Storage Systems and Technologies 2005 Proceedings 22nd IEEE  13th NASA Goddard Conference on  pp 81\22693 April 2005 19 


 M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolving a ten year old data system,\224 Space Mission Challenges for Information Technology 2006 SMC-IT 2006 Second IEEE International Conference on  pp 8 pp.\226 July 2006  S Marle y  M Moore and B Clark 223Building costeffective remote data storage capabilities for nasa's eosdis,\224 Mass Storage Systems and Technologies 2003 MSST 2003 Proceedings 20th IEEE/11th NASA Goddard Conference on  pp 28\22639 April 2003  223Earth science data and information system esdis project.\224 http://esdis.eosdis.nasa.gov/index.html  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolution of the earth observing system eos data and information system eosdis\\224 Geoscience and Remote Sensing Symposium 2006 IGARSS 2006 IEEE International Conference on  pp 309\226312 31 2006Aug 4 2006  223Earth science mission operations esmo 224 http://eos.gsfc.nasa.gov/esmo  E Masuoka and M T eague 223Science in v estig ator led global processing for the modis instrument,\224 Geoscience and Remote Sensing Symposium 2001 IGARSS 01 IEEE 2001 International  vol 1 pp 384\226386 vol.1 2001  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Earth observing system eos data and information system eosdis 227 evolution update and future,\224 Geoscience and Remote Sensing Symposium 2007 IGARSS 2007 IEEE International  pp 4005\2264008 July 2007  D McAdam 223The e v olving role of tape in the data center,\224 The Clipper Group Explorer  December 2006  223Sun microsystems announces w orld s 002rst one terabyte tape storage drive.\224 http://www.sun.com/aboutsun/pr/200807/sun\003ash.20080714.2.xml July 2008  223P anasas 227 welcome 224 http://www panasas.com  R Domikis J Douglas and L Bisson 223Impacts of data format variability on environmental visual analysis systems.\224 http://ams.confex.com/ams/pdfpapers/119728.pdf  223Wh y did nasa choose hdf-eos as the format for data products from the earth observing system eos instruments?.\224 http://hdfeos.net/reference/Info docs/SESDA docs/NASA chooses HDFEOS.php July 2001  R E Ullman 223Status and plans for hdfeos nasa's format for eos standard products.\224 http://www.hdfeos.net/hdfeos status HDFEOSStatus.htm July 2001  223Hdf esdis project.\224 http://hdf.ncsa.uiuc.edu/projects/esdis/index.html August 2007  223W elcome to the ogc website 224 http://www.opengeospatial.org 2008  223Open gis Gis lounge geographic information systems.\224 http://gislounge.com/open-gis Christine M Hartzell received her B.S in Aerospace Engineering for Georgia Institute of Technology with Highest Honors in 2008 She is currently a PhD student at the University of Colorado at Boulder where she is researching the impact of solar radiation pressure on the dynamics of dust around asteroids She has spent two summers working at JPL on the data handling system for the HyspIRI mission with particular emphasis on the cloud detection algorithm development and instrument design Jennifer Carpena-Nunez received her B.S in physics in 2008 from the University of Puerto Rico where she is currently a PhD student in Chemical Physics Her research involves 002eld emission studies of nanostructures and she is currently developing a 002eld emission setup for further studies on nano\002eld emitters The summer of 2008 she worked at JPL on the HyspIRI mission There she was responsible for the science analysis of the data handling system speci\002cally de\002ning the data level and processing and determining potential mission collaborations Lindley C Graham is currently a junior at the Massachusetts Institute of Technology where she is working towards a B.S in Aerospace Engineering She spent last summer working at JPL on the data handling system for the HyspIRI mission focusing on developing a data storage and distribution strategy 20 


David M Racek is a senior working toward a B.S in Computer Engineering at Montana State University He works in the Montana State Space Science and Engineering Laboratory where he specializes in particle detector instruments and circuits He spent last summer working at JPL on compression algorithms for the HyspIRI mission Tony S Tao is currently a junior honor student at the Pennsylvania State University working towards a B.S in Aerospace Engineering and a Space Systems Engineering Certi\002cation Tony works in the PSU Student Space Programs Laboratory as the project manager of the OSIRIS Cube Satellite and as a systems engineer on the NittanySat nanosatellite both of which aim to study the ionosphere During his work at JPL in the summer of 2008 Tony worked on the communication and broadcast system of the HyspIRI satellite as well as a prototype Google Earth module for science product distribution Christianna E Taylor received her B.S from Boston University in 2005 and her M.S at Georgia Institute of Technology in 2008 She is currently pursing her PhD at the Georgia Institute of Technology and plans to pursue her MBA and Public Policy Certi\002cate in the near future She worked on the ground station selection for the HyspIRI mission during the summer of 2008 and looks forward to working at JPL in the coming year as a NASA GSRP fellow Hannah R Goldberg received her M.S.E.E and B.S.E from the Department of Electrical Engineering and Computer Science at the University of Michigan in 2004 and 2003 respectively She has been employed at the Jet Propulsion Laboratory California Institute of Technology since 2004 as a member of the technical staff in the Precision Motion Control and Celestial Sensors group Her research interests include the development of nano-class spacecraft and microsystems Charles D Norton is a Principal Member of Technical Staff at the Jet Propulsion Laboratory California Institute of Technology He received his Ph.D in Computer Science from Rensselaer and his B.S.E in Electrical Engineering and Computer Science from Princeton University Prior to joining JPL he was a National Research Council resident scientist His work covers advanced scienti\002c software for Earth and space science modeling with an emphasis on high performance computing and 002nite element adaptive methods Additionally he is leading efforts in development of smart payload instrument concepts He has given 32 national and international keynote/invited talks published in numerous journals conference proceedings and book chapters He is a member of the editorial board of the journal Scienti\002c Programming the IEEE Technical Committee on Scalable Computing a Senior Member of IEEE recipient of the JPL Lew Allen Award and a NASA Exceptional Service Medal 21 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207–216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Int’l Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Int’l Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





