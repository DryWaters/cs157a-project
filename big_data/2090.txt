Associations and Rules in Data Mining: a Linkage Analysis Witold Pedrycz Department of Electrical  Computer Engineering University of Alberta, Edmonton Canada pedrycz@ee.ualberta.ca and Systems Research Institute, Polish Academy of Sciences 0 1-447 Warsaw Poland Abstract  We discuss a problem of synthesis and analysis of granular rules emerging in data mining. Two descriptors of the rules that is relevance and consistency being viewed individually and en block are 
introduced The relevance of the rules is quantified in terms of the data being covered by the antecedents and conclusions standing there While this index describes each rule individually the consistency of the rule deals with the quality of the rule viewed vis-a-vis other rules It expresses how much the rule \223interacts\224 with others in the sense that its conclusion is distorted by the conclusion parts coming from other rules We show how the rules are formed by means of fuzzy clustering and their quality can be evaluated in terms of the above indexes Global characteristics of a set 
of rules are also discussed and\222 related to the number of information granules being constructed in the data space I. INTRODUCTION Rule-based systems are granular and highly modular models  1][4][5 6][7][8 9][11 121  13 141 The granularity of rules becomes fully reflected in the form of the antecedents and conclusions and is quantified in the language of sets fuzzy sets, rough sets, probability, to name the main formal vehicles The evident transparency of these systems is their genuine asset. Several design issues remain still open and tend to become even more profound as we move toward developing larger 
systems a The origin of the rules and their eventual quantification e.g confidence or relevance of relationships where the confidence measure is expressed in relation to experimental data proceeding with further detailed construction and refinement of the rule-based architecture More descriptively, one can consider the outcome of the link analysis to arise as a web of connections between the granules. As fundamentally we distinguish between input and output variable\(s there are two essential facets of the analysis that deal with the following aspects a\expressing strength between the granules in the input and output space \(relevance of the rule\and b\completing a consistency \(crosstalk\analysis in which we 
quantify an interaction between the given link and the links that are invoked owing to the interaction \(overlap\between the information granules In the language of the calculus of rule-based systems, the notion of relevance is linked with the notion of strength of the rule whereas the second aspect of consistency is concerned with an interaction between the information granules The material is organized into 6 sections. First, in Section 2 we discuss clustering as a basic means of information granulation and pose the problem of constructing rules Section 3 contains a discussion of two descriptors of the rules namely rule relevance and rule consistency Experimental studies are covered in Section 
4 while conclusions are included in Section 5 11 CLUSTERING AS A VEHICLE OF INFORMATION GRANULATION Quite often the Fuzzy C-Means FCM algorithm arises as a basic vehicle of data granulation As the method is well known in the literature, cf 2 we will not discuss it here but b The dimensionality problem becomes of concern when the number of variables in the rules increases then the number of rules tend to explode at the exponential rate Fuzzy clustering gives rise to fuzzy sets and fuzzy relations being thus constructed in a fully algorithmic fashion. They are examples of information granules regarded as 
a basic building canvass of numerous fuzzy models For given collections of such information granules, studying and quantifying relationships between them leads to the emergence of the rule-based models The preliminary development step of this nature is referred to as a link analysis  a phase in which we reveal and quantify dependencies between information granules before rather clarify the notation and cast the method in the setting of the problem at hand The input and output data spaces are denoted by X and Y respectively. The numeric data set under discussion assumes a form of input  output pairs x\(k y\(k k=1,2 
 N where xk E R\224 and Yk E R\224 The results of clustering carried out for x and Y separately are discrete fuzzy sets more precisely fuzzy relations defined over these data sets In general we end up with 223c[l]\224 fuzzy relations in x say A A2  A 41 and 223c[2]\224 fuzzy relations in Y namely B1 B2 0-7803-7280-8/02/$10.00 02002 IEEE 867 


 B cp Technically they are organized in a form of partition matrices so we have TAl 1 TB1 1 Our ultimate goal is a formation of a thorough and constructive description of a web of directed links between Ai and Bj In a nutshell, such task gives rise to a collection of rules, see Figure 1  ifAithenB The techniques of fuzzy clustering, no matter what type of objective function we employ, share several main features whose understanding is of importance in the framework of this problem Clusters are direction-free relational constructs: in the clustering process there are no provisions as to a possible direction between the variables Figure 1 Rule-based system as a web of directed links between information granules \(shadowed regions\formed in the space of antecedents and conclusions through fuzzy clustering This implies that the resulting granules do not accommodate any request that relates to the mapping itself and are not formed in a way they reflect a directionality component In contrast any mapping \(function is a directional construct. If we establish a link between Ai and B its nature needs to be inspected with respect to the directionality of the rule obtained in this manner More formally if there is no directionality component or it is ignored we are essentially assessing properties of a Cartesian product of the two information granules namely A x Bj Fuzzy clusters information granules are sound building blocks of rule based systems as they provide an answer to the two challenges outlined at the beginning of the previous section First they reflect the structure of the data so we anticipate that any cluster comes with enough experimental evidence behind it otherwise it would not have been formed in the first place\Second as we are concerned with fuzzy relations rather than fuzzy sets the dimensionality problem does not arise For instance if  then we have potentially 223c\224 meaningful rules in spite of the potentially high dimensionality of the input and output space. Otherwise as it happens quite often when dealing with the individual fuzzy sets we end up with a combinatorial explosion of the collection of the rules produced at this level In what follows by proceeding with information granules formed by clustering we synthesize a collection of rules and analyze their properties The crux of this development is as follows With each Ai i=1,2  c[l we associate a single granule in the conclusion space, that is we form a mapping Ai 3 Bj 1 the way of determining the associations will be discussed later on\Following this procedure we build a collection of rules R where its cardinality is equal to In a concise way we can summarize the rules as a list of pairs of indexes of the respective information granules where jk is in the range of integers from 1 to c[2 The characteristics of this form of assignment between the information granules are quantified in the next Section 111 CHARACTERISTICS OF THE RULES There are two main descriptors of the rules. The first one reflects the experimental evidence behind the rule association\The second captures the relationships between the rules discussed as a collection of entities Relevance Let us view a certain rule as a Cartesian product\meaning that we do not consider the \223direction\224 of the rule but look at it as an entity linking two information granules defined in the two different spaces \(input-output\The rule 1 comes with the \(experimental\relevance equal to N rel\(Ai X Bj  Ai X k yk  k=l 2 where 223t\224 is a t-norm viewed here as a model of an and logical connective If rel\(.\attains higher values we say that the rule comes with more experimental relevance in other words it is more justifiable from the experimental point of view\For fixed 223i\224 we order all associations \(rules Ai 3B1 Ai  Bz  Ai 3 B c[2 according to the associated relevance level. The highest value of the relevance identifies a rule of the form Ai  B Q In light of the above realization properties of t-norms we come up with a straightforward monotonicity property namely If Ai A\222i and B B\222 then rel\(Ai XB 5 rel\(A\222i XB\222 Intuitively, note that if we increase the size of the information granules this change contributes to the increasing level of relevance of the particular rule as in this 0-7803-7280-8/02/$10.00 92002 IEEE 868 


way we tend to 223cover\224 more data and thus elevate an experimental evidence of this rule AI andA2 different The relevance defined above exhibits a close analogy to the notion of rule support encountered in data mining that is articulated in the language of probability theory and reads in the form conflicting rules exhibit rules are different similar conclusion Support\(A9B Prob\(A X B Consistency The associations we have constructed so far were totally isolated We have not expressed and quantified possible interactions between the rules Nevertheless, this interaction does exist owing to the nature of the overlapping fuzzy sets relations Considering two rules we note that their antecedents and conclusions overlap to a certain degree. The overlap at the condition end could be very different than the one encountered at the conclusion end The differences between the overlap levels shed light on an important issue of consistency of the rules In turn this leads to a detection of conflicting rules where the term of conflict itself is rather continuous than binary so we talk about a degree of conflict or equivalently, a degree of consistency The problem of conflicting rules is well known in the literature and has a long path in the research in rule  based systems especially those in the realm of fuzzy controllers, cf IO Bearing in mind the origin of the control knowledge this effect was attributed mainly to some deficiencies of knowledge acquisition when working with a human expert As in this study we are concerned with an automated vehicle of information granulation, the effect of conflict is a result of incompatibility of information granules in the spaces of conditions and conclusions Before we move on with a detailed quantification of this effect let us concentrate on Table 1. It summarizes four different scenarios of interaction occurring between two rules AI B1 and A2 3 B2 I BI and B2 similar I BI and B2 different A and A similar I rules are redundant I rules are We develop an index that captures the effect of consistency of two rules In its construction we follow the observations coming from Table 1 The consistency measure is developed in two steps by a expressing a measure of consistency of the rules for a single data point x\(k y\(k and b constructing a global performance measure over all data X v Note that the term of similarity is invoked at the level of the information granules rather than original data so in essence we are looking at Al\(x\(k and Az\(x\(k along with Bl\(y\(k and B2\(y\(k To express a degree of similarity we use the formula that is deeply rooted in the language of logic and set theory we say that two sets are equal if the first is included in the another and vice versa The continuous version of this statement being realized in the framework of membership grades of the respective fuzzy sets reads as A x\(k  A2 x\(k  A W  A2 x\(k t\(A 2 x\(kN  A I x\(k 3 we observe that the implication in logic corresponds to the inclusion operation in set theory The implication is implemented in the form of the residuation operation implied by a certain t-norm, namely a  b  sup{c E 0,1 I atc I b a b E 0,1 The consistency is low only if AI and A2 are similar and BI and B2 are different This naturally leads to the following expression as a measure of consistency Al\(X\(k  A2\(x\(k  B,\(Y\(k  B,\(Y\(k 4 To gain a better insight into the character of this expression we plot it for the residuation generated by the product operation, refer to Figure 2 Figure 2 Plot of the residuation c  d induced by the product operation, \(\223c\224 and \223d\224 denote a level of similarity in the space of antecedents and conclusions 0-7803-7280-8/02/$10.00 02002 IEEE 869 


The above expression concerns a single data point. Naturally a sum over the entire data is a legitimate global measure of consistency of the two rules \(rule-1 and rule-2 reasons we discuss this selection in more detail later on The relevance of the rules are shown in Figure 3 I 2 A,\(x\(k  A,\(x\(k  cons\(l,2  N k=l Likewise we may like to express a consistency of a given rule versus all other rules in the ruleset R This leads to the expression Cons\(i R  cons\(i j j=l j#i 6 where 223i\224 and 2213\222\222 are indexes of the rules in R IV EXPERIMENTS In this section we are concerned with a synthesis and analysis of fizzy rules for one selected data sets available on the WWW ftp://flp.ics.uci.edu/pub/machine-leaming databases namely fuel consumption In both cases we use the FCM method set up in the same way across all experiments The fizzification factor m standing in the standard objective function is equal to 2 where c  c[l or c[2 The dissimilarity between the pattems is expressed in terms of a weighted Euclidean distance A partition matrix is initialized randomly. Once the information granules clusters have been generated the analysis of the rules is completed in terms of their relevance and consistency. Furthermore we carry out a global analysis as to the number of rules and quantify them as an overall collection and derive characteristics related to their suitability in the description of data As to the implementation details of t-norms we use a product operation The implication is also induced by the same t-norm The dataset under consideration comes from the StatLib library that is maintained at Carnegie Mellon University. The data concerns a city-cycle fuel consumption that is expressed in miles per gallon and has to be predicted on a basis of 7 attributes number of cylinders displacement horsepower weight, acceleration model year, and origin\All but the fuel consumption are treated as inputs City-cycle fuel consumption is an output variable \(conclusion We choose the number of clusters \(information granules to be equal to c[l  c[2  7 this is done for illustrative output Fig   input Jre 2213 Relevance of the rules In each row of the matrix Figure 3 we have at least one fairly dominant association By selecting the dominant links we end up with the following seven rules: 1 3 5 2 3 7 3  3 4 5 536 6 4 7 34 wherethe above is a schematic summary of the links associations between the information granules in the input and output space. Noticeably all rules are quite similar in terms of their relevance One may anticipate this to be a result of using clusters as generic building blocks of the rules that comes with a similar experimental evidence behind the fizzy relations\consistency levels of these rules are as follows Again, these levels of consistency are fairly similar across all the rules with a single exception rule 33 3 is the most consistent with the corresponding value of the consistency level of 3.45 Now we investigate a situation where there is a significantly different number of the information granules in the input and output spaces. More specifically we analyze a c[1 3 and and b c[l 10 and The results are visualized in Figure 4 and Tables 2 and 3 Figure 4 Relevance of the rules for c[l]=3 and c[2]=10 0-7803-7280-8/02/$10.00 02002 IEEE 870 


I 1 rule I 6 2 7 relevance consistency 28.63 0.76 23.85 1.02 3 8 I 33.98 Table 2 Rules associations formed by the information granules for c[l]=3 and c[2]=10 1.20 rule 1 1 233 3+1 relevance consistency 39.32 3.26 18.98 2.23 33.16 3.43 7 2 I 26.36 I 3.54 8 3 18.20 2.25 4+3 5+1 6 3   9 1 I 22.02 I 2.30 10 3 22.23 2.3 1 18.62 2.27 23.54 2.35 19.16 2.25 Table 3 Rules associations formed by the information granules for c[l]=3 and c\(2]=10 The most striking is a fact of increasing consistency of the rules with the increasing number of the information granules in the condition space The consistency goes down significantly when the values of c[2 get lower. The change in c[21 from 10 to 3 decreases the level of consistency by a factor of 2 V CONCLUSIONS The study focused on the synthesis of information granules fuzzy sets and fuzzy relations and generation of rules \(associations composed of them. Such associations are characterized by two indexes The first one is about the relevance of the rule and expresses how much experimental evidence is behind the association. The second one is about a directional aspect of the construct and describes much a given rule interacts with all others and produces a crosstalk relational\effect We also looked into an interesting numeric quantification of the rules with respect to the size of the vocabulary of the information granules both at the antecedent and conclusion part of the rules. It was revealed that there are some cutoff values of these granules beyond which the quality of the rules drops off significantly The issue of the numeric quality of the rules has not been discussed at all namely, a problem of expressing the quality of the rules vis-&vis the original experimental numeric data In other words we have not studied the features of the transformation of the inference results coming from the rules into numeric representations This phase is definitely related with the clustering mechanism itself the number of clusters in the space of conclusion and a way of aggregation of the conclusions The above analysis imposes a minimal level of structural dependencies between the rules The rules developed here are the direct product of data summarization as we use fizzy clustering to reveal and capture the structure of the data It should be stressed that the cluster-based rules help avoid combinatorial explosion in cases of high dimensional spaces The language of relations rather than fuzzy sets becomes instrumental in this setting 0-7803-7280-8/02~$10.00 02002 JEEE 871 References 1 A Bardossy L Duckstein Fuzzy Rule-Based Modeling with Application toGeophysica1 Biological and Engineering Systems CRC Press Boca Raton 1995 2 J.C Bezdek Pattern Recognition with Fuzzy Objective Function Algorithms Plenum Press N York 1981 3 B Bouchon-Meunier M Rifqi S Bothorel Towards general measures ofcomparison ofobjects Fuzzy Sets andSystems 84,2 1996, 143-153 4 0 Cordon, M.J. del Jesus, F Herrera A proposal on reasoning methods in fuzzy rule based classification systems Znt J ofApproxima6e Reasoning 20 1999 2145 5 0 Cordon F Herrera Villar P Analysis and guidelines to obtain a good uniform fuzzy partition granularity for fuzzy rule-based systems using simulated annealing Int J of Approximate Reasoning Vol 25 3 2000 6 M. Delgado F Gomez-Skarmeta and F Martin A fuzzy clustering-based prototyping for hzzy rule-based modeling IEEE Transactions on Fuzzy Systems 5\(2 1991,223-233 7 M.Delgado A.F. Gomez-Skamets F Martin A methodology to model fuzzy systems using fuzzy clustering in a rapid-prototyping approach Fuz Sets and Systems vol 97 no.3 1998,287-302 8 D Dubois H Prade What are fuzzy rules and how toiuse them Fuzzy Sets and Systems 84 Y996,169-185 9 H Ishibuchi IC Nozaki N Yamamoto H Tanaka Selecting fuzzy if then rules for classification problems using genetic algorithms IEEE Transactions on Fuzzy Systems Vol 3 3 1995,260i270 10 W Pedrycz Fuzzy Control and Fuzzy Systems 2"d.edition Research Studies Press, Chichester 1993 1 1 E H. Ruspini On the semantics of fuzzy logic International Journal of Approximate Reasoning 5 199 1 45-88 12 T Sudkamp, Similarity, interpolation and fuzzy rule construction Fuzzy Sets andSystems vol 58 no 1 1993,13-86 13 T. A Sudkamp, R J Hammell 11 Granularity and specificity in fuzzy function approximation in Proc NAFIPS-98 105-109 1998 14 R R. Yager D P Filev Essentials of Fuzzy Modeling and Control J Wiley New York 1994 187-215 


18 Number of nodes 2 4 8 Number of nodes 2 4 8 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 amount of transaction WS cluster NPA CC Shift 0.148 0.117 0.258 2.180 2.180 0.766 1.855 1.855 0.469 SR2201 0.369 0.356 0.889 0.385 0.385 1.114 0.378 0.378 0.842 Figure 4 The execution time and the commu nication time vs the number of transactions 3 140 160 i Shin  cc  i _  20 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 number of transactions Figure 5 The ratio of the execution time for each parallel algorithm to the execution time for corresponding sequential algorithm shows that the execution time and the communication time for all algorithms increase in proportion to the number of transactions The execution time for the Shift is shorter than those for the other algorithms The advantage of parallelization is evident when the number of transactions is more than 2500 Figure 5 shows that the ratio of the execution time of a parallel algorithm to that of a sequential algorithm decreases as the number of transactions increase 5.3 Relation between execution time and the number of nodes The execution time and the communication time of three parallel algorithms on the WS cluster when the number of transactions was 5000 and the minimum support was 1 are plotted in Figure 6 against the number of nodes For all algorithms the execution time 6 5 4 I3 _ E 2 1 0 2 4 8 16 32 number of nodes Figure 6 The execution time and the commu nication time vs the number of nodes decreases when the number of nodes increases, whereas the communication time increases when the number of nodes increases The performance of Shift algorithm is the best and there is little difference between the performances of the NPA and CC algorithms 5.4 Results on the parallel computer The execution time and the communication time of the three algorithms running on the parallel computer when the number of transactions was 5000 and the min imum support was 1 is plotted in Figure 7 against the number of nodes The communication times for three algorithms running on the 11's cluster and on the par allel computer are listed along with the numbers of nodes in Table 3 Table 3 shows that the communication times for NPA and CC algorithms are shorter on the parallel computer than they are in the WS cluster environment 366 


NPA  1 EO 1 number of nodes Figure 7 The execution time and the commu nication time on the SR2201 On the other hand, the communication time for Shift algorithms is shorter on the WS cluster than that on the SR2201 6 Discussions 6.1 Execution on a workstation cluster The execution times for NP.4 and CC algorithms increase rapidly with a decrease in the minimum sup port This is because the number of candidate itemsets increases when the minimum support decreases and these algorithms scan the all candidate itemsets in the transaction database at each node But because the Shift algorithm partitions the candidate itemsets into each node the execution time of scanning is shorter than that for the other algorithms and does not in crease as rapidly when the minimum support decreases The execution times for all algorithms increase with an increase in the number of transactions and the ra tio of the execution time of each parallel algorithm to the execution time of the corresponding sequential al gorithm decreases with an increase in the number of transactions This ratio for the Shift algorithm reaches a minimum value when the number of transactions is 4000 and the algorithm converges more rapidly than the other algorithms The execution times for all algorithms decreases with an increase in the number of nodes The re duction of the communication time in Shift operations seems to be due to the scanning processing and com munication processing being executed asynchronously Because data are searched in parallel and all amounts of searched data are almost the same for all these al gorithms CPU processing time which is the difference of the communication time from the execution time is almost the same for all algorithms Our cost analysis showed that the amount of com munication was smallest for the CC algorithms but the execution times for NPA and CC algorithms were almost the same in our experiments This seems to be because that the size of data used in our experiment is not so large 6.2 Execution on the parallel computer The execution times for all algorithms were much longer on the parallel computer than that were in the WS cluster environment We think this is because amount of CPU memory available on the parallel com puter was insufficient On the other hand the com munication time was more stable on the parallel com puter and the time for communication between nodes is shorter on the parallel computer In other words the ratio of the communication time to the execution time is large in a WS cluster environment and the commu nication time has a great influence on the execution time In the WS cluster environment the communication time for the Shift algorithm which uses shift opera tions is less than that for the CC algorithm which uses broadcast operations On the other hand, the commu nication time for the Shift algorithm on the parallel computer is lager than that for the CC algorithm The Shift algorithm is therefore effective in a LVS cluster environment that can execute shift operations rapidly 7 Conclusion The distributed algorithms proposed in this pa per are effective when parallel processing distributed throughout clustered computers is used to mine databases for association rules When we implemented these algorithms on a SVS cluster and on a parallel com puter so that we could evaluate their performance we found that the Shift algorithm was the most effective when there was a large number of candidate itemsets and processor nodes in the WS cluster environment This is because the ratio of communication time to ex ecution time is large in a WS cluster environment and the communication time therefore has a great influence on the execution time We intend to perform more analysis about the com munication cost between nodes We also intend to per form further experiments with large size data which are used at companies and institutes We also intend to develop a new algorithm to share the loads among nodes because real data are often distributed unevenly 367 


References 141 111 121 31 R Agrawd and R Srikant 223Fast Algorithms for Mining Association Rules,\224 Proc of Int\222l Conf on Very Large Data Bases pp.487-499 1994 R Srikant and R Agrawal 223hslining Generalized Asso ciation Rules,\224 Proc of Int\222l Conf on Very Large Data Bases pp.407-419 1995 N Megiddo and R Srikant 223Discovering Predictive As sociation Rules,\224 Proc ofthe 4th Int\222l Conf on Knowl edge Discovery an Databeses and Data Mining 1998 I51 161 E.H Han G Karypis and V Kuniar 223Scalable par allel data mining for association rules;\224 Proc of ACM SIGMOD Int\222l Conf pp.277-288 1997 T Shintani and M Kitsuregawa 223Iniplenientation of Parallel Mining Association Rules and their Evalua tion,\224 JSPP\22296 pp.97-104 June 1996 L Harada N Akaboshi K Ogihara and R Take 223Par allel Algorithm with Load Balancing for Mining Associ ation Rules,\224 IEZCE Trans on Info and Syst V-ol.J82 D-1 No.1 pp.70-81 January 1999 368 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


