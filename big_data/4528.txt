                   
                
                
               
             
              
                
           


                                                                                                                                         


                                                                                                                                               


V M E 013 Performance Degradation PD g 2  2 P P 013 DCCR Distributed Causal Congestion Relationship A Allocating Accounting for Resos and Charging Resources B Resource Pricing  This simpli\002es our ability to track prices across the cluster when pricing for an entire VME and reduces the amount of communication performed between HAs and the PM By performing price changes on the entire VME we can provide a faster response to reduce congestion rather than repeatedly changing prices per VM All VMEs that belong to the DCCR set with a VM whose performance degradation triggers a price adjustment will have their price increased The amount of the price increase for each VME would depend on these factors listed above as well as on the cluster-wide policy Third in order to be more 003exible in changing the price based on the policy de\002ne we use two policy-speci\002c parameters i per-VME Basis for the duration of the epoch I/O usage is obtained from IBMon which samples each VM's I/O queues to estimate its current I/O demand Controlling I/O usage however is not easily done in SR-IOV environments 1 the VM-device interactions bypass the hypervisor and prevent its direct intervention and 2 SR-IOV IB devices carry out I/O via asynchronous DMA operations directly to/from application memory Software that wraps device calls with additional controls would negate the low-overhead SR-IOV bypass solution The current HA therefore relies on the relatively crude method of CPU capping to indirectly control the I/O allocations available to the VM Our prior work prototyped this method for paravirtualized IB devices W ith DRX we ha v e e xtended it to SR-IO V devices The DRX infrastructure can be used for many purposes including tracking charging and usage control Key to this paper is its use for ensuring isolation for I/O-intensive distributed datacenter applications Speci\002cally isolation must be provided for a set of VMEs co-running on a cluster of machines This requires monitoring for and tracking due to interference by VMEs mathematically shown as follows for a VM that has reduced performance  1 The equation identi\002es the VME or set of VMEs that is affecting the performance of VM  Note that those VMEs also contain VMs that are on the same physical machine as VM  the latter denoted by the function P Knowledge about this potentially resulting in a large set of culprit VMs is the basis on which DRX manages interference The next step will be to identify those VMs in that set that are actually causing the interference being observed followed by mitigation actions that prevent them from doing so The next section explains the techniques and steps used in detail IV DRX R ESOURCE M ANAGEMENT M ECHANISMS We use the concept of Resos described in as a Resource currency for VMs or EMs acting on behalf of entire VMEs in the case of DRX to buy resources for their execution In this section we explain how the components described in the section before interact to implement our DRX mechanisms which use Resos and  which affects how the price increases for a VME i The de\002nes the weight or priority for the VME i and is between 0 and 1 The is the policy coef\002cient for   The PM is responsible for a global resource management of the cluster and it ensures resources are allocated in an appropriate manner to meet the resources of the VMs However to improve scalability for VM resource management the PM allocates a certain number of Resos per EM called EM Allocation Each EM Allocation depends on the set of all resources present in the cluster and on the resource management policy see Section V It also depends on the number of VMs present in each VME in order to avoid a completely unfair distribution of resources Further each EM is responsible for distributing Resos to its VMs For the sake of simplicity we assume EMs distribute Resos to its VMs equally unless we state otherwise is our policies Since we consider only CPU and IB resources for management we assign Resos to VMs only for these resources We use an Epoch and Interval-based model for accounting of Resources where one epoch is equal to 60 seconds and each interval is 1 second A certain number of Resos allows the VM to buy resources from the host Every epoch the EM distributes a new allocation of Resos to its VMs Next every interval the Host Agent deducts Resos from the VM's Resos allocation 226 i.e charges VMs 226 to account for the CPU and I/O consumed by each VM in that interval Any resource allocation that needs to be applied for a VM is performed based on the resource management policies When VMs consume resources they spend Resos allocated to them by their respective EM In order to control the rate at which applications consume resources speci\002cally I/O and to deal with possible congestion where other application's VMs are no longer able to receive their resource share and make adequate progress DRX dynamically changes the resource price with granularity of VMEs By increasing the price of the resource VMs can only afford a limited quantity since they have a limited number of Resos with them According to Congestion Pricing principles 21 this implies that the demand for the resource would reduce which in turn would reduce the congestion of that resource Next we explain the key concepts in the DRX resource pricing methods First the price increase intrinsically depends on the amount of congestion-caused VME VM P\(VM  016 i i j k j k i i i i i i i distributed interference  The formulation below considers reduced performance by some VM f across VMEs caused by their I/O activities Such interference occurs when VMEs share physical links using them in ways that cause one VME's actions to affect the performance of another A more formal statement of distributed interference considers VMs communicating across physical links affected by other VMEs using what we term DCCR of a VM/VME We 002nd the PD for a VM using IBMon to detect changes in the IB usage The PD is the percentage change in the CQEs for RDMA or I/O Bytes for IPoIB or RDMA port counters generated by the VM To maintain a certain SLA the VM needs to generate a required number of CQEs/Bytes When this CQE rate falls below the SLA IBMon detects it and we can report the difference between it and the SLA to the PM Second pricing is performed on a VM f f f 016 


N C 022 016P 016C          100   1     OC A Testbed 013 C f f OP OC A Equal-Blame Policy B Workloads EL EL Pricing Function N P 016P 013 OP 016C   OP 0 016C IO 003 016P 026 s for request completion We use Hadoop's Terasort with a 10GB dataset as a representative for data analytic computing to generate distributed interference For Hadoop workloads we use the job running time as the performance metric Linpack Ns  300 1000 7500 is a characteristic MPI workload for clusters and uses G\003ops as its performance metric We run the workloads in a staggered manner where we start and let the Hadoop job run for 30s before starting the Linpack job Next we start the Nectere workload and run all the jobs till Nectere completes successfully This ensures that the workloads are performing suf\002cient I/O communication before Nectere starts The Hadoop and Linpack workloads are con\002gured to use 32 VMs each and 2 VMs for Nectere In the symmetric con\002guration each physical machine is running 4 VMs of each benchmark Additionally we also use 2 asymmetric con\002gurations 226  In Asymm1 we have more Linpack and Hadoop VMs upto 16 total on the same physical machine as the Nectere VMs Therefore this con\002guration should cause more interference to the Nectere application In Asymm2 there is only one VM each of Linpack and Hadoop along with the Nectere VM This explores the other end of the asymmetry where there is minimal interference Each DRX policy ensures that when Nectere is running it maintains the CQE/s within a SLA limit of 15 we can 003 003 000 000 003 a VME i and is de\002ned for each policy in Section V We also use the Old Price or of the VME to 002nd the New Price Using the factors described above we generally de\002ne our  Old Cap for VM OC  VME Priority   and a cpucap policy co-ef\002cient  which de\002nes the conversion of Price into a CPUCap Generally we de\002ne the new CPU Cap for a VM j belonging to VME i as where N\(DCCR  denotes the number of VMEs in the DCCR set of VME x EL and RL denote the  of Epoch Left on HA h and  of Resos Left for VM j and for VME i and VM j are as follows where N denotes the number of VMEs in the DCCR set of VME x and IO the amount of IO performed by VME i The rest of the formula is the same as the Equal-Blame Policy VI E VALUATION core beta version of the drivers based on OFED 1.5 for the hosts and guests con\002gured to enable 16VFs so we can run upto 16VMs on each host The IB cards are connected via a 36-port Mellanox IS5030 switch Since we do not have explicit control over the RDMA I/O performed by the VMs we use the rather crude methods of CPU capping to reduce the amount of I/O the VM actually performs We have shown in that by throttling CPU we can control the amount of I/O the VM performs Therefore we again use the CPU capping mechanism provided by the hypervisor to control the VM's I/O usage The capping degree depends on the policy being implemented In general the CPUCap for a VM depends on the New Price for the VME i NP This policy is implemented to show a naive method of charging VMEs when there is congestion In this case each VME is charged i.e its price is increased equally for all VMEs responsible for congestion For example if the performance degradation reported by a HA is 30 then with 2 VMEs in the DCCR of congested VME x each VME's price is increased by 15 The goal of this policy is to have a lightweight and simple mechanism by which we can charge VMEs on congestion occurrence We de\002ne the Price and CPU Cap functions for a VME i and VM j as follows In order to improve a naive policy we now consider the amount of I/O generated by a VME in order to increase price Therefore price increases for a VME are directly proportional to the I/O generated by the VME and the performance degradation From the earlier example if the VMEs perform I/O in a ratio of 9:1 the effective price increase for VME1 would be 27 and VME2 3 The PM can 002nd the aggregated I/O from the HAs to compute the I/O ratio between VMEs and therefore adjust prices accordingly The goal of this policy is to charge the VMEs more based on the fraction of performance degradation they caused For this policy Our testbed consists of 8 Relion 1752 Servers Each server consists of dual hexa-core Intel Westere X5650 CPUs HT enabled 40Gbps Mellanox QDR MT26428 ConnectX-2 In\002niBand HCA 1 Gigabit Ethernet and 48GB of RAM Our host OS is RHEL6.3 OS with KVM and the guest OS are running RHEL6.1 Each guest is con\002gured with 1 VCPU pinned to a PCPU 2GB of RAM and an IB VF We use the mlx4 We use three benchmarks each representing a different type of cluster workload Nectere is a serv er clientbased 002nancial transactional workload with low latency characteristics We measure its performance in terms of P D VME i respectively 016P 003 B Hurt-Based Policy i i i x i i i j i i j i j i j i i i j i i i x i i i j i j i j i i i i j i i x j i h j i x h j i i j i i i N k k j i h j i i N P OP OC N P DCCR V P OLICIES FOR A D ISTRIBUTED R ESOURCE E XCHANGE Given the various components and mechanisms of DRX in Section III and IV we now describe various ways in which these components can interact to provide distributed resource management 2 100 as follows Asymm2 C CPU Capping 023 P and  P D  016P  013  N P  013  016C N C OP N RL IO  016C RL 003 Asymm1   OP 016 


Figure 3 shows the impact of the distributed and local policies on workload performance The CC-Dist and PC-Dist policies do help in reducing Nectere latency but not to its SLA level These also have a much greater impact on the performance of Linpack and Hadoop because of the continuous capping The EB-Local and HB-Local policies cannot reduce the latency for Nectere below the SLA because Linpack VMs on other machines are actually causing congestion by sending data to its VMs collocated with Nectere However in the case of the EB-Dist and HB-Dist policies Nectere can meet its SLA of 15 This demonstrates  Figure 4 shows the impact of each policy on the latency of the Nectere application bottom graph It also shows the change in the metric used by DRX to manage I/O performance 226 CQE/s Both the Equal-Blame and Hurt-Based policies   a Equal-Blame Policy b Hurt-Based Policy Fig 5 Comparison of VME Prices CPU Cap and SLA Difference for Hadoop and Linpack VMEs easily con\002gure other values from the base value We design our experiments in order to highlight some of the important features of DRX and their impact on the selected workloads We also use two CPU Capping-only policies denoted as CC and PC In CC CPUCap we decrease the CPU Cap steadily by 5 upto 25 for the interfering VMs In PC Proportional Capping we decrease the CPU Cap for the interfering VMEs based on their I/O Ratio metrics Therefore each VM would have its CPU Cap reduced by a fraction of the 5 based on the I/O Ratio With these additional policies we show the importance of Pricing over only performing CPU Capping We have also con\002gured two sub-types of policies for each of the main policies In one all price changes for a VME apply to its VMs across the entire cluster termed a Distributed policy In the other the price changes for a VM apply only at the HA that is reporting the congestion termed a Local policy Broadly we divide the results into three different categories i policy performance ii policy sensitivity to resources usage patterns and iii limitations and overhead of DRX in different workload con\002gurations which we describe next m Workload CQE/s for 64 KB Nectere Monitor Instance Native Nectere\(64KB Request Latency SLA Diff CPU Cap Price \(Resos/MTU SLA=15 0  10  20  30  40  50  60 Nectere-Latency Hadoop Linpack SLA Diff CPU Cap Price \(Resos/MTU SLA=15 s Nectere Request 2500 3000 3500 4000 4500 0 500 1000 1500 2000 2500 3000 SLA=15                                                                                                                     40 45 50 55 60 65 70 75 80 0 50000 100000 150000 200000 250000 300000 350000 400000 450000 SLA=15 Fig 3 Effect of Policies on Performance of Nectere Hadoop and Linpack Workloads Fig 4 Effect of Policies on the running average of Nectere CQE/s and Latency High CQE/s denotes Low Latency Hadoop VME IBMTU Price Linpack VME IBMTU Price  Linpack CPUCap  Hadoop CPUCap   Workload Performance with Policies and Symmetric Configuration Non-Interfered Server                                                                    Interfered Server                                                                     EB-Dist Policy                                                                     HB-Dist Policy                                                                    20 19 18 17 16 15 14 13 12 1 1.2 1.4 1.6 1.8 2 40 50 60 70 80 90 100 22 20 18 16 14 12 10 8 1 1.5 2 2.5 3 3.5 40 50 60 70 80 90 100 NoMgmt CC-Dist PC-Dist EB-Local EB-Dist HB-Local HB-Dist VME IBMTU Price CPUCap   the feasibility of resource pricing as a vehicle to reduce congestion as well as the importance of performing distributed resource management actions as enabled by DRX C Policy Performance Performance Degradation 


Essentially EB and HB policies serve two respective methods for SLA satisfaction 226 1 In order to evaluate the effect of resource usage patterns on the effectiveness of DRX we use two more workload con\002gurations In one con\002guration we use an instance of Nectere along with 2 Linpack instances In the second we use an instance of Nectere along with the Hadoop MRBench application and a smaller data size for Linpack We observe from Figure 6 that when the interfering workloads perform similar amounts of I/O both EB and HB policies behave equally well however since EB treats both VMEs similarly at all times and applies the same cap simultaneously it achieves a lower latency for Nectere In the adjacent graph HB becomes more aggressive than EB and it caps both Linpack and MRBench much more This is because as HB performs the capping it leads to oscillations in which VME domanates the I/O Ratio  We show in Figure 7 that for two different workload con\002gurations the DRX policies affect them differently When there is a lot of interference in the Asymm1 con\002guration none of the policies can satisfy the Nectere SLA This is because despite CPU capping the VMs still generate suf\002cient I/O to cause congestion for Nectere In this case having more support from the hardware to control I/O would be very useful In the Asymm2 case where Nectere has minimal interference DRX ensures that other workloads are perturbed much less or not at all Here both Linpack and Hadoop perform very close to their baseline values These results highlight  which forces HB to perform large amounts of cap alternately on the VMEs This is not evident in Figure 3 as the difference in the I/O Ratio between Hadoop and Linpack is smaller Therefore we 002nd that HB   swings in the I/O Ratio while EB is less sensitive to differences in the generated I/O E Limitations and Overhead of DRX fast-acting while not performing graceful degradation of workloads 2 slow-acting while providing graceful degradation to other workloads Future policies will be extended with mechanisms to detect such oscillations and to further limit their aggressivness under such circumstances Performance Degradation Workload Workload Performance with Policies and Asymmetric Configuration 95 Nectere HPL1 HPL2 Nectere MRBench HPL 0  10  20  30  40  50  60  70  80  90 Nectere-Latency Hadoop Linpack  Distributed Rate Limiting for Networks Performance Degradation Workload multiple policies can be constructed and con\002gured to meet the SLA values for applications D DRX Sensitivity to Resources Fig 6 Comparison of DRX Policy Sensitivity with two different workloads sizes are effective in reducing contention effects and providing performance within the guaranteed SLA levels 226 they reach the same value for latency though their impact on the interfering workload performance is different Also the HB-Dist policy provides the least degradation of Hadoop and Linpack workloads while meeting the SLA for Nectere The EB-Dist policy degrades the workloads more since it increases the prices equally for both VMEs which negatively impacts how fast the CPUCap is reduced As a result the EB-Dist policy is more reactive or Fig 7 Performance of Policies Local and Distributed with Asymmetric Workload Con\002guration Asymm1 and Asymm2 refer to the types of workload deployment to SLA violations as highlighted by the CPU Cap reductions versus price increases shown in Figure 5a EB penalizes interfering VMs more than HB and assesses a lower CPU Cap for Hadoop Linpack at 60 Figure 5b shows the more VII R ELATED W ORK In this section we brie\003y discuss prior research related to DRX is more sensitive to large   EB-Local-Asymm1 Policy EB-Dist-Asymm1 Policy HB-Local-Asymm1 Policy HB-Dist-Asymm1 Policy NoMgmt-Asymm1 EB-Local-Asymm2 Policy EB-Dist-Asymm2 Policy HB-Local-Asymm2 Policy HB-Dist-Asymm2 Policy NoMgmt-Asymm2 EB-Dist HB-Dist  the limited utility of CPU Capping in extreme interference and also the low overhead caused by DRX components and their management actions DRX Sensitivity to Resource Usage Patterns 0  20  40  60  80  100            fast-acting nature of the HB-Dist policy where the CPUCap of the interfering workloads is decreased much more gradually than EB-Dist HB-Dist allocates a higher CPU Cap to Hadoop 71 and lower CPU Cap to Linpack 50 since these are based on the I/O Ratio between the VMEs For both these policies the PM always responds to a SLA violation messages within 5ms therefore This result highlights an important aspect of DRX Many recent efforts have explored distributed control for providing network guarantees for cloud-based workloads These have looked at providing min-max fairness to workloads pro viding minimum bandwidth guarantees or using congestion noti\002cations from switches 20 pro vides a detailed surv e y of these approaches There are also other efforts that provide network guarantees for per-tenant and inter tenant communication Authors in Gatek eeper 27 enforce limits per tenant per physical machine by providing exact egress and ingress bandwidth values In DRX by providing prices and setting CPU Cap limits we similarly enforce network limits per tenant per physical machine These approaches show that providing distributed control for networks is becoming important for cloud systems HowDRX always detects and acts upon congestion in a timely manner slow-acting 


Proc of MiddleWare IPDPS Proceedings of VEE ACM SIGOPS Workshop ACM SIGCOMM Economics and Resource Management HPCVirtualization Workshop EuroSys NSDI Ottawa Linux Symposium ever while these approaches may work well for Ethernetbased para-virtualized networks they do not yet explore high performance devices like In\002niBand or SR-IOV devices DRX borrows some ideas like minimum guarantees and tenant fairness VM ensembles are similar to tenants from these efforts to show that distributed control for networks is still required and feasible for hardware-based virtualized networks VEE USENIX ATC Proceedings of IEEE Cloud DRX also relies on the effects of Congestion Pricing on resource usage and allocation These ideas have been explored before in network congestion avoidance 21 platform ener gy management as well as in m ark et-based strate gies to allocate resources Ho we v er  to our kno wledge ours is the 002rst to combine congestion pricing to provide a distributed control over In\002niBand network usage VIII C ONCLUSIONS AND F UTURE W ORK This paper addresses the unresolved problem of crossapplication interference for distributed applications running on virtualized settings This problem occurs not only with software-virtualized networking but also with newer high performance fabrics that use hardware-virtualization techniques like SR-IOV which grants VMs direct access to the network As a result this removes the hypervisor from the communication path as well as the control over how VMs use the fabric The performance degradation from co-running set of VMs is particularly acute for low latency applications used in computational 002nance In this paper we describe our approach called Distributed Resource Exchange or DRX which offers hypervisor-level methods to mitigate such inter-application interference in SRIOV-based cluster systems We monitor VM Ensembles 226 a set of VMs part of a distributed application 226 which enables controls that apportion interconnect bandwidth across different VMEs by implementing diverse cluster-wide policies Two policies are implemented in DRX to assign Equal Blame to interfering VMEs or to look at how much Hurt they are causing and therefore showing the feasibility of such distributed controls The results demonstrate that DRX is able to maintain SLA for low-latency codes to within 15 of the baseline by controlling collocated data-analytic and parallel workloads Limitations of the DRX approach are primarily due to its current method to mitigate interference which is to cap the VMs that over-use the interconnect and cause hurt Our future work therefore will consider utilizing congestion control mechanisms present on current In\002niBand hardware to mitigate the sending rate of certain QPs in order to remove our reliance on CPU Capping R EFERENCES  D Abramson et al Intel V irtualization T echnology for Directed I/O Intel Technology Journal Proceedings of WIOV  10\(3 2006  AMD I/O V irtualization T echnology http://tin yurl.com/a6wsdwe  H Ballani K Jhang T  Karagiannis and C K et al Chatty T enants and the Cloud Network Sharing Problem In  2013  M Ben-Y ehuda J Mason O Krie ger  J Xenidis L V  Dorn A Mallick J Nakajima and E Wahlig Utilizing IOMMUs for Virtualization in Linux and Xen In  2006  B Briscoe and M Sridharan Netw ork Performance Isolation in Data Centres using Congestion Exposure ConEx 2012 http://datatracker.ietf.org/doc/draft-briscoe-conex-data-centre  L Cherkaso v a and R Gardner  Measuring CPU Ov erhead for I/O Processing in the Xen Virtual MachineMonitor In  2005  Y  Dong Z Y u and G Rose SR-IO V Netw orking in Xen Architecture Design and Implementation In  2008  S Go vindan A R Nath A Das B Ur g aonkar  and A Si v asubramaniam Xen and co Communication-Aware CPU Scheduling for Consolidated Xen-based Hosting Platforms In  2007  C Guo G Lu H J W ang and S Y  et al SecondNet A Data Center Network Virtualization Architecture with Bandwidth Guarantees In  2010  D Gupta L Cherkaso v a R Gardner  and A V ahdat Enforcing Performance Isolation Across Virtual Machines in Xen In  2006  V  Gupta A Ranadi v e A Ga vrilo vska and K Schw an Benchmarking Next Generation Hardware Platforms An Experimental Approach In  2012  Apache Hadoop http://hadoop.apache.or g  High Performance Linpack http://www netlib or g/benchmark/hpl  P  K e y  D Mcaule y  P  Barham and K Lae v ens Congestion Pricing for Congestion Avoidance Technical report Microsoft Research 1999  K Lai L Rasmusson E Adar  S Sorkin L Zhang and B A Huberman Tycoon an Implemention of a Distributed Market-Based Resource Allocation System Technical report HP Labs Palo Alto CA USA 2004  T  Lam S Radhakrishnan A V ahdat and G V ar ghese NetShare Virtualizing Data Center Networks across Services Technical Report CS2010-0957 University of California San Diego 2010  M Lee A S Krishnakumar  P  Krishnan N Singh and S Y ajnik Supporting Soft Real-Time Tasks in the Xen Hypervisor In  2010  J Liu Ev aluating Standard-Based Self-V irtualizing De vices A Per formance Study on 10 GbE NICs with SR-IOV Support In  2010  A Menon J R Santos Y  T urner  G J Janakiraman and W Zwaenepoel Diagnosing Performance Overheads in the Xen Virtual Machine Environment In  2005  J Mogul and L Popa What W e T alk About When W e T al k About Cloud Network Performance  2012  R Neugebauer and D McAule y  Congestion Prices as Feedback Signals An Approach to QoS Management In  2000  L Popa G K umar  M Cho wdhury  and A K et al F airCloud Sharing the Network in Cloud Computing In  2012  X Pu L Liu Y  Mei S Si v athanu Y  K oh and C Pu Understanding Performance Interference of I/O Workload in Virtualized Cloud Environment In  2010  A Ranadi v e A Ga vrilo vska and K Schw an IBMon Monitoring VMM-Bypass In\002niBand Devices using Memory Introspection In  2009  A Ranadi v e A Ga vrilo vska and K Schw an ResourceExchange Latency-Aware Scheduling in Virtualized Environments with High Performance Fabrics In  2011  A Ranadi v e M K esa v an A Ga vrilo vska and K Schw an Performance Implications of Virtualizing Multicore Cluster Machines In  2008  P  V  Soares J R Santos N T oli a and D Guedes Gatek eeper Distributed Rate Control for Virtualized Datacenters Technical Report HPL-2010-151 HP Labs 2010  InterContinental Exchange http://www theice.com  C W ang I A Rayan G Eisenhauer  K Schw an and et al VScope Middleware for Troubleshooting Time-Sensitive Data Center Applications In  2012  H Zeng C S Ellis A R Lebeck and A V ahdat Currentc y A Unifying Abstraction for Expressing Energy Management Policies In  2003 ACM CCR Proceedings of IEEE Cluster MiddleWare VEE Proceedings of ACM CoNext Proceedings of SHAW HPCVirtualization Workshop Eurosys Proceedings of the USENIX Annual Technical Conference 


a Outside View b Inside View Figure 13  Mock cave test site Tunnel extends approximately 300m Cave 003oor is covered in rocky material to emulate planetary terrain Site contains surface terrain and tunnel inside building Exploration Performance Experiments A comparison of exploration performance between distributed centralized and uncoordinated task allocation was conducted using a 2-robot team of 2D mapping robots For the uncoordinated runs tasks were randomly assigned to a robot and the robot randomly decided whether to keep the task not taking into account any costs associated with that robot's performance of the task Maps at a resolution of 0.05 meters per pixel were built from 5 runs of each type Each run lasted 15 minutes The operator indicated tasks that should be performed and the system assigned these tasks to a robot In 3 out of 5 runs for each set the 002rst selected task was in the direction of the bridge in the other 2 it was in the direction of the dead-end to the right of the starting position in Figure 12 The same operator selected tasks for all runs Tables 1 2 and 3 show results for each run Percent explored is the percentage of the explorable area as determined from the ground truth model that the robot team explored Unique to total is the ratio of the area explored by a single robot to the total area explored by the team This metric gives a sense of how much overlapping work the robots are doing The average percent explored for runs with a 002rst task in the direction of the bridge was 68 and for runs with the 002rst task in the direction of the dead-end 67 The average ratio of unique to total explored for these cases was 0.45 and 0.44 respectively The average over all runs was 67 of explorable area covered and a ratio of 0.45 unique to total explored area Figure 14 shows merged maps for the runs with the largest and smallest explored areas in this experiment Table 1  2D Mapping Results Distributed Run Bridge 1st  Explored Unique:Total 1 1 80 0.57 2 1 52 0.43 3 1 94 0.45 4 0 97 0.76 5 0 60 0.37 Mean 77 0.52 Std Dev 20 0.15 Table 2  2D Mapping Results Centralized Run Bridge 1st  Explored Unique:Total 1 1 75 0.58 2 0 52 0.46 3 1 55 0.29 4 0 49 0.15 5 1 62 0.59 Mean 59 0.41 Std Dev 10 0.19 Table 3  2D Mapping Results Uncoordinated Run Bridge 1st  Explored Unique:Total 1 1 51 0.20 2 0 54 0.26 3 1 73 0.49 4 0 87 0.65 5 1 68 0.42 Mean 67 0.41 Std Dev 15 0.18 These results show high performance variation within run sets and do not show signi\002cant difference between sets The direction of the 002rst assigned task did not signi\002cantly affect results Limited navigation and path planning capabilities on individual robots common for all runs likely introduces signi\002cant randomness If the robots could more reliably complete their assigned tasks differences between task allocation strategies would likely become more evident Exploration of larger areas could also make differences clearer as more tasks would need to be assigned The lack of signi\002cant differences between allocation methods is somewhat encouraging however It indicates that distributed task allocation the method believed to be most promising for planetary missions does not perform any worse than other methods in early tests The uncoordinated method while by far the simplest would fail once robots with different capabilities are introduced Failure would occur for example if a 3D modeling task were assigned to a 2D mapping robot Mapping and Modeling Experiments An experiment including both 2D mapping and 3D modeling was conducted at the patio test site In this experiment the two 2D mapping robots were operated as described in section 6 A mapped area was then selected by the operator for 3D modeling and the 3D modeling robot was sent to complete that task There was no time limit on the run Figure 15 shows 9 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


