Interpolation Results in the Split Inversion Neural Network Algorithm Alastair D McAulay and Ramesh Ravula Wright State University Department of Computer Science and Engineering Dayton Ohio 45435 ABSTRACT An advantage of a neural network over a look-up table is the ability to act as a higher form of associative memory For example, a network in addition to giving correct results for the pattern 
examples that are used to train the network can also perform sophisticated interpolation In this paper neural networks trained with the split inversion neural net work algorithm are shown to provide complex interpolation when used as associative memories The neural network is applied to the design of a beam-truss-spring structure The results obtained using the neural network algorithm are shown to be superior to those of a look-up table INTRODUCTION obtained with 
the neural network algorithm and the look up table are discussed compared and the superiority of neural networks illustrated SPLIT INVERSION LEARNING ALGORITHM The concept for the split inversion algorithm was ex plained in reference a The considerable speed advantage of this algorithm over the backpropagation algorithm was described by the author in reference 3 The split inversion algorithm in contrast to the backpropagation algorithm 4 computes 
the weights for both the output and the hidden layers so as to minimize the square error at the output of the network 5 A least square solution for a weight update-increment vector Aw may be computed for all weights at the same time using conjugate gradients 5 from AAw  d 1 where A is computed from derivatives of the outputs 
with respect to the weights and d is an output error vector The weights are updated at the I th iteration Computer aided engineering design tools are good for analyzing a hypothesised design but are of little help in cre ating the design in the first place Expert systems could be applied to the design task but these are proving to be wiuij\(l 1  Wij\(1  
Awtj\(1 2 APPLICATION TO BEAM-TRUSS-SPRING DESIGN cumbersome This is because whenever an expert increases his expertise new rules must be added to the existing sys tem that do not conflict with them Addition of more rules makes the system slower and less efficient Neural networks have advantages for engineering design The concept of associative memory which is an integral part of human expertise is inherent in neural 
networks l As sociative memories unlike conventional memories can asso ciate a new situation with previous ones The neural net work provides a higher form of associative memory than a look-up table because of its ability to provide complex interpolation It will be shown in this paper that the split inversion neural network algorithm can exhibit this higher form of associative memory when applied to the design of engineer ing systems An example from structural design 
a beam truss-spring is used to illustrate this property The results A beam-truss-spring is considered from structural engi neering figure 1 It is simple to compute the deflections U under a loading p for a specific structure for which the lengths of the members I and their stiffnesses s are known Figure 1 Structural beam-truss-spring design example 695 CH2759-9/89/0000-0695 1 OO  1989 lEEE 


0 0 OI 1251i11 12s1 6s,f 4s,e2 6s1e zS1e2 0 0 0 12\(s,+sJ 6s,-12s2 12S2 12s2P 0 1 6 e3 2s412  1 2s2 1 zS2 I 2s4 e2 Symmetric Figure 2 Stiffness matrix for design example This is an analysis problem involving solving the linear equations for deflections U given a design B and a loading P The inverse or design problem of determining the lengths of the members 1 and/or their stiffnesses s for a given load and deflection is a very much harder problem. In general the solution is not unique and may not be computable The rea son is that the design parameters are distributed throughout the equations and may arise in a complicated manner e.g length arises with powers in B figure 2 The uniqueness issue is bypassed by fixing certain parameters B\(1,s  p 3 Input Load IP to P lection U to U A neural network may be constructed and trained to perform the inversion or design problem The sequence of actions taken to train the neural network are as follows 1 Compute the matrix B in equation 3 figure 2 using a selected stiffness vector s and length 1 2 Use equation 3 with load vector p to compute cor responding deflection vector U 3 Repeat 1 and 2 with other design parameters s and I and with other load vectors p 4 Associated vectors are determined where load and de flection vectors are concatenated to form an input vec tor and the corresponding stiffness vector and length form the output vector 5 The network is trained using the split inversion algo rithm for these sets of input and output vectors The network will now provide at the output a length and stiffness vector given a deflection vector and load vector at the input Further the deflection vector and load vector need not be one of those used for training The network will interpolate to handle those that fall between training cases output length 8 stiffness Figure 3 Neural network for structural example 696 


linear  non-linear 3.2 2.8 2.6 2.4 2.2 Linear interpolation 1.8 0.8 0.6 0.4 0.2 1 0,1,1111111111 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5 7 rms of deflection Figure 4 Illustrates interpolation for neural network SAMPLE RESULT SHOWING INTERPOLATION The neural network used to accomodate the correct num ber of inputs and outputs for the beam-truss-spring in fig ure 1 is shown in figure 3 The network is trained with four design examples In the recall phase if the network is given any input pattern that it has been trained with the net work responds with the corresponding output pattern The question addressed next is the behavior when an input is provided that was not one used in training In order to demonstrate interpolation the stiffness of the structure and the load were kept constant and the length 1 is the only design parameter to be estimated Let us consider two input training patterns say pattern two and pattern four, named because they correspond to a design length of two and four respectively The rms values of the deflection vectors corresponding to patterns two and four are plot ted against their corresponding design lengths as the dark squares at the extremeties of the lines in figure 4 Now assume that we wish to estimate a design length for a deflection that is midway between that for pattern two and that for pattern four We consider every element of the deflection vector to be midway between those for the two pat terns If a look-up table is used, a length midway between that for pattern two and pattern four would be selected that is three This is shown as a linear interpolation by the dark square labelled linear interpolation in figure 4 It is obvious from the beam-truss-spring design equations 3 and the matrix figure 2 that the deflection vector is not linearly related to the length so that this solution is not likely to be very good If the midway deflection vector is input to the trained neural network a different design length is obtained la belled neural network output and marked with an open box in figure 4 This can be shown to be superior to the lin ear result as follows The design length obtained is used to determine the matrix B figure 2 Equation 3 is used to compute a deflection vector The rms of this deflection vector is computed and plotted as the actual value in fig ure 4 The result is close to the nonlinear interpolated re sult obtained from the neural network This shows that the network produces a better interpolation than that obtained from the linear interpolation with the look-up table In practice it is always possible to determine the accu racy of the interpolation by using the interpolated design with analysis tools to see how well the resulting design meets its specification CONCLUSIONS This paper showed that the split inversion neural net work algorithm can provide accurate interpolation for non linear situations that would be difficult to obtain with a look-up table The nonlinear relations need never be known The network learns these from training with representative examples The discretization of the parameters used in ob taining training samples and the nature of the underlying equations determine the accuracy of the interpolation This approach was shown to be valuable for engineering design where expertise is often dependent on association with pre vious designs rather than by following a set of rules REFERENCES 1 2 3 4 5 McAulay A.D 223Neural networks as a new strategy for computing\224, NAECON May 1988 McAulay A.D 224Engineering Design Using Split In version Learning\224 IEEE First Annual International Conference on Neural Networks June 1987 McAulay A.D 224Optical Neural Network for Engi neering Design\224 NAECON May 1988 Rumelhart D.E., Hinton G.E and Williams R.J 224Learn ing Internal Representations by Error Propagation\224 in Parallel Distributed Processing Explorations in the Microstructure of Cognition The MIT Press, Cam bridge 1986 McAulay A.D 224Conjugate Gradients on Optical Cross bar Interconnected Multiprocessor\224 Journal of Paral lel and Distributed Processing Feb 1989 697 


 0 0.2 0.4 0.6 0.8 1 1.2 0 100 200 300 400 500 600 domain labels confidence Series1 Figure  Example f patterned data tribution and the e\002ect on the run time of the algorithm We also consider the behavior of the algorithm on real world data selected from a census database prediction of marital status from income and from forestry data prediction f round cover type from elevation For the patterned synthetic data and for the real rld data sets we 223nd that he dramatic improvement observed on unpatterned random data is not typical and that the behavior of the RS algorithm on the real world data more closely matches our patterned data model for which quadratic runtime dominates We conclude that such adverse data is not rare 1 Random patterned data Our synthetic data was generated according to the following method Each bucket received support as in W e assumed ho w e v e r that con\223dence w ould be a function of he numeric attribute value i.e the bucket number so that low con\223dence buckets would tend to cluster as uld high con\223dence buckets We generated a simple triangular wave-form with rying numbers of peaks and lleys A typical peak had con\223dence values rising from 2 to 8 and then falling back to 2 Call this multipeak piecewise linear function function f  We randomly generated the con\223dence for bucket i to be a binomially distributed ratio with mean f  i  and N  20 trials Figure 1 shows a typical example data set enerated in this manner We do not mean to suggest that this synthetic model is 215the right\216 one indeed we do not believe such a thing exists The point is to consider how the lgorithm performs when parameters such as the size and frequency of peaks change empirically validating the 1 o pecial e\001ort was made to 223nd hese data sets hey were selected due to their availability their size and the presence of a quantitative attribute with many possible values that was likely o be predictive of an associated categorical attribute 0 0.2 0.4 0.6 0.8 1 1.2 2500 3000 3500 4000 4500 5000 domain labels - income confidence s ingle census data Figure  Census data tradeo\002 suggested between the two terms in the complexity bounds for the divide and conquer algorithm We generated data with n  2000 4000 8000 and 16000 buckets distinct domain lues for the quantitative attribute This was done in two ways r 215\223xed peaks\216 data the number of peaks for these datasets were respectively n 50 n 100 n 200 and n 400 thus keeping the number of peaks constant t 40 and hence the peak width increasing from 50 to 400 For 215\223xed peak width\216 data the number of peaks was set at n 200 for each data set so that the peak width was 223xed at 200 and the number of peaks varied from 0 to 80 Because each valley was likely o contain at least one partition point and the neighborhood around each peak was unlikely to do so this e\002ectively allowed control of the parameters b max here the peak width and m here the number of peaks in the RS algorithm Real-world data In addition to the patterned synthetic data we extracted a real world data set from the 1999 census data for the Los Angeles/Long Beach area The total-familyincome and marital-status attributes were projected from a database f 88443 households with positive income levels The marital-status ttribute s summarized by a boolean attribute whose lues were 0 for a married head of household and 1 otherwise Finally the data was bucketed ccording to the total-familyincome so that one record existed for every value in the income domain A record consisted of an income level together with a tally of the number of households with that income nd a tally of the number f unmarried heads f household The largest 223nal data set consisted of 11990 records This same procedure was followed n smaller census data sets to achieve smaller domains for our tests Figure 2 shows a small portion of the census data set As a simple test point for scienti\223c data in this conProceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


 0 0.2 0.4 0.6 0.8 1 1.2 0 100000 200000 300000 400000 500000 600000 domain labels \(n=1978 elevations confidence l odgepole pine forestry data Figure 3 Forestry data text we tested the RS algorithm on the forestry data available from the UCI KDD archive at http://kdd.ics.uci.edu We used a similar procedure to that of the census data to extract the elevation and forest cover type for 581,012 soil samples The tion and cover type attributes were projected from the set f bservations The cover type attribute s condensed into a single lean attribute whose lue was 1 if the forest cover was 215lodgepole pine\216 and 0 otherwise Finally the data was bucketed according to unique elevations Each record in the 223nal data set consisted of an elevation followed by a tally of the number of observations that were taken at that elevation and another tally of the number of samples at that elevation classi\223ed as cover type 215lodgepole pine\216 The 223nal data set had only 1978 elements tions Figure 3 shows the entire forestry data set 3.3 Results of RS lgorithm on patterned and census data We ran 2 the RS algorithm on each patterned data set for k  5 intervals We 223x the value of k sincewe are most interested in characteristics of the data and how hose characteristics 002ect running time r each setting of the parameters each experiment was run 30 times and the average run times were recorded s expected because there were stretches of correlated con\223dence within each peak there were sequences of buck2 The experiments presented in this paper were implemented in Perl and performed on an iBook laptop The goal of our investigation was not to determine the maximum problem sizes that can be handled by the algorithm based on the limits of current technology but rather to see how well the algorithm scales with various parameters Our esults are o\001 by a 223xed constant factor when compared to implementations with faster hardware and software Our run times are consistently 50 times slower than that of the implementation used by Rastogi  Shim      0 50 100 150 200 250 300 350 400 450 0 4000 8000 12000 16000 domain size \(n run t ime  s  fixed peak width n/200  fixed peaks \(40  uniform random data  census data  forestry data Figure  ize s run ime minconf  65 k=5 ets that neither merged together in the initial preprocessing phase nor resulted in a partition point Figure 4 gives the running time for 223nding 5 intervals as a function of the domain size We expect the most salient parameter to be the peak width  b max  As discussed earlier this lue is a small constant for the uniform random data so run time scales linearly with a small slope as the amount of data increases For the 223xed peak-width data  n 200 we also have a linear increase in running time but with a larger coe\001cient as b max is likely to be near 200 However when the number of peaks is 223xed the number of buckets b max within each peak grows with he amount of data and the algorithm exhibits its characteristic quadratic increase in running time Also shown are the running times for the census datasets Our admittedly subjective evaluation is that this data behaves more like 223xed-peak data hence exhibits quadratic running time Also striking is the amount of time required for 223nding he best 5 intervals on the fewer than 2000 records of the forestry data set Referring back to Figure 3 we see that the data has a single peak nd it appears that not much can be gained by partitioning the peak into 223ve interls so as to exclude some small amount of 215uncon\223dent\216 data We suspect that the algorithm spends a lot of time 223ghting the law of diminishing returns This view is supported by our sampling results in the next section where most of the gain in support can be obtained by king at relatively few records Another view is given in Figure 5 where for a 223xed data size runtime is plotted against the number of peaks in the synthetic data set The census and uniform random lines are plotted for comparison and do not vary s the number of peaks increases for a 223xed data size the number of domain lues within a peak decreases  b max  and the running time decreases quadratically consistent with the complexity bounds Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


   0 10 20 30 40 50 60 70 80 90 0 50 100 number of peaks run t ime  s  synthetic peaked data  uniform random data, unknown # of peaks  census data unknown # of peaks Figure  Shape s run time n=4000 minconf=0.65 k=5    0.00 100.00 200.00 300.00 400.00 500.00 600.00 700.00 0.5 0.6 0.7 0.8 0.9 1 minconf runtime  s  census data n=3888 uniform random data  peaked data, 20 peaks Figure 6 Minconf vs run time n=4000 k=3 of the RS algorithm Finally Figure 6 shows the e\002ects of varying the demanded minimum con\223dence while holding the other parameters 223xed Rastogi  Shim noted that as minconf approached the mean con\223dence 5 for their uniform random data the runtime increased dramatically This phenomenon holds s well for our synthetic patterned data as well as for the census data which had mean 63 explaining why the curve is shifted It seems reasonable that for any data set there would be many possible feasible intervals with con\223dence close to that of the mean so setting minconf near the mean is inviting an algorithm to consider perhaps far more possibilities than is practical with perhaps only nominal gain in support 4 Sampling We consider sampling as an alternative means for e\001ciently handling data with large domains and for which the divide and conquer algorithm o\002ers no signi\223cant speed up While a worst-case quadratic or any polynomial time algorithm is theoretically acceptable in data mining pplications it is often impractical to implement an algorithm which requires more than a 223xed number one say of scans of the data Absent a linear time exact algorithm we turn to sampling to reduce the problem size and when we run the S lgorithm on a sample-driven coarsening of the data thereby reduce the computation time Our data summarization technique is similar to that employed by Fukuda et al Let D denote the original data  D   n andlet B denote a coarsening of D  Create B as follows 200 Select a random sample of buckets S from D accordingtosupport  S   s  This can be done in linear time using the reservoir sampling of  200 If sample is unsorted sort ccording to the numerical attribute of interest in time O  s log s  200 Partition the original data into a new set of buckets B  whose bucket boundaries are hose in S  Thus B consists of the buckets in S  together with a new bucket for every interval between sampled buckets  B 006 2 s 1.Thisrequirestime O  n log s  if the data is unsorted O  n  otherwise The burning question is how many examples do we need to assure that the support of an optimal solution on our sample is likely o deviate only slightly from the support of an optimal solution on the whole data set We choose a dense enough sample so that with high probability any l containing more than some small user speci\223ed mount f data is likely to be sampled nd thus when the actual data is compiled into the new buckets no interval\220s support deviates from its original support by more than this small amount Then this new set f bucketed data is used as input to algorithm RS The choice of parameters gives the user a tradeo\002 between the error hat she is willing to tolerate and the amount of time required by the RS algorithm on the sample De\336nition 1 An 002 signi\223cant interval I  is n interval with sup I  002  We se a sample f su\001cient size from D to assure with high probability that a bucket is sampled for every 002 signi\223cant interval in D Insodoing,weassure whp that the buckets representing the accumulation of data between sampled buckets have weight no more than 002  Call he resulting coarsened data B an 002 coarsening of D  Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


Lemma 2 Given an independent uniform sample S of size s from a relation R containing n rows if s 007 max 4  002 log 2 003  16 002 log 13 002  then Pr  b an 002 signi\223cant interval I with I 004 S  t   003  Proof llows from the fact that the VC-dimension of the class of intervals is 2 and the su\001cient sample size unds given by Blumer et l Consider any interval I on the original data D and notice that the support of I canbeestimatedonthe bucketed data B by selecting the largest l I 001  I 001 004 I  so that the endpoints of interval I 001 fall on bucket boundaries Then sup I  212 sup I 001   2 002 since the 002 error can occur at each of he 2 endpoints ow let I  005 I k be a k interval and let I 001  005 I 001 k where I 001 k 004 I k  are the intervals of largest support whose endpoints fall on sampled bucket boundaries as in the single interval case Then because there are k intervals sup I  212 sup I 001   2 k\002 Thuswehave Lemma 3 Given a data set D and B an 001 2 k coarsening of D  for any k interval I on D and I 001 on B with I 001 argmax i 002 I sup i   sup I  212 sup I 001  002  We have assured that with probability more than 1 212 003  the support of any k interval can be computed on our coarsened data set with error no more han 002  That is for association rule F  A 002 I 001 C sup A 002 I  can be approximated by some interval I 001 on the set of buckets B so that sup A 002 I  212 sup A 002 I 001  002  Notice that the argument holds for the condition A 002 I 003 C as well That is there exists an interval I 001 002 B such that sup A 002 I 003 C  212 sup A 002 I 001 003 C   2 002 Thisobservation aids in analyzing the error in estimating con\223dence Finally we are in a position to bound the error incurred by running the RS algorithm on our coarsened data set B  The optimal solution to the problem is a measure of support There are two types of error in support we can incur First we may overlook small intervals We have bounded the magnitude f this type of error by 002  Second algorithm RS searches among intervals exceeding the minimum con\223dence threshold for the optimal solution Danger lies in the case we fail to consider some su\001ciently con\223dent interval because we underestimate the con\223dence of that interval on the bucketed data If an interval is actually con\223dent and we cannot detect it we are at risk of tossing out large chunks of support Instead of quantifying this neglected support we create a new slightly lower con\223dence bound that these de\223cient intervals will almost certainly meet and so they will be considered by the algorithm for inclusion in the optimal solution to the problem In the next lemma we show that any con\223dent interval on the data set D can be pproximated by an interval of slightly lower con\223dence on the coarsened data set B  ence the theoretical results show that for a suitably smaller con\223dence threshold which necessarily depends on he optimal support with high probability only the 223rst kind of error where small intervals re overlooked can occur Lemma 4 For any k interval I on D let k interval I 001 004 I be the largest subinterval of I on B  Suppose the minimum con\223dence threshold is 001  and that sup I  212 sup I 001  002  Then with probability greater than 1 212 003  conf I  007 001 001 conf I 001  007 001 212 002 1 212 001  sup I 001   The proof which is largely lgebraic is ilable from the authors Here\220s an intuitive explanation The fact that the original region meets minimum con\223dence is an indication that the average con\223dence over the region is no less than the threshold Since the sampled portion f the region is de\223cient the unsampled region must have excess con\223dence How de\223cient can the con\223dence of the sample be At most the unsampled region has 1 212 001  002 excess where the 1 212 001 risesbecause we are measuring excess above 001  This excess in terms of average con\223dence for the sample must be normalized by the support of the sample In e\002ect we are redistributing the excess across a region of size sup I 001  The previous lemmas combine to give the following bounds demonstrating that small error  002  in total support can be achieved on a sample if a suitably smaller value of con\223dence depending on the support is chosen Our empirical results demonstrate however that this reduction in the minconf 001 is not necessary in practice Theorem 5 Let RS  D 001  denote the maximum support k interval exceeding on\223dence 001 on data set D Let B   b 1 b 2 b m  be an 001 2 k coarsening of D  Then sup RS  D 001  212 sup RS  B 001 212 002 1 212 001  sup RS  D 001   002 These nalytic results imply a reasonable practical approach to sampling with the only complication arising from the fact that the error in con\223dence depends on the support of the interval of interest The sampling bounds are independent of the size of the domain and thus tradeo\002s are simply between sample size and accuracy In the next section we demonstrate that convergence to optimal solutions occur surprisingly quickly across di\002erent data sets The bounds we obtained are signi\223cantly di\002erent than those given by Toivonen and Zaki e t a l 11 for example In both of these papers a goal is to estimate the support of an itemset so as to determine with high probability whether or not it quali\223es as 215frequent\216 by meeting a minimum support threshold n this context dependence n the error parameter 002 is quadratic that is there is a factor of 1 001 2  whereas our bound improves this by relying nly linearly on 1 001 The Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


key di\002erence is that we are using sampling not to obtain uniformly good estimates f the support f intervals but rather to select admissible endpoints of intervals The actual support f these intervals re computed exactly by a linear scan f the original data So the error induced by sampling is not from inaccurate estimation of support but rather solely from the necessity of representing an arbitrary interval using only 215admissible\216 intervals with endpoints in the sample RS Algorithm run on sampled data Our sampling experiments were conducted as follows for a data set D  and for sample size s and for 30 repetitions a sample of s buckets was randomly selected from D and the optimized rt set was found on the sample The maximum support discovered among the 30 runs is compared to optimal We performed the following experiments using our synthetic nd real rld data models 200 223xed peak width  n 200 varied data size n 002  2000  4000  8000  16000   200 223xed data size  n  4000 varied number of peaks p 002 n 400 n 200 n 100 n 50   200 223xed number of peaks  p  40 varied data size n 002 2000  4000  8000  16000   200 Census varied n 002 1836  3888  6661  11990   200 Forestry 223xed n  1978 All of our experiments demonstrate that convergence to the optimal support is quite rapid for all data sizes and independent of the variability in our synthetic data The results are remarkably consistent with more than 95 convergence occurring for all sample sizes larger than 500 The same results are observed for the census data as well as for the forest cover data We show here a typical convergence graph from our experiments  Real Data 0.90 0.92 0.94 0.96 0.98 1.00 1.02 0 500 1000 1500 2000 sample size fraction of optimal 1836 records, census 3888 records, census 6661 records, census 11990 records, census forestry data These experimental results re consistent with the theoretical results derived in the previous section Indeed the theory promises that convergence to optimal for a particular level of minconf 001 willoccur,givenarelaxation in 001  Our experiments indicate that such a relaxation is unnecessary When viewed together he theoretical nd experimental results demonstrated here 002er sampling s a reasonable approach to data reduction in the case of optimized support association rule 223nding The theoretical results suggest the appropriate tradeo\002s between accuracy and sample size to be considered by the practitioner References  R  A gra w a l T Imielinski and A N Sw ami Mining association rules between sets of items in large databases In P Buneman and S Jajodia editors Proceedingsofthe 1993 ACM SIGMOD International Conference on Management of Data  pages 207\226216 Washington D.C 26\226 28 1993  A  B lumer A Ehrenfeuc h t  D  H aussler and M K W armuth Learnability and the Vapnik-Chervonenkis dimension Journal of he ACM JACM  36\(4 October 1989  T  F ukuda Y Morimoto S  Morishita and T kuyama Data mining using two-dimensional optimized association rules scheme algorithms and visualization In Proceedings of the 1996 ACM SIGMOD international conference on Management of data  pages 13\22623 ACM Press 1996  T  F ukuda Y Morimoto S  Morishita and T Tokuyama Mining optimized association rules for numeric attributes n Proceedings of the 336fteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems  pages 182\226191 ACM Press 1996  J  H ipp U G 250 untzer and G Nakhaeizadeh Algorithms for association rule mining 226 a general survey and comparison SIGKDD Explorations  2\(1 July 2000  R  R astogi and K Shim Mining optimized supp ort rules for numeric attributes Information Systems  26\(6 444 2001  R  R astogi and K  Shim Mining optimized asso ciation rules with categorical and numeric attributes Knowledge and ata Engineering  14\(1 2002  H  T oiv onen Sampling l arge databases for asso ciation rules n T  Vijayaraman A P Buchmann C Mohan and N L Sarda editors In Proc 1996 Int Conf Very Large Data Bases  pages 134\226145 Morgan Kaufman 09 1996  J  S  V itter Random sampling with a r eserv oir ACM Transactions on Mathematical Software  11\(1 Mar 1985  J Wijsen and R Meersman On the c omplexit y o f m ining quantitative association rules Data Mining and Knowledge Discovery  2\(3 1998  M J Zaki S  P arthasarath y  W  L i and M Ogihara Evaluation of sampling for data mining of association rules n 7th International Workshop on Research Issues in Data Engineering RIDE\32597  Birmingham UK Apr 1997  D Zelenk o Optimizing disjunctiv e asso ciation rules I n Proe.ofPKDD\32599,LectureNotesinComputerScience LNAI 1704  pages 204\226213 Springer-Verlag 1999 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


of the query expression without ha ving the global view of the in ten tion There is a big c hance that the enco ded pro cedure ma y not b e the b est w a y to compute the rules dep ending on the database instance F urthermore as w e understand it their prop osals require p oten tially large n um ber of name generation for relations and attributes The names that are needed are usually database dep enden t and th us p ossibly cannot b e gathered at query time An additional pro cess needs to b e completed to gather those v ariables b efore actual computations can b egin 5  9 Optimization Issues While it w as in tellectually c hallenging to dev elop a declarativ e expression for asso ciation rule mining from deductiv e databases there are sev eral op en issues with great promises for resolution In the w orst case the least xp oin tneedsto generate n 2 tuples in the rst pass alone when the database size is n  Theoretically  this can happ en only when eac h transaction in the database pro duces an in tersection no de and when they are not related b y subset-sup erset relationship In the second pass w e need to do n 4 computations and so on The question no w is can w e a v oid generating and p erhaps scanning some of these com binations as they will not lead to useful in tersections F or example the no de b 0 3 in gure 11 is redundan t A signican t dierence with apriori lik e systems is that our system generates all the item sets top do wn in the lattice without taking their candidacy as a large item set in to consideration Apriori on the other hand do es not generate an y no de if their subsets are not large item sets themselv es and thereb y prunes a large set of no des Optimization tec hniques that exploit this so called an ti-monotonicit y prop ert y of item set lattices similar to apriori could mak e all the dierence in our setup The k ey issue w ould b e ho ww e push the selection threshold minim um supp ort inside the top do wn computation of the no des in the lattice in our metho d F or the momen t and for the sak e of this discussion let us consider a higher supp ort threshold of 60 for the database T of gure 9 No w the l-en v elop e will b e the one sho wn in ligh ter dashed lines in gure 11 and the no des under this line will b e the large item sets Notice that no ww eha v eto discard no des ad 2 0 and d 0 2 to o This raises the question is it p ossible to utilize the supp ort and condence thresholds pro vided in the query and prune candidates for in tersection an y further Ideas similar to magic sets transformation 3  24 ma y be b orro w ed to address this issue The only problem is that pruning of an y no de dep ends on its supp ort coun t whic h ma y come at a later stage By then all no des ma y already ha v e b een computed and th us pushing selection conditions inside aggregate op erator ma y b ecome non-trivial Sp ecial data structures and indexes ma y also aid in dev eloping faster metho ds to compute ecien t interse ction joins that w e ha v e utilized in this pap er W e lea v e these questions as op en issues that should be tak en up in the future F ortunately though there has been a v ast b o dy of researc h in optimizing Datalog programs including recursiv e programs suc h as the one w e ha v e used in this pap er and hence the new questions and researc h 5 Recall that their prop osal requires one to express the mining problem to the system using sev eral queries and up date statemen ts that utilizes information ab out the database con ten ts to ac hiev e its functionalit y  c hallenges that this prop osal raises for declarativ e mining ma y exploit some of these adv ances Needless to emphasize a declarativ e metho d preferably a formal one is desirable b ecause once w e understand the functioning of the system w e will then be able to select appropriate pro cedures dep ending on the instances to compute the seman tics of the program whic hw e kno wis in tended once w e establish the equiv alence of declarativ e and pro cedural seman tics of the system F ortunately  w e ha v e n umerous pro cedural metho ds for computing asso ciation rules whic h complemen t eac h other in terms of sp eed and database instances In fact that is what declarativ e systems or declarativit y buy us  a c hoice for the most ecien t and accurate pro cessing p ossible 10 Conclusion Our primary goal for this pap er has b een to demonstrate that mining asso ciation rules from an y rst-order kno wledge base is p ossible in a declarativ ew a y  without help from an y sp ecial to ols or mac hinery  and that w e can no wha v ea v ery in tuitiv e and simple program to do so W eha v esho wn that it is indeed p ossible to mine declarativ ekno wledge b y exploiting the existing mac hinery supp orted b ycon temp orary inference engines in programming languages e.g Prolog or kno wledge base systems e.g RelationLog XSB LD L  CORAL All w e require is that the engine b e able to supp ort set v alued terms grouping aggregate functions and set relational op erators for comparison functionalities whic hmostofthesesystemscurren tly supp ort W e ha v e also demonstrated that our formalism is grounded on a more mathematical foundation with formal prop erties on whic h the seman tics of the R ULES system rely  W e ha v e also raised sev eral op en issues related to eciency and query optimization whic h should b e our next order of business As future researc h w e plan to dev elop optimization tec hniques for mining queries that require non-trivial lo ok ahead and pruning tec hniques in aggregate functions The dev elopmen ts presen ted here also ha v e other signican t implications F or example it is no w p ossible to compute c hi square rules 4 using the building blo c ks pro vided b y our system Declarativ e computation of c hi square rules to our kno wledge has nev er b een attempted for the man y pro cedural concepts the computation of c hi square metho d relies on In a separate w ork 2 w e sho w that the coun ting metho d prop osed in this pap er can be eectiv ely utilized to generate the exp ectations needed in order to compute suc h rules rather easily  These are some of the issues w e plan to address in the near future The motiv ation imp ortance and the need for in tegrating data mining tec hnology with relational databases has b een addressed in sev eral articles suc h as 12  13 They con vincingly argue that without suc h in tegration data mining tec hnology ma y not nd itself in a viable p osition in the y ears to come T o b e a successful and feasible to ol for the analysis of business data in relational databases suc htec hnology m ust b e made a v ailable as part of database engines and as part of its declarativ e query language Our prop osal for declarativ e mining bears merit since it sheds ligh t on ho w rst order databases can be mined in a declarativ e and pro cedure indep enden t w a y so that the optimization issues can b e delegated to the underlying database engine Once suc h argumen ts are accepted sev eral systems 9 


related issues b ecome prime candidates for immediate atten tion F or example traditionally database systems supp orted declarativ e querying without the necessit y to care ab out the pro ceduralit y of the queries In this pap er w eha v e actually demonstrated that asso ciation rule mining can b e view ed as a Datalog query  It is immediate that a direct mapping from the Datalog expressions presen ted in this pap er to SQL can be dev elop ed with no problem at all W e can then rely on ecien t database pro cessing of the query in an optimized fashion Hence w ecomeclose to the essence of the visions expressed b y the leading database researc hers and practioners 12  References 1 Rak esh Agra w al and Ramakrishnan Srik an t F ast algorithms for mining asso ciation rules in large databases In VLDB  pages 487{499 1994 2 Anon ymous A declarativ e metho d for mining c hisquare rules from deductiv e databases T ec hnical rep ort Departmen t of Computer Science Anon ymous Univ ersit y USA F ebruary 2001 3 C Beeri and R Ramakrishnan On the po w er of magic In Pr o c e e dings of the 6th A CM Symp osium on Principles of Datab ase Systems  pages 269{283 1987 4 Sergey Brin Ra jeev Mot w ani and Craig Silv erstein Bey ond mark et bask ets Generalizing asso ciation rules to correlations In Pr o c A CM SIGMOD  pages 265 276 1997 5 D Chimen ti et al The LD L system protot yp e IEEE Journal on Data and Know le dge Engine ering  2\(1 90 1990 6 Jia w ei Han Jian P ei and Yiw en Yin Mining frequen t patterns without candidate generation In Pr o c A CM SIGMOD  pages 1{12 2000 7 Marcel Holsheimer Martin L Kersten Heikki Mannila and Hann uT oiv onen A p ersp ectiv e on databases and data mining In Pr o c of the sixth A CM SIGKDD Intl Conf  pages 150{155 Mon treal Queb ec 1995 8 Flip Korn Alexandros Labrinidis Y annis Kotidis and Christos F aloutsos Ratio rules A new paradigm for fast quan tiable data mining In Pr o c of 24th VLDB  pages 582{593 1998 9 Brian Len t Arun N Sw ami and Jennifer Widom Clustering asso ciation rules In Pr o c of the 3th ICDE  pages 220{231 1997 10 Mengc hi Liu Relationlog At yp ed extension to datalog with sets and tuples In John Llo yd editor Pr oc e e dings of the 12th International L o gic Pr o gr amming Symp osium  pages 83{97 P ortland Oregon Decem ber 1995 MIT Press 11 Rosa Meo Giusepp e Psaila and Stefano Ceri An extension to SQL for mining asso ciation rules Data Mining and Know le dge Disc overy  2\(2 1998 12 Amir Netz Sura jit Chaudh uri Je Bernhardt and Usama M F a yy ad In tegration of data mining with database tec hnology  In Pr o c e e dings of 26th VLDB  pages 719{722 2000 13 Amir Netz Sura jit Chaudh uri Usama M F a yy ad and Je Bernhardt In tegrating data mining with SQL databases In IEEE ICDE  2001 14 Ra ymond T Ng Laks V S Lakshmanan Jia w ei Han and Alex P ang Exploratory mining and pruning optimizations of constrained asso ciation rules In Pr o c A CM SIGMOD  pages 13{24 1998 15 Jong So o P ark Ming-Sy an Chen and Philip S Y u An eectiv e hash based algorithm for mining asso ciation rules In Pr o c A CM SIGMOD  pages 175{186 1995 16 Karthic k Ra jamani Alan Co x Bala Iy er and A tul Chadha Ecien t mining for asso ciation rules with relational database systems In Pr o c e e dings of the International Datab ase Engine ering and Applic ations Symp osium  pages 148{155 1999 17 R Ramakrishnan D Sriv asta v a and S Sudarshan CORAL  Con trol Relations and Logic In Pr o c of 18th VLDB Confer enc e  pages 238{250 1992 18 Konstan tinos F Sagonas T errance Swift and Da vid Scott W arren XSB as an ecien t deductiv e database engine In Pr o c of the A CM SIGMOD Intl Conf  pages 442{453 1994 19 Sunita Sara w agi Shib y Thomas and Rak esh Agra w al In tegrating mining with relational database systems Alternativ es and implications In Pr o c A CM SIGMOD  pages 343{354 1998 20 Ashok a Sa v asere Edw ard Omiecinski and Shamk an tB Nav athe An ecien t algorithm for mining asso ciation rules in large databases In Pr o c of 21th VLDB  pages 432{444 1995 21 Pradeep Sheno y  Ja y an t R Haritsa S Sudarshan Gaura v Bhalotia Ma y ank Ba w a and Dev a vrat Shah T urb o-c harging v ertical mining of large databases In A CM SIGMOD  pages 22{33 2000 22 Abraham Silb ersc hatz Henry F Korth and S Sudarshan Datab ase System Conc epts  McGra w-Hill third edition 1996 23 Shib y Thomas and Sunita Sara w agi Mining generalized asso ciation rules and sequen tial patterns using SQL queries In KDD  pages 344{348 1998 24 J D Ullman Principles of Datab ase and Know le dgeb ase Systems Part I II  Computer Science Press 1988 25 Mohammed J Zaki Generating non-redundan t association rules In Pr o c of the 6th A CM SIGKDD Intl Conf  Boston MA August 2000 1 0 


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


