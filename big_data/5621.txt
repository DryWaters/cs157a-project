 1 Compact Weighted Associative Classification S.P.Syed Ibrahim 1  K. R. Chandran 2  M. S. Abinaya 3  1 Assistant Professor, Department of Computer Science and Engineering PSG College of Technology, Coimbatore, India sps_phd@yahoo.co.in 2 Professor of IT and Head of Computer and Information Sciences PSG College of Technology, Coimbatore, India chandran_k_r@yahoo.co.in 3 PG student, Dept of Computer and Information Sciences PSG College of Technology, Coimbatore,India  abi.psgtech@gmail.com   Abstract-- Weighted association rule mining reflects semantic significance of item by considering its weight Classification extracts set of rules and constructs a classifier to predict the new data instance. This paper proposes compact weighted associative classification method, which integrates weighted association rule mining and classification for constructing an efficient weighted associative classifier. Compact weighted associative classification algorithm randomly chooses one non class attribute from dataset and all the weighted class association rules are generated based on that attribute The weight of the item is considered as one of the parameter in generating the weighted class association rules. In this proposed work, weight of item is computed by considering quality of the transaction using link based model. Experimental results show that the proposed system generates less number of high quality rules  Keywords  Association Rule Mining, Classification Associative Classification    I  INTRODUCTION   Data mining principally deals with extracting knowledge from dataset. In a world where data is all around us, the need of the hour is to extract knowledge or interesting information, which is hidden in the available data. Association rule mining is concerned with extracting a set of highly correlated features shared among a large number of records in a given database For example on mining the database of a store association rule mining can bring out relationship between the items in the store based on the customers buying patterns. Although classical association rule mining algorithm reflects the statistical relationship between items, it does not reflect the semantic significance of the items T o m eet th e us er obj ectiv e and business value, various weighted association rule mining meth  7  12] a r e pr opos ed bas e d on  the weightage to items. Here the weight is mostly based on utility of an item such as profit  Classification is also one of the most important tasks in data mining. Classification builds a model known as classifier and can be used to predict classes to new records. For example, a risk classification model can be built from a dataset of previous credit card customers and applied to classify the risk levels of new customers   Weighted association rule mining applies unsupervised learning where no class attribute is involved in finding the association rule. On the other hand, classification uses supervised learning where class attribute is involved in the construction of the classifier. Both, weighted association rule mining and classification are significant and efficient data mining techniques. So integration of these two data mining techniques may provide efficient associative classifi 13 w e  h a v e propos ed w e igh t ed  associative classification based on CBA algorithm and weight for each item is generated randomly   In this paper we proposed a new feature which includes 1. Weight for each item is calculated using HITS model. 2. The new compact weighted associative classification \(CWAC\ is proposed. The proposed CWAC algorithm is completely varies from CBA. In CBA, Apriori association rule mining algorithm is directly applied to find the class association rules Whereas Compact weighted class association rule generation algorithm randomly chooses one non class attribute from dataset and all the items are generated only based on that attribute. Thus this algorithm reduces number of itemset generation. Finally the proposed algorithm calculates the weighted support and weighted confidence for each item and determines whether the item is frequent or not    IEEE-International Conference on Recent Trends in Information Technology, ICRTIT 2011 978-1-4577-0590-8/11/$26.00 ©2011 IEEE MIT, Anna University, Chennai. June 3-5, 2011 1099 


 2 II RELATED WORK  A Association Rule Mining   Association Rule Mining \(ARM\ [1  ha s b e co m e  one of the important data mining tasks. ARM is an unsupervised data mining technique, which works on variable length data, and it produces clear and understandable rules. The basic task of association rule mining is to determine the correlation between items belonging to a transactional database. In general, every association rule must satisfy two user specified constraints, one is support and the other is confidence The support of a rule X Y \(X and Y are items defined as the fraction of transactions that contain X and Y, while the confidence is defined as the ratio support\(X and Y\pport\(X\ So, the target is to find all association rules that satisfy user specified minimum support and confidence values  B.  Associative Classification  Associative classification was first introduced in   w h i c h f o c u s on i n t e g r at i n g t w o k n o w n dat a  m i ni ng tasks, association rule discovery and classification. The integration done is focused on a special subset of association rules whose right hand side is restricted to the class attribute; for example, consider a rule R: X  Y, Y must be a class label. Associative classification generally involves two stages. The first stage, it adopts the association rule generation methods like Apriori candidate generation g r o w t h 5] alg orithm s to  generate class association rules. For example CB  method employs Apriori candidate generation and other associative methods such as CPAR M AR 9 and Lazy associative classification m e th ods  adopts FP growth algorithm for rule generation. The above step generates huge number of rules. So in the next stage, generated rules are ranked and the rules that satisfy certain threshold conditions are used to construct the classifier. After rule ranking, only the high-ranking rules are chosen to build a classifier and the rest are pruned  The associative classification method uses support and confidence measures to evaluate the rule quality  Support and confidence are given as  Support \(X  Y\           Occurrence \(X U Y  Total number of transaction    1 Confidence \(X  Y\ Occurrence\(X U Y    Occurrence \(X\      ----- \( 2  Any Item has a support larger than the user minimum support and confidence is called frequent itemset. The support and confidence measures used in association rule mining and associative classification treats each transaction items equally but different transactions have different weights in real-life data sets 12  C. Weighted Association Rule Mining  Classical ARM framework assumes that all items have the same significance or importance i.e. their weight within a transaction or record is the same weight=1 per item\which is not always the case. In the supermarket context, some items like jewellery designer clothes, etc., are of much significant in terms of revenue or profit by the store. Hence weight can be used as a parameter to generate association rule mining called as weighted association rule mining [7, 12 T h e  generation of weighted association rules is based on user specified minimum weighted support and minimum weighted confidence thresholds. The use of weighted support and weighted confidence leads to useful mechanisms to prioritize the rule according to their importance, instead of their support and confidence alone  D. Weighted Associative Classification  In e h a v e propos ed  w e i g h t ed as s o ciati v e  classification, which integrates weighted association rule mining and classification to construct the efficient weighted associative classifier. Weighted associative classifier extracts special subset of association rules called weighted class association rules \(WCARs Weighted association rule mining uses weight as one parameter but here weights for each item item are assigned randomly. But it is very difficult to assign weights to each item  D.  HITS Model   Klein s ed HIT S al g o rithm i n bipartite  graph and weights are derived from the internal structure of the database. This proposed method uses HITS model to derive the weight for each item. Then these weights are used to compute the Weighted Class Association Rules  III PROPOSED SYSTEM  A. Problem Definition  Let database D is a set of instances where each instance is represented by < a 1 a 2 a m C>, where a 1  a 2 a m are attributes and C are class value each has weights. A common rule is defined as x c, where x is a set of non class attributes and c is class label. The quality measurement factor of a rule is weighted IEEE-ICRTIT 2011 1100 


 3 support and weighted confidence where weighted support is \(Occurrence \(x & y c / |D|\* \(weight \(x weight \(y\ + weight \(c\|D| denotes the total number of instances in database and weighted confidence is Occurrence \(x & y c / Occurrence \(x c\* \(weight x\weight \(y\weight \(c\ule items that satisfy minimum weighted support and weighted confidence are called frequent rule items, while the rest are called infrequent rule items. Here the task is to generate the Weighted Class Association Rules \(WCARs\ that satisfies both minimum weighted support and minimum weighted confidence constraints. Then these WCARs are used to construct a classifier based on Confidence Support size-of-the rule Antecedent  B. Weighted Associative Classification Rule Mining  Classification and weighted association rule mining are two different techniques involved in rule mining Integration of classification and weighted association rule mining consist of two steps. In the first step the weighted associative classification rules are generated using Compact Weighted Class Association Rule Generation algorithm and the second step deals with the classifier to order the rules generated in step one  C. Compact Weighted Associative Classification Algorithm  Compact weighted class association rule generation algorithm is shown in Figure 3.1  INPUT: DATA SET OUTPUT: ITEM SET  Step 1: Divide the dataset into training and testing dataset Step 2 : Choose an attribute randomly other than class attribute from the training dataset Step 3: Generate one item based on the selected attribute Step 4 : Calculate weighted support Step 5: If weighted support of item is greater than minimum weighted support then generate two itemset and so forth Step 6: Calculate weighted confidence for all itemset  CLASSIFIER ALGORITHM  Step 1: Rank the rules based on Weighted Confidence Weighted Support and Size of the rule antecedent Step 2:  Classify the test dataset using these ruleset and obtain the classifier accuracy  Fig 3.1 Compact Weighted Associative Classification Algorithm Weighted support and weighted confidence are given as  WSup \(X U Y  C\=\(Occurrence \(X U Y  C / |D Weight \( X\ +Weight\(Y\ Weight\(C     3   W Conf \(X U Y  C\ = WSup\(XUY  C  4 WSup \(X  C  Where X and Y are items in the dataset, C is the set of class label. In frequent rules are pruned to get frequent class association rules. The steps are shown in the Figure 3.2     Fig 3.2 Steps in Compact Weighted Associative Classification  The minimum weighted support and minimum weighted confidence are user defined threshold values The itemset that has weighted support and weighted confidence above the threshold value are called as frequent itemset and others are called as infrequent itemset which are pruned during rule generation process. This is followed by the classifier construction To built the classifier all the generated rules are ranked based on Weighted Confidence, Weighted Support and size-of-the rule antecedent Then best rules are chosen and used in the classifier  IV. SIMULATED EXAMPLE  Following example is used to explain how the HITS algorithm is constructed. In the Table 4.1 sample transaction are given. This sample dataset values are transformed into bipartite graph as shown in Figure 4.1    Calculate the item wei g h t usin g HITS Model Generate class association rules using CWAC algorithm based on weighted support and wei g hted confidence Rule rankin g and p runin g Construct the Classifier Predict the new dataset Compact Weighted Associative Classification 1101 


 4 T ABLE 4.1 SAMPLE  DATASET  TID Transaction 1 A B C D E 2 C F G 3 A B 4 A 5 C F G H 6 A F G H  A  1     B              2    C   3    D   4    E    5    F   6    G   H HUB  A U T HO R I T Y   Fig 4.1 Bipartite Graph representation of sample dataset  The following equations are used to perform each iteration  auth\(item hub \(transaction\                   --- \(5  hub \(Transaction auth \(item 6  T ABLE 4.2 ITEM  WEIGHT  Item weight Item weight A 4414 E 1746 B 2531 F 3278 C 5025 G 4662 D 1746 H 3203  The Table 4.2 shows the authority weight for each item of sample dataset of Table 4.1  Let us consider Table 5.1. In the first step if CWAC algorithm randomly chooses outlook attribute then   attribute values with combination of rules Outlook, Temp Play}, {Outlook, Humidity  Play}, {Outlook, Windy Play} will be generated But the rules with {Temp, Humidity play}, {Temp Windy play}, and {Humidity, Windy play attribute combination rules will not be generated. This shows number of rules generated will be less compared with the CBA algorithm  V  EXPERIMENTAL RESULTS  Consider a dataset given in the Table 5.1 consists of 14 transactions and 2 class labels. The weight of each item is calculated using HITS algorithm and weights are computed as  Sunny = 0.22, Overcast=0.19 Rainy =0.24, Hot=0.17, Mild=0.29, Cool=0.18 High=0.31, Normal=0.34, False=0.26, True=0.39, yes 0.45, No = 0.20. In the first step ,Compact weighted associative classification rule mining algorithm will find all the weighted class association rules based on weighted support and weighted confidence threshold  T ABLE 5.1 SAMPLE DATASET  The rules whose weighted support and weighted confidence, higher than user defined minimum threshold is known as weighted class association rules WCARs\. Then WCARs are given to classifier where it is sorted based on weighted Confidence, Weighted Support and Rule length in descending to construct the accurate Compact Weighted Associative Classifier  T ABLE 5.2   COMPARISON OF  CLASS ASSOCIATION RULES  Associative Classification Compact Weighted Associative Classification Sunny yes=2 sup\=14 Sunny no=3 sup\=21 Sunny -> Yes = 2 Wsup\ = 9.38 Sunny -> No = 3 Wsup\ = 8.82 Outlook Temp Humidity windy play Sunny Hot High False No Sunny Hot High True No Overcast Hot High False Yes Rainy Mild High False Yes Rainy Cool Normal False Yes Rainy Cool Normal True No Overcast Cool Normal True yes Sunny Mild High False No Sunny Cool Normal False Yes Rainy Mild Normal False Yes Sunny Mild Normal True Yes Overcast Mild High True Yes Overcast Hot Normal False Yes Rainy Mild High True No IEEE-ICRTIT 2011 1102 


 5 Overcast yes= 4 sup\=28 Rainy yes=3 sup\=21 Rainy no=2 sup\=14 Hot yes=2 sup\=14 Hot no=2 sup\=14 Mild yes=4 sup\=28 Mild no=2 sup\=14 Cool yes=3 sup\=21 High yes=3 sup\=21 High no=4 sup\=28 Normal yes=6 sup\=42 TRUE yes=3 sup\=21 TRUE no=3 sup\=21 FALSE yes=6 sup\=42 FALSE no=2 sup\=14  Sunny normal yes=2 sup\=14 Sunny hot no=2 sup\=14 Sunny high no=3 sup\=21 Sunny FALSE no=2 sup\=14 Overcast hot yes= 2 sup\=14 Overcast high yes= 2 sup\=14 Overcast normal yes= 2 sup\=14 Overcast TRUE yes= 2 sup\=14 Overcast FALSE yes= 2 sup\=14 Rainy mild yes=2 sup\=14 Rainy normal yes=2 sup\=14 Rainy FALSE yes= 3 sup\=21 Rainy TRUE no= 2 sup\=14 Hot FALSE yes=2 sup\=14 Hot high no=2 sup\=14 Mild high yes=2 sup\=14 Mild normal yes= 2 sup\=14 Mild TRUE yes=2 sup\=14 Mild FALSE yes=2 sup\=14 Mild high no=2 sup\=14 overcast -> Yes = 4 Wsup\ = 18.56 rainy -> Yes = 3 Wsup\ = 14.49 rainy -> No = 1 Wsup\ = 3.08 hot -> Yes = 2 Wsup\ = 8.68 hot -> No = 2 Wsup\ = 5.18 mild -> Yes = 4 Wsup\ = 21.46 mild -> No = 1 Wsup\ = 3.43 cool -> Yes = 3 Wsup\ = 13.33 cool -> No = 1 Wsup\ = 2.66 high -> Yes = 3 Wsup\ = 15.96 high -> No = 3 Wsup\ = 10.71 false -> Yes = 3 Wsup\ = 14.91 false -> No = 2 Wsup\ = 6.44 true -> Yes = 6 Wsup\ = 36.12 true -> No = 2 Wsup\ = 8.26 normal -> Yes = 6 Wsup\ =33.97 normal -> No = 1 Wsup\ = 4.2  Sunny -> hot -> No = 2 Wsup\ =8.26 Sunny -> mild -> No = 1 Wsup\ =4.97 Sunny -> mild -> Yes = 1 Wsup\ = 6.58 Sunny -> cool -> Yes = 1 Wsup\ = 5.95 overcast -> hot -> Yes = 2 Wsup\ = 11.3 overcast -> mild -> Yes = 1 Wsup\ =6.51 overcast -> cool -> Yes = 1 Wsup\ = 5.74 rainy -> mild -> Yes = 2 Wsup\ = 13.7 rainy -> cool -> No = 1 Wsup\ =4.34 rainy -> cool -> Yes = 1 Wsup\ = 6.09 Sunny -> mild -> high -> No = 1 Wsup\ = 7.14 Sunny -> mild ->normal -> Yes 1 Wsup\ = 9.1 rainy -> cool -> normal -> No 1 Wsup\ = 6.72 rainy -> cool -> normal -> Yes 1 Wsup\ = 8.47 rainy -> cool -> normal -> false No = 1 Wsup\ = 8.54 rainy -> cool -> normal -> true > Yes = 1 Cool normal yes=3 sup\=21 Cool FALSE yes= 2 sup\=14 High FALSE yes=2 sup\=14 High TRUE no=2 sup\=14 High FALSE no=2 sup\=14 Normal TRUE yes=2 sup\=14 Normal FALSE yes= 4 sup\=28 Wsup\ =11.2    The proposed system is compared with the classical associative classification algorithm \(CBA meth T h e t a bl e 5.2 s h o w s t h e num ber o f rul es generated. The result shows proposed method generates minimal non redundant weighted class association rules on the same time these rules are accurate because of the usage of weights for each item in dataset  A. Accuracy Computation  Accuracy measures the ability of the classifier to correctly classify unlabeled data. It is the ratio of the number of correctly classified objects to the total number of objects in the test dataset   T ABLE 5.3   DATASET DESCRIPTION  T ABLE 5.4 ACCURACY COMPARISON  Dataset CBA  CWAC Pima 73.18 74.2  The performance of the proposed algorithm is evaluated by comparing it with the CBA Algorithm   Conclusion  The aim of integrating classification and weighted association rule mining is to address some important requirements arising from modern data mining processes. The development of weighted associative classification using compact weighted associative classification \(CWAC\ algorithm, greatly reduce the number of rules in the classifier. This paper shows how Dataset Transaction Classes Items Pima 768 2 15 Compact Weighted Associative Classification 1103 


 6 to generate Compact Weighted Class Association Rules, which may greatly improve the classification accuracy. This plays a vital role in market basket analysis, medical diagnosis and in many other applications. Experimental results show that the proposed Compact Weighted Associative Classification CWAC\ method outperformed the CBA method. This work can be further applied for more number of benchmark datasets  References  g ar w a l R, I m ieli n s k i T an d S w a m i A  Mi n i ng  Association rules between sets of items in large databases, proc of the 1993 ACM SIGMOD International conference on Management of Data Washington,DC,1993,pp.207 2 g ar w a l.R a n d Srik a n t R, Fast al g o rith m  f o r mining association rules in large data bases,Proceedings of the 20 th international conference on very Large Data Base\(VLDB94 Santiago,chile,1994, pp 487-499  Baralis E., C h ius a n o S   Graza, P   O n  s u pport thresholds in associative classification. In roceedings of the 2004 ACM Symposium on Applied Computing.  Nicosia, Cyprus: ACM Press.2004  E. & T o rin o P  A  laz y approach to pruning classification rules. Proceedings of the 2002 IEEE International Conference on Data Mining \(ICDM02\, Maebashi City, Japan, p 35.2004 5 J H a n J  P e i  a n d Y  Y i n  M i n i n g Fr e q ue nt  Patterns without Candidate Generation, Proc ACM SIGMOD, 2000 6 n g Yao  H.J Ha m ilto n  Min i n g ite m  set u t ilitie s from transaction data bases, data and Knowledgs Engineering,pp.603-626,volime 59,issu 3 \(2006  n  Fe ng s h an  B a i, Min ing W e igh t ed  Association Rules without Preassigned Weights Proceedings of IEEE Transactions on Knowledge and Data Engineering. VOL.20 No. 4 April 2008 8 K l e i nb e r g J  M  A ut ho r i t a t i v e So ur c e s i n a  Hyperlinked Environment, J. ACM, vol. 46, no. 5 pp.604-632, 1999  i W., Ha n  J. & P e i, J C M A R   A ccu rate an d  efficient classification based on multiple-class association rule. In Proceedings of the International Conference on Data Mining ICDM01\an Jose,CA, pp. 369376.2001 10 Li u B   H s u W  M a  Y   I nt e g r a t i ng  classification and association rule mining. In Proceedings of the International Conference on Knowledge Discovery and Data Mining. New York, NY: AAAI Press,pp. 8086,1998  a m k um ar.G.D,  R a nk a S  an d T s u r S  W eig h ted  Association Rules: Model and Algorithm, Proc ACM   SIGKDD, 1998  lai m a n K h an M, M u y e ba,Fran s  C o en e n  A Weighted utility Framework for Mining Association Rules. In the Proceedings of second UKSIM European Symposium on Computer Modeling and Simulation  ed Ibrah i m  S. P  C h a n dran K.R   A b i n a y a M.S  Integration of weighted association rule mining and Classification in the proceedings of National Conference on Recent Advances in Computer Vision & Information Technology, March 7 2011, pp 231-235  Sy ed Ibrah i m  S.P  C h a n dran  K.R  J a bez  Christopher.J, An Evolutionary Approach for Ruleset Selection in a Class Based Associative Classifier in the European Journal of Scientific Research, ISSN 1450-216X, Vol.50 No.3 2011\p.422-429  Tao, F Mu rtag h  F  F a rid, M W eigh ted Association Rule Mining Using Weighted Support and Significance Framework. In: Proceedings of 9th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 661666 Washington DC \(2003  n  X an d  Ha n  J, C PAR   C l ass i f i cat i on Bas e d on Predictive Association Rules, Proc. Third SIAM Intl Conf. Data Mining \(SDM03\y 2003 IEEE-ICRTIT 2011 1104 


 Figure 4  Compression of the action table from Nursery dataset of size 250Mb into FP-Tree with varying minimum support values VII  C ONCLUSION AND FUTURE WORK  In this paper, we propose the action table as the ideal search domain for action rules mining. The action table transforms the complex problem of finding action rules from a plain decision table, into finding action rules from an action table. As a result, the problem of action rules mining is reformulated into association-mining In practice, we applied FAARM on the Hepatitis and Nursery datasets and compared the results and performances with AAR and ARD. Although the space and time complexity associated with generating the action table are O\(n 2 experiments show that FAARM has a better execution time on relatively small dataset, over ARD and AAR Generating the action table directly into the FP-Tree could mitigate the space complexity associated with action table. As a future work, we propose to look at parallel implementation of Apriori and FP-Growth to test the scalability of using the action table with large datasets R EFERENCES  1  Z. Pawlak, Information systems - theoretical foundations, Information Systems Journal, Elsevier, Vol 6, 1981, 205-218 2  J. Han, J. Pei, and Y. Yin, Mining frequent patterns without candidate generation, ACM SIGMOD International Conference on Management of Data, 2000, 1 12 3  Z.W. Ras and A. Wieczorkowska,  "Action-Rules: How to Increase Profit of a Company",  The Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases, 587-592 4  Z. He, X. Xu, S. Deng, R. Ma, Mining action rules from scratch, Expert Systems with Applications, Elsevier, Vol 29, No. 3, 2005, 691-699 5  Z.W. Ras and A. Dardzinska,  "Action Rules Discovery without Pre-existing Classification Rules",   The Sixth International Conference on Rough Sets and Current Trends in Computing, 2008, 181-190 6  Z.W. Ras, A. Dardzinska, L. Tsay,  and H. Wasyluk Association Action Rules",   IEEE International Conference on Data Mining Workshops, 2008, 283-290 7  Qiang Yan, Jie Yin, Charles Ling, Tielin Chen,"Postprocessing Decision Trees to Extract Actionable Knowledge", IEEE International Conference on Data Mining, 2003,  685-688 8  Z.W. Ras and L. Tsay,  "Discovering Extended ActionRules \(System DEAR\,   International IIS IIPWM'03 Conference, 2003, 293-300 9  L. Tsay and Z.W. Ras,  "Action rules discovery: system DEAR2, method and experiments",   Journal of Experimental & Theoretical Artificial Intelligence, 2005 119-128   Z.W. Ras, E. Wyrzykowska,  and H. Wasyluk,  "ARAS Action Rules Discovery Based on Agglomerative Strategy",   Third International Workshop on Mining Complex Data, 2007, 196-208   http://archive.ics.uci.edu/ml/datasets   S. Im and Z.W. Ras,  "Action Rule Extraction from a Decision Table: ARED",   International Syposium on Methodologies for Intelligent Systems, 2008, 160-168   J. S. Deogun, V. V. Raghavan, and H. Sever, Rough set based classification methods and extended decision tables International Workshop on Rough Sets and Soft Computing, 1994, 302-309   R. Agrawl and R. Srikant, Fast algorithm for mining assocation rules, International Conference on Very Large Data Bases, 1993, 487-499  
404 


a  Figure 1.  The original Share-struct and the steps of S?s change C. Share-FPM algorithm In this subsection, we will develop an efficient algorithm for mining all frequent patterns Algorithm 2 Share-FPM Input: S?, the Share-struct constructed based on Algorithm 1 and s, the minimum support threshold Output: The complete set of frequent patterns Method:Call Share-FPM \(S?,?, s Procedure Share-FPM \(S? ,?, s 1 for each entry si in S 2   if \(si.local-count<s and si->new! =NULL 3     add all si ->news children to the new fields of relevant entries in S 4   else ? =?si; ?-SD = ?-SD 5     if \(si ->new != NULL 6       if si is the only active entry 7         ?-SD=?-SDsi 8         add all children of si to relevant entries in S 9         si ->old= S 10      else ?-Postfix = ?-Postfix ?si 11        if \(si ->old == S 12          call Inherit\(si, S TID Items Bought \(Ordered 100 f, a, c, d, g, i, m, p f, c, a, m, p 200 a, b, c, f, l, m, o f, c, a, b, m, l, o 300 b, f, h, j, o f, b, o 400 b, c, k, s, p c, b, p 500 a, f, c, e, l, p, m, n f, c, a, m, p, l 1431 13        else call Initialize \(S?, si->new, si->old 14        if \(si ->old == NULL 15          flag_upload =TRUE 16        si->old = S 17        call Share-FPM \(S?, ?, s 18        if \(flag_upload ==TRUE 19          call Upload \(S?, S 20    else if \(si ->old ? S 21      ?-SD=?-SD?the items that appear after ? in ?-SD 22      ?-Postfix=the items that appear after ? in ?-Postfix 


23      for each pi in ?-Postfix 24        ? = ??pi 25        ?-SD = the items that appear before pi in ?-SD 26        call Share-FPM \(pi->old, ?, s 27      call Generate-FP \(?, ?-SD, ?-Postfix 28 call Generate-FP \(?,?-SD, ?-Postfix the end of Share-FPM The procedures Inherit, Upload and Generate-FP are shown in the following Procedure Inherit \(s, S if s->old can be released then //memory management S? = s->old else create a new Share-table S? by inheriting s->old call Initialize \(S?, s->new  Procedure Upload \(S?, S for each entry si in S upload all new and old fields in S? to old fields in S add si.local-count in S? to si.local-count in S  Procedure Generate-FP \(?,?-SD, ?-Postfix for each nonempty combination ? of the items in ?-SD generate pattern ?? ? with support minimum support of items in it for each item pi in ?-Postfix for each combination ? of the items which appear before pi generate pattern ?? ? ? pi with support minimum support of items in it  D. Share-UFPM Algorithm According to our mining model description, each utility frequent pattern is also frequent. After Share-FPM algorithm finds all frequent patterns, the Share-UFPM algorithm scans the database once to check whether each frequent pattern candidate algorithm is as follow Algorithm 3 Share-UFPM Input:   S?, s Output:   UFP, utility frequent patterns in DB Method:  Call Share-UFPM\(S?, s 


Procedure Share-UFPM\(S?, s UFP FP = Share-FPM \(S?,?, s for each transaction Ti ? DB for each candidate c ? FP if \(c ? Ti and u \(c, Ti c.support for each candidate c ? FP if \(c.support ? s UFP = UFP + c return UFP  IV. PERFORMANCE STUDY To evaluate the efficiency and effectiveness of our algorithms, we have done extensive experiments on various kinds of datasets with different features. The experiments are based on a 2.4GMHz Pentium IV PC with 512MB main memory and 60 GB hard driver, running on Microsoft Windows 2000 Server. All the programs are written in Microsoft/Visual C++6.0 The measured performance is algorithms execution time on the datasets with different minimum support threshold. The execution time only includes the disk reading time \(scan datasets output frequent patterns speed of disk writing A. Datasets and characteristics We use real world and synthetic data for our performance study. The basic characteristics of datasets are listed in the following The real world dataset called Retail is achieved from a retailing company. Retail contains products from various categories. There are 16469 items and 88162 transactions in the dataset. Each transaction consists of the products purchased by a customer at a time point. Its average transaction size and average maximal potentially frequent patterns size are 10.3 and 3. The size of this dataset is 4M The synthetic data sets which were used for the experiments were achieved from the online FIMI repository See the RUL: http://fimi.cs.helsinki.fi/. The data sets are T10I4D100K and T40I10D100K. In T10I4D100K, the average record size and average maximal potentially frequent patterns 


size are 10 and 4. In T40I10D100K, they are 40 and 10. The numbers of transactions in both two dataset are set to 100K There are exponentially numerous frequent patterns when the support threshold goes down B. Experimental results In order to mine the utility frequent patterns, we randomly generate the count of each item between 1 and 6. In fact, most items are in the low profit range, we synthetically generate utility values of each item from 0.01 to 10.00, using a log normal distribution. For instance, Fig.2 shows the distribution of the utility values of items in T10I4D100K 1432 0 1 2 3 4 5 6 7 8 9 10 0 20 40 60 80 100 120 140 160 180 N um be r o f i te m s Utility value  Figure 2.  Utility value distribution in T10I4D100K For selecting appropriate utility thresholds, we use the average transaction utility value to constraint the utility threshold instead of randomly choosing it. For example, in Table 1, where the average transaction utility value is 40. If the utility threshold is equal to 25%, it represents that ? = 10 40  25% =10 We compare the performance of Share-FPM with BUUFM [5], an up to date algorithm for utility frequent patterns 


mining. Fig.3 through Fig.5 show the performance curves of two algorithms on three datasets respectively. We can see that the Share-UFPM algorithm outperforms BU-UFM on all datasets, and the performance gap becomes significant when the minimum support threshold drops low enough 0.0 0.2 0.4 0.6 0.8 1.0 1.2 0 10 20 30 40 50 60 70 80 90 100 110 120 Ti m e S ec on ds  Minimum support \(%0 Share-UFPM BU-UFM Utility threshold=5  Figure 3.  Fig.12 Utility frequent patterns mining on Retail 0.0 0.2 0.4 0.6 0.8 1.0 0 50 100 150 200 250 300 350 


400 450 500 550 600 Ti m e S ec on ds  Minimum support \(%0 Share-UFPM BU-UFM Utility threshold=5  Figure 4.  Fig.13 Utility frequent patterns mining on T10I4D100K 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0 50 100 150 200 250 300 350 400 450 500 550 Ti m e S ec on ds  Minimum support Share-UFPM 


BU-UFM Utility threshold=5  Figure 5.  Utility frequent patterns mining on T40I10D100K V. CONCLUSIONS In this paper, we introduce a utility frequent pattern mining model based on a share strategy to find the combination of items with high frequencies and utilities. This model first find all patterns with a given minimum support threshold. In this step, a share strategy gives a way to share most of the results from the previous mining process instead of separating them distinctively, thereby dramatically reducing the cost of computation. And then all patterns that do not satisfy a minimum utility threshold are pruned The extension of our technique, for maintenance of the already mined utility frequent patterns when updating databases, is an interesting topic for future research REFERENCES 1] R. Agrawal, T. Imielinski, A. Swami, Mining association rules between sets of items in large databases, In: Proceedings of the 1993 ACM-SIGMOD, Washington, DC, 1993, 207216 2] J. Han, H. Cheng, D. Xin, X. Yan, Frequent pattern mining: current status and future directions, Data Min knowl Disc 2007, 55-86 3] Y. Liu, W. Liao, A.Choudhary, A two-phase algorithm for fast discovery of high utility itemsets, Lecture Notes in Artificial Intelligence 2005, 3518:689-695 4] Y. Liu, W. Liao, A. Choudhary, A fast high utility itemses mining algorithm, In: Proceeding of the 2005 ACM SIGKDD workshop on utility-based data mining, Chicago, Illinois, USA, 2005, 90-99 5] J. Yeh, Y. Li, C. Chang, Two-phase algorithms for a novel utilityfrequent mining model, Lecture Notes in Artificial intelligence 2007 4819: 433-444 6] C, Aaron, F. John, Association mining, ACM Computing Surveys 2006, 38\(2 7] H.Yao, H. Hamilton, C. Butz, A foundational approach to mining itemset utilities from databases, In: Proceeding of the 4th SIAM International Conference on Data Mining, Lake Buena Vista, Florida 2004, 428-486 8] H. Yao, H. Hamilton, L. Geng, A unified framework for utility based measures for mining itemsets, In: Proceedings of ACM SIGKDD 2nd workshop on utility-based data mining, New York, NY, 2006, 28-37 9] J. Han, M. Kamber, Data mining: concepts and techniques, 2nd edn 


Morgan Kaufmann. 2006 10] J. Han, J. Pei, Y. Yin, Mining frequent patterns without candidate generation, In: Proceeding of the 2000 ACM-SIGMOD international conference on management of data, Dallas, TX, 2000, 112 


2] S. Brin, R. Motwani, J. D. Ullman, and S. Tsur Dynamic Itemset Counting and Implication Rules for Market Basket Data," in Proceedings of the 1997 ACM SIGMOD international conference on Management of data, Tucson, Arizona, United States 1997, pp. 255-264 3] J. S. Park, M. S. Chen, and P. S. Yu, "An Effctive Hash based Algorithm for mining association rules in Prof. ACM SIGMOD Conf Management of Data New York, NY, USA, 1995, pp. 175 - 186 4] R. Agrawal, T. ,PLHOL?VNL DQG $. Swami, "Mining Association Rules between Sets of Items in Very Large Databases," in Proceedings of the 1993 ACM SIGMOD international conference on Management of data, Washington, D.C., 1993, pp. 207-216 5] H. Mannila, H. Toivonen, and A. I. Verkamo Efficient Algorithms for Discovering Association Rules," in AAAI Workshop on Knowledge Discovery in Databases, 1994, pp. 181-192 6] R. Srikant and R. Agrawal, "Mining Generalized Association Rules," in In Proc. of the 21st Int'l Conference on Very Large Databases, Zurich Switzerland, 1995 7] R. Srikant, Q. Vu, and R. Agrawal, "Mining association rules with item constraints," in In Proc 3rd Int. Conf. Knowledge Discovery and Data Mining, 1997, pp. 67--73 8] A. Savasere, E. Omiecinski, and S. B. Navathe, "An Efficient Algorithm for Mining Association Rules in Large Databases," in Proceedings of the 21th International Conference on Very Large Data Bases 1995, pp. 432 - 444 9] H. Mannila, "Database methods for data mining," in The Fourth International Conference on Knowledge Discovery and Data Mining, 1998 10] B. Liu, W. Hsu, and Y. Ma, "Mining Association Rules with Multiple Minimum Supports.," in SIGKDD Explorations, 1999, pp. 337--341 11] H. Yun, D. Ha, B. Hwang, and K. H. Ryu, "Mining association rules on significant rare data using relative support.," Journal of Systems and Software archive vol. 67, no. 3, pp. 181 - 191, 2003 


12] M. Hahsler, "A Model-Based Frequency Constraint for Mining Associations from Transaction Data Data Mining and Knowledge Discovery, vol. 13, no 2, pp. 137 - 166, 2006 13] L. Zhou and S. Yau, "Association rule and quantitative association rule mining among infrequent items," in International Conference on Knowledge Discovery and Data Mining, San Jose, California 2007, pp. 156-167 14] C. Ordonez, C. Santana, and L. d. Braal, "Discovering Interesting Association Rules in Medical Data," in Proccedings of ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, 2000, pp. 78-85 15] L. J. Sheela and V. Shanthi, "DIMAR - Discovering interesting medical association rules form MRI scans," in 6th International Conference on Electrical Engineering/Electronics, Computer Telecommunications and Information Technology 2009, pp. 654 - 658 16] C. Ordonez, N. Ezquerra, and C. A. Santana Constraining and summarizing association rules in medical data," Knowledge and Information Systems vol. 9, no. 3, pp. 259 - 283, September 2005 17] H. Pan, J. Li, and Z. Wei, "Mining Interesting Association Rules in Medical Images," Lecture Notes In Computer Science, vol. 3584, pp. 598-609, 2005 18] S. Doddi, A. Marathe, S. S. Ravi, and D. C Torney Discovery of association rules in medical data Medical Informatics and the Internet in Medicine, vol 26, no. 1, pp. 25-33, January 2001 86 


the time needed for execution exceeded 100000 seconds Thus, from this analysis we see that FPrep, which uses FCM clustering, clearly outperforms the CLARANS and CURE based methods on the basis of speed. The execution times for CLARANS and CURE mentioned in fig. 7 and Table II do not include the time required to create fuzzy sets, and calculate the membership value  for each numerical data point in every fuzzy set for the numerical attribute under consideration. These times also do not take into account the time required to transform crisp numerical attributes to fuzzy attributes, and derive the fuzzy dataset from the original crisp dataset The fuzzy partitions generated for each of the five numerical attributes for the USCensus1990raw dataset are shown in Table III. Coincidentally, generating three fuzzy partitions for each numerical attribute seemed a perfect fit In addition to the superior speeds achieved by FPrep, as illustrated in fig. 7 and Table II, Table III indicates the semantics and the quality of the fuzzy partitions generated by FPrep. Moreover, the number of frequent itemsets generated by a fuzzy ARM algorithm \(like fuzzy ARMOR and fuzzy Apriori minimum support threshold, is illustrated in fig. 8   Fig. 7. Algorithm, numerical attribute comparison based on speed \(log10 seconds   Fig. 8. Number of frequent itemsets for various minimum support values  B. Results from Second Dataset We have also applied FPrep on the FAM95 dataset http://www.stat.ucla.edu/data/fpp transactions. Of the 23 attributes in the dataset, we have used the first 18, of which six are quantitative and the rest are binary. For each of the six quantitative attributes, we have generated fuzzy partitions using FPrep. A thorough analysis with respect to execution times, has already been performed on the USCensus1990raw dataset \(which is manifolds bigger in size than the FAM95 dataset both on the basis of number of transactions and number of unique values for numerical 


attributes dataset has been done solely to provide further evidence of the quality and semantics of the fuzzy partitions generated by FPrep. The details of the same are in Table IV. In this case, the number of fuzzy partitions is different for different numerical attributes. Thus, the number and type of fuzzy partitions to be generated is totally dependent on the attribute under consideration. A graphical representation of the fuzzy partitions generated for the attribute Age has already been provided in fig. 5, and clearly shows the Gaussian nature of the fuzzy partitions. The nature and shapes of fuzzy partitions for the rest of the attributes are also similar. Last, the number of frequent itemsets generated for different minimum support values is illustrated in fig. 8  C. Analysis of Results With FPrep, we can analyze and zero in on the number and type of partitions required based on the semantics of the numerical attributes, which the methods detailed in [19 20] do not necessarily facilitate. Then, FPrep, backed by FCM clustering, takes care of the creating the fuzzy partitions, especially assigning membership values for each numerical data point in each fuzzy partition. In section 8.A we have already shown that FPrep is nearly 9 to 44 times faster than the CURE-based method, and 2672 to 13005 times faster than the CLARANS-based method. FPrep is not only much faster than other related methods, but also generates very high quality fuzzy partitions \(Table III and IV much user-intervention. We have created a standard way of representing any fuzzy dataset \(converted from any type of crisp dataset efficacy of the same is corroborated by the successful implementation of Fuzzy Apriori and Fuzzy ARMOR on the fuzzy dataset \(converted from crisp version of FAM95 dataset an initial implementation of Fuzzy ARMOR, are very encouraging. FPrep, when used in conjunction with these fuzzy ARM algorithms, generates a pretty good number of high-quality frequent itemsets \(fig. 8 frequent itemsets generated for a particular minimum support is same, irrespective of the fuzzy ARM algorithm 


used IX. CONCLUSIONS In this paper we have highlighted our methodology, called FPrep, for ARM in a fuzzy scenario. FPrep is meant for seamlessly and holistically transforming a crisp dataset into a fuzzy dataset such that it can drive a subsequent fuzzy ARM process. It does not rely on any non-fuzzy techniques and is thus more straightforward, fast, and consistent. It facilitates user-friendly automation of fuzzy dataset 1 0 1 2 3 4 5 Age - 91 Hours - 100 Income3 4949 Income2 13707 Income1 55089 Ti m e lo g1 0 se co nd s Numerical Attribute - Number of Unique Values FCM CURE CLARANS 0 500 1000 1500 2000 2500 


3000 0.075 0.1 0.15 0.2 0.25 0.3 0.35 0.4 N o o f F re qu en t I te m se ts Minimum Support USCensus1990 FAM95 generation through FCM, and subsequent steps in preprocessing with very less manual intervention and as simple and straightforward manner as possible. This methodology involves two distinct steps, namely creation of appropriate fuzzy partitions using fuzzy clustering and creation of fuzzy records, using these partitions, to get the fuzzy dataset from the original crisp dataset FPrep has been compared with other such techniques, and has been found to better on the basis of speed. We also illustrate its efficacy on the basis of quality of fuzzy partitions generated and the number of itemsets mined by a fuzzy ARM algorithm which is preceded by FPrep. This preprocessing technique provides us with a standard method of fuzzy data \(record that it is useful for any kind of fuzzy ARM algorithm irrespective of how the algorithm works. Furthermore, this pre-processing methodology has been adequately tested with two disparate fuzzy ARM algorithms, Fuzzy Apriori and Fuzzy ARMOR, and would also work fine with other fuzzy ARM algorithm REFERENCES 1] Zadeh, L. A.: Fuzzy sets. Inf. Control, 8, 338358 \(1965 2] Chen G., Yan P., Kerre E.E.: Computationally Efficient Mining for Fuzzy Implication-Based Association Rules in Quantitative Databases. International Journal of General Systems, 33, 163-182 


2004 3] Hllermeier, E.: Fuzzy methods in machine learning and data mining Status and prospects. Fuzzy Sets and Systems. 156, 387-406 \(2005 4] De Cock, M., Cornelis, C., Kerre, E.E.: Fuzzy Association Rules: A Two-Sided Approach. In: FIP, pp 385-390 \(2003 5] Yan, P., Chen, G., Cornelis, C., De Cock, M., Kerre, E.E.: Mining Positive and Negative Fuzzy Association Rules. In: KES, pp. 270-276 Springer \(2004 6] De Cock, M., Cornelis, C., Kerre, E.E.: Elicitation of fuzzy association rules from positive and negative examples. Fuzzy Sets and Systems, 149, 7385 \(2005 7] Verlinde, H., De Cock, M., Boute, R.: Fuzzy Versus Quantitative Association Rules: A Fair Data-Driven Comparison. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics 36, 679-683 \(2006 8] Dubois, D., Hllermeier, E., Prade, H.: A systematic approach to the assessment of fuzzy association rules. Data Min. Knowl. Discov., 13 167-192 \(2006 9] Dubois, D., Hllermeier, E., Prade, H.: A Note on Quality Measures for Fuzzy Association Rules. In: IFSA, pp. 346-353. Springer-Verlag 2003 10] Hllermeier, E., Yi, Y.: In Defense of Fuzzy Association Analysis IEEE Transactions on Systems, Man, and Cybernetics - Part B Cybernetics, 37, 1039-1043 \(2007 11] Agrawal, R., Imielinski, T., Swami, A.N.: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Record, 22, 207216 \(1993 12]  Agrawal, R., Srikant, R.: Fast Algorithms for Mining Association Rules. In: VLDB, pp. 487-99. Morgan Kaufmann \(1994 13] Han, J., Pei, J., Yin, Y.: Mining Frequent Patterns without Candidate Generation. In: SIGMOD Conference, pp. 1-12. ACM Press \(2000 14] Han, J., Pei, J., Yin, Y., Mao, R.: Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach. Data Mining and Knowledge Discovery, 8, 5387 \(2004 15] Pudi V., Haritsa J.R.: ARMOR: Association Rule Mining based on Oracle. CEUR Workshop Proceedings, 90 \(2003 16] Dunn, J. C.: A Fuzzy Relative of the ISODATA Process and its Use in Detecting Compact, Well Separated Clusters. J. Cyber., 3, 32-57 1974 17] Hoppner, F., Klawonn, F., Kruse, R, Runkler, T.: Fuzzy Cluster Analysis, Methods for Classification, Data Analysis and Image Recognition. Wiley, New York \(1999 


18] Bezdek J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Kluwer Academic Publishers, Norwell, MA \(1981 19] Fu, A.W., Wong, M.H., Sze, S.C., Wong, W.C., Wong, W.L., Yu W.K. Finding Fuzzy Sets for the Mining of Fuzzy Association Rules for Numerical Attributes. In: IDEAL, pp. 263-268. Springer \(1998 20] Kaya, M., Alhajj, R., Polat, F., Arslan, A: Efficient Automated Mining of Fuzzy Association Rules. In: DEXA, pp. 133-142. Springer \(2002 21] Mangalampalli, A., Pudi, V. Fuzzy Association Rule Mining Algorithm for Fast and Efficient Performance on Very Large Datasets In FUZZ-IEEE, pp. 1163-1168. IEEE \(2009 22] Kaya, M., Alhajj. Integrating Multi-Objective Genetic Algorithms into Clustering for Fuzzy Association Rules Mining. In ICDM, pp. 431434. IEEE \(2004  Table II. Algorithm, numerical attribute comparison based on speed \(seconds  Algorithm Age - 91 Hours - 100 Income3 - 4949 Income2 - 13707 Income1 - 55089 FCM 0.27 0.3 3.13 6.28 79.4 CURE 0.25 0.25 28.67 163.19 3614.13 CLARANS 1.3 1.34 8363.53 78030.3 Table III. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions Age Old Middle Aged Young Hours More Average Less Income1 High Medium Low Income2 High Medium Low Income3 High Medium Low  Table IV. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions AGE Very old Around 25 Around 50 Around 65 Around 35 HOURS Very High Zero Around 40 Around 25 INCHEAD Very less Around 30K Around 50K Around 100K INCFAM Around 60K Around 152K Around 96K Around 31K Around 8K TAXINC Around 50K Around 95K Around 20K Very less FTAX Around 15K Very less Around 6K Very high Around 33K  


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


