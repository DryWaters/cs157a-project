Finding Interesting Associations without Support Pruning Edith Cohen 003 Mayur Datar y Shinji Fujiwara z Aristides Gionis x Piotr Indyk  Rajeev Motwani k Jeffrey D Ullman 003\003 Cheng Yang yy Abstract Association-rule mining has heretofore relied on the condition of high support to do its work ef\256ciently In particular the well-known a-priori algorithm is only effective when the only rules of interest are relationships that occur very frequently However there are a number of applications such as data mining identi\256cation of similar web documents clustering and collaborative 256ltering where the rules of in 
terest have comparatively few instances in the data In these cases we must look for highly correlated items or possibly even causal relationships between infrequent items We develop a family of algorithms for solving this problem employing a combination of random sampling and hashing techniques We provide analysis of the algorithms developed and conduct experiments on real and synthetic data to obtain a comparative performance analysis 1 Introduction A prevalent problem in large-scale data mining is that of association-rule mining  256rst introduced by Agrawal Imielinski and Swami 1  T h is c h a lle n g e is s o m e tim e s referred to as the market-basket problem due to its origins in the study of consumer purchasing patterns in retail stores 
although the applications extend far beyond this speci\256c 003 AT&T Shannon Lab Florham Park NJ y Department of Computer Science Stanford University Supported by School of Engineering Fellowship and NSF Grant IIS-9811904 z Hitachi Limited Central Research Laboratory This work was done while the author was on leave at the Department of Computer Science Stanford University x Department of Computer Science Stanford University Supported by NSF Grant IIS-9811904  Department of Computer Science Stanford University Supported by Stanford Graduate Fellowship and NSF Grant IIS-9811904 k Department of Computer Science Stanford University Supported in 
part by NSF Grant IIS-9811904 003\003 Department of Computer Science Stanford University Supported in part by NSF Grant IIS-9811904 yy Department of Computer Science Stanford University Supported by a Leonard J Shustek Fellowship part of the Stanford Graduate Fellowship program and NSF Grant IIS-9811904 setting Suppose we have a relation R containing n tuples over a set of boolean attributes A 1 A 2  A m Let I  f A i 1 A i 2  A i k g and J  f A j 1 
A j 2  A j l g be two sets of attributes We say that I  J is an association rule if the following two conditions are satis\256ed support 320 the set I  J appears in at least an s fraction of the tuples and con\256dence 320 amongst the tuples in which I appears at least a c fraction also have J appearing in them The goal is to identify all valid association rules for a given relation To some extent the relative popularity of this problem can be attributed to its paradigmatic nature the simplicity of the 
problem statement and its wide applicability in identifying hidden patterns in data from more general applications than the original market-basket motivation Arguably though this success has as much to do with the availab ility of a surprisingly ef\256cient algorithm the lack of which has stymied other models of pattern-discovery in data mining The algorithmic ef\256ciency derives from an idea due to Agrawal et al 1 2 c a l l e d a-priori  which exploits the support requirement for association rules The key observation is that if a set of attributes S appears in a fraction s of the tuples then any subset of S also appears in a fraction 
s of the tuples This principle enables the following approach based on pruning to determine a list L k of all k sets of attributes with high support 256rst compute a list L k 000 1 of all  k 000 1 sets of attributes of high support and consider as candidates for L k only those k sets that have all their  k 000 1 subsets in L k 000 1  Variants and enhancements of this approach underlie essentially all known ef\256cient algorithms for computing association rules or their variants Note that in the worst 
case the problem of computing association rules requires time exponential in m  but the a-priori algorithm avoids this pathology on real data sets Observe also that the con\256dence requirement plays no role in the algorithm and indeed is completely ignored until the end-game when the highsupport sets are screened for high con\256dence Our work is motivated by the long-standing open question of devising an ef\256cient algorithm for 256nding rules that have extremely high con\256dence  but for which there is no or extremely weak support For example in market-basket data the standard association-rule algorithms may be useful 


for commonly-purchased i.e high-support items such as 252beer and diapers,\272 but are essentially useless for discovering rules such as that 252Beluga caviar and Ketel vodka\272 are always bought together b ecause there are only a few people who purchase either of the two items We develop a body of techniques which rely on the con\256dence requirement alone to obtain ef\256cient algorithms There are two possible objections to removing the support requirement First this may increase the number of rules that are produced and make it dif\256cult for a user to pin-point the rules of interest But note that in most of the applications described next the output rules are not intended for human analysis but rather for an automated analysis In any case our intent is to seek low-support rules with con\256dence extremely close to 100 the latter will substantially reduce the output size and yet leave in the rules that are of interest Second it may be argued that rules of low support are inherently uninteresting While this may be true in the classical market-basket applications there are many applications where it is essential to discover rules of extremely high con\256dence without regard for support We discuss these applications brie\257y and give some supporting experimental evidence before turning to a detailed description of our results One motivation for seeking such associations of high con\256dence but without any support requirement is that most rules with high support are obvious and well-known and it is the rules of low-support that provide interesting new insights Not only are the support-free associations a natural class of patterns for data mining in their own right they also arise in a variety of applications such as copy detection 320 identifying identical or similar documents and web pages 4 13  clustering 320 identifying similar vectors in high-dimensional spaces for the purposes of clustering data 6 9 a nd collaborative 256ltering 320 tracking user behavior and making recommendations to individuals based on similarity of their preferences to those of other users 8 16  Note that each of these applications can be formulated in terms of a table whose columns tend to be sparse and the goal is to identify column pairs that appear to be similar without any support requirement There are also other forms of data mining e.g detecting causality 15  w h e r e it is important to discover associated columns but there is no natural notion of support We describe some experimental results for one such application mining for pairs of words that occur together in news articles obtained from Reuters The goal was to check whether low-support and high-con\256dence pairs provided any interesting information Indeed the similar pairs proved to be extremely interesting as illustrated by the representative samples provided in Figure 1 A large majority of the output pairs were names of famous international personalities cities terms from medicine and other 256elds phrases Category Examples Dalai Lama Names Meryl Streep Bertolt Brecht Buenos Aires Darth Vader pneumocystis carinii Terminology meseo oceania 256brosis cystic avant garde Phrases mache papier cosa nostra hors oeuvres presse agence encyclopedia Britannica Misc Relations Salman Satanic Mardi Gras emperor Hirohito Figure 1 Examples of different types of similar pairs found in the news articles from foreign languages and other miscellaneous items like author-book pairs and organization names We also obtained clusters of words i.e groups of words in which most pairs have high similarity An example is the cluster  CHESS  T IMMAN K ARPOV S OVIET I VANCHUK P OLGER  which represents a chess event It should be noted that the pairs discovered have very low support and would not be discovered under the standard de\256nition of association rules Of course we can run the a-priori algorithm with very low support de\256nition but this would be really slow as indicated in the running time comparison provided in Section 5 2 Summary of Results The notion of con\256dence is asymmetric or unidirectional and it will be convenient for our purpose to work with a symmetric or bi-directional measure of interest At a conceptual level we view the data as a 0/1 matrix M with n rows and m columns Typically the matrix is fairly sparse and we assume that the average number of 1s per row is r and that r<<m  For the applications we have in mind n could be as much as 10 9  m could be as large as 10 6 and r could be as small as 10 2  De\256ne C i as the set of rows that have a 1 in column c i  also de\256ne the density of column c i as d i  j C i j n Wede\256nethe similarity of two columns c i and c j as S  c i c j  j C i  C j j j C i  C j j  


That is the similarity of c i and c j is the fraction of rows amongst those containing a 1 in either c i or c j  that contain a 1 in both c i and c j  Observe that the de\256nition of similarity is symmetric with respect to c i and c j  in contrast the con\256dence of the rule f c i g c j g is given by Conf  c i c j  j C i  C j j j C i j  To identify all pairs of columns with similarity exceeding a prespeci\256ed threshold is easy when the matrix M is small and 256ts in main memory since a brute-force enumeration algorithm requires O  m 2 n  time We are more interested in the case where M is large and the data is disk-resident In this paper our primary focus is on the problem of identifying all pairs of columns with similarity exceeding a pre-speci\256ed threshold s 003  Restricting ourselves to this most basic version of the problem will enable us clearly to showcase our techniques for dealing with the main issue of achieving algorithmic ef\256ciency in the absence of the support requirement It is possible to generalize our techniques to more complex settings and we discuss this brie\257y before moving on to the techniques themselves It will be easy to verify that our basic approach generalizes to the problem of identifying high-con\256dence association rules on pairs of columns as discussed in Section 6 We omit the analysis and experimental results from this version of the paper as the results are all practically the same as for high-similarity pairs It should be noted that several r ecent papers 3 14 15 have expressed dissatisfaction with the use of con\256dence as a measure of interest for association rules and have suggested various alternate measures Our ideas are applicable to these new measures of interest as well A major restriction in our work is that we only deal with pairs of columns However we believe that it should be possible to apply our techniques to the identi\256cation of more complex rules this matter is discussed in more detail in Section 6 All our algorithms for identifying pairs of similar columns follow a very natural three-phase approach compute signatures  generate candidates and prune candidates  In the 256rst phase we make a pass over the table generating a small hash-signature for each column Our goal is to deal with large-scale tables sitting in secondary memory and this phase produces a 252summary\272 of the table that will 256t into main memory In the second phase we operate in main memory generating candidate pairs from the column signatures Finally in the third phase we make another pass over the original table determining for each candidate pair whether it indeed has high similarity The last phase is identical in all our algorithms while scanning the table data maintain for each candidate column-pair  c i c j  the counts of the number of rows having a 1 in at least one of the two columns and also the number of rows having a 1 in both columns Consequently we limit the ensuing discussion to the proper implementation of only the 256rst two phases The key ingredient of course is the hashing scheme for computing signatures On the one hand it needs to be extremely fast produce small signatures and be able to do so in a single pass over the data Competing with this goal is the requirement that there are not too many falsepositives  i.e candidate pairs that are not really highlysimilar since the time required for the third phase depends on the number of candidates to be screened A related requirement is that there are extremely few ideally none false-negatives  i.e highly-similar pairs that do not make it to the list of candidates In Section 3 we present a family of schemes based on a technique called Min-Hashing MH which is inspired by an idea used by Cohen 5 t o es t i m at e t h e s i ze o f t r an s i t i v e closure and reachab ility sets see also Broder 4   T h e i d e a is to implicitly de\256ne a random order on the rows selecting for each column a signature that consists of the 256rst row index under the ordering in which the column has a 1 We will show that the probability that two columns have the same signature is proportional to their similarity To reduce the probability of false-positives and false-negatives we can collect k signatures by independently repeating the basic process or by picking the 256rst k rows in which the column has 1's The main feature of the Min-Hashing scheme is that for a suitably large choice of k  the number of falsepositives is fairly small and the number of false-negatives is essentially zero A disadvantage is that as k rises the space and time required for the second phase candidate generation increases Our second family of schemes called Locality-Sensitive Hashing LSH  is presented in Section 4 and is inspired by the ideas used by Gionis Indyk and Motwani f or hi ghdimensional nearest neighbors see also Indyk and Motwani 11   T h e b a s ic id e a h e r e is to im p lic itly p a r titio n t h e set of rows computing a signature based on the pattern of 1's of a column in each subtable for example we could just compute a bit for each column in a subtable denoting whether the number of 1's in the column is greater than zero or not This family of schemes suffers from the disadvantage that reducing the number of false-positives increases the number of false-negatives and vice versa unlike in the previous scheme While it tends to produce more falsepositives or false-negatives it has the advantage of having much lower space and time requirements than Min-Hashing We have conducted extensive experiments on both real and synthetic data and the results are presented in Section 5 As expected the experiments indicate that our schemes outperform the a-priori algorithm by nearly an order of magnitude They also illustrate the point made above about the trade-off between accuracy and speed in our two algorithms If it is important to avoid any false-negatives than we recommend the use of the Min-Hashing schemes 


which tend to be slower However if speed is more important than complete accuracy in generating rules than the Locality-Sensitive Hashing schemes are to be preferred We conclude in Section 6 by discussing the extensions of our work alluded to earlier and by providing some interesting directions for future work 3 Min-Hashing Schemes The Min-Hashing scheme used an idea due to Cohen  in the context of estimating transitive closure and reachability sets The basic idea in the Min-Hashing scheme is to randomly permute the rows and for each column c i compute its hash value h  c i  as the index of the 256rst row under the permutation that has a 1 in that column For reasons of ef\256ciency we do not wish to explicitly permute the rows and indeed would like to compute the hash value for each column in a single pass over the table To this end while scanning the rows we will simply associate with each row a hash value that is a number chosen independently and uniformly at random from a range R  Assuming that the number of rows is no more than 2 16  it will suf\256ce to choose the hash value as a random 32-bit integer avoiding the 252birthday paradox\272 12 of ha vi ng t w o r o w s g e t i d e n t i c a l ha s h v a l u e  Furthermore while scanning the table and assigning random hash values to the rows for each column c i we keep track of the minimum hash value of the rows which contain a 1 in that column Thus we obtain the Min-Hash value h  c i  for each column c i in a single pass over the table using O  m  memory Proposition 1 For any column pair  c i c j   Pr  h  c i  h  c j   S  c i c j  j C i  C j j j C i  C j j  This is easy to see since two columns will have the same Min-Hash value if and only if in the random permutation of rows de\256ned by their hash values the 256rst row with a 1 in column c i is also the 256rst row with a 1 in column c j In other words h  c i  h  c j  if and only if in restriction of the permutation to the rows in C i  C j  the 256rst row belongs to C i  C j  In order to be able to determine the degree of similarity between column-pairs it will be necessary to determine multiple say k  independent Min-Hash values for each column To this end in a single pass over the input table we select in parallel k independent hash values for each row de\256ning k distinct permutations over the rows Using O  mk  memory during the single pass we can also determine the corresponding k Min-Hash values say h 1  c j   h k  c j   for each column c j under each of k row permutations In effect we obtain a matrix c M with k rows m columns and c M ij  h i  c j  wherethe k entries in a column are the Min-Hash values for it The matrix c M can be viewed as a compact representation of the matrix M  We will show in Theorem 1 below that the similarity of column-pairs in M is captured by their similarity in c M  De\256nition 1 Let b S  c i c j  be the fraction of Min-Hash values that are identical for c i and c j  i.e b S  c i c j   jf l j 1 024 l 024 k and c M li  c M lj gj k  jf l j 1 024 l 024 k and h l  c i  h l  c j  gj k  We have de\256ned b S  c i c j  as the fraction of rows of b S in which the Min-Hash entries for columns c i and c j are identical We now show that b S  c i c j  is a good estimator of S  c i c j   Recall that we set a threshold s 003 such that two columns are said to be highly-similar if S  c i c j  025 s 003  Assume that s 003 is lower bounded by some constant c  The following theorem shows that we are unlikely to get too many false-positives and false-negatives by using b S to determine similarity of column-pairs in the original matrix M  Theorem 1 Let 0  016  1  017  0  and k 025 2 016 000 2 c 000 1 log 017 000 1  Then for all pairs of columns c i and c j  we have the following two properties a If S  c j c j  025 s 003 025 c then b S  c i c j  025 1 000 016  s 003 with probability at least 1 000 017  b If S  c j c j  024 c then b S  c i c j  024 1  016  c with probability at least 1 000 017  We sketch the proof of the 256rst part of the theorem the proof of the second part is quite similar and is omitted Fix any two columns c i and c j having similarity S  c j c j  025 s 003 Let X l be a random variable that takes on value 1 if h l  c i  h l  c j   and value 0 otherwise de\256ne X  X 1    X k By Proposition 1 E  X l  S  c i c j  025 s 003  therefore E  X  025 ks 003  Applying the Chernoff bound w i t h t h e r a ndom variable X  we obtain that Pr  X 1 000 016  ks 003  024 Pr  X 1 000 016  E  X  024 e 000 016 2 E  X  2 024 e 000 016 2 ks 003 2 024 e 000 016 2 kc 2 017 To establish the 256rst part of the theorem simply notice that b S  c i c j  X=k  Theorem 1 establishes that for suf\256ciently large k iftwo columns have high similarity at least s 003 n M then they agree on a correspondingly large fraction of the Min-Hash values in c M  conversely if their similarity is low at most c n M then they agree on a correspondingly small fraction of the Min-Hash values in c M Since c M can be computed 


in a single pass over the data using O  km  space we obtain the desired implementation of the 256rst phase signature computation We now turn to the task of devising a suitable implementation of the second phase candidate generation 3.1 Candidate Generation from Min-Hash Values Having computed the signatures in the 256rst phase as discussed in the previous section we now wish to generate the candidate column-pairs in the second phase At this point we have a k 002 m matrix c M containing k Min-Hash values for each column Since k<<n  we assume the c M is much smaller than the original data and 256ts in main memory The goal is to identify all column-pairs which agree in a large enough fraction at least to 1 000 016  s 003  of their Min-Hash values in c M  A brute-force enumeration will require O  k  time for each column-pair for a total of O  km 2  Wepresenttwo techniques that avoid the quadratic dependence on m and are considerably faster when as is typically the case the average similarity S  P 1 024 i;j 024 m S  c i c j  m 2 is low Row-Sorting For this algorithm view the rows of c M as a list of tuples containing a Min-Hash value and the corresponding column number We sort each row on the basis of the Min-Hash values This groups together identical MinHash values into a sequence of 252runs.\272 We maintain for each column an index into the position of its Min-Hash value in each sorted row To estimate the similarity of column c i with all other columns we use the following algorithm use m counters for column c i where the j th counter stores the number of rows in which the Min-Hash values of columns c i and c j are identical for each row 1  k  index into the run containing the Min-Hash value for c i  and for each other column represented in this run increment the corresponding counter To avoid O  m 2  counter initializations we re-use the same O  m  counters when processing different columns and remember and re-initialize only counters that were incremented at least once We estimate the running time of this algorithm as follows Sorting the rows requires total time O  km log m   thereafter indexes on the columns can be built in time O  km   The remaining time amounts to the total number of counter increments When processing a row with column c i  the number of counter increments is in fact the length of a run The expected length of a run equals the sum of similarities P m j 1 S  c i c j   Hence the expected counter-increment cost when processing c i is O  k P m j 1 S  c i c j   and the expected combined increments cost is O  k P 1 024 i;j 024 m S  c i c j   O  k Sm 2   Thus the expected total time required for this algorithm is is O  km log m  km 2 S   Note that the average similarity S is typically a small fraction and so the latter term in the running time is not really quadratic in m as it appears to be Hash-Count The next section introduces the K-MinHashing algorithm where the signatures for each column c i is a set S IG i of at most but not exactly k Min-Hash values The similarity of a column-pair  c i c j  is then estimated by computing the size of S IG i  S IG j  clearly it suf\256ces to consider ordered pairs  c j c i  such that j<i  This task can be accomplished via the following hash-c ount algorithm We associate a bucket with each Min-Hash value Buckets are indexed using a hash function de\256ned over the Min-Hash values and store column-indexes for all columns c i with some element of S IG i hashing into that bucket We consider the columns c 1 c 2  c m in order and for column c i we use i 000 1 counters of which the j th counter stores S IG j  S IG i  For each Min-Hash value v 2 S IG i  we access its hash-bucket and 256nd the indexes of all columns c j  j<i  which have v 2 S IG j  For each column c j in the bucket we increment the counter for  c j c i   Finally we add c i itself to the bucket Hash-Count can be easily adapted for use with the original Min-Hash scheme where we instead want to compute for each pair of columns the number of c M rows in which the two columns agree To this end we use a different hash table and set of buckets for each row of the matrix c M andexecute the same process as for K-Min-Hash The argument used for the row-sorting algorithm shows that hash-count for Min-Hashing takes O  k Sm 2  time The running time of Hash-Count for K-Min-Hash amounts to the number of counter increments The number of increments made to a counter  c i c j   is exactly the size of j S IG i  S IG j j Asimple argument see Lemma 1 shows that the expected size E fj S IG i  S IG j jg is between min f k j C i  C j jg S  c i c j  and min f 2 k j C i  C j jg S  c i c j   Thus the expected total running time of the hash-table scheme is O  k Sm 2  in both cases 3.2 The K-Min-Hashing Algorithm One disadvantage of the Min-Hashing scheme outlined above is that choosing k independent Min-Hash values for each column entailed c hoosing k independent hash values for each row This has a negative effect on the ef\256ciency of the signature-computation phase On the other hand using k Min-Hash values per column is essential for reducing the number of false-positives and false-negatives We now present a modi\256cation called K-Min-Hashing K-MH in which we use only a single hash value for each row se tting the k Min-Hash values for each column to be the hash values of the 256rst k rows under the induced row permutation containing a 1 in that column A similar approach was also mentioned in 5 b u t w i t hout a n a n a l ys i s   In ot he r words for each column we pick the k smallest hash values for the rows containing a one in that column If a column c i has fewer 1s than k  we assign as Min-Hash values all 


hash values corresponding to rows with 1s in that column The resulting set of at most k Min-Hash values forms the signature of the column c i and is denoted by S IG i  Proposition 2 In the K-Min-Hashing scheme for any column c i  the signature S IG i consists of the hash values for a uniform random sample of distinct rows from C i  We remark that if the number of 1s in each column is signi\256cantly larger than k  then the hash values may be considered independent and the analysis from Min-Hashing applies The situation is slightly more complex when the columns are sparse which is the case of interest to us Let S IG i  j denote the k smallest elements of C i  C j if j C i  C j j k then S IG i  j  C i  C j  We can view S IG i  j as the signature of the 252column\272 that would correspond to C i  C j  Observe that S IG i  j can be obtained in O  k  time from S IG i and S IG j since it is in fact the set of the smallest k elements from S IG i  S IG j SinceS IG i  j corresponds to a set of rows selected uniformly at random from all elements of C i  C j  the expected number of elements of S IG i  j that belong to the subset C i  C j is exactly j S IG i  j j\002j C i  C j j  j C i  C j j  j S IG i  j j\002 S  c i c j  Also,S IG i  j  C i  C j  S IG i  j  S IG i  S IG j  since the signatures are just the smallest k elements Hence we obtain the following theorem Theorem 2 An unbiased estimator of the similarity S  c i c j  is given by the expression j S IG i  j  S IG i  S IG j j j S IG i  j j  Consider the computational cost of this algorithm While scanning the data we generate one hash value per row and for each column we maintain the minimum k hash values from those corresponding to rows that contain 1 in that column We maintain the k minimum hash values for each column in a simple data structure that allows us to insert a new value smaller than the current maximum and delete the current maximum in O log k  time The data structure also makes the maximum element amongst the k current Min-Hash values of each column readily available Hence the computation for each row is constant time for each 1 entry and additional log k time for each column with 1 entry where the hash value of the row was amongst the k smallest seen so far A simple probabilistic argument shows that the expected number of rows on which the k Min-Hash list of a column c i gets updated is O  k log j C i j  O  k log n  It follows that the total computation cost is a single scan of the data and O  j M j  mk log n log k  where j M j is the number of 1s in the matrix M  In the second phase while generating candidates we need to compute the sets S IG i  j for each column-pair using merge join  O  k  operations and while we are merging we can also 256nd the elements that belong to S IG i  S IG j  Hence the total time for this phase is O  km 2   The quadratic dependence on the number of columns is prohibitive and is caused by the need to compute S IG i  j for each columnpair Instead we 256rst apply a considerably more ef\256cient biased approximate estimator for the similarity The biased estimator is computed for all pairs of columns using HashCount in O  k Sm 2  time Next we perform a main-memory candidate pruning phase where the unbiased estimator of Theorem 2 is explicitly computed for all pairs of columns where the approximate biased estimator exceeds a threshold The choice of threshold for the biased estimator is guided by the following lemma Lemma 1 E fj S IG i  S IG j jg  min f 2 k j C i  C j jg 024 S  c i c j  024 024 E fj S IG i  S IG j jg  min f k j C i  C j jg  Alternatively the biased estimator and choice of threshold can be derived from the following analysis Let  c i c j  be a column-pair with j C i j\025 j C j j de\256ne C ij  C i  C j  As before for each column c i  we choose a set S IG i of k Min-Hash values Let S IG ij  S IG i  C ij and S IG ji  S IG j  C ij  Then the expected sizes of S IG ij and S IG ji are given by k j C ij j  j C i j and k j C ij j  j C j j Also j S IG i  S IG j j  min  j S IG ij j  j S IG ji j   Hence can compute the expected value as E  j S IG i  S IG j j  k X x 0 k X y 0 Pr  j S IG ij j  x  Pr  j S IG ji j  y jj S IG ij j  x  min  x y   Since j C i j  j C j j wehave E  j S IG ij j  024 E  j S IG ji j  We assume that Pr  j S IG ij j  j S IG ji j  031 0 or P k y  x P  j S IG ji j  y jj S IG ij j  x  031 1  Then the above equation becomes E  j S IG i  S IG j j   k X x 0 k X y  x Pr  j S IG ij j  x  Pr  j S IG ji j  y jj S IG ij j  x  x  k X x 0 Pr  j S IG ij j  x  x k X y  x Pr  j S IG ji j  y jj S IG ij j  x   E  S IG ij   Thus we obtain the estimator E  j S IG i  S IG j j  031 k j C ij j  j C i j  We use this estimate to calculate j C ij j and use that to estimate the similarity since we know j C i j and j C j j  We compute j S IG i  S IG j j using the hash table technique that we have described earlier in Section 3.1 The time required to compute the hash values is O  j M j  mk log n log k  as described earlier and the time for computing j S IG i  S IG j j is O  k Sm 2   


4 Locality-Sensitive Hashing Schemes In this section we show how to obtain a signi\256cant improvement in the running time with respect to the previous algorithms by resorting to Locality Sensitive Hashing LSH technique introduced by Indyk and Motwani 11 i n de s i gning main-memory algorithms for nearest neighbor search in high-dimensional Euclidean spaces it has been subsequently improved and tested in 7 W e a ppl y t he L S H framework to the Min-Hash functions described in earlier section obtaining an algorithm for similar column-pairs This problem differs from nearest neighbor search in that the data is known in advance We exploit this property by showing how to optimize the running time of the algorithm given constraints on the quality of the output Our optimization is input-sensitive  i.e takes into account the characteristics of the input data set The key idea in LSH is to hash columns so as to ensure that for each hash function the probab ility of collision is much higher for similar columns than for dissimilar ones Subsequently the hash table is scanned and column-pairs hashed to the same bucket are reported as similar Since the process is probabilistic both false positives and false negatives can occur In order to reduce the former LSH ampli\256es the difference in collision probabilities for similar and dissimilar pairs In order to reduce false negatives the process is repeated a few times and the union of pairs found during all iterations are reported The fraction of false positives and false negatives can be analytically controlled using the parameters of the algorithm Although not the main focus of this paper we mention that the LSH algorithm can be adapted to the on-line framework of 10  I n p a r t i c ul a r  i t f ol l o w s from our a n a l ys i s t h a t each iteration of our algorithm reduces the number of false negatives by a 256xed factor it can also add new false positives but they can be removed at a small additional cost Thus the user can monitor the progress of the algorithm and interrupt the process at any time if satis\256ed with the results produce so far Moreover the higher the similarity the earlier the pair is likely to be discovered Therefore the user can terminate the process when the output produced appears to be less and less interesting 4.1 The Min-LSH Scheme We present now the Min-LSH M-LSH scheme for 256nding similar column-pairs from the matrix c M of Min-Hash values The M-LSH algorithm splits the matrix c M into l sub-matrices of dimension r 002 m  Recall that c M has dimension k 002 m  and here we assume that k  lr  Then for each of the l sub-matrices we repeat the following Each column represented by the r Min-Hash values in the current sub-matrix is hashed into a table using as hashing key the concatenation of all r values If two columns are similar there is a high probability that they agree in all r Min-Hash values and so they hash into the same bucket At the end of the phase we scan the hash table and produce pairs of columns that have been hashed to the same bucket To amplify the probability that similar columns will hash to the same bucket we repeat the process l times Let P r l  c i c j  be the probability that columns c i and c j will hash to the same bucket at least once since the value of P depends only upon s  S  c i c j   we simplify notation by writing P  s   Lemma 2 Assume that columns c i and c j have similarity s and also let s 003 be the similarity threshold For any 0  016 017  1  we can choose the parameters r and l such that 017 For any s 025 1  016  s 003  P r l  c i c j  025 1 000 017 017 For any s 024 1 000 016  s 003  P r l  c i c j  024 017 Proof By Proposition 1 the probability that columns c i  c j agree on one Min-Hash value is exactly s and the probability that they agree in a group of r values is s r If we repeat the hashing process l times the probability that they will hash at least once to the same bucket would be P r l  c i c j  000 1 000 s r  l  The lemma follows from the properties of the function P  Lemma 2 states that for large values of r and l  the function P approximates the unit step function translated to the point C  s 003  which can be used to 256lter out all and only the pairs with similarity at most s 003  On the other hand the time/space requirements of the algorithm are proportional to k  lr  so the increase in the values of r and l is subject to a quality-ef\256ciency trade-off In practice if we are willing to allow a number of false negatives  n 000 ndfalse positives  n   we can determine optimal values for r and l that achieve this quality Speci\256cally assume that we are given an estimate of the similarity distribution of the data de\256ned as d  s i  to be the number of pairs having similarity s i  This is not an unreasonable assumption since we can approximate this distribution by sampling a small fraction of columns and estimating all pairwise similarity The expected number of false negatives would be P s i 025 s 0 d  s i  1 000 P  s i   and the expected number of false positives would be P s i s 0 d  s i  P  s i  Therefore the problem of estimating optimal parameters turns into the following minimization problem minimize l 001 r subject to 032P s i 025 s 0 d  s i  1 000 P  s i  024 n 000 P s i s 0 d  s i  P  s i  024 n  This is an easy problem since we have only two parameters to optimize and their feasible values are small integers Also the histogram d  001  is typically quanti\256ed in 10-20 bins 


One approach is to solve the minimization problem by iterating on small values of r  256nding a lower bound on the value of l by solving the 256rst inequality and then performing binary search until the second inequality is satis\256ed In most experiments the optimal value of r was between 5 and 20 4.2 The Hamming-LSH Scheme We now propose another scheme Hamming-LSH HLSH for 256nding highly-similar column-pairs The idea is to reduce the problem to searching for column-pairs having small Hamming distance  In order to solve the latter problem we employ the techniques similar to those used in 7 t o solve the nearest neighbor problem We start by establishing the correspondence between the similarity and Hamming distance the proof is easy Lemma 3 S  C i C j  j C i j  j C j j\000 d H  c i c j  j C i j  j C j j  d H  c i c j   It follows that when we consider pairs  c i c j  such that the sum 032  j C i j  j C j j is 256xed then the high value of S  c i c j  corresponds to small values of d H  c i c j  and vice versa Hence we partition columns into groups of similar density and for each group we 256nd pairs of columns that have small Hamming distance First we brie\257y describe how to search for pairs of columns with small Hamming distance This scheme is similar to to the technique from  a n d c a n be analyzed using the tools developed in there This scheme 256nds highly-similar columns assuming that the density of all columns is roughly the same This is done by partitioning the rows of database into p subsets For each par tition process as in the previous algorithm We declare a pair of columns as a candidate if they agree on any subset Thus this scheme is exactly similar to the earlier scheme except that we are dealing with the actual data instead of Min-Hash values However there are two problems with this scheme One problem is that if the matrix is sparse most of the subsets just contain zeros and also the columns do not have similar densities as assumed The following algorithm which we call H-LSH  improves on the above basic algorithm The basic idea is as follows We perform computation on a sequence of matrices with increasing densities we denote them by M 0 M 1 M 2   The matrix M i 1 is obtained from the matrix M i by randomly pairing all rows of M i  and placing in M i 1 the 252OR\272 of each pair 1 One can see that for each i  M i 1 contains half the rows of M i for illustration purposes we assume that the initial number of rows is a power of 2 The algorithm is applied to all matrices 1 Notice that the 252OR operation\272 gives similar results to hashing each columns to a set of increasingly smaller hash table this provides an alternative view of our algorithm in the set A pair of columns can become a candidate only on a matrix M i in which they are both suf\256ciently dense and both their densities belong to a certain range False negatives are controlled by repeating each sample l times and taking the union of the candidate sets across all l runs Hence kr rows are extracted from each compressed matrix Note that this operation may increase false positives We now present the algorithm that was implemented Experiments show that this scheme is better than the MinHashing algorithms in terms of running time but the number of false positives is much larger Moreover the number of false positives is increases rapidly if we try to reduce the number of false negatives In the case of Min-Hashing algorithms if we decreased the number of false negatives by increasing k  the number of false positives would also decrease The Algorithm 1 Set M 0  M and generate M 1 M 2  as described above 2 For each i 025 0  select k sets of r sample rows from M i  3 A column pair is a candidate if there exists an i such that i the column pair has density in 1 t  t 000 1 t  in M i  and ii has identical hash values essentially identical r bit representations in at least one of the k runs Note that t is a parameter that indicates the range of density for candidate pairs and we use t 4 in our experiments 5 Experiments We have conducted experiments to evaluate the performance of the different algorithms In this section we report the results for the different experiments We use two sets of data namely synthetic data and real data Synthetic Data The data contains 10 4 columns and the number of rows vary from 10 4 to 10 6  The column densities vary from 1 to 5 and for every 100 columns we have a pair of similar columns We have 20 pairs similar columns whose similarity fall in the ranges 85 95 75 85 65 75 55 65 and 45 55 Real Data The real data set consists of the log of HTTP requests made over a period of 9 days to the Sun Microsystems Web server  www.sun.com  The columns in this case are the URL's and the rows represent distinct client IP addresses that have recently accessed the server An entry is set to 1 if there has been at least one hit for that URL from that particular client IP The data set has about thirteen thousand columns and more than 0.2 million rows Most of the 


 0 20 40 60 80 100 0 0.5 1 1.5 2 2.5 3 3.5 4 x 10 7 Number Similar pairs Similarity Histogram of Sun data set 60 65 70 75 80 85 90 95 100 0 1 2 3 4 5 6 7 8 9 10 x 10 4 Number Similar pairs Similarity Histogram of Sun data set Figure 2 The 256rst 256gure shows the similarity distribution of the Sun data The second shows again the same distribution but it focuses on the region of similarities that we are interested in columns are sparse and have density less than 0.01 The histogram in Figure 2 shows the number of column pairs for different values of similarity Typical examples of similar columns that we extracted from this data were URLs corresponding to gif images or Java applets which are loaded automatically when a client IP accesses a parent URL To compare our algorithms with existing techniques we implemented and executed the a-priori algorithm 1 O f course the a-priori is not designed for this setting of low support but it is the only existing technique and gives us a benchmark against which we can compare the improvements afforded by our algorithms The comparison was done for the news articles data that we have mentioned in Section 1 We conducted experiments on the news article data and our results are summarized in Figure 3 The a-priori algorithm cannot be run on the original data since it runs out of memory Therefore we performed support pruning to remove columns that have very few ones in them It is evident that our techniques give nearly an order of magnitude improvement in running time for support threshold below 0  1  a-priori runs out of memory on our systems and does a lot of thrashing Note that although our algorithms are probabilistic they report the same set of pairs as reported by a-priori 5.1 Results We implemented the four algorithms described in the previous section namely MH K-MH H-LSH and M-LSH All algorithms were compared in terms of the running time and the quality of the output Due to the lack of space we report experiments and give graphs for the Sun data which in any case are more interesting but we have also performed tests for the synthetic data and all algorithms behave similarly The quality of the output is measured in terms of false Support threshold 0  1 0  15 0  2 Number of columns after pruning 15559 11568 9518 A-priori sec 96.05 79.94 MH sec 71.4 44.8 25.8 K-MH sec 87.6 52.0 36.0 H-LSH sec 15.6 6.7 6.0 M-LSH sec 10.7 9.7 5.1 Figure 3 Running times for the news articles data set positives and false negatives generated by each algorithm To do that we plot a curve that shows the ratio of the number of pairs found by the algorithm over the real number of pairs computed once off-line for a given similarity range e.g Figure 7 The result is typically an 252S\272-shaped curve that gives a good visual picture for the false positives and negatives of the algorithm Intuitively the area below the curve and left to a given similarity cutoff corresponds to the number of false positives while the area above the curve and right to the cutoff corresponds to the number of false negatives We now describe the behavior of each algorithm as their parameters are varied    55 65 75 85 95 0 0.2 0.4 0.6 0.8 1    Fraction of pairs found Similarity Performance of MH on Sun data set, f=80 k = 10 k = 20   k = 50   k = 100   k = 200   10 20 50 100 200 500 200 400 600 800 1000 1200 1400 1600 1800 K Value Total time \(sec Running time of MH on Sun data set,f=80 a b   55 65 75 85 95 0 0.2 0.4 0.6 0.8 1    Fraction of pairs found Similarity Performance of MH on Sun data set,k=500 f = 70 f = 75   f = 80   f = 85   f = 90   70 75 80 85 90 1750 1800 1850 1900 1950 F Value Total time \(sec Running time of MH on Sun data set,k=500 c d Figure 4 Quality of output and total running time for MH algorithm as k and s are varied MH and K-MH algorithms have two parameters s 003 the 


   55 65 75 85 95 0 0.2 0.4 0.6 0.8 1     Fraction of pairs found Similarity Performance of K-MH on Sun data set,f=80 k = 10 k = 20   k = 50   k = 100   k = 200     10 20 50 100 200 500 100 150 200 250 K Value Total time \(sec Running time of K-MH on Sun data set,f=80 a b   55 65 75 85 95 0 0.2 0.4 0.6 0.8 1     Fraction of pairs found Similarity Performance of K-MH on Sun data set,k=500 f = 70 f = 75   f = 80   f = 85   f = 90     70 75 80 85 90 200 210 220 230 240 250 260 270 280 290 300 F Value Total time \(sec Running time of K-MH on Sun data set,k=500 c d Figure 5 Quality of output and total running time for K-MH algorithm as k and s are varied user speci\256ed similarity cutoff and k  the number of MinHash values extracted to represent the signature of of each column Figures 4\(a and 5\(a plot 252S\272-curves for different values of k for the MH and K-MH algorithms As the k value increases the curve gets sharper indicating better quality In Figures 4\(c and 5\(c we keep k 256xed and change the value s 003 of the similarity cutoff As expected the curves shift to the right as the cutoff value increases Figures 4\(d and 5\(d show that for a given value of k the total running time decreases marginally since we generate fewer candidates Figure 4\(b shows that the total running time for MH algorithm increases linearly with k However this is not the case for K-MH algorithm as depicted by Figure 5\(b The sub-linear increase of the running time is due to the sparsity of the data More speci\256cally the number of hash values extracted from each column is upper bounded by the number of ones of that column and therefore the hash values extracted do not increase linearly with k  We do a similar exploration of the parameter space for the M-LSH and H-LSH algorithms The parameters of this algorithm are r and l  Figures 7\(a and 6\(a illustrate the fact that as r increases the probability that columns mapped to the same bucked decreases and therefore the number of false positives decreases but as a trade-off consequence the number of false negatives increases On the other hand Figure 7\(c and 6\(c shows that an increase in l  corresponds to an increase of the collision probability and therefore the   55 65 75 85 95 0 0.2 0.4 0.6 0.8 1   Similarity Fraction of pairs found Performance of H-LSH on Sun data set, l = 8 r = 32 r = 40   r = 48 r = 56     32 40 48 56 60 80 100 120 140 160 180 200 220 Value of parameter r Total time \(sec Running time of H-LSH on Sun data set, l = 8 a b   55 65 75 85 95 0 0.2 0.4 0.6 0.8 1   Similarity Fraction of pairs found Performance of H-LSH on Sun data set, r = 40 l = 4 l = 8   l = 16 l = 32     4 8 16 32 90 100 110 120 130 140 Value of parameter l Total time \(sec Running time of H-LSH on Sun data set, r = 40 c d Figure 6 Quality of output and total running time for H-LSH algorithm as r and l are varied number of false negatives decrease but the number of false positives increases Figures 7\(d and 6\(d show that the total running time increases with l since we hash each column more times and this also results in an increase in the number of candidates In our implementation of M-LSH the extraction of min hash values dominates the total computation time which increases linearly with the value of r  This is shown in Figure 7\(b On the other hand in the implementation of H-LSH checking for candidates dominates the running times and as a result the total running time decreases as r increases since less candidates are produced This is shown in Figure 6\(b We now compare the different algorithms that we have implemented When comparing the time requirements of the algorithm we compare the CPU time for each algorithm since the time spent in I/O is same for all the algorithms It is important to note that the for all the algorithms the number of false negatives is very important and this is the quantity that requires to be kept in control As long as the number of false positives is not too large i.e all of candidates can 256t in main memory we can always eliminate them in the pruning phase To compare the algorithms we 256x the percentage of false negatives that can be tolerated For each algorithm we pick the set of parameters for which the number of false negatives is within this threshold and the total running time is minimum We then plot the total running time and the number of false positives against the false negative threshold 


   55 65 75 85 95 0 0.2 0.4 0.6 0.8 1   Fraction of pairs found Similarity Performance of M-LSH on Sun data set, l = 10 r = 10 r = 20   r = 50 r = 100     10 20 50 100 0 50 100 150 200 250 300 350 400 450 500 Total time \(sec Value of parameter r Running time of M-LSH on Sun data set, l = 10 a b   55 65 75 85 95 0 0.2 0.4 0.6 0.8 1   Fraction of pairs found Similarity Performance of M-LSH on Sun data set, r = 10 l = 10 l = 20   l = 50 l = 100     10 20 50 100 0 50 100 150 200 250 300 350 400 450 500 550 time \(sec Value of parameter l Running time of M-LSH on Sun data set, r = 10 c d Figure 7 Quality of output and total running time for M-LSH algorithm as r and l are varied Consider Figures 8\(a and 8\(c The Figures show the total running time against the false negative threshold We can see that the H-LSH algorithm requires a lot of time if the false negative threshold is less while it does better if the limit is high In general the M-LSH and H-LSH algorithms do better than the MH and K-MH algorithms However it should be noted that H-LSH algorithm cannot be used if we are interested in similarity cutoffs that are low The graph shows that the best performance is shown by the M-LSH algorithm Figure 8 gives the number of false positives generated by the algorithms against the tolerance limit The false positives are plotted on a logarithmic scale In case of H-LSH and M-LSH algorithms the number of false positives decreases if we are ready to tolerate more false negatives since in that case we hash every column fewer times However the false positive graph for K-MH and MH is not monotonic There exists a tradeoff in the time spent in the candidate generation stage and the pruning stage To maintain the number of false negatives less than the given threshold we could either increase k and spend more time in the candidate generation stage or else decrease the similarity cutoff s and spend more time in the pruning stage as we get more false positives Hence the points on the graph correspond to different values of similarity cutoff s 003 with which the algorithms are run to get candidates with similarity above a certain threshold As a result we do not observe a monotonic behavior in case of 0.01 0.05 0.1 0.5 1 5 10 0 50 100 150 200 250 300 350 400 450 500     Total time \(sec False Negative Threshold Time vs False Negatives, Similarity = 85 MH K-MH   H-LSH   M-LSH 0.01 0.05 0.1 0.5 1 5 10 10 4 10 5     False Negative Threshold False Positives vs False Negatives, Similarity = 85 MH K-MH   H-LSH   M-LSH a b 0.01 0.05 0.1 0.5 1 0 20 40 60 80 100 120 140 160 180 200     Total time \(sec False Negative Threshold Time vs False Negatives, Similarity = 95 MH K-MH   H-LSH   M-LSH 0.01 0.05 0.1 0.5 1 10 4 10 5     Number of False positives False Negative Threshold False Positives vs False Negatives, Similarity = 95 MH K-MH   H-LSH   M-LSH c d Figure 8 Comparison of different algorithms in terms of total running time and number of false positives for different negative thresholds these algorithms We would like to comment that the results provided should be analyzed with caution The reader should note that whenever we refer to time we refer to only the CPU time and we expect I/O time to dominate in the signature generation phase and pruning phase If we are aware about the nature of the data then we can be smart in our choice of algorithms For instance the K-MH algorithm should be used instead of MH for sparse data sets since it takes advantage of sparsity 6 Extensions and Further Work We brie\257y discuss some extensions of the results presented here as well as directions for future work First note that all the results presented here were for the discovery of bi-directional similarity measures However the Min-Hash technique can be extended to the discovery of column-pairs  c i c j  which form a high-con\256dence association rule of the type f c i g c j g but without any support requirements The basic idea is to generate a set of Min-Hash values for each column and to determine whether the fraction of these values that are identical for c i and c j is proportional to the ratio of their densities d i d j  The analytical and the experimental results are qualitatively the same as for similar 


column-pairs We can also use our Min-Hashing scheme to determine more complex relationships e.g c i is highly-similar to c j _ c j 0  since the hash values for the induced column c j _ c j 0 can be easily computed by taking the component-wise minimum of the hash value signature for c j and c j 0  Extending to c j  c j 0 is more dif\256cult It works as follows First observe that 252 c i implies c j  c j 0 272 means that 252 c i implies c j 272and\252 c i implies c j 0 272 The latter two implications can be generated as above Now we can conclude that 252 c i implies c j  c j 0 272if and only if the cardinality of c i is roughly that of c j  c j 0  This presents problems when the cardinality of c i is really small but is not so dif\256cult otherwise The case of small c i may not be very interesting anyway since it is dif\256cult to associate any statistical signi\256cance to the similarity in that case It is also possible to de\256ne 252anti-correlation,\272 or mutual exclusion between a pair of columns However for statistical validity this would require imposing a support requirement since extremely sparse columns are likely to be mutually exclusive by sheer chance It is interesting to note that our hashing techniques can be extended to deal with this situation unlike a-priori which will not be effective even with support requirements Extensions to more than three columns and complex boolean expressions are possible but will suffer from an exponential overhead in the number of columns References 1 R  A g r a w a l  T  I m i e lin s k i a n d A  S w a m i  M in in g Association Rules Between Sets of Items in Large Databases In Proceedings of the ACM SIGMOD Conference 1993 pp 207\261216  R  A gra w a l a n d R  S ri ka nt  F a s t A l gori t h m s for M i n i n g Association Rules In Proceedings of the 20th International Conference on Very Large Databases 1994  S  B ri n R Mot w a n i  J  D  U l l m a n  a nd S  T s ur  D ynamic itemset counting and implication rules for market basket data In Proceedings of the ACM SIGMOD Conference on Management of Data 1997 pp 255\261 264 4 A  B r o d e r  O n t h e r es emb l an ce an d c o n t ai n m en t o f documents In Compression and Complexity of Sequences SEQUENCES'97  1998 pp 21\26129 5 E  C o h e n  S i z e E s tim a tio n F r a m e w o r k w ith A p p lic a tions to Transitive Closure and Reachab ility Journal of Computer and System Sciences 55 1997 441\261453 6 R O  D u d a an d P E H ar t  Pattern Classi\256cation and Scene Analysis  A Wiley-Interscience Publication New York 1973  A  G i oni s  P  Indyk a n d R  M ot w a ni  S i m i l a ri t y S e a r c h in High Dimensions via Hashing In Proceedings of the 25th International Conference on Very Large Data Bases  1999 pp 518\261529  D  G ol dbe r g  D  N i c hol s  B M O k i  a n d D  T e rry  Using collaborative 256ltering to weave an information tapestry Communications of the ACM 55 1991 1\261 19  S  G uha  R  R a s t ogi  a nd K  S h i m  C U R E A n E f 256cient Clustering Algorithm for Large Databases In Proceedings of the ACM-SIGMOD Internati onal Conference on Management of Data 1998 pp 73\26184 10 J  M  H e lle r s te in  P  J  H a a s  a n d H  J  W a n g  O n lin e A g gregation In Proceedings of the ACM-SIGMOD International Conference on Management of Data  1997  P  Indyk a n d R  M ot w a ni  A pproxi m a t e N e a r e s t N e i ghbor Towards Removing the Curse of Dimensionality In Proceedings of the 30th A nnual ACM Symposium on Theory of Computing 1998 pp 604\261613  R Mot w a n i a nd P  Ra gha v a n Randomized Algorithms Cambridge University Press 1995 13 N  S h iv ak u m ar an d H  G ar ci aM o l i n a B u i l d i n g a S cal able and Accurate Copy Detection Mechanism In Proceedings of the 3rd Internati onal Conference on the Theory and Practice of Digital Libraries  1996  C S i l v e r s t e i n S  Bri n  a nd R Mot w a n i  Be yond Ma r ket Baskets Generalizing Association Rules to Dependence Rules In Proceedings of the ACM SIGMOD Conference on Management of Data 1997 pp 265\261 276 In Data Mining and Knowledge Discovery 2 1998 69\26196  C S i l v e r s t e i n S  Bri n  R  M ot w a ni  a nd J  D  U l l m a n  Scalable Techniques for Mining Causal Structures In Proceedings of the 24th Internati onal Conference on Very Large Data Bases  1998 pp 594\261605 16 H  R  V a r i an an d P  R es n i ck  E d s  C A C M S p eci al I s s u e on Recommender Systems Communications of the ACM 40 1997 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


