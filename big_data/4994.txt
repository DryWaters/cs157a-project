Large-scale graph analytics is a central tool in many fields, and exemplifies the size and complexity of Big Data applications. Recent distributed graph processing frameworks utilize the venerable model and promise scalability for large graph analytics. This has been made popular by Googleês Pregel, which provides an architecture design for BSP graph processing. Public clouds offer democra 
Abstract Bulk Synchronous Parallel \(BSP 
Optimizations and Analysis of BSP Graph Processing Models on Public Clouds  Mark Redekopp, Yogesh Simmhan, and Viktor K. Prasanna University of Southern California, Los Angeles CA 90089 redekopp, simmhan, prasanna}@usc.edu   
tized access to medium-sized compute infrastructure with the promise of rapid provisioning with no capital investment. Evaluating BSP graph frameworks on cloud platforms with their unique constraints is less explored Here, we present optimizations and analyses for computationally complex graph analysis algorithms such as betweenness-centrality and all-pairs shortest paths on a native BSP framework we have developed for the Microsoft Azure Cloud, modeled on the Pregel graph processing model. We propose novel heuristics for scheduling graph vertex processing in swaths to maximize resource utilization on cloud VMs that lead to a 3.5x performance improvement. We explore the effects of 
graph partitioning in the context of BSP, and show that even a well partitioned graph may not lead to performance improvements due to BSP's barrier synchronization. We end with a discussion on leveraging cloud elasticity for dynamically scaling the number of BSP workers to achieve a better performance than a static deployment, and at a significantly lower cost 
I I NTRODUCTION  The Big Data paradigm refers not just to the size of data but also its complexity. Graph data structures and 
Keywords- Graph analytics; Cloud computing; Pregel MapReduce; Bulk Synchronous Parallel; Betweennness centrality 
 
algorithms exemplify these challenges. Large-scale graph datasets with billions of vertices are becoming common in the context of social networks and webscale datasets. Graph analytics attempt to extract useful information such as central vertices or clusters from the structure of the graph and related attributes and is an important tool for epidemiology [1   pr o tein inte ra cti o ns  2  an d ev en e c o l og i c a l c o nnectivity  T h e m a jo r ity of these datasets exhibit small-world properties [4 ith small average diameter and high clustering.  Recognizing the importance of scalable graph analytics in both scientific and commercial domain, many new parallel and distributed graph processing approaches [5    have emerged in the academic and open source 
communities. These frameworks often provide a simplified programming abstraction \(e.g. vertex-centric or MapReduce\o aid programmers in mapping algorithms while providing scalability.  Meanwhile, the frameworks themselves partition the graph data across machines, handle synchronization and communication and provide a simplified user interface to the system At the same time public cloud computing services that offer either compute infrastructure-as-a-service IaaS\r a software platform-as-a-service \(PaaS\ have gained popularity. The ability to rapidly provision compute resources on-demand promises cost scalability based on utilization and democratizes resource access for the çlong tailé of data-driven science.  Popular 
commercial/public cloud service providers include Amazon EC2 1 and Microsoft Azure 2 However, these benefits come with the overhead associated with 
infrastructure platform 
virtualization on commodity hardware execution in a controlled environment, and the inability to control exact VM placement and thus communication latency/bandwidth. In addition, multitenancy impacts performance consistency. Users choosing a commercial, public cloud service may want to trade dollar cost against performance or even reliability. Attaching a real monetary cost to public cloud resources also limits their typical usage for 
scientific computing to 10-100 s of cores rather than the 1000ês of cores, typical of a private/academic HPC center 
Thus public clouds fit the scalability and manageability sweet-spot for scientific applications with resource needs that fall beyond a single large server and below HPC/supercomputing resources 
8  One of the key gaps in literature is an evaluation of graph processing frameworks on public cloud platforms. While these frameworks may offer a certain scalability on HPC clusters w ith fast interconnect   the i r be ha vi o r o n vi r t ua l i z ed c o m m o d ity  ha rd w a r e  that is accessible to a wider population of users is less 
understood. MapReduce \(MR  h a s b e e n w e ll explored for graph processing  d e s p i t e i t s or igins in  processing tuple-based datasets, due to its pervasiveness in large-scale data processing. Of particular novel interest is the Bulk Synchronous Parallel \(BSP\[1  model that has seen a revival of-late for 
vertex-centric 
  1 Amazon Web Services. http://aws.amazon.com 2 Microsoft Azure. http://www.microsoft.com/windowsazure  
2013 IEEE 27th International Symposium on Parallel & Distributed Processing 1530-2075/13 $26.00 © 2013 IEEE DOI 10.1109/IPDPS.2013.76 203 


adaptive and generalizable vertex and superstep scheduling heuristics a priori analyze the impact of graph partitioning for BSP frameworks  we present a model for exploiting the elasticity of clouds to adaptively scale the number of VMs based on demand A Large-scale graph processing frameworks MapReduce map reduce do not implicitly share state Bulk Synchronous Parallel & Pregel vertex-centric supersteps 
graph processing, championed by Googleês Pregel. BSP uses the notion of independent tasks that run concurrently within a superstep, synchronize and exchange messages at the end of a superstep, and then start the next superstep. In mapping this to graph processing Pregel associates graph partition\(s\ with each machine a user logic with every graph vertex that runs as a task in a superstep and allows the user logic to exchange messages with other vertices \(usually to neighboring vertices\ at superstep boundaries Open source BSP frameworks for graph processing such as Apache Hama [12 an d Gi ra ph 3 are still evolving. As a result, the performance characteristics of BSP, for different varieties of graph algorithms and on public clouds, are less known. Furthermore, these open implementations, while offering a BSP programming model, rely on the Hadoop MapReduce framework for their execution rather than use a native execution model suited for BSP. As a result, optimization approaches for graph processing using the BSP model, and an analysis of their performance, are harder to narrow down and evaluate without Hadoopês side-effects In this paper, we make three primary contributions One, we introduce  to the BSP graph processing model that alleviate the burden of correctly predicting appropriate resource provisioning levels These execution heuristics are incorporated into our own .NET implementation of a BSP distributed graph processing framework, based on Pregelês programming abstractions, which natively runs on the Microsoft Azure Cloud platform. At the same provisioning level, our evaluations show that our heuristics can achieve up to 3.5x speedup. Second, we showing that while partitioning can be effective at reducing inter-worker communication it can also be a source of load imbalance th at can cancel the positive effects of partitioning on BSP frameworks Finally and show the promise of this technique for BSP graph processing.  Using experimental results on statically provisioned VMs our extrapolation shows that this technique offers better runtimes than even statically over-provisioned VMs and at a reduced monetary cost In all of our work we use a classical betweennesscentrality \(BC\ algorithm over several real çsmallworldé network datasets as a stress case for both the BSP model and cloud platform.  We also include results from PageRank as a baseline and All Pairs Shortest Path \(APSP\or select comparisons.  Algorithmic and programmatic optimizations of these graph applications are non-goals  3 http://incubator.apache.org/giraph Our work is scoped to evaluate the efficacy of using BSP to scale graph problems beyond a single local server into the cloud to benefit a broad category of researchers rather than offer comparisons with problems that run on HPC-class clusters. Further, we recognize that the real monetary cost of running applications on public clouds will limit their use to medium-scale graph problems II B ACKGROUND AND R ELATED W ORK  Many distributed programming frameworks exist to process large datasets. These offer programming abstractions for developers to describe their algorithms and operations, and map them to the underlying distributed compute fabric for scalable and efficient execution and its variants such as iterative MapReduce\ is arguably the most pervasive large-scale data processing model with its numerous implementations  14  15  1 6]. Algo r i thm s a r e de scr i b e d in  two user kernels and The concurrent Map tasks transform input data to intermediate key-value tuples that are then grouped by key and aggregated by concurrent Reduce tasks. The framework handles the scheduling of Map and Reduce tasks, data storage and distribution between tasks through a distributed file system, and coordination of compute hosts. While MapReduce provides efficient, scalable operation for a large class of tuple-based data-intensive applications, it is less efficient for graph processing [5 17   T h is is  due to the fact that Map and Reduce tasks between tasks or across iterations Thus, graph algorithms represented using MapReduce e.g. [10  at ru n f o r m an y iterat i on s in cu r th e overhead associated with communicating the graph structure to Map or Reduce tasks at each iteration Several graph processing frameworks have been proposed recently to specifically address these s hortcom  7   M a ny  use a programming abstraction where computation is described for a generic vertex that operates independently and communicates with other vertices through \(bulk\ message-passing at synchronization points called 11 T h e fra m e w o r k  coordinates data and message distribution, mapping vertex tasks to compute hosts, and coordination Further, these frameworks implicitly distribute the graph structure across hosts and provide constructs for the user program to access this structure. An example of this BSP approach is shown in the inset of Figure 1 In this space, Googleês Pregel [5 a s p r o pos ed a  tightly-coupled parallel abstraction for graph processing using explicit message-based communication. While the Pregel framework itself is proprietary, early public 
 
 
204 


all Other frameworks disk-based memory-based MapReduce Bulk Synchronous Parallel BSP large, shared memory processors or a loosely-coupled execution model B Graph Algorithm and Communication Patterns 
implementations of this model include Apache Hama   G i r ap h Go ld e n Orb 4 and, recently, GPS  T h e former two actually use Hadoop MapReduce as their execution model; while not specifically designed for clouds, they can possibly run on EC2. Pregel distributes a graph's vertices across the available compute hosts with applications explicitly passing data messages along the edges of the graph.  It leverages the BSP approach by aggregating outgoing message traffic and delivering it by the end of a superstep. Incoming messages are available to a vertex only at the beginning of the next superstep. This synchronized messagepassing paradigm simplifies reasoning about application logic as well as coordination issues with the framework.  User code simply describes how a generic vertex should process any received messages from the previous superstep \(usually from its neighboring vertices\ update the state of the vertex, and emit new messages to be delivered to a vertex for processing in the next superstep. Computation completes when all vertices either vote-to-halt or have no incoming messages.  Graph structure and vertex state are maintained across supersteps making it more attractive than MapReduce Pregelês BSP approach offe rs scalability with the graph size but its tightly-coupled distributed architecture has two side-effects. First, communication between two vertices connected by an edge usually requires network I/O as the vertices may reside on different hosts. Even if the graph is partitioned intelligently to reduce edge-cuts \(which is often not so\ an all-to-all network communication between hosts is required between supersteps. Second, a superstep completes and the next initiated only when vertices have completed their processing and the outgoing messages delivered Thus, \(costly\ barrier synchronization is required at each step and a slow worker causes all to wait idly until it is finished In the BSP domain, GPS is the m o st sim ila r to our work.  It extends the Pregel API to allow certain global computation tasks to be specified and run by a master worker.  It also explores partitioning effects on BSP performance while introducing certain dynamic repartitioning approaches.  However, it uses PageRank as its most intensive algorithm and suffers from the same lack of evaluation as the Pregel model for more intensive algorith ms such as betweenness-centrality In the context of parallel graph processing, frameworks can be categorized based on their primary message buffering approach as  or with different performance and scalability properties. The common implementations of offer a disk-based communication paradigm across distributed operations Ö data exchange between Map and Reduce phases is often through a  4 http://goldenorbos.org  distributed file system. The model such as Giraph and Apache Hama also utilize a disk-based approach However, in many cases memory-based message buffering systems can offer superior performanc ue t o its higher ba nd w i d t h  compared to disk I/O.  Several frameworks acknowledge this benefit and seek to exploit it by using in-memory architectures. GPS appears to use an inmemory message buffering approach.  Trinity [7 e s  beyond this and maintains th e entire graph in an inmemory, partitioned data storage layer that facilitates either a message passing \(similar to BSP\ or sharedmemory paradigm.  While offering novel in-memory data layouts and optimizations akin to Pregel's combiners that reduce communication, it was evaluated on a cluster with high-speed in terconnect rather than commodity clouds and was not publicly available at the time of this writing At the extreme edge of the spectrum, several frameworks store the entire graph structure in the memory of each worker machine.  Using a task parallel or tuple-space approach [2 ea ch h o s t o r  p r oces s o r  core performs the computation for a subset of vertices in the graph \(though that may require read access to the whole graph\ using their own in-memory copy of the graph data structure, combining vertex results before computation completes.  While using this approach either using 21 2 sho ul d y i e l d better performance it limits scalability with the graph size.  As graph datasets grow into the billions of vertices and graph algorithms require significant state to be maintained per vertex, this constraint of storing the graph in a single worker becomes too restrictive and leads to virtual memory thrashing Alternate frameworks for distributed graph analytics include [6 d 2 2   T h es e u s e alt ern ativ e p r og ram ming abstractions and are beyond the scope of this work.  Again, our work is not targeted at scaling massive graphs on large HPC systems or private datacenters with 1000ês of cores of çfreeé cycles available, but rather focuses on medium-sized graphs that are practically suited for pay-as-you-go public clouds running on commodity hardware, and offering democratized access to a wider group of scientists Graph analysis algorithms can be categorized based on their complexity.  The ubiquitous PageRank algorithm [2  r uns for m a n y iter a t i o n s w ith e v er y iteration passing a message along each edge \(to and from neighboring vertices\.  However, several interesting graph analysis algorithms exhibit greater complexity.  These include betweenness-centrality \(BC\, allpairs shortest paths \(APSP\, and community detection CD\.  Often, evaluation of graph frameworks fail to consider these algorithms and instead opt for less complex algorithms such as PageRank. Consequently 
 
205 


Figure 1 
frameworks that scale to billions of vertices for PageRank may fall orders of magnitude shorter for BC We focus our work on this class of high complexity graph algorithms and choo se betweenness-centrality BC\ as a representative algorithm for our evaluation BC is commonly used to find key vertices in a graph defined as those sitting upon a large percentage of shortest paths between two other vertices.  Computing BC on an unweighted, undirected graph requires a breadth-first traversal be performed  This often makes BC intractable for large graphs if not scaled appropriately and also serves as a stress test for a graph processing framework. For our evaluation we use a parallel BC version based on BrandÎsê algorithm [2   which performs a breadth-firs t traversal of the entire graph rooted from each vertex maintaining the number of shortest paths that pass through a given vertex and then accumulates these values by walking back up the tree formed by the traversal.  Each vertex initiates a traversal and produces a score for all other vertices These scores are then summed over all the traversals to produce a centrality score for each vertex.  We include APSP in several evaluations III BSP  F RAMEWORK I MPLEMENTATION ON A ZURE  The original Pregel framework is proprietary to Google and the only public BSP implementations available at the time of our evaluation were layered on top of Hadoop, using the MapReduce programming model, which does not allow a fair comparison of the BSP approach. Hence, we developed a native .NET/C implementation of a BSP graph processing framework Pregel.NET 5 which uses the Pregel programming model and runs on the public cloud. The Azure cloud offers a Platform-as-a-Service PaaS\ where .NET applications, called can be run on one or more VM instances. As with other infrastructure and platform cloud providers, Azure offers services for basic network communication virtual TCP sockets\, reliable message passing queues\nd persistent storage \(blob files and tables  5  http://ganges.usc.edu/wiki/Cloud_Computing  all built for scalability Our Pregel.NET architecture \(Figure 1\ consists of three Azure roles: a implementing a web form for job submission and status, a role that runs on a single VM instance and coordinates the supersteps of the BSP, and a role several VM instances of which hold the distributed graph partitions and perform vertex-centric tasks Requests from the user are sent from the web role to the job manager for processing through Azure queues.  The manager parses the request, which specifies the desired graph application, graph file location on cloud blob \(file\ storage, and number of partition workers \(also called çworkersé\ The manager replicates the request based on the desired number of workers and places them in a queue that any available worker can accept.  Workers that accept the request then read the graph file from blob storage and load the vertices that belong to their partition based on different partition schemes \(e.g. hash, METIS\ Workers report back to the manager with their logical ID so the manager can build a mapping of these workers and broadcast the topology so th at the workers can establish peer-to-peer communication channels. Computation begins with the manager placing superstep tokens into a stepé queue that workers monitor. The manager then waits for each worker to check in at the end of a superstep.  If no vertex has an incoming message and all are inactive, the manager stops computation, collects results, and notifies the user via the Web UI Partition workers \(or çworkers perform compute tasks on their partition of graph vertices in each BSP superstep. Workers wait on the step queue for the beginning of a superstep, and then call a user-defined method on each vertex in parallel using .NET's task parallel library that leverages multiple CPU cores. The method contains the application \(e.g. BC\ logic and is also templatized for user-defined object types to be associated with Vertex, Edge, and Message. The framework determines  
Pregel.NET BSP architecture on the Azure Cloud.  The Pregel/BSP model partitions vertices in the graph across workers. Workers in each superstep call for vertex and communicate emitted messa ges along edges, either as çbulké data messages to remote workers or through in-memory buffers until the next superstep. Inset on right shows the BSP dataflow 
from each vertex Microsoft Windows Azure roles Web role job manager partition worker Job Manager Partition Workers compute compute 
compute 
 
206 


compute Communication Control messages data messages without spilling over to virtual memory on disk 
if a message emitted by is destined for a vertex on a remote worker and, if so, places it in a memory queue for network transmission. If the message is for a local vertex, it is delivered in-memory There are two types of communication that take place as part of the BSP execution control and data are used to synchronize the start and end of a superstep across all worker instances. Vertices within a superstep pass between each other and these carry userdefined message payload In the BSP model, a synchronization barrier at the end of a superstep is reached when two conditions are satisfied: all vertices have completed their computation and all messages generated by a source vertex have been delivered to the sink vertex. The Job Manager coordinates this barrier operation, waiting for each worker to respond with a message in a çbarrieré queue Each message indicates how ma ny vertices are active in a worker's partition and allows the manager to determine when to halt computation. Since these messages are small and are generated only once per worker per superstep, we use Azure queues as a convenient and reliable transport The number of data messages generated in a superstep can be high and requires low-latency, highbandwidth worker-to-worker communication. We implement this using Azureês TCP Endpoint socket communication between every pair of workers.  This data communication stack is shared by all vertices in a particular worker \(i.e. the number of sockets are a function of the number of workers\ Since jobs can be long running, these connections are reestablished persuperstep to avoid socket timeouts. Message transfers can be demanding, but the BSP model does not impose any constraints on message order. Independent background threads running on multi-cores perform data communication. To better utilize network bandwidth we buffer serialized messages bound for a particular remote worker to perform çbulké transfers.  A receive thread at the remote partition worker, deserializes the messages and routes them to the target vertex's incoming queue for processing on the next superstep Our design inherits only the core BSP execution pattern and the Pregel graph programming model While our work can easily be extended to support more advanced Pregel features such as combiners, aggregators and fault recovery, these are not the focus of our evaluation or optimizations and thus are omitted.  The goal is to allow a fair evaluation of the Pregel BSP model on public clouds.  In addition, the impact of these advanced features is algorithm dependent with some algorithms unable to exploit them fully IV V ERTEX S CHEDULING H EURISTICS FOR O PTIMIZING BSP  G RAPH E XECUTION  Graph applications composed using the Pregel/BSP model encounter performance and scalability bottlenecks for applications that exhibit a log-normal distribution in messages exchanged during the supersteps when operating on small world graphs. In other words, applications like BC and all-pairs shortest path when mapped to a BSP model, sharply ramp up the cumulative messages exchanged between supersteps as their traversals reach supernodes in the small world graph, and drain down with a long tail of supersteps whose total number typically corresponds to the graphês diameter. Figure 3 shows an example of messages exchanges per superstep for BC and APSP. These applications can be distinguished from those like PageRank that exhibit a uniform distribution in messages across supersteps. Issues with scalability for applications with variable message exchange patterns arise due to the corresponding changes in resources required over different supersteps that can lead to costly money\ over-provisioning or costly \(time\ underprovisioning of partition workers. Here, we introduce heuristics to optimize the co sts associated with using the BSP model on public clouds for the class of graph applications that have non-uniform message patterns over supersteps, and use the BC application as an exemplar to discuss the benefits The number of data messages exchanged between workers across supersteps can be extremely large O\(|V||E|\or BC and is a function of the graph and application.  On BSP frameworks messages must be buffered since they must be delivered by the end of a superstep but not drained \(processed\ until the next This can be done either using \(costly\ disk access or by retaining them in memory, le ading to memory pressure We abjure disk-based buffering since it uniformly adds a multiplicative overhead that is comparable to the diskbased communication of Hadoop. Hence we would like to maximize memory utilization for buffering messages which may be even worse than disk-based buffering due to virtual memories random access \(vs. sequential for disk-based buffering\ I/O patterns Making the problem more complex is the variability in the number of messages per superstep.  In graph traversal algorithms such as BC, messages are passed to the collection of newly discovered vertices, or frontier at a particular superstep This frontier starts as the neighbors of the source vertex but quickly grows as neighbors, in turn, emit messages to their neighbors For graphs exhibiting small-world or scale-free structure, the vast majority of vertices will be discovered in a few steps, causing a near-exponential ramp-up in messages exchanged. Since Pregel/BSP allows a message to traverse only a single edge per superstep we expect a peak number of bi-sectional messages to be 
 
207 


Graph Vertices Edges 90% eff diameter 
exchanged at the superstep that roughly equals the average shortest path length. For a small-world graph with a million edges, this peak can be more than 70% of a graph's edges for a breadth first traversal. Because BC additionally performs a backward traversal of the BFS tree, this peak will then tape r off as data is passed back up the tree The number of messages exchanged per superstep impacts network, memory and CPU resource needs Network is obvious due to message delivery to remote vertices. The incoming messages are also buffered in memory till the next superstep starts and the user logic drains the input message queue. Further, the CPU utilization for delivering messages by our framework is comparable to the userês vertex compute logic. All of this leads to oscillations in resource utilization across supersteps. This is exacerbated by the Pregel model where all vertices logically start at the same time leading to |V| traversals starting at the same time for BC, amplifying the spike in utilization. Consequently the number of buffered messages can easily overwhelm the physical memory and punitively spill over to virtual memory on disk. Provisioning enough workers aggregate memory capacity\o store the frontier of |V traversals all at once may require a prohibitive number of cloud VM's \(i.e. 1000s\that are liable to be idle during non-peak supersteps. Alternatively, this offers an opportunity to shape the resource demand by flattening and shifting this peak to allow for more uniform resource usage with better cost/performance tradeoffs We propose a for Pregel/BSP, where computation for only a subset, or of vertices is initiated at a time, though that computation may require action from all vertices in the graph as a traversal progresses. For BC this would mean starting traversals from the first swath of  vertices, allowing the computation to proceed, and then starting the next swath of vertices in succession until all V| vertices have been initiated and completed. This approach allows resource utilization to be spread over time and shaped based on the swath size and the initiation interval between swaths.  Clearly the swath size should be set as large as possible to provide good utilization but small enough to allow messages during peak supersteps to fully reside in physical memory. To this end, we propose several heuristics for choosing the swath size and deciding when to initiate the next swath We first attempt to use intrinsic graph properties such as average edge degree or degree distribution to predict how many traversals may be performed concurrently without overflowing memory. However, this is difficult given the diversity of graphs and no consistent prediction model could be extracted. We then investigate two approaches based on runtime decisions.  One is to run a few small swaths of vertices as a run, monitoring its peak memory usage, and then extrapolate the results based on available VM memory to a static swath size for use during the rest of the computation. The alternative heuristic we propose is an system that allows the size to vary from one swath to the next based on the maximum memory usage between one swath initiation and the next. While many possible control strategies could be used, we select a simple linear interpolation that bases the next swath size on the peak memory usage of the previous In the Pregel/BSP model computation concludes when all vertices vote to halt and have no incoming messages to process. Thus, the baseline implementation of executing swaths would be for the entire swath of vertices to complete before starting the next swath.  However, this causes resources to be underutilized for all but the peak supersteps. In addition, the more the supersteps required, the greater the synchronization overheads.  Since utilization is low for the tail of a swath, we can initiate the next swath and overlap their execution. The trigger for initiating another swath can be a number of supersteps or determined dynamically by monitoring the application Initiating another swath too soon before the peak utilization of the previous swath may exacerbate the memory utilization; too late and resources are underutilized. Ideally, we want to keep memory usage near the physical memory threshold w ithout exceeding it.  Thus setting the static initiation interval to the number of supersteps required for a swath to reach its peak utilization is desirable.  An accurate prediction of this interval is not unreasonable since the average shortest path length is nearly constant over number of vertices for small world networks \(think 6-degrees from Kevin Bacon\ and is usually on the order of 5-10 for smallworld networks. Alternatively, we propose a  initiation heuristic that can monitor the message traffic memory utilization, or even number of active vertices those that have not voted to halt\ determine when the peak utilization has passed. For BC, since the number of messages will begin to decrease once a traversal reaches the bottom of its BFS tree and begins to reverse, our dynamic initia tion heuristic monitors the statistics of sent messages from one superstep to the next until a superstep shows an increase followed by a decrease in message traffic i.e. a phase change in the messages transferred across supersteps 
SlashDot0922 \(SD web-Google \(WG cit-Patents \(CP LiveJournal \(LJ Table 1. Evaluation datasets and their properties  
82,168 948,464 4.7 875,713 5,105,039 8.1 3,774,768 16,518,948 9.4 4,847,571 68,993,773 6.5  
costly vertex-scheduling model swath k Swath Size Heuristics Swath Initiation Heuristics 
sampling adaptive static dynamic 
208 


002\003\004\005\006\006 002\003\004\005\006\002 002\003\004\005\006\007 002\003\004\005\006\010 002\003\004\005\006\011 002\003\004\005\006\012 002\003\004\005\006\013 002\003\004\005\006\014 002\003\004\005\006\015 002\003\004\005\006\016 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 035\024\027\022\036\030\037\020 \024!"\034\033\026 036\024\025#\024$#\004%\020\027!\031\030\024\034#&\030'\020#\(\\003 032\033\025\020+\033\034  032\\032 006 006\003\012 002 002\003\012 007 007\003\012 010 010\003\012 011 011\003\012 012 002\010\012\014\016\002\002\002\010\002\012\002\014\002\016\007\002\007\010\007\012\007\014\007\016\010\002\010\010 037\020"\033\025\020#0\020\035\035\033\025\020\035#&"\033\034\035$\020""\0201 0\030\026\026\030\024\034\035 030'\020#\(\!2\020"\035\031\0202 032\033\025\020+\033\034,#0\020\035\035\033\025\020\035 0\020\035\035\033\025\020\035#\(\002#\\017\033\0313 032\#0\020\035\035\033\025\020\035#\(\002#\\017\033\0313 
Figure 2 Figure 3 
V E XPERIMENTAL S ETUP  We implement and run multiple graph algorithms including BC, PageRank and All-Pairs shortest path using our Pregel.NET BSP framework on the Microsoft Azure public cloud platform.  We use four real datasets   o f va r y ing siz e s th a t e x hib it sm a ll-w o r l d  pro p e r ties:  SlashDot \(SD\nd LiveJournal \(LJ\ social networks, a web graph from Google \(WG\, and a patent citation network \(CP\, whose statistics are shown in Table 1. Our implementation does not perform any aggressive memory management or compression of NET object representation of graphs; hence the sizes of graphs selected were conservative to fit available VM memory. However, these do not detract from the key benefits of the heuristics and the analysis of Pregel/BSP that we present empirically We use large Azure VM instances for all partition worker roles and small instances for web UI and manager roles since the latter just perform coordination Large VMs have 4-cores rated at 1.6GHz, 7GB RAM and 400Mbps network and cost $0.48/VM-Hour. Small ones are exactly a fourth of these specifications. These extents were chosen based on the Azure resources made available as part of a research grant, but the scalability trends can be extrapolated to larger numbers of VMs Given the time complexity of the BC algorithm, our large graph datasets can run for days or even weeks Thus, we perform a 4-hour run for each of our experiments and extrapolate these results to the entire graph VI E VALUATION OF B ASELINE AND H EURISTICS  Our Pregel.NET BSP framework can be used to implement and run a variety of graph applications on the Azure cloud. As a basic comparison, we implement PageRank, BC, and All-Pairs Shortest Path \(APSP\, and evaluated their performance with two graphs, WG and CP, using 8 worker VMs. The LJ graph would not fit within the available physical memory of the workers for BC and APSP due to the large numbers of messages that they generate during their traversal. LJ was however run for PageRank since it is a less demanding application Figure 2 shows the time taken \(log scale\ to run these applications for the graph sizes. As noted before, these times are extrapolated from running smaller sampling runs over a subset of the vertices. PageRank however was run to completion over 30 iterations. As can be seen, both BC and APSP take 4 orders of magnitude longer to complete than PageRank for the same graph sizes. BC and APSP are inherently superlinear in computational complexity compared to PageRank as they perform graph traversals rooted at every vertex while PageRank performs pairwise edge traversals from every vertex to its neighbors Further, if we consider the communication complexity of these applications using the BSP model, we notice that PageRank has a constant number of messages exchanged between workers across different supersteps Figure 3 shows a straight line at ~637,000 average messages exchanged by each of eight workers for the 30 supersteps required to complete PageRank for the WG graph, leading to a uniform performance profile and predictable resource usage. However, the messages transferred for BC and APSP over different supersteps has a triangle waveform pattern, which spikes to a peak of 4.7M and 3M messages respectively Ö just for a single swath of seven vertices and not for all vertices in the graph. The ramp up and down of messages over supersteps, and their repetitive nature as subsequent swaths of vertices are executed, leads to non-uniform resource usage, in particular, memory usage for buffering these messages between supersteps. The communication cost of message transfer also causes BC and APSP them to take much longer to complete than PageRank   
Total time taken \(log scale\ to perform PageRank, BC and All-Pairs Shortest Path \(APSP\ for the WG and CP graphs on 8 workers. LJ is shown for PageRank The average number of messages transferred per worker across supersteps for the WG graph for one static swath \(BC and APSP\, and for the entire graph \(PageRank 
  
 
Since BC traverses the entire graph rooted at each vertex, extrapolating results from a subset of vertices is reasonable and was empirically verified A Standard  Graph Applications using Pregel.NET 
209 


Speedup of Swath Size Heuristics vs. Baseline largest successful single swath size running on 8 workers for the BC application. Taller is better Memory usage over time for running BC on WG graph. Heuristics use a 6GB memory threshold. VMs have 7GB physical memory. Curves close to 6GB imply good memory utilization. Those near 7GB hit virtual memory   Speedup of Swath Initiation Heuristics vs Baseline Sequential Initiation. All run the BC application on 8 workers. Taller is better Message transfers over time for the various initiation heuristics for running BC on WG graph. Flatter is better 
Figure 4 Figure 5 Figur e 6  Figure 7 
B Swath Size Heuristics the more memory that is utilized \(while staying within physical memory limits the faster the completion time     
 
 
baseline sampling heuristic adaptive heuristic 
The original Pregel/BSP approach of running all vertices in parallel can be detrimental to performance because of the memory demand for buffering messages across supersteps.  In fact, in a cloud setting, spilling to virtual memory can lead workers to seem unresponsive and the cloud fabric to restart the VM, as we have observed. This is particularly a concern for applications like BC and APSP that show a non-uniform resource usage pattern. For e.g., in Fig. 15 \(top\BC and APSP for the WG graph had to be run in small swaths of around 10 vertices as otherwise they would well surpass available memory during their peaking supersteps. Our swath size and initiation heuristics attempt to break vertex computations into a smaller number of swaths that are run iteratively to better control memory demand.  The alternative would require scaling up to 1000's of VM's to accommodate the memory demand As a we manually found the largest swath size we could successfully complete the BC application using 8 workers \(7GB memory each\or our WG and CP graphs while allowing them to spill to virtual memory \(40 and 25 swathe sizes, respectively\. Next we used our proposed swath size heuristics and to pick smaller swath sizes automatically and iteratively execute each swath for the same total number of vertices as the baseline single swath \(40 and 25\n both these heuristics the target maximum VM memory utilization threshold is conservatively set to 6GB since over-estimation can be punitive.  We ran these experiments using both 8 and 4 workers Figure 4 summarizes the relative performance gain of our swath size heuristics for performing BC on WG and CP graphs compared to the baseline approach that uses 8 workers. The sampling heuristic yields a speedup of nearly 2.5-3 versus the single large swath baseline while the dynamic heuristic yields a speedup of up to 3.5.  Figure 5 illustrates the corresponding physical memory usage during these executions. The baseline spills beyond available physical memory \(flat at 7GB the dynamic heuristic stays close to the target memory threshold of 6GB while the static heuristic stays close to it, but less often so. Intuitively This allows the adaptive     
006\003\006 006\003\012 002\003\006 002\003\012 007\003\006 007\003\012 010\003\006 010\003\012 011\003\006 015#\017\024",\020"\0354#\2\026\030\034\025 3 015#\017\024",\020"\0354#/1\0332\031\030\037\020  011#\017\024",\020"\0354#/1\0332\031\030\037\020  020\0201!2#\037\035\003#\015#5\024",\020"4#\\025\026\020\022\\017\033\0313 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 002 6#\032\020"$\024"'\033\034\027\020#\024$#\026\033"\025\020\035\0314#\035\030\034\025\026\020#\035\017\033\0313#\0313\033\031 027\033\034#\035!\027\027\020\035\035$!\026\0267#\020%\020\027!\031\020#\024\034#\0313\020#\025\030\037\020\034#\025"\03323 006 002 007 010 011 012 013 014 015 006 002\006\006\006 007\006\006\006 010\006\006\006 011\006\006\006 012\006\006\006 013\006\006\006 014\006\006\006 0\020'\024"7#8\031\030\026\0309\033\031\030\024\034#\(\023 030'\020#\(\035 015#\017\024",\020"\0354#\\030\034\025\026\020\022\012\006 015#\017\024",\020"\0354#0!\026\031\0302\026\020\022\004%\031"\0332\024\026\033\031\0201 015#\017\024",\020"\0354#0!\026\031\0302\026\020\022/1\0332\031\030\037\020#.\024\034\031"\024\026 011#\017\024",\020"\0354#0!\026\031\0302\026\020\022/1\0332\031\030\037\020#.\024\034\031"\024\026 9!"\020#\036\033 025 020#:\034\035\031\033\034\027\020#0\033%#\0323 7 035\030\027\033\026#0\020'\024 7 036\030'\030\031 020!"\030\035\031\030\027 033"\025\020\031#\03237\035\030\027\033\026#0\020'\024"7#\036\030'\030\031 006\003\006 006\003\007 006\003\011 006\003\013 006\003\015 002\003\006 002\003\007 002\003\011 002\003\013 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 0201!2#\037\020"\035!\035#\\026#;\020!"\030\035\031\030\027 030\033\026 7\034\033'\030\027 031\030\027#\011 031\030\027#\012 031\030\027#\013 002#6#\032\020"$\024"'\033\034\027\020 024$#\\020<!\020\034\031\030\033\026#:\034\030\031\030\033\031\030\024\034 006\003\004\005\006\006 002\003\004\005\006\013 007\003\004\005\006\013 010\003\004\005\006\013 011\003\004\005\006\013 012\003\004\005\006\013 013\003\004\005\006\013 014\003\004\005\006\013 015\003\004\005\006\013 006 002\006\006\006 007\006\006\006 010\006\006\006 011\006\006\006 012\006\006\006 037\020"\033\025\020#0\020\035\035\033\025\020\035#\\020\034\031#\032\020"#5\024",\020 004%\020\027!\031\030\024\034#&\030'\020#\(\\020\027\024\0341\035 020\034\031\030\033\026 033\031\030\027#\013 7\034\033'\030\027 
210 


006\003\006 006\003\007 006\003\011 006\003\013 006\003\015 002\003\006 002\003\007 032\033\025\020+\033\034 017\020\021\022\023\024\024\025\026\020 017\020\021\022 023\024\024\025\026\020 032\\032#\(\017\020\021\022 023\024\024\025\026\020 032\033\025\020+\033\034,#\(\027\030\031\022 032\033\031\020\034\031\035 027\030\031\022 032\033\031\020\034\031\035 032\\032#\(\027\030\031\022 032\033\031\020\034\031\035 004%\020\027!\031\030\024\034#&\030'\020#>\024"'\033\026\0309\0201#\031\024#;\033\0353\0201#\032\033"\031\030\031\030\024\034\030\034\025 033\0353\030\034\025 0\004 020\033'\030\034\025 032\020"$\024"'\033\034\027\020#\024$#;\033\0353\030\034\025 
Figure 8 
heuristic to execute BC on just 4 workers in roughly two-thirds the time as the baseline using 8 workers providing users with cost-performance tradeoffs in a pay-as-you-go cloud environment. The automation offered by the adaptive heuristic to the end user also eliminates the guesswork of picking a static baseline or any potential non-uniformity in sampling using the sampling heuristic C  Given the need to run computation as a series of smaller \(optimally sized\at hs, is important to decide when we initiate the next swath. Our initiation heuristics attempt to overlap execution of multiple swaths to flatten the resource \(memory, network, CPU\ usage variations causes by different supersteps within a single swath. In BC and APSP, we observe a triangle waveform with a central peak; this heuristic is not relevant for applications like PageRank with uniform resource usage. Besides improving resource utilization, overlapping consecutive swath iterations also reduce the cumulative supersteps required and thus reduces the total overhead spent on synchronization between supersteps Figure 6 compares the relative performance of our initiation heuristics for the BC application normalized to a baseline approach that runs strictly  non-overlapping iterations. These run on 8 workers Figure 7 shows the corresponding messages transferred between supersteps over time, spanning swath iterations. The initiates a new swath every supersteps while the  performs initiation when it detects a peak in the number of messages exchanged. Static-Nês performance depends on the graph and the value of that is chosen.  If the average shorte st path is greater than  we will be initiating new heur istics before the previous swath has hit its peak, thereby exacerbating the resource demand. If the average shortest path length is well distributed or is \(just\ shorter than it leads to better performance. So 4 for the larger CP graph actually works best. Our dynamic he uristic eliminates this guesswork as it picks the initiation point at runtime without user input or graph preprocessing. Using this dynamic initiation heuristic we achieve up to 24 speedup vs. sequential initiation for the WG graph. The message transfer plot in Figure 7 corroborates this While sequential shows the message transfers peak and fall to zero \(thus showing more variability and poorer utilization\, Static-6 \(which is optimal but handselected\ maintains a higher message rate while dynamic is a bit more conservative, but automated VII E VALUATING I MPACT OF G RAPH P ARTITIONING ON P REGEL NET Our Pregel.NET framework is agnostic to how the graphs are partitioned and assigned to workers. The default mode performs a simple hash over the vertex ID to determine the target worker partition. Several works have shown that intelligent graph partitioning can improve the performance of distributed graph algorithms [19  26 a nd i t is relevant to examine if these benefits carry over to the Pregel/BSP model also METIS is a commonly used strategy that provides good quality in-place partitioning that minimizes edge-cuts across partition  Rec e n t w o rk o n a p pr o x im ate partitioning using a single graph scan offers an alternative for partitioning online as the graph is read from storage P a ge R a nk is o f t e n used in l iter a tur e  to validate the effectiven ess of these partitioning strategies. However, as we have seen, PageRank implemented using Pregel/BSP has a uniform message profile while BC and APSP have a triangle waveform message profile. We analyze the consequence of this on the performance gains from intelligent graph partitioning Clearly, the benefit of partitioning comes in reduced communication time since messages to remote vertices incur additional delay due to serialization and network I/O when compared to in-memory messages sent to local vertices. Since many distributed graph algorithms are dominated by communication rather than computation, partitioning can improve overall performance However, the barrier synchronization model in Pregel/BSP means that the total time spent in a superstep is determined by the slowest worker in the superstep. Hence, the balance of work amongst workers in a superstep is as import ant as the cumulative number of remote messages generated in a superstep. Since vertices communicate with their neighbors along edges in the Pregel/BSP model and partitioning seeks to collocate a majority vertex neighbors in the same partition, there may arise çlocal maximasé in specific partitions where more vertices are active during the course of execution of a graph application. This difference in workload can cause underutilization of workers that wait for over utilized workers at the superstep barrier  
Relative time taken by PageRank, APSP and BC to run on WG and CP graphs partitioned using METIS and Streaming, normalized to Hashing approach. Smaller is better  
  
sequentially Static-N heuristic Dynamic heuristic 
Swath Initiation Heuristics N N N N N  
211 


006 002\006 007\006 010\006 011\006 012\006 013\006 014\006 015\006 016\006 002\006\006 006 012\006\006 002\006\006\006 002\012\006\006 007\006\006\006 007\012\006\006 010\006\006\006 010\012\006\006 011\006\006\006 033\0353\0201 0\004 031"\020\033'\030\034\025 0#8\031\030\026\0309\033\031\030\024\034 004%\020\027!\031\030\024\034#&\030'\020#\(\035\020\027\003 024'2!\031\020\005:AB 033""\030\020"#5\033\030\031 8\031\030\026 006 006\003\012 002 002\003\012 007 007\003\012 010 010\003\012 011 011\003\012 012 0202#\014 0202#\015 031\0202#\016 031\0202#\002\006 0\020\035\035\033\025\020\035#\\020\034\031#-7#\004\033\0273#5\024",\020 0\030\026\026\030\024\034\035 017\020\021\022\023\024\024\025\026\020#;\033\0353\0201#\032\033"\031\030\031\030\024\034\030\034\025 5\006 5\002 5\007 5\010 5\011 5\012 5\013 5\014 006 006\003\012 002 002\003\012 007 007\003\012 010 010\003\012 011 011\003\012 012 0202#\014 0202#\015 0202#\016 0202#\002\006 0\020\035\035\033\025\020\035 020\034\031#-7#\004\033\0273#5\024",\020 0\030\026\026\030\024\034\035 017\020\021\022\023\024\024\025\026\020#0\004&:\#\032\033"\031\030\031\030\024\034\030\034\025 5\006 5\002 5\007 5\010 5\011 5\012 5\013 5\014 006 002\006 007\006 010\006 011\006 012\006 013\006 014\006 015\006 016\006 002\006\006 006 012\006\006 002\006\006\006 002\012\006\006 007\006\006\006 007\012\006\006 033\0353\0201 0\004 031"\020\033'\030\034\025 0#8\031\030\026\0309\033\031\030\024\034 004%\020\027!\031\030\024\034#&\030'\020#\(\035\020\027\003 024'2!\031\020\005:AB 033""\030\020"#5\033\030\031 8\031\030\026 006 006\003\012 002 002\003\012 007 007\003\012 010 010\003\012 011 011\003\012 012 0202#\016 031\0202#\002\006 031\0202#\002\002 031\0202#\002\007 0\020\035\035\033\025\020\035#\\020\034\031#-7#\004\033\0273#5\024",\020 0\030\026\026\030\024\034\035 027\030\031\022\032\033\031\020\034\031\035#;\033\0353\0201#\032\033"\031\030\031\030\024\034\030\034 025 5\006 5\002 5\007 5\010 5\011 5\012 5\013 5\014 006 006\003\012 002 002\003\012 007 007\003\012 010 010\003\012 011 011\003\012 012 031\0202#\016 031\0202#\002\006 031\0202#\002\002 031\0202#\002\007 0\020\035\035\033\025\020\035#\\020\034\031#-7#\004\033\0273#5\024",\020 0\030\026\026\030\024\034\035 027\030\031\022\032\033\031\020\034\031\035#0\004&:\#\032\033"\031\030\031\030\024\034\030\034\025 5\006 5\002 5\007 5\010 5\011 5\012 5\013 5\014 
We evaluate the impact of graph partitioning using the best-in-class METIS partitioner as well as the best heuristic \(linear-wei ghted deterministic, greedy approach partitioner from [26 an d com p a r e  them against a baseline that uses simple of vertices by their IDs. We run PageRank, BC and APSP over the WG and CP graphs on 8 workers for this evaluation. Hash, METIS, and Streaming produce 8 partitions whose percentage of remote edges are 87 18% and 35% for the WG graph and 86%, 17% and 65% for the CP graph; smaller this number, lower the edge cuts across partitions, and METIS proves a low edge cut for both graphs. Given the large sizes of the graphs, we run these experiments on the same set of vertices as our other experiments \(50 vertices for CP and 75 vertices for WG\. We report these results when using Pregel.NET without our swath heuristics however, the trends we observe are consistent even with heuristics turned on, though the absolute performance is uniformly better Figure 8 shows the relative time taken when using the METIS and streaming partitioning normalized to hashing for PageRank, BC and APSP running on WG and CP. We see that the WG graph sees a relative improvement of nearly 42-50% for METIS for the three applications, while this improvement drops to 24-35 for the streaming partitioning. When running Pregel.NET with heuristics turn ed on, we see a best case improvement of 5x in relative time taken by METIS for BC on WG compared to hashing \(graph not shown These are consistent with results reported in  However, we also see that the CP graph does not show such a marked improvement in performance due to better partitioning, despite its edge cut ratios from different partitioning being similar to WG. In fact hashing is faster than METIS and Streaming for APSP on this graph. It is worthwhile to investigate this consistent lack of improvement for the CP graph as opposed to WG. Figure 9 shows the runtime for BC broken into compute+I/O time and the synchronization barrier wait time components for the WG graph and Figure 12 does the same for CP. The plots also show the VM utilization %, calcul ated as the time spent in compute and I/O communication against the total time including barrier wait time\ on the secondary Y-axis We see that the VM utilization % for hashing is higher though the total time taken is also higher, for both WG and CP. METIS shows the inverse property, having lower utilization but also lower total time. This is explained by looking at the number of messages emitted by workers in a supe rstep for both hashing and METIS, shown in Figures 10 and 11 for WG, and in Figures 13 and 14 for CP. We expect that a hashed assignment of vertices to a partition would spread communication roughly evenly over all workers, while also increasing the number of remote communications required. The latter contributes to the increased total time while the former leads to a uniform number of    
Figure 9 Figure 10 Figure 11 Figure 12 Figure 13 Figure 14 
 taken for BC on a subset of with  shows the ratio of Compute+I/O time to total time   transferred by each worker in the peak supersteps of BC performed over using    transferred by each worker in the peak supersteps of BC performed over       taken for BC on a subset of with  shows the ratio of Compute+I/O time to total time   transferred by each worker in the peak supersteps of BC performed over using    transferred by each worker in the peak supersteps of BC performed over   
in-place streaming hashing 
Total time WG graph different partitioning Utilization Number of messages WG graph Hash partitioning Number of messages WG graph using METIS partitioning Total time CP graph different partitioning Utilization Number of messages CP graph Hash partitioning Number of messages CP graph using METIS partitioning 
212 


Bottom Top A B 
Figure 15 Figure 16 
messages seen for all workers in a superstep \(Figures 10 and 13\. When looking at the messages sent by workers in a superstep for METIS, we see that there are message load imbalances within work ers in a superstep, caused due to concentration of vertices being traversed in that superstep in certain partitions This variability is much more pronounced in CP as compared to WG \(Figures 11 and 14\ E.g. in superstep 9 for CP, twice as many messages \(4M\ are generated by a worker compared to another \(2M\.  For Pregel BSP, the time taken in a superstep is determined by the slowest worker in that superstep. Hence increase d variability in CP causes even çgoodé partitioning strategies to cause an increase in total execution time wh en using the Pregel/BSP model VIII A NALYSIS OF E LASTIC C LOUD S CALING  Cloud environments offer elasticity Ö the ability to scale-out or scale-in VMs on-demand and only pay for what one uses [28   On th e f l i p s i de  on e en ds u p  paying for VMs that are acquired even if they are underutilized. We have already shown the high variation in compute/memory resources used by algorithms like BC and APSP across different supersteps. While our earlier swath initiation heuristics attempt to flatten these out by overlapping swath executions, one can consider leveraging the cloudês elasticity to, instead, scale up and down the concurrent workers \(and graph partitions\ allocated in each superstep The peak and trough nature of resource utilization combined with Pregel/BSPês synchronous barrier between supersteps offers a window for dynamic scaleout and Öin at superstep boundaries. Peak supersteps can greatly benefit from additional workers, while those same workers will contribute to added synchronization overhead for trough supersteps We offer an analysis of the potential benefits of elastic scaling by extrapolating from observed results for running BC on WG and CP graphs, using four and eight workers.  To provide a fair and focused comparison, we turned off swath heuristics in favor of fixed swath sizes and initiation intervals Figure 15 \(Bottom\ plots the speedup of BC running on eight workers when normalized to BC running on four workers, at corresponding supersteps.  The number of workers does not impact the number of supersteps We also plot the number of active vertices \(i.e. vertices still computing for a given swath\these supersteps which is a measure of how much work is required \(Fig 15 \(Top\. We find that we occasionally get superlinear speedup spikes \(i.e. >2x\ that shows a strong correlation with the peaks of active messages, for both WG and CP graphs. At other times, the sp eedup is sublinear or even a speed-down \(i.e. <1\responding to inactive vertices.  The superlinear speedup is attributable to the lower contention and reduced memory pressure for 8 workers when the active vertices peak \(similar to what we observed for the swath initiation heuristics Similarly, the below par speedup during periods of low activity is contributed by the increased overhead of barrier synchronization across 8 workers. Intuitively, by dynamically scaling up the number of workers for supersteps with peaking active vertices and scaling them down otherwise, we can leverage the superlinear speedup and get more value per worker Using a threshold of 50% active vertices as the threshold condition for between 4 and 8 workers in a superstep, we extrapolate the time per superstep and compared this to the fixed 4 and 8 worker runtimes. We also compute the best-case run time using an çoracleé approach to i.e. for each superstep, we pick the minimum of the 4 or 8 workerês time.  Note that these projections do not yet consider the overheads of scaling, but are rather used to estimate the potential upside if we had an ideal or an automated heuristic for scaling. The total time estimates for running BC on WG and CP graphs, normalized to  
 plot shows speedup of 8 workers relative to 4 workers, for each superstep, when running BC on WG and CP graphs plot shows the number of vertices active in that superstep Estimated time for BC using elastic scaling, normalized to time taken for 4 workers. Normalized cost is shown on secondary Y axis WG graph shown on left CP graph shown on right. Smaller is better 
022\011 022\010 022\007 022\002 006 002 007 006 002 007 010 011 012 013 014 015 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 027\031\030\037\020#@\020"\031\030\027\020\035 0201!2#\024$#\015#5\024",\020"#\017\003"\003\031\003#\011#5\024",\020"\035 024"'\033\026\0309\0201#\\031\020 2 035#\032\020"#+!\034 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 017\020\021\022\023\024\024\025\026\020#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 027\030\031\022\032\033\031\020\034\031\035#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 036\030\034\020\033"#\\0201!2 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 017\020\021\022\023\024\024\025\026\020#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035\031 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 002\003\011 002\003\013 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 033\026\030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 027\030\031\022\032\033\031\020\034\031\035#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035 031 
 
dynamically scaling ideal scaling 
Our hypothesis is that an intelligent adaptive scaling of workers can achieve a similar performance as a large, fixed number of workers, but with reduced cost 
213 


Nature Nature Ecological Applications Nature ACM International Conference on Management of Data \(SIGMOD In Parallel Object-Oriented Scientic Computing \(POOSC Science Communications of the ACM ACM Workshop on Mining and Learning with Graphs Communications of the ACM HotCloud Proceedings of the 19th ACM International Symposium on High PErformance Distributed Computing HPDC Knowledge and Information Systems KAIS International Conference on Computational Science IEEE International Conference on Cloud Computing Technology and Science ACM/IEEE Conference on Advances in Social Network Analysis and Mining \(ASONAM IEEE International Parallel and Distributed Processing Symposium \(IPDPS International Conference on Distributed Computing and Networking Journal of Mathematical Sociology International Conference on Parallel Processing Communications of the ACM 
 
observed time taken using 4 workers, are plotted in Figures 16\(A\ and 16\(B We see that our dynamic scaling heuristic using the percentage of active vertices achieves nearly the same CP\ or better \(WG\ performance as a fixed 8 worker approach. Clearly there is benefit of using fewer workers for low utilization su persteps to eliminate the barrier synchronization overhead. Also, the dynamic scaling heuristic performs almost as well as the ideal scaling. Finally, when we consider the monetary cost of the proposed approaches, assuming a pro-rata normalized cost per VM-second plotted on the secondary Y axis, we see that dynamic scaling is comparable \(CP\ or cheaper \(WG\ than a 4 worker scenario while offering the performance of an 8 worker deployment IX C ONCLUSION  In conclusion, we introduce optimization and heuristics for controlling memory utilization and show they are critical to performance.  By breaking computation into swaths of vertices and using our sizing heuristics we achieve up to 3.5x speedup over the maximum swath size that does not cause the a failure.  In addition overlapping swath executions can provide a 24% gain with automated heuristics and even greater speedup when a priori knowledge of the network characteristics is applied This evaluation offers help to eScience users to make framework selection and cost-performancescalability trade-offs. Our he uristics are generalizable and can be leveraged by other BSP and distributed graph frameworks, and for graph applications beyond BC. Our work uncovered an unexpected impact of partitioning and it would be worthwhile, in future, to examine the ability to pred ict, given certain graph properties, a suitable partitioning model for Pregel/BSP It may also be useful to perform such evaluations on larger graphs and more numbers of VMs. At the same time, it is also worth considering if non-linear graph algorithms are tractable in pr actice for large graphs in a distributed environment B IBLIOGRAPHY  1  F  L i lj er os C   Ed l i n g L  A m a r a l H  S t an ley   and Y    berg The web of human sexual contacts 
vol. 411, pp. 907908, 2001   H Je o n g  S   Ma so n A  L   B a ra b s i  a nd Z   Oltva i  L e t ha l i t y  and centrality in protein networks vol. 411, pp. 41-42 2001   O. B o din and E   E s t r ada    U s i n g n e t w ork c e nt r a l i t y  m e a s ures t o  manage landscape connectivity vol 18, no. 7, pp. 1810-1825, October 2008   D. W a ts s  and S  S t r ogat z  C olle c t i v e  d y nam i cs of  s m a ll-w orl d   networks vol. 393, no. 6684, pp. 440Ö442, June 1998   G  Ma lew i c z   M A u s t er n A   Bik  J   Dehn er t I  Hor n   N. L e i s er and G. Czajkowski, "Pregel: A system for large-scale graph processing," in 2010   D. G r egor  and A  L u m s dain e  T h e  pa r a llel  B G L  A gen e r i c  library for distributed graph computations," in 2005   B. S h a o  H. W a n g  and Y  L i T he T r init y G r aph E n g i n e    Microsoft Research, Technical Report MSR-TR-2012-30, 2012   A  F ox  C lo ud c o m putin g w h at  s  in it for m e  as  a  s c i e n tis t     vol. 331, pp. 406-407, 2011   S. G h e m a w a t  and J  De an   Map re duc e s i m p lifi e d data  processing on large clusters vol 51, no. 3, pp. 107-113, 2008   J  L i n and M. S c hat z   Des i g n  patt er n s  for eff i ci ent gr aph algorithms in MapReduce," in 2010   L   Va l i ant   A b r id g i n g m o d e l f or pa r a llel com putati o n  vol. 33, no. 8, pp. 103-111, 1990 12 a c h e  Ha ma    O n l i n e    http://hama.apache.org   13 Ap a c h e  Ha d o op    O n l i n e    http://hadoop.apache.org     M Z a h a r i a, M. Ch ow dhu ry M F r ank l in S  S h e n k e r, and I   Stoica, "Spark: Cluster Computing with Working Sets," in 2010   J  Ekana y ak e e t a l     T w i st er A  r untim e f o r it er ati v e  MapReduce," in Chicago, 2010, pp. 810-818   U. K a n g  C  T s o u rakakis   and C. F a l outs o s  Peg a s us   Minin g  Peta-scale Graphs," in 2010   M. P a c e  B S P vs  MapR e duc e    in vol. 103.2081, 2012   S. Seo  E  Yoo n, J  K i m  S  J i n  J-S. K i m   and S   Ma e n g HAMA: An Efficient matrix computation with the MapReduce framework," in 2010, pp. 721-726   S. S a l i h ogl u  and J  W i d o m  G PS A G r a ph P r oc e s s i n g Sy s t em    Stanford University, Technical Report 2011   R L i cht e n w a l t e r and N   Cha w la D is Ne t  A fr am ew ork for  distributed graph computation," in  2011   K  Maddu r i  D. E d i g er K   J i an g  D. Bad e r  and D  Cha v a r riaMiranda, "A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets," in 2009   E  K r e p s k a, T  K i el m a nn, W  F o kkink, H   Ba l, "A  hi g h level framework for distributed processing of large-scale graphs," in 2011, pp. 155-166   L   Pa ge  S  B r in R. M o t w ani and T  W i nogr ad  T h e P a geRank citation ranking: Bringing order to the web," Stanford InfoLab Technical Report 1999-66, 1999   U  Brand  s  A f a s t er  a l gor ith m for  b e t w eenn e s s c e nt r a l i t y    vol. 25, no. 2, pp. 163-177 2001   Stan fo r d  Net w or k A na l y s is Pro j e c t  O n l in e    http://snap.stanford.edu    I  S t ant o n and G  K l i o t, "S t r e a m i n g G r aph P a rtiti o n in g  for L a rge Distributed Graphs," Microsoft Corp., Technical Report MSRTR-2011-121, 2011   G   K a ry pis and V   K um a r A fas t and hi g h qua l i t y m u l t i l evel scheme for partitioning irregular graphs," in 1995, pp. 113-122   M. A r m b r u s t e t  a l   A v i ew of  c l o u d  c o m putin g    vol. 53, no. 0001-0782, pp. 50-58 April 2010  
214 


  13  or gani c  c he m i s t r y  i n our  Sol ar  Sy s t e m       Xi a n g  L i r e c e i v e d h i s B  S   m is tr y  fr o m  th e  P e k in g  U n iv e r s ity  C h in a  in  2 0 0 3  and P h D   i n P hy s i c al  C he m i s t r y  f r om  t he  J ohns  H opk i ns  Un i v e r s i t y  i n  2 0 0 9   He  h a s  b e e n  a  R e s e a r c h  A s s o c i a t e  wi t h  a  j o i n t  a p p o i n t m e n t  a t  t h e  U n i v e r s i t y  o f  M a r y l a n d   Ba l t i m o r e  C o u n t y  a n d  N AS A G o d d a r d  S p a c e  Fl i  Ce n t e r  s i n c e  2 0 1 1   H i s  r e s e a r c h  f o c u s e s  o n  t h e  d e t e c t i o n  of  t r ac e  e l e m e nt  and as t r obi ol ogi c al l y  r e l e v ant  or gani c  mo l e c u l e s  i n  p l a n e t a r y  s y s t e ms   l i k e  M a r s   He  i s  es p eci a l l y i n t er es t ed  i n  t h e d evel o p m en t  o f  T i m e of  and I on T r ap m as s  s pe c t r om e t e r s w i t h v a r i o u s i o n i z a t i o n  ng te c h n iq u e s   Wi l l  B r i n c k e r h o f f  sp a c e  sc i e n t i st  i n  t h e  Pl a n e t a r y  En v i r o n m e n t s  La b  a t  N A S A  s  G o d d a r d  Spac e  F l i ght  C e nt e r  i n Gr e e n b e l t   M D w i t h  pr i m ar y  r e s pons i bi l i t y  f or  th e  d e v e lo p m e n t o f th e  L D TO F  m a s s  s p e c t r o  th is  p r o je c t H e  h a s  fo c u s e d  re c e n t l y  o n  t h e  d e v e l o p m e n t  o f  m i n i a t u re  l a se r d ma s s  s p e c t r o me t e r s  f o r  f u t u r e  p l a n e t a r y  mi s s i o n s  a l o n g  wi t h  b a s i c  e x p e r i m e n t a l  r e s e a r c h  i n  a s t r o b i o l o g y  a n d  p r e bi ot i c  s y nt he s i s   D r   B r i nc k e r hof f  i s  i nv ol v e d i n t he  de v e l opm e nt  of  m as s  s pe c t r om e t e r  f or  bot h t he  2011 Ma r s  S c i e n c e  L a b o r a t o r y  a n d  t h e  2 0 1 8  E x o Ma r s  mi s s i o n s   


  14   


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a ìkey, valueî list using an XSTL  Queries made against this list of ìkey, valueî pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


