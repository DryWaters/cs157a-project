A Survey of Stability Analysis of Feature Subset Selection Techniques Taghi M Khoshgoftaar   Alireza Fazelpour   Huanjing Wang   and Randall Wald   Florida Atlantic University  Western Kentucky University Email khoshgof@fau.edu afazelpo@my.fau.edu huanjing.wang@wku.edu rwald1@fau.edu Abstract With the proliferation of high-dimensional datasets across many application domains in recent years feature selection has become an important data mining task due to its capability to improve both performance and computational efìciencies The chosen feature subset is important not only due to its ability to improve classiìcation performance 
but also because in some domains knowing the most important features is an end unto itself In this latter case one important property of a feature selection method is stability which refers to insensitivity robustness of the selected features to small changes in the training dataset In this survey paper we discuss the problem of stability its importance and various stability measures used to evaluate feature subsets We place special focus on the problem of stability as it applies to subset evaluation approaches whether they are selected through lterbased subset techniques or wrapper-based subset selection techniques as opposed to feature ranker stability as subset evaluation stability leads to challenges which have been the subject of less research We also discuss one domain of particular importance where subset evaluation and the 
stability thereof shows particular importance but which has previously had relatively little attention for subset-based feature selection Big Data which originates from bioinformatics Keywords Feature selection subset evaluation stability stability measure similarity measure I I NTRODUCTION One major challenge facing researchers working with Big Data is high dimensionality which occurs when a dataset has a large number of features independent attributes To resolve this problem researchers often utilize a feature selection process to identify and remove features which are irrelevant not useful in classifying the data and redundant provide the same information as other features Selecting a subset of features that are most relevant to the class attribute is a necessary step to create a more meaningful and usable 
model Feature selection has been applied in many application domains speciìcally those known for high dimensionality such as software metrics analysis te xt mining 8 gene e xpression microarray analysis image analysis 29 web mining 31 and intrusion detection The main goal of feature selection is to identify irrelevant or redundant features and select a subset of features which minimizes the prediction errors of classiìers Reducing the number of features in a dataset can lead to simple and faster model training along with improved classiìer performance Another beneìt of feature selection is to gain a better understanding of the model built with a selected subset of features Feature selection techniques are divided into two broad categories wrapper-based and lter-based The former category utilizes a supervised learning algorithm in the process of selecting feature subsets 
The downside of a wrapper-based technique is its high cost of computation and the risk of an overìtted model On the other hand lter-based feature selection utilizes the intrinsic characteristics of the data to evaluate the attributes for inclusion in the subset of features The lter method has advantages over the wrapper since it is faster computationally and it selects features using the data characteristics rather than an external learner Both of these groups can again be divided into either ranker-based or subset evaluationbased depending on whether they examine features individually or in groups Rankers are much more computationally efìcient but are unable to identify redundant features while subset evaluation is the opposite in both of these properties One common criterion to evaluate feature selection methods is the performance of a chosen classiìer trained with the selected 
feature subset More recently some studies have considered another criterion for evaluating feature selection methods namely stability The concept of stability has been extensively studied in the context of inductive learning systems The stability e.g rob ustness of feature selection methods has been used to examine the sensitivity of these methods to changes in their input data Stability is normally deìned as the degree of agreement between its outputs to randomly selected subsets of the same input data 22 The need for consistent outputs from feature selection methods is very important in application domains where the prime concern is knowledge discovery Those methods whose outputs are insensiti v e to changes in the input data are said to be stable and they are preferred over those methods that produce inconsistent outputs To measure the stability of a feature subset selection technique a 
similarity measure is needed to assess the overlap of a pair of feature subsets The similarity measure is used to compute the stability of a feature subset selection technique as the average similarity over all pairs of feature subsets The similarity measures discussed in this survey paper are the Hamming distance 7 the Jaccard index and the closely-related T animoto distance 6 Spear manês rank correlation coefìcient the consistenc y inde x 20 and Shannon entropy One challenge when e v aluating stability is the question of whether the metric makes sense when the feature selection technique can potentially produce feature subsets of varying sizes as is the case for lter-based subset evaluation and wrapperbased subset selection techniques All of the discussed measures other than consistency index Spearmanês rank correlation coefìcient and Shannon entropy are able to properly calculate stability even in 
this case The remainder of this paper is presented as follows Section II presents an overview of feature selection techniques including lterbased wrapper-based and embedded approaches Section III outlines methods of measuring the stability of feature subset selection techniques In Section IV we take a closer look at one particular area of feature selection stability research which yet remains a challenge the stability of subset evaluation for big bioinformatics data Finally Section V presents our ideas for future work while Section VI covers our conclusions 424 IEEE IRI 2013, August 14-16, 2013, San Francisco, California, USA 978-1-4799-1050-2/13/$31.00 ©2013 IEEE 


II F EATURE S ELECTION T ECHNIQUES Due to the increased prevalence of high-dimensional datasets across many application domains feature selection has become an essential preprocessing step in many data mining tasks in addition to being important on its own for knowledge discovery Saeys et al listed three main objecti v e s for a feature selection technique 1 to improve the performance of a classiìcation model and avoid overìtting 2 to improve the efìciency of the model in terms of cost and time and 3 to provide deeper insight into the underlying characteristics of the dataset Feature selection techniques can be organized into three categories depending on how they utilize various feature selection search methods when constructing the classiìcation model 1 lter-based methods 2 wrapper-based methods and 3 embedded methods Due to space limitations we brieîy describe all three feature selection techniques in this survey paper below as well as the different search techniques used with subset evaluation approaches and we ask interested readers to consult Saeys et al Liu and Y u 21 and Guyon and Elisseeff for more information The rst reference provides an excellent taxonomy of feature selection techniques with pros cons and examples of each method The second provides a three-dimensional framework based on search strategies to identify subsets of features data-mining tasks classiìcation vs clustering and evaluation criteria to assess the goodness of the selected feature subset The third outlines key approaches used for attribute selection including feature construction feature ranking multivariate feature selection efìcient search methods and feature validity assessment methods A Filter-based Feature Selection Filter-based feature selection techniques are divided into two subcategories 1 lter-based feature ranking and 2 lter-based feature subset evaluation Filter-based feature selection utilizes the intrinsic characteristics of the data using some kind of statistical criterion to evaluate the merit of 1 each individual feature in the case of lterbased feature ranking known as ranker or univariate techniques or 2 the entire feature subsets in the case of lter-based subset evaluation known as subset-evaluation or multivariate techniques The beneìts of the multivariate techniques are as follows they model feature dependencies they are independent of the classiìer and they are more efìcient than wrapper methods in terms of computational complexity Their dra wbacks are as follo ws the y are slo wer than univariate ranker techniques they are less stable than univariate techniques and they ignore interaction with the classiìer so they are outperformed by the wrapper technique in general Bol  on-Canedo et al and Peteoro-Barral et al 26 conducted some experiments to investigate the stability of three lter-based subset evaluation methods These techniques are as follows 1 Correlation-based Feature Selection CFS 2 the Consistency-based Filter and 3 the INTERA CT algorithm 36 The CFS is a multivariate algorithm that evaluates feature subsets according to a correlation based heuristic function Theoretically  redundant features and irrelevant features should be ignored The Consistencybased Filter e v aluates each subset of features by the le v e l o f consistency in the class values for the training dataset The INTERACT algorithm is a feature subset method based on symmetrical uncertainty SU that handles feature interaction and selects relevant features B Wrapper-based and Embedded Feature Selection Wrapper-based feature selection techniques utilize a supervised learning algorithm in the process of selecting feature subsets As with lter-based techniques wrappers can be ranker or subset evaluation techniques although the latter is far more common with the exception of Support Vector Machine-based feature ranking which uses the individual feature weights assigned during the creation of the Support Vector Machine learner to rank the features With wrapperbased subset evaluation however features are evaluated in context i.e dependencies and correlations between features are considered This is useful for discovering which features are either redundant or correlated with each other On the other hand searches through such a large space of dimensionality to nd the optimal set of features are infeasible The beneìts of the wrapper-based subset evaluation techniques are as follows they model feature dependencies and they interact with the classiìer so they have the potential to outperform rankers Their drawbacks are as follows they are computationally expensive and probably prohibitive for datasets with high-dimensionality and they have higher risk of overìtting Due to their complexity few papers have compared wrapper-based subset evaluation techniques with lter-based techniques Kohavi and John compared models b uilt with wrapper feature selection to those using one form of lter-based feature ranking as well as a model built with no feature selection and found that wrapper-based subset selection was able to remove noisy and redundant features and help improve the performance of classiìcation models Hall and Holmes in v estigated six attrib ute selection techniques that produce ranked lists of attributes and applied them to several data sets from the UCI machine learning repository comparing these with a lter-based approach They found no single best approach for all situations but overall wrappers were the best attribute selection schema in terms of accuracy if the speed of execution was not considered Inza et al compared six lter based feature rank ers four which operate on discrete features two which operate on continuous features to a wrapper-based approach on gene microarray data and found that wrapper-based gene selection outperformed lterbased rankers for most learners aside from na  ve Bayes while selecting far fewer features This additional power came at the cost of increased computational load however Embedded techniques integrate the search for an optimal feature subset into the process of building a classiìer Like wrapper techniques embedded methods are speciìc to a given classiìcation algorithm The beneìts of the embedded techniques are as follows they model feature dependencies they interact with the classiìer and in terms of computational complexity they are better than wrapper methods Their drawbacks are as follows they are computationally more expensive than lter-based subset evaluation techniques and they are classiìer dependent C Search Techniques Dunne et al stated that the most common search strate gies to generate subsets of features are based on stepwise addition or deletion of features A sequential selection proceeds by adding or removing features from the current set to form a new set Then this new set is evaluated using some validation procedure such as cross-validation and if the new set of features is superior then it replaces the current best set and this process continues until no more valid operations addition or deletion can be carried out or if no new candidate set outperforms the best current one Forward sequential selection FSS begins with an empty set of features genes and searches for the best feature to be added at each 425 


step If all the features are added or there is no improvement from adding any further features the search stops and returns the current set as the optimal set The FSS has a maximum search length of N iterations N is the total number of features present in the training dataset The objective of the search is to add only the relevant features to the current optimal set while ignoring the irrelevant and redundant features Backward sequential selection BSS starts with a full set including all features and searches for a feature to be removed at each step The resulting set is evaluated using some validation procedure such as cross-validation and if the new set of features is optimal then it replaces the current best set this process continues until reaching an empty set or the subsequent removal of any feature degrades the current performance and the search ends The objective of the BSS is to consider contribution of all feature rst and tries to remove the most irrelevant or redundant feature leaving a smaller and more optimal set The BSS is more computationally expensive than FSS because the BSS starts with all of the features at the start of the search process However the BSS outperforms the FSS technique in terms of classiìcation performance due to the fact that it initially includes all the features in the set A third type of search strategy is called hill-climbing that either adds or removes one feature at a time The search starts with a random set of features and then attempts the effect of toggling the current status of each feature in the set i.e removing an existing feature from the set or adding a feature that is not currently in the set then evaluate the new set and choose the optimum set The stopping criteria for the search process is when we reach the limit on the number of iterations that we set and return the last optimum set of features III S TABILITY OF F EATURE S ELECTION T ECHNIQUES The stability of feature subset selection techniques is deìned as insensitivity of the result of a feature selection method to minor variations in the training dataset This issue is crucial in many applications where feature selection is used as a knowledge discovery tool to identify the top features relevant to the phenomena of interest Generally the classiìcation performance is considered the ultimate quality measure not only for assessing classiìcation models but even when evaluating the feature selection algorithms The perfor mance of a classiìer based on particular sets of selected features does not necessary imply that these selected sets are robust thus the corresponding feature selection technique is stable For example in biomarker identiìcation knowledge discovery application a feature selection algorithm may choose different subsets of features for different subsamples variations in the training set but most of these subsets can produce similar results in terms of classiìcation performance 35 Such instability undermines the conìdence of domain experts in any of the various subsets of features selected for the task of biomarker identiìcation in particular or knowledge discovery in general It is well known that the stability of a feature selection technique does not reveal much about the performance of the selected features because in high-dimensional datasets man y features are correlated with each other and/or redundant for the task at hand Therefore the question of how to select the most relevant features from different runs of the feature selection technique is very crucial The stability of feature selection is very critical and misleading conclusions may be drawn when ignoring stability issues of feature subset selection techniques Measuring stability requires two aspects a framework for studying stability and a stability measurement The framework describes how to make changes on the input datasets in order to study the stability of a given technique while the stability measurement is the speciìc metric for measuring stability We further classify the stability measurement to two categories measurement for same size of feature subsets and for varying sizes of feature subsets A Framework One common method for making changes on the input dataset is perturbation Consider a dataset with m instances a smaller dataset can be generated by keeping a fraction c of instances and randomly removing 1  c of instances from the original data where c is greater than 0 and less than 1 For a given c  this process can be performed x times This will create x new subsamples each having c  m instances where each of these new subsamples is unique since each was built by randomly removing 1  c   m instances from the original dataset In circumstances involving class imbalance datasets with extreme variety in class sizes this subsampling may be performed on a per-class basis to ensure the subsamples have the same class ratios as the original dataset Researchers then apply the feature selection method in question to each of the datasets all of the reduced datasets and sometimes the original dataset as well and create a feature subset for each of the subsamples Some researchers consider the random subsamples the perturbed datasets from the original dataset and compare the feature subsets chosen on these subsamples with each other others compare the feature subsets chosen on the subsamples with those chosen from the original dataset Wang et al performed an empirical study to in v estigate the stability robustness and classiìcation performance of eighteen lterbased feature ranking techniques on four different levels of perturbation  c was set to 95 90 80 or 66.67 on three real-world software engineering datasets Results demonstrated that the number of instances deleted from the dataset affects the stability of the feature ranking techniques the fewer instances removed from a given dataset the less the selected features will change when compared to the original dataset and thus the feature ranking performed on this dataset will be more stable The perturbation method above does not control for the degree of overlap between the subsamples being compared instead leaving this to random chance This makes it difìcult to determine whether the similarity between feature subsets is due to the similarity of the underlying datasets or is a property of the feature selection technique used Wang et al proposed a Fix ed-Ov erlap P artitions Algorithm which will create two subsets which have the same number of instances and a speciìed level of overlap Note in this algorithm that c  the desired degree of overlap can vary from 0 to 1 including the endpoints A choice of c 0 will nd two entirely disjoint subsets which will each contain half of the instances from the original dataset On the other hand c 1 will create two copies of the original dataset which share all instances This is generally not an interesting case to study but is permitted by the algorithm Results show that once again the degree of overlap and feature subset size do affect the stability of feature selection methods Although few works consider the impact of dataset similarity when performing perturbation experiments one paper by Alelyani et al did so In this paper  the authors noted that without controlling for similarity it is difìcult to tell whether two feature subsets are different due to underlying stability issues with the ranker or due to differences in the datasets they were drawn from To evaluate this the researchers sampled 25 of the instances into one subset and 426 


then created nine more subsets with exactly c of their instances in common with the rst The pairwise stability of the features from these subsets were evaluated as c varied from 0 to 1 They found that some algorithms were not able to outperform the inherent stability of the underlying datasets and so should not be considered stable regardless of their stability performance Haury et al considered the role of o v erlap when measuring the stability of gene subsets In addition to other analysis of their datasets the researchers considered the fraction of instances in common when comparing feature lists generated from subsamples of the original data which either have 80 or 0 overlap They also compared feature lists among four distinct but related datasets They found that the similarity measures for the 0 overlap case more closely resembled the between-datasets case than did the results from the 80 overlap case However unlike the 0 case where it is noted that the original data was divided into two mutually-exclusive groups which therefore have 0 overlap for the 80 case the two groups were generated by adding 80 of the data from the original dataset into each group and then splitting the remaining 20 in half and putting each half into one of the groups Thus the 80 refers to proportion of the original data shared by the two groups not the overlap between the two groups This makes it difìcult to generalize the approach to create datasets with arbitrarily-chosen overlaps Another method to make changes on the input data is cross validation During the experiments x runs of n fold cross-validation are performed For each of the n folds one fold is used as test data while the other n  1 are used as the training data For the purpose of measuring stability researchers apply the feature selection method on the n  1 folds at each run Loscalzo et al used 10-fold crossvalidation to evaluate stability of a feature selection method They apply SVM-RFE to 9 out of the 10 folds repeatedly The stability of SVM-RFE is calculated based on the average pair-wise subset similarity of the top 10 features selected over the 10 folds B Stability Metrics In order to measure stability rst we have to decide upon the measurement metric In recent years there have been a number of different stability measurements implemented for this exact purpose These stability measurements are based on the Hamming Distance the Jaccard index and its generalization the T animoto distance 6 the consistency index and consistenc y-based similarity measure Spearman s rank correlation coef cient 17 and Shannon entropy Dunne et al e v aluate the stability of a feature selection method using Hamming distance Let S i and S j be subsets of features H  S i S j  n  k 1  S ik  S jk  1 where n is the total number of features in the dataset and S ik denotes the k th feature of subset S i  Each subset is represented by a binary vector and each value in this vector is either 1 or 0 The value 1 at the position k of this binary vector indicates that the feature k is in the set and the value 0 at the position k indicates that the feature k is not in the set Thus given a set W of subsets of features the total Hamming distance H t  is computed as follows H t   W  1  i 1  W   j  i 1 H  S i S j  2 The overall stability across all pairwise feature subset in S is then deìned by the average normalized Hamming Distance obtained as follows  H  2  H t  n  W    W  1 3 Somol and Novovi  cov  a e xtended a number of stability metrics including Hamming Distance They deìned the Normalized Hamming Index as NHI 1  H  S i S j  n 4 The overall stability across all pairwise feature subset in W is then deìned by the average normalized Hamming Distance obtained as follows AN HI  2     W  1 i 1   W  j  i 1 NHI  S i S j   W    W  1 5 These two measures give the variation information in a set of feature subsets The higher the average normalized Hamming Distance the more information One drawback of the measures is that they do not count the intersection between two subsets Kalousis et al Alelyani et al 1 and Peteiro-Barral et al 26 used the Jaccard index or Jaccard similarity coef cient as a metric for comparing the diversity of subsets of features Let S i and S j be two different subset of features the Jaccard index is deìned as the cardinality of the intersection divided by the cardinality of the union of the two sets It is shown in Equation 6 J  S i S j   S i  S j   S i  S j  6 Alelyani et al used the Jaccard inde x t o measure similarity between feature sets and deìned the stability as the average of similarities across all W runs of the feature selection algorithm The stability index is shown in Equation 7 S J  2  W    W  1  W  1  i 1  W   j  i 1 J  S i S j  7 The stability index  S J  deìned in Equation 7 varies in the interval of where v alues near zero mean the feature selection results are not stable and values near 1 mean the results are stable The value 1 means the results are identical By some set operations the Equation 6 can be written as Equation 8 T  S i S j  J  S i S j    S i    S j  2  S i  S j   S i    S j  S i  S j  8 Kalousis et al used the T animoto coef cient that is a generalized version of Jaccard index to measure similarity between two subsets of features genes The stability is computed as an average across several runs of a cross-validation procedure The Tanimoto coefìcient supports multiple classes and in the case of binary classes it is reduced to the Jaccard index The Tanimoto distance is shown in Equation 8 and it measures the amount of overlap between two sets of arbitrary cardinality Peteiro-Barral et al used the Jaccard inde x with some v ariations across two datasets using three lter-based subset evaluation CFS Consistency and INTERACT described in section II above they concluded that the Jaccard index is more inîuenced by the number of instances in the training dataset Kuncheva de v eloped an enhanced similarity measure called consistency index that takes into account the similarity between 427 


subsets of features due to chance randomness Let S i and S j be subsets of features with equal cardinality i.e  S i    S j   t  The consistency index is deìned in Equation 9 I C  S i S j  dn  t 2  t  n  t  9 where d is the cardinality of the intersection between two subsets S i and S j  n is the total number of features in the training datasets and 1 I C  S i  S j   1 The greater the consistency index the more similar the subsets are A consistency index close to zero means that both subsets are similar to being drawn by chance randomness Kuncheva et al e xtended their consistenc y inde x t o e xpand beyond the comparison of just two subsets by taking the average across all pairwise consistency indices Note that all feature subsets have the same size k  AI C  2  W    W  1  W  1  i 1  W   j  i 1 I C  S i S j  10 To improve Kunchevaês similarity measure Lustgarten et al propose the similarity measure S a  S i S j   S i  S j   S i  S j  n min   S i    S j    max 0   S i    S j  n  11 Note that S a varies from 1 to 1 where a value of 0 represents the stability of random feature selection positive values indicate particularly stable feature selection and negative values represent stability lower than that of random feature selection A measure was devised which combines the results of multiple measures into the new stability index called adjusted stability measure ASM that takes role of chance into consideration and that can calculate the stability of lists of varying size The ASM for W subsets is calculated as ASM  2  W    W  1  W  1  i 1  W   j  i 1 S a  S i S j  12 Lustgarten et al presented their results on a single proteomic dataset using three feature selection techniques They showed that their adjusted measure presents improved evaluation of the stability of the feature selection methods The above stability metrics measure similarity on feature subsets These subsets can be selected by either feature subset selection methods or feature ranking methods where the top t features are selected from a ranked list Kalousis et al presented a stability measure that use Spearmanês rank correlation coefìcient S P  S i S j   6  k  S ik  S jk  2 t  t 2  1  13 where S ik and S jk are the rank of feature k in rankings S i and S j  respectively and t is the number of features being selected For a set W of subsets of features obtained from  W  ranking lists the overall stability is computed as follows AS P  2  W    W  1  W  1  i 1  W   j  i 1 S P  S i S j  14 Shannon entropy-based stability measurement starts from the idea that feature selection is choosing one of the 2 n possible feature subsets given the original n features Random feature selection will choose randomly from within this collection while stable feature selection will consistently choose the same or similar subsets Perturbing the data allows for sampling the space of feature subsets chosen by a given feature selection approach and observing the entropy of the resulting subsets allows the user to see how much variation there is among these Shannon entropy is deìned as H  X   m  i 1 p  x i og 2  p  x i  15 where X is a random variable representing the m possible outcomes in this case m 2 n for the number of possible feature subsets and p  x i  is the probability of the i th outcome In total N total trials are performed and G jt is the frequency of the j th feature subset which contains t features Note that j ranges from 1 to C  n t   n t   because that is how many possible combinations of n features can be made choosing t at a time We let G jt  G jt N be the normalized frequency of each feature subset We then can nd the overall entropy from a collection of subsets with a chosen t   k   C  n,t   j 1 G jt log 2  G jt  16 Among these similarity measures only the stability measure proposed in 20 the Spearman s rank correlation coef cient and the Shannon entropy require that the feature subset size be the same for all compared subsets All other measurements can be used with varying feature subset sizes and thus are appropriate for considering the similarity of subset evaluation techniques where the feature subset size is not guaranteed to be constant for all subsets IV S POTLIGHT T OPIC S UBSET E VALUATION FOR B IOINFORMATICS While high dimensionality is important across a number of different application domains one of particular note is bioinformatics This application domain due to the underlying nature of the problem is rife with Big Data as it frequently has extremely high numbers of features 2,000 or more and a large number of redundant or irrelevant features and in addition frequently leads to datasets with very small numbers of instances 200 or fewer For example in the case of gene microarray datasets gene expression levels are collected for thousands of genes but only a relatively small number of patients are available in each study and the majority of genes will not have direct relevance to the medical question being addressed on the other hand those genes which are relevant may be part of a regulatory network that shows highly correlated e.g redundant behavior Although these problems are those most in need of more advanced feature selection approaches such as lter-based subset evaluation wrapper-based subset selection and embedded approaches the large number of features has so far stymied most efforts These problems apply even more so to the question of feature selection stability the large and noisy collection of features and the biological importance of selecting the right genes heightens the importance of gene selection stability but the necessity of generating multiple feature subsets to compare for stability purposes magniìes the computational challenges by at least an order of magnitude In this section we review a number of papers which have begun to consider the topic of stability in bioinformatics although we also discuss how these were limited in their scope by the magnitude of the problem Zengyou He and Weichuan Yu pro vided a r e vie w o f the topic of stable biomarker selection They identiìed the underlying biological and methodological sources of instability They then discussed a number of approaches for improving the stability of feature selection in bioinformatics One approach relevant for feature ranking but not 428 


subset selection is ensemble ranking which builds multiple ranked lists either through different perturbations of the original data or through different choices of feature ranking technique and combines these Alternately one may a priori give higher weight to features which are known through domain knowledge to have importance for the problem at hand As for more general approaches one cause for instability is multiple biomarkers being highly redundant with one another and thus varying which is selected when using subset evaluation techniques since the rst biomarker chosen from each group will preclude the inclusion of the others and which happens to be chosen rst may vary depending on data modiìcations Thus by identifying these groups in advance either through data-driven approaches such as clustering knowledge-driven approaches using existing results the stability of such feature selection approaches may be improved Following this discussion of techniques to improve feature selection stability the authors presented a number of metrics for measuring stability including both metrics which require a constant feature subset size and hence which work best with rankers and those which do not have this requirement and hence are appropriate for evaluating the stability of subset-based techniques The authors concluded that stability is an important element of biomarker selection which must be considered in order to contextualize the results Our critique of this paper is that subset selection techniques such as lter-based subset evaluators and wrapper-based subset selection are not mentioned explicitly and in fact subset evaluation is only mentioned in passing as one application of the more general stability metrics Also as this is a review paper no experiments were performed Somol and Novovi  cov  a conducted a comprehensi v e study of stability of feature selection techniques and investigated the problem of evaluating the stability of feature selection techniques that produce subsets of varying size They examined a number of existing stability metrics and note that many of these suffer at least one of two drawbacks 1 various stability measures are differently bounded and thus are difìcult to compare and 2 most stability measures are considered for feature selection techniques with speciìed subset size although many feature selection techniques allow the subset size to be optimized in the process of searching for the most relevant and discriminatory features genes The second issue varying subset size is crucial particularly for the subset-based feature selection techniques lter-based subset evaluation and wrapper-based subset selection since the number of selected features are optimized by the search algorithms and the same algorithm operating on slightly perturbed versions of the same data may produce subsets of varying size In fact stability metrics may unfairly assign greater stability to larger subsets simply due to the larger chance for random overlap The authors referred to this problem as subset-size bias problem The authors discussed how each metric performs in terms of these drawbacks and also proposed a number of new metrics both extending the existing metrics and in one case designing a metric from scratch to have neither drawback To evaluate the new stability metrics the researchers conduced an empirical study on real data available from the UCI Repository They used six datasets with 10 65 features and one dataset with 10,105 features they used three wrapper techniques Bayesian classiìer assuming normal distribution 3-nearest neighbor with majority voting and support vector machine on the former datasets and applied a lter technique to the latter dataset 10,105 features This is due to the fact that applying wrapper techniques to high-dimensional datasets is not practical in terms of computational complexities Our critique on the shortcoming of this work is that although the proposed framework is useful and examines the problem of feature stability metrics which are appropriate for subset evaluation-based feature selection the wrapper-based feature selection techniques were only applied to datasets which are not highdimensional 10Ö65 features while a lter-based feature selection technique ranker was used for the dataset with high-dimensionality 10,105 features Thus although the proposed stability metrics and framework are useful for understanding high-dimensional data they were not actually demonstrated in this capacity in the context of subset evaluation-based feature selection Lustgarten et al proposed a stability measure called the Adjusted Stability Measure ASM based upon extending the consistency index to varying feature subset size as opposed to Unadjusted Stability Measure USM based on the Jaccard index that computes robustness of a feature selection technique with respect to random feature selection method The authors stated that the ASM is superior to other measures that do not account for random feature selection a property it shares with the consistency index That is ASM is capable of indicating how much better or worse a feature selection method is over one that selects features at random The researchers also discussed many of the stability measures presented in Section III above To demonstrate how these two measures ASM and USM give different results the authors measured the stability of wrapperbased subset evaluation using greedy forward selection based on three classiìcation algorithms Support Vector Machine Logistic Regression and Na  ve Bayes with a proteomic dataset obtained from the University of Pittsburgh Cancer Institute The dataset contains 240 instances and a total of 70 features protein probes and a binary class variable cancer vs healthy They concluded that lower stability may indicate either the feature selection method is not robust or the data contains many correlated redundant or noisy features measures that evaluate quality of feature set are unrelated to the measures of stability a stability measure provides no information on the quality of the feature set and a quality measure such as AUC provides no information on the stability of the selected features Our critique on the shortcoming of this work is that the chosen dataset contains only 70 features which is not high-dimensional compared to bioinformatics datasets with features in the range of thousands or even tens of thousands D  az-Uriarte and Alvarez de Andr  es e xamined the performance and stability of an embedded ensemble feature technique centered around random forests an ensemble technique which builds a collection of decision trees which each use random subsets of the data and features In their technique a random forest is constructed and the genes which are used with the least frequency are discarded This process is repeated until a stopping criterion is reached and the genes which remain are those selected In addition the random forest model from this collection which best minimizes the number of features while not sacriìcing performance is used as the classiìcation model This embedded feature selection approach was tested on both simulated and real data The simulated data containing a variable number of classes 2Ö4 independent dimensions 1 3 and genes per dimension 5Ö100 Ten real-world microarray datasets were also used with between 2,000 and 10,000 genes in each dataset The random forest-based approach was compared with three more traditional classiìers Diagonal Linear Discriminant Analysis k-Nearest Neighbor and Support Vector Machines each using F-ratio gene ranking as their feature selection technique In addition two established embedded techniques Shrunken Centroid and Nearest Neighbor  Variable Selection were employed in this comparison Classiìcation performance was evaluated by nding the error rate weighted between resubstitution training set and hold-out set error and the stability of different feature selection techniques was 429 


estimated using the frequency with which features selected from the whole dataset were also selected when using bootstrapped versions of the data The authors found that their proposed random forestbased approach can lead to performance comparable with the existing lter-based ranking and embedded techniques although it does not always produce the best results Of the ten real-world datasets four showed their best performance with lter-based feature selection three were best with one of the previously-described embedded techniques one was best with plain random forest e.g without building multiple random forest models and two were best with the proposed embedded ensemble random forest approach However the proposed approach did consistently produce small feature subsets while the other embedded approaches each left over 1,000 features for four of the ten datasets In terms of stability the proposed approach gave much less stable gene subsets than the previouslydescribed embedded techniques although this apparent stability may have arisen from the larger feature subset sizes of those techniques Overall the authors felt the smaller feature subset size and similar performance justiìed their proposed ensemble approach Our main critique of this paper would be that stability is analyzed in a very na  ve fashion and that only embedded techniques are considered not lter-based feature subset selection or wrapper-based techniques Also although the proposed approach shows promise the random variable subset step inherent in the construction of a random forest model poses the risk of decreasing stability and increasing the chances that an important feature from the original dataset is not considered at all V F UTURE W ORK Based on the lack of research which has examined subset evaluation-based feature selection either lter-based or wrapperbased in the context of bioinformatics more work is needed in this area For example although more works are beginning to consider the problem of choosing an appropriate stability metric there is no consensus as to which are most appropriate In addition no paper considers both lter-based feature subset evaluation and wrapperbased subset selection in the same work especially in the context of biologically-relevant datasets e.g  2,000 features The large scale of data does pose real challenges and thus a major research target should be approaches which can mitigate these challenges while still bringing the full power of subset evaluation to Big Data bioinformatics datasets One potential avenue for future research is hybrid feature selection which couples a lter-based feature ranker with a subset-based feature selection technique The ranker can reduce the number of features down to a more manageable level while the subset-based technique will be able to eliminate redundant features and nd features that are truly able to improve performance A related concept is performing subset-based selection on a large but not intractable dataset and then comparing these results with a ranker performed on the same dataset This will help demonstrate how these two techniques compare and in particular how many features must be selected from the ranker before all of the important features as deìned using the subset-based results are included Thus this could lead the way for a more data-driven hybrid approach which optimizes the number of features chosen in the rst stage in order to select the important features without spending too much extra time during the second subset-based phase Finally future work may consider other ensemble and resampling techniques to reduce the number of features to a manageable level before performing subset evaluation in order to nd a near-optimal subset without searching the entire feature space All of these have the potential to open new avenues for identifying the most important genes for bioinformatics problems even when starting with thousands or tens of thousands of genes More generally future work on the topic of subset selection stability across all domains will include an empirical study using various feature selection techniques including lter-based subset evaluation and wrapper-based subset selection over several datasets from different application domains social media software quality prediction or biomedical datasets Initially we will focus on social media and software quality prediction models because these datasets have lower number of features in general Eventually we will perform an empirical study of subset-based feature selection techniques on biomedical datasets with high-dimensionality using a cluster of computers using HPCC High Performance Computing Cluster that is a massive parallel-processing computing platform to solve Big Data problems VI C ONCLUSIONS With the proliferation of high-dimensional datasets in recent years feature selection has received attention of researchers and data-mining practitioners due to its capability to improve both performance and computational efìciencies A feature subset selected by a feature selection technique is evaluated for relevance toward a task such as classiìcation or identiìcation of the top relevant features corresponding to a phenomenon of interest One important characteristics of a feature selection technique is stability which refers to insensitivity robustness of the selected features to minor changes in the training dataset In this survey paper our focus is on the problem of stability its importance and various stability measures used to evaluate subsets of selected features These subsets of features may be generated using lter-based wrapper-based or embedded techniques although we particularly focus on metrics which can be used for subset evaluationbased approaches In order to measure the stability of a feature selection technique a similarity measure is needed to assess the overlap of a pair of feature subsets The similarity measure computes the stability of a feature subset selection technique as the average similarity over all pairs of feature subsets We discussed various similarity measures in this survey paper such as the Jaccard index the Hamming distance the Consistency index Spearmanês rank correlation coefìcient and Shannon entropy Throughout this study we have discussed current research in stability analysis of feature subset selection techniques within the domain of bioinformatics and have identiìed the shortcomings of these works to explore possible opportunities for future work R EFERENCES  S Alelyani Z Zhao and H Liu  A dilemma in assessing stability of feature selection algorithms in IEEE 13th International Conference on High Performance Computing and Communications HPCC  September 2011 pp 701Ö707  V  Bol  on-Canedo N S  anchez-Maro  no and A Alonso-Betanzos A review of feature selection methods on synthetic data Knowledge and Information Systems  vol 34 no 3 pp 483Ö519 2013  M Dash and H Liu Consistenc y-based search in feature selection  Artiìcial intelligence  vol 151 no 1 pp 155Ö176 2003  R D  az-Uriarte and S Alvarez de Andr  es Gene selection and classiìcation of microarray data using random forest BMC Bioinformatics  vol 7 no 1 p 3 2006 A v ailable http://www.biomedcentral.com/1471-2105/7/3  T  G Dietterich and G Bakiri Solving multiclass learning problems via error-correcting output codes Journal of Artiìcial Intelligence Research  vol 2 no 263 p 286 1995  R O Duda P  E Hart and D G Stork Pattern Classiìcation  Wiley 2012 A v ailable http://books.google.com/books?id Br33IRC3PkQC 430 


 K Dunne P  Cunningham and F  Azuaje Solutions to instability problems with sequential wrapper-based approaches to feature selection Journal of Machine Learning Research  2002  G F orman  A n e xtensi v e empirical study of feature selection metrics for text classiìcation J Mach Learn Res  vol 3 pp 1289Ö1305 2003  T  R Golub D K Slonim P  T amayo C Huard M Gaasenbeek J P  Mesirov H Coller M L Loh J R Downing M A Caligiuri C D Bloomìeld and E S Lander Molecular classiìcation of cancer Class discovery and class prediction by gene expression monitoring Science  vol 286 no 5439 pp 531Ö537 1999 A v ailable http://www.sciencemag.org/cgi/content/abstract/286/5439/531  I Guyon and A Elisseef f  A n introduction to v ariable and feature selection J Mach Learn Res  vol 3 pp 1157Ö1182 2003  I Guyon J W eston S Barnhill and V  V apnik Gene selection for cancer classiìcation using support vector machines Machine Learning  vol 46 pp 389Ö422 2002 A v ailable http dx.doi.org/10.1023/A:1012487302797  M Hall Correlation-based feature selection for machine learning  Ph.D dissertation The University of Waikato Hamilton New Zealand April 1997  M A Hall Correlation-based feature selection for discrete and numeric class machine learning in Proceedings of the Seventeenth International Conference on Machine Learning  Morgan Kaufmann 2000 pp 359 366  A.-C Haury  P  Gestraud and J.-P  V ert The inîuence of feature selection methods on accuracy stability and interpretability of molecular signatures PLoS ONE  vol 6 no 12 p e28210 12 2011 Available http://dx.doi.org/10.1371%2Fjournal.pone.0028210  Z He and W  Y u  Stable feature selection for biomark er disco v ery   Computational Biology and Chemistry  vol 34 no 4 pp 215Ö225 2010 A v ailable http://www sciencedirect.com/science/article pii/S1476927110000502  I n Inza P  Larra  naga R Blanco and A J Cerrolaza Filter versus wrapper gene selection approaches in dna microarray domains Artiìcial Intelligence in Medicine  vol 31 no 2 pp 91Ö103 June 2004  A v ailable http://dx.doi.or g/10.1016/j.artmed.2004.01.007  A Kalousis J Prados and M Hilario Stability of feature selection algorithms a study on high-dimensional spaces Knowledge and Information Systems  vol 12 no 1 pp 95Ö116 May 2007 Available http://dx.doi.org/10.1007/s10115-006-0040-8  R K oha vi and G H John Wrappers for feature subset selection  Artiìcial Intelligence  vol 97 no 1-2 pp 273Ö324 Dec 1997  A v ailable http://dx.doi.or g/10.1016/S00043702\(97 X  P  Kr  zek J Kittler and V Hlav  ac Improving stability of feature selection methods in 12th International Conference on Computer Analysis of Images and Patterns CAIP  ser Lecture Notes in Computer Science Springer August 2007 pp 929Ö936  L I K unche v a   A stability inde x for feature selection  i n Proceedings of the 25th IASTED International Multi-Conference Artiìcial Intelligence and Applications  Anaheim CA USA ACTA Press 2007 pp 390Ö395 A v ailable http://portal.acm.or g citation.cfm?id=1295303.1295370  H Liu and L Y u   T o w ard inte grating feature selection algorithms for classiìcation and clustering IEEE Transactions on Knowledge and Data Engineering  vol 17 no 4 pp 491Ö502 April 2005  S Loscalzo L Y u  and C Ding Consensus group stable feature selection in Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD 09 New York NY USA ACM 2009 pp 567Ö576  J L Lustgarten V  Gopalakrishnan and S V iswesw aran Measuring stability of feature selection in biomedical datasets in AMIA 2009 Annual Symposium Proceedings  2009 pp 406Ö410  H Nguyen K Frank e and S Petro vic Impro ving ef fecti v eness of intrusion detection by correlation feature selection in International Conference on Availability Reliability and Security ARES 10  2010 pp 17Ö24  M S Pepe R Etzioni Z Feng J D Potter  M  L  Thompson M Thornquist M Winget and Y Yasui Phases of biomarker development for early detection of cancer Journal of the National Cancer Institute  vol 93 no 14 pp 1054Ö1061 2001  D Peteiro-Barral V  Bol  on-Canedo A Alonso-Betanzos B GuijarroBerdinas and N Sanchez-Marono Scalability analysis of lter-based methods for feature selection Advances in Smart Systems Research  vol 2 no 1 pp 21Ö26 2012  R Real and J M V a r g as The probabilistic basis of jaccard s inde x of similarity Systematic Biology  vol 45 no 3 pp 380Ö385 1996  A v ailable http://www jstor or g/stable/2413572  Y  Sae ys I n Inza and P  Larranaga  A re vie w of feature selection techniques in bioinformatics Bioinformatics  vol 23 no 19 pp 2507 2517 2007 A v ailable http://bioinformatics.oxfordjournals org/cgi/content/abstract/23/19/2507  J Shotton J W inn C Rother  and A Criminisi T e xtonboost for image understanding Multi-class object recognition and segmentation by jointly modeling texture layout and context International Journal of Computer Vision  vol 81 no 1 pp 2Ö23 2009 A v ailable http://dx.doi.org/10.1007/s11263-007-0109-1  P  Somol and J No v o vi  cov  a Evaluating stability and comparing output of feature selectors that optimize feature subset cardinality IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 32 no 11 pp 1921Ö1939 2010  R W ald T  M Khoshgoftaar  A  Napolitano and C Sumner  Using Twitter content to predict psychopathy in 11th International Conference on Machine Learning and Applications ICMLA  vol 2 2012 pp 394 401  H W ang T  M Khoshgoftaar  and R W ald Measuring rob ustness of feature selection techniques on software engineering datasets in 2011 IEEE International Conference on Information Reuse and Integration IRI  August 2011 pp 309Ö314  H W ang T  M Khoshgoftaar  and A Napolitano Softw are measurement data reduction using ensemble techniques Neurocomputing  vol 92 pp 124Ö132 2012  H W ang T  M Khoshgoftaar  R  W ald and A Napolitano  A no v e l dataset-similarity-aware approach for evaluating stability of software metric selection techniques in 13th IEEE International Conference on Information Reuse and Integration  August 2012  L Y u  C  Ding and S Loscalzo Stable feature selection via dense feature groups in Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  ser KDD 08 New York NY USA ACM 2008 pp 803Ö811 A v ailable http://doi.acm.org/10.1145/1401890.1401986  Z Zhao and H Liu Searching for interacting features  i n Proceedings of the 20th International Joint Conference on Artiìcal Intelligence  Morgan Kaufmann Publishers Inc 2007 pp 1156Ö1161 431 


  9  A X    S C H EM A TI C E S U M M A R Y O F M A JO R H ELI O LI B I N TER F A C ES     H ELI O LI B ES  S Y N TA C TI C  I N TER F A C ES   enum  DataType     OBJECT  DOUBLE  S TRING  TIME  VECTORIJK    Indexable R   R get int  index   public  int  size    DataArray<T extends  Indexable<T     DataIdentity getIdentity   DataType getType   MaplikeMetadata getMetadata    Table   String getName   IMaplikeMetada ta getTableMetadata   int  getNumCols   DataArray<?> getColumn int  i     TableWithTime extends  Table     int  getTimeArrayIndexForColumn int  dataColIndex    TableWithDetailedTime extends  TableWithTime     int  getMeasurementWindowColIndex int  data ColIndex   int  getExposureTimeColIndex int  dataColIndex   ITimeSystem<?> getTimeSystem     MagneticFieldData extends  TableWithDetailedTime     MagColIndices getMagColIndices   MagDatasetContentInfo getDatasetContentInfo    


  10  RE P RE S E NT AT I V E  H B S NCE C  IN T E R F A C E S   MA G N E T I C  F I E L D  D A T A   MagDatasetContentInfo extends  CoordFrameSrc     boolean  hasBTotal   boolean  hasBTotalUncertainty   boolean  hasBVector\(ICoordFrame frame   boolean  hasBVectorUncertainties\(ICoordFrame fram e   String getDataSetName    MagneticFieldData extends  TableWithDetailedTime     MagColIndices getMagColIndices   MagDatasetContentInfo getDatasetContentInfo     EN ERG ET I C  PART I C L E D AT A   PChannelInfo     String getParameterKey   String get Name   String getDescription   Set<IParticleCalibration> getCalibrations   IParticleSpecies getSpecies   IParticleChannelBounds getBounds   IFOVKey getFOVKey    ParticleInfoSrc extends  ICoordFrameSrc     int  getNumChannels   IPCh annelInfo getChannelInfo int  chnlIdx   String getDataName   ISpectralInfo getSpectralInfo    ParticleData extends  TableWithDetailedTime, ParticleInfoSrc     int  getValueColumnIndexForChannel int  channelIndex, ParticleCalibration cal   int  getU ncertColumnIndexForChannel int  channelIndex, ParticleCalibration cal  int  getParticleFlowDirectionVec torIJKIndex int  channelIndex, ICoordFrame coordFrame     EVEN T  D AT A h a s  n o  i n t e r f a c e   i t  s  j u s t  a n d   I n f o   i n t e r f a c e  a d d e d  t o  a  T a b l e W i t h T i m e  


  11   CONDUIT  I n te r fac e s   Timestamp  public  abstract  boolean  hasTimestamp  public  abstract  TSEpoch getEpoch  boolean  hasMET  public  abstract  MET getMET    PacketHeader extends  Timestamp  int  offsetBytesToCargo   void  movePositionToStartOfNextPacket\(ByteBuff er inputBuffer   int  cargoLength    TelemetryRecord extends  Timestamp   int  getNumFields   Field getField int  fieldNum   Field getField\(String fieldName      TelemetryRecordSource   int  getNumberOfRecords   TelemetryRecord getTelemetryRecord int  recordNum   Object getPacketType this is usually an instrument specific enum  value    Subpacket extends  Comparable<Subpacket>,TelemetryRecordSource   IPacketHeader getIPacketHeader       


   12 The scenario tests are designed through a process o f grouping the FM requirements into sequences. The pr ocess is performed using a requirements-to-test allocatio n matrix in which each requirement is assigned to a test or group of tests The selection of which tests form the incomp ressible test suite is performed through consultation and it eration of the core test list with the systems engineering tea m Following the allocation of all requirements to the  various tests, individual test plans are developed that inc lude a set of starting conditions and high-level outline of test and verification activities. Each test plan includes th e following information test name document number tier desig nation test objective, requirements fully or partially ver ified by the test, minimum spacecraft component set required to run the test, initial conditions for the test, required sup port, planning notes, and the high-level test outline   Figure 12 shows the activity flow for each scenario  test during the system-level test phase  Following the development of the system-level verification and te st plan individual test procedures are generated for each t est. A test procedure is a configuration managed document that describes the detailed information necessary to run  the test on the spacecraft and the list of steps taken to ex ecute the test. The test procedure is also the as-run documen t used by the test conductor or FM lead engineer to document the major milestones of the test When possible the te st procedure will be dry run on the high-fidelity simu lators to identify and correct procedural errors. The test pr ocedure is then executed on the observatory during I&T \(eventu ally the procedure is run on both spacecraft Any deviation s from the test procedure are documented within the test p rocedure as red lines \(i.e., permanent changes to the test p rocedure\ or black lines i.e changes for the current procedur e If the test fails due to test procedure errors the test p rocedure is corrected and the process is repeated However if the test fails due to an implementation error in flight hard ware flight software or autonomy a formal discrepancy is documented in the Anomaly/Problem Failure Reporting  APFR system any changes or corrections to autono my flight software, or hardware are implemented and th e test is re-run until the anomaly is resolved and the test r uns to successful completion After the completion of each  test a review of the test procedure as-run information a nd spacecraft data is performed by a member of the FM team Following the review of the test data, the FM lead engineer signs the test procedure confirming that the test w as run and the review accurately described the success or fail ure of the test  n	\r\n\n  n        n   n\r     n r\r\n\r r\n\r r\r\n\r    0/-1  23  n  4      Figure 12:  Fault Protection System-Level Test Proc ess Flow 4  CONCLUSION The RBSP spacecraft are designed to operate for mul tiple years in the extreme environment of the Earthís rad iation belts.  To facilitate this mission, a robust fault management system is integrated into the system architecture f rom the earliest stages of the design.  This fault manageme nt system was rigorously tested during I&T and the results o f this testing show that the RBSP spacecraft are protected against on-orbit fault conditions and therefore have a hig h probability of meeting the mission objectives  The  RBSP fault management design, implementation and test ap proach demonstrate that robust fault-tolerant designs are  achievable for complex systems operating in harsh environments even for systems with only limited or  selective redundancy 5  REFERENCES  1  J   S t r a t t o n  a n d  N   F o x   R a d i a t i o n  B e l t  S t o r m  P robes RBSP\ Mission Overview Proc. of the 2012 IEEE Aerospace Conf. Vol. 027 No. xxx Big Sky, Montana USA, March 2012. The Johns Hopkins University Applied Physics Laboratory, Laurel, MD 20723   2  K   K i r b y  e t  a l   R a d i a t i o n  B e l t  S t o r m  P r o b e s   R B SP Spacecraft and impact of environment on observatory  design Proc. of the 2012  IEEE Aerospace Conf.,Vol  027 No. 1759 Big Sky, Montana, USA, March 2012 The Johns Hopkins University Applied Physics Laboratory, Laurel, MD 20723  


WeD6 \226 Queuing Theory Visitor Center   Chair Starobinski, David Boston University On the Channel-Sensitive Delay Behavior of LIFO-Backpressure 715  Si, Wei Boston University  Starobinski, David Boston University  Transient Flow Level Models for In terference-Coupled Cellular Networks 723  326hmann, David Technische Universit\344t Dresden  Fehske, Albrecht Technische Universit\344t Dresden  Fettweis, Gerhard Technische Universit\344t Dresden  When do Redundant Requests Reduce Latency 731  Shah, Nihar B University of California, Berkeley  Lee, Kangwook University of California, Berkeley  Ramchandran, Kannan University of California, Berkeley  Scheduling in Cooperative Cognitive Radio Networks 739  Das, Dibakar Rensselaer Polytechnic Institute  Abouzeid, Alhussein A Rensselaer Polytechnic Institute  Codreanu, Marian University of Oulu  Batch Job Scheduling for Reducing Water Footprints in Data Center 747  Ren, Shaolei Florida International University  Bits through Bufferless Queues 755  Tavan, Mehrnaz Rutgers University  Yates, Roy D Rutgers University  Bajwa, Waheed U Rutgers University  ThA1 \226 Information-Theoretic Security Library   Chair Kiyavash, Negar University of Illinois  Organizer\(s Ba ar, Tamer University of Illinois  Kiyavash, Negar University of Illinois Networked Cyber-Physical Systems: Interdep endence, Resilience and Information Exchange 763  Zhu, Quanyan Princeton University  Bushnell, Linda University of Washington  On the Secure Interference Channel 770  Somekh-Baruch, Anelia Bar-Ilan University  How Many Antennas does a Cooperative Jammer Need for Achieving the Degrees of Freedom of Multiple Antenna Gaussian Channels in the Presence of an Eavesdropper 774  Nafea, Mohamed Pennsylvania State University  Yener, Aylin Pennsylvania State University  Distributed Consensus with Byzantine Adversaries n/a  Liu, Xiangyang University of Maryland  Gao, Peixin University of Maryland  Baras, John University of Maryland  


Why Cyber-Insurance Contracts Fail to Reflect Cyber-Risks 781  Schwartz, Galina University of California, Berkeley  Shetty, Nikhil Insieme Networks  Walrand, Jean University of California, Berkeley  Performance-Aware IP Address Randomi zation in Moving Target Defense n/a  Clark, Andrew University of Washington  Sun, Kun George Mason University  Poovendran, Radha University of Washington  ThA2 \226 Distributed Stor age: Theory and Practice Solarium   Chair Milenkovic, Olgica University of Illinois  Organizer\(s Dimakis, Alex University of Texas, Austin  Milenkovic, Olgica University of Illinois Explicit Maximally Recoverable Codes with Locality n/a  Gopalan, Parikshit Microsoft Research  Huang, Cheng Microsoft Research  Jenkins, Bob Microsoft Research  Yekhanin, Sergey Microsoft Research  On the Interior Points of the Storage-Repa ir Bandwidth Tradeoff of Regenerating Codes 788  Sasidharan, Birenjith Indian Institute of Science  Kumar, P. Vijay Indian Institute of Science  A Family of Locally Recoverable Codes n/a  Tamo, Itzhak University of Maryland  Barg, Alexander University of Maryland  Locality and Availability in Distributed Storage n/a  Rawat, Ankit Singh University of Texas, Austin  Papailiopoulos, Dimitris University of Texas, Austin  Dimakis, Alex University of Texas, Austin  Vishwanath, Sriram University of Texas, Austin  Data Secrecy in Distributed Storage Systems under Exact Repair n/a  El Rouayheb, Salim Illinois Institute of Technology, Chicago  Goparaju, Sreechakra Princeton University  Poor, H. Vincent Princeton University  Calderbank, A. Robert Duke University  LRC Erasure Coding in Windows Storage: From Cloud to Desktop n/a  Huang, Cheng Microsoft Research  ThA3 \226 Information Theory II Butternut   Chair  Walsh, John MacLaren Drexel University Matroid Bounds on the Region of Entropic Vectors 796  Li, Congduan Drexel University  Walsh, John MacLaren Drexel University  Weber, Steven Drexel University  


Groups and Information Inequalities in 5 Variables 804  Markin, Nadya Nanyang Technological University  Thomas, Eldho Nanyang Technological University  Oggier, Fr\351d\351rique Nanyang Technological University  Superadditivity of Quantum Channel Coding Rate with Finite Blocklength Quantum Measurements 810  Chung, Hye Won Massachusetts Institute of Technology  Zheng, Lizhong Massachusetts Institute of Technology  Information Rates in the Optica l Nonlinear Phase Noise Channel 818  Dar, Ronen Tel Aviv University  Shtaif, Mark Tel Aviv University  Feder, Meir Tel Aviv University  An Outer Bound of the Capacity Re gion of Biometric Systems under Keys, Secrets, and Privacy Requirements 824  Lai, Po-Hsiang Washington University in Saint Louis  O'Sullivan, Joseph A Washington University in Saint Louis  Generalized Cut-Set Bounds for Broadcast Networks 832  Salimi, Amir Texas A&M University  Liu, Tie Texas A&M University  Cui, Shuguang Texas A&M University  ThA4 \226 Sensor Networks Pine   Chair Scaglione, Anna University of California, Davis How to Sleep, Control and Transfer Data in an Energy Constrained Wireless Sensor Network 839  Venkateswaran, Vijay Alcatel-Lucent Bell Labs  Kennedy, Irwin O Alcatel-Lucent Bell Labs  Network Observability and Localization of the So urce of Diffusion based on a Subset of Nodes 847  Zejnilovic, Sabina Carnegie Mellon University  Gomes, Jo\343o Instituto Superior T\351cnico  Sinopoli, Bruno Carnegie Mellon University  The Central Detection Officer Problem: SALS A Detector and Performance Guarantees 853  Li, Xiao University of California, Davis  Poor, H. Vincent Princeton University  Scaglione, Anna University of California, Davis  A New Graph Model with Random Edge Values: Connectivity and Diameter 861  La, Richard J University of Maryland, College Park  Kabkab, Maya University of Maryland, College Park  


ThA5 \226 Source Coding Lower Level   Chair Cohen, Asaf Ben-Gurion University Correlated Sources with Actions 869  Sabag, Oron Ben Gurion University  Permuter, Haim H Ben Gurion University  Cohen, Asaf Ben Gurion University  Non-Asymptotic Bounds on Fixed Leng th Source Coding for Markov Chains 875  Hayashi, Masahito Nagoya University  Watanabe, Shun University of Tokushima  Efficient Similarity Queri es via Lossy Compression 883  Ochoa, Idoia Stanford University  Ingber, Amir Stanford University  Weissman, Tsachy Stanford University  Results on the Optimal Memory-Assisted Univer sal Compression Performance for Mixture Sources 890  Beirami, Ahmad Georgia Institute of Technology  Sardari, Mohsen Georgia Institute of Technology  Fekri, Faramarz Georgia Institute of Technology  Interactive Function Computation with Reconstruction Constraints 896  Ebrahim Rezagah, Farideh Polytechnic Institute of New York University  Erkip, Elza Polytechnic Institute of New York University  Distortion Rate Function of Sub-Nyquist Sa mpled Gaussian Sources Corrupted by Noise 901  Kipnis, Alon Stanford University  Goldsmith, Andrea J Stanford University  Weissman, Tsachy Stanford University  Eldar, Yonina C Technion - Israel Institute of Technology  ThB1 \226 Dynamic Games Library   Chair Liu, Mingyan University of Michigan Joint Control of Transmission Power and Channel Switching against Adaptive Jamming 909  Wang, Qingsi University of Michigan  Liu, Mingyan University of Michigan  Characterization and Computation of Loca l Nash Equilibria in Continuous Games 917  Ratliff, Lillian J University of California, Berkeley  Burden, Samuel A University of California, Berkeley  Sastry, S. Shankar University of California, Berkeley  Multiagent Inverse Reinforcement Learni ng for Zero-Sum Stochastic Games n/a  Beling, Peter University of Virginia  Cogill, Randy University of Virginia  Lin, Xiaomin University of Virginia  A Dynamic VCG Mechanism for Random Allocation Spaces 925  Balandat, Maximilian University of California, Berkeley  Tomlin, Claire J University of California, Berkeley  


Nonstationary Resource Sharing with Imperfect Binary Feedback An Optimal Design Framework for Cost Minimization 932  Xiao, Yuanzhang University of California, Los Angeles  van der Schaar, Mihaela University of California, Los Angeles  ThB2 \226 Topics in Information Theory III Solarium   Chair Viswanath, Pramod University of Illinois  Organizer\(s Viswanath, Pramod University of Illinois  De Novo RNA Shotgun Sequencing: Fundamental Limits n/a  Kannan, Sreeram University of Illinois  Pachter, Lior University of California, Berkeley  Tse, David University of California, Berkeley  Queue Length as an Implicit Communication Channel 940  Park, Se Yong University of California, Berkeley  Sahai, Anant University of California, Berkeley  Polytope Codes for Large-Alphabet Channels 948  Fan, Xiaoqing Cornell University  Wagner, Aaron B Cornell University  Ahmed, Ebad LSI Corporation  Data-Driven Management of Infrastructure Networks n/a  Rajagopal, Ram Stanford University A Derivation of the Asymptotic Random-Coding Prefactor 956  Scarlett, Jonathan University of Cambridge  Martinez, Alfonso Universitat Pompeu Fabra  Guill\351n i F\340bregas, Albert ICREA and Universitat Pompeu Fabra  ThB3 \226 Discrete Event Systems Butternut   Chair  Tarraf, Danielle C Johns Hopkins University On Exploiting Algebraic Structure in Control of Finite State Machines 962  Tarraf, Danielle C Johns Hopkins University  A Bridge between Decentralized and Coordination Control 966  Komenda, Jan Czech Academy of Sciences  Masopust, Tom\341\232 Czech Academy of Sciences  A Model Checking Framework for Linear Time Invariant Switching Systems using Structural Systems Analysis 973  Ramos, Guilherme Instituto Superior T\351cnico  Pequito, S\351rgio Carnegie Mellon University and Instituto Superior T\351cnico  Aguiar, A. Pedro University of Porto  Ramos, Jaime Instituto Superior T\351cnico  Kar, Soummya Carnegie Mellon University  


Robust Supervisory Control of Ne tworked Discrete Event Systems 981  Wang, Fei Tongji University  Shu, Shaolong Tongji University  Lin, Feng Wayne State University  Probability Bounds for False Alarm Anal ysis of Fault Detection Systems 989  Hu, Bin University of Minnesota  Seiler, Peter University of Minnesota  ThB4 \226 Network Inference Pine   Chair Sanghavi, Sujay University of Texas, Austin  Organizer\(s Kiyavash, Negar University of Illinois  Sanghavi, Sujay University of Texas, Austin Robust Structure Estimation of M aximum Causal Entropy Processes 996  Ziebart, Brian D University of Illinois, Chicago  The Squared-Error of Generalized LASSO: A Precise Analysis 1002  Oymak, Samet California Institute of Technology  Thrampoulidis, Christos California Institute of Technology  Hassibi, Babak California Institute of Technology  Localized Minimax Complexity of Stochastic Convex Optimization n/a  Zhu, Yuancheng University of Chicago  Lafferty, John University of Chicago  Non-Convex Inference via Alternatin g Minimization: Provable Guarantees n/a  Sanghavi, Sujay University of Texas, Austin  Loop Calculus and Bootstrap-Belief Propagatio n for Perfect Matchings on Arbitrary Graphs n/a  Chertkov, Michael Los Alamos National Laboratory  Gelfand, Andrew University of California, Irvine  Shin, Jinwoo Massachusetts Institute of Technology  ThB5 \226 Control and Optimization Problems in Electrical Energy Systems I Lower Level   Chair Dominguez-Garcia, Alejandro University of Illinois  Organizer\(s Dominguez-Garcia, Alejandro University of Illinois  A Rank Minimization Algorithm to Enhance Semidefinite Rel axations of Optimal Power Flow 1010  Louca, Raphael Cornell University  Seiler, Peter University of Minnesota  Bitar, Eilyan Cornell University  Power System Structure and Confidenti ality Preserving Transformation of Optimal Power Flow Model 102 1  Borden, Alexander R University of Wisconsin, Madison  Molzahn, Daniel K University of Michigan  Lesieutre, Bernard C University of Wisconsin, Madison  Ramanathan, Parmeswaran University of Wisconsin, Madison  


Incentive Design for Direct Load Control Programs 1029  Alizadeh, Mahnoosh University of California, Davis  Xiao, Yuanzhang University of California, Los Angeles  Scaglione, Anna University of California, Davis  van der Schaar, Mihaela University of California, Los Angeles  Energy Positioning: Control and Economics \226 Part 1 n/a  Kirschen, Daniel University of Washington  Hiskens, Ian University of Michigan  Pandzic, Hrvoje University of Washington  Qiu, Ting University of Washington  Wang, Yishen University of Washington  Energy Positioning: Control and Economics \226 Part 2 n/a  Hiskens, Ian University of Michigan  Kirschen, Daniel University of Washington  Xue, Mengran University of Michigan  Almassalkhi, Mads Root3 Technologies Ltd  Felder, Jennifer University of Michigan  The Redistribution of Power Flow in Cascading Failures 1037  Lai, Chengdi California Institute of Technology  Low, Steven H California Institute of Technology  ThB6 \226 Multiuser Detecti on and Estimation Theory Visitor Center   Chair Chowdhury, Mainak Stanford University Phase and Power Estimation for Per-Hop MultiUser Detection in Frequency-Hopping Systems 1045  Qiu, David MIT Lincoln Laboratory  Royster, Thomas C MIT Lincoln Laboratory  Block, Frederick J MIT Lincoln Laboratory  Non-Coherent Multi-User Detection of DPSK Signals after Differential Demodulation 1052  Qiu, David MIT Lincoln Laboratory  Block, Frederick J MIT Lincoln Laboratory  The Incidence and Cross Methods for Efficient Radar Detection 1059  Fish, Alexander University of Sydney  Gurevich, Shamgar University of Wisconsin, Madison  Capacity Analysis of Uplink Multi-User SC-FDMA System with Frequency-Dependent I/Q Imbalance 1067  Ishaque, Aamir RWTH Aachen University  Sakulkar, Pranav RWTH Aachen University  Ascheid, Gerd RWTH Aachen University  Reliable Uncoded Communication in th e Underdetermined SIMO MAC with Low-Complexity Decoding 1 075  Chowdhury, Mainak Stanford University  Goldsmith, Andrea Stanford University  Weissman, Tsachy Stanford University  


ThC1 \226 Networks, Ga mes and Algorithms III Library   Chair  Williams, Steven R University of Illinois  Organizer\(s Hajek, Bruce University of Illinois  Srikant, R University of Illinois Competitive Equilibrium in Electricity Markets wi th Heterogeneous users and Ramping Constraints n/a  Malekian, Azarakhsh Massachusetts Institute of Technology  Ozdaglar, Asu Massachusetts Institute of Technology  Wei, Ermin Massachusetts Institute of Technology  A Lyapunov Optimization Approach to Repeated Stochastic Games 1082  Neely, Michael J University of Southern California  Road Traffic Networks: Optimal Transport and Incentives 1090  Mandayam, Chinmoy V Stanford University  Prabhakar, Balaji Stanford University  Online Stochastic Ad Allocation: Simu ltanenous and Bicriteria Approximations n/a  Mirrokni, Vahab Google Research  A Processor-Sharing Heuristic fo r Multipath Congestion Control n/a  Walton, Neil Stuart University of Amsterdam  Anselmi, Jonatha Basque Center for Applied Mathematics  D'Auria, Bernardo Universidad Carlos III de Madrid  ThC2 \226 Distributed and Controlled Sensing I Solarium   Chair Nedich, Angelia University of Illinois  Organizer\(s Ba ar, Tamer University of Illinois  Nedich, Angelia University of Illinois Veeravalli, Venugopal University of Illinois  Asymptotic Optimality Results for Controlled Sequential Estimation 1098  Atia, George University of Central Florida  Aeron, Shuchin Tufts University  Even Symmetric Parallel Linear Determinis tic Interference Channels are Inseparable 1106  Mukherjee, Pritam University of Maryland  Tandon, Ravi Virginia Polytechnic Institute and State University Ulukus, Sennur University of Maryland  Estimation Over the Collision Channel: Structural Results 1114  Vasconcelos, Marcos M University of Maryland  Martins, Nuno C University of Maryland  Physical Watermarking and Authenti cation in Cyber-Physical Systems n/a  Weerakkody, Sean Carnegie Mellon University  Mo, Yilin California Institute of Technology  Sinopoli, Bruno Carnegie Mellon University  


Distributed Linear Estimation of Dynamic Random Fields 1120  Das, Subhro Carnegie Mellon University  Moura, Jos\351 M.F Carnegie Mellon University  ThC3 \226 Active Learning, Search and Visual Recognition Butternut   Chair Raginsky, Maxim University of Illinois  Co-Chair Lazebnik, Svetlana University of Illinois   Organizer\(s Javidi, Tara University of California, San Diego  Lazebnik, Svetlana University of Illinois Raginsky, Maxim University of Illinois  Working Title: Learning to Recognize Everything n/a  Berg, Alexander C University of North Carolina, Chapel Hill  Visual Attributes for Enhanced Human-Machine Communication 1126  Parikh, Devi Virginia Polytechnic Institute and State University Active Learning of Linear Separators n/a  Balcan, Maria Florina Georgia Institute of Technology  Discriminative Value of Information for Structured Prediction n/a  Taskar, Ben University of Washington  Extrinsic Jensen\226Shannon Divergence and Noisy Bayesian Active Learnin\g 1128  Naghshvar, Mohammad University of California, San Diego  Javidi, Tara University of California, San Diego  Chaudhuri, Kamalika University of California, San Diego  Universal Random Number Generators for Finite Memory Sources n/a  Seroussi, Gadiel Universidad de la Rep\372blica, Uruguay  Weinberger, Marcelo J Center for Science of Information  ThC4 \226 Information Theory III Pine   Chair Avestimehr, Salman Cornell University Approximate Capacity of the Two-User MI SO Broadcast Channel with Delayed CSIT 1136  Vahid, Alireza Cornell University  Maddah-ali, Mohammad Ali Alcatel-Lucent Bell Labs  Avestimehr, A. Salman Cornell University  K-User Symmetric MIMO Distributed Full-D uplex Network via Wireless Side-Channels 1144  Bai, Jingwen Rice University  Dick, Chris Xilinx, Inc  Sabharwal, Ashutosh Rice University  Degrees of Freedom Region for MIMO Interferen ce Channel with Limite d Receiver Cooperation 1152  Ashraphijuo, Mehdi Columbia University  Aggarwal, Vaneet AT&T Labs-Research  Wang, Xiaodong Columbia University  


Degrees of Freedom of the MIMO Rank-Def icient Interference Channel with Feedback 1159  Chae, Sung Ho Korea Advanced Institute of Science and Technology  Suh, Changho Korea Advanced Institute of Science and Technology  Chung, Sae-Young Korea Advanced Institute of Science and Technology  On the Optimality of Trea ting Interference as Noise 1166  Geng, Chunhua University of California, Irvine  Naderializadeh, Navid Cornell University  Avestimehr, A. Salman Cornell University  Jafar, Syed A University of California, Irvine  Asymmetric Compute-and-Forward 1174  Ntranos, Vasilis University of Southern California  Cadambe, Viveck R Massachusetts Institute of Technology  Nazer, Bobak Boston University  Caire, Giuseppe University of Southern California  ThC5 \226 Pricing and Contractive Mechanisms for Wireless Data Services Lower Level   Chair Wang, Qiong University of Illinois  Organizer\(s Andrews, Matthew Alcatel-Lucent Bell Labs  Baryshnikov, Yuliy University of Illinois Wang, Qiong University of Illinois  Smart Data Pricing for the Internet: Agenda & Research Directions 1182  Sen, Soumya University of Minnesota  Market Structures for Wirele ss Service with Shared Spectrum 1188  Berry, Randall Northwestern University  Honig, Michael Northwestern University  Subramanian, Vijay Northwestern University  Nguyen, Thanh Northwestern University  Vohra, Rakesh Northwestern University  Fostering Wireless Spectrum Sharing via Subsidization 1192  Yuksel, Murat University of Nevada, Reno  Quint, Thomas University of Nevada, Reno  Guvenc, Ismail Florida International University  Saad, Walid University of Miami  Kapucu, Naim University of Central Florida  Stable Real-Time Pricing and Sc heduling for Serving Opportunistic users with Deferrable Loads 1200  Dalkilic, Ozgur Ohio State University  Eryilmaz, Atilla Ohio State University  Lin, Xiaojun Purdue University  Implementing Sponsored Conten t in Wireless Data Networks 1208  Andrews, Matthew Alcatel-Lucent Bell Labs  


ThC6 \226 Deletion Codes: Bounds and Applications Visitor Center   Chair Kiyavash, Negar University of Illinois  Organizer\(s Dolecek, Lara University of Califo rnia, Los Angeles  Kiyavash, Negar University of Illinois A Practical Framework for Efficient File Synchronization 1213  Bitouz\351, Nicolas University of California, Los Angeles  Sala, Frederic University of California, Los Angeles  Tabatabaei Yazdi, S.M. Sadegh University of California, Los Angeles  Dolecek, Lara University of California, Los Angeles  An Improvement of the Deletion Channel Capacity Upper Bound 1221  Rahmati, Mojtaba Arizona State University  Duman, Tolga M Arizona State University and Bilkent University  On the Number of Subsequences Obtained via the Deletion Channel n/a  Yuvalal, Liron The Open University of Israel  Langberg, Michael State University of New York, Buffalo Efficient Interactive Algorithms for F ile Synchronization under General Edits 1226  Venkataramanan, Ramji University of Cambridge  Narasimha Swamy, Vasuki University of California, Berkeley  Ramchandran, Kannan University of California, Berkeley  ThD1 \226 Sparse Data Analysis Library   Chair Studer, Christoph Rice University Recovering Sparse Low-Rank Bloc ks in Tandem Mass Spectrometry n/a  Studer, Christoph Rice University  Pope, Graeme ETH Zurich  Navarro, Pedro Johannes Gutenberg University Mainz  Baraniuk, Richard Rice University  GROTESQUE: Noisy Group Testing \(Quick and Efficient 1234  Cai, Sheng The Chinese University of Hong Kong  Jahangoshahi, Mohammad Sharif University of Technology  Bakshi, Mayank Institute of Network Coding  Jaggi, Sidharth The Chinese University of Hong Kong  Compressed Sensing of Streaming Data 1242  Freris, Nikolaos M 311cole Polytechnique F\351d\351rale de Lausanne  326\347al, Orhan 311cole Polytechnique F\351d\351rale de Lausanne  Vetterli, Martin 311cole Polytechnique F\351d\351rale de Lausanne  A Fast Hadamard Transform for Signals with Sub-Linear Sparsity 1250  Scheibler, Robin 311cole Polytechnique F\351d\351rale de Lausanne  Haghighatshoar, Saeid 311cole Polytechnique F\351d\351rale de Lausanne  Vetterli, Martin 311cole Polytechnique F\351d\351rale de Lausanne  


Sample-Optimal Average-Case Sparse Fo urier Transform in Two Dimensions 1258  Ghazi, Badih Massachusetts Institute of Technology  Hassanieh, Haitham Massachusetts Institute of Technology  Indyk, Piotr Massachusetts Institute of Technology  Katabi, Dina Massachusetts Institute of Technology  Price, Eric Massachusetts Institute of Technology  Shi, Lixin Massachusetts Institute of Technology Guarantees of Total Variation Mi nimization for Signal Recovery 1266  Cai, Jian-Feng University of Iowa  Xu, Weiyu University of Iowa  ThD2 \226 Distributed and Controlled Sensing II Solarium   Chair Veeravalli, Venugopal University of Illinois  Organizer\(s Ba ar, Tamer University of Illinois  Nedich, Angelia University of Illinois Veeravalli, Venugopal University of Illinois  Adaptive Stochastic Convex Optimization Over Networks 1272  Towfic, Zaid J University of California, Los Angeles  Sayed, Ali H University of California, Los Angeles  A Routing Problem in a Simple Queueing Syst em with Non-Classical Information Structure 1278  Ouyang, Yi University of Michigan  Teneketzis, Demosthenis University of Michigan  On the Necessary Conditions for Distributed Observability n/a  Doostmohammadian, Mohammadreza Tufts University  Khan, Usman A Tufts University  Sequential Supervised Learning n/a  Wang, Joseph Boston University  Trapeznikov, Kirill Boston University  Saligrama, Venkatesh Boston University  Fusion Center Feedback for Quasi-Decentr alized Estimation in Sensor Networks 1285  Michelusi, Nicol\362 University of Southern California  Mitra, Urbashi University of Southern California  ThD3 \226 Information Theory IV Butternut   Chair Bhashyam, Srikrishna Indian Inst. of Tech, Madras The Gaussian Two-Way Diamond Channel 1292  V, Prathyusha Indian Institute of Technology, Madras  Bhashyam, Srikrishna Indian Institute of Technology, Madras  Thangaraj, Andrew Indian Institute of Technology, Madras  Capacity to within a Constant Gap fo r a Class of Interference Relay Channels 1300  Bassi, Germ\341n Sup\351lec  Piantanida, Pablo Sup\351lec  Yang, Sheng Sup\351lec  


The State-Dependent Broadcast Channel with Cooperation 1307  Dikstein, Lior Ben Gurion University of the Negev  Permuter, Haim H Ben Gurion University of the Negev  Steinberg, Yossef Technion - Israel Institute of Technology An Analysis of the Joint Compute-and-Forward Decoder for the Binary-Input Two-Way Relay Channel 1314  Hern, Brett Texas A&M University  Narayanan, Krishna Texas A&M University  The Generalized Degrees of Freedom of the Int erference Relay Channel with Strong Interference 1321  Gherekhloo, Soheyl Ruhr Universit\344t Bochum  Chaaban, Anas Ruhr Universit\344t Bochum  Sezgin, Aydin Ruhr Universit\344t Bochum  Optimal Jamming Over Additive No ise: Vector Source-Channel Case 1329  Akyol, Emrah University of California, Santa Barbara  Rose, Kenneth University of California, Santa Barbara  ThD4 \226 Control and Optimization Problems in Electrical Energy Systems II Pine   Chair Dominguez-Garcia, Alejandro University of Illinois  Organizer\(s Dominguez-Garcia, Alejandro University of Illinois  Distributed Stopping in Average Consensus via Event-Triggered Strategies 1336  Manitara, Nicolaos University of Cyprus  Hadjicostis, Christoforos N University of Cyprus  Stochastic Models and Control fo r Electrical Power Line Temperature 1344  Bienstock, Daniel Columbia University  Blanchet, Jose Columbia University  Li, Juan Columbia University  Applicability of Topology Control Algorit hms \(TCA\e Power System 1349  Goldis, Evgeniy A Boston University  Li, Xiaoguang Boston University  Caramanis, Michael C Boston University  Keshavamurthy, Bhavana PJM  Patel, Mahendra PJM  Rudkevich, Aleksandr M Newton Energy Group Ruiz, Pablo A  Charles River Associates Retail Pricing for Stochastic Demand with Unknown Parameters An Online Machine Learning Approach 1353  Jia, Liyan Cornell University  Zhao, Qing University of California, Davis  Tong, Lang Cornell University  Virtual Oscillator Control for Voltage Source Inverters 1359  Dhople, Sairaj V University of Minnesota  Johnson, Brian B National Renewable Energy Laboratory  Hamadeh, Abdullah O Rutgers University  


ThD5 \226 Networks, Gam es and Algorithms IV Lower Level   Chair Lu, Yi University of Illinois  Organizer\(s Hajek, Bruce University of Illinois  Srikant, R University of Illinois Contagion and Observability in Security Domains 1364  Bachrach, Yoram Microsoft Research Cambridge Draief, Moez Imperial College  Goyal, Sanjeev University of Cambridge  Targeted Matrix Completion n a  Ruchansky, Natali Boston University  Crovella, Mark Boston University  Terzi, Evimaria Boston University  Random Matrix Theory Approach to Spectral Clustering n/a  Lelarge, Marc INRIA and \311cole Normale Sup\351rieure Optimal Distributed Scheduling in Wirele ss Networks under SINR Interference Model 1372  Chaporkar, P Indian Institute of Technology, Mumbai  Proutiere, A KTH Royal Institute of Technology  Load Balancing with Deadlines and Graph Constraints n/a  Moharir, Sharayu Arun University of Texas, Austin  Sanghavi, Sujay University of Texas, Austin  Shakkottai, Sanjay University of Texas, Austin  Curbing Delays in Datacenters Need Time to Save Time n/a  Alizadeh Attar, Mohammadreza Insieme Networks  Katti, Sachin Stanford University Prabhakar, Balaji Stanford University  ThD6 \226 Topics in Cryptography Vistior Center   Chair Prabhakaran, Manoj University of Illinois  Organizer\(s Duursma, Iwan University of Illinois  Prabhakaran, Manoj University of Illinois How to Encrypt Software  n/a  Sahai, Amit University of California, Los Angeles  On the Cost of Information-Theoretic Cryptography n/a  Ishai, Yuval Technion - Israel Institute of Technology From Unprovability to Environmentally Friendly Protocols n/a  Canetti, Ran Boston University and Tel Aviv University  Lin, Huijia Massachusetts Institute of Technology and Boston University  Pass, Rafael Cornell University  Cryptography, Causality, and Coding n/a  Smith, Adam Pennsylvania State University  


Obfuscation for Evasive Functions n/a  Canetti, Ran Boston University and Tel Aviv University  Bitansky, Nir Tel Aviv University  Barak, Boaz Microsoft Research  Kalai, Yael Microsoft Research  Paneth, Boaz Microsoft Research  Sahai, Amit University of California, Los Angeles FrPP - Plenary Talk:  C odes for the Storage Cloud Library   Chair Ba ar, Tamer University of Illinois Codes for the Storage Cloud n a  Ramchandran, Kannan University of California, Berkeley  FrA1 \226 Wireless Communications II   Chair Duel-Hallen, Alexandra North Carolina State University Small Cell Networks: Speed based Power Allocation 1380  Kavitha, Veeraruna Indian Institute of Technology, Bombay  Capdevielle, Veronique Alcatel-Lucent Bell Labs  Gupta, Manu K Indian Institute of Technology, Bombay  Distributed Power Control in Femto Ce lls using Bayesian Density Tracking 1388  Hanif, Ahmed Farhan Institut Mines-T\351l\351com-T\351l\351com SudParis  Tembine, Hamidou Sup\351lec  Assaad, Mohamad Sup\351lec  Zeghlache, Djamal Institut Mines-T\351l\351com-T\351l\351com SudParis  Renewable Energy Scheduling for Fading Ch annels with Maximum Power Constraint 1394  Wang, Zhe Columbia University  Aggarwal, Vaneet AT&T Labs-Research  Wang, Xiaodong Columbia University  Analysis and Design of Spectrum Sh aring in Cognitive Femtocell Networks 1401  Zhou, Xiangwei Southern Illinois University, Carbondale  Al-Hraishawi, Hayder Southern Illinois University, Carbondale  Jia, Yupeng National Instruments  Channel-Adaptive Spectrum Detection and Sensin g Strategy for Cognitive Radio Ad Hoc Networks 1408  Lu, Yuan North Carolina State University  Duel-Hallen, Alexandra North Carolina State University  Quantized Auction Schemes for Secondary Spectrum Markets 1415  Palguna, Deepan Purdue University  Love,  David J Purdue University  Pollak, Ilya Purdue University  Performance Analysis of Coexisting Secondary users in Heterogeneous Cognitive Radio Network 1422  Li, Xiaohua State University of New York, Binghamton  Xiong, Chengyu State University of New York, Binghamton  


FrA2 \226 Statistical Signal Processing Solarium   Chair  Bajwa, Waheed U Rutgers University Nearly Optimal Sample Size in Hypothesis Testing for High-Dim ensional Regression 1427  Javanmard, Adel Stanford University  Montanari, Andrea Stanford University  Distributed Online Big Data Classifi cation using Context Information 1435  Tekin, Cem University of California, Los Angeles  van der Schaar, Mihaela University of California, Los Angeles  Compressed Hypothesis Testing: To Mix or Not to Mix 1443  Xu, Weiyu University of Iowa  Lai, Lifeng Worcester Polytechnic Institute  Sparse Signal Recovery under Poisson Statistics 1450  Motamedvaziri, Delaram Boston University  Rohban, Mohammad H Boston University  Saligrama, Venkatesh Boston University  Efficient Probabilistic Group Testing based on Traitor Tracing 1458  Laarhoven, Thijs Eindhoven University of Technology  Computational and Statistical Tradeoffs via Convex Relaxation n/a  Chandrasekaran, Venkat California Institute of Technology  Jordan, Michael I University of California, Berkeley  Time-Variant Regularization in Affine Projection Algorithms 1466  Ba, Amadou IBM Research  McKenna, Sean IBM Research  Cloud K-SVD: Computing Data-Adaptive Representations in the Cloud 1474  Raja, Haroon Rutgers University  Bajwa, Waheed U Rutgers University  FrA3 \226 Information Aggregation Over Social Networks Butternut   Chair Hassanzadeh, Farzad University of Illinois  Organizer\(s Milenkovic, Olgica University of Illinois  Yaakobi, Eitan California Institute of Technology The Maximum Likelihood Approach to Voting on Social Networks 1482  Conitzer, Vincent Duke University  Building Consensus via Iterative Voting n/a  Farnoud \(Hassanzadeh California Institute of Technology  Yaakobi, Eitan California Institute of Technology  Touri, Behrouz Georgia Institute of Technology Bruck, Jehoshua California Institute of Technology  CP-Nets with Indifference  1488  Allen, Thomas E University of Kentucky  


Computing Parametric Rankin g Models via Rank-Breaking n/a  Azari Soufiani, Hossein Harvard University  Parkes, David Harvard University  Xia, Lirong Rensselaer Polytechnic Institute  On the Dynamics of Influence Networks via Reflected Appraisal n/a  Bullo, Francesco University of California, Santa Barbara Social Group Utility Maximization Game with Applications in Mobile Social Networks 1496  Gong, Xiaowen Arizona State University  Chen, Xu Arizona State University  Zhang, Junshan Arizona State University  Credibility Optimization and Power Co ntrol for Secure Mobile Crowdsourcing 1501  Ahmed, Kishwar Florida International University  Ren, Shaolei Florida International University  Turnewitsch, Vance Marietta College  Vasilakos, Athanasios V National Technical University of Athens  Group Learning and Opinion Diffusion in a Broadcast Network 1509  Liu, Yang University of Michigan  Liu, Mingyan University of Michigan  FrA4 \226 Network Coding Pine   Chair Dimakis, Alex University of Southern California Secure Network Coding with Erasures and Feedback 1517  Czap, L\341szl\363 311cole Polytechnique F\351d\351rale de Lausanne  Fragouli, Christina 311cole Polytechnique F\351d\351rale de Lausanne  Prabhakaran, Vinod M Tata Institute of Fundamental Research  Diggavi, Suhas University of California, Los Angeles  Index Coding Problem with Side Information Repositories 1525  Shanmugam, Karthikeyan University of Texas, Austin  Dimakis, Alexandros G University of Texas, Austin  Caire, Giuseppe University of Southern California  Complexity and Rate-Distortion Tradeoff via Successive Refinement 1531  No, Albert Stanford University  Ingber, Amir Stanford University  Weissman, Tsachy Stanford University  On a Capacity Equivalence Between Mult iple Multicast and Multiple Unicast 1537  Wong, M.F California Institute of Technology  Langberg, M State University of New York, Buffalo Effros, M California Institute of Technology  On the Capacity of Sum-Networks 1545  Rai, Brijesh Kumar Indian Institute of Technology, Guwahati  Das, Niladri Indian Institute of Technology, Guwahati  


Duality Codes and the Integral ity Gap Bound for Index Coding 1553  Yu, Hao University of Southern California  Neely, Michael J University of Southern California  On the Structure of Approximately Optimal Schedules for Half-Duplex Diamond Networks 1561  Brahma, Siddhartha 311cole Polytechnique F\351d\351rale de Lausanne  Fragouli, Christina 311cole Polytechnique F\351d\351rale de Lausanne  326zg\374r, Ayfer Stanford University  FrA5 \226 Topology and Control Lower Level   Chair Baryshnikov, Yuliy University of Illinois  Co-Chair Belabbas, Mohamed-Ali University of Illinois   Organizer\(s Baryshnikov, Yuliy University of Illinois  Belabbas, Mohamed-Ali University of Illinois Double Bracket Flows, Toda Flows and Rigid Body Toda 1567  Bloch, Anthony M University of Michigan  Gay-Balmaz, Fran\347ois 311cole Normale Sup\351rieure  Ratiu, Tudor S 311cole Polytechnique F\351d\351rale de Lausanne  Topological Obstructions to Dist ributed Feedback Stabilization 1573  Mansouri, Abdol-Reza Queen's University  Contact Geometry of Optimal Control Problems n/a  Ohsawa, Tomoki University of Michigan, Dearborn  Equivariant Morse Theory for Formation Control 1576  Helmke, Uwe University of W\374rzburg  Anderson, Brian D.O Australian National University  Towards Discrete Geometric Boundary Co ntrol of Lagrangian Field Theories n/a  Leok, Melvin University of California, San Diego  Structure and Geometry of Minimum-Tim e Trajectories for Planar Rigid Bodies 1584  Futuna, Andrei A Dartmouth College  Wang, Weifu Dartmouth College  Lyu, Yu-Han Dartmouth College  Balkcom, Devin Dartmouth College  Real and Apparent Synchronization n/a  Brockett, Roger Harvard University  Rauch and Bonnet-Myers Type Comparison Theorems in Sub-Riemannian Geometry n/a  Zelenko, Igor Texas A&M University  


FrA6 \226 Privacy and Big Data Visitor Center   Chair Oh, Sewoong University of Illinois  Organizer\(s Duchi, John University of California, Berkeley  Oh, Sewoong University of Illinois Viswanath, Pramod University of Illinois  Local Privacy and Statistical Minimax Rates 1592  Duchi, John C University of California, Berkeley  Jordan, Michael I University of California, Berkeley  Wainwright, Martin J University of California, Berkeley  Differential Privacy, Equilibrium, and Efficient Allocation of Resources 1593  Roth, Aaron University of Pennsylvania  A Bayesian Method for Matching Tw o Similar Graphs without Seeds 1598  Pedarsani, Pedram 311cole Polytechnique F\351d\351rale de Lausanne  Figueiredo, Daniel R Federal University of Rio de Janeiro  Grossglauser, Matthias 311cole Polytechnique F\351d\351rale de Lausanne  Privacy as a Coordination Game 1608  Ghosh, Arpita Cornell University  Ligett, Katrina California Institute of Technology  De-Anonymizing Private Data by Matching Statistics 1616  Unnikrishnan, Jayakrishnan 311cole Polytechnique F\351d\351rale de Lausanne  Movahedi Naini, Farid 311cole Polytechnique F\351d\351rale de Lausanne  Robust Subspace Iteration and Privacy-Preserving Spectral Analysis 1624  Hardt, Moritz IBM Research Almaden  Privacy-Utility Tradeoff und er Statistical Uncertainty 1627  Makhdoumi, Ali Massachusetts Institute of Technology  Fawaz, Nadia Technicolor   


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a ìkey, valueî list using an XSTL  Queries made against this list of ìkey, valueî pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


