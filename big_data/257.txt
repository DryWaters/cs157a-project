1 Data Mining Application in Customer Relationship Management Of Credit Card Business Ruey-Shun Chen, Ruey-Chyi Wu and J. Y. Chen Institute of Information Management  National Chiao Tung University, Taiwan rschen@iim.nctu.edu.tw Abstract     First, we classify the selected customers into clusters using RFM model to identify high-profit, gold customers. Subsequently we carry out data mining using association rules algorithm. We measure the similarity, difference and modified difference of mined association rules based on three rules, i.e. Emerging Patten Rule, Unexpected Change Rule, and Added/Perished Rule. In the meantime, we use rule matching threshold to derive all types of rules and explore the rules with significant change 
based on the degree of change measured. In this paper, we employ data mining tools and effectively discover the current spending pattern of customers and trends of behavioral change which will allow management to detect in a large database potential changes of customer preference, and provide as early as possible products and services desired by the customers to expand the clientele base and prevent customer attrition Keywords  Data mining, Customer relationship management  Credit card 1 Introduction Customer relationship management, through which, banks hope to identify the preference of different customer groups products and services tailored to their liking to enhance the cohesion between credit card customers and the bank, has 
become a topic of great interest   Shaw et mainly d e scrib es how to incor porate data mining into the framework of marketing knowledge management. The systemic application of data mining techniques reinforces the knowledge management process and allows marketing personnel to know their customers well to provide better services. Differing from Berry an  Shaw proposes the view of data visualization, believing that data visualization software allows marketers to see complex data depicted in three dimensions or colors and to slice, rotate or zoom the data to obtain different levels of details  depicts a method to d e tect ch anges o f  customer behavior at different time snapshots from customer profiles and sales data. The common approach is to discover changes from 
two datasets and generate rules from each dataset to carry out rule matching. In this study, we propose three types of behavioral change rulesÑEmerging Pattern Rule, Unexpected Change Rule, and Added/Perished Rule  2 Literature review 2.1 Customer relationship management Kalakota & Ro defines CRM as th e strateg y integrating sales, marketing and service, which unites operating procedures and technology to better understand customers from different perspectives. Kandell views CRM as a customer centric initiative that regards customer lifecycle as an important business asset and aims to retain customers and enhance customer satisfaction. According to CRM is not a 
new concept. Many businesses have practiced it for a long time for instance, by memorizing customerês background and spending habits and introducing promotions targeting certain customers based on the information obtained  2.2 Experimental Model Bult and Wa point out that RFM recen cy  frequency and monetary value\ is most commonly used for selection or segmentation analysis in direct marketing; through RFM, marketer can sort out target customers from a huge list of customers for its marketing activity. Sung and use non-transformed RFM values directly as input variable for model building and then categorize customers into groups using cluster analysis where different marketing strategies may 
be formulated for different customer clusters. Goddman [6 holds that RFM analysis helps avoid waste of time and energy on cultivating low-profit customers and generate better return on capital by investing more marketing resources on higher profit customer segments The customer value matrix proposed by Marcu is a method suitable for small and med-sized enterprises to analyze customer value. Customer value matrix uses two variables frequency and money to explain customer value, while the third variable çrecencyé provides the time of customer contact to combine with frequency and average monetary value of purchase. Figure 1 depicts the customer value matrix  
Figure 1 Customer value Matrix Figure 2 System Framework  Data mining helps users to identify valuable patterns contained in diverse data and their relations so as to help the major decisions. Currently businesses apply data mining in many fields, i.e finance, banking, and manufacturing. Generally 
speaking, data mining can create the following models: \(1 classification; \(2\er analysis; and \(3\association analysis 3 Problem description Using the association rules in data mining techniques to analyze in detail customer transactions can learn which products might be purchased by customers at the same time and based on the rules for combining popular products Proceedings of the 29th Annual International Computer Software and Applications Conference \(COMPSACê05 0730-3157/05 $20.00 © 2005 IEEE 


2 marketing personnel or corporate decision makers can formulate more appealing marketing plan or operational rules and actively offer products that might interest the customers Thus clusters may be used as basis for segmenting marketing targets. In consideration that customerês spending behavior might change with time, we will also explore such time-varying and dynamically adjusted spending patterns 4 System analysis and design 4.1 System framework     As shown in Figure 2, the system framework for the study contains customer clustering, creation of association rules, and mining change of rules at different time  4.2 Relationship between rules at different time periods We identify higher profit customers using clustering algorithm, select datasets of two different time periods, and generate the association rules in those two periods using association rules algorithm. We first measure the similarity and difference between the rules, then use rule matching threshold to decide the type of the rule, and finally evaluate the degree of change to identify a significant trend of spending behavioral change. We use the term çsimilaré and çdifferenté to depict the disparity between rules. Here we use a rule matching threshold RMT to decide th e m easur e of dif f erence between rul es shown in Figure 3  Figure 3 Different Types of Changes 5 System implementation 5.1 Data description     The data for the empirical study are credit card customer data and spending records in 2003 and 2004 from banks. In light that the study focuses on the spending behavior of customers, all purchases of the same customer using more than one credit card were combined under said customer. There were altogether 10304500 and 1146200 pieces of purchase data for 2003 and 2004 respectively, which were matched using a program, and data of the same customer were kept. Finally 1063000 pieces of data for each year were used as dataset for the study 5.2 Weka data mining tool     Weka \(Waikato Environment for Knowledge Analysis a Java-based data mining tool developed by Waikato University. After loading the dataset into it, the preprocess function of Weka allows the user to input undesired attributes to prevent them from affecting the quality of extracted knowledge. Next, the user can use one of the three algorithms to mine the data: Classification, Clustering, and Association Rule 6 Results     In this section, we used Weka to run the model. First, we clustered the 2003 customers to identify the most influential group in term of business profit and find their behavioral pattern. The 2004 dataset was used to observe change of rules in different time periods. These steps can help decision makers understand customer needs and adjust their marketing strategy accordingly to seek maximum return under limited resources 6.1 Classification analysis     We clustered the 2003 customers using the RFM model Given that çmost recent purchase date \(Recent\ no significant influence in the analysis of customer value in credit card transactions, we considered only two factors -frequency and monetary value \(M\in clustering analysis. Cluster 0 and Cluster 2, which collectively accounted for 80% of bank customers, but had brought little profit to the bank \(spending less than $4500,000 per person each year\anks are advised against spending too much energy and costs on these types of customers. Cluster 3 and Cluster 4 were more loyal, but did not generate high profit for the bank. How to enhance their value to the bank should be the focus of marketing efforts. Cluster 1 accounted for 3.3% of credit card customers and they were the gold customers with high loyalty and high profit, and hence one of the bankês most important assets. Losing such customers will result in substantial loss to the bank. Thus the bank should endeavor to provide better services and greater added value to retain them. A business only stands to win if it is able to use information and professional knowledge effectively to create value for its customers and solidify its relationship with them     At 5.5% support and 75% confidence level, we identified 641 association rules in 2003 dataset and 578 association rules in 2004 dataset using the Apriori algorithm provided in Weka 7 Conclusion We used association rules algorithm to mine the data to detect the association rules in each time period. We measured the similarity, difference and modified difference of the mined association rules based on three definitions and used the self-set rule matching threshold \(RMT\ to find all types of rules  References  Alex Sbesbunoff Winning CRM Strategies ABA Banking Journal 1999  Vince Kellen  CRM Measurem ent Fram eworks Blue Wolf White Paper p.4, 2002  Ott, J., çSuccessfully develop ment and Imp lementing Continuous relationship management eBusiness executive report 2000, 26-30  Adriaans P. and Zantin ge D., çData Mining  Addson-Wesley Harlow, 1996  A. Berson, K Thear ling and S. Smith, çBuilding Data Mining Applications for CRM McGraw-Hill 2000  Jill D y chÈ, çTh e CRM Handbo ok: A Business Guide to  Customer Relationship Management Addison Wesley Professional 2002  Swift, Ron ald  S., çAcceler at in g Custom er R e lationships Using CRM and Relationship Technologies Prentice Hall PTR 2001  Song H.S., Kim J.K and Kim S.H. çMining th e change of  customer behavior in an internet shopping mall Expert Systems with Applications 2001  Shaw M.J., Subramaniam C., Tan G.W. and Welge  M.E.,éKnowledge management and data mining for marketingé, Decision Support Systems 2001 1 Grupe, G. H., Owrang, M. M., çDatabase Mining Discovering New Knowledge and Cooperative  Advantage,é 12, 1995, 26-31 Proceedings of the 29th Annual International Computer Software and Applications Conference \(COMPSACê05 0730-3157/05 $20.00 © 2005 IEEE 


is used because we handle integers representing indexes of transactions There are at least two ways to tackle Radix Trees It depends on the kind of strings we have If we use variable length strings then every nodes in the tree can store a word internal nodes and external nodes or leaves see Figure 1 Otherwise if we use xed length strings a node stores a word if and only if it is a leaf see Figure 1  0 0 0 1 1 0 0 1 1 xed length strings 1 0 0 1 1 variable length strings Stored values  0 1 4 and 7 Figure 1 Different representations of Radix Trees The bottom tree on Figure 1 has two sorts of nodes black nodes and white nodes The white color means that no word is stored in a node of this color Conversely a black node means that a word is stored in the node For instance if we are looking for the word 10 in a Radix Tree with variable length strings we follow the right edge  1  then we follow the left edge  0  and we check the color of the node If the color is white then the word doesnêt exist in the tree otherwise the color is black see Figure 1 by construction In a Radix Tree with xed length strings we donêt need any colour because if a word doesnêt exist then there is no path for it in the tree We do prefer to use such structure because it is more convenient for implementing tree management operations efìciently and easily Radix Trees have the property to represent sets of string in a sorted way Their tree structure makes the set operations union intersection easier to parallelize In the operations we consider now see Figure 2 the size of data is constant and known 0 0 0 1 1 1 1  0 0 0 1 0 0 1 1  0 0 0 1 1 0 0 1 1 Union of Radix Tree 0 0 0 1 1 1 1  0 0 0 1 0 0 1 1  0 0 0 1 1 1 Intersection of Radix Tree Figure 2 Basic useful operations on Radix Trees 3.2 Operations on Radix Trees The union of two Radix Trees representing sets is the Radix Tree representing the union of the sets The intersection of two Radix trees representing sets is the Radix Tree representing the intersection of the sets A possible implementation of a tree structure consists in using nodes that contain pointers on successors This method has some drawbacks in particular it uses a lot of memory and it limits the princi pe of spatial locality the next element to handle is located in memory near the current element Note also that multithreadin g operations of Figure 2 is straitforward For the intersect operation for instance the principle is as follows We consider the two roots If they have two left children then we add a left child to the new tree and we start a new thread in order to build the left part of the intersection The same construction is made for the right child Some technical problems occur with such algorithm First of all since we deal with tree height of say 40 e.g we handle set of 2 40 elements the number of created threads may overpass the physical limit of the operating system Second unbalanced computation may occur For instance assume that we decide to x the maximal number of active threads to 2 and we have no thread scheduling mechanism Starting from the root we decide to start one thread to realize the intersection for the left child and we also start one thread for realizing the intersection o n the right child If the Proceedings of the Fifth Mexican International Conference in Computer Science \(ENCê04 0-7695-2160-6/04 $20.00 © 2004 IEEE 


left child has many more internal nodes than the right child the computation will not terminate at the same time and the rst terminating thread could be reused for the computation on the left child So we need also efìcient scheduling policies for thread management 3.3 Radix Trees storage Due to the huge quantity of data to deal with in practical applications we have to nd methods to store them on disk Since databases are too important to t in main memory we need to balance in and out-of-core computations In Kos04 a method has been introduced and implemented successfully This method represents the Radix Tree by bit vectors stored on disk The solution adopted in Kos04 t o i mpl ement R a di x trees on disk is to store them on an organization with multiple les We choose a le organization with few les by directory in order to avoid costly le system operations and not too large les to operate fast database updates Indeed too many les in the same directory could slowdown the application and using too large les cause poor response time for updates We use a directory tree st ructure containing small les quickly updatable In K the items of the d atabase are inde x e d b y s tor ing their identiìer in a Radix Tree stored in a directory tree structure where each directory contains three les  A le to store the thesaurus database item lexicon of the items and the offset of their bit vector on the second le 1  A le to store the bit vectors of the identiìers for level n 2  A le to store a permutation of the words giving the lexicographical order 3 To store the Radix Trees on disk we are using a bit vector of size n per word le 2 Radix Trees are using an alphabet of size 2 n  Each preìx designates the next directory containing the path to the databaseês line of the word An internal directory may contain 2 n subdirectories Let us call k the height of the bit vector hierarchy of Figure 4 Assume the indexes of a found line are i 0   i k  1 then the corresponding line is i k  1 2 n  i k  2 2 n    2 n  i 1 2 n i 0     The permutation le sto res a permutation p from 0 t  1 onto itself where t is the cardinality of the thesaurus The i th word in lexicographical order is the p  i  th word of the thesaurus Thus the thesau rus has not to be maintained in lexicographical order which eases the addition of words because only the permutation le has to be rewritten A search           Thesaurus Offset in 2nd file Permutation  W1 offset W2 offset  W1 W2 file \(1 file \(3 file \(2 Figure 3 Radix Trees hierarchical representation in the permutation le is equivalent to a search in a sorted array so the complexity is in O log n  where n is the number of words in the directory thesaurus In order to search the lines where an item appears we consult the permutation le le 3 This gives us the position of the word in the thesaurus le 1 where we can read its bit vector offset in the le 2 From here we can deduce the next directories to visit         root directory a directory of level 1 a directory of level 2     a directory of final level  file \(2 file \(2 file \(2 file \(2    a directory of level 1  file \(2   Figure 4 Radix Trees hierarchical representation The representation of integer set with Radix Trees allows us to save space and to implement efìcient searches Indeed the common preìxes of different integers are stored only once Furthermore each value is inserted and found in constant time depending of the integerês representation Proceedings of the Fifth Mexican International Conference in Computer Science \(ENCê04 0-7695-2160-6/04 $20.00 © 2004 IEEE 


size unlike a list structure linear or logarithmic time depending on the list organization Radix Trees are currently used successfully in Kos04 for building a sequential SQ L service as it is deìned for database systems UW The k e y for ef cient parallel implementations of tree management operations union intersect is that computation can be achieved concurrently on each node at a same level in the Radix Tree whose concrete implementation follows Figure 4 We also use Radix Tree structures in the context of association rules discovery in particular in the context of candidate generation 4 Radix Tree for parallel association rules discovery algorithms 4.1 Candidates representation The aim of this section is to show that using Radix Tree for association rules discovery algorithm can improve performance of the whole discovery process Based on our experience Kos04 w ith the u s e of R a dix T rees for the i mplementation of an SQL service performance will be improved signiìcantly First results with our codes implementing a SQL service based partially on Radix Trees demonstrate sigiìcant improvements by a factor at least 5 when we experiment with the TPC-C Transaction Processing Performance Council benchmark C and comparing to commercial SQL services In Kos04 R a di x T rees are us ed t o code t h e l i n e i nde x e s of each item in a database For instance consider the Accident table of Figure 5 It has several columns and each of them is treated separately for each of these columns one builds its thesaurus and for each word of the thesaurus we build the set of line indexes it occurs at  Client Id Max Amount Seller Kind of Cont Min Ref Acc. Id 1 2 3 4 5 6 7 House Car House House Car Family House 1 2 1 1 2 3 4 Contract Date 12ä21ä1992 02ä24ä2000 11ä28ä1996 05ä30ä2001 07ä17ä1992 04ä13ä1998 09ä11ä1999 450,000 230,000 780,000 830,000 12,000 27,500 1,000,000 2 17 11 2 3 2 2 900 11,000 2,400 1,350 830 912 100 Figure 5 The Accident table For instance the column Kind of Contractéês thesaurus is House Car Family and the sets of line indexes are House occurs at indexes 1  3  4 and 7  Car occurs at indexes 2 and 5 and Family occurs at index 6  Now suppose that we have the following query nd all items with the property Kind of Contract  House and Max Amount  500 000 In order to solve the query we intersect the corresponding thesaurus Radix Trees Here we nd  4  7   The intersection can be implemented efìciently because the computational cost is bounded by the number of bits in the representation of integers in fact we use xed size alphabet and not by the number of items in the two sets Let us now consider the use of Radix Trees in our context of Association Rules Discovery The rst idea is to put the list of candidates identiìer in Radix trees stored locally on each node For instance if we have four items A B C D we start with the complete binary tree as depicted on Figure 6  A B CD t1 t2 t4 t1 t4 Figure 6 Candidate representation By adding for each item A B C D a Radix Tree that contains the line indexes of transaction identiìer we call such Radix Tree a transaction tree  the support evaluation phase consists now in the intersection between transaction trees then by counting the number of leaves We note here that building the tree of candi dates and all the transaction identiìer lists can be done in one scan of the database The candidates k-itemsets are then represented in the same way For instance if the path in the Radix Tree to the item A is 00 and the path to B is 01  the path to AB will be 0001  Note also that the cost of computing supports is given by a very simple intersection operation between trees Moreover as we make progress in the computation we can store on local disks the partial supports in order to retrieve it efìciently in case of a reuse 4.2 Candidates generation The new candidates for association mining rules discovery can be generated by joining the members of a same Proceedings of the Fifth Mexican International Conference in Computer Science \(ENCê04 0-7695-2160-6/04 $20.00 © 2004 IEEE 


  B CD 0 0 1 1 1 0 AB AC AD 0 1 1 1 0 A The equivalence class based on prefix A Figure 7 Equivalence class on Radix Tree equivalence class By coding itemset in Radix Trees as described on Figure 6 all the members of an equivalence class are in the subtree of the itemset deìning the class Indeed the itemset that deìnes a class is the preìx of all the members of this class In a Radix Tree where all elements of a subtree have the same preìx equivalence classes can be viewed as subtrees of it For instance the equivalence class S A is the subtree rooted in A see Figure 7 Now in order to generate the next candidate sets we have to join the members of the equivalences classes According to our join operation Ra dix Tree implementation of the itemsets consists in rooting the initial subtree on each leaf considering only leaves obtained by omitting the left neighbours ot the current itemset see Figure 8 A B C D AA AB AC AD CA CB CC CD  Figure 8 Candidates generation For instance to obtain the ABC candidate we join AB and AC with a Radix Tree rooting operation To get all candidate sets we just have to proceed the rooting of subtrees with elimination of left nodes on each leaf In our case ABC  AB  AC  ABD  AB  AD  ACD  AC  CD etc Unfolding this algor ithms leads to the tree presented on Figure 9 Moreover by performing the intersection of the identiìer trees list in parallel we obtain the support of the new candidate As with the Count Distribution Algorithm the  AB CD AB AC AD BC BD CD ABC ABD ACD BCD ABCD Figure 9 Candidate representation nal only communication between processors consists in broadcasting local supports to evaluate the global support But before obtaining the distant supports of a candidate set we can eliminate from the local tree those candidate that donêt appear in the local database i.e supports equal to 0 for instance We know that if AB donêt appear in the base ABC cannot appear later We can also overlap the beginning of the next phase of the algorithm with receiving all the supports from other processors Indeed eliminating an invalid candidate which is detected after total support evaluation corresponds to a low cost operation If the items are homogenous distributed over the transactions we may assume to anticipate candidates set construction in a good way A formal presentation of our parallel algorithm and expected performance are now introduced 5 Parallel algorithm First of all if we have p processors we assume that the transaction database is splitted into p chunks one chunck per processor Second in the rst part of the algorithm we have to construct the tree of items and their transactions trees It can be done in one scan of the local database and for each processor in parallel To evaluate the support of an itemset we just have to intersect Radix Trees and proceed to the count of their leaves that can be done at the same time during the intersect operation For instance the support of ABC is the number of leaves of the tree produced by intersecting AB and AC transactions trees See Figures 8 and 9 for a illustration The construction of candidates sets is done as explained above 4.2 Finally our main algorithm is stated as follows Proceedings of the Fifth Mexican International Conference in Computer Science \(ENCê04 0-7695-2160-6/04 $20.00 © 2004 IEEE 


Algorithm executed on each Proc 0  i  p   Initially each processor has locally n/p lines of the transaction database where n is the total number of lines and p is the processor number 1In parallel for each processor Scanning of the local database for construction of 1-itemset tree 2In parallel for each processor do Broadcast supports  This part can be de-synchronized   to perform overlapping see above  Wait for all supports from others Perform the sum reductions Elimination of unsufìcient itemsets support L k  rest of C k Construction of new candidates sets C k 1  while  C k 1     3frequent itemsets   L k 5.1 Hints for complexity analysis We introduce here some discussion about the cost of each step of the parallel algorithm in terms of time and space 5.1.1 Construction of the item trees As pointed out previously our algorithm requires only one pass over the database This pass aims to build our Radix Trees Then we operate only on Radix Trees 5.1.2 Support evaluation and bad candidates elimination We can make the count of tree leaves at the same time we construct the transaction tree i.e by doing intersection operations The time co mplexity of an intersection is bounded by the number of different items in the database and not by the number of items in the database This property justify the use of Radix Trees Thus the local support is known when the candidate set is constructed If an itemset support is null i.e the itemset do not appear in the local database we can immediatly eliminate it even if the itemset appears in another partition of the database For all local support that are not null we can start a new construction of candidate supersets before knowing the total support A candidate elimination consists in cutting an edge in a tree 5.1.3 Candidate set construction The construction of new candidates consists in rooting subtrees on leaves We eliminate from the tree the candidates that have unsufìcient supports and the nodes that make repetition in the subtree rooted for instance we donêt add the item A to the itemset ABCxxx In doing this we do not construct unnecessary candidates To obtain the support of newly created candidates we just have to proceed to the intersection of the transaction tree of the added item with the transaction tree of the leaf where it has been rooted At any time the algorithm knows the previous level of the itemsetês tree to generate new candidates and evaluate their supports So we can save memory by deleting the upper levels 5.2 Scheduling policies for thread management Radix Trees operations can also be parallelized by using threads This is particular useful if we run the algorithm on a parallel machine with SMP nodes Let us consider the union operation in parallel of two Radix Trees Starting from the roots of the two trees one strategy is to activate a thread for computing the union of the two left children and to activate a thread to compute the union for the two right children We apply r ecursively this principle until we reach the maximum number t max of authorized threads In this case we have to wait for the completion of one thread before going on The key idea of our thread management strategy is the following When a node has two subtrees and it remains an idle thread we use it on one of the subtrees The other subtree will be managed by the thread running on the current node An operation union on a node is completed when it is also completed on the subtrees of the node So to minimize the idle time on each processor we decide to launch threads on the subtree containing the less number of nodes In the worst case the thread nishes its work in the same time than the thread which launched it At this point the thread can declare itsef as an available thread without waiting for the result of the other subtree Pictorially speaking we proceed as depicted on gure 10 where each arrow symbolizes the work of a processor Moreover in the reminder of the subsection we consider the case t max 4   Figure 10 Thread management policy Proceedings of the Fifth Mexican International Conference in Computer Science \(ENCê04 0-7695-2160-6/04 $20.00 © 2004 IEEE 


On gure 11 we can see that we have no idle time when we use the scheduling policy described above compared to the opposite scheduling policy illustrated on gure 12  W = 6 W = 2 W = 3 W = 3 W = 2 W = 6 T1 T2 T3 T4 Figure 11 Preferred policy with 4 processors W = 6 W = 2 W = 3 W = 3 W = 2 W = 6 T1 T2 T3 T4 Figure 12 Invert policy with 4 processors On Figure 11 threads T1 and T2 nish their work one round after T3 and T4 Then T3 and T4 can be reused in other operations On Figure 12 threads T1 and T2 nish their work one round before T3 and T4 but have to wait the completion of T3 and T4 to root the results 6 Conclusion In this paper we have introduced a parallel algorithm using Radix Tree structures in order to discover association rules in a transaction database Our algorithm has many interesting features It scans the base only once performs candidate generation in parallel with only few integer exchanges representing supports computed locally between processors Based on our experience Kos04 we gues s t hat i mplementations will encompass ex isting implementation because we know that one key to get performance for association mining is the way we manadge intersect operation In our case we have proposed a new approach that permit us to compute the candidate support by the intersection of Radix Trees Radix Trees offers a good compromise Kos04 b etween the storage size required to store them and the efìciency to retreive any information mapped to integers We are currently implementing the associa tion rules discovery algorithm presented in this paper and we also implement an efìcient multithreaded library in order to accomplish in parallel Radix Tree operations References AIS93 Rak esh A g r a w al T o masz Imielin sk i an d A ru n N  Swami Mining association rules between sets of items in large databases In P Buneman and S Jajodia editors Proceedings of the 1993 ACM SIGMOD Int Conf on Management of Data  pages 207Ö216 Washington D.C 26Ö28 1993 J Ruomi ng Ji n and Gagan Agra w a l  An ef  c i e nt associ ation mining implementation on clusters of SMP pages 156Ö156 K Mi chel K o skas A h i e rarchi cal dat abase management al gorithm To appear in the annales du Lamsade 2004 url http://www.lamsade.dauphine.fr UW Jef fre y D  U l l m an and Jenni fer D  W i dom First Course in Database Systems A 2/e  Prentice Hall 2002 Z Mohammed J Z aki  Paral l e l a nd di st ri b u t e d a ssoci ation mining A survey IEEE Concurrency  7\(4\:14Ö25 1999 ZPL Mohammed Ja v eed Z a ki  S ri ni v a san Part hasarat hy  a nd Wei Li A localized algorithm for parallel association mining In ACM Symposium on Parallel Algorithms and Architectures  pages 321Ö330 1997 Proceedings of the Fifth Mexican International Conference in Computer Science \(ENCê04 0-7695-2160-6/04 $20.00 © 2004 IEEE 


3,4,7,8?9 0.9,1 9 0.9,1 8 0.9,1 7 0.9,1 4 0.9,1 311 0.4,1 0.3,1 6 0.3,1 5 0.3,1 210 0.1,1 0.3,1 1 0.3,0.75 6\(11 6,11 0.3,0.75 0.2,0.5 2,11 0.2,0.5 1,11 6 is transferred B FS Phase D FS Phase 5 is absorbed Absorb Null Figure 2. An Illustration of the Two-Phase Maximal Hyperclique Pattern Mining Process De?nition 10 The Equivalence Item Set of a pattern equivalence, is the item set for storing  ís PE items If we ?nd an item is a PE item of a pattern  , we could add it to  .equivalence. While the super patterns of are generated, they will succeed their own PE items from equivalence. In this case, the items of  are separated in  .item and  .equivalence, and the target pattern of should be  .item *  .equivalence De?nition 11 Union of a pattern,  .union, is             u n i o n  i s  t h e  t a r g e t  i t e m s e t  o f     I n d e e d  support\(  .union item Proceedings of the 16th IEEE International Conference on Tools with 


Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE De?nition 12 Size of a Pattern\(P P on the size of P.item, no matter how many items in P.equivalence De?nition 13 Sub pattern: For two pattern      , if item is a subset of   .item, we say that   is a sub pattern of   , even   .union is not a subset of   .union, and is a super pattern of   . If the size of   is smaller than the size of   ,   is a pure sub pattern of De?nition 14 For a Pattern   , if ií is   ís equivalence item and all the items in    .item are lexicographic before item   , we say item   is a Pro equivalence item of De?nition 15 For a Hyperclique Pattern    , if item   is both    ís PE item and Pro equivalence item, the item is a Pro Pure equivalence item \(PPE item 3.2. Pruning Methods in the BFS phase At the initial stage, the algorithm generates the size1 patterns and counts the support of these patterns All the items which have support less than the userspeci?ed support threshold are pruned. Meanwhile, these items are sorted during this stage. For instance, consider the example dataset shown in Table 1, item      Also, as shown in Figure 2, the algorithm constructs the size-1 hyperclique patterns and sort all items in lexicographic order In the BFS phase, the algorithm applies an apriori-like approach to generate the size-L hyperclique patterns from size-\(L-1 strategies applied in this phase as the following Prevalence Pruning. In the apriori-like algorithm a size-k pattern generated by joining two size-\(k-1  and     If   "  and  


exist, the algorithm rst checks whether all the other size-\(k-1 exist. If one of the sub patterns does not exist,   is not a hyperclique pattern and can be pruned. This is a standard pruning method [1 H-con?dence Pruning. Before generating a size-k pattern   , we could calculate the ratio   If this ratio is less than 1 2 , hconf 2 , since support   as shown in Figure 2, support\(1 3 hconf\(  1,3  4 5 6   5   6   5  7  7 2 therefore the pattern  1,3  is pruned Equivalence Pruning. We apply the 9    method to reduce the number of patterns generated. If support  should be a PPE item of    "  , and be absorbed into    "  .equivalence. In Figure 2, support\(  5 6 5 set and prune  5, 6 When generating a size-k hyperclique pattern    , if the items in its size-\(k-1 lence sets are PE items of    ,    can succeed these items to its own equevalence set. For instance, in Figure 2, both  1, 5  and  2, 5  succeed item 6 from 


5  .equivalence, but do not succeed item   since it would break the limitatin of h-con?dence When    "  absorbing item   , all the equivalence items of the other size-\(k-1 5               5                    "      , are also equivalence items of    "  .    "  could tranfer these items to    "  .equivelance if they are PE items. In Figure 2, while generating the pattern  1, 2, 5  from  1, 2  ,  1 5  and  2, 5  , the pattern  1, 5  will absorb item  , and transfer item   from the pattern  1, 5 Indeed, when generating    , if item   is in   equivalence, it is unnecessary to generate the but transfer the PE items in the other size-\(k-1 equivalence set to    "  .equvialence After generating the size-k hyperclique patterns, we could check all the size-\(k-1 graphic order. For a size-\(k-1 is not a subset of any size-k patternís union, it will be impossible to generate a hyperclique pattern whose union is the superset of    "  .union in the following process. If this union is not a subset of an itemset in current Maximal Hyperclique Pattern Set \(MHPS is a maximal hyperclique pattern and could be added to the MHPS. For example, in Figure 2, after generating the size-2 patterns, the union of  1, 2  is  1, 2, 5, 6  , and no superset in either size-3 patternsí union or MHPS. Hence, the algorithm adds the union into MHPS. For pattern  1, 5  , the union of this pattern is  1, 5, 6  , and this pattern has no superset in size-3 patternsí union, but has a superset in MHPS hence this pattern is pruned 3.3. Pruning Methods in the DFS phase In the BFS phase, the algorithm has identi?ed all the size-L hyperclique patterns. At the beginning of the DFS phase, the algorithm adds the tail items to these patternsí tail sets. For a size-L hyperclique pattern item=             @  , if there is an item   such that: \(1 item   A B   .equivalence, \(2 lexicographic before ií, and \(3   have been generated, the algorithm adds item   to   .tail. For instance, in Figure 2, item 8 and 9 are added to  3, 4, 7  ís tail set The super patterns of a hyperclique pattern generated with the item in   .tail, and succeed the PE item from   .equivalence Proceedings of the 16th IEEE International Conference on Tools with 


Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE EquivalencePruning. Similar to the BFS phase, if a tail item   is a PE item, we will add   the patternís equivalence set. If the size-1 pattern     ís equivalence set is not null the super patterns will succeed PE items from this set Full Pruning. When we process the Pattern   , if the union of   .item,   .equivalence and   .tail is a subset of a pattern in current MHPS, all of the patterns generated by   cannot be MHP since they have a super Hyperclique Pattern. We could prune this pattern directly. In Figure 2 when we process  3, 7, 8  , which tail set is  9  ,  3, 4, 7, 8 9  has already been added toMHPS.Wewill ?nd   is a subset of  3, 4, 7, 8, 9  , and prune  3, 7, 8 LeftMost Pruning. When processing a hyperclique pattern   , if the pattern in the end of this path is found to be MHP, all the patterns in the other paths should not be MHP In this case, we could skip these patterns[6]. In Figure 2 the end of left most path of  3, 4, 7  is  3, 4, 7, 8, 9  , and we ?nd this pattern isMHPS, we can skip all the other paths of  3, 4, 7  , and continue to process the next pattern Dynamic Reordering. Bayardo showed that the bene?t of dynamically reordering super patterns of   is signi?cant [5]. The mining speed will be 2 to 4 times faster. We sort the super patterns in the increasing order of support H-con?dence Pruning. Similar to the BFS phase, for a tail item     we could prune   from   .tail Prevalence Pruning. Since we have generated the sizeL hyperclique patterns in the BFS phase, for a hyperclique pattern   , if one size-L sub pattern of   is not generated, we need not generate it In the DFS phase, if a hyperclique pattern cannot generate any super hyperclique pattern, or none of these super hyperclique patterns could succeed all the items in its equivalence set, it will be impossible to ?nd a super union of this patternís union in the future. We will check this union with MHPS. If there is no super pattern in MHPS, we will add the union to MHPS 


4. The MHP Algorithm Figure 3 shows an overview of the hybrid MHP algorithm for mining maximal hyperclique patterns. As can be seen, there are two phases: the BFS phase and the DFS phase in the algorithm 4.1. Algorithm Description In the ?rst BFS phase, Initial Function generates the size-1 frequent patterns, which are also size-1 hyperclique patterns, and items are sorted in order. In Generate and Prune Super Function, the prevalence pruning, hcon?dence pruning, and equivalence pruning are applied to prune the search space and size-k hyperclique patterns are generated from size-\(k-1 ter extracting the size-k patterns, the algorithm extracts all size-\(k-1 union in size-k hyperclique patterns to       ! . In Check and Add Function, the algorithm checks the patterns in       ! , if their unions are not subsets in MHPS these unions are added into MHPS In the second phase, the Append Tail Function adds the size-L patternsí tail item. Extract MHP is the major function for DFS mining. The traditinal optimal methods, full pruning, leftmost pruning, and equivalence pruning, and new methods, prevalence pruning and hcon?dence pruning, are implemented in Function Generate and Prune Super. The Sort and Append Tail Function implements the dynamic sorting and add tail items for the super patterns. Finally, the algorithm checks whether the pattern being processed is in MHPS or not by the function Check and Add 4.2. Completeness and Correctness Here, we prove the completeness and correctness of our MHP algorithm. To facilitate our discussion, we ?rst introduce some lemma and a new concept, Covering Pattern Lemma 2 If a hyperclique pattern   ! is generated in the BFS phase, none of the item in   ! .itemset could be a PPE item of any sub pattern of Proof: This lemma proof as well as some following lemma proofs are presented in our Technical Report [7 Lemma 3 When a hyperclique pattern   ! is generated in the BFS phase and the size of   !  " , if # an item i 1 2 3 ií  is also a hyperclique pattern,   ! .item ií  will be generated by our algorithm Lemma 4 If a hyperclique pattern   ! is generated in the BFS phase, all of its PPE items would be added to the  


equivalence by the algorithm Lemma 5 For an equivalence item which is transferred by a hyperclique pattern   , it could also be added to equivalence set with absorbing or succeeding if we do not use transferring method Lemma 6 For a hyperclique pattern   , all items in equivalence are PPE items of some sub pattern of this pattern De?nition 16 For a hyperclique pattern  ! , if \(1 is a hyperclique pattern, \(2 item, and \(3 item,  $ is a Covering Pattern of  ! . Obviously support\(  $ .item item union Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE MHS ALGORITHM Input: \(a b c d The retrieve level of the BFS phase Output: \(1 MHPS with support hconf and its superset without both such two properties Variables: k: the itemset size  a set of size hyperclique patterns   a set of size candidate maximal hyperclique patterns set of maximal hyperclique patterns a set of superset generate from Phase I: generate Hyperclique Patterns by BFS 1.    = Initial  2. for \(k=1 k 3  Generate and Prune Super  4 


  set of patterns in without superset union in  5. Check and Add     Phase II: extract Maximal Hyperclique Patterns from    by DFS 6. Append Tail 7. for 8. Extract MHP Function Extract MHP\(Pattern 9.       =Generate and Prune Super 10. Sort and Append Tail 11. for  item 12. Extract MHP 13. if  .union havenít a super union in 14. Check and Add\(  ,MHPS Figure 3. Overview of the MHP Algorithm Lemma 7 If a pattern is a hyperclique pattern, one of its Covering Pattern must be generated Lemma 8 TheMHP algorithm is complete. In other words all the Maximal Hyperclique Patterns will be identi?ed by the MHP algorithm Lemma 9 The MHP algorithm is correct. In other words any patterns identi?ed by the MHP algorithm are maximal hyperclique patterns Note that if we set the search depth in the BFS phase large enough, our algorithmbecomes a pure BFS algorithm This means equivalence pruning will work correctly in an apriori-like algorithm for mining maximal hyperclique pattern. Additionally, if we set the h-con?dence threshold to zero, the algorithm ?nds maximal frequent itemsets 5. Experimental Evaluation In this section, we present extensive experiments to evaluate the performance of the MHP algorithm. Speci?cally we demonstrate: \(1 son between the MHP algorithm and standard maximal frequent pattern mining algorithms and \(2 MHP algorithm on ?nding maximal hyperclique patterns 5.1. The Experimental Setup Our experiments were performed on two real-world date sets    ! " and    ! " # , which are benchmark 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207ñ216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intíl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intíl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





