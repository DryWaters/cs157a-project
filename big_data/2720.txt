Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 THE RESEARCH ON MODEL OF MINING ASSCICIATION RULES BASED ON QUANTITATIVE EXTENDED CONCEPT LATTICE DE-XING WANG XUE-GANG w mal WANG Department of Computer Science and Technology, Hefei University 01 Technology, Hefei 230009,China E-MAIL wangdexing198706@sina.com xueghu@mail.hf.ah::n wangzb@mail.hf.ah.cn Abstract Concept Lattice represents knowledge with the relationships between the intension and the extension of concepts and the relationships between the generalization and the specialization 
of concepts, thus it is properly applied to the desription of mining association rules in databases The Quantitative Extended Concept Lattice QECL evolves from concept lattice by introducing equivalent relationship to its intension and quantity to its extension which further enriches the relationships between its intensions. Based on QECL we can mine association rules comparing with well-known Apriori Mining association rules on QECL does not need to scan databases for many times has higher quality of time wmplexity and shows 
association NI on the Hasse diagram of QECL more visual and concise, moreover it can he used to mine association des interactively according to user's subjective interest Keywords Data mining Association rules; Concept lattice; Frequent item sets 1 Introduction Concept lattice I also called Galois lattice\was firstly proposed by R Wille in 1982 and built on binary relationships between the intensions and extensions of concepts the extensions of which represent a set of objects the intension represents the common features that all objects in the extension have and concept lattice reflects entity attribute relationships between objects and the corresponding Hasse diagram 
of concept lattice vividly visualizes the relationships in the concepts. However, there are some limitations in the concept lattice, xuegang Hu have improved Concept Galois Lattice to Extended Concept Lattice \(ECL By introducing equivalent relationships to the intensions of concepts 2J.41 then enriches the relationships between intensions of concepts, afterwards, developed the Extended Concept Lattice to Quantitative Extended Concept Lattice QECL by quantifying its extensions of the extended concept lattice in a sense He have enriched concept lattice theory and made quantitative extended concept lattice present knowledge more concise and distinct Mining association Rules s6,71 is'an important task in 
KDD it is to generate all association rules whose supports and confidences are bigger than the user-specified minimum supports and minimum confidences respectively Some approaches have been proposed for minimg association rule based on concept lattice I In the paper we will present a model to mine association rules on quantitative extended concept lattice The paper is organized as follows In Section 2 we review the preliminaries of association rules Section 3 introduces basic notions of concept lattice extended concept lattice and quantitative extended concept lattice Section 4 presents mining association rule on QECL Section 5 describes the generation algorithm of QECL and its analysis in 5;ection 
6 Section 7 shows an example of mining associatim rules on QECL and the comparison with Apriori on well-known marked basket analysis 2 Association rule Mining association rules is an important branch of data mining which describes potential relations among data items attribute variant in databases The well-known Apriori algorithm was proposed by R Agrawal et al in 1993. Mining awociation rules can be stated as follows Let I=[il il __ i  he a set of items Let D the task relevant data be a set of transactions where each 
transaction T is it set of items such that Td The quantities of items boughi are not considered Each transaction is assigned an identifier, called TID Let A be a set of items A transaction T is said to contain A if and only if ACT An association rule is an implication of the form A*B where Ad BcI and AnB=0 The rule A*B holds in the transaction set Cl with support s where s is the percentage of transactions iii D that contain AuB i.e., both 
A and B This is taken to tie the probability P AuB The rule A*B has confidence c in the transaction set D if c is the percentage of tansactions in D containing A that also contain B This is taken to he the conditional probability P BIA That is Siipport A*B AuB Confidence A=$B BIA Mining assoc.lation rules is to mine strong association rules that satisfy the user-specified both minimum support threshold and coofidence threshold SupporC A3B A 0-7803-7508-4/02/$17.00 WO02 IEEE 134 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 3 Concept lattice A context is defined as a triple C 0 D RI  where 0 is a set of objects D is a set of attributes, and R is the binary relationships between 0 and D according to the inclusion relationships between extensions there is a unique ordered set that describes the structure of inherent lattice which defines natural groupings and relationship descriptions between the objects and their attributes in the context This structure is also known as Concept galois lattice Define 3.1 Each couple such as A B derived from the context is called a concept where AE P 0 BE P \(D\and P O P D are the power sets of 0 and D respectively. A and B create a connection in terms of the following properties A'=\(meDlVgeA g R m J B'=\(gEOlVgEB g Rm Where A'=B B'=A A is called the extension of concept and B is called the intension denoted as Extension\(C and Intension \(C respectively Defme 3.2 In the context C=\(O D R for a given concept A, B if B CA and Bi'=A then B is called an equivalent feature grouping of A simply called equivalent grouping In particular A is also an equivalent feature group of itself, then Equ A B,I B CA and can be defined Define 3.3 A couple A,B derived from context C is called a basic concept where AsP\(O BEP\(D and B\200Equ\(A If two concepts CI A,B CZ A,Bz are equivalent two basic concepts B Bz are equivalent grouping, then we also call that B1 BZ are equivalent basic intensions, therefore, C=\(A, Equ\(A U B2 is obtained the rest may be deduced by analogy, When there are k basic concepts CI CZ __ Ck  we can deduce C A Equ\(A A BI U Bz U B3  U Bt which is called an extended concept or simply called concept and B,\(I=1,2  k is called equivalent intension Define 3.4 In a ECL partial order relation   between concept C,=\(AI BI and Cz=\(A2, B2 is defined as CI<C2~BzCBI then C1 is the sub-concept \(son\of CZ and Cz is the supconcept father of C,, that is the relationship between Cz and CI is the relationship father and son If CI<C2, there isn't a concept C=\(A B such that satisfy CI<C<C2, C1<Cz is called an immediate-sub concept immediate-sup-concept-relation between C and Cz the Hasse diagram of concept lattice is generated according to the partial order relationship If CI<C2 is an immediate-sub concept-immediate-sup-conce-pt there exists a edge from cz to c If concepts CI=\(AI B1 and Cz=\(Az B2 in a ECL satisfy AlcAz in the context C there is a sub-concept-sup concept-relation between CI and C2 denoted as CI<C2 Concept sets and their immediate-sub-concept -immediate sup-concept relationships in the context consist of a complete concept is called Extended Concept Lattice simply called ECL It can be proofed that GCL and ECL is isomorphic In particular the concept containing all objects of C is called full concept and empty concept is such a concept that contains none of objects of C The sets of concept with these equivalent relationships constitute a complete lattice structure is also an Extended Concept Lattice Define 3.5 The Generalized Super-concept of concept C denoted as super\(C can be defined as follows 0 The immediate supconcept of C is generalized Super\(C C<Cp CI E snper\(C Generalized Super\(C includes of generalized Super\(C of the immediate sup-concept of C namely C<CI C E super\(C C2 E super\(C J Define3.6 As for concept C=\(A B C'=\(IAI B is called Quantitative Extended Concept of C, C is real concept of C where IAl is the cardinal of extension A We mark basic quantitative extended concept comprised of intension B by C B and extensions of those are N by BasC \(N The equivalent basic concepts CI=\(A Bl Cz=\(A B2 are corresponding to equivalent basic quantitative extended concepts CI IAl Bl C,'=\(IAI B2 respectively however the relationships between quantitative extended concepts depend on relationships between real concepts that is C,<Cz if and only if Cl'<C Lattice derived from quantitative extended concepts is called Quantitative Extended Concept Lattice simply called QECL Since we have ignored the specific extension information realization of some problems solutions will be changed Theorem 3.1  Concept CI=\(BI and concept CZ=\(BZ whose intensions cannot intersect each other is equivalent in a BasC \(N\if and only if C B,uBz BasC N Theorem 3.2 If concept CI=\(Nl Bl concept CZ=\(N B2 in aBasC N satisfy B,,Bz then CI and Cz is equivalent Theorem 3.3:In a BasC\(N C,=\(NI,BI NZ.BZ if and only if 3BII~BI.3BZ,~BZ satisfying C\(Bl B2 Theorem 3.4 CI=\(NlrBI and Cz\(N~.Bz in a BasC\(N satisfying B,2Bz and NIfNz then C2Esuper\(C1 Above the proofs of theorems we can prove them by the definitions and theorems of extended concept lattice, here an abridged 4 Mining association rules in quantitative extended concept lattice As for concept C=\(IAI B=\(Bl, B2  Bk in a QECL, we can introduce Support B of intension B to concept C M is the sum of transactions in D IAl is the extension cardinal of concept C Define 4.1 If Support B of concept C in a QECL is big than the support threshold, C is called a frequent concept where each intension B i=l,..k of B is a basic frequent items the redundant intension of B is called reaundant frequent items 135 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 45 November 2002 136 We can acquire basic frequent items from frequent concepts in a QECL obviously each grouping basic intension in frequent concepts is basic frequent item sets Since redundant intension is obtained from basic intension therefore we can also obtain basic frequent item sets from the corresponding to basic intensions of frequent concepts Theorem 4.1:Frequent items is corresponding to the intension of ECL each other Theorem 4.2 IfC=\(IAI B=\(BI BZ  Bk is a concept in a QECL, the support of B i=l  k\is IAI/M where IAl is the extension cardinal of C, M is the sum of transactions in D If B is frequent item sets whose support is bigger than the support threshold B must be a group of equivalent intension \(basic or redundant intension of the concept Base on the theorems mentioned above we could mine association rules in a given QECL Theorem 43:OIf concepts CI=\(Nl,B1 Cz=\(NZ,Bz satisfy CZ~sup\(CI we can gain association rule Bz 3 BI-Bz, its confidence is NI I Nz otherwise BI-Bz s Bz its confidence is 100 If concepts C1=\(NlrBI I Cz=\(Nz,Bz do not satisfy Czesup\(C1 and there exists nonempty common maximum-sub-concept C=\(N,B namely 3C\222=\(N\222,B\222 which makes CE sup\(C\222 AC;c sup\(C ACzs sup\(C\222 true there are association rules between BI and Bz BISBZ BZ 3 B their confidences are N/NI NiNz respectively 5 The generation algorithm of quantitative extended concept lattice From above the discussion in Section 3 we can obtain the algorithm tn generate quantitative extended concept lattice which should be inserted initial quantitative extended concepts corresponding to initial concepts one by one then adjust it according to Theorem 3.1-3.4 The algorithm is written as following insert a quantitative extended concept into QECL I A quantitative extended concept C=\(N,B\is inserted in the QECL, if there is equivalent concept of C in the QECL such that 3Cli Cli oC we merge the intension of C into the intension of Cli then finish the operation of insertion that is Intension\(Cli Clj C Else Won the assumption that CO is an immediate sup-concept of C fmtly insert C as a sub-concept of C into the QECL fmish the operation in turn as follows 0 each CI~\(C~~<C&;~<C connects C as an immediate sup-concept of Cli,at the same time remove the connections between C;i and Co  insert the concept Ck=\(Nk,Ik that is generated when C intersects concepts in the QECL into the QECL by the way the judgment of the relationships of equivalence and father son in the algorithm can\222be implemented according to Theorem 3.1-3.4 Attribute setA=\(al d an in the context 0 D R partition generated by attribute a can be expressed by Algorithm Insertion QECL C Oillvil Oizlvi z Oidvi where Oi]vij is a object set 0 of attribute ai whose value is vij therefore we can gain a array of in tial quantitative extended concepts QC\(ai N,~,ai=,vi Ni~,q=vi  Ni~,q=vm  by attribute q The generation algorithm of the QECL Create-In-Attr QECL is belov  1  Initialization To generate QECL with the complete concept 0  1 and the null concept  1 all as only two concepts do for C;EC\(ai\do Insertion\(QECL Ci 2 for I:=l to n 6 Algorithm malysis Compared with Apriori The generation algorithm of QECL has some advantages  time complexity: Apriori algorithm is an influential algorithm for inining frequent item sets for Boolean association rule Apriori employs an iterative approach known as a level-wise search where k-item sets are used explore k+l sets. Its time complexity is 0 2\224 In the worst case, since confined conditions of the thresholds of support and confidence are added in Apriori, in practice Its time complexity, will be improved better However, time complexity of the QECL generation algorithm is exponential inn In practice the QECL has typically 0 nZ or even 0 n rather than 0 2 224 resulting in a typical running time oi\221 0 n3 221*I which makes it feasible to generate QECL in large contexts On storage cclmplexity There isn\222t obvious difference they all need large storage On the mining association rules The Hasse diagram of QECL can show all frequent item sets and their supports on itself while Apriori can not on the one band when attributes object the conditions of mining association rule have been changed, Apriori has nothing to do but repeat However If attributes and objects have not changed when the mining association rules conditions have changed QECL will not rxonstruct again we can mine association rules until they satisfy the user appointed conditions on the other band, QECL can present association rules concise and distinct while Apriori algorithm\221s presentation is complicated and Ibstract 1 7 An example of mining association rules in QECL From the definitions and theorems mentioned above, we can mine associition rules on a QECL taking a typical Example 6.1 as market basket analysis on page 232 in literature 6 There are nine transactions and 5 Items in ALLllLectronics transaction database D that is M=9 we use data in Tabte 1 to generate QECL according to the definitions and theorems in above Sections, draw the corresponding Hasse diagram of QECL from the ALLELectronics transactions database D as shown in Fig 1 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 I Table 1 I 6.11 7.12 6.13 4,1112 4,1113 4,1213 2,1214.14 Fig I The Hasse diagram of QECL corresponding to the context in Table 1 In Fig.1 the support of frequent item sets is not divided by M=9 in order to be convenient for explanation we can easily obtain frequent item sets, for example such as the support of frequent item sets I1 1 12 and 13 is 6,7,6 respectively the support of frequent item sets 11121 III3 12131 I214 14 is 4,4,4,2 respectively the support of frequent item sets I11213 IlI215 I5 1 111214 is 2,2,1 respectively etc.'61 Similarly we can generate association rules from the Hasse diagram in Fig 1,for example association rule I1 A 12-15 the support of frequent item sets 1112 and 111215 IS is 4.2 respectively, then the confidence of association rule I1 AI2A5 is 24 as such I2AI5-11 11312AI5 I2d1 A I5  whose confidence is 2/2  2/6  2/7 respectively However the association rule I2 A 15311 Since concept I215 does not exist There is an common maximum-sub-concept 111215,,15 whose support is 2 we can obtain the association rule 12A IS-Il,whose confidence is u2 The results are identified with Apriori in generating association rules Finally we can generate association rules base on QECL the approach presents association rules visual vivid and concise, improves mining association rules efficiency and avoids the blindness Above all user can interactively mine useful association rules in Hasse diagram of concept lattice according to his subjective interests 8 Conclusions In the paper we discuss the algorithm of mining association rules base on QECL Since frequent item sets and their supports are shown on the Hasse diagram of QECL it is more convenient for us to mine association rules improve mining association rules pertinence, reduce searching space of the algorithm therefore it is obvious superior to Apriori which need to scan databases for a great number of times and sometimes repeat from scratch when the thresholds of support and confidence of association rules have been changed Acknowledgements The paper is supported by the National Natural Science Foundation of P R China under Grant No.69985004 the Education Foundation of Anhui Province of China &der Grant No.2000j1168zd china References R Wille Restructuring lattice theory An approach based on hierarchies on concepts in Ordered Sets I Rival ed Dordrecht-Boston Reidel pp.445-470 1982 Hu Xuegang Etc The Design Knowledge Representation and Reasoning in Intelligence CAD Based-on Extended Concept Lattice In Proc of 3rd International Conf on Computer-Aided Industrial Design and Conceptual Design Hong Kong pp 460-463 Nov 2000 Hu Xuegang Etc The Research on Design Knowledge Representation and Acquisition in Intelligence CAD In Proc of 2nd International Conf on Computer-Aided Industrial Design and Conceptual Design, Bangkok, Thailand, pp 236-239 Nov.26-28 1999 Hu xnegang The Research of Models on Knowledge Discovery in Databases PH.D  Hefei university of Technology, Hefei 2000,Anhui China In Chinese Raskesh Agrawal Tomasz hielinski Arun Swami Mining Association Rules between Sets of Items in Large Databases Proceedings of ACM SIGMOD Conference on Management of Data, Washington DC pp.207-216 May 1993 Jiawei,Han Micheline Kamber Data Mining concepts and techniques High Education Press Morgan Kaufman Publishers pp.227-236.2001 137 


Proceedings of the First International Conference on Machme Learning and Cybernetics Beijing 4-5 November 2002 7 Jitender Deogun Vijay V Raghavan Heayri Sever Association Mining and Formal Concept Analysis In Anita W San C Eds Roc of the RSDMGRC\22298 Duke Elsevier Science Publishers 1998 Gregor Snelting Reengineer of Configuration Based on Mathematical Concept Analysis ACM Transactions on software Engineering and methodology 5\(2 pp146 189,1996 8 138 


Figure 6c shows the plot of the outputs of the nonlinear unit z  tanh\(wT v The nonlinear unit is capable of learning a projection vector w that provides good data clustering For comparison Figure 6d shows the plot of the outputs of the nonlinear unit with variance scaling but without the linear PCA preprocessing z  tanh\(w i XI  3 The result demonstrates the necessity of having the linear network removes all linear structure More feeding the data into nonlinear units Qi y xm 2 1 p xw up 2 1 0 1 2 1 0.5 0 0.5 1 wmpmqm d 1 0.5 0 0.5 1 Figure 6 Data clustering using projections to a the first principal component of the linear PCA network b the second principal component of the linear PCA network c the first principal component of the nonlinear unit with a linear PCA preprocessing and d the first principal component of the nonlinear unit without a linear PCA preprocessing 5 Conclusion We have demonstrated the behavior of nonlinear units by employing a probability integral transformation interpretation of the unit activation function This approach leads to a better understanding of the learning caphlities of the nonlinear units Two-layer networks with linear units in the hidden layer are proposed to enhance the caplnlity of nonlinear units The hidden layer extracts all linear structure in the input data and allows the nonlinear units to exclusively manipulate the nonlinear information The approach also suggests and justifies potential applications of Hebbian learning in nonlinear units for signal extraction and data clustering Acknowledgments The authors would like to thank Paul Watta for his critical reading of this manuscript References Hassoun M H Sudjianto A and Mortazavian H 1993 On the Stability of Hebb's Rule submitted to Neural Networks Johnson N L and Kotz S 1970 Distribution in Statistics Continuous Univariate Distributions-2 John Wiley New York Karhunen J and Joutsensalo, J 1992 Nonlinear Hebbian Algorithm for Sinusoidal Frequency Estimation Artrficial Neural Networks 2 Prm ICANN-92 Brighton UK Aleksander I and Taylor, J eds North-Holland 1099-1 102 Qa E 1982 A Simplified Neuron Model as a Principal Component Analyzer Journal of Mathematical Biology 15,267-273 Oja E and Karhunen J 1993 Nonlinear PCA Algorithms and Applications Report A18 Helsinki University of Technology Espoo Finland Oja E Ogawa H and Wangviwattam J 1991 Learning in Nonlinear Constrained Hebbian Networks Arti9cial Neural Networks Proc IC4 91 Espoo Finland Kohonen T Makisara K Simula O and Kangas J eds North-Holland 3 85-390 Rohatgi V K 1984 Statistical Znference John Wiley  Sons New York Sanger D T 1989 Optimal Unsupervised Learning in a Single-layer Linear Feed-fomd Neural Network Neural Networks 2,459-473 Shapiro J L and Prugel-Bennett A 1992 Unsupervised Hebbian Learning and the Shape of the Neuron Activation Function Artificial Neural Network 2 Proc ICANN-92 Brighton UK Aleksander I and Taylor J eds North-Holland 1099-1102 Sow W R and Kammen D M 1991 Correlations in Illgh Dimensional or Asymmetric Data Sets Hebbian Neuronal Processing Neural Networks 4,337-347 Taylor J G and Coombes S 1993 Learning Higher Order Correlations Neural Networh 6 423-427 1252 


because they will not appear in any large itemset in the later iteration At any k-th iteration some items in db or DB which is not needed for finding large itemsets in the next iteration can be identified and hence removed At any k-th iteration during the scan in the incre ment db while FUP is counting the support for sets in the candidate sets C and W for each transaction TI the Reduce-db function is called It counts for each I E TI the number of sets in C and W which contain I This number gives an upper bound on the num ber of large k-itemsets that contain I If this number is smaller than k then I cannot belong to any large k+l and hence can be removed from all the transactions Using this number Reduce-db can prune off some items from db After the set C has been pruned against db it can be seen that any items in DB which does not belong to any set in Lk or C will not belong to any large IC  l-itemset Therefore in the scanning of DB to compute the supports of sets in C all items that do not belong to any set in Lk or C can be removed In the FUP algorithm, the function Reduce-DB performs this reduction In FUP we have also integrated the direct hashing technique in 9 which further reduces the number of the candidate sets used in iteration two 4 Performance Study In order to assess the performance of FUP experi ments are conducted to compare its performance with that of Apriori and DMP The experiments were per formed on an AIX system on an RS/6000 workstation with model 410 As will be presented in the follow ing the result shows that FUP is much faster than the most successful mining algorithm with respect to updating association rules FUP performs 2 to 6 times faster than DHP for a moderate size database of 100,000 transactions When the database is scaled up to 1,000,000 transactions the speed-up is 2 to 16 times As explained before the key of the speed-up lies on the much smaller amount of candidate sets In some cases the number of candidate sets generated were counted, and it was found that the amount gen erated in FUP is reduced to the range of 1.5  5 of that in DHP This is a very significant reduction As mentioned above we also tested FUP with some very large databases It was found that FUP actually performs much better in larger databases 4.1 Generation of synthetic data The databases used in our experiments are syn thetic data generated using the same technique intro duced in l and modified in 9 The parameters used ILI Number of transactions in database DB Number of transactions in the increment d Mean size of the transactions Mean size of the maximal potentially large itemsets Number of potentially large itemsets Number of items Table 1 Parameter Table TlO.l4.DlOO.dl 3 8.00 1 1 a E 6.00 E 4.00 0 z 2*oo 3 0.00 6.00 4.00 2.00 1.00 0.75 W Minimum support DHPFUP  pri01-i Figure 2 Performance Ratio are similar to those in 9 except that the size of the increment is an additional parameter Table 1 is a list of the parameters used in our synthetic database In the following we use the notation Tx.Iy.Dm.dn modified from the one used in 9 to denote a database in which D  m thousands d  n thousands IT1  x and 111  y In our experiments we set ILI  2000 N  1000 and the secondary parameters S  5 P  50 and Mj  2000 S is the clustering size used in the generation of potential large itemsets P is the pool size to store potential large itemsets from which transactions will receive their items Mj is the multiplying factor associated with the pool Readers not familiar with these parameters please refer to l The way we create our increment is a straight for ward extension of the technique used to synthesize the database In order to do comparison on a database of size D with an increment of size d A database of size D  d is first generated and then the first D transactions are stored in the database DB and the remaining d transactions is stored in the increment db Since all the transactions are generated from the same statistical pattern, it models very well real life 91 112 


11 0.14.01 0O.dl 5 0.06 a 0.05 f 0.04 Z 8 0.03 C 0 0 5 0.02 g 8 0.01 K 0.00 6.00% 4.00 2.00 1.00 0.75 Minimum Support 0 FUP/DHP FUP/APRIORI i3 E 3 Q 2.5 e 2 1.5 4 15K 25K 75K 125K 175K 250K 350K In creme n t Size Figure 3 Reduction on Candidate Sets Figure 4 Speed Up Ratio vs Increment Size updates 4.2 We have compared the performance of FUP against that of DHP and Apriori The first comparison was done on an updated database T10.14.DlOO.dl The performance ratios between them are shown in Fig ure 2 In our implementation of the DHP a hash table of size 100 is used and hashing is only used in the generation of the size-2 candidate sets This is the same policy used in 9 For small support FUP is 3 to 6 times faster than DHP and 3 to 7 times faster than Apriori For larger support, it is less costly to re-run the mining algorithm on the updated database since the number of large itemsets is relatively smaller In terestingly FUP is still 2 to 3 times faster in this case 4.3 Reduction on the number of candi As explained before FUP substantially reduces the number of candidate sets generated The effect is par ticularly significant at the first iteration In Figure 3 the chart shows the ratio of the number of candidate sets generated by FUP when comparing with the two mining algorithms The amount of reduction ranges from 98 to 95% when FUP is compared to DHP It is even greater when it is compared with Apriori 4.4 Performance of FUP with large incre ment In general the larger the increment is the longer it would take to do the update Also the gain in speed up would slow down Two sets of experiments have been performed to support this analysis A database T10.14.DlOO.dmwith updates of lK 5K and 10K were generated and different updates with different sup ports were done by FUP and DHP For the same sup FUP versus DHP and Apriori date sets port the speed-up ratio decreases when update size increases For example when the support is 2 the ratio decreases from 5.8 to 3.7 We also want to find out whether the decreas ing of the performance ratio as the size increases in the update would eventually bring the performance of FUP down to that of DHP In the same setting of T10.14.DlOO.dm we increase the increment size m from 10K gradually to 350K for comparison The per formance ratio is plotted in Figure 4 A gradually level off only appears when the increment size is about 3.5 times the size of the original database The fact that FUP still exhibits performance gain when the incre ment is much larger than the original database shows that it is very efficient 4.5 Small overhead of FUP We also have done some experiments for the pur pose of analyzing the overhead incurred by the FUP In general if the time to compute the set L\222 from an updated database DB U db is added to the time to compute the original set L of large itemsets from the database DB by a mining algorithm the sum would be larger than that if the same mining algorithm was applied directly on DBUdb to compute L\222 The differ ence of these two time values is a measurement of the overhead of the update If the overhead is small, then it indicates that the update was done very efficiently We have designed some experiments to analyze the overhead of FUP by measuring this difference It was found that the bigger the increment is the smaller this overhead becomes In our experiment what was dis covered is that when the increment is much smaller than the original database, the 0verhea.d percentage 113 


ranges around 10  15 Once the increment is larger than the original size the overhead decreases very rapidly from 10 to 5 This is a very encouraging result because it shows that FUP not only can benefit update with small increment it actually works very well in the case of large increment 4.6 Performance in scaled-up databases Our last experiment is done in a scaled-up database The database is T10.I4.D1000.d10 which contains 1 million transactions The performance ratio between DHP and FUP in this scaled-up database ranges from 3 to 16 The result shows that the gain from FUP will in fact increase if the database becomes larger This shows that FUP is very adaptive to size increase and can be applied to very large databases 5 Discussion and Conclusions We studied an efficient fast incremental updating technique for maintenance of the association rules dis covered by database mining The developed method strives to determine the promising itemsets and hope less itemsets in the incremental portion and reduce the size of the candidate set to be searched against the original large database The method is implemented and its performance is studied and compared with the best algorithms for mining association rules studied so far The study shows that the proposed incremen tal updating technique has superior performance on database updates in comparison with direct mining from an updated database The incremental updating technique is applicable to the databases which allow frequent or occasional updates when new transaction data are added to a transaction database We have also investigated the cases of deletion and modification of a transaction database Recently there have been some interesting stud ies at finding multiple-level or generalized association rules in large transaction databases 6 111 The exten sion of our incremental updating technique for mainte nance of multiple-level or generalized association rules in transaction databases is an interesting topic for fu ture research References R Agrawal T Imielinski and A Swami Mining Association Rules between Sets of Items in Large Databases In Proc 1993 ACM-SIGMOD Int Conf Management of Data 207-216 May 1993 R Agrawal and R Srikant Fast algorithms for mining association rules. In Proc 1994 Int Conf Very Large Data Bases pages 487-499 Santiago Chile September 1994 D.W Cheung A W.-C Fu and J Han Knowledge discovery in databases A rule-based attribute-oriented approach In Proc 1994 Int 2221 Symp on Methodologies for Intelligent Systems pages 164-173 Charlotte North Carolina Octo ber 1994 U M Fayyad G Piatetsky-Shapiro P Smyth and R Uthurusamy Advances in Knowledge Dis covery and Data Mining AAAI/MIT Press 1995 J Han Y Cai and N Cercone Data driven discovery of quantitative rules in relational databases IEEE Trans. Knowledge and Data En gineering 5:29-40 1993 J Han and Y Fu Discovery of multiple-level association rules from large databases In Proc 1995 Int Conf Very Large Data Bases Zurich Switzerland Sept 1995 M Klemettinen H Mannila P Ronkainen H. Toivonen and A I Verkamo Finding inter esting rules from large sets of discovered associa tion rules In Proc 3rd Int\222I Conf on Informa tion and Knowledge Management pages 401-408 Gaithersburg Maryland Nov 1994 R Ng and J Han Efficient and effective cluster ing method for spatial data mining In Proc 1994 Int Conf Very Large Data Bases pages 144-155 Santiago Chile September 1994 J.S Park M.S Chen and P.S Yu An effec tive hash-based algorithm for mining association rules In Proc 1995 ACM-SIGMOD Int Conf Management of Daia San Jose CA May 1995 G Piatetsky-Shapiro and W J Frawley Knowl edge Discovery in Ratabases AAAI/MIT Press 1991 R Srikant and R Agrawal Mining generalized association rules In Proc 1995 Int Conf Very Large Data Bases Zurich Switzerland Sept 1995 114 


The disadvantage of this rule-oriented control strategy is that it imposes a restriction on the mixing of forward and backward chaining rules such that a forward chaining rule cannot read any data written by backward chaining rules STO87 To describe this problem let the following be a series of rules Ra to Rd and the resuls REa to REd derived by these rules Ra Rb Rc Rd DB  R  REb  R  REd Also let Ra and Rb be defined as backward chaining rules and Rc and Rd as forward chaining rules If the original database DB is updated rules Rc and Rd though they are forward chaining rules will not be triggered to update the result REd until someone requests the data of REb Thus REd may be iriconsistent with the base data To overcome this problem we use a result-oriented control strategy in which we specify for each result derived subdatabase whether it is to be pre-evaluated or post evaluated The same rule may follow the forward or backward chaining strategy depending on whether the derived subdatabae is to be pre or post-evaluated To illustrate by the example above assume that REd is defined as pre-evaluated and REb is defined as post evaluated Whenever the database DB is updated the rules Ra Rb Rc and Rd will be triggered in the forward chaining fashion to keep REM which is explicitly stored up-to-date REb on the other hand will be evaluated whenever a retrieval operation is issued against it In this case the rules Ra and Rb that derive REb are applied in the backward chaining fashion Thus Ra and Rb follow one control strategy when deriving RFxl and the other control straregy when deriving REb This technique offers more flexibility and alleviates the restriction in POSTGRES described above 7 Conclusion In this paper we have introduced the induced generalization association construct and presented a deductive rule-based language for object-oriented databases The world of subdatabases is closed under this language which facilitates defining inference chains in which each rule derives a new subdatabase based on the subdatabases derived by previous rules in the chain The transitive closure operation can be specified in our language in the form of looping rather than in a recursive form A result-oriented control strategy to be used as the underlying implementation technique has also been introduced in this paper ACKNOWLEDGEMENTS Research on the rule-based language was supported by the U.S West Advanced Technologies grant number UPN 88071315 Work on the Object-Oriented Query Language OQL was supported by the Navy Manufacturing Technology Program through the National Institute of Standards and Technology formerly the National Bureau of Standards grant number 60NANB4wO17 and by the National Science Foundation grant number DMC-8814989 The development efforts are supported by the Florida High Technology and Industry Council grant number UPN 85100316 BIBLIOGRAPHY ALA89a A.M Alashqur S.Y.W Su and H Lar OQL A Quy Language for Manipulating Object-onented Datah Accepted for Publication the 15th VLDB Int Con 1989 ALA89b A.M Alashqur A Query Model and Query and Knowledge Definition Langwi~es for Object-oriented Databases a Ph.D BAN87 BAT85 cER86 CHA84 COD79 DEL88 DIT86 HS87 FOR88 GAL84 HAM81 m7 JAR84 LAM89 MAI88 RAS88 STO87 SU89 TY88 U85 VAS84 Thesis Univedty if Florida 1989 Jay Banerjee et al Data Model Issues for Object-Oriented Aplications ACM Trans on Ofice Information Systems January 1987 D Batory and W Kim Modeling Concepts for VLSI CAD objects ACM TODS September 1985, pages 322-346 Stefan0 Ceri George Gottlob and Gio Wiederhold Interfacing Relational Databases and Prolog Efficiently Roc of the 1st Intl Con on Expert Database Systems 1986 C L Chang and A Walker PROSQL a PROLOG Programming Interface with SQLDS F'mxdngs of the 1st Intl Workshop on Expert Database Systems 1984 E Codd Extend~ng the Database Relational Model to Capture More Meaning ACM TODS Vol 4 No 4 1979 Lois ML Delcambre and James N Etheredge A self Controlling Interpreter for the relational Production Language Roceedings of ACM SIGMOD Conference on Management of Data 1988, pages 396403 KR Dimich Object-oriented Database Systems the Notion and Issues Roc of rhe Intl Workshop on Object-Oriented Database Systems califomia September 1986 D.H Fishman et al Iris An Object-Oriented Database Management System ACM Transaction on Oftixe Informarion Systems January 1987 Pages 4869 S Ford et al Zeitgeist Database support for object-oriented rogramming in the F  gs of the Second International Workshq on Object-oriented Database Systems 1988 Heme Gallaire Jack Mier and Jean-Marie Nicolas Logic and Databases A Deductive Approach ACM Computing Surveys June 1984 Pages 153-185 M Hammer and D McLeod Database Description with SDM A Semantic Associon Model ACM TODS Sepember 1981 R Hull and R King Semantic Database Modeling Survey Applications and Research Issues ACM Computing Surveys September 1987 Mauhias Jark Jim Clifford and Yannis Vassiliou An Optimizing hlog Front-End to a Relational Query System Roc of ACM SIGMOD Con on Management of Data 1984 H.M Lam S Su and A.M Alashqur Integrating the Concepts and Techniqws of Semantic Data Modeling and the Objectdented wradigm Roc of the 13th Intl Computex Software and ApptiCationS Conference COMSAC 89 1989 Christcphe de Maindreville and Eric Simon A Production Rule Based Approach to Deductive Databases Roc of the 4th Intl Con on Data Engineering California 1988 L Raschid and S.Y.W Su A Transaction-oriented Mechanism to Control Precessing in a Knowledge Base Management System Pmc of the Intl Con on Expert Database Systems 1988 Michael Stonebraker Eric Hanson and Chin-Heng Hong The Design of the POSTGRES Rules System Roc of the 3rd Intl Con on Data Engineering California 1987 S.Y.W Su V KrishnamurIhy and H Lam An Object oriented Semantic Association Model OsAM appearing in A.I in Indus&l Engineering and Manufacturing Theoretid Issues and Applications S Kumara et al eds American Institute of Industrial Engineering 1989 Frederick Ty G-OQL Graphics Interface to the Object Oriented Query Language OQL Master thesis University of Florida 1988 Jeffrey ullman Implementation of Logical Query Languages for Databases ACM TODS September 1985 Y Vassiliou J Clifford and M Jark Access to Specific Declarative Knowledge by Expert Systems The Impact of hg"'ning Decision Suppat Systems 1 1 1984 67 


 s_suppkey s_nationkey ps_partkey ps_suppkey ps_supplycost p_partkey p_name   l_partkey l_discount l_quantity l_orderkey l_suppkey l_extendedprice o_orderkey o_orderdate n_nationkey n_name p_partkey p_name   246\262 1 2 3 4 5 7 6 8 9 10,#11,#12,#13 14 15 16 17 18 1 2 Figure 11 Execution plan of TPC-D query 9 for transposed files 2 0 20 40 60 80 100 0 50 100 150 200 Time [s CPUusage NetSend NetRecv Disk 10 8 6 4 0 Throughput [MB/s CPU usage 8 9 10 13 Figure 12 Execution trace of TPC-D query 9 with transposed files 11 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


of I/O But the resulting speedup compared with the previous plans exceeds 2 which is quite satisfactory Table 3 shows the results of above right-deep rd left-deep ld and transposed file tp methods along with the reported results of other commercial systems for 100GB TPC-D query 9 Because our system lacks the software and maintenance price metrics the overall system price can\220t be determined accurately Hardware components themselves cost less than 0.5M We can observe that our system achieves fairly good performance Above all the execution time with the transposed files is twelve times as short as the most powerful commercial platform These results strongly support the effectiveness of the commodity PC based massively parallel relational database servers  System Exec Time Price Teradata on NCR 5100M 160 000 133MHz Pentium 20GB Main Memory 400 Disk Drives 953.3 17M Oracle 7 n DEC AlphaServer 8400 12 000 437MHz DECchip 21164 24GB Main Memory 84 Disk Drives 1884.9 1.3M Oracle 7 n SUN UE6000 24 000 167MHz UltraSPARC 5.3GB Main Memory 300 Disk Drives 2639.3 2.1M IBM DB2 PE on RS/6000 SP 306 96 000 112MHz PowerPC 604 24GB Main Memory 96 Disk Drives 2899.4 3.7M Oracle 7 n HP9000 EPS30 12 000 120MHz PA7150 3.75GB Main Memory 320 Disk Drives 7154.8 2.2M Our Pilot System 100 000 200MHz Pentium Pros 6.4GB Main Memory 100 Disk Drives rd 193.7 ld 177.2 tp 77.1 see text Table 3 Execution time of 100 GB TPC-D Q9 on several systems 12 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


4 Data mining 4.1 Association rule mining Data mining which is a recent hot research topic in the database field is a method of discovering useful information such as rules and previously unknown patterns existing behind data items It enables more effective utilization of transaction log data which have been just archived and abandoned Among the major applications of data mining is association rule mining so called 217\217basket analysis.\220\220 Each of the transaction data typically consists of a set of items bought in a transaction By analyzing them one can derive some association rule such as 217\21790 of the customers who buy both A and B also buy C.\220\220 In order to improve the quality of obtained rules a very large amount of transaction data have to be examined requiring quite a long time to complete First we introduce some basic concepts of association rule Let 000 000 001 000 1 001\000 2 001\002\002\002\001\000 000 002 be a set of items and 003 000 001 003 1 001\003 2 001\002\002\002\001\003 001 002 be a set of transactions where each transaction 003 002 is a set of items such that 003 002 004\000  n itemset 004 has support 005 in the transaction set 003 if 005  f transactions in 003 contain 004  here we denote 005 000 005\006\007\007\b\t 003 001 004 002 An association rule is an implication of the form 004 005 n  where 004\001 n 004 000  and 004 006 n 000 007  Each rule has two measures of value support and confidence  The support of the rule 004 005 n is 005\006\007\007\b\t 003 001 004 b n 002  The confidence 013 of the rule 004 005 n in the transaction set 003 means 013 of transactions in 003 that contain 004 also contain n  which can be written as 005\006\007\007\b\t 003 001 004 b n 002 f\005\006\007\007\b\t 003 001 004 002  For example let r 1 000 001 1 001 3 001 4 002  r 2 000 001 1 001 2 001 3 001 5 002  r 3 000 001 2 001 4 002  r 4 000 001 1 001 2 002  r 5 000 001 1 001 3 001 5 002 be the transaction database Let minimum  support and minimum confidence be 60 and 70 respectively First all itemsets that have support above the minimum support called large itemsets  are generated In this case the large itemsets are 001 1 002 001 001 2 002 001 001 3 002 001 001 1 001 3 002  Then for each large itemset 004  n association rule 004 t n 005 n 001 n 004 004 002 is derived if 005\006\007\007\b\t 003 001 004 002 f\005\006\007\007\b\t 003 001 004 t n 002 n minimum confidence  The results are 1 005 3 001 005\006\007\007\b\t 003 000 60 001 b\016\017 000\020\021\016\013\021 000 75 002 and 3 005 1 001 005\006\007\007\b\t 003 000 60 001 013\b\016\017 000\020\021\016\013\021 000 100 002  The most well known algorithm for association rule mining is the Apriori algorithm[1 We have studied several parallel algorithms for mining association based on Apriori One of these algorithms called HPA Hash Partitioned Apriori is discussed here Apriori first generates candidate itemsets and then scans the transaction database to determine whether each of the candidates satisfies the user specified minimum support and minimum confidence Using these results the next candidate itemsets are generated This continues until no itemset satisfies the minimum support and confidence The most naive parallelization of Apriori would copy the candidates over all the processing node and make each processing node scan the transaction database in parallel Although this works fine when the number of candidates is small enough to fit in the local memory of a single processing node memory space utilization efficiency of this method is very poor For large scale data mining the storage required for the candidates exceeds the available memory space of a processing node This causes memory overflow which results in significant performance degradation due to an excessive amount of extra I/Os HPA partitions the candidate itemsets among the processing nodes using a hash function as in the parallel hash join which eliminates broadcasting of all the transaction data and can reduce the comparison workload significantly Hence HPA works much better than the naive parallelization for large scale data mining The 022 th iteration pass 022  f the algorithm is as follows 1 Generate the candidate itemsets Each processing node generates new candidate itemsets from the large itemsets of the last  001 022 t 1 002 th iteration Each of the former itemsets contains 022 items while each of the latter itemsets contains 001 022 t 1 002 items They are called 022 itemsets and 001 022 t 1 002 itemsets respectively The processing node applies the hash function to each of the candidates to determine the destination node ID If the candidate is for the processing node itself it is inserted into the hash table otherwise it is discarded 13 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


 30 40 50 60 70 80 90 100 110 120 30 40 50 60 70 80 90 10 0 Execution Time [s Number of Nodes Figure 13 Execution time of HPA program pass 2 on PC cluster 2 Scan the transaction database and count the support count Each processing node reads the transaction database from its local disk 000 itemsets are generated from that transaction and the same hash function used in phase 1 s applied to each of them Each of the 000 itemsets is sent to certain processing node according the hash value For the itemsets received from the other nodes and those locally generated whose ID equals the node\220s ID the hash table is searched If hit its support count value is incremented 3 Determine the large itemset After reading all the transaction data each processing node can individually determine whether each candidate 000 itemset satisfy user-specified minimum support or not Each processing node sends large 000 itemsets to the coordinator where all the large 000 itemsets are gathered 4 Check the terminal condition If the large 000 itemsets are empty the algorithm terminates Otherwise the coordinator broadcasts large 000 itemsets to all the processing nodes and the algorithm enters the next iteration 4.2 Performance evaluation of HPA algorithm The HPA program explained above is implemented on our PC cluster Each node of the cluster has a transaction data file on its own hard disk Transaction data is produced using data generation program developed by Agrawal designating some parameters such as the number of transaction the number of different items and so on The produced data is divided by the number of nodes and copied to each node\220s hard disk The parameters used in the evaluation is as follows The number of transaction is 5,000,000 the number of different items is 5000 and minimum support is 0.7 The size of the data is about 400MBytes in total The message block size is set to be 16KBytes according to the results of communication characteristics of PC clusters discussed in previous section The disk I/O block size is 64KBytes which seems to be most suitable value for the system Note that the number of candidate itemset in pass 2 s substantially larger than for the other passes which relatively frequently occurs in association rules mining Therefore we have been careful to parallelize the program effectively especially in pass 2 so that unnecessary itemsets to count should not be generated 14 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


The execution time of the HPA program pass 2 is shown in figure 13 as the number of PCs is changed The maximum number of PCs used in this evaluation is 100 Reasonably good speedup is achieved in this application as the number of PCs is increased 5 Conclusion In this paper we presented performance evaluation of parallel database processing on an ATM connected 100 node PC cluster system The latest PCs enabled us to obtain over 110Mbps throughput in point-to-point communication on a 155Mbps ATM network even with the so-called 217\217heavy\220\220 TCP/IP This greatly helped in developing the system in a short period since we were absorbed in fixing many other problems Massively parallel computers now tend to be used in business applications as well as the conventional scientific computation Two major business applications decision support query processing and data mining were picked up and executed on the PC cluster The query processing environment was built using the results of our previous research the super database computer SDC project Performace evaluation results with a query of the standard TPC-D benchmark showed that our system achieved superior performance especially when transposed file organization was employed As for data mining we developed a parallel algorithm for mining association rules and implemented it on the PC cluster By utilizing aggregate memory of the system efficiently the system showed good speedup characteristics as the number of nodes increased The good price/performance ratio makes PC clusters very attractive and promising for parallel database processing applications All these facts support the effectiveness of the commodity PC based massively parallel database servers Acknowledgment This project is supported by NEDO New Energy and Industrial Technology Development Organization in Japan Hitachi Ltd technically helped us extensively for ATM related issues References  R Agrawal T Imielinski and A Swami Mining association rules between sets of items in large databases In Proceedings of ACM SIGMOD International Conference on Management of Data  pages 207--216 1993  R Agrawal and R Srikant Fast algorithms for mining association rules In Proceedings of International Conference on Very Large Data Bases  1994  A C Arpaci-Dusseau R H Arpaci-Dusseau D E Culler J M Hellerstein and D A Patterson High-performance sorting on Networks of Workstations In Proceedings of International Conference on Management of Data  pages 243--254 1997  D.S Batory On searching transposed files ACM TODS  4\(4 1979  P.A Boncz W Quak and M.L Kersten Monet and its geographical extensions A novel approach to high performance GIS processing In Proceedings of International Conference on Extending Database Technology  pages 147--166 1996 15 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


 R Carter and J Laroco Commodity clusters Performance comparison between PC\220s and workstations In Proceedings of IEEE International Symposium on High Performance Distributed Computing  pages 292--304 1995  D.J DeWitt and J Gray Parallel database systems  The future of high performance database systems Communications of the ACM  35\(6 1992  J Gray editor The Benchmark Handbook for Database and Transaction Processing Systems  Morgan Kaufmann Publishers 2nd edition 1993  J Heinanen Multiprotocol encapsulation over ATM adaptation layer 5 Technical Report RFC1483 1993  M Kitsuregawa M Nakano and M Takagi Query execution for large relations on Functional Disk System In Proceedings of International Conference on Data Engineering  5th pages 159--167 IEEE 1989  M Kitsuregawa and Y Ogawa Bucket Spreading Parallel Hash:A new parallel hash join method with robustness for data skew in Super Database Computer SDC In Proceedings of International Conference on Very Large Data Bases  16th pages 210--221 1990  M Laubach Classical IP and ARP over ATM Technical Report RFC1577 1994  D.A Schneider and D.J DeWitt Tradeoffs in processing complex join queries via hashing in multiprocessor database machines In Proceedings of International Conference on Very Large Data Bases  16th pages 469--480 1990  T Shintani and M Kitsuregawa Hash based parallel algorithms for mining association rules In Proceedings of IEEE International Conference on Parallel and Distributed Information Systems  pages 19--30 1996  T Sterling D Saverese D.J Becker B Fryxell and K Olson Communication overhead for space science applications on the Beowulf parallel workstaion In Proceedings of International Symposium on High Performance Distributed Computing  pages 23--30 1995  T Tamura M Nakamura M Kitsuregawa and Y Ogawa Implementation and performance evaluation of the parallel relational database server SDC-II In Proceedings of International Conference on Parallel Processing  25th pages I--212--I--221 1996  TPC TPC Benchmark 000\001 D Decision Support Standard Specification Revision 1.1 Transaction Processing Performance Council 1995 16 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


In accordance with 1910.97 and 1910.209 warning signs are required in microwave areas For work involving power line carrier systems this work is to be conducted according to requirements for work on energized lines Comments s APPA objects to the absolute requirement implied by the word ensure regarding exposure to microwave radiation and recommends revision of s l iii to read when an employee works in an area where electromagnetic radiation levels could exceed the levels specified in the radiation protection guide the employer shall institute measures designed to protect employees from accidental exposure to radiation levels greater than those permitted by that guide  I1 an employee must be stationed at the remote end of the rodding operation Before moving an energized cable it must be inspected for defects which might lead to a fault To prevent accidents from working on the wrong cable would require identification of the correct cable when multiple cables are present Would prohibit an employee from working in a manhole with an energized cable with a defect that could lead to a fault However if the cable cannot be deenergized while another cable is out employees may enter the manhole but must protect against failure by some means for example using a ballistics blanket wrapped around cable Requires bonding around opening in metal sheath while working on cable Underaround EIectrical Installations t Comments t This paragraph addresses safety for underground vaults and manholes The following requirements are contained in this section Ladders must be used in manholes and vaults greater than four feet deep and climbing on cables and hangers in these vaults is prohibited Equipment used to lower materials and tools in manholes must be capable of supporting the weight and should be checked for defects before use An employee in a manhole must have an attendant in the immediate vicinity with facilities greater than 250 volts energized An employee working alone is permitted to enter briefly for inspection housekeeping taking readings or similar assuming work could be done safely Duct rods must be inserted in the direction presenting the least hazard to employees and APPA recommends that OSHA rewrite section 7\regarding working with defective cables This rewrite would include the words shall be given a thorough inspection and a determination made as to whether they represent a hazard to personnel or representative of an impending fault As in Subsection \(e EEI proposes the addition of wording to cover training of employees in emergency rescue procedures and for providing and maintaining rescue equipment Substations U This paragraph covers work performed in substations and contains the following requirements Requires that enough space be provided around electrical equipment to allow ready and safe access for operation and maintenance of equipment OSHA's position A2-16 


is that this requirement is sufficiently performance oriented to meet the requirements for old installations according to the 1987 NEW Requires draw-out circuit breakers to be inserted and removed while in the open position and that if the design permits the control circuits be rendered inoperative while breakers are being inserted and removed stated in the Rules and requests that existing installations not be required to be modified to meet NESC APPA recommends that Section u 4 i which includes requirements for enclosing electric conductors and equipment to minimize unauthorized access to such equipment be modified to refer to only those areas which are accessible to the public Requires conductive fences around substations to be grounded Power Generation v Addresses guarding of energized parts  Fences screens, partitions or walls This section provides additional requirements and related work practices for power generating plants  Entrances locked or attended Special Conditions w  Warning signs posted  Live parts greater than 150 volts to be guarded or isolated by location or be insulated  Enclosures are to be according to the 1987 NESC Sections llOA and 124A1 and in 1993 NESC  Requires guarding of live parts except during an operation and maintenance function when guards are removed barriers must be installed to prevent employees in the area from contacting exposed live parts Requires employees who do not work regularly at the substation to report their presence Requires information to be communicated to employees during job briefings in accordance with Section \(c of the Rules Comments U APPA and EEI provide comments as follows Both believe that some older substations \(and power plants would not meet NESC as This paragraph proposes special conditions that are encountered during electric power generation, transmission and distribution work including the following Capacitors  Requires individual units in a rack to be short circuited and the rack grounded  Require lines with capacitors connected to be short circuited before being considered deenergized Current transformer secondaries may not be opened while energized and must be bridged if the CT circuit is opened Series street lighting circuits with open circuit voltages greater than 600 volts must be worked in accordance with Section q\or t and the series loop may be opened only after the source transformer is deenergized and isolated or after the loop is bridged to avoid open circuit condition Sufficient artificial light must be provided where insufficient naturals illumination is present to enable employee to work safely A2-17 


US Coast Guard approved personal floatation devices must be supplied and inspected where employees are engaged in work where there is danger of drowning Required employee protection in public work areas to include the following  Warning signs or flags and other traffic control devices  Barricades for additional protection to employees  Barricades around excavated areas  Warning lights at night prominently displayed Lines or equipment which may be sub to backfeed from cogeneration or other sources are to be worked as energized in accordance with the applicable paragraphs of the Rules Comments w APPA submits the following comments regarding this Special Conditions section Recommends that the wording regarding capacitors be modified to include a waiting period for five minutes prior to short circuiting and grounding in accordance with industry standards for discharging of capacitors For series street light circuits, recommends that language be added for bridging to either install a bypass conductor or by placement of grounds so that work occurs between the grounds Recommends modification of the section regarding personal floatation devices to not apply to work sites near fountains decorative ponds swimming pools or other bodies of water on residential and commercial property Definitions x This section of the proposed Rules includes definitions of terms Definitions particularly pertinent to understanding the proposal and which have not previously been included are listed as follows Authorized Employee  an employee to whom the authority and responsibility to perform a specific assignment has been given by the employer who can demonstrate by experience or training the ability to recognize potentially hazardous energy and its potential impact on the work place conditions and who has the knowledge to implement adequate methods and means for the control and isolation of such energy CZearance for Work  Authorization to perform specified work or permission to enter a restricted area Clearance from Hazard  Separation from energized lines or equipment Comments x The following summarizes the changes in some of the definitions which APPA recommends Add to the definition for authorized employee It the authorized employee may be an employee assigned to perform the work or assigned to provide the energy control and isolation function  Recommends that OSHA modify the definition for a line clearance tree trimmer to add the word qualified resulting in the complete designation as a qualified line clearance tree trimmer Recommends that OSHA modify the definition of qualified employee" to remove the word construction from the definition since it is felt that knowledge of construction procedures is beyond the scope of the proposed rule resulting in APPA's new A2-18 I 


wording as follows more knowledgeable in operation and hazards associated with electric power generation transmission and/or distribution equipment Recommends that OSHA add a definition for the word practicable and replace the word feasible with practicable wherever it appears in the proposed regulations and that practicable be further defined as capable of being accomplished by reasonably available and economic means OTHER ISSUES Clothing OSHA requested comments on the advisability of adopting requirements regarding the clothing worn by electric utility industry employees EEI has presented comments which indicates research is underway prior to establishing a standard for clothing to be worn by electric utility employees However EEI's position is that this standard has not developed to the extent that it could be included in the OSHA Rules Both APPA and EEI state that they would support a requirement that employers train employees regarding the proper type of clothing to wear to minimize hazards when working in the vicinity of exposed energized facilities Grandfathering Due to the anticipated cost impact on the utility industry of the proposed Rules requiring that existing installations be brought to the requirements of the proposed Rules both APPA and EEI propose that the final Rules include an omnibus grandfather provision This provision would exempt those selected types of facilities from modification to meet the new rules EEI states that if the grandfathering concept is incorporated that electric utility employees will not be deprived of proper protection They propose that employers be required to provide employees with a level of protection equivalent to that which the standard would require in those instances in which the utility does not choose to modify existing facilities to comply with the final standard Rubber Sleeves OSHA requests comments from the industry on whether it would be advisable to require rubber insulating sleeves when gloves are used on lines or equipment energized at more than a given voltage EEI states its position that utilities should continue to have the option of choosing rubber gloves or gloves and sleeves to protect employees when it is necessary to work closer to energized lines than the distances specified in the clearance tables Preemuting State Laws EEI requests that the final Rules be clear in their preempting state rules applicable to the operation and maintenance work rules for electric power systems. This is especially critical since some states now have existing laws which are more stringent than the proposed OSHA Rules Examples are 1 in California and Pennsylvania where electric utility linemen are prohibited from using rubber gloves to work on lines and equipment energized at more than certain voltages and 2 in California and Connecticut where the live line bare hand method of working on high voltage transmission systems is prohibited One utility Pacific Gas  Electric has obtained a variance from the California OSHA to perform live line bare-hand transmission maintenance work on an experimental basis Coiiflicts Between the Rilles and Part 1926 Subpart V Since many of the work procedures in construction work and operation and maintenance work are similar and difficult to distinguish between EEI requests that the final order be clear in establishing which rule has jurisdiction over such similar work areas A2-19 v 


IMPACTS ON COSTS AND ASSOCIATED BENEFITS In its introduction to the proposed rules OSHA has provided an estimate of the annual cost impact on the electric utility industry for the proposed des of approximately 20.7 million OSHA estimates that compliance with this proposed standard would annually prevent between 24 and 28 fatalities and 2,175 injuries per year The utilities which have responded to this proposed standard through their respective associations have questioned the claims both of the magnitude of the cost involved and the benefit to the industry in preventing fatalities and lost-time injuries Both EEI and APPA feel that the annual cost which OSHA estimates are significantly lower than would be realized in practice Factors which APPA and EEI feel were not properly addressed include the following OSHA has not accurately accounted for cost of potential retroactive impacts including retrofitting and modifying existing installations and equipment OSHA has not consistently implemented performance based provisions in proposed rules  many portions require specific approaches which would require utilities to replace procedures already in place with new procedures Estimates were based on an average size investor-owned utility of 2,800 employees and an average rural cooperative of 56 employees, which are not applicable to many smaller systems such as municipal systems OSHA has not adequately addressed the retraining which would be necessary with modifying long-established industry practices to be in accordance with the OSHA rules EEI claims that OSHA's proposed clearance requirements would not allow the use of established maintenance techniques for maintaining high voltage transmission systems and thus would require new techniques For an example of the cost which is estimated to be experienced as a result of the new Rules one of the EEI member companies has estimated that approximately 20,000 transmission towers would need to be modified to accommodate the required step bolts in the Rules at an estimated cost of 6,200,000 Additionally this same company estimates that the annual cost of retesting live line tools for its estimated 1,000 tools would be 265,000 Additionally, both EEI and APPA question the additional benefits which OSHA claims would result from implementation of the new Rules APPA questions the estimates of preventing an additional 24 to 28 fatalities annually and 2,175 injuries per year in that it fails to account for the fact that the industry has already implemented in large part safety measures which are incorporated in the Rules EEI and APPA also point out that many preventable injuries cannot be eliminated despite work rules enforcement and safety awareness campaigns since many such accidents which result in fatalities are due to employee being trained but not following the employer's training and policies PRESENT STATUS OF RULES According to information received from the OSHA office in February 1993 the final Rules are to be published no later than July 1993 and possibly as soon as March 1993 OSHA closed their receipt of comments in March 1991 and no further changes in the rules are thought possible A2-20 


CONCLUSION The OSHA 1910.269 which proposes to cover electric utility operation and maintenance work rules affects a multitude of working procedures as are summarized in this paper It is not possible at the present time to assess the final structure of the Rules as may be proposed in 1993 or subsequent years Since the comments from the utility associations APPA and EEI were made following the initial release of the proposed OSHA Rules in 1989 a significant amount of time has elapsed where other events have occurred which may affect the form of the final Rules The 1993 NESC went into effect in August 1992 and includes some of the requirements to which the commenters objected For example a significant requirement in the Part 4 of the 1993 NESC requires that rubber gloves be utilized on exposed energized parts of facilities operating at 50 to 300 volts This requirement is in conflict with EEl\222s proposed change to the OSHA Rules which would still allow working such secondary facilities without the use of rubber gloves Electric utilities are advised to review the January 31 1989 proposed operation and maintenance Rules as summarized in this paper and to review their procedures which would be affected by application of the Rules Many of the procedures proposed in the Rules provide valuable guidance in electric utilities\222 operation and maintenance activities Where the cost impact is not significant, it is recommended that utilities consider implementing such procedures in expectation of the Rules being published in the next few months Also it would be appropriate for electric utilities to review the 1993 edition of the NESC since there are portions of the Rules which have resulted in changes in the NESC These changes mainly occur in Part 4 Rules for the Operation of Electric Supply and Communications Lines and Equipment The concerns which the commenters have addressed regarding the cost impact and the resulting benefits experienced as a result of the promulgation of the Rules are real ones and must be addressed in the final Rules As a result this paper cannot present a conclusion regarding the full impact of the Rules The development of such Rules continue to be an ongoing matter and will undoubtedly require later analysis when the final rules are published A2-21 


