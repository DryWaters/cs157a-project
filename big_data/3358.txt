C TC Correlating Tree Patterns for Classiìcation Albrecht Zimmermann Bj orn Bringmann Machine Learning Lab Albert-Ludwigs-University Freiburg Georges-K ohler-Allee 79 79110 Freiburg Germany E-mail  azimmerm,bbringma  informatik.uni-freiburg.de Abstract We present C TC  a new approach to structural classiìcation  It uses the predictive power of tree patterns correlating with the class values combining state-of-the-art tree mining with sophisticated pruning techniques to nd the 
k most discriminative pattern in a dataset In contrast to existing methods C TC uses no heuristics and the only parameters to be chosen by the user are the maximum size of the rule set and a single statistically well founded cut-off value The experiments show that C TC classiìers achieve good accuracies while the induced models are smaller than those of existing approaches facilitating comprehensibility 1 Introduction 
Classiìcation is one of the most important data mining tasks Whereas traditional approaches have focused on at representations using feature vectors or attribute-value representations there has recently been a lot of interest in more expressive representations such as sequences trees and graphs 4 5 1 12 3  M ot i v at i ons for t h i s i n t eres t i ncl ude drug design since molecules can be represented as graphs or sequences Classiìcation of such data paves the way towards drug design on the screen instead of extensive experiments in the lab Regarding documents XML essentially a tree-structured representati 
on is becoming ever more popular Classiìcation in this context allows for more efìcient dealing with large amounts of electronic documents Existing approaches to classify structured data can be categorized into various categories namely propositionalization approaches 5 1 association rule approaches 12 and integrated techniques 11 9 3 The y d i f fer l a r gel y i n the way they derive structural features for discriminating between examples belonging to the different classes They share the need for the user to set parameters inîuencing the feature derivation and the use of heuristics Also the rst 
two approaches typically produce large feature sets making classiìers rather difìcult to interpret In this work we present C TC  an approach situated between the association rule technique and integrated systems It is motivated by recent results on nding correlated patterns allowing to nd the k best  i.e most discriminating features according to a convex maximization criterion such as  2 8 Rather than generatin g the complete set of patterns satisfying a given criterion and post-processing them or searching for good features in a heuristic manner C 
TC computes the set of k best patterns by employing a branchand-bound search Only two parameters have to be speciìed decoupling the success of the classiìer from decisions about parameters inîuencing the search process The paper is organized as follows in Section 2 we describe earlier work on the topic and relate it to our approach in Section 3 we discuss technical aspects of our method and outline our algorithm in Section 4 the experimental evaluation is explained and results are discussed We conclude in Section 5 and point to future work directions 
2 Related work Structural classiìcation has been done with different techniques Firstly there are several propositionalization approaches e.g 5 and  1   W hile details may dif fer  t he basic mechanism in these approaches is to rst mine all patterns that are unexpected according to some measure typically frequency Using these instances are transformed into bitstring representation and classiìers trained These approaches can show excellent performance and have access to the whole spectrum of machine learning techniques but there are also problems The decision which patterns to consider meaningful e.g frequent ones will have an effect on the quality of the model The resulting feature set can be 
very large requiring pruning of some kind Finally interpretation of the resulting model is not easy especially if the classiìer is non-symbolic e.g a SVM A second group of approaches is similar to the associative classiìcation approach Again une xpected patterns 1 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


are mined each of them associated with the class value An example is Zaki et al s X R ULES classiìer Each pattern is considered as predicting its class Usually the resulting rule set has to be post-processed and/or a conîict resolution technique employed As in the propositionalization techniques the choice of mining constraints is not straightforward and the resolution technique can strongly inîuence performance as has been shown e.g in 10 13  Additionally the resulting classiìer often consists of thousands of rules making interpretation by the user again difìcult Finally there exist integrated techniques that do not mine all patterns but generate features during classiìer construction Since structural data can be represented in predicate logic techniques such as F OIL 11 and P ROGOL 9 can be used for the task of structural classiìcation While ILP approaches are elegant and powerful working on large datasets can be too computationally expensive Approaches such as DT-GBI 3 c ons truct the features us ed by graphmining Most integrated approaches have in common that feature induction is done in a heuristic way e.g using beam search The user sets the parameters governing this search such as beam size and maximum number of literals per rule in F OIL and beam size maximum number of specializations per node and possibly minimum frequency in DT-GBI In contrast only two parameters have to be speciìed for C TC  The rst one is the maximum rule size giving the user an intuitive way to decide on the complexity of the resulting model The second one the cut-off value below which to not consider rules interesting anymore is optional By setting this value the user can enforce the signiìcance of included rules By basing it on e.g the p-values for the  2 distribution the user has a well-founded guide-line for choosing this value 3 Methodology In this section we sketch the pattern matching notion used by the C TC approach discuss upper bound calculation the main component of the principled search for the most discriminating pattern and formulate the algorithm itself 3.1 Matching embedded trees Several types of structured data exist such as graphs trees and sequences In this paper we will focus on tree structured data like XML only Thus we need a notion for matching tree structured data We use tree embedding to compare our approach with Zaki et al s technique Due to space constraints we cannot give a formal deìnition here and refer the reader to 12 This particular notion is more exible than simple subtrees and the mining process is still efìcient In general other matching notions see 4 and e v en dif ferent representations could be used with C TC  This includes not only Table 1 A contingency cable c 1 c 2 T y T x T  y T x T  T m  y T n  m   x T  y T  n  x T m n  m n        y c x Threshold f\(k k f\(c f\(l l   x ,y c ,c ä\(x äy y ,y x 0,0 y T c ,c uu ll T x T T T T x äy ,0 T y T T T A Convex function B Convex hull of  s domain Figure 1 Convex function and convex hull of the set of possible  x  T y  T  other notions of matching trees but also graphs sequences etc since the general principles of our approach apply to all domains 3.2 Correlation measures A correlation measure compares the expected frequency of the joint occurence of a pattern and a certain class value to the observed frequency If the resulting value is larger than a given threshold the deviation is considered statistically signiìcant enough to assume a causal relationship between the pattern and the class We organize the observed frequencies in a contingency table cf Table 1 Since x T and y T are sufìcient for calculating the value of a correlation measure on this table we view these measures as real-valued functions on N 2 for the remainder of this paper While we focus on binary class problems a multi-class problem can be tackled by choosing the right measure and training round-robin classiìers Since correlation measures are not anti-\one directed search becomes more difìcult But if they are convex an upper bound for the score of future patterns can be calculated making it possible to prune the search tree 3.3 Convexity and upper bounds Convex functions like  2 and Information Gain see 8 for a proof take their extreme values at the points forming the convex hull of their domain D  Consider the graph of f  x  in Figure 1\(A with D   k l  which also makes those points the convex hull Obviously f  k  and f  l  are locally maximal and f  l  the global maximum Given the current value of the function at f  c   evaluating f at k and l allows to check whether it is possible for any future value of c to put the value of f over the threshold For the two-dimensional case the extreme values are reached at the vertices of the enclosing polygon such as the parallelogram in Figure 1\(B in our case This parallelogram encloses all possible tuples  x  T y  T  that cor2 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


respond to occurence counts of specializations of the current pattern T  The upper bound on a measure   T   is ub   T max    y T y T    x T  y T  0  since  0  0  and  x T y T  represent uninteresting patterns For an indepth discussion of upper bound calculation we refer the reader to 8 13 3.4 The C TC algorithm Given k and  user theC TC algorithm Algorithm 1 constructs an ordered list of at most k best-scoring rules that exceed at least  user  Starting from the most general empty tree pattern all tree patterns are canonically enumerated Since correlation measures are neither monotone nor anti-monotone the upper bound is used to prune i.e only patterns whose upper bound exceeds  user and the k thbest signiìcance score seen so far are specialized Algorithm 1 The C TC algorithm  correlation measure  user cut-offvalue k maximum size of rule set 1 S   2 E NUMERATE K-B EST S UBTREES  S   user   3 return S E NUMERATE K-B EST S UBTREES  S t    1 for all canonical expansion t  of t do 2 if   t     then 3 S  S  t   4 if  S  k then 5 S  S  arg min s  S   s  6  min s  S   s  7 if ub   t     then 8 E NUMERATE K-B EST S UBTREES  S,t    Once the k best patterns have been found each pattern is treated as a rule predicting the more frequent class among the training instances covered by it In case of a tie the majority class in the dataset is predicted 3.5 Classiìcation strategies The computed ruleset can be used in different ways in the classiìcation process A rst simple strategy often refered to as decision list DL uses the rst rule from the ordered ruleset matching an instance for classiìcation A second strategy called majority vote MV collects all rules and in the least complex version predicts the class that is predicted by the majority of the matching rules Since it is counter-intuitive that rules of different strength are given the same weight we also evaluate two discounting strategies from the eld of associative patterns 6 12  O ne i s t h e average strength method AvgStr introduced by Zaki et al in 12 F o r each clas s  the s trength e.g conìdence w  r  t this class of the rules matching the instance is added up and normalized by dividing by the number of rules The class with highest average strength is predicted or the majority class if no class achieves higher than default strength Finally the weighted  2 heuristic WChi introduced by Han et al 6 discounts the  2 value for each rule against the maximum  2 value that rule could have attained In all cases the majority class is predicted if no rule matches the instance to be classiìed 4 Experimental evaluation For the experimental evaluation we compared our approach to Zaki et al s X R ULES 12 and T REE 2 anintegrated approach introduced in 2 b as ed on s i milar principles as C TC ontheXMLdatausedin[12  The XML data used in our experiments are log les from web-site visitors sessions They are separated into three weeks CSLOG1 2 and 3 and each session is classiìed based on whether the visitor came either from an edu domain or from any other domain For the mining process we set k  1000 and  user 3  84  the 90%-p value for the  2 distribution For the comparison with T REE 2 we built decision trees with the same cut-off value In each setting we used one set of data for training and another one for testing Following Zakiês notation CSLOG x y denotes that we trained on set x andtestedonset y  The results are summarized in Table 2 Each column is labeled with the setting and reports the error rate for the classiìer and its complexity Complexity for XR ULES and C TC are given in number of rules for T REE 2 in number of inner nodes The mining step itself gave rise to less than 1000 rules for C TC on each dataset ranging from 497 on CSLOG2 to 981 on CSLOG12 To arrive at the subset of rules used in the classiìer whose size is reported in Table 2 the n best rules mined were evaluated on a validation set half the test set with n ranging from 10 to the maximum number mined with increments of ten A graph showing the effects of using a subset of the rules mined is shown in Figure 2 It is interesting that less than 100 rules cause high error rates and increasing n past 200 decreases the quality of the classiìer again The resulting classiìer was then evaluated on the other half of the test set As expected T REE 2 has the least complex models since it creates features on-demand It is however outperformed by XR ULES and the rst three C TC versions MV,DL,AvgStr except for the rst setting XR ULES  models are two orders of magnitude larger than C TC s while not being signiìcantly better than the rst three of the C TC versions on the 5 level for all settings XR ULES shows better performance than C TC using the Weighted Chi heuristic on the settings CSLOG2-3 and CSLOG12-3 5 Conclusion and future work In this work we presented C TC  a rule-based approach to structural classiìcation Using an optimal branch-andbound search the algorithm nds the k most discriminating 3 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


 0.165 0.17 0.175 0.18 0.185 0.19 0 50 100 150 200 250 300 350 400 450 500 550 600 Error Rate on Testset Size of Rule Set MV DL   AvgStr   WChi   Figure 2 Error rates for different classiìcation strategies for the CSLOG1-2 setting Table 2 Error rates and complexity for XR ULES different C TC strategies and T REE 2 Setting CSLOG1-2 CSLOG2-3 CSLOG12-3 CSLOG3-1 XR ULES 17.01 28911 15.39 19098 14.70 29098 16.19 31661 C TC MV 16.77 130 16.05 150 15.73 170 16.50 220 C TC DL 16.69 130 16.10 150 15.76 170 16.23 220 C TC AvgStr 16.69 130 16.08 150 15.71 170 16.37 220 C TC WChi 16.99 130 17.17 150 16.47 170 16.47 220 T REE 2 17.53 66 18.09 57 17.42 103 18.69 60 patterns in a data set and uses them for prediction This allows the user to separate the success of the classiìer from decision about the search process unlike in approaches that use heuristics Basing the criterion for inclusion in the rule set on statistically well founded measures rather than arbitrary thresholds whose meaning is somewhat ambiguous gives the user better guidance for selecting this parameter It also alleviates the main problem of the support-conìdence framework namely the generation of very large rule sets that are incomprehensible to the user and possibly include uninformative rules w.r.t classiìcation As the experiments show C TC classiìers are effective while being less complex than existing rule-based approaches By having users supply a parameter restricting the maximal size of the induced rule set we give them the opportunity to build models that they still consider comprehensible Furthermore evaluating the subset of the induced rule set consisting of the l highest-ranking rules on a validation set and selecting the l giving the best results offers a straight-forward way of tuning the classiìerês performance So far we have restricted ourselves to a single representation trees  a single measure and evaluated four possible classiìcation strategies Future work will include evaluating other correlation measures and applying our approach to different representations Finally selecting the subset of rules to actually use in the classiìer is done heuristically so far To base model construction on optimal search seems to be a promising research direction Acknowledgments We would like to thank Mohammed J Zaki for providing the datasets and the XR ULES algorithm Furthermore we would like to thank Andreas Karwath Kristian Kersting and Luc De Raedt for interesting discussions and comments to our work References 1 B  B r i ngmann and A Kar w at h F r equent S M I L E S  I n Lernen Wissensentdeckung und Adaptivit at Workshop GI Fachgruppe Maschinelles Lernen part of LWA 2004  2004  B  B ri ngmann and A Z i mmermann T REE 2 Decision trees for tree structured data to appear in PKDD 05  W  G eamsakul  T  M at suda T  Y oshi da H Mot oda and T Washio Performance evaluation of decision tree graphbased induction In G Grieser Y Tanaka and A Yamamoto editors Discovery Science  pages 128Ö140 Sapporo Japan Oct 2003 Springer 4 P  K ilp el  ainen Tree Matching Problems with Applications to Structured Text Databases  PhD thesis University of Helsinki 1992  S  Kramer  L  D e Raedt and C Helma Molecular feature mining in HIV data In F Provost and R Srikant editors Proc KDD-01  pages 136Ö143 New York Aug 26Ö29 2001 ACM Press  W  L i  J  Han and J P e i CMAR Accurate and ef  c ient classiìcation based on multiple class-association rules In N Cercone T Y Lin and X Wu editors Proceedings of the 2001 IEEE International Conference on Data Mining  pages 369Ö376 San Jos e California USA 2001 IEEE Computer Society 7 B  L iu  W  H su  a n d Y  M a  I n t e g ratin g c lassiìcatio n a n d association rule mining In R Agrawal P E Stolorz and G Piatetsky-Shapiro editors KDD  pages 80Ö86 New York City New York USA Aug 1998 AAAI Press 8 S  M o r ish ita an d J  S ese T r a v ersin g itemset lattices with statistical metric pruning In PODS  pages 226Ö236 Dallas Texas USA May 2000 ACM 9 S  M uggl et on I n v e r s e e nt ai l m ent and PROGOL  New Generation Computing  13\(3&4\:245Ö286 1995 10 S M u tter  M  Hall an d F  F ran k  Usin g c lassiìcatio n t o e v a luate the output of conìdence-based association rule mining In G I Webb and X Yu editors Australian Conference on Artiìcial Intelligence  pages 538Ö549 Cairns Australia Dec 2004 Springer  J R  Qui nl an L ear ni ng l ogi cal de ni t i ons f r o m r el at i ons Machine Learning  5:239Ö266 1990  M J Z a ki and C C Aggarw al X Rules an ef fecti v e s tructural classiìer for XML data In L Getoor T E Senator P Domingos and C Faloutsos editors KDD  pages 316 325 Washington DC USA Aug 2003 ACM 13 A Zimmerman n a n d L De Raed t Co rclass Co rrelated association rule mining for classiìcation In E Suzuki and S Arikawa editors DS 2004  pages 60Ö72 Padova Italy Oct 2004 Springer 4 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


  Proceedings of the F o ur th Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  Guangzhou, 18-21 A ugust 2005 fres h m a n \(1 2   fa ulty 1 9   n d fi nanc e 12  w h er e t h e wei g ht s i n t h e pare nt he se s i n di cat e t h e cont ri b u t i ons o f  t h e m easures t o t h e overal l ranki ng   T a ble 2. Measures to Rank Universities and W e ights in 2002  Meas u r e  M e a n i n g  W e i g h t R 1  Judgm e n t of sc hola r  e xpe rts a nd e n te rprise rs 15 R 2  R 3  R 4  R 5  R 6  N u m b e r of doc tor subje c t s Nu m b er o f m a s t er s u b j ect s  N u m b e r of na tiona l ke y subje c t s N u m b e r of na tiona l ke y la bs or re se a r c h c e n te rs N a tiona l ke y hum a n ism re se a r c h ba se  4 4 2 4 4 6 4 2 4 4 R 7  R 8  R 9  R 10  R 11  R 12  R 13  SC I EI  IST P  CS TP  SSC I A&HCI  Pa pe rs inde xe d by C h ina Soc i e t y Sc ie nc e  5 5 3 7 2 9 2 0 4 2 2 2 1 5 R 14  R 15  Qua lity of Fre s hm e n  Pe rc e n ta ge of postgra dua te  5 9 6 1 R 16  R 17  R 18  R 19  P e r cen t a g e o f t each er s wi t h P h Ds  Nu m b er o f  Acad em i c i a n s  N u m b e r of C h a ngjia ng Sc hola r s Rad i o o f Nu m b er o f t each er an d s t u d e n t  8 0 5 0 4 0 2 0 R 20  R 21  R 22  Am ount of research f ee of full-tim e teacher N u m b e r of books in libra ry  Ar eas o f t each i n g an d r e s e ar ch  6 0 3 0 3 0      T h e 2 2 p e rf orm a nce m easure s pr o duc e 2 2  i n di vi d u a l  ran k i n gs a n d a n  ove ral l ra nki ng  Usi n g t h e m odel of part i a l o r d e r g e n e ralized  roug h set th eory in trodu ced b e fo re we can fin d re d u c t s n d the co re o f t h e or der e d in f o rm ation table. W e  can also i n duce orde ri ng r u l e s o f t h e o r de re d i n fo rm atio n tab l e as fo llo ws y x y x y x rule y x y x y x rule y x y x y x rule y x y x rule o R R o R R o R R o R 002  003 002  003 002  003 002  1      1      1      1  113 6 14 6 14 13 10 4 3 2 1  whi c h i s t h e sa m e as t h e rul e s  m i ned by  usi n g t h e a p pr oac h  in tr odu ced  b y  Sai Y  and Y a o Y  Y  in   3  I n o r d e r t o co m pare t h e m e t hod p r esent e d i n t h i s pa per a n d t h e m e t hod i n  3   we i m pl em ent o u r a p pr oach by M A T L AB  6 0 wi t h  di f feren t nu m b ers o f  un iv ersities in th e o r d e red i n fo rm atio n  t a bl e The com p ari s on graph i s i n t h e fol l o wi ng fi gure     As s h own in the figure above  whe n the r e a r e five  u n i v e rsities i n  th e OIT  th e ap pro ach p r op osed in [3  n e eds  0 25s t o m i ne t h e or de red r u l e  whi l e t h e m e t h o d  p r esent e d in th is p a p e r need s 0  21 9s W ith th e nu mb er of ob j ects in  t h e O I T i m provi ng g r eat l y t h e t i m e con s um pt i on o f t h e form er increas es greatly whi l e the increm ent of tim e consu m p tio n in creases slowly If th ere are 2 0  univ e rsities in th e OIT  th e fo rm er m e th o d need s 3  34 8s t o  retriev e t h e ru les  whi l e t h e ap pr oach i n t h i s  pa per  onl y  nee d s 0  5 9 4 s  just  o n e t o si xt h o f t h e fo rm er The t i m e of t h e a p pr oac h i n  3 i s  m u ch l o n g er t h an t h e m e t hod prese n t e d i n  t h i s pa per  si nc e it am p l y th e card i n ality o f t h e un iv erse greatly wh ich wil l  com p l i cat e t h e probl em great l y  W h i l e t h e m e t hod proposed   Figure 1. Comparison of Ef ficiency of the two methods   in th is pap e r is ef ficien t, esp e cially fo r th e l a r g e nu m b er of u n i v e rsities in th e o r d e red in fo rm atio n tab l e 5 Concl u si ons Th is p a p e r st ud ies ho w to m i n e  ru les i n  or der e d info r m at i on t a bl e vi a  a r o u g h set t h eory  base d m e t h o d  It s m a in tar g et is to find asso ciatio n b e tween  o r d e ri n g of co nd ition  at t r i but es a nd t h at of deci si on attribu t es. C o m p ared  with trad itio n a l roug h set th eo ry t h is p a p e r g e n e ralizes roug h set  un de r t h e c o re rel a t i on of O I T  part i a l or der rel a t i o n   Exp e rim e n t s o n ord e ring  o f  u n i v e rsities sho w t h at m e th od pr o pose d i n t h i s pa per i s o f  hi ghe r e f fi ci ency t h an p r e v i o us work References  1   Y a o, Y.Y. and Sai Y Mi n i ng ord e r i ng  r u les u s ing roug h set th eory [J], B u lletin  of In tern ati o n a l R o ugh  Set Soci et y  5, 99-106, 2001 2  Yao  Y  Y  an d Sai  Y O n m i ni ng or de ri n g  rul e s [R    m a nuscri p t  2001 3  Sai, Y  Ya o Y Y. a n d Z h o n g   N Data A n aly s is an d  Mining i n Ordered Inform at ion Tables A], Proceedings of 2001 IEEE Confe r e n ce on Data M i ning [C  497-504, 2001 4  Gedi ga, G a nd D unt sc h, I R o u gh a p p r o x i m ati on q u a lity rev i sited  J], Artificial In tellig ence 1 3 2   219-234, 2001  5   Paw l ak, Z. R o ugh Classif i catio n J Inter n ation a l Journal of M a n-M achi n e St udi es, 20, 469-483, 1983  6   Paw l ak Z Ro ugh sets [J In tern atio n a l Jo urn a l o f  Co m p u t er an d In fo r m atio n  Scien ces, 11 5 34 1 356  1982   2031 


  Proceedings of the F o ur th Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  Guangzhou, 18-21 A ugust 2005 7  Pawl ak Z  R o ug h Set s  The o ret i cal A s pec t s of R easoni ng about Data [M], Kluwer Academ ic Publishers  B o st on, 1991 8  C ohe n  W W Scha pi re R  E an d Si n g er Y  Lear ni n g  t o o r de r t h i n g s J   Ad va nc es i n Ne u r al I n f o rm at i on Processi ng Sy st em s,10,1998 9 www.netbig.com  EB/OL     2032 


For each un-instantiated variable Xij of ci  Do For each link t = H\(t  Xij =d\(t, Xij interprets ci  Do Compute the score sijk of t according to Eq.\(1 2 If sijkts  Put \(t,sijk Find in each Sij an element \(tij, sij H\(ti1 ti2  H\(tih and the sum of scores 6j sij is maximal Replace the missing value of Xi1, Xi2  Xih in ci by Xi1=d\(ti1,Xi1 ti2,Xi2  Xih =d\(tih,Xih Return D  the updated D Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE Incomplete case di Incomplete case di Does t H\(t  Xj=zjk exist in T Does t H\(t  Xj=zjk exist in T For each missing datum Xj=? in di For each missing datum Xj=? in di Find elements in L1 or L2 that contain {Xj =zjk} and can consistently interpret di Find elements in L1 or L2 that contain {Xj =zjk} and can consistently interpret di Compute score\(t according to Eq.\(1 2 Compute score\(t according to Eq.\(1 2 Does H\(t consistently interpret di Does H\(t consistently interpret di Apply the value to complete the missing data Apply the value to complete the missing data Figure 4. The proposed completing procedure Table  1. \(a b transactional form of the dataset Case X1 X2 X3 X4  TID Items c1 2 1 ? ?  t1 X1=2,X2=1 c2 2 1 2 1  t2 X1=2,X2=1, X3=2, X4=1 c3 2 1 2 1  t3 X1=2,X2=1, X3=2, X4=1 c4 1 2 1 ?  t4 X1=1,X2=2, X3=1 c5 1 2 1 2  t5 X1=1,X2=2, X3=1, X4=2 c6 ? 1 2 1  t6 X2=1, X3=2, X4=1 c7 1 ? 1 ?  t7 X1=1, X3=1 c8 1 2 1 1  t8 X1=1,X2=2, X3=1, X4=1 c9 1 ? 1 ?  t9 X1=1, X3=1 c10 1 2 1 2  t10 X1=1,X2=2, X3=1, X4=2 Table  2. Data associations of D ID Data association supp conf r1 {X3=1  X1=1} 0.60 1.00 r2 {X2=2, X3=1  X1=1} 0.40 1.00 r3 {X2=2  X1=1} 0.40 1.00 r4 {X2=1  X1=2} 0.30 0.75 r5 {X3=2, X4=1  X2=1} 0.30 1.00 r6 {X3=2  X2=1} 0.30 1.00 r7 {X1=2  X2=1} 0.30 1.00 r8 {X4=1  X2=1} 0.30 0.75 r9 {X1=1  X3=1} 0.60 1.00 


r9 {X1=1  X3=1} 0.60 1.00 r10 {X2=2  X3=1} 0.40 1.00 r11 {X1=2,X2=2  X3=1} 0.40 1.00 r12 {X2=1, X4=1  X3=2} 0.30 1.00 r13 {X2=1  X3=2} 0.30 0.75 r14 {X4=1  X3=2} 0.30 0.75 r15 {X2=1, X3=2  X4=1} 0.30 1.00 r16 {X3=2  X4=1} 0.30 1.00 r17 {X2=1  X4=1} 0.30 0.75 5. Experiments In order to test the effectiveness of our method, several experiments are performed and the results are compared with RBE. The datasets, DA and DB, are generated syntactically. In DA, data are randomly generated and the variables in DA have no particular dependency relations. In DB, three variables, X1, X2 and X3, are defined and each variable has three different instantiations, xij, i=1,2,3 and j=1,2,3. Cases in DB are generated \(1 X1=x11  X2=x22}, {X2=x21  X3=x32}, and X3=x33  X1=x13}, and \(2 associations do not hold. The datasets, Monk, TAE and Solaris are from [4]. Table 3 describes the basic information of these datasets. In these datasets, we randomly remove some data from completed cases and test if our method can successfully recover the missing ones. For nm missing data, the accuracy of recovery D is defined as 1- nw/nm if nw data are incorrectly guessed From the experimental results shown in Table 4, our method is more accurate than RBE is Table 3. Description of the test datasets Dataset ID cases variables niv missing ratio source DB -1 15 DB -2 30 3 3 20 DA -1 15 DA-2 100 6 2 20 Syntactic Monk -1 10 Monk -2 415 7 3 20 TAE -1 20 TAE -2 151 5 3 30 Solaris -1 5 Solaris -2 1066 13 4 10 UCI [4 niv: # of instances in each variables \(in average Table 4. Experimental results Our method RBEDataset ID \(s, c, nda s, c, nda DB -1 \(25,65, 6 15,50,8 DB -2 \(25,65, 6 15,50,8 DA -1 \(20,20,350 30,40,179 DA-2 \(20,20,350 30,40,179 Monk -1 \(15,15,115 10,35,181 Monk -2 \(15,15,115 10,35,181 TAE -1 \(10,30,73 5,60,115 TAE -2 \(10,30,73 5,60,115 Solaris-1 \(40,40,2926 60,60,988 Solaris-2 \(40,40,2926 60,60,988 s, c, nda data associations and nda  is the number of data associations obtained 6. Discussions &amp; Conclusion We presented in this paper a new method for completing missing data using data associations. The basic concept is that association rules describe the dependency relationships among data entries in a Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE 


0-7695-2291-2/04 $ 20.00 IEEE dataset and missing data should hold the similar relationships. One of the advantages of using this approach for completing missing data is that data associations can be easily and reasonably obtained Most reasonable datasets include reasonable association rules. Clearly, data associations truly describe the cross-relation among data instantiations more accurately. The score function that we presented in Eq.\(1 2 There may exist other measures, such as [16], that reveal more information for the applicability of data associations. The score function can be further improved. Unfortunately, the accuracy of the proposed method depends on the number of data associations When the threshold of minsupp and minconf are low there will be a large number of data associations being generated, resulting in inefficiency of the completing procedure. The future work includes developing a better informative score function and reducing the number of data associations and determining suitable values of minsupp and minconf. Currently, in this paper, only discrete data are discussed and the data existing in datasets are assumed to be correct and noise-free. We are extending the proposed method to handle non-discrete, noisy, and incorrect datasets 7. References 1] Agrawal, R., and Srikant, R. \(1994  Fast algorithm for mining association rules  in the Proceedings of the International Conference on VLDBases, pp. 487-499 2] Agrawal, R., Imielinksi, T., and Swami, A. \(1993  Dataset mining: a performance perspective  IEEE TKDE vol. 5, no. 6, pp. 914-925 3] Agrawal, R., Srikant, R., and Vu, Q. \(1997  Mining association rules with item constraints  in the Proceedings of the Third International Conference on KDD, Newport Beach, California, pp. 67-73 4] Black, C., Keogh, E., and Merz, C.J. \(1999 repository of machine learning databases, URL http://www.ics.uci.edu/~mlearn/MLRepository.html 5] Buntine, W.L. \(1994  Operations for Learning with Graphical Models  Journal of AI, vol. 2, pp. 159-225 6] Chen, M.S., Han, J., and Yu, P.S. \(1996  Data mining an overview from a database perspective  IEEE TKDE, vol 8, no. 6, pp.866-883 7] Coenen, F., Goulbourne, G., and Leng, P. \(2004  Tree structures for mining association rules  Data Mining and Knowledge Discovery, vol. 8, no. 1, pp.25-51 8] Dempster, A. P., Laird, N.M., and Rubin, D.B. \(1977  Maximum likelihood from incomplete data via the EM algorithm  Journal of the Royal Statistical Society, vol. 39 no. 1, pp.1-38 9] Frick, J.R., and Grabka, M.M. \(2003  Missing Income Information in Panel Data: Incidence, Imputation and its Impact on the Income distribution  in the Proceedings of the Workshop on Item-Non-response and Data Quality in Large Social Surveys, October 9-11 10] Giudici, P., and Castelo, R. \(2003  Improving Markov Chain Monte Carlo model search for data mining   Machine Learning, vol. 50, no. 1-2, pp.127-158 11] Hern  ndez, M.A., and Stolfo, S.J. \(1998  Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem  DMKD, vol. 2, no. 1, pp.9-37 12] Intelligent CAM Systems Laboratory  Methodologies for dealing with the Missing Data   http://www.eng.uc.edu/icams/resources/missing_data.htm 13] Kryszkiewicz, M., \(2000  Probabilistic Approach to Association Rules in Incomplete Databases  Web-Age Information Management, pp. 133-138 14] Kryszkiewicz, M., and Rybinski, H. \(1999 


14] Kryszkiewicz, M., and Rybinski, H. \(1999  Incomplete Database Issues for Representative Association Rules  ISBN: 3-540-65965-X, pp. 583-591 15] Little, R.J.A., and Rubin, D.B. \(2002 analysis with missing data, Wiley, New York, ISBN 0471183865 16] Omiecinski, E.R. \(2003  Alternative interest measures for mining associations in databases  IEEE TKDE vol. 15, no. 1, pp.57-69 17] Piramuthu, S. \(1998  Evaluating feature selection methods for learning in data mining applications  in the Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, vol. 5, pp. 294-301 18] Pyle, D. \(1999 Morgan Kaufmann Publishers, Inc.ISBN:1-55860-529-0 19] Ragel, A., and Cremilleux, B. \(1999  MVC - A preprocessing Method to deal with missing values   knowledge based system, vol. 12, pp.285-291 20] Ramoni, M., and Sebastiani, P. \(2000  Bayesian Inference with Missing Data Using Bound and Collapse   Journal of Computational and Graphical Statistics, vol. 9, no 4, pp. 779-800 21] Ramoni, M., and Sebastiani, P. \(2001  Robust Bayes Classifiers  AI, vol. 125, no. 1-2, pp. 207-224 22] Ramoni, M., and Sebastiani, P. \(2001  Robust Learning with Missing Data  Machine Learning, vol. 45, no 2 , pp. 147-170 23] Scott, R.E. \(1993 Logic and Practice, SAGE Publications, ISBN: 0803941072 24] Zaki, M.J., and Hsiao, C.J. \(2002  CHARM: An efficient algorithm for closed itemset mining  in the Proceedings of the Second SIAM International Conference on Data Mining Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE pre></body></html 


13: else 14: E|i?1| = E|i?1| ? s The backward process in Algorithm 1, generates level-wise every possible subset starting from the borProceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE der de?ned by Edge without getting into equivalence classes which have been already mined \(Line 10 such subset satis?es the constraint then it can be added to the output \(Line 12 reused later to generate new subsets \(Line 14 have a monotone constraint in conjunction, the backward process is stopped whenever the monotone border B+\(Th\(CM Lines 3 and 8 4.3. Closed Constrained Itemsets Miner The two techniques which have been discussed above are independent. We push monotone constraints working on the dataset, and anti-monotone constraints working on the search space. It  s clear that these two can coexist consistently. In Algorithm 2 we merge them in a Closet-like computation obtaining CCIMiner Algorithm 2 CCIMiner Input: X,D |X , C, Edge,MP5, CAM , CM X is a closed itemset D |X is the conditional dataset C is the set of closed itemsets visited so far Edge set of itemsets to be used in the BackwardMining MP5 solution itemsets discovered so far CAM , CM constraints Output: MP5 1: C = C ?X 2: if  CAM \(X 3: Edge = Edge ?X 4: else 5: if CM \(X 6: MP5 = MP5 ?X 7: for all i ? flist\(D |X 8: I = X ? {i} // new itemset avoid duplicates 9: if  Y ? C | I ? Y ? supp\(I Y then 10: D |I= ? // create conditional fp-tree 11: for all t ? D |X do 12: if CM \(X ? t 13: D |I= D |I ?{t |I  reduction 14: for all items i occurring in D |I do 15: if i /? flist\(D |I 16: D |I= D |I \\i // ?-reduction 17: for all j ? flist\(D |I 18: if supD|I \(j I 19: I = I ? {j} // accumulate closure 20: D |I= D |I \\{j 21: CCIMiner\(I,D |I , C,B,MP5, CAM , CM 22: MP5 = Backward-Mining\(Edge,MP5, CAM , CM For the details about FP-Growth and Closet see [10 16]. Here we want to outline three basic steps 1. the recursion is stopped whenever an itemset is found to violate the anti-monotone constraint CAM Line 2 2  and ? reductions are merged in to the computation by pruning every projected conditional FPTree \(as done in FP-Bonsai [7 Lines 11-16 3. the Backward-Mining has to be performed to retrieve closed itemsets of those equivalence classes which have been cut by CAM \(Line 22 5. Experimental Results The aim of our experimentation is to measure performance bene?ts given by our framework, and to quantify the information gained w.r.t. the other lossy approaches 


approaches All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository1, and the constraints were applied on attribute values \(e.g. price gaussian distribution within the range [0, 150000 In order to asses the information loss of the postprocessing approach followed by previous works, in Figure 4\(a lution sets given by two interpretations, i.e. |I2 \\ I1 On both datasets PUMBS and CHESS this di?erence rises up to 105 itemsets, which means about the 30 of the solution space cardinality. It is interesting to observe that the di?erence is larger for medium selective constraints. This seems quite natural since such constraints probably cut a larger number of equivalence classes of frequency In Figure 4\(b built during the mining is reported. On every dataset tested, the number of FP-trees decrease of about four orders of magnitude with the increasing of the selectivity of the constraint. This means that the technique is quite e?ective independently of the dataset Finally, in Figure 4\(c of our algorithm CCIMiner w.r.t. Closet at di?erent selectivity of the constraint. Since the post-processing approach must ?rst compute all closed frequent itemsets, we can consider Closet execution-time as a lowerbound on the post-processing approach performance Recall that CCIMiner exploits both requirements \(satisfying constraints and being closed ing time. This exploitation can give a speed up of about to two orders of magnitude, i.e. from a factor 6 with the dataset CONNECT, to a factor of 500 with the dataset CHESS. Obviously the performance improvements become stronger as the constraint become more selective 1 http://fimi.cs.helsinki.fi/data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Information loss Number of FP-trees generated Run time performance 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 10 5 10 6 m I 2 I 1  PUMSB@29000 CHESS @ 1200 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 6 10 


10 1 10 2 10 3 10 4 10 5 10 6 10 7 m n u m b e r o f fp t re e s PUMSB @ 29000 CHESS @ 1200 CONNECT@11000 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 m e x e c u ti o n  ti m e  s e c  CCI Miner  \(PUMSB @ 29000 closet         \(PUMSB @ 29000 CCI Miner  \(CHESS @ 1200 closet         \(CHESS @ 1200 CCI Miner  \(CONNECT @ 11000 closet         \(CONNECT @ 11000 a b c Figure 4. Experimental results with CAM ? sum\(X.price 6. Conclusions 


6. Conclusions In this paper we have addressed the problem of mining frequent constrained closed patterns from a qualitative point of view. We have shown how previous works in literature overlooked this problem by using a postprocessing approach which is not lossless, in the sense that the whole set of constrained frequent patterns cannot be derived. Thus we have provided an accurate de?nition of constrained closed itemsets w.r.t the conciseness and losslessness of this constrained representation, and we have deeply characterized the computational problem. Finally we have shown how it is possible to quantitative push deep both requirements \(satisfying constraints and being closed process gaining performance bene?ts with the increasing of the constraint selectivity References 1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases In Proceedings ACM SIGMOD, 1993 2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules in LargeDatabases. InProceedings of the 20th VLDB, 1994 3] R. J. Bayardo. E?ciently mining long patterns from databases. In Proceedings of ACM SIGMOD, 1998 4] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Adaptive Constraint Pushing in frequent pattern mining. In Proceedings of 7th PKDD, 2003 5] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi ExAMiner: Optimized level-wise frequent pattern mining withmonotone constraints. InProc. of ICDM, 2003 6] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Exante: Anticipated data reduction in constrained pattern mining. In Proceedings of the 7th PKDD, 2003 7] F. Bonchi and B. Goethals. FP-Bonsai: the art of growing and pruning small fp-trees. In Proc. of the Eighth PAKDD, 2004 8] J. Boulicaut and B. Jeudy. Mining free itemsets under constraints. In International Database Engineering and Applications Symposium \(IDEAS 9] C. Bucila, J. Gehrke, D. Kifer, and W. White DualMiner: A dual-pruning algorithm for itemsets with constraints. In Proc. of the 8th ACM SIGKDD, 2002 10] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings of ACM SIGMOD, 2000 11] L.Jia, R. Pei, and D. Pei. Tough constraint-based frequent closed itemsets mining. In Proc.of the ACM Symposium on Applied computing, 2003 12] H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations: Extended abstract In Proceedings of the 2th ACM KDD, page 189, 1996 13] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang Exploratory mining and pruning optimizations of constrained associations rules. In Proc. of SIGMOD, 1998 14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules In Proceedings of 7th ICDT, 1999 15] J.Pei, J.Han,andL.V.S.Lakshmanan.Mining frequent item sets with convertible constraints. In \(ICDE  01 pages 433  442, 2001 16] J. Pei, J. Han, and R. Mao. CLOSET: An e?cient algorithm formining frequent closed itemsets. InACMSIGMODWorkshop on Research Issues in Data Mining and Knowledge Discovery, 2000 17] J. Pei, J. Han, and J. Wang. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD  03, August 2003 18] L. D. Raedt and S. Kramer. The levelwise version space algorithm and its application to molecular fragment ?nding. In Proc. IJCAI, 2001 


ment ?nding. In Proc. IJCAI, 2001 19] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proceedings ACM SIGKDD, 1997 20] M. J. Zaki and C.-J. Hsiao. Charm: An e?cient algorithm for closed itemsets mining. In 2nd SIAM International Conference on Data Mining, April 2002 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207ñ216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intíl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intíl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





