RUL SET THE WITH GEN PROGRA IN AND ITS TLICATION TO MEDICAL TA ANALYSIS Yasser Hassan Eiichiro Tazaki Department of Control and Systems Engineering Toin University of Yokohama Japan Abstract A methodology for using Rough Set for preference modeling in decision problem is presented in this paper where we will introduce a new approach for deriving knowledge rules from medical database based on Rough Set combined with Genetic Programming. Genetic Programming belongs to the most newly techniques in applications of Artificial Intelligence 
Rough Set Theory which emerged about twenty years ago is nowadays rapidly developing branch of Artificial Intelligence and soft computing At the first glance the two methodologies we talk about have not in common. Rough Set constructs representation of knowledge in terms of attributes semantic decision rules, etc On the contradictory Genetic Programming attempts to automatically create computer programs from a high-level statement of the problem requirements But in spite of these differences it is interesting to try to incorporate both approaches into combined system. The challenge is to get as much as possible from this association 
1 Introduction Knowledge discovery in database called data mining, aims at the disco information from large collection of data 1,2 Rough set methodology for knowledge discovery provides a powerful tool for knowledge discovery from large and incomplete data A number of algorithms and systems have been developed based on the rough set theory for example see 3,4,5,6,7,8,9,10,11,12,13,2 which may induce a set of decision rules from a given data and may use induced rules to classify future examples. Most of them are attempting to find and select the best minimal set of rules 
the use only minimal subset of attributes from the given data The advantage of employing various sources of knowledge and various structures of knowledge in data mining and knowledge discovery implies that new algorithmic method are desirable for hybrid systems in which rough set will be applied along with methods based on the following Fuzzy Set Neural Nets Genetic Algorithms etc 8,2 The main reason for combining different techniques in the hybrid systems is that a single technique is often not appropriate for every domain or dataset Another reason is that such a hybrid system has an advantage over 
a single method approach because the technologies complement each other's shortcomings. Here we will present a new approach to task of incorporating rough set and genetic programming methods into one system for decision or classification support We want not only to be determining the outcomes of new knowledge based on data but also we are interested in analyzing the structure of the model to gain new insight into the problem at hand By model structure we mean the type of the model's different components, how they related to each other and how they can be interpreted Finally we will 
describe application of our approach for mining decision rules on medical data 2 Genetic Rouvh Induction GlU 2.1 Rough Set Theorv Rough set concept which introduced by Pawlak 14 and one of its essential merits is its direct relation to classification problems  151 is founded on the assumption that each object is associated with some information data knowledge Objects that characterized by the same information are said to be indiscernible in the view of available data This induces the indiscernibly relation which is the mathematical base of rough set theory 
Information system IS=\(U,C is used for representing knowledge where U is non-empty finite set of objects called universe and C is non empty finite set of attributes Decision table A=\(U Cu{d is a special case of information system introduced in rough set theory as tool to present data where C is called condition attributes and dE C is called decision attribute Let v be the value set for attribute c then the attribute c can be considered as a map C:U i.e c\(x E C Let XcU 
be a set of objects and BcC be a set of attributes the indiscernibly relation is defined as I B   x y E U x U c\(x  c\(y VCE B Objects x y satisfying the relation I\(B are indiscernible by attributes from B An order pair AS=\(U,I\(B is called an approximation space 385 Seventh Australian and New Zealand Intelligent Informa~on Systems Conference 18-21 November 2001 Perth Western Australia 


According to I\(B we can define two crisp sets  B X and B X called lower and upper approximation of the set of objects X in the approximation space AS as B X x E U IB\(x G X and   B x x E U IB\(X n x  1  X consists of all objects that can be with certainty classified as elemenfs of set X and B X consists of all objects that can be possibly classified as elements of set X The difference BNB\(X  B X  X is called boundary of X which contains all objects that cannot be classified either to X or complement of X The indiscernibly relation I\(B partitions the set U into number of disjoint equivalent classes denote by U/I\(B X2  Xn where X,+XJ for ever   i=l Decision rules can which represent r is a decision table and V=u V,:CE c is a set of values for attributes then the decision rule is a logical form There exist several measure evaluate the decision rule 7,8,2,12 The classification accuracy and coverage of rule r are defined as follows IF CI VI   cn VJ The simple strength of set of rules is defined as for ut E U is a tested object Rul X is the set of all rules for decision class X and MRul X,,u G Rul X is a set of rules that matching tested object for decision class X 2.2 Genetic Rough Induction GRI Genetic programming 16,17,18,19 is one of several problem solving methods based on analogy computation to natural evolution under title evolutionary computations developed by John Koza which automatically creates a computer program from a high-level statement of problem's requirements. Since Genetic programming depends on tree-structure representation we use C4.5 algorithm 20 to convert the data from decision table into tree-structure For our further considerations let us assume that we are given a data in the form of decision table A=\(U Cu{d by running C4.5 to construct number of trees we convert the data into tree representation Each tree T over the decision table A consists of terminal nodes \(classes of decision attribute non terminal nodes the set of attributes C and edges the attribute values V The complete path 2 in the tree T is a sequence s 0 4   v d v,+l where vo is the root of tree v,+l is the terminal one 4  d are the edges If v is the initial then v,+l is the terminal of edge d i=O  m Let h\(s  m be defined as the length of path s if PATH\(T is the set of all complete paths in the tree T then s\PATH\(T is called the depth of tree T A decision rule r is associated with a complete path s over the tree T which is denoted by r  rule\(s The set of paths PATH\(T\gives us a set of rules S where each rule rE S is associated with each path s EPATH\(T Sometimes the rules derived from some paths may have a high error rate or may duplicate rules derived from other paths so the algorithm usually yields fewer rules than the number of paths in the tree Let ST\(A be the set of all trees over a decision table A that are constructed by run C4.5 number of times The set of trees ST\(A represents a population in genetic programming concept and each tree is an individual from this population The number of trees in ST\(A is called the population size M STo\(A is the set of trees over A in generation 0 or initial population, and so ST,\(A is the population at generation i The terminal nodes in each tree from ST\(A\are assigned values from vd so we can say that the classes of decision attribute give here a set called terminal set in genetic programming In the same manner the fimction set is the set of attributes C where each attribute is a function c\(x By applying genetic operators we build a new population from old ones We will define three types of genetic operators crossover operator mutation operator and reproduction operator Crossover operator 18 is a mapping C:ST\(A  ST\(A i.e C\(T T2 TI Ti  where T,,T2 E ST,\(A are called parents and TI Ti E ST,+,\(A are called offspring It operates on two parental trees and creates two new two-offspring consisting of parts of each parent The offspring are inserted into the new population at the next generation ST,+,\(A\These offspring trees are typically of different sizes and shapes than their parents 386 Seventh Australian and New Zealand Intelligent Information Systems Conference 18-21 November 2001 Perth Western Australia 


Mutation operator is a mapping M:ST\(A ST\(A It generates a unique offspring tree T from exist tree T where M\(T T for TE ST,\(A and T E ST,+I\(A It operates on one parental tree and creates one new offspring to be inserted into the new population at the next generation Reproduction operator is a mapping R:ST\(A  ST\(A where it selects one individual T and makes a copy of the tree for inclusion in the next generation of the population I.e R\(T for T E ST\(A To evaluate each individual in the population we covert the tree into set of rules and evaluate it Let define the significant of the set of rules p as 4 _ t=l P n where strength\(X,,uJ is defined in formula 3 but here since we only use training cases to generate rules and testing cases to measure the efficient of the method see the experiments section so we select ut\200 U and make summation over t to all cases in the training examples where n is the number of cases Since we divide by number of cases n so 0 p SI The value p can be called the fitness value of this set of rules that derived from tree T so each individual has a value p called the fitness value of this individual i.e is the fitness value of individual i Depend on the fitness value the genetic operators select the individuals to be processed \(the better the fitness the more likely the individual is to be selected The task is to find all rules such that the significant of set of rules is at least a threshold This is the maximum significant that is found in all previous generations and there is a chance that offspring are fitter than parents Where there is no predefined limit for combinations of attributes in the left-hand side of rule and the right-hand side is not fixed either; this is important so that unexpected rules are not ruled out before the processing start Sometimes the combination of two unimportant attributes may result in a very good model or may two attributes that are found to be important may depend on each other and combining them would not add anything. And here the search space of the rules has exponential size in the number of attributes We mainly achieve two points beside the strength of the set of rules in the fitness measure The number of rules and average rule length This depends on the data we use so we will define the fitness function f as f  al\(strength of rules p  a*\(no. of rules  a3 l/average rule length Uno of rules where a q q and CL are parameters and by controlling these parameters we can control which rules we will get e.g a2 and yqcontrol what we need large or few number of rules as a result 2.3 The algorithm In this subsection we present a classification algorithm based on the techniques described in the previous subsection ln Decision Table Output: Set of rules Process Stepl Read the decision table determine the upper and lower approximation for each class in the decision table determine the thresholds h for classification accuracy and 8 for the coverage Step2 Generate number of trees  M the population size by using C4.5 and running it M times In each run of 04.5 method we change the probability of pruning for tree to get differ one. In the end of run C4.5 method we store all trees as initial population Step3 lteratively perform the following sub-steps until reach the maximum of generation a Evaluate each individual in the population by the following i Convert the tree into set of rules each rule is associated with a complete path ii Remove the duplicated rules iii Compute the classification accuracy and rule coverage for each rule and remove the rule if its accuracy or coverage less than the thresholds h and 8 Use the formula 4 as fitness measure to assign value for each set of rules b Create a new population by applying the following operations i Reproduce an existing individual selected based on its fitness and copy it into the new population ii Create two new individuals from two existing individuals by genetically recombining randomly chosen parts of two existing trees using crossover operation Create new individual from existing one by randomly mutating a randomly chosen part of selected tree using mutation operation Step 4 The best-so-far individual is designated as the result of run i.e the set of rules 3 Experiments To verify the usefulness of the presented methodology, a computational experiment has been performed We report results of experiments on the Medical data Meningitis dataset This data was colleted at the Medical Research Institute Tokyo Medical and Dental University It has 140 cases each of which is described by 38 attributes 19 numerical and 19 categorical The attribute iv iii 3 87 Seventh Australian and New Zealand Intelligent Information Systems Conference 18-21 November 2001 Perth Western Australia 


descriptions exist in Table 1 Some instances of the data are with missing values Ve divided this data into 121 training cases and 19 testing cases. The problem is to find factors important for three decisions: Diagnosis DIAG Detection of bacteria or virus CULT-FIND and Prediction prognosis COURSE The dataset was divided into a training set and testing set and the computations of rules have been done only to training data The results of computations of rules were applied to the classification the objects from the testing dataset To evaluate the classification process we use a measure called classifier\222s error rate 8 which is defined by the ratio of the number of error to the number of all cases We will compare the results of our method with that standard rough set me set we use ROSETTA software to induce the rules from the dataset In the classification process we use the same technique as in C4.5 that the rules are ordered and the first rule that covers a case is taken as the operative one. The default rule rule without conditions that is put in the end of the list of rules comes into play when no other rule covers a case The algorithm requires a set of parameters that have to be manually specified and may have considerable impact on the performance of the algorithm Furthermore, it is desirable to repeat the same process with different sets of parameters We give in details the best set of parameters that we found through the run of our method 36 I I  Neisseria, Strepto, Staphylo Memory-loss N LC Bechet Sinusitis COURSE N P Tb Influenza, Measles Pi El I Varicella Rubella Adeno 31 CULTURE 38 DAR.4-P ABPC+FMOX Amncsia Headche Ataxia DM Hepatits TB N P RISK Grouped 37 I RISK 1 Broncho Myeloma LC-DM I Decision attribute Diagnosis \(DIAG={Bacteria Virus We use 33 attributes \(Personal Information, Present History Physical Examination Laboratory Examination and Therapy and Course and take only group attributes e.g we take attribute COURSE and delete attribute C-COURSE see Table 1 The best set of rules is obtained in generation 2 Table 3 with number of rules fewer and shorter average rule length than which is obtained from C4.5 or standard rough set method The error rate of the set of rules that is obtained from our method is the same as that is obtained from C4.5 method but the rules are obtained from standard rough set method have very high error rate with this data The fitness function here depends on parameters al=0.9 q=O.l a3 0 and O In another run for our method we get from generation 58 the best set of rules, where it has also the same error rate  0.00 but number of rules is 4.00 and average rule length is 1.00 see third row of Table 3 Table 2 shows the run parameters that are used in our experiments where we mainly depend on crossover operator probability rate is 0.8 Two sets of rules that are obtained from our method and C4.5 method are showed in Table 4 Table 2 Run parameters for medical data Decision Diagnosis and Prediction Max generation Po dation size Max de th for trees Crossover rate 0.8 388 Seventh Australian and New Zealand Intelligent Information Systems Conference 18-21 November 2001 Perth Western Australia 


Mutation rate Error I rules 1 Average rule size 1 rate 1 0.1 I c4.5 I 4 I 1.75 I 0.00 I Reproduction rate 0.00 Yo 0.00 Yo 2.00 78.9  Rough Set 0.1 Table 4 Sets of rules are resulted from experiments  with medical data  GRI CELL-POLY 220  Class BACTERIA CT-FIND  Abnormal CELL-MONO  1 2  Class BACTERIA CELL-MONO  1 2 VIRUS  Class  rules Default Class VIRUS Decision attribute Detec Error rate Average rule size cision Diagnosis C4.5 method CELL-POLY 220  Class BACTERIA CT-FIND Abnormal CELL-MONO  1 2 3 Class BACTERIA CELL_POLY<=220 CELL-MONO 12 VIRUS CT-FIND  Normal CELL-POLY 220 VIRUS Default Class: VIRUS on CULT FIND={T  Class  Class I rules Error rate Average rule size Ia.5 I 1 I 6  26.3  GRI I I I I J c4.5 GRI Rough Set medical data decision attribute Detection I GRI I c4.5 5 2.40 21.1 Yo 92 11.00 63.2 Yo AGE<=46 COLD <=7 NAUSEA 9 LOC o ONEST  ACUTE BT 36.1 GCS lo COLD 7 COLD 9 ONEST=CHRONIC AGE 37 COLD 7 Default Class F  attribute Predic  Class F  Class F  Class T  Class T  Class T AGE<=46 COLD <=7 NAUSEA 9 LOC o ONEST  ACUTE BT 36.1  Class F Default Class F ion COURSE In D We use with this data 16 attributes Personal Information Present History Physical Examination and we ignore Laboratory Examination. The rules resulted from our run with new method and C4.5 method are showed in Table 9 The best set of rules is obtained in generation 4 Table 8\with number of rules average rule length and error rate is same as C4.5 method but better than standard rough set method, where from Table 9 we observe that the rules obtained from our method differ than that are obtained from C4.5 method except only one rule is the same The parameters for run are showed in Table 2 as in decision attribute DIAGNOSIS  rr 7 Parameters for run in mi:alii  Decision attribute Detection Max eneration Po ulation size Max de th for trees Crossover rate Mutation rate 5.26 5.26 Yo 14.00 73.7  Rough Set Table 9 Rules resulted from experiments with FOCAL  FEVER 8 SEX M FOCAL  AGE 62 3 Class N 3 89 Seventh Australian and New Zealand Intelligent Information Systems Conference 18-21 November 2001 Perth Western Australia 


3 Class P SEX M AGE 62 LOC 2 BT  38.6 Default Class: N  ClassP  Class P  LOC 2 STIFF  2 AGE 62 FEVER 2 1  Class P FOCAL   ClassN Default Class N Legarding the application we introduced our approach achieves a more accuracy than heuristics of C4.5 and standard rough set theory But C4.5 is faster, however the execution time was not a major tasks involved in its utilization. These results are suggesting that our method can be treated as a promising tool for extracting laws from experimental datasets and its performance is fully comparable with the performance of other classification systems 4 Conclusions On the basis set theory, we have presente ich provides an efficient and for knowledge discovery in database system In the hybrid framework rough set theory and genetic programming are integrated into hybrid system and used cooperatively to generate a-set of rules from database We believe that successful research requires good co-operation between theoreticians and practitioners so we presented in this paper the analysis of structure of the model for our new method and compared standard method for extracting laws from decision table based on rough set theory and that based on C4.5 algorithm with our new method on medical dataset. We observe that the rules extracted by our new method are relatively better predisposed in classification than both C4.5 and standard rough set approaches Reference l Mannila H Inductive Database and Condensed Representations for Data Mining processing of International Logic Programming Symposium 1997 pp 21-30  Ziarako W Ed Rough Sets Fuzzy Sets, and Knowledge Discovery Springer Verlag Berlin 1994  Hassan Y and Tazaki E Knowledge Discovery 221Using Rough Set Combined with Genetic Programming Accepted from JSAI International Workshop on Rough Set Theory and Granular Computing 2001  Dong J.Z Zhong N and Ohsuga S Probabilistic Rough Induction Yamakawa T and Matsumoto G edt Methodologies for the Conception Design and application of SoR Computing and Information/Intelligent Systems LISUKA\22298 World Scientific, 1998,pp 943-946 5 Dong J.Z Zhong N and Ohsuga S Rule Discovery by Probabilistic Rough Induction Japanese Soc Artificial Intelligence, 2000 pp 274 286 6 Garcia A and Shasha D Using Rough Sets to Order Questions Leading to Database Queries In Nagib C Callaos ed Proceedings of the International Conference on Information Systems Analysis and Synthesis ISAS\22296 July 22-26 7 Nakayama H Hattori Y and Ishii, R Rule Extraction based on Rough Set Theory and its Application to Medical data Analysis IEEE SMC\22299 Conference Proceedings 1999 8 Polkowski, L and Showron A Rough sets in Knowledge Discovery, Physica Nerlag, 1998 9 Siromoney A and Inoue K Elementary Sets and Declarative Biases in a Restricted GRS-ILP model Slovene Soc. Informatika24, 2000 pp 125 135 IO Stepaniuk J and Maj M Data Transformation and Rough Sets, Principles of Data Mining and Knowledge Discovery Second European Symposium PKDD\22298, pp 441-9,1998 ll Tsumoto S and Tanaka H Incremental learning of probabilistic rules from clinical databases In Proceedings of the Sixth International Conference Information Processing and Management of Uncertainty in Knowledge-Based Systems IpMU\22296 Granada Spain 1996 pp 1457-1462 I21 Tsumoto S and Tanaka H Discovery of Approximate Medical Knowledge based on Rough Set Model Principles of Data Mining and Knowledge Discovery Second European Symposium PKDD\22298, pp 468-76,1998 13 Ziarako W Discovery Through Rough Set Theory, Communications of the ACM Vol. 42 No I41 Pawlak Z Rough Sets, International Journal of Computer and Information Science Vol 1 1 No  151 Pawlak Z Rough Classification, International Journal of Man-Machine studies 20 1984 pp 469 483 16 Kinnear Jr K.E Advances in Genetic Programming Mit Press, 1994 I71 Koza J.R Genetic Programming 111 Darwinian Invention and Problem Solving San Francisco CA 1999 I81 Dumitrescu D Lazzerini B Jain L.G and Dumitrescu A Evolutionary Computation, CRC Press, 2000 I91 Longdon William B Genetic Programming and Data Structure: Genetic Programming  Data Structure  Automatic Programming Amsterdam Kluwer 1998 20 Quinlan J.R C4.5 Programs for Machine Learning Morgan Kaufinann San Marteo CA 1993 Orlando, USA 1996, pp 555-560 11, 1999 pp 54-57 5 1982, pp 341-356 390 Seventh Australian and New Zealand Intelligent Infomation Systems Conference 18-21 November 2001 Perth Western Australia 


1232.119s 90,146,85 FHUT 185.933 s 20,142,05 HUTMFI PEP 103.492 s 21.582 s 9,150,058 1,331,158 2.904 s Mushroom at 1 support Scaled 1 Ox vertically With Reordering Figure 8 FH+HM 103.323 s 9,150,030 Figures 8 and 9 show the effects of each component of the MAFIA algorithm on the mushroom dataset at 1 minimum support The number of transactions was increased by repeating all transactions in the database by a certain scaling factor We call this form of scaling vertical scaling In this case mushroom was scaled ten times vertically Note that vertical scaling will not change the search space and will only affect the time taken for counting the support of itemsets The components of the algorithm are represented in a cube format where the running times and number of lattice nodes visited during the MAFIA search for all possible combinations are shown The top of the cube shows the time for a simple traversal where the full search space is explored while the bottom of the cube corresponds to all three pruning methods being used Two separate cubes with and without dynamic reordering\rather than one giant cube are presented for readability Note that all pruning components yield some savings in running time but that certain components are more effective than others In particular HUTMFI and FHUT yield very similar results since they use the same type of superset pruning but with different methods of implementation The efficient MFI lookups that HUTMFI uses to check for frequency explain why HUTMFI outperforms FHUT see Section 3 It is also FH+PEP HM+PEP 16.925 s 8.943 s 1,134,863 535,813 Mushroom at 1 support Scaled lox vertically Without Reordering Figure 9 interesting to see that adding FHUT when HUTMFI is already performed yields very little savings i.e from HM to HM+FH or from HM+PEP to ALL the running times do not significantly change HUTMFI checks for the frequency of a node\222s HUT by looking for a frequent superset in the MFI while FHUT will explore the leftmost branch of the subtree rooted at that node Apparently, there are very few cases where a superset of a node\222s HUT is not in the MFI but the HUT is frequent PEP has the biggest effect of the three pruning methods All of the running time of the algorithm occurs at the lower levels of the tree where the border between frequent and infrequent itemsets exists and since PEP is most likely to trim out large sections at the lower levels this pruning yields the greatest results  Dynamically reordering the tail also has dramatic savings cf Figure 8 with Figure 9 It is interesting to note that without PEP dynamic reordering runs nearly an order of magnitude faster than the static ordering while with PEP it is 223only\224 3-5 times faster Since both PEP and reordering remove elements from a node\222s tail it is not surprising that they overlap in their efficacy 5.2 Comparison With Depthproject We tested the algorithm on 223real\224 datasets containing long patterns that have been used in earlier work  1,7 449 


These datasets are publicly available from the UCI Machine Learning Repository http://www.ics.uci.edu/-mlearn/MLRepository html At the lowest supports tested the longest patterns in these databases have over 20 items, making any algorithm that examines all possible subsets of these patterns or a significant portion thereof infeasible This makes the task of finding the patterns computationally intensive despite the small size of the databases For some of the experiments the databases were scaled vertically by Time Comparison on Connect4.data 1000 MAFIA  Depthproject 100 h  U 10  I 1 I 0.1 90 80 70 60 50 40 30 20 10 Min Support  Figure 10 I IlllW wJrlllJ"llsull UII c.lless.ulla 100 MAFIA DP I 60 55 50 45 40 35 30 25 20 Min Support  Figure 12 concatenating copies of the database together This only affects the time counting takes since the bitmaps compressed or not are longer and the search space examined remains constant Figures 10  12 illustrate the results of comparing MAFIA to our implementation of the Depthproject method the state-of-the-art method for finding maximal patterns I The x-axis is the user-specified minimum support, while the y-axis uses a logarithmic scale to show the running time Time Comparison on Mushroom.data 10 MAFIA  Depthproject I 1 h Y cn  i I o 1 0.01 10 8 6 4 2 0 Min Support  Figure 11 scaleup OT r;ness.aata 45 40 35 MAFIA DP 30 rr 25 20 E 15 10 5 0 0 5 10 15 20 25 Scaleup factor Figure 13 Table 1  Reduction Factor of Nodes Considered Due to PEP Pruning 450 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


