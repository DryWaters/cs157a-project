People produce hundreds of millions of microblogs everyday. With its 140-character message, Microblog has yielded an enormous corpus of information, which is noisy but informative in some way. However, previous work with standard 
Abstract 
Civil Transportation Event Extraction from Chinese Microblog  Jiaxi XIONG Department of Information Security and Engineering Shanghai Jiaotong University Shanghai, China Email: jiaxi.xiong@sjtu.edu.cn Yonggang HAO Department of Information Security and Engineering Shanghai Jiaotong University Shanghai, China Email: hg1673@163.com Zheng HUANG Department of Information Security and Engineering Shanghai Jiaotong University Shanghai, China Email: huang-zheng@sjtu.edu.cn  
NLP tools of event extraction performs poorly on Microblog. In this paper, we adopt a series of methods to extract events from Chinese microblogs. In particular, we grab the chatters from Sina Weibo to extract civil transportation information. We eliminate buzz in Weibo, use CRF methods to filter microblogs so as to focus on transportation, and we also use CRF to recognize named entities and to extract events 
 I NTRODUCTION  Social Networking has developed dramatically in recent years. By exploring sites as Facebook and Twitter, Renren and Weibo likewise for Chinese, users publish their thoughts, share their thoughts, comment on othersê, at an unprecedented rate Sina Weibo, launched in august 2009, is an extremely popular online microblog service consisting of more than 500 
KeywordsÑEvent Extraction; Chinese Microblog; NLP; NER CRF 
 
I 
millions of users. Yet the amount of microblogs people produce everyday through Weibo has exceeded 100 million 
1 
 Not surprisingly, the enormity of information presents high opportunities to exploit this corpus of data to obtain real-time information, as well as to make certain prediction. In this case it is quite demanding that we extract events effectively through these microblogs. The first step is to grab microblogs. Rather than Weibo API, here we have chosen python tools Scrapy to fulfill the task Event extraction for Chinese characters is composed of several modules: Word segmenting, Part-of-Speech tagging POS tag\, Named Entity Recognition \(NER\ and Event tagging. Compared with foreign language as English, Chinese phrase doesnêt contain space which serves as natural segmenting tag. Therefore, word segmenting is a thorny point 
P RECISION AND R ECALL OF NER USING M ODEL TRAINED ON THE CORPORA OF L IBERATION DAILY   W E CAN SEE A 30 DECREASE IN F 1  SCORE WHEN WE APPLY THE MODEL ON W EIBO CORPORA  
 
in Chinese event-extraction. We have tried several tools for this task, and eventually we have chosen Language Technology Platform \(LTP Event extraction for Chinese microblog is even harder than traditional text. Previous research in event extraction has focus TABLE I NE Type Corpora Precision Recall F1 
Name Liberation Daily 96.65% 97.86% 96.87 Name Weibo 61.05 62.33 61.85 Toponymy Liberation Daily 92.89 93.36 92.97 
Toponymy Weibo 72.04% 77.35% 75.47 
 mainly on news article. Trained on this genre of text traditional NLP tools performed well and steadily in similar text. Unfortunately, with the development of social media, the performances of these çoff the shelfé tools are weak on microblog corpora, as we can see in Table 1. There are several 
reasons for this Firstly, due to the popularization of the user community of microblog, unlike those news articles written by professional journalist which are strict, structured and conformist, the content of microblog is unstructured, occupied by large amount of casual expressions, and even contain some emoticons like ORZé and 001\000 
 003R\003%\001∂\001\004  003R\003ç\001§\001j\001§\001\001 Secondly, since there is word limit of 140, users tend to minimize their expressions, which makes it difficult to extract whole essential factors of one event from a single microblog. In addition, this kind of microblog is filled with userês personal preference rendering it more confusing to extract event equitably. Thirdly todayês hot issue may well be in tomorrowês landfill, especially when it comes to such vogue case as cyber-style of writing 
Zhenhuan Styleé, originated from a hit drama in early 2012 have once occupied Weibo for several months, yet seldom mentioned after one year. Models trained on these pop elements will soon lose its effec tiveness and become redundant Last but not least, microblog users frequently mention their mundane events in daily life, such as what they eat and where they have been, which are only informative to their close social network relationships. Therefore, these redundancies of information will have to be removed in order to better extract event of general interest After close scrutiny of the existing NER and Event tagging tools, we have chosen CRF++, a simple, customizable, and open source implementation of Conditional Random Fields 1, http://baike.baidu.com/view/2762127.htm 
2013 International Conference on Cloud Computing and Big Data 978-1-4799-2829-3/13 $26.00 © 2013 IEEE DOI 10.1109/CLOUDCOM-ASIA.2013.48 577 
2013 International Conference on Cloud Computing and Big Data 978-1-4799-2830-9/14 $31.00 © 2014 IEEE DOI 10.1109/CLOUDCOM-ASIA.2013.48 577 


3 4 5 
 
A Scrapy 000¸\000 To Virtually log in and grab microblog B LTP 000¸\000 To segment sentence and to POS tag 
  
CRFs\ for segmenting/labeling sequential data. Meanwhile we wrote a series of scripts to eliminate the interference caused by casual expressions in Weibo In this paper, we focus on a specific task: to extract civil transportation information using chatter from Sina Weibo. For one thing, even though there are several Chinese microblog networks, Weibo possess the biggest user community and therefore yield the biggest dataset. For another, we can get a relatively more precise character dataset and avoid certain buzz since we have confined the topic in an isolated circle 000¸\000 civil transportation in this case. In order to focus on this one topic we also use method of CRF to filter the microblogs so that our basic corpora are solely consisting of clean microblogs concerning transportation The rest of the paper is organized as follows. Next we introduce the overview of our microblog processing system and the proposed approaches. From Section 4 to 9, we will describe each procedure of our system respectively. Then in section 10 we describe the testing standards and present our analysis on the result we obtained. We cite related work in section 11, and we conclude in section 12 II S YSTEM O VERVIEW  An overview of the several components of our system for extracting civil transportation information from Weibo is presented in Fig.1 In the first place we virtually log in Sina Weibo, and grab microblogs from certain account who regularly issue contents concerning civil transportation. A raw dataset is obtained in the end  In the second place, we use a series of scripts to remove noise such as emoticons and unrelated links. Next, we introduce method of CRF to filter unrelated topics, and we obtain our basic corpora free of noise  In the third place, microblogs are segmented and POS tagged. We then annotate the data in order to make training set and test set. By adopting the method of CRF again, we obtain a model through training on the dataset Finally, we use this model on the testing set and get the precision and recall rate. We add at last a script to calculate the F score III P ROPOSED A PPROACH  As mentioned above, our system are composed of three modules: python tools Scrapy to grab microblog, LTP tools to segment words and CRF++ to do NER and Event Tag Sina Weibo provides official APIs for grabbing microblogs However, after experimentation and examination, we found that official APIs have several limitations for the number of microblog grabbed, the IP for log in and the authority of grabbing, so that they cannot satisfy our needs in the research So we chose a web crawling tools Scrapy  Fig. 1 There are dozens of Chinese natural language processing tools. To find one with highest efficiency we have chosen three most famous and widely used tools: ICTCLAS Institute of Computing Technology, Chinese Lexical Analysis\, LTP  Language Technology Platform\, and Stanford Word Segmenter  2, http://scrapy.org 3, http://www.ictclas.org 4, http://www.ltp-cloud.com 5, http://nlp.stanford.edu/software/segmenter.shtml 
System Overview Scrapy is a fast high-level screen scraping and web crawling framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing2 
Weibo  Raw corpora Test Set Train Set Grabbing Weibo Filter Se g mentin g POSta g NER Event Tag Annotation Result Model Standardization Basic corpora Display Corpora Expansion 
  
578 
578 


 
Principle G = \(V, E Y= \(Yv\ v V X,  Y P \(Yv|X, Yw,  w? v\ =  p \(Yv|X,  Yw,  w~v 1 Tool 
      
y=argmax p\(Y|X 
 To evaluate the effects of the three tools on Weibo text, we select randomly 100 Chinese microblogs, segment and tag them manually, then we tried the three tools on the same sample. The results are shown in Table 2.  Given the result of comparison, we finally chose LTP as our tools for sentence segmenting and POS tagging. LTP can achieve a relatively high accuracy in NER, so we also use it to do this task. The result of NER by LTP will be listed as a reference for later use Conditional Random Fields \(CRFs\re a class of statistical modeling method often used in pattern recognition and machine learning to make structured prediction. As an ordinary classifier predicts a label for a single sample regardless of the neighboring" samples, a CRF can take into account the context; For example, the linear chain CRF popular in NLP predicts sequences of labels for sequences of input samples Conditional Random Fields can be defined as followed [13    001\000 Let be a graph such that so that Y is indexed by the vertices of G. Then is a conditional random field in case, when conditioned on X, the random variables Yv obey the Markov property with respect to the graph where w~v means that w and v are neighbors in G Here X may range over natural language sentences and Y stand for the label sequence CRF is an undirected graphical model. Its nodes are divided into two disjoint sets X and Y, X stands for the observed variables, Y stands for the output variables. Then the conditional distribution p\(Y|X\ is modeled. The aim of the CRF is to find out the label sequence y 001\031 Y, who maximize the conditional probability for a sequence X  Therefore, Named Entity task can be regarded as a sequence labeling task. Thus CRF can be used to do NER Specifically, CRFs are often applied in shallow parsing named entity recognition and gene finding, among other tasks being an alternative to the related hidden Markov models. In computer vision, CRFs are often used for object recognition and image segmentation CRF++ is a simple, customizable, and open source implementation of  CRFs  for segmenting and labeling sequential data. CRF++ is designed for generic purpose and will be applied to a variety of NLP tasks, such as Named Entity Recognition, Information Extraction and Text Chunking   To use CRF++, a training file, a test file and a template file are necessary. We need to write a specific template file in which we describe which features are used in training and testing. Fig.2 reveals the format of training file and the specification of the signs in a template file IV G RABBING M ICROBLOGS  To successfully grab microblog, our approach consists of two modules The first one is responsible for virtually login. By analysis the URL for first login, we obtain four values: çservertime time of server, çnonceé random number of login, çRSA Pubkeyé RSA public key, and çrsakvé. After encryption, we encapsulate these information along with user name and password into a form demanded by Sina Weibo, post it to server, and then we will receive the response and complete the virtual login. Later we submit request repetitively to crawl microblogs From the grabbed data, we obtain the user account, content of a microblog, time of issue, number of comment, number of forward, etc. In our case, we are only interested in the content of microblogs, while others will become a sort of noise during later analysis. Therefore, our second module analyzes the grabbed html data by using regular expression, solely extracting the content of each microblogs After these operations, we obtain raw corpora of microblogs V S TANDARDDIZING M ICROBLOGS  Obviously, the raw corpora of microblogs contain huge amounts of noise, which are useless in the processing of microblogs, thus to be eliminated in standardization. Our standardization module has functions as follows Removing meaningless signs, such as ç#topic name Emoticon  use r na m e   s o m e w e b l i nks l i ke http://t.cn/z8ne0w2é, etc Discarding microblogs shorter than 7 words Transforming complex Chinese into simple type, in order to avoid word repetition in later analysis We may delete some useful information during the process of this module, but we do reduce the redundancies in later process.  We test the performance of NER in both unstandardized dataset and standardized dataset in order to see the effect of our standardization VI F ILTING M ICROBLOGS  Since we grabbed microblogs from the accounts who frequently issue microblogs concerning transportation, the majority of these microblogs indicate the real-time traffic jam traffic accident, construction information, etc. Not surprisingly most of these accounts, especially for those ordinary users compared with some official accounts who focus on reporting transportation information\, may issue some noisy unrelated microblogs, such as forecasting the weather, saying good night or reporting their location. Fig.2 demonstrates two microblogs from a same Weibo uid. The first one is about traffic condition while the second has nothing to do with transportation 6, http://CRFpp.googlecode.com/svn/trunk/doc/index.html?source=navbar  
TABLE II A CCURACY OF THE THREE TOOLS FOR PROCESSING W EIBO TEXT  
 
6 
ICTCLAS 83.2 75.38 LTP 83.44 74.1 Stanford Seg 81.5  
C CRF 000¸\000 To filter microblogs, to do NER and to tag event 
002 002 002 
579 
579 


  
    
Tag Symbol Implication B-E Begin of Event I-E Internal of Event E-E End of Event O OTHER K-E Keyword of Event D-E Description of Event T-E Time of Event L-E Location of Event 
Example of two microblogs obtained from the same uid ç207073197 the first one is useful transportation information telling which road has a traffic jam in Shanghai, while the second one is just an advertisement of a musical program, that is to say, noise In order to discern those microblogs who really talk about transportation, we first scan them to identify some features. For example, if a microblog contains words like C E VENT TAGS SYMBOLS SET  
 
  Fig. 2 road 030 016\011 traffic jam\t is very likely to have something to do with transportation. On the other hand, if a microblog contains words like 016 weather 001\007 it is most probably noise. In this way, we define the first feature as çWhether this microblog contains the word C road\. If so, we mark a ç1é at the end otherwise we mark a ç0é. Similarly, we define other 6 features At the end, we annotate manually 900 microblogs whether they are about transportation With the seven features and the manually annotation result we train 470 microblogs and obtain a CRFs model. We then use the rest 430 microblogs to test our model. We find out that our model achieves a high precision of 86.95%. With this model, we can easily annotate following grabbed microblogs and filter those noisy topics unrelated to transportation VII S EGMENTING M ICROBLOGS AND POS  T AGGING  LTP is a language platform based on XML presentation Thus our basic corpora in form of TXT will first be transformed into XML. Then we invoke module IRLAS, which is responsible for segmenting and POS tagging In our basic corpora, each microblog is stocked in one line so that LTP can read and process the input file line by line After processing, we got our result file also in format TXT Each line lay one segmented word, along with its POS tag and NER result VIII E XTRACTING N AMED E NTITIES  Our origin dataset consists of 3 columns: segmented word POS Tag, NER by LTP. To extracting named entities, we have to first annotate manually each line with correct named entity tags.  Assuming that the longest named entity is composed of 7 Chinese characters, in the template file, we define the window size as 7. In other words, when we processed each segmented Chinese character, we examined three characters before it itself and three characters after it. We also consider the relations between the three columns In this way, our template contains 70 macro template structures.  By using this template to train our origin dataset we obtain a CRF model that can be used to tag named entities  TABLE III IX E XTRACTING E VENTS  We use our CRF model obtained during the process of NER to tag named entity of another basic dataset \(result of LTP\and get a new dataset consist of 4 columns, of which the first \(segmented words\he second \(POS tag\ and the fourth our NER result\are useful in this module Similarly, we annotate each line manually with event tags Table 3 lists the possible symbols of event tags As for the training template, we adopt seven different templates so as to select the one with best performance In the first one, we just considered the relationship between two interfacing line. The window size is 5. In template 2, since some named entities and event body are longer than five words we adjust the window size to 7. In template 3 we considered the relationship between two columns, that is to say, between words and POS tags, POS tags and NER references, and words and NER references. We also considered relationships between three interfacing lines. Template 4 abandons POS tags and focuses on relationships between words and NER references While template 5 abandons NER references and focus on relationships between words and POS tags. In template 6 we neglect the relations between columns and focus solely on relations between lines in one column. In the last template, we considered all the relationships. There are 70 template structures in this template We obtain 7 CRF models for Event tagging with different performance. We choose the best one amongst as our model to tag event X E XPERIMENT  To prepare for the experiment, we first establish our corpora of experiment. Then we do the segmenting and POS tag. We annotate the dataset, and then we do the NER and Event tagging In order to get real-time transportation information, we first chose 12 Weibo accounts who officially issue microblogs concerning transportation information of Shanghai. We also selected 50 accounts of ordinary users who occasionally report traffic condition when they actually meet some specific traffic condition. After virtually log in Sina Weibo, we encapsulated the uids of these accounts into a list and transmit it to Scrapy as a reference. Then we ran Scrapy and crawled microblogs from 
A Corpora of Experiment 
580 
580 


002 stands for the relative weight of P in R Normally 002 equals 1, we get an F1 score 
P R F1 Unstandardized 73.59% 57.85% 64.99 Standardized 86.33% 66.85% 75.35 Template No Feature Iteration training time\(s Size Model\(kb 1 3508246 109 606.70 22262 2 9742306 111 763.59 69526 3 10113045 113 787.62 72133 4 13263404 124 960.45 106647 5 14446950 76 667.04 113030 6 14335048 112 921.01 112238 7 14901880 117 1011.96 116453 Models P R F-1 F-0.1 1 37.24% 79.05% 50.63% 78.18 2 37.62% 81.45% 51.46% 80.52 3 38.18% 81.67% 52.03% 80.76 4 16.76% 74.38% 27.36% 71.93 5 27.00% 84.80% 40.96% 83.04 6 35.01% 83.56% 49.34% 82.42 7 36.49% 82.01% 50.52% 81.01 000\023\000\021\000\023\000\023\000\010 000\024\000\023\000\021\000\023\000\023\000\010 000\025\000\023\000\021\000\023\000\023\000\010 000\026\000\023\000\021\000\023\000\023\000\010 000\027\000\023\000\021\000\023\000\023\000\010 000\030\000\023\000\021\000\023\000\023\000\010 000\031\000\023\000\021\000\023\000\023\000\010 000\024\000\025\000\026\000\027\000\030\000\031\000\032 0003\000U\000H\000F\000L\000V\000L\000R\000Q 000 
   
B Evaluation Criteria C Result and Analysis of NER D Result and Analysis of Event Tagging 
    
these accounts. Our original dataset of 2000 microblogs were obtained from 1 st Aug 2013 to 7 th Aug 2013 In order to improve and perfect our corpora during the process of NER, we also added corpora obtained from Revolution Daily about transportation from 13 th Aug 2013 to 17 th Aug 2013 To evaluate the performance of CRF++ on named entity recognition and event tag, we use three parameters: P precision\ R \(recall rate\, and F \(F-measure Precision measures the percentage of correct named entities tagged by CRF++ over the total number of named entities tagged by CRF++. Recall measures the percentage of named entities tagged by CRF++ over the total number of named entities in the file. F-measure, a measure that combines precision and recall, is the harmonic mean of precision and recall In equation 2 We use our CRF model described in section 8 to test on the two set of testing data: the unstandardized dataset \(with noise like ç@usernameé\ and the standardized dataset. Their performances are in table 4 An increase of 17.3% of precision can be observed from the NER result, which means by eliminating informal expressions our efficiency of extracting named entities increases Our seven templates produce seven models during the training. Table 5 lists the training result of the seven templates including their number of features, number of iteration, training time and the size of each model file From the result of training in table 5, we can see that as we aggrandize the window size, considering more relations between columns and lines, it will cost longer time to train a model, and the model size will become larger By using these seven template files, we obtain seven models. Table 5 and Fig. 3 illustrate the performance of the seven models. From Fig 4, we can see that for model 1, 2 and 3 as we expand the scope of relationships, both P and R augment which means in certain scope, the more features we consider the better result we will obtain TABLE IV  TABLE V  Fig. 3 
NER  R ESULTS  T RAINING R ESULTS OF SEVEN TEMPLATES FOR E VENT T AGGING  TABLE VI T ESTING R ESULTS OF SEVEN MODELS FOR E VENT T AGGING  Example of input and output data for LTP Model 4 neglect the result of POStag, and it receive the lowest precision rate. In other words, simply considering the relation between words and its NER tags cannot effectively identify its event tag. Compared with model 4, model 5 achieves a better performance in both precision and F-measure which means that POS tags can affect the result of event tagging more deeply than NER tags do As we compare the performance between model 6 and 7 we found that the consideration of tags in one lines will increase the recall rate while reduce the precision. In other words, the relationships between segmented words, POS tags and NER tags as a whole can help improve the recognition of events, but may affect the judgment of event type in a bad way We can see that the performance of model 7 is worse than that of model 3, which means, with more thorough consideration of all the relationships, there will be more limitation in recognition of named entity and event tag, thus reduce the recall rate Overall, we chose model 3 as our CRF model for event tagging 
   002\002 002 002 
2 
rp rp measureF 
 1  
581 
581 


2010 IEEE/WIC/ACM International Conference on  \(Volume:1 Acoustics Speech and Signal Processing, ICASSP-92. USA,1992:633-636 Proceedings of the 5 th SIGHAN Workshop on Chinese Language Processing. Australia, 2006: p173-176 Proceedings of the 18th ACM SIGKDD international Conference on Knowledge Discovery & Data Mining KDD 2012 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing \(EMNLP 2011 In Proc of ICML, pp.282-289, 2001 In Proc. of HLT/NAACL 2003 In ACL,2011 In WWW,2010 IJACSA\ International Journal of Advanced Computer Science and Applications, Vol. 4, No.6, 2013 2011 002 133-138 In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media. 2010  
             
   
Finally, to demonstrate the civil transportation event we extract from the microblog, we designed a script to find out all the segmented words whose event tag is K-E\(keyword of event\, B-E\(begin of event\, L-E\(location of event\ or T-E\(time of event\, according to the tag set in table 3. With these four elements, we can easily confirm the event information. Table 7 illustrates an example of our Event Tag XI R ELATED W ORK  There has been relatively little previous work on building NLP tools for Chinese Microblog. Yet for common named entity recognition and research on English Microblog as Twitter, there are several There has been considerable published result on general Named Entity Recognition. The methods can be divided into three categories: method based on rules, method based on statistics and method based on the mixture of the two. For method based on statistics, there are mainly three models Maximum Entropy, Hidden Markov and Conditional Random Fields.  Della Pietra and Mercer R L were the first to induce maximum entropy model into natural language processing [2 Chen A, Peng F and Shan R built up conditional random fields model and maximum entropy model, and obtained an F value up to 86.2  As for the research on information extraction within microblog, by using distant supervision, Benson et al. trained a relation extractor which identifies artist and venue mentioned in twitter of users who list their location as New York  T  Sa k a ki M O k aza ki a n d Y Ma t s uo d e si gned a system proved to be capable of recognizing almost all earthquakes reported by the Japan Meteorological Agency They realize this system by training a classifier to recognize tweets reporting earthquakes in Japan Ri t t er A a nd Cl ark S have designed a novel part-of-speech tagger module and word segmenting method; they have also introduce entity classification so that the precision of named entity recognition were higher than that of traditional methods [4   XII C ONCLUSION  In this paper we have shown an event extraction system which can extract civil transportation event from microblog text like Weibo. We have used Scrapy to crawl microblogs CRF methods to standardize and filter microblogs, LTP to segment each blog and tag it with POS, and CRF methods again to do named entity recognition and event tag. At last we obtained a model trained on our corpus. By using this model we can tag event from newly grabbed microblogs and extract their event information. The precision of our NER reaches xxx and the precision of event tagging reaches xxx. At last we can obtain from each microblog the keyword of event, the event phrase, its location, and its time The civil management can benefit greatly from our system Event extracted from social media will help the city managers to know what's going on in the city and they can be more prepared for those coming events. For example, city administration can allocate adequate public transportation service for a big event which involves several thousand people Another example is that city administration can allocate enough police force to the place that a crime prone event is going to happen.  In this way, event extraction can make the smart city even smarter XIII A CKNOWLEDGMENT  This material is based upon work 000S\000D\000U\000W\000L\000D\000O\000O\000\\\000\003\000V\000X\000S\000S\000R\000U\000W\000H\000G\000\003 000E\000\\\000\003\000,\000%\0000\000\003\0006\000K\000D\000U\000H\000G\000\003\0008\000Q\000L\000Y\000H\000U\000V\000L\000W\000\\\000\003\0005\000H\000V\000H\000D\000U\000F\000K\000\003\0003\000U\000R\000J\000U\000D\000P\000\021  R EFERENCES  1 
TABLE VII E XAMPLE OF E VENT T AG ABOUT A CIVIL FLOOD IN CERTAIN PLACES  AND CALLS ATTENTION OF ALL THE DRIVERS   A FTER PROCESSING  WE IDENTIFY THE KEY WORD R OAD  EVENT F LOOD  LOCATION OF EVENT C ENTURY A VERNUE  ZHOUHAI  ETC  AND TIME OF EVENT 17:05 Sitaram Asur and Bernardo A.Huberman, Predicting the Future With Social Media. Web Intelligence and Intelligent Agent Technology \(WIIAT  2 Della Pietra S, Della Pietra V, Mercer R L, et al. Adaptive language modeling using minimum discriminant estimation[C   3 Chen A, Peng F, Shan R, et al. Chinese named entity recognition with conditional probabilistic models[C    4 Alan Ritter, Mausam, Oren Etzioni and Sam Clark, Open Domain Event Extraction from Twitter  5 Alan Ritter, Sam Clark, Mausam, and Oren Etzioni, Named Entity Recognition in Tweets: An Experimental Study  6 J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields Probabilistic models for segmenting and labeling sequence data 7 F. Sha and F. Pereira. Shallow parsing with conditional random fields 8 E. Benson, A. Haghighi, and R. Barzilay. Event discovery in social media feeds  9 T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake shakes twitter users real-time event detection by socialsensors   Dr.Rakesh ch. Balabantaray, Suprava Das and Kshirabdhi Tanaya Mishra, Case Study of Named Entity Recognition in Odia Using CRF Tools   WENG Jui-Yu 9 YANG Cheng-Lun 9 CHEN Bo-Nian.IMASS 9 An Intelligent Microblog Analysis and Summarization System[C   Portland 002 Oregon 002 Association for Computational Linguistics 002   Brendan OêConnor, Michel Krieger, David Ahn, TweetMotif Exploratory Search and Topic Summarization for Twitter 
 
Origin Microblog 000\006\025ÇC√\007â\000\006\000\024\000\032\000\035\000\023\000\030 007⁄\002»\011´\034àLº\025E\012°\002»\#\006 K\001√\035<P¨E•\005≈EØ\001√\024Ù\035<E•\004h9Ü\001√\022\031\021ä\0369\001√\003Í4 016˚F'1\035C√!â\033‚\034\022/√"\010\002»\005\010\034›E:EZ\030oJÓ\001ƒAÀEõ\025T PRPJ\012,\021„\025ó\024‘E:\001ƒ\000\003 K-E C√!â\000\003 B-E 010\000\003 L-E 006#K\000\003\035<P¨\000\003\024Ù\035<\000\003\022\031\021ä\0369\000\003\003Í4~\016˚F'\000\003 T-E 000\024\000\032\000\035\000\023\000\030\000\003 
E Demonstration of Events 
582 
582 


spectrum allocation in \(b\ooks totally different from the previous examples  as possible for appropriate tone separation that is as equal space of target spectral lines as possible You may need to perform some trial measurements to find a well distributed spectrum condition In Figures 16 and 17, the blurred waveforms and the eye patterns are successfully refined at the end by the processing described in Section 2 based on Equation \(11   Figure 17 5 Gbps 63-bit PRBS with Jitter 5 kHz 200 ps-pp  in Equation \(4\ So you need to look for as good a number of  Lecture 4.3 INTERNATIONAL TEST CONFERENCE                                        7    Figure 15 contrasts the original blurred PRBS waveform d\The PRBS is dramatically refined. When reshuffling the waveform \(c big open eye pattern is restored as \(e 3.2  Different Conditions Let\222s verify the effect in different conditions. Figure 16 shows the 6 Gbps 127-bit PRBS with a 5 kHz 100 ps-pp jitter. Because of changing the bit rate, the FFT spectrum b\of the measured waveform looks totally different from the case of 7 Gbps depicted in Figure 7 \(a   Figure 16 6 Gbps 127-bit PRBS with Jitter 5 kHz 100 ps-pp  Figure 17 shows the 5 Gbps 63-bit PRBS with 5 kHz 200 ps-pp jitter. Both the bit rate and the PRBS length are changed. So the The sampler used in this work covers bandwidth up to 10 GHz When testing 7 Gbps 127-bit PRBS stream, 181 tones are accommodated in the 10 GHz band. When testing 6 Gbps 127-bit PRBS and 5 Gbps and 63 bits, 212 and 126 tones are accommodated respectively in the band Figure 18 shows a more practical example that is an emulation of SSC applied bit stream. The test signal is the 7 Gbps 31-bit PRBS, which is frequency modulated \(FM with a 31.5 kHz triangle signal. The frequency deviation is programmed as 0.01% of the bit rate that is 700 kHz Figure 18 \(a\ is the primitive measured waveform, which is directly processed by the FFT, disclosing the frequency spectrum as \(b\ince the test signal is generated by using a UHF AWG \(arbitrary waveform generator\, the programmed PRBS waveform is intentionally limited its spectrum into the main lobe only. So there are 31 tones captured in the baseband. The sampling rate is tuned up to make every tone distributed evenly in the baseband. Each tone is numbered from 1 to 31. Each tone is FM modulated The bin #31 corresponds to the bit clock and it is the most deviated of all. Reshuffling the primitive waveform \(a\ by manner of CWR with the key number Nx Nx Nx the two sequences of the PRBS waveform is reconstructed as \(c Since the measurement is gr eatly influenced by the SSC the recovered PRBS waveform looks too smeared to see a clear trace. As discussed in Section 2.4, performing the PM demodulation and further processing, the phase trend is disclosed as \(d\. Then the differential of the phase trend reveals the triangle frequency trend as \(e\ whose spectrum is shown in \(f\ The triangle frequency is 31.5 kHz and the frequency deviation is 700 kHz-pp. As cleaning up the SSC trend according to the pr ocedures discussed in the  b\nd 17 \(b\how, the spectrum allocation looks greatly different in an under-sampling situation. There are lots of possible numbers available for 


Then if you want to increase the tone spacing, reduce the PRBS length. The available tone space determines the recoverable jitter frequency and amplitude or the frequency deviation of SSC  Waveform samplers utilize aliasing to measure UHF signals. In under-sampling situation, jitter higher than half the sampling rate cannot be distinguished from the original existing jitter in the baseband 5. Conclusion Mixed signal ATE integrates a wideband waveform sampler which is a very useful resource to measure UHF  Lecture 4.3 INTERNATIONAL TEST CONFERENCE                                        8    previous sections, the smeared waveform \(c\s refined to the clear trace as \(g it is reshuffled by the key number 31, the big open eye is reconstructed as \(h     Figure 18 7 Gbps 31-bit PRBS with 31.5 kHz Triangle SSC  4 Discussion and Limitation The waveform sampler used in this work has an analog bandwidth \(ABW\ 10 GHz and an actual sampling rate  Fs 110 Msps. All signal tones in the ABW should be captured in the baseband of 55 MHz. When it captures 65536 points N f data, the primitive frequency resolution is Fs  N that is approximately 1.7 kHz. So jitter more than several kHz can be removed appropriately Figure 19 shows a part of Figure 7, illustrating the closeup primitive spectrum. Each tone is scattered by the jitter The tone spacing is approximately 275 kHz wide so that the PM spread spectrum up to +/-138 kHz would not cross over each other. Then jitter up to 138 kHz can be removed in this situation. If you need to remove specific jitter frequency, you could apply a LPF processing in the PM demodulation scheme  Figure 19 Acceptable Modulation Range  The number of bits of a PRBS corresponds to the number of tones contained in the main lobe or a side lobe of its spectrum. Therefore the bit rate of PRBS decides the way of multi-tone spectrum distribution. The ABW of the waveform sampler determines how many tones would be captured in the measurement. If you select an available sampler in an ATE, the ABW and Fs are given. Then the number of bits and the bit rate of the test signal PRBS should be carefully examined when making a test plan Table 2 shows how many tones would be considered in the ABW of 10 GHz. The table says that the lower the bit rate the more tones are captured in the baseband, which means the tone spacing becomes narrower for the fixed baseband width. On the other hand, the shorter the PRBS length, the fewer tones will be in the baseband   Table  2 Number of Tones to Be Accommodated  In general, y ou should use a waveform sampler with as high an actual sampling rate as possible for expanding the baseband width 


 Lecture 4.3 INTERNATIONAL TEST CONFERENCE                                        9    signals including high-speed digital waveforms. Unlike high-end oscilloscopes, ATE usually does not have CDR capability integrated into the hardware. While a waveform sampler performs under-sampling of the test signal, the captured waveform is influenced by slow jitter. The purpose of this work is to remove slow jitter effects such as SSC in a high-speed digital signal and restore a fine waveform and eye pattern. The test signal is a PRBS bit stream. Slow jitter reduction is realized by post processing whose steps are  PRBS becomes extremely wideband multi-tone  Jitter is PM onto each tone  Original tones can be recovered by Equation \(11 which is constructed by the modulation signal e measured signal \(cos  d its orthogonal signal sin    Each modulated tone is extracted as a group and demodulated by the ODM, recovering the modulation signal   The sin  is generated by 90 degree rotation from the cos  The FFT & IFFT method takes care of this processing  Recovered tones are equalized power-wise to their original modulated state  CWR by the factor of Nx reconstructs a PRBS waveform  A second CWR by the factor of the number of bits eg. 127\onstructs an eye pattern Finally the developped processing procedure succeeded to reconstruct clear refined PRBS waveforms and wide open eye patterns. The key in the processing is that Equation 11\ is derived as a relatively simple combination of acceptable processing components. This paper mainly discussed an example of a 7 Gbps 127-bit PRBS signal containing a jitter of 5kHz. The waveform sampler employed in the experiment has an actual sampling rate of 110 Msps and the ABW of 10 GHz. Reduction of slow jitter and SSC is demonstrated in the examples. The multitone spectrum appearance in under-sampling situation greatly depends on the PRBS bit length and the bit rate of the signal. The sampling condition should be carefully chosen for the aliased multi-tone spectra to fall in the baseband with as even spacing as possible to avoid overlapping of the jitter-modulated spectrum with each other. If the Fs and ABW of sampler are given, the higher the PRBS bit rate and the shorter the PRBS bit length, the better for wider tone spacing 6. References 1  Takashi Ito, Hideo Okawara, and Jinlei Liu, \223RNA Advanced Phase Tracking Method for Digital Waveform Reconstruction,\224 Proceedings IEEE Int Test Conference 2012, Paper18.3 2  223V93000 System Reference Manual,\224 Advantest 2013 3  Matthew Mahoney, \223Chapter 4: Coherence and Coherent Sampling,\224 DSP Based Testing of Analog and Mixed-Signal Circuits IEEE Computer Society Press 1987, pp.45-58 4  Hideo Okawara, \223DSP-Based Testing Fundamentals 47: Coherent Waveform Reconstruction,\224 Advantest\222s Go/semi Newsletter December 2012 5  Hideo Okawara, \223DSP-Based Testing Fundamentals 8: Under-sampling,\224 Verigy\222s Go/semi Newsletter  December 2008 6  Hideo Okawara, \223Elegant Construction of SSC Implemented Signal by AWG and Organized Undersampling of Wideband Signal,\224 Proceedings IEEE Int Test Conf 2011, Paper 11.2    


  


Bottom Top A B 
Figure 15 Figure 16 
messages seen for all workers in a superstep \(Figures 10 and 13\. When looking at the messages sent by workers in a superstep for METIS, we see that there are message load imbalances within work ers in a superstep, caused due to concentration of vertices being traversed in that superstep in certain partitions This variability is much more pronounced in CP as compared to WG \(Figures 11 and 14\ E.g. in superstep 9 for CP, twice as many messages \(4M\ are generated by a worker compared to another \(2M\.  For Pregel BSP, the time taken in a superstep is determined by the slowest worker in that superstep. Hence increase d variability in CP causes even çgoodé partitioning strategies to cause an increase in total execution time wh en using the Pregel/BSP model VIII A NALYSIS OF E LASTIC C LOUD S CALING  Cloud environments offer elasticity Ö the ability to scale-out or scale-in VMs on-demand and only pay for what one uses [28   On th e f l i p s i de  on e en ds u p  paying for VMs that are acquired even if they are underutilized. We have already shown the high variation in compute/memory resources used by algorithms like BC and APSP across different supersteps. While our earlier swath initiation heuristics attempt to flatten these out by overlapping swath executions, one can consider leveraging the cloudês elasticity to, instead, scale up and down the concurrent workers \(and graph partitions\ allocated in each superstep The peak and trough nature of resource utilization combined with Pregel/BSPês synchronous barrier between supersteps offers a window for dynamic scaleout and Öin at superstep boundaries. Peak supersteps can greatly benefit from additional workers, while those same workers will contribute to added synchronization overhead for trough supersteps We offer an analysis of the potential benefits of elastic scaling by extrapolating from observed results for running BC on WG and CP graphs, using four and eight workers.  To provide a fair and focused comparison, we turned off swath heuristics in favor of fixed swath sizes and initiation intervals Figure 15 \(Bottom\ plots the speedup of BC running on eight workers when normalized to BC running on four workers, at corresponding supersteps.  The number of workers does not impact the number of supersteps We also plot the number of active vertices \(i.e. vertices still computing for a given swath\these supersteps which is a measure of how much work is required \(Fig 15 \(Top\. We find that we occasionally get superlinear speedup spikes \(i.e. >2x\ that shows a strong correlation with the peaks of active messages, for both WG and CP graphs. At other times, the sp eedup is sublinear or even a speed-down \(i.e. <1\responding to inactive vertices.  The superlinear speedup is attributable to the lower contention and reduced memory pressure for 8 workers when the active vertices peak \(similar to what we observed for the swath initiation heuristics Similarly, the below par speedup during periods of low activity is contributed by the increased overhead of barrier synchronization across 8 workers. Intuitively, by dynamically scaling up the number of workers for supersteps with peaking active vertices and scaling them down otherwise, we can leverage the superlinear speedup and get more value per worker Using a threshold of 50% active vertices as the threshold condition for between 4 and 8 workers in a superstep, we extrapolate the time per superstep and compared this to the fixed 4 and 8 worker runtimes. We also compute the best-case run time using an çoracleé approach to i.e. for each superstep, we pick the minimum of the 4 or 8 workerês time.  Note that these projections do not yet consider the overheads of scaling, but are rather used to estimate the potential upside if we had an ideal or an automated heuristic for scaling. The total time estimates for running BC on WG and CP graphs, normalized to  
 plot shows speedup of 8 workers relative to 4 workers, for each superstep, when running BC on WG and CP graphs plot shows the number of vertices active in that superstep Estimated time for BC using elastic scaling, normalized to time taken for 4 workers. Normalized cost is shown on secondary Y axis WG graph shown on left CP graph shown on right. Smaller is better 
022\011 022\010 022\007 022\002 006 002 007 006 002 007 010 011 012 013 014 015 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 027\031\030\037\020#@\020"\031\030\027\020\035 0201!2#\024$#\015#5\024",\020"#\017\003"\003\031\003#\011#5\024",\020"\035 024"'\033\026\0309\0201#\\031\020 2 035#\032\020"#+!\034 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 017\020\021\022\023\024\024\025\026\020#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 027\030\031\022\032\033\031\020\034\031\035#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 036\030\034\020\033"#\\0201!2 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 017\020\021\022\023\024\024\025\026\020#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035\031 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 002\003\011 002\003\013 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 033\026\030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 027\030\031\022\032\033\031\020\034\031\035#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035 031 
 
dynamically scaling ideal scaling 
Our hypothesis is that an intelligent adaptive scaling of workers can achieve a similar performance as a large, fixed number of workers, but with reduced cost 
213 


Nature Nature Ecological Applications Nature ACM International Conference on Management of Data \(SIGMOD In Parallel Object-Oriented Scientic Computing \(POOSC Science Communications of the ACM ACM Workshop on Mining and Learning with Graphs Communications of the ACM HotCloud Proceedings of the 19th ACM International Symposium on High PErformance Distributed Computing HPDC Knowledge and Information Systems KAIS International Conference on Computational Science IEEE International Conference on Cloud Computing Technology and Science ACM/IEEE Conference on Advances in Social Network Analysis and Mining \(ASONAM IEEE International Parallel and Distributed Processing Symposium \(IPDPS International Conference on Distributed Computing and Networking Journal of Mathematical Sociology International Conference on Parallel Processing Communications of the ACM 
 
observed time taken using 4 workers, are plotted in Figures 16\(A\ and 16\(B We see that our dynamic scaling heuristic using the percentage of active vertices achieves nearly the same CP\ or better \(WG\ performance as a fixed 8 worker approach. Clearly there is benefit of using fewer workers for low utilization su persteps to eliminate the barrier synchronization overhead. Also, the dynamic scaling heuristic performs almost as well as the ideal scaling. Finally, when we consider the monetary cost of the proposed approaches, assuming a pro-rata normalized cost per VM-second plotted on the secondary Y axis, we see that dynamic scaling is comparable \(CP\ or cheaper \(WG\ than a 4 worker scenario while offering the performance of an 8 worker deployment IX C ONCLUSION  In conclusion, we introduce optimization and heuristics for controlling memory utilization and show they are critical to performance.  By breaking computation into swaths of vertices and using our sizing heuristics we achieve up to 3.5x speedup over the maximum swath size that does not cause the a failure.  In addition overlapping swath executions can provide a 24% gain with automated heuristics and even greater speedup when a priori knowledge of the network characteristics is applied This evaluation offers help to eScience users to make framework selection and cost-performancescalability trade-offs. Our he uristics are generalizable and can be leveraged by other BSP and distributed graph frameworks, and for graph applications beyond BC. Our work uncovered an unexpected impact of partitioning and it would be worthwhile, in future, to examine the ability to pred ict, given certain graph properties, a suitable partitioning model for Pregel/BSP It may also be useful to perform such evaluations on larger graphs and more numbers of VMs. At the same time, it is also worth considering if non-linear graph algorithms are tractable in pr actice for large graphs in a distributed environment B IBLIOGRAPHY  1  F  L i lj er os C   Ed l i n g L  A m a r a l H  S t an ley   and Y    berg The web of human sexual contacts 
vol. 411, pp. 907908, 2001   H Je o n g  S   Ma so n A  L   B a ra b s i  a nd Z   Oltva i  L e t ha l i t y  and centrality in protein networks vol. 411, pp. 41-42 2001   O. B o din and E   E s t r ada    U s i n g n e t w ork c e nt r a l i t y  m e a s ures t o  manage landscape connectivity vol 18, no. 7, pp. 1810-1825, October 2008   D. W a ts s  and S  S t r ogat z  C olle c t i v e  d y nam i cs of  s m a ll-w orl d   networks vol. 393, no. 6684, pp. 440Ö442, June 1998   G  Ma lew i c z   M A u s t er n A   Bik  J   Dehn er t I  Hor n   N. L e i s er and G. Czajkowski, "Pregel: A system for large-scale graph processing," in 2010   D. G r egor  and A  L u m s dain e  T h e  pa r a llel  B G L  A gen e r i c  library for distributed graph computations," in 2005   B. S h a o  H. W a n g  and Y  L i T he T r init y G r aph E n g i n e    Microsoft Research, Technical Report MSR-TR-2012-30, 2012   A  F ox  C lo ud c o m putin g w h at  s  in it for m e  as  a  s c i e n tis t     vol. 331, pp. 406-407, 2011   S. G h e m a w a t  and J  De an   Map re duc e s i m p lifi e d data  processing on large clusters vol 51, no. 3, pp. 107-113, 2008   J  L i n and M. S c hat z   Des i g n  patt er n s  for eff i ci ent gr aph algorithms in MapReduce," in 2010   L   Va l i ant   A b r id g i n g m o d e l f or pa r a llel com putati o n  vol. 33, no. 8, pp. 103-111, 1990 12 a c h e  Ha ma    O n l i n e    http://hama.apache.org   13 Ap a c h e  Ha d o op    O n l i n e    http://hadoop.apache.org     M Z a h a r i a, M. Ch ow dhu ry M F r ank l in S  S h e n k e r, and I   Stoica, "Spark: Cluster Computing with Working Sets," in 2010   J  Ekana y ak e e t a l     T w i st er A  r untim e f o r it er ati v e  MapReduce," in Chicago, 2010, pp. 810-818   U. K a n g  C  T s o u rakakis   and C. F a l outs o s  Peg a s us   Minin g  Peta-scale Graphs," in 2010   M. P a c e  B S P vs  MapR e duc e    in vol. 103.2081, 2012   S. Seo  E  Yoo n, J  K i m  S  J i n  J-S. K i m   and S   Ma e n g HAMA: An Efficient matrix computation with the MapReduce framework," in 2010, pp. 721-726   S. S a l i h ogl u  and J  W i d o m  G PS A G r a ph P r oc e s s i n g Sy s t em    Stanford University, Technical Report 2011   R L i cht e n w a l t e r and N   Cha w la D is Ne t  A fr am ew ork for  distributed graph computation," in  2011   K  Maddu r i  D. E d i g er K   J i an g  D. Bad e r  and D  Cha v a r riaMiranda, "A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets," in 2009   E  K r e p s k a, T  K i el m a nn, W  F o kkink, H   Ba l, "A  hi g h level framework for distributed processing of large-scale graphs," in 2011, pp. 155-166   L   Pa ge  S  B r in R. M o t w ani and T  W i nogr ad  T h e P a geRank citation ranking: Bringing order to the web," Stanford InfoLab Technical Report 1999-66, 1999   U  Brand  s  A f a s t er  a l gor ith m for  b e t w eenn e s s c e nt r a l i t y    vol. 25, no. 2, pp. 163-177 2001   Stan fo r d  Net w or k A na l y s is Pro j e c t  O n l in e    http://snap.stanford.edu    I  S t ant o n and G  K l i o t, "S t r e a m i n g G r aph P a rtiti o n in g  for L a rge Distributed Graphs," Microsoft Corp., Technical Report MSRTR-2011-121, 2011   G   K a ry pis and V   K um a r A fas t and hi g h qua l i t y m u l t i l evel scheme for partitioning irregular graphs," in 1995, pp. 113-122   M. A r m b r u s t e t  a l   A v i ew of  c l o u d  c o m putin g    vol. 53, no. 0001-0782, pp. 50-58 April 2010  
214 


  13  or gani c  c he m i s t r y  i n our  Sol ar  Sy s t e m       Xi a n g  L i r e c e i v e d h i s B  S   m is tr y  fr o m  th e  P e k in g  U n iv e r s ity  C h in a  in  2 0 0 3  and P h D   i n P hy s i c al  C he m i s t r y  f r om  t he  J ohns  H opk i ns  Un i v e r s i t y  i n  2 0 0 9   He  h a s  b e e n  a  R e s e a r c h  A s s o c i a t e  wi t h  a  j o i n t  a p p o i n t m e n t  a t  t h e  U n i v e r s i t y  o f  M a r y l a n d   Ba l t i m o r e  C o u n t y  a n d  N AS A G o d d a r d  S p a c e  Fl i  Ce n t e r  s i n c e  2 0 1 1   H i s  r e s e a r c h  f o c u s e s  o n  t h e  d e t e c t i o n  of  t r ac e  e l e m e nt  and as t r obi ol ogi c al l y  r e l e v ant  or gani c  mo l e c u l e s  i n  p l a n e t a r y  s y s t e ms   l i k e  M a r s   He  i s  es p eci a l l y i n t er es t ed  i n  t h e d evel o p m en t  o f  T i m e of  and I on T r ap m as s  s pe c t r om e t e r s w i t h v a r i o u s i o n i z a t i o n  ng te c h n iq u e s   Wi l l  B r i n c k e r h o f f  sp a c e  sc i e n t i st  i n  t h e  Pl a n e t a r y  En v i r o n m e n t s  La b  a t  N A S A  s  G o d d a r d  Spac e  F l i ght  C e nt e r  i n Gr e e n b e l t   M D w i t h  pr i m ar y  r e s pons i bi l i t y  f or  th e  d e v e lo p m e n t o f th e  L D TO F  m a s s  s p e c t r o  th is  p r o je c t H e  h a s  fo c u s e d  re c e n t l y  o n  t h e  d e v e l o p m e n t  o f  m i n i a t u re  l a se r d ma s s  s p e c t r o me t e r s  f o r  f u t u r e  p l a n e t a r y  mi s s i o n s  a l o n g  wi t h  b a s i c  e x p e r i m e n t a l  r e s e a r c h  i n  a s t r o b i o l o g y  a n d  p r e bi ot i c  s y nt he s i s   D r   B r i nc k e r hof f  i s  i nv ol v e d i n t he  de v e l opm e nt  of  m as s  s pe c t r om e t e r  f or  bot h t he  2011 Ma r s  S c i e n c e  L a b o r a t o r y  a n d  t h e  2 0 1 8  E x o Ma r s  mi s s i o n s   


  14   


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a ìkey, valueî list using an XSTL  Queries made against this list of ìkey, valueî pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


