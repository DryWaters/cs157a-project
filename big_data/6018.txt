Towards Model-Driven Engineerin g for Big Data Analytics An Exploratory Analysis of Domain-Specific Lang uages for Machine Learning 
 
Abstract 
 Graphical models and general purpose inference algorithms are powerful tool s for moving from imperative towards declarative specification of machine learning problems. Although graphical models define 
  Dominic Breuker University of Muenster - ERCIS breuker@ercis.de   
the principle information ne cessary to adapt inference algorithms to specific pr obabilistic models, entirely model-driven development is not yet possible. However, generating executable co de from graphical models could have several advanta ges. It could reduce the skills necessary to impleme nt probabilistic models and may speed up development pr ocesses. Both advantages address pressing industry needs. They come along with increased supply of data scientist labor, the demand of which cannot be fulfilled at the moment. To explore the opportunities of model-driven big data analytics, I review the main modeling 
languages used in machine learning as well as inference algorithms and corresponding software implementations. Gaps hampering direct code generation from graphical models are identified and closed by proposing an initial conceptualization of a domain-specific modeling language 
1. Motivation 
is the term for a new type of analysts possessing both the business knowledge to understand the problems companies are faced with as well as the technical skills to generate decision-relevant information from large-scale datasets. In times in which big 
 
Data Scientist 
data is on everyoneês lips, the data scientist is 
a popular job description. Some even ca ll it the sexiest job of the 21 st century Business intelligence \(BI\itionally been concerned with topics such as gathering, storing, and integrating data from variou s sources to make them available through data wareho uses  T h is data m a y be used purely for reporting or ad-hoc querying, but also for more advanced analytics  T h e f o r m er is  supported by technologies such as online analytical 
processing \(OLAP\ while for the latter, probabilistic analysis techniques can be used with great success In the context of big data much research focusses on technological aspects such as coping with petabytescale datasets. Various approaches to accomplish this are discussed in the literature   w i t h t h e Map R ed u c e  paradigm being the most well-known  Yet eve n  more important than having the technology to process data is hiring talented people capable to make sense of it. Data alone is worthless without analysis. Demand for data scientists is way ahead of supply   
One reason why data scientists are that hard to find can be found by considering the skillset they have to possess. The machine learning and data mining communities have developed a long list of different algorithms for tackling all sorts of analysis problems Among the most popular examples are k-means for clustering and support vector machines for classification  Soph isticated sof t w a re tools such as WEKA  SA S, or S P SS prov ide readyto use i m p l e m e n ta tions of many algorithms, making them easy to apply In any application though it is the data scientistês task 
to find a mapping between these algorithms and the particular problem at hand. The challenge is that there is not necessarily an exact fit. Hence, a combination of different techniques might be most appropriate. In other cases, custom modifications might be necessary Even worse, an analyst needs deep understanding of both the domain and the various technical algorithms to successfully do an analysis a rare combin  On l y do m a in e x p e rts can tel l w h ich qu e s tions are worth asking and which peculiarities of a 
 
problem can be exploited to fine-tune m  Onl y  a technical expert is capable of implementing them Hence, applied data analysis by means of standard tools is a much more creati ve design process than its name might suggest. Consequently, the machine learning community has put considerable efforts into developing a theory of designing customizable algorithms instead of proving a set of black-box procedures. Probabilistic graphical models are one piece of the puzzle They constitute a versatile tool for representing probabilistic models, either standard ones \(e.g., regression 
2014 47th Hawaii International Conference on System Science 978-1-4799-2504-9/14 $31.00 © 2014 IEEE DOI 10.1109/HICSS.2014.101 758 


002  003 004 005   003 004 005  002\(\003 002  003 004 005  002  003   006 002  003 004 005  007,\010 
Figure 1. Simple directed graphical model with three variables 
A C B P\(A,B,C  P\(A|C\P\(C|B\P\(B 
2. Graphical Models 2.1 The Purpose of Graphical Models 
or entirely customized models. Algorithms adapting flexibly to varying model st ructures are the other piece They allow deriving concrete implementations from a single theory   That being said, implementi ng an algorithm specific to a given graphical model in software is still a demanding and effortful task. Fortunately, there is software shielding users from these complex issues. An example is Infer.NET [1 a library prov iding  f a cili ties for probabilistic model specification via an application programming interface \(API\fter choosing one of the available algorithms, the library takes over and executes it on the model. This way of implementing data analytics techniques is called a model-based approach to machine learning   While such frameworks greatly reduce the complexity of implementing machine learning approaches the full potential is not yet leveraged. Modeling is done programmatically via calls to an API. With probabilistic models being represented graphically anyways, it is easily conceivable to do the same thing entirely via visual notations. Not only may this further boost productivity. It may also facilitate collaboration in teams of technical and domain experts Software engineers in the field of model-driven engineering \(MDE\ have long acknowledged the usefulness of visual notations for designing software systems. Numerous frameworks have been developed   T h eir co m m o n t h e m e i s t h at \(1 ai n speci f ic  modeling language \(DSML\ is used to express functionality in terms of the domain under analysis and that 2\ormation engine generates executable code  Mo st i m portantl y t h ese tech n i qu e s a v oid errors  made when programming manually and allow specifying functionality declaratively instead of imperatively Although empirical research is costly and therefore scarce, DSMLs proved to increase productivity in several empirical stud 16   Consequently, the aim of this paper is to explore the opportunities of defining a DSML for probabilistic modeling. The design goal is to allow for entirely visual specification from which executable code can be generated that fits models to data. What I present in this paper is a first conceptualization of such a language. It synthesizes various modeling constructs presented in machine learning literature and extends them with the information necessary to derive executable code This information is identified by studying the Infer.NET API. The ultimate goal of this research is to make the process of analyz ing big data both easier and more efficient, thereby helping to close the gap between supply and demand of data scientists The DSML I propose is not domain specific with respect to an industry or a particular analytical class Rather, it is specific to th e domain of doing inference in probabilistic models with no or partial observations To establish a background on graphical models, I review probabilistic modeling languages, the algorithms they can be used with, and corresponding software packages in Section 2. Section 3 reflects graphical models against the backdrop of MDE The DSML for probabilistic models together with an outline of how code generation can be accomplished are discussed in Section 4. Finally, Section 5 concludes and outlines future research Graphical models are a tool to describe probabilistic models visually. The main ingredients of these models are random variables. A graphical model describes the joint distribution over all the variables it con  C onsider t h e s i m p le e x a m ple ill u s trated  in Figure 1. The graph represents a probability distribution over a set of three random variables  The purpose of graphical models is to support a user in asking questions about random variables very much in the way a database answers questions about the data it stores For in stance, a u s er m i gh t  want to know the distribution over variable A only, i.e with B and C marginalized out To illustrate how graphical models can help answering these questions, consider an example in which all variables A, B, and C are discrete and can assume six different values \(1 to 6 For each variable, the probabilities of this happening may depend on all other variables. The question we are seeking an answer to shall be: what is the distribution  Given the joint distribution this question is easy to answer. Simply marginalize out B and C, i.e sum over all the possible combinations of values B and C can assume  As B and C 
759 


002  014 015  002  014  015  012\002\(\015 
s variableês value N   TF 
6 011 36 006 002  003 004 005  007,\010  006 002  003  004 005  012 007,\010 002  005  004  012\002\(\004 002  003 004 005  002  003  005  012\002  005  004  012\002\(\004 002  003  002  003  013 002  003  004 005  012\002  005  004  012\002  004  007,\010 013 002  003  005  012\002  005  004  012\002  004  007,\010 013 002  003  005  012\013 002  005  004  012\002  004  007 010 6 011 6\0122 
Construct Symbol Meaning 
can assume values from 1 to 6, there are such combinations. While this solution works for toy problems it soon becomes intractable when models have not three but thousan ds of variables The remedy comes in form of conditional independencies that can be explo ited to speed up the computations. Using the product rule of probability 1 we can rewrite which brings no immediate gain Inspecting the formula in Figure 1 though we see that factorizes to  which means that A is conditionally independent of B given C. This is also reflected in the graph visualizing the distribution. There is no arc from B to A. As a consequence, we can change the order of summing and multiplying when computing  The effect of this change is that we no longer compute a single sum with terms but two individual sums with only terms. Thus, this computation is three times as fast as the original one While only discrete variables have been used in the example above, the theory is more general than that   R andom v a riab les m a y eq u all y w e ll h a v e  diff e r ent domains and different distributions \(e.g., real values and Gaussians\ A wide range of stochastic models can be expressed in this way. Conditional independencies encoded in a graph structure allow answering questions regarding marginal distributions by doing a series of local computations While traditional machine learning techniques estimate parameters of models the Bayesian paradigm is gaining widespread popularity in recent times. In Bayesian models a parameter is equipped with a distribution \(the  is treated as a random variable The goal is not to estimate a particular value for the parameter but to infer a distribution over values after observing training data \(the  Key selling arguments include making uncertainty in parameter estimates explicit \(in form of variance in the posterior and enabling an analyst to specify domain knowledge not reflected in the data \(by modifying the prior\[20   As a consequence, the only two things needed in the toolbox of a Bayesian ar e a modeling language to express distributions as graphs and a so called inference algorithm, i.e., an algorithm processing these 1 The product rule of probability says that Common to all graphical models is that they are used to represent conditional independencies between sets of random variables. Also, they are all graphs, i.e they consist of vertices and edges. Apart from that different classes of graphical models exist. Most relevant in this context are directed graphical models undirected graphical models, and factor graphs   It is worth noting that graphical models are not as standardized as the Unified Modeling Language UML o r s o f t w a re en gin e ering or th e Bus i n e ss Process Model and Notation \(BPMN f o r bu sin e ss  process modeling. Different authors may thus use different modeling constructs. My treatment follows Bishops introduction to this topic  bu t so m e e x te n sions from other literature are integrated as well Latent variable Defines a random variable Observed variable Defines a random variable cl amped to an observation Parameter Defines a parameter of the model Dependency relationship Indicates that the distribution of one random variable depends on another variable or parameter Plate Defines an area that is repeated N times Gate Depending on a random variable, it selects different parts of the model with r espect to the random 
 graphs to answer questions regarding conditional marginal distributions \(for this reason graphical models are most popular among Bayesians   T h e u p co m ing two subsections are devoted to introduce these two types of tools 
Table 1. Constructs of directed graphical models 
2.2 Modeling Languages 
prior posterior 
760 


y x N   T F b p 
002  004  002  003  005   016 017 020 017  020\021 016\021 022  016 023  023  024 025 024 026  027 002  030  031 023 032 011   033 034  020 017  022  016 017 023  032 011  035 017\036\025 032 011 002  023  037   034  023  0  025    002  020\021  016  031,\030, ,\032 011  020\021 016\021 
A variableês random variableês distribution may depend on them 
002 
Directed graphical models represent a joint distribution by breaking it down into local pieces. All constructs introduced in the following are presented in Table 1. The main building blocks are random variables, denoted by circles. Th ey are connected by directed arcs which specify th e dependency relationships among them conditional distribution depends on all its parent va riables but no others.  The joint distribution emerges when all conditional distributions are multiplied. In the example of Figure 1 variable B has no parent. Hence, the corresponding distribution is independent of A and C. Variable A though has a parent and so the corresponding distribution depends on it If a model is to be fitted to data, training data must be part of it This is done by making a distinction between latent and observed variables. The latter are shaded grey to visualize the difference. It is said that such a variable is clamped to a value Another necessity is to express parameters of the model which is done using small black circles Similar to observed random variables they have constant values. Like variables, they may be sources of arcs as a  However, they do not have a distribution themselves As models may consist of huge numbers of random variables, syntactic sugar is usef ul to express structural analogies. This is achieved with the plate notation. A plate is an area within th e model that is repeated a number of times. This way, one can for instance repeat a part of the model for each point in the training data  A similar construct developed by Minka and Winn  be u s ed to de s c ribe m i x t u re  m odels in a co m pact way. It is called a gate and can be used to switch on and off different parts of a model depending on a discrete\ variable Each value of this random variable is associated with an area in the model. The entire model is a mixture of all areas weighted with the respective mixing probabilities Consider the example of Bayesian polynomial regression adapted from Bishop  It illu strates th e use  of all these constructs \(c.f. Figure 2 A set of training data consisting of pairs of real variables is given. The task is to predict using a new data point To explain the data, the goal is to fit a polynomial of order k with coefficient vector To account for the uncertainty associated with this explanation, a Gaussian noise model defines the likelihood function To keep things simple is assumed to be known Following the Bayesian approach, we also want to account for uncertainty regarding the polynomial itself. Hence variance in its parameters is formalized using a zerocentered, multivariate Gaussian prior with known precision  A typical question could now be to infer the predictive distribution to reason about likely values of given the training data, and the model specification Another question could be assessing the quality of this model. A Bayesian way of doing so is to compute model evidence, i.e., the likelihood of the model with all random variables marginalized out [25  T h e h i g h er  the evidence, the better is th e model. Hence, this quantity can be used to compare models. To represent this a gate has been added in Figure 2 to switch between the regression model and an empty model using a Bernoulli random variable Model evidence is computed by inferring the posterior over 2  Undirected graphs are an other class of graphical models. The most obvious difference to directed graphs is that edges are undirected. A more important difference is that the set of conditional independencies that can be encoded is not the same. As a consequence some models expressed in one language cannot be converted to the other notation A p art f r o m  u n d i rected arrows, the modeling language is the same Factor graphs are a third notation for distributions As discussed above, grap hical models describe how a joint distribution can be decomposed into factors. Factor graphs are an explicit representation of this factorization T h eir basic co nstr u cts are listed in Table 2 However, they are not restricted to only these three Apart from directed dependencies, any constructs used in the directed graphical models may be used too x n y n 
N f b b 
2 
002 003 
Figure 2. Graphical model for polynomial regression \(adapted from Figure 8.7 
761 


A C B P\(A,B,C  f 1 f 2 f 1  P\(A|C f 2  P\(C|B\P\(B 
Variable Defines a rand om variable Factor Correspond to factors of the joint distribution Relationship Indicates which random variables are part of which factor Random variables in factor graphs are again represented in form of circles, possibly clamped to observed values. The factors of a joint distribution are associated with constructs connected via undirected arcs to all variables used in the factors. An example is illustrated in Figure 3. The joint distribution is split up into two factors and As there can be different ways of splitting up a joint distribution, there can be different factor graphs describing the same joint distribution The goal of inference algorithms is to compute unknown quantities from those that are known. Given a set of latent random variables and a set of observed random variables  probabilistic inference can be described as the problem of computing a distribution over the latent variables given the observed on A s  m e ntio n e d ab ov e in a B a y esia n a p proach in which traditional parameters are treated as latent variables, inference can be used to solve a wide range of problems such as prediction for new data A number of general purpose algorithms have been developed in the past refers to the fact that these algorithms are not tailored to a specific probabilistic model. Instead, they can be adapted to different models, and graphical models provide a formalism facilitating these adaptions On a broad level, inference algorithms can be classified into exact and approximate algorithms. Exact are all algorithms that always deliver the correct solution Notable examples are the jun ction tree algorithm  which works on any kind of graphical model, and the sum-product algorithm w h ic h can be ap plied  to  tree-structured factor graph Problematic about these algorithms is that they are often inapplicable to real world problems. On the one hand, the sum-product structure and can only be used with a limited number of models. On the other hand, the more general junction tree algorithm is computationally intractable for all but the simplest model  E x a ct algorit hm s are al so restricted to  discrete variables or linear Gaussian models   Unfortunately, one cannot expect to find an efficient and exact solution for the general case  Hence, there is a huge field of research dealing with approximate inference. There are two main types of methods: deterministic and stochastic approximations A famous representative of the first type is mean field approximation. The idea is to replace the original intractable distribution with an approximate form with additional independencies added to ensure tractability Minimizing the difference between the original distribution and the approximate form delivers an approximation that can be used for inference. Variational Message Passing \(VMP  is a general purpose algorithm doing this on graphical models. Alternatively, the Expectation propagation \(EP ca n be  used. The difference compared to VMP is that the minimization is done in another way. As approximations, these algorithms do not necessarily return a correct solution. They may be far off and do not provide an estimate of accuracy Stochastic approximations do not approximate the functional form of a distribution but perform sampling instead. The idea is that any property of a distribution can be approximated by drawing and averaging over a sufficiently large number of samples. For virtually all models of interest, Markov Chain Monte Carlo MCMC\methods can be used. They construct a markov chain whose stationary di stribution is the one from which samples shall be drawn A popu lar s p ecial  case of MCMC is Gibbs sampling which can be applied to a wide range of graphical m   A comparison of these inference algorithms is found in Table 3. Stochastic inference is better with respect to accuracy. It will converge to solutions of any accuracy eventually while deterministic algorithms may deliver bad results no matter how long they run Sampling can also be applied to a wider range of problems. It can be cumbersome to apply deterministic 
Construct Symbol Meaning 
algorithm severely restricts a graphês the approximationês 
Table 2. Basic constructs of factor graphs Figure 3. A factor graph corresponding to the directed graphical model of Figure 1 2.3 Inference Algorithms 
022 025 022 011 031  031  002  031   031   
factor General purpose 
762 


2.4 Modeling environments 
Criterion Inference algorithms Name Language Inference algorithms 
Table 3. Comparison of classes of approximate inference algorithms Deterministic Stochastic Table 4. Software packages for graphical models Deterministic Stochastic 
algorithms to more elaborate distributions  T h e y score though when it comes to performance. Sampling may require long running times until convergence is reached. Only for few, very large models, sampling may be more efficient It is also n o t trivial to co n figure a sampler and to diagnose convergence. Deterministic methods can be used more easily Accuracy  Expressiveness  Performance Usability There are several software packages that implement general purpose inference algorithms for graphical models. Murphy i e w ed a num ber of  th e m   While the published review is relatively old, the accompanying website 2 is updated regularly. At the time of writing it lists 68 packages, each of which is characterized along several dimensions. To identify suitable candidates for a machine learning MDE approach, I filtered the list in the following way First, all software packages not having an API were discarded. There are several s tandalone software tools While some of them are well-developed, their major disadvantage is that their models cannot be integrated as components into other software environments Second, all software packages not fully supporting continuous nodes were discarded. Many of the tools work only with discrete random variables or may support continuous variables by discretization or sampling only. Expressiveness is severely reduced when using only discrete variables. Hence, focusing on the more flexible packages seems reasonable Five software packages were left for closer inspection after filtering. They ar e listed in Table 4 along with the corresponding programming languages as well as the inference algorithms they support. Almost all packages offer support for inference via stochastic approximations. Infer.NET d JA GS 34] s u pport ordinary Gibbs sampling. Stan uses a method called Hamiltonian Monte Carlo Sampling  2 http://www.cs.ubc.ca/~murphyk/Software/bnsoft.html Last updated 12 February 2013 Blaise 3 uses some other form of MCMC. Regarding Blaise, I was not able to find source code for download. Deterministic approximations are supported only by BayesBlock 4 through a variational algorithm and by Infer.NET, which allows users to choose between VMP and EP As discussed in Section 2.3, both deterministic and stochastic inference approa ches have their pros and cons. Hence, I chose the Infe r.NET library as a starting point to explore the possibilities of model-driven probabilistic data analysis as it supports both approaches Infer.NET is a C# library that allows specifying probabilistic models via its API. Internally, it compiles the code to more efficient code on which inference algorithms can be run automatically Models are created by defining variable objects and connecting them with factor objects. Hence, it closely resembles the factor graph thinking. Variables may have prior distributions if used in conjunction with distribution factors. Their distribution may also be defined as a function of other variables. For instance, a variable could be the sum of two others Infer.Net offers a wide array of distributions and other types of factors BayesBlocks Python C x Blaise Java x Infer.NET C x x JAGS Java x Stan C R x From the perspective of model-driven engineering graphical models constitute an interesting development in the field of machine learning. The most important aspect is the move away from standard procedures for specific problems that must be adapted and glued together. Instead, these new approaches strictly separate the problem definition from the solution strategy. Declarative modeling languages like A Mathematical Programming Language \(AMPL\ [36 dem o n s trate t h e  usefulness of this idea for the case of optimization  3 http://publications.csail.mit.edu/abstracts/abstracts07 bonawitz/bonawitz.html 4 http://research.ics.aalto.fi/bayes/software 
3 A Model-driven Critique 
userês 
763 


may play the same role in todayês lgorithmês ed factors \(sum of two or more variables inner product of random vectors, É\hich is doc necessary to specify the variablesê roles 
  
variables random observed type value type is parameter for produces Node name Plate 
4. Model-driven Engineering for Machine Learning 4.1 A Graphical Modeling Language 
problems. Users can focus on the important things modeling the domain mathematically and they can use any method they want to have the problem solved Reliable standard software with a declarative interface has been a key driver in the success story of linear programming as it enabled widespread industry applications  G rap h ical m o d els a n d g e n e ral p u rpose inference algorithms big data analytics challenges Moreover, graphical models could be interpreted as a DSML for probabilistic modeling. They provide dedicated constructs needed to specify a probabilistic model on a conceptual level. They also come with general purpose algorithms working on these models Algorithms adapting themselves to the graph structure could be interpreted as transformation engines generating model-specific inference code. Hence, they offer the two constituent elements of MDE technology: a DSML and a transformation engine  Although some literature on graphical models may suggest a graphical model is all one needs to do inference, this is not entirely true. Graphical models are meant as a device conveying the structure of a model to ease the inference a derivation. However they leave many things unspecified. Among them are obvious things such as the types of distributions being used, but also more complicated aspects such as which variables are connected with each other when their relationships cross the borders of plates. Consider the simple example of Figure 1. Whether the variables A B, and C are discrete or re al and which distributions are used to define the model cannot be seen. As a consequence, a direct correspondence of visual models and program code cannot be established Consistent with this observation, none of the libraries in table 4 provides facilities for graphically specifying graphical models. Instead APIs are called to create them. While the process of modeling resembles that of drawing a model \(and specifying, on the way, the additional information that is needed\ no actual visualization connected to the code is created I argue that this way of modeling in code leaves untapped a considerable part of the benefits graphical models may provide. Most importantly, visual languages can further boost productivity. Direct improvements might be realized since specifying models graphically avoids mistakes in the source code. Much more significant though can be secondary effects. Machine learning components integrated into information infrastructures of enterprises will not be developed by a single person but by teams that are subject to employee turnover. New employees could more easily familiarize themselves with visual models than with code A direct, formal connection between visual models and code also ensures alignment that can otherwise be lost easily over time as the software is modified It is worth noting that modeling Bayesian networks is different than using grap hical user interfaces \(GUIs of statistical software packages such as SPSS. The former define distributions \(cf. Section 2.1\hile the latter are used to chain to gether the statistical techniques implemented in the software package To develop a conceptual model of an engine transforming a graphical model to executable code, the first step is to define a DSML containing all necessary information. Graphical modeling languages are a good starting point but do not contain sufficient information To approach the problem of designing the DSML, I have analyzed the Infer.NET modeling API against the backdrop of the modeling languages discussed in Section 2.2. The result is an initial proposal for a DSML A model of its abstract syntax can be found in Figure 4. The rationale behind it is as follows The main constructs of Infer.NET models are which are either or Both have associated data types stor ed in the attribute and the latter have an attribute called to store the observation. Although it is po ssible to define constants as well their use is discouraged. The code generated by the library must be recompiled each time a constant value changes. Observed variables can be changed without recompilation Variables are connected to each other via factors They define the distributions over the variables. Hence Infer.NET models are very similar to factor graphs. A difference is that they can be thought of as directed Factors can be of different types. They broadly fall into the classes of distribution factors \(Gaussian, discrete umented in the attribute  A factor is parameterized by at least one variable, which is captured in the relationship. If more than one variable is required and they cannot be used interchangeably, it is e.g., mean and variance parameters of a Gaussian All factors define a distribution over exactly one random variable which is indicated by the relationship Both variables and factors are subsumed in the abstract entity type which can be given a textual description \(the attribute\des can lie in two types of areas. They can be in a in which case 
764 


Range name new  Range plateSize\Named name 
Figure 4 Abstract syntax of the DSM L in entity relationship notation 
plateSize is higher than selects GateOption Gate value is nested in priors infer Observed Variable predict GenerateModel InferPosteriors MakePredictions GenerateModel GenerateModel 
they are not single nodes bu t arrays of nodes indexed by the plate. The index runs from one to If a node belongs to multiple plates, it becomes a highdimensional jagged array. To consistently index these arrays, an arbitrary total order must be defined on the plates. It is codified in the relationship It can occur that a variable being a parameter for a factor is associated to a diff erent set of plates than the factor is. In this case, it must be specified how the mismatch should be resolved There can be two cases First, the factor can lie in a plate in which the variable is not \(more factors than variables\. This can be resolved by feeding the variable into all the factors. The other possibility is that th e variable lies in a plate in which the factor is not \(more variables than factors Similarly to the first case, all variables can be fed into the factor \(if the specific type of factor can handle them\ any case, if only a specific variable or factor should be used and not all, a selector variable must be defined. The ternary relationship codifies this Nodes may also lie within a to allow for describing mixture models Gate options belong to exactly one which in turn belongs to exactly one random variable. The valu e associated with a gate option must be a possible value of that random variable. It is specified using the attribute. Gate options may be nested in each other as documented in the relationship When using the model, the purpose is to infer the posterior distributions conditioned on all observations to make predictions for new data. Hence, it must be indicated which observed variables  replaced with inferred poster iors. This is done using the Boolean attribute of the entity type After setting the posteriors, training data will be replaced with new data. Any variable for which no new data will be available can be marked as one for which a posterior predictive distribution shall be evaluated This is done using the attribute Based on the DSML from Section 4.1 a simple code generation scheme can be defined. The basic skeleton consists of a single C# class with three methods  and Most relevant is which is why I discuss code generation for this method only Ignoring gates for the moment can be structured into different areas as illustrated in Figure 5. First, ranges must be defined that are used as indices for plates. This is done by processing all plates of a model and inserting a line of code for each of them 
4.2 Code Generation Scheme 
is parameter for produces is contained in is higher than plateSize role D,T infer selects is contained in belongs to is switched by predict is nested in D,T value 1,n 1,n 0,n 0,n 0,n 0,n 0,n 0,n 0,n 0,n 0,n 0,n 0,n 1,1 1,n 1,1 0,n 1,1 1,1 type type value name name 
Factor Variable Node Plate Random Variable Observed Variable Gate GateOption 
765 


Harvard Business Review Communications of the ACM Communications of AIS Communications of the ACM 
5. Conclusion, Limitations, and Outlook 6. References 
Range definition area Variable definition area Factor definition area Branching block area 
Variable varName dataType varDistName varName varDistName var3Name var1Name var2Name selects GateOption Branching block area branch 
Model area 
varName Variable New dataType Named varName Variable  dataType varName varName Variable  dataType Random\(varDistName var3Name = var1Name > var2Name using  Variable If\(branch one model area  using  Variable IfNot\(branch other area  
T. H. Davenport and D. J. Patil, çData Scientist\004: The Sexiest Job Of the 21st Century Data Scientist S. Chaudhuri, U. Dayal, and V. Narasayya, çAn overview ss intelligence technology  H. J. Watson, çTutorial: Business Intelligence Present, and Future  S. Madden, and M. Stonebraker, çA Comparison of Scale Data Analysis,é in  S. Ghemawat, çMapReduce\004: Simplified Data Processing on Large Clusters  
Figure 5 Structure of the model area 
The second step is to define all variables. Again this can be done by proce ssing all entities of type and appending lines of code and are taken from the corresponding attributes of the entities. Additionally, the variable must be declar ed as a class attribute The third step is to define the factors. Hence, all entities of type factor must be processed to append new lines of code in the factor definition area. The structure The first line assigns a distribution object to a variable  is an object representing a distribution. The second line defines the distribution over a Boolean variable as the probability of being larger than  If any variable belongs to a plate, the variable definition code is changed to an array version with corresponding dimensions. The factor definition also iterates over these dimensions. Special care is necessary if entries in the relationship are encountered Fortunately, it can be handled easily by using the selecting variable as an index Finally, it must be accounted for the gates used in the model. This is done by nesting model areas into each other in the same way the entities are nested. A designated root mod el area is created first All other model areas are nested by putting them into the of their parent model area Gates are defined in Infer NET using different selection methods. If the selecting variable is Bernoulli, the code will look like this Motivated by the lack of data scientists in industry I have proposed an initial conceptualization of a DSML supporting code generation from visual representations of probabilistic models for big data analytics Starting at existing notations, extensions have been defined based on analysis of the Infer.NET modeling API How modeling constructs correspond to code has been illustrated informally by a rudimentary code generation scheme. However, no actual code generator for the Infer.NET library has been implemented yet. Doing so would substantiate the claim that the DSML does in fact cover all constructs necessary. I plan to address this limitation in the future by implementing a DSML  Another limitation is the focus on only a single library, namely Infer.NET Referring back to the analogy of linear programming, the ideal DSML would be library-independent and could generate code for a wide range of them Unfortunately these libraries are in an earlier stage of developmen t. In the near future, I do not expect them to be s tandardized an d stable enough to ensure easy interoperabilit y. Integrating other libraries can therefore be a long term goal only It might be fruitful though to analyze standalone software packages that have been discarded in this have GUIs, yet they are often meant to be used for educational examples instead of sophisticated models \(e.g., DoodleBUGS, the GUI of the BUGS project  eles s s u c h  GUIs may inform the design of concrete syntax for the DSML and could also reveal deficits that have not shown up so far. A review is left to future research 
1  no. October, 2012 2  of busine vol. 54, no. 8, pp. 88 98, 2011 3  Past vol. 25, no. 39 pp. 487 510, 2009 4 A  P a v l o, E P a u lso n  A. Rasin, D. J A b ad i, D. J. De W i tt Approaches to Large 2009, pp. 165 178 5 J. De an an d   vol. 51, no. 1, pp. 1 13, 2008 
of the code depends on a fa ctorês type. Examples are using Microsoftês Visualization and Modeling SDK paperês review. Some of them 
SIGMODê09 
766 


Harvard Business Review Knowledge and Information Systems SIGKDD Explorations Advances in Knowledge Discovery and Data Mining IEEE World Congress on Computational Intelligence \(WCCI 2008 Philosophical transactions Series A Mathematical physical and engineering sciences Journal of Visual Languages Computing Computer 9th Workshop on Domain-Specific Modeling at OOPSLA 9th International Software Product Line Conference Pattern Recognition and Machine Learning The Elements of Statistical Learning - Data Mining, Inference, and Prediction Statistical Science Neural Information Processing Systems NIPS Machine Learning: A Probalbistic Perspective Journal of the Royal Statistical Society Series B \(Methodological IEEE Transactions on Information Theory Artificial Intelligence Journal of Machine Learning Research Statistics in Medicine Bayesian Analysis International Society for Bayesian Analysis Bulletin 3rd International Workshop on Distributed Statistical Computing \(DSC 2003 AMPL: A Modelling Language for Mathematical Programming Operations Research 
UAIê01 
6   October, 2012 7  X  W u  V  Kum a r, J. Ross Qu in lan J. G h o s h, Q  Ya n g  M. Motoda, G. J. McLachlan, A. Ng, B. Liu, P. S. Yu, Z vol. 14, no. 1, pp. 1 37, 2007 8 M Ha ll, E Fran k  G  Holm es B P f ah rin g e r P   vol. 11, no. 1 pp. 10 18, 2009 9 U. M  Fay y ad  G  P iatetsk y S h ap iro, an d  P  Smy t h   in  1996, pp. 1 34 10  2008, pp. 1 24 11 T  Mi n k a, J. W in n J. G u iver, an d D  Kn o w les 12  vol. 371, no. 1984, 2013 13  the visual model-driven development of next generation software, vol. 17, no. 6, pp. 528 550, 2006 14   vol. 39, no. 2, pp. 25 31, 2006 15  Use of Domain, 2009 16  Specific 2005, vol. 3714, pp. 198 209 17  rial on Learning With Bayesian 18  19 C M Bisho p  New York: Springer, 2006 20 T  Ha stie, R. T i b s h irani an d  J. Friedm a n   2nd ed. 2009 21   vol. 19, no. 1, pp. 140 15  5, 2004 22  Version 2 23  24  2008, pp. 1073 1080 25 K Mu r p h y  The MIT Press, 2012 26  computations with probabilities on graphical structures and vol. 50, no. 2 pp. 157 224, 1988 27  the Sum, vol. 47, no. 2, pp. 498 519, 2001 28 P inference in Bayesian belief networks is NP, vol. 60, pp. 141 153, 1993 29  vol. 6, no 1, pp. 661 694, 2005 30  2001, pp. 362 369 31 D L u n n D. S p ieg elh a lter, A  T h o m as, an d N Best vol. 28, no. 25, pp. 3049 3067, 2009 32  M P. W a nd, J  T  O r m e r od S. A  P a doa n  a n d R   distributions vol. 6, no. 4, pp. 1 48 2011 33   vol. 14, pp. 13 15, 2007 34  Bayesian Gr 2003 35  V ersion 1.3 36 R  F o u rer D M. G a y  an d B W  Ke rn igh a    Monterey: Duxbury Press, Brooks/Cole Pub. Co., 2002 37  vol. 50, no. 1, pp. 42 47, 2002 
A. McAfee and E. Brynjolfsson, çBig Data: The Management Revolution Zhou, M. Steinbach, D. J. Hand, and D. Steinberg, çTop 10 algorithms in data mining  Reutemann, and I. H. Witten, çThe WEKA data mining software: an update  From data mining to knowledge discovery: an overview,é in  C. M. Bishop, çA New Framework for Machine Learning,é in  Infer.NET 2.5,é 2012 C. M. Bishop, çModel based Machine Learning H. Giese and S. Henkler, çA survey of approaches for intensive systems  D. C. Schmidt, çModel Driven Engineering  J. K‰rn‰, J. P. Tolvanen, and S. Kelly, çEvaluating the Specific Modeling in Practice,é in J. P. Tolvanen and S. Kelly, çDefining Domain Modeling Languages to Automate Product Derivation\004 Collected Experiences,é in  D. Heckerman, çA Tuto Networks,é Redmond, 1996 M. I. Jordan, çAn introduction to probabilistic graphical models,é 2003 M. I. Jordan, çGraphical models  OMG, çUnified Modeling Language \(UML 4.1 August 2011.é 2011 OMG, çBusiness Process Model and Notation \(BPMN Version 2.0 January 2011.é 2011 T. Minka and J. Winn, çGates: A graphical notation for mixture models,é in  S. L. Lauritzen and D. J. Spiegelhalter, çLocal their application to expert systems  F. R. Kschischang and B. J. Frey, çFactor Graphs and Product Algorithm  Dagum and M. Luby, çApproximating probabilistic hard  J. Winn and C. M. Bishop, çVariational Message Passing  T. P. Minka, çExpectation propagation for approximate Bayesian inference,é in  The BUGS project: Evolution, critique and future directions  Fr¸hrwirth, çMean field variational Bayes for elaborate   K. P. Murphy, çSoftware for Graphical models: a review  M. Plummer, çJAGS\004: A Program for Analysis of aphical Models Using Gibbs Sampling JAGS\004 Just Another Gibbs Sampler,é in Stan Development Team, çStan Modeling Language Userês Guide and Reference Manual 2013 G. B. Dantzig, çLinear Programming  
767 


             


                                          


                                             


                      


                                               


   


                                


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


