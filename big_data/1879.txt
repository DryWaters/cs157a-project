Interestingness Peculiarity and Multi-Database Mining Ning Zhong Dept of Information Engineering Maebashi Institute of Technology 460 1 Kamisadori-Cho, Maebashi 37 1 Japan E-mail  zhong  maebashi-it  ac .j p Muneaki Ohshima Graduate School Maebashi Institute of Technology 460 1 Kamisadori-Cho, Maebashi 37 1 Japan Abstract In order to discover 
new surprising interesting pat terns hidden in data peculiarity oriented mining and multi database mining are required In the papel we introduce peculiarity rules as a new class of rules which can be dis covered from a relatively low number of peculiar data by searching the relevance among the peculiar data We give a formal interpretation and comparison of three classes of rules association rules exception rules and 
peculiarity rules as well as describe how to mine more interesting pe culiarity rules in multiple databases 1 Introduction The goal of this work can be summarized in a phrase peculiarity oriented mining in multiple databases for dis covering interesting patterns There are two keywords in this phrase The first keyword is peculiarity which is a kind of in terestingness long identified as an important problem 
in data mining lo 23 241 Peculiarity unexpected rela tionships/rules with common-sense may be hidden in a relatively low number of data Thus we may focus on some interesting data \(peculiar data\and then we find more novel and interesting rules \(peculiarity rules\from the data We argue that the peculiarity rules are a typical regular ity hidden in a lot of scientific, statistical and transaction databases Sometimes, the ordinary association rules with common-sense cannot be found from numerous scientific 
statistical or transaction data or although they can be found the user may be uninterested in the rules because data are Y.Y Yao Dept of Computer Science University of Regina Regina Saskatchewan S4S OA2 Canada E-mail yyao@cs.uregina.ca Setsuo Ohsuga Dept of Infor and Computer Science Waseda University 3-4 1 Okubo Shinjuku-Ku Tokyo 169, Japan rarely specially collectedkored in a database for the pur pose of 
mining knowledge in most organizations The second keyword is multiple databases which are the objects of discovery and learning So far the main stream in the KDD community is limited to rule discovery in a single universal relation or an information table  1 111 Multi database mining is to mine knowledge in multiple related information sources. Generally speaking, the task of multi database mining can be divided into three levels 1 Mining from multiple relations in 
a database Although theoretically any relational database with multiple relations can be transformed into a single uni versal relation, practically this can lead to many issues such as universal relations of unmanageable sizes, in filtration of uninteresting attributes loss of useful rela tion names an unnecessary join operation, and incon veniences for distributed processing 2 Mining from multiple relational databases Some concepts, regularities, causal relationships, and rules cannot be discovered if we just search a sin gle database because the knowledge hides in 
multiple databases basically 3 Mining from multiple mixed-media databases Many datasets in the real world contain more than just a single type of data 115 231 For example medi cal datasets often contain numeric data e.g test re sults\images \(e.g. X-rays\nominal data \(e.g. person smokes/does not smoke and acoustic data e.g the recording of a doctor\222s voice How to handle such multiple data sources is a new challenging research is sue 566 0-7695-1119-8/01 17.00 0 2001 IEEE 


The rest of this paper is organized as follows: Section 2 discusses more detail on interestingness and peculiarity Section 3 gives a formal interpretation and comparison of three classes of rules association rules exception rules and peculiarity rules Section 4 presents a method of peculiarity oriented mining. Section 5 extends the peculiarity oriented mining into multi-database mining Finally Section 6 gives concluding remarks 2 Interestingness and Peculiarity Generally speaking, hypotheses \(knowledge generated from databases can be divided into the following three types incorrect hypotheses, useless hypotheses and new surprising interesting hypotheses The purpose of data mining is to discover new surprising interesting knowl edge hidden in databases. Hence, the evaluation of interest ingness \(including peculiarity surprisingness, unexpected ness usefulness novelty should be done in pre-processing and/or post-processing of the knowledge discovery pro cess 3 5 6 9 231 Here 223evaluating in pre-processing\224 is to select interesting data before hypotheses generation 223evaluating in post-processing\224 is to select interesting rules after hypotheses generation Furthermore, interestingness evaluation may be either subjective or objective  101 Here 223subjective\224 means user-driven that is asking the user to explicitly specify what type of data or rules are interesting and uninteresting and the system then generates or retrieves those matching rules 223objective\224 means data-driven that is analyzing structure of data or rules predictive perfor mance statistical significance and so forth Zhong Yao and Ohsuga proposed peculiarity rules as a new class of rules 23 A peculiarity rule is discovered from peculiar data by searching the relevance among the peculiar data Roughly speaking, data are peculiar if they represent a peculiar case described by a relatively low num ber of objects and are very different from other objects in a dataset Although it looks like the exception rule from the viewpoint of describing a relatively low number of objects the semantic of the peculiarity rule is with common-sense which is a feature of the ordinary association rule 11 141 Illustrative Example The following rule is a pecu liarity one that can be discovered from a relation called Supermarket-Sales see Table 1  in a Supermarket-Sales database rule1  meat-sale\(1ow A vegetable-sale\(1ow A We can see that this rule just covers data in one tuple on July-30 and its semantic is with common-sense. Hence al gorithms for mining association rules and exception rules may fail to find such useful rules However a manager of the supermarket may be interested in such rule because it fruits-sale\(1ow  turnover\(very4ow Table 1 Supermarket-Sales July-2       320 shows that the turnover was a marked drop In order to discover such peculiarity rule we first need to search peculiar data in the relation Supermarket-Sales From Table 1 we can see that the values of the attributes meat-sale vegetable-sale and fruits-sale on July-30 are very different from other values in the attributes Hence the values are regarded as peculiar data Furthermore rule1 is generated by searching the relevance among the peculiar data. Note that we use the qualitative representation for the quantitative values in the above rules The transformation of quantitative to qualitative values can be done by using the following background knowledge on information gran ularity Basic granules bgl  high low very-low bgn  large small very-small bgs  many little very-little      Specific granules kanto-area  Tokyo Tiba, Saitama   chugoku-area   Yamaguchi, Hiroshima, Shimane  yamaguchi-prefecture   Ube Shimonoseki        That is meat-sale  12 vegetable-sale  IO fruits-sale  15 and turnover  100 on July 30 are replaced by the gran ules, \223low\224 and \223very-low\224, respectively 3 Interpretation of Rules This section gives the formal interpretation and compar ison of three classes of rules association rules, exception rules and peculiarity rules 3.1 A Framework for the Interpretation of Rules Typically a rule can be expressed in the form q5 3 J where q5 and Q are formulas of certain language used to de scribe objects tuples in the database In order to have a precise interpretation of rules we need a formal model in which various components of rules can be interpreted We adopt the decision logic language \(DL-language studied by Pawlak  111 in Tarski\222s style through the notions of a 567 


model and satisfiability The model is a database S consist ing of a finite set of objects U An object 11 E U either satisfies a formula 4 written IC ks 4 or in short z k 4 or does not satisfy the formula written a 4 The satisfia bility depends on the semantic interpretation of expressions and must be defined by a particular rule mining method In general, it should satisfy the following conditions  111 1 z  iff not  4 2 3 4 5 z I 4 A  iff I 4 and 5 I 1 2 k 4V iff IC k 4 or5 I 111 z k 4   iff z k v 7 z 4 iff z  4   andz k   d1 where 1 A V  and  are standard logical connectives If d is a formula, the set rns 4 defined by ms\(d  E U I z I 4 1 is called the meaning of the formula 4 in S If S is un derstood we simply write rn\(4 Obviously, the following properties hold  111 4 474  d4 b 44 A   44  4 c 44 v 1  44 U m\($>1 4 44    44 U 4 e 44 5 1  m\(d n m U 44 n 4N The meaning of a formula 4 is the set of all objects hav ing the property expressed by the formula 4 Conversely 4 can be viewed as the description of the set of objects m\(4 Thus a connection between formulas and subsets of U is established A formula 4 is said to be true in a database S written ks d if and only if m\(4  U namely 4 is satisfied by all objects in the universe Two formulas q5 and  are equivalent in S if and only if m\(4  m By definition the following properties hold  111 6 ks 4 iff 44  U1 ii S 14 iff m\(4  0 iv ks q5  iff m\(4  rn Thus we can study the relationships between concepts de scribed by formulas based on the relationships between their corresponding sets of objects A rule 4   can be interpreted by logical implication namely the symbol is interpreted as the logical implica tion  In most cases, the expression 4   may not be true in a database Only certain objects satisfy the expres sion 4   The ratio of objects satisfying 4   can be used to define a quantitative measure of the strength of the rule i4 ps 4   iff 44 G d II  4 a b d C d where 1 I denotes the cardinality of a set It measures the de gree of truth of the expression 4   in a database A prob lem with the logic implication interpretation can be seen as follows For an object if it does not satisfy 4 by defini tion, it satisfies 4   Thus even if the degree of truth of 4   is very high we may not conclude too much on the satisfiability of  given the object satisfies 4 In reality we want to know the satisfiability of  under the condition that 4 is satisfied In other words, our main concern is the satisfiability of  in the subset m\(4 Obviously logical im plication is inappropriate in this case. For the same reason the notion of conditional has been proposed and studied in the context of rule based expert systems 4 Totals a+b c+d 3.2 Probabilistic Interpretations of Rules In data mining rules are typically interpreted in terms of probability A detailed analysis of probability related measures associated with rules has been given by Yao and Zhong  191 We review a few relevant measures. The char acteristics of an rule 4   can be summarized by the following contingency table Totals 1 a+c b+d I a+b+c+d=n a  Im\(4 A 11 6  Im\(dJ A  c  IN74 A 111 d  lm\(iq5A  From the contingency table, different measures can be de fined to reflect various aspects of rules The generality of 4 is defined by 3 which indicates the relative size of the concept 4 A con cept is more general if it covers more instances of the uni verse If G\(4  a then lOOcu of objects in U satisfy  The quantity may be viewed as the probability of a ran domly selected element satisfying 4 Obviously we have 0 I G\(4 I 1 The absolute support of  provided by 4 is the quantity AS\(4    AS 4 The quantity 0 5 AS\($Id 5 1 shows the degree to which 4 implies  If AS\(q!~14  a then lOOa of objects Id f 74411 a    Im\(4 a+b 568 


satisfying q5 also satisfy  It may be viewed as the condi tional probability of a randomly selected element satisfying 11 given that the element satisfies 4 In set-theoretic terms it is the degree to which m\(4 is included in m Clearly AS\(Qlq5  1 if and only if m\(q5 C m\(11 The change ofsupport of  provided by q5 is defined by CS\(q5  1  CS\($I4  AS\(11,P  G\(11 5 Unlike the absolute support, the change of support varies from  1 to 1 One may consider G  to be the prior prob ability of 11 and AS\($Iq5 the posterior probability of  af ter knowing 4 The difference of posterior and prior prob abilities represents the change of our confidence regarding whether q5 actually relates to 11 For a positive value, one may say that 4 is positively related to  for a negative value one may say that q5 is negatively related to 11 The generality G is related to the satisfiability of 11 by all objects in the database and AS\(4  11 is related to the satisfiability of 11 in the subset m\(q5 A high AS  11 does not necessarily suggest a strong association between q5 and 11 as a concept  with a large G value tends to have a large AS\(4   value The change of support CS\(q5   may be more accurate  an  U  b a  C  a  b 3.3 Comparison of Association Rules, Exception Rules and Peculiarity Rules Within the proposed framework we can easily analyze the ordinary association rules by a slightly different formu lation Let I denote a set of items and T denote a set of transactions. For each item i E I we define an atomic ex pression FtZ  i  1 with the satisfiability given by 6 t E T t  F{a iff t contains i 7 and m\(F  t E T I t contains 2 For each subset A I we define a formula FA  AaEA F A transaction satisfies the formula FA if it con tains all items in A For two disjoint subsets of items A and B an association rule can be expressed as FA  FB The association rule FA  FB is interpreted as saying that a customer who purchases all items in A tends to purchase all items in B Two measures, called the support and the confidence, are used to mine association rules They are indeed the gener ality and absolute support SWP\(FA  FB  G\(FA A FE  G\(J\222AuB conf\(F  Pi  AS\(FA  FB 8 By specifying threshold values of support and confident one can obtain all association rules whose support and con fident are above the thresholds Association rules can be extended to non-transaction databases so that both the left hand and right hand sides are formulas expressing properties of objects in a database With an association rule, it is very tempting to relate a large confidence with a strong association between two concepts However such a connection may not exist Suppose we have conf\(q5  11  0.90 If we also have G\(q5  0.95 we can conclude that 4 is in fact negatively associated with  This suggests that an association rule may not reflect the true association Conversely, an association rule with low confidence may have a large change of support In mining association rules concepts with low support are not con sidered in the search for association On the other hand two concepts with low supports may have either large con fidence or a large change of support In summary algo rithms for mining association rules may fail to find such useful rules. Other mining algorithms are needed Exception rules have been studied as extension of associ ation rules to resolve some of the above problems  141 For an aSS6CiatiOn rule 4   with high confidence one may associates an exception rule 4 A q5\221  i Roughly speak ing  can be viewed as the condition for exception to rule q5  11 To be consistent with the intended interpretation of exception rule it is reasonable to assume that q5 A q5\222   have a high confidence and low support. More specifically we would expect a low generality of q5 A 4\222 Otherwise, the rule cannot be viewed as describing exceptional situations Consequently, exception rules cannot be discovered by as sociation rule mining algorithms Recently Zhong Yao and Ohsuga 23 241 identified and studied a new class of rules called peculiarity rules In mining peculiarity rules one considers the distribution of attribute values More specifically, attention is paid to ob jects whose attribute values are quite different from that of other objects This is referred to as peculiarity data iden tification After the isolation of peculiarity data, peculiar ity rules with low support and high confidence and conse quently high change of support, are searched Although a peculiarity rule may share the same properties with an ex ception rule as expressed in terms of support and confi dence it does not express exception to another rules Se mantically they are very different. Furthermore, algorithms forming peculiarity rules are different from mining associ ation rules and exception rules It should be realized that peculiarity rules only represent a subset of all rules with high change of support Based on the above discussion we can qualitatively characterize association rules, exception rules and peculiar ity rules as shown in Table 2 From the viewpoint of support both exception rules and peculiarity rules attempt to find rules that are missed by as sociation rule mining methods While exception rules and peculiarity rules have a high change of support values, in 569 


Table 2 Qualitative characterization of asso ciation rules, exception rules, and peculiarity rules Rule I G\(SUDD I AS\(conf I CS I semantic xi1 Xn1 Association rule  I High 1 High I Unknown I common x,2  xaj  rim Xn2  xnj  Inn sense Exception rule Unknown High exceprion Peculiaritv rule  I Low 1 High 1 High 1 common dicating a strong association between two concepts, associ ation rules do not necessarily have this property All three classes of rules are focused on rules with high level of ab solute support For exception rule it is also expected that the generality of 4 A 4\222 is low For peculiarity, the generali ties of both 4 and 1c are expected to be low In contrast, the generality of right hand of an exception rule does not have to be low From Table 2 one may say that rules with high abso lute support and high change of support are of interest The interesting on the generality of rules depends on the partic ularly application The use of generality \(support in asso ciation rule mining is mainly for the sake of computational cost, rather than semantics consideration Exception rules and peculiarity rules are two subsets of rules with high ab solute support and high change of support It may be in teresting to design an algorithm to find all rules with high absolute support and high change of support 4 Peculiarity Oriented Mining The main task of mining peculiarity rules is the identi fication of peculiarity data According to our previous pa pers 23 241 peculiarity data are a subset of objects in the database and are characterized by two features 1 very dif ferent from other objects in a dataset and 2 consisting of a relatively low number of objects There are many ways of finding the peculiar data In this section we describe an attribute-oriented method 4.1 Finding the Peculiar Data Table 3 shows a relation with attributes AI Az    A In Table 3 let xij be the ith value of Aj and n the number of tuples The peculiarity of xij can be evaluated by the Peculiarity Factor PF\(xij Table 3 A sample table relation I Ai 1 Az 1  I Aj I  I A  Table 4 An example of peculiarity factors for a continuous attribute Region I ArableLand Hokkaido I Yumuguchi Okinawu 147 59.4 It evaluates whether xij occurs in relatively low number and is very different from other data xkj by calculating the sum of the square root of the conceptual distance between xij and xkj The reason why the square root is used in Eq 9 is that we prefer to evaluate closer distances for a relatively large number of data so that the peculiar data can be found from a relatively low number of data Major merits of the method are the following 0 It can handle both the continuous and symbolic at tributes based on a unified semantic interpretation Background knowledge represented by binary neigh borhoods can be used to evaluate the peculiarity if such background knowledge is provided by a user If X is a continuous attribute and no background knowledge is available in Eq 9 10 N\(xij,xkj  xij  xkjl Table 4 shows the calculation of peculairity factor If X is a symbolic attribute andor the background knowledge for representing the conceptual distances between xij and Xkj is provided by a user the peculiarity factor is calculated by the conceptual distances N\(xij,xkj 8 18 23 241 However, the conceptual distances are assigned to 1 if no background knowledge is available There are two major methods for testing if the peculiar data exist or not it is called selection ofpeculiar data after the evaluation for the peculiarity factors The first is based on a threshold value as shown in Eq 1 I threshold  mean of PF\(xij  cr x standard deviation of PF\(xij 11 570 


where cy can be adjusted by a user and cy  1 as default The threshold indicates that a data is a peculiar one if its PF value is much larger than the mean of the PF set In other words if PF\(zij is over the threshold value sij is a peculiar data One can,observe that peculiar data can be selected objec tively by means of the threshold we are proposing, and the subjective factor \(preference of a user can also be included in the threshold value by adjusting the cy The other method for selection of peculiar data uses the chi-square test that is useful when the data size is suffi ciently large 2 4.2 Attribute Oriented Clustering Searching the data for a structure of natural clusters is an important exploratory technique Clusters can provide an informal means of assessing interesting and meaningful groups of peculiar data Attribute oriented clustering is used to quantize contin uous values and eventually perform conceptual abstrac tion 21 In the real world, there are many real-valued at tributes as well as symbolic-valued attributes In order to discover the better knowledge, conceptual abstraction and generalization are also necessary Therefore attribute ori ented clustering is a useful technique as a step of the pecu liarity oriented mining process It is a key issue that how to do clustering in the environ ment in which background knowledge on information gran ularity can either be used or not according to whether such background knowledge exists Our approach is to provide various methods in the mining process so that the different data can be handled effectively If background knowledge on information granularity as stated in Section 2 is available it is used for conceptual ab straction \(generalization andor clustering If no such background knowledge is available, the near est neighbor method is used for clustering of continuous values attributes 7 4.3 An Algorithm Based on the above-stated preparation an algorithm of Step 1 Execute attribute oriented clustering for each at Step 2 Calculate the peculiarity factor PF\(zij in Eq 9 Step 3 Calculate the threshold value in Eq 1 1 based on Step 4 Select the data that are over the threshold value as finding peculiar data can be outlined as follows tribute respectively for all values in an attribute the peculiarity factor obtained in Step 2 the peculiar data Step 5 If the current peculiarity level is enough then goto Step 7 Step 6 Remove the peculiar data from the attribute and thus we get a new dataset. Then go back to Step 2 Step 7 Change the granularity of the peculiar data by us ing background knowledge on information granularity if the background knowledge is available Furthermore the algorithm can be done in a parallel distributed mode for multiple attributes relations and databases because this is an attribute-oriented finding method 4.4 Relevance Among the Peculiar Data A peculiarity rule is discovered from the peculiar data which belong to a cluster by searching the relevance among the peculiar data Let X\(z and Y\(y be the peculiar data found in two attributes X and Y respectively We deal with the following two cases 0 If both X\(s and Y\(y are symbolic data the rele vance between X\(s and Y\(y is evaluated by that is the larger the product of the probabilities of PI and PZ is the stronger the relevance between X\(z and Y\(y is If both X\(z and Y\(y are continuous attributes, the relevance between X\(s and Y\(y is evaluated by us ing the method developed in our KOSI system 22 Furthermore, Eq 12\is suitable for handling more than two peculiar data found in more than two attributes if X\(z or Y\(y is a granule of the peculiar data 5 Multi-Database Mining Building on the preparatory in the previous sections this section extends peculiarity oriented mining into multi database mining 5.1 Peculiarity Oriented Mining in Multiple Databases Generally speaking, the tasks of multi-database mining for the first two levels stated in Section 1 can be described as follows First the concept of a foreign key in the relational databases needs to be extended into a foreign link because we are also interested in getting to non-key attributes for data mining from multiple relations in a database 1161 A 571 


major work is to find peculiar data in multiple relations for a given discovery task while foreign link relationships ex ist In other words our task is to select n relations which contain the peculiar data among m relations m 2 n with foreign links The method for selecting n relations among m relations can be divided into the following steps Step 1 Focus on a relation as the main table and find the peculiar data from this table Then elicit the peculiar ity rules from the peculiar data by using the methods stated in Section 4 Step 2 Find the value\(s of the focused key correspond ing to the mined peculiarity rule or peculiar data in Step 1 and change its granularity of the value\(s of the focused key if the background knowledge on informa tion granularity is available Step 3 Find the peculiar data in the other relations or databases\corresponding to the value or its granule of the focused key Step 4 Select n relations that contain the peculiar data among m relations m 2 n In other words we just select the relations that contain peculiar data relevant to the peculiarity rule mined from the main table A peculiarity rule can be discovered from peculiar data hidden in multiple relations by searching the relevance among the peculiar data If the peculiar data X\(x and Y\(y are found in two different relations we need to use a value or its granule in a key or foreign key/link as the relevance factor K\(lc to find the relevance between X\(z and Y\(y Thus the relevance between X\(z and Y\(y is evaluated by R2  Pl\(K\(lc z K\(lc Y 13 Region Yamaguchi    Furthermore, the above-stated methodology can be ex tended for mining from multiple databases A chal lenge in multi-database mining is a semantic heterogene ity among multiple databases because usually no explicit foreign key/link relationships exists among them Hence the key issue of the extension is how to findkreate the rel evance among different databases In our methodology we use granular computing techniques based on semantics, ap proximation and abstraction for solving the issue 8 201 We again use the illustrative example mentioned at Sec tion 2 If a manager of the supermarket found that the turnover was a marked drop in one day from a supermarket sale database he/she may not understand the deeper rea son Although rule1 as a peculiarity rule \(see Section 2 can be discovered from the supermarket-sales database, the deeper reason why the turnover was a marked drop is not explained well However if we search several related in formation sources such as a weather database as shown in Date   I Weather July-I   sunny July-2    cloud Table 5 Weather 1 t  1 July:30 1 t   I typhinino 2 I July-31 cloud     Table 5 we can find that there was a violent typhoon that day Hence we can understand the deeper reason why the turnover was a marked drop For this case, the granule of aa'dx  Ube in Table 1 needs to be changed into region  yamaguchi for creating explicit foreign link between the supermarket-sales database and the weather database. This example will be further described in the next section 5.2 Representation and Re-learning We use the RVER Reverse Variant Entity-Relationship model to represent the peculiar data and the conceptual re lationships among the peculiar data discovered from mul tiple relations databases 23 Figure l as an example shows the results mined from two databases on supermar ket sales at Yamaguchi prefecture and the weather of Japan The point of which the RVER model is different from an ordinary ER model is that we just represent the attributes that are relevant to the peculiar data and the related pecu liar data \(or their granules in the RVER model Thus, the RVER model provides all interesting information that is rel evant to some focusing e.g turnover  very-low region  yamaguchi and date  July-30 in the supermarket-sale database\for learning more interesting rules among multi ple relations \(databases Re-learning means learning more interesting rules from the RVER model For example, the following rule can be learned from the RVER model shown in Figure 1  rule2  weather\(typhoon  turnover very-low We can see that a manager of the supermarket may be more interested in rule2 rather than rulel because rule2 shows a deeper reason why the turnover was a marked drop 6 Concluding Remarks We presented a method of mining peculiarity rules from multiple databases and a formal interpretation and compar ison of three classes of rules association rules exception rules and peculiarity rules We showed that such peculiar ity rules represent a typically unexpected, interesting regu larity hidden in databases Here we should mention that Liu's group systemati cally investigated how to analyze the subjective interesting 572 


A Supermarket Sales DB Meat-sales\(low  I I A weather Figure 1 The RVER model mined from two databases ness of association rules 9 101 The work of his group is about subjective evaluation of interestingness in post processing i.e evaluating the mined rules In contrast our work is about objective evaluation of interestingness in pre processing i.e selecting interesting data \(peculiar data be fore rule generation In particular our approach can mine a new class of rules peculiarity rules in multiple databases So far as examples with respect to the first two levels of the multi-database mining tasks mentioned in the Introduc tion a number of databases such as Japan-survey amino acid data weather supermarket, web-log have been tested for our approach. The results have been partly discussed in our papers 23 241 Currently we are also working on the third level of the multi-database mining task that is mining from multiple mixed-media databases  171 Our fcture work includes developing a systematic method to nine the rules from multiple databases where there are no explicitly foreign key link\relationships and to induce more interesting rules from the RVER model dis covered from multiple databases by cooperatively using in ductive and abductive reasoning References I Agnwal R et al 221.Fast Discoveryof Association Rules\224 Advances in Knowledge Discovery and Data Mining  1996\307-328 2 Bhattacharyya G.K and Johnson R.A Statistical Concepts and Methods John Wiley  Sons 1977 3 Freitas A.A 223On Objective Measures of Rule Surprisingness\224 J Zytkow and M Quafafou eds Principles of Data Mining and Knowledge Discovery LNAl 1510 Springer 1998 1-9 4 Goodman I.R Nguyen H.T and Walker E.A Conditional Inference and Logic,for Intelligent Systems North-Holland 1991 5 Hilderman R.J Mining Summaries from Databases using Domain Generalizativn Graphs and Objective Measures of Interestingness Ph.D. thesis, University of Regina 2000 Hilderman R.J and Hamilton H.J 223Evaluation of Interestingness Measures for Ranking Discovered Knowledge\224 D Cheung G.J Williams Q. Li Eds Advances in Knowledge Discovery and Data Mining LNAI 2035 Springer 2001\247-259 Johnson R.A and Wichern D.W Applied Multivariate Statistical Analysis Prentice Hall 1998 Lin T.Y 223Granular Computing on Binary Relations 1 Data Mining and Neighborhood Systems\224 L Polkowski and A Skowron eds Rough Sets in Knowledge Discovery Vol 1 Physica-Verlag 1998 Liu B Hsu W and Chen S 223Using General Impressions to Analyze Discovered Classification Rules\224 Proc Third International Confer ence on Knowledge Discovery and Data Mining KDD-97 AAA1 Press 1997\31-36 107- 121 IO Liu B Hsu W Chen S and Ma Y 223Analyzing the Subjec tive Interestingness of Association Rules\224 IEEE Intelligent Systems  1 I Pawlak Z Rough Sets Theoretical Aspects of Reasoning about Data Kluwer Academic Publishers, Dordrecht 1991 I21 Ribeiro J.S Kaufman K.A and Kerschberg, L. \223Knowledge Dis covery from Multiple Databases\224 Proc First Inter Con on Knowl edge Discovery and Data Mining KDD-95 1995 240-245 I31 Silberschatz A and Tuzhilin A 223What Makes Patterns Interesting in Knowledge Discovery Systems\224 IEEE Trans Knowl. Data Eng  141 Suzuki E 223Autonomous Discovery of Reliable Exception Rules\224 Proc Third Inter Con5 on Knowledge Discovery and Data Mining KDD-97 AAA1 Press 1997\259-262 I51 Thrun S et al 223Automated Learning and Discovery\224 AI Magazine I61 Wrobel S 223An Algorithm for Multi-relational Discovery of Sub groups\222\222 J Komorowski et al. \(eds Principles of Data Mining and Knowledge Discovery LNAl 1263 Springer 1997 367-375 I71 Wu J and Zhong N 223An Investigation on Human Multi-Perception Mechanism by Cooperatively Using Psychometrics and Data Mining Techniques\224 Proc. 5th World Multi-Conference on Systemics Cyber netics and Informatics SCI-01 in Invited Session on Multimedia Information: Managing and Processing Vol X 2001\285-290  181 Yao Y.Y 223Granular Computing using Neighborhood Systems\224 Roy R., Furuhashi T Chawdhry P.K eds Advances in Soji Computing Engineering Design and Manufacturinn Springer 1999 539-553  191 Yao Y.Y and Zhong N 223An Analysis of Quantitative Measures As sociated with Rules\224 N Zhong and L Zhou eds Methodologies for Knowledge Discovery and Data Mining LNAI 1574 Springer 1999 479-488 20 Zadeh L A 223Toward a Theory of Fuzzy Information Granulation and Its Centrality in Human Reasoning and Fuzzy Logic\224 Fuzzy Sets and Systems Elsevier Science Publishers 90 1997 1 11-127 21 Zhong N and Ohsuga S 223DiscoveringConcept Clusters by Decom posing Databases\224 Data  Knowledge Engineering Vol 12 No.2 Elsevier 1994 223-244 22 Zhong N and Ohsuga S 223KOSI  An Integrated System for Dis covering Functional Relations from Databases\224 Journal of Intelligent Information Systems Vo1.5 No I Kluwer 1995\25-50 23 Zhong N Yao Y.Y and Ohsuga S 223Peculiarity Oriented Multi Database Mining\224 J Zytkow and J Rauch \(eds Principles ofDora Mining and Knowledge Discovery LNAI 1704 Springer  1999 136 146 24 Zhong N Ohshima M and Ohsuga S 223Peculiarity Oriented Min ing and Its Application for Knowledge Discovery in Amino-acid Data\224, D. Cheung, G.J. Williams Q Li \(eds Advances in Knowledge Discovery and Data Mining LNAI 2035 Springer 2001 260-269 VOI.15 NOS 2000\47-55 8\(6 1996 970-974 Fall 1999 78-82 573 


Figure 8 Visual interface for Moridou system Search EngineTest Page 0 UI 0 5 5 Keyword plealet Figure 9 Prototype system in hcterogeneous environment 283 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


