Mining Negative Association Rules 003 Xiaohui Yuan Bill P Buckles EECS Dept Tulane University New Orleans LA 70118 f yuanx buckles g eecs.tulane.edu Zhaoshan Yuan CSI Dept Hefei Univ of Tech Hefei Anhui P.R.C 230009 zsyuan@hfut.edu.cn Jian Zhang EECS Dept Tulane University New Orleans LA 70118 zhangj@eecs.tulane.edu Abstract The focus of this paper is the discovery of negative association rules Such association rules are complementary to the sorts of association rules most often encountered in 
literatures and have the forms of X  Y or  X  Y  We present a rule discovery algorithm that 336nds a useful subset of valid negative rules In generating negative rules we employ a hierarchical graph-structured taxonomy of domain terms A taxonomy containing classi\336cation information records the similarity between items Given the taxonomy sibling rules duplicated from positive rules with a couple items replaced are derived together with their estimated con\336dence Those sibling rules that bring big con 
336dence deviation are considered candidate negative rules Our study shows that negative association rules can be discovered ef\336ciently from large database 1 Introduction Data mining is the process of extracting implicit previously unknown and potentially useful information from large quantities of data Through the accretion of current data with historical data enterprises 336nd themselves in possession of larger data sets in electronic form than at any time heretofore Various techniques have been employed to convert the data into information including clustering classi 
336cation regression association rule induction sequencing discovery and so forth In general an association rule represents a relationship between two sets of items in the same se It can be written in the form X  Y whereX and Y are item sets i.e values from stipulated domains and X  Y    The left-hand side LHS of the rule is called the antecedent while the right-hand side RHS is called the consequence 1 This work was supported in part by DoD EPSCoR and the State of Louisiana under grant F49620-1-0351 
M u ch ef fort has b een de v o t e d a nd al gori t h ms propos ed for ef\336ciently discovering association rules 1 2 13 5 12 6  I n a p p l y i n g th ese a lg o r ith ms q u a n tity measu r emen ts for generality and correctness are developed which are support and con\336dence respectively The support easures the frequency with which an item set appears in the database and the support or a ule is de\336ned to be the frequency of occurrence that contains the union of items in the antecedent and consequent For instance in a given database 266 out of 1000 records contain 322bread\323 and then the support is 26.6 A rule with low support usually indicates it is not very important However the relative frequency of 
the occurrence of the combinations which is measured by con\336dence is essential in discovering strong rules Con\336dence is computed as the ratio of the frequency of cooccurrence of antecedent and consequence divided by the support of the antecedent It measures the correctness of the rule with the predictability of the rule In the above example if 218 records contain both 322bread\323 and 322butter\323 then the frequency of occurrence of 323bread and butter\323 is 21.8 Thus the con\336dence of the rule 322bread  butter\323 which means if bread is purchased butter is also purchased will be 0.218/0.266 which is 81.95 Thus this rule can be restated as 32281.95 of transactions that include purchasing 
bread also include purchasing butter.\323 The mining problem consists of 336nding all association ules having support and con\336dence that satisfy both prede\336ned minimum support  minsup  and minimum con\336dence  minconf  respectively In addition to the sort of association rule discussed above there are rules that imply negative relationships Such rules are called negative association rules 9 A n e g ative association rule also describes relationships between item sets and implies the occurrence of some item sets characterized by the absence of others In contrast to negative 
association rules the classical rules ost frequently studied re called positive association rules here Algorithms for discovering negative rules are occasionally discussed 9 10  A s hok et al   9  generat e s n e g at i v e r ul es bas e d o n a complex measure of rule parts Brin et al 10 g eneralize tions \(ISCC\22202 1530-1346/02 $17.00 \251 2002 IEEE 


positive and negative rules into correlations and induce with the dependence between two item sets In brief mining negative association rules consists of de\336ning a heuristic search procedure Heuristics tend to favor one set of solutions over others The heuristics cited above are not designed to 336nd all possible negative association rules It is the ones that are most useful that the heuristic should target for discovery Here we argue that criterion for negative association rules utility is its relationship to a valid positive rule In this article we present a method to discover negative rules following the support-con\336dence scheme together with prior knowledge of the relationships between term values in the item sets In the remainder of this paper we start with presenting rule effectiveness measures in section 2 then give the problem statement and formal description in section 3 Section 4 explains the algorithm and the experimental results follow in section 5 Discussion and conclusions are in section 6 2 Rule Effectiveness easures Rule induction is the process of discovering rules that reach a prede\336ned support-con\336dence threshold If we consider the problem of determining which rules are meaningful from an information-theoretic aspect those with high occurrence rate and corresponding low entropy tend to be rules P Smyth et al 11 propos ed an al t e rnat e e ntropy measure of information level for rules and Gregory Piatesky-Shapiro u sed t he measure b ased directly on probabilities However in mining association rules the purpose is to 336nd the most often regularity That means the way to measure 322goodness\323 should be different Fortunately in our situation namely mining negative association rules measuring the deviation is more critical than identifying information level itself Generally items from the same 324market basket\325 tend to appear in the same set of transactions If rules are modi\336ed through substitution of items within the given basket we naturally expect the truth to still hold with respect to the same measure However careful computation can show that the measure of some rules can deviate greatly from what would be expected Such rules include negative association rules A simple example of a negative rule is 32278 of transactions that include purchasing bread and milk do not include butter.\323 Example Table 1\(a shows synthetic data for vehicle purchase information Assumption 1 The minimum support is 30 and minimum con\336dence is 70 Assumption 2 The numeric attribute AGE ranges from 18 to 70 and is quantized into two groups less than thirty and over thirty Item sets that satisfy minsup 30 are listed in Table 1\(b The rule that satis\336es both minimum support and minimum con\336dence criterion is 322age  30  coupe\323 the Table 1 a vehicle purchase information b large item sets with minimum support of 30 SSN Name Age Vehicle Type 711205331 John Smith 23 Coupe 831213362 Tom Lee 45 Truck 914223733 Rachel White 50 Va n 751238334 Jack Gates 27 Sedan 116243395 Peter Cruise 38 Sedan 271253036 Sally Lent 20 Coupe 318261337 Mike Swami 41 Truck 491273328 Erin Buckles 22 Coupe 510283339 Nick Park 39 Coupe 611294340 Jane Chen 40 Sedan a Item sets Support Age  30 40 Age  30 60 Coupe 40 Sedan 30 Age  30 coupe 30 b con\336dence of which is 75 However if we are also looking for negative association rules there exists a rule 322age  30  324not purchasing coupe\323\325 which has a con\336dence of 83.3 For the purpose of identifying purchase pattern it is obvious that the latter has better predictive ability The preceding example illustrates that negative association rules are as important as positive ones However there are at least two aspects making mining negative rules dif\336cult First we cannot simply pick threshold values for support and con\336dence that are guaranteed to be effective in sifting both positive and negative rules Second in a practical database thousands of items are included in the transaction records However there may be a large number of items in the domain of an attribute that do not appear in the database or appear an insigni\336cant number of times An absent item is de\336ned to be one that occurs an insigni\336cant number of times in the transaction set If we consider negative rules for which the HS or RHS is a combination of absent as well as frequently used items then there will be a great many of them In this case a valid rule however is not necessarily a useful one This article addresses both issues 3 Problem Statement The negative association rule differs from its positive counterpart not only in the mining procedure but also in tions \(ISCC\22202 1530-1346/02 $17.00 \251 2002 IEEE 


form Before we present the problem of discovery of negative association rules let\325s state its formal de\336nition 3.1 De\336nition of Negative Association Rule Let I  f i 1 i 2 i n g be a set of items where each item i j corresponds to a value of an attribute and is a member of some attribute domain D h  f d 1 d 2 d s g  i:e i j 2 D h  For simplicity we make domains discrete Let T  f 034 1 034 2 034 p g beatable,where T 022 D 1 002 D 2 002  002 D q  The Cartesian product speci\336es all possible combinations of values from the underlying domains 3 Each transaction 034 j  h g 1 g 2 g m i in table T is associated with an unordered attribute value set t j  f g 1 g 2 g m g using the obvious transform where t j 022 I  We say that t j contains itemset X  which is a subset of I iffor 8 i k 2 X i k 2 t j A positive association rule represents a relationship between two sets of items in the form of X  Y where X 032 I  Y 032 I and X  Y    It might be noted that our de\336nition of rules differs from previously published study 1 2 i n t hat our i t e m domai ns are not necessary binary We say that t j does not contain itemset X 022 I if 9 i k 2 X  i k 2 t j A negative association rule is an implication of the form X  Y or  X  Y  where X 032 I  Y 032 I and X  Y   Note that although rule in the form of  X  Y contains negative elements it is equivalent to a positive association rule in the form of Y  X  Therefore it is not considered as a negative association rule In contrast to positive rules a negative rule encapsulates relationship between the occurrences of one set of items with the absence of the other set of items Our explanation is based on the former negative rule X  Y  however we note in the following descriptions where there are differences The rule X   Y has support s in the data sets if s of transactions in T contain itemset X while do not contain item set Y  The support of a negative association rule supp  X   Y   is the frequency of occurrence of transactions with item set X in the absence of item set Y Let U be the set of transactions that contain all items in X Therule X   Y holds in the given data set database with con\336dence c if c of transactions in U do not contain item set Y  Con\336dence of negative association rule conf  X   Y   can be calculated with P  X  Y  P  X  where P  001  is the probability function Previous algorithms for discovering positive association rules such as Apriori and AprioriTid 2 req u i re mu ltip le passes over the database The support and con\336dence of item sets are calculated during iterations However it is dif\336cult to count the support and con\336dence of non-existing items in transactions To avoid counting them directly we can compute the measures through those of positive rules Given supp  X  Y  and conf  X  Y   the support and con\336dence of the negative rule X  Y can be computed as supp  X  Y  supp  X  000 supp  X  Y  1 conf  X  Y  000 conf  X  Y  2 The computation of the support and con\336dence of negative rules of form  X  Y differ which is given in equation 3 and 4 supp   X  Y  supp  Y  000 supp  Y  X  3 conf   X  Y  P  Y  1 000 P  X  1 000 conf  Y  X  4 3.2 Mining Negative Association Rule Mining association rule as most frequently presented involves 336nding rules that satisfy both user-speci\336ed minimum support and minimum con\336dence We cannot intuitively 322complement\323 the thresholds to 336nd negative rules That is we cannot 336nd positive rules with small support and con\336dence values such that by applying equations 2 and 4 the corresponding negative rules are exposed That will result in many uninteresting rules To eliminate unwanted rules and focus on potential interesting ones we predict possible interesting negative association rules by incorporating domain knowledge of the data sets 3.2.1 Locality of Similarity Let T be the taxonomy of the given item sets Taxonomy T consists of vertexes and directed edges Each vertex represents a class The vertex with in-degree of zero is the most general class and the vertexes having out-degrees of zero are the most speci\336c classes and correspond to items Two vertexes are connected with a directed edge which represents an is-a relationship An example is shown in Figure 1 Figure 1 Example of taxonomy In most cases taxonomies over the items can be devised They provide domain knowledge of the item sets Within tions \(ISCC\22202 1530-1346/02 $17.00 \251 2002 IEEE 


the taxonomy two kinds of relationships are important 321 vertical relationships and horizontal ones 13 4 7 The vertical relationship semantics is that the lower level vertex values are instances of the values of immediate predecessor vertexes i.e the is-a relationship In 13  a v e rtical relationship is used to discover generalized association rules The semantics of the horizontal relationship is that the vertexes on the same level having the same immediate predecessor  siblings to borrow from rooted tree terminology encapsulate similarity among classes We call sibling relationships locality of similarity LOS Items belonging to the same LOS tend to participate in similar association rules This is generally true because members of such groups tend to have similar activity patterns For example in a retail database instances are items involved in transactions and customers are participants If there is no preference for each person the purchase probability of each item will be evenly distributed over all brands Items fall into the same LOS can be denoted as  i 1 i 2 i m  where i 1 i 2 i m 2 I are members of a LOS and are siblings n the taxonomy shown in Figure 1 324IBM Aptiva\325 and 324Compaq Deskpro\325 are on the same level and are inherited from 324Desktop System\325 thus they belong to one LOS that can be written as 324IBM Aptiva\325 324Compaq Des Ho we v e r  the L OS can be extended to different levels following the same parent node For instance it is more reasonable to put 324IBM Aptiva\325 324Compaq Deskpro\325 324Notebook\325 and 324Parts\325 into one LOS when viewing the database at a more abstract level Intuitively siblings are in the same LOS Further similarity is inheritable Formally the similarity assumption in this paper is stated as follows a Let X  f i 1 i 2 i h i m g  Y  f j 1 j 2 j l g and X Y 032 I X  Y    b 9 i h 2 I i k 2 X and  i h i k   such that supp  i h  025 minsup  supp  i k  025 minsup  If 9  r  X  Y   where rule r consists of antecedent X and consequence Y thatis supp  X  Y  025 minsup conf  X  Y  025 minconf  there is a possibility that the inequality conf  X 0  Y  025 minconf is true if the item set X 0  f i 1 i 2 i k i m g is the same as X except item i k is substituted for i h  We denote the rule X 0  Y as rule r 0  We say that rule r  X  Y and rule r 0  X 0  Y are sibling rules to each other 3.2.2 Discovering Negative Rules The use of localities of similarity provides clues to potentially useful negative rules Suppose that items i h i k 2 I are members of a LOS that is  i h i k  Ifrule r  X  Y is true and i h 2 X  then based on the similarity assumption by substituting i k for i h in the antecedent of rule r  a sibling positive rule r 0  X 0  Y is generated However if such association is not supported which means the occurrence of Y is not related to X 0  then the corresponding negative association may exist A salience measure needs to be de\336ned In the context of the taxonomy the salience measure is de\336ned as a distance between con\336dence levels SM  j conf  r 0  000 E  conf  r 0  j 5 Where conf  r 0  is the actual con\336dence of rule r 0 computed with equations 2 or 4 E  conf  r 0  is the estimated con\336dence of rule r 0  which is de\336ned to be equal to the con\336dence of r based on the similarity assumption A large value for SM is evidence for accepting the hypothesis that X 0  Y is false That is X 0  Y may be true From the information theoretic perspective a large SM value 336es large information gain In a sense 322interestingness\323 is associated with high entropy If the candidate negative association rule satis\336es both support and con\336dence criteria it is kept In brief to qualify as a negative rule it must satisfy two conditions 336rst there must exist a large deviation between the estimated and actual con\336dence and second the support and con\336dence are greater than the minima required 3.2.3 Pruning A major consideration in mining negative rules is avoiding the many uninteresting rules This is overcome mostly by a prediction procedure However in addition to the pruning process in discovering positive rules some redundancy introduced by negative rules need to be described In constructing candidate negative rules there are possibilities that an equivalent or similar pair is generated such as X  Y and Y   X  Obviously if both rules appear in the result based on the equivalence theory only one of them is enough to represent the information carried by both Thus one is pruned Another redundancy exists when items from aLOS  i 1 i 2 i m  constitute m 000 k negative rules for which k rules are coupled with positive ones and all are sibling rules The pruning will either keep all positive ones or keep all negative ones that have high con\336dence An example is the pruning between rule 322 F emal e   BuyH at 0 323 and 322  Male   BuyH at 0 323 In this case the domain of attribute gender only includes two values F emal e and Male  which means  Male is equivalent to F emal e  Thus only one rule either Female   BuyH at 0 or  Male   BuyH at 0  is saved 4 Algorithm The discovery procedure can be decomposed into three stages 1 336nd a set of positive rules 2 generate negative tions \(ISCC\22202 1530-1346/02 $17.00 \251 2002 IEEE 


rules based on existing positive rules and domain knowledge 3 prune the redundant rules The algorithm is shown in List 1 In generating negative rules both RHS negative rules and LHS negative rules are developed However in identifying candidate negative rules salience measures are computed for each type of negative rules based on different equations namely equation 2 and equation 4 List 1 Mining negative association rules 1  Finding all positive rules 2 FreqSet 1  f frequent 1-itemsets g  3 k  2 4 while  FreqSet k 000 1 6    5 for all transactions g 2 DataSet 6 C andidateS et t  subset  C andidateS et k g 7 for all candidates c 2 C andidateS et t 8 c.count  c.count  1 9 endfor 10 endfor 11 FreqSet k  f c 2 C andidateS et t j c.count 025 minsup g  12 k=k+1 13 endwhile 14  Generate Positive Rules with Apriori 15 postiveRule  genRule F r eq S et k  16 le  le 17  Generate Negative Rules 18 Delete all items t from the taxonomy t 2 F r eq S t 1 19 for all rules r 2 postiveRule 20 tmpRuleSets  genNegCand\(r 21 for all rules tr 2 tmpRuleSets 22 if SM\(tr.conf t.conf  confDeviate 23 le  f Rule Neg\(tr j Neg\(tr\upp  minsup Neg\(tr  minconf g  24 endif 25 endfor 26 endfor 27  Pruning 28 if all members of LOS have common itemset that form f r 1 r 2 r n g\022 Rule 29 delete r k where r k falls in the categories see 2.2.4 30 endif 5 Results Our experiments are performed on a database of TV cable transactions A few sample records are listed in Table 2 Different support-con\336dence thresholds were tested Table 3\(a gives the LOS for discovering negative rules A few positive and negative rules are listed in Table 3\(b They are generated under the support and con\336dence constraints of 18 and 70 respectively Note that rules in the right column are negative rules discovered with respect to positive rules in the left column Figure 2 shows the number of positive rules and negative rules vs user-speci\336ed minimum support and minimum con\336dence Thus the algorithm can successfully generate negative rules and the number of negative rules discovered is reasonable From Figure 2 we can see that the number of negative rules tends to be related to the number of positive rules However it is inversely proportional to the minimum support threshold The eason is less and less high-support 1-item set survives with increasing support threshold which reduces the number of candidate negative rules signi\336cantly During the discovery of negative rules positive rules which are prior information generally remain unchanged Nevertheless after negative rules are generated both positive and negative rules are reconsidered together to reduce redundancy That is positive rules and negative rules are allowed to interact and some of the rules are eliminated because they contain the same predictive information as other rules In this case only rules that have higher con\336dence are kept Table 2 Sample data Sex Marital TV Type TV Program Promotion M Married Subscription Sports New Customer M Married Subscription Sports Addon Package M Married Subscription ABC News Renewal M Married Subscription ABC News Renewal F Single Subscription ABC News No Promotion M Married Subscription ABC News Renewal F Single Subscription Super Station No Promotion F Single Pay Per View Movies New Customer M Married Subscription Movies No Promotion F Single Pay Per View Movies No Promotion M Married Speci\336c Events Sports New Customer F Single Speci\336c Events Sports No Promotion 6 Discussion and Conclusion We described a novel strategy for mining negative rules The algorithm predicts useful negative rules with respect to existing positive rules by employing domain knowledge namely taxonomy of the data set From the given taxonomy sibling rules are derived together with their estimated con\336dence Rules that have a large con\336dence deviation are considered candidate negative rules Negative rules are generated from those candidates that satisfy support and con\336dence constraints Furthermore the pruning step reduces redundancy among the negative rules Our experiments indicate the ef\336cacy of this strategy Given the number of positive rules P and the average size of the LOS L the complexity of the algorithm is O  P 002 L   Note that the tions \(ISCC\22202 1530-1346/02 $17.00 \251 2002 IEEE 


Table 3 a LOS used in the example b positive and negative rules generated M F  Single Married C oreside Subscription Speci\336c Events Pay Per Vie ABC Fox SUper Station TNT Movie Sports Addon Package New Customer No Promotion Renew a Positive Rules Conf Negative Rules Conf ABC News  MAR 72.3  Sports  MAR 70 Movies  SUB 99  Sports  SUB 86 Movies  SE 99 MAR Married SUB Subscription SE Speci\336c Events b Figure 2 Positive and negative rules discovered under different supports and con\336dences complexity does not depend on the number of transactions since it is assumed that the supports of item sets have been counted and stored for use in this as well as other mining applications However if we are considering discovering positive rules which is necessary in generating negative rules the algorithm must browse all combinations of items The complexity of discovering positive rules depends on not only the number of transactions but also the sizes of attribute domains as well as the number of attributes The overall complexity will be proportional to that of discovering positive rules The performance is also affected by the choice of minimum support A lower minimum support produces more numerous item sets nd with the same e onstraint more positive rules will be rated which adds to computation expense From 336gure 6 we can see the trend in the number of negative and the number of positive rules with different minimum support A contribution of the strategy described here versus the mining of negative rules described elsewhere 9 10 i s t h at our s t rat e gy provides a uniform measure of usefulness based on domain knowledge and takes advantage of ready-to-use statistical data produced during the generation of positive rules The computation of negative support and con\336dence is grounded in statistical theory The experimental data suggests that by using LOS to predict the occurrence of negative rules combining with reasonable interestingness measurement as well as a viable pruning strategy our algorithm yields results that are more accordance with expectation References  R  A gra w al  T  I mi el i n aki  and A  S w a mi  M i n i n g a ssoci ation rules between sets of items in large databases In Proc of the ACM SIGMOD Conference on Management of Data  Washington D.C May 1993 2 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules In Proc of the 20th Int\325l Conf on Very Large Databases  Santiago Chile September 1994  R  E l m asri and S  B  N a v at he Fundamentals of Database Systems 3rd Edition  Addison-Wesley 2000 4 S  F o r tin a n d L  L iu  A n o b j e c t o rie n te d a p p r o a c h to m u ltilevel association rule In Proc of Int\325l Conf on Information and Knowledge Management  November 1996 5 J  H a n a n d Y  F u  M i n i n g m u ltip le le v e l a sso c i a tio n r u l e s in large databases IEEE Trans on Knowledge and Data Engineering  11\(5 1999  M  K lemettinen H  M annila P  R onkainen H  T oi v onen and A I Verkamo Finding interesting rules from large sets of discovered association rules In 3rd Int\325l Conf on Information and Knowledge Management  Gaithersburg Maryland November 1994 7 B  L e n t A Sw a m i a n d J  W id o m  C lu ste r in g a sso c i a tio n rules In Proc of the 13th International Conference on Data Engineering  1997 8 G  P ia te tsk y Sh a p i ro  D isc o v e ry  a n a ly sis a n d p re se n t a tio n of strong rules Knowledge Discovery in Databases  1991  A  S a v asere E  Omi eci nski  a nd S  Na v a t h e Mi ni ng for strong negative associations in a large database of customer transations n Proc of Int\325l Conf on Data Engineering  rary 1998  B  S e r g e y  M  R aj ee v  and S  C r a i g  B e yond mar k et baskets Generalizing association rules to correlations In Proceedings of the ACM SIGMOD international conference on Management of data  1997  P  S m yt h a nd R  M Goodman R ul e i nduct i o n u si ng i n f o r mation theory Knowledge Discovery in Databases  1991 12 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables SIGMOD Record  25\(2 June 1996  R  S r i k ant a nd R  Agr a w a l  Mi ni ng gener a l i zed associ ation es Future Generation Computer Systems  13\(2-3 November 1997 tions \(ISCC\22202 1530-1346/02 $17.00 \251 2002 IEEE 


    f      I i    0 0 m m 100 10 am 1m 1110 lam m am m n am a0 la0 IBO Eo Im 1100 am a minl  10 b mind  20 Figure 6 Performance on Connect-4 11 Figure 7 shows the running time of the three algorithms on the mushroom and pumsb datasets with K set to 500 and mind ranges from 0 to 25 For the mushroom dataset when minl is less than 6 all three algorithm have simi lar low running time TFP keeps its low running time for the whole range of mind and starts to outperform CHARM when minl is as low as 6 and starts to outperform CLOSET when minl is equal to 8 Pumsh has very similar results as connect-4 and mushroom datasets i 1 m j i s 21 di I 2 U  0 I IO I3 1 1 I IO I I 1l ndWRI YUdWR a Mushrwm b Pumsb Figure 7 Performance on Mushroom and Pumsb Sparse Dataset Experiments show that TFP can effi ciently mine sparse datasets without minsupport It has comparable performance with CHARM and CLOSET for low mid and outperforms both on higher mind Figure 8a shows the running times of TFP CHARM and CLOSET on T1014D100K with K fixed at 100 and minl ranges from 1 to 10 Again it demonstrates TFP's strength in dealing with long minl At minl  8 the performance of CHARM and CLOSET starts deteriorating while TFP re tains its good performance Figure 8b shows the perfor mance on the same dataset but with minl fixed at 8 and varying K from 200 to 2000 The curves show that when K is above 400 the running times of CHARM and CLOSET are around 3 times slower than TFP The experiments on the gazelle dataset are shown in Fig ure 9 For smaller K TFP outperforms both CHARM and CLOSET for minl greater than or equal to 5 For K  500 TFP continues to outperform CLOSET for mind greater than or equal to 5 and has similar performance as CHARM Rom this performance study we conclude that TFP has good overall performance for both dense and sparse datasets Its running time is nearly constant over a wide range of K and mind values for dense data Unlike CHARM and CLOSETwhose performance deteriorates as mind increases b L=8 Figure 8 Performance on T1014D100K a K  inn b K  500 Figure 9 Performance on Gazelle TFP's running time stays low The reason is inherent from the mining strategy of TFP CHARM and CLOSET In mast time the support for long patterns is lower than that of short patterns Thus even with the optimal support given both CLOSET and CHARM are unable to prune short fre quent patterns early thus causing much time spent on min ing useless patterns On the other hand TFP is able to use the minl length restriction to cut many short frequent patterns early thus improves its running time instantly In addition TFP does not include any nodes that reside above minl level to participate in the mining process As mind increases more nodes reside above the minie level of the tree means that less conditional FP-trees need to he built thus keeps the running time low Besides the good performance over long minl values the performance of TFP over short minl values even when mind  1 i.e no length constraint is still comparable to that of CLOSET and CHARM In such cases the run ning times between the three do not differ much and both CLOSET and CHARM were run with the optimal support threshold while TFP was not given any support threshold Scalability Test Our performance tests showed that the running time of TFP increases linearly with increased dataset size 5 DISCUSSION In this section we discuss the related work how to gener ate association rules from the mined topk frequent patterns and how to push constraints into the mining process 5.1 Related work Recent studies have shown that closed patterns are more desirable 5 and efficient methods for mining closed pat 217 


terns such as CLOSET 7 and CHARM B have been de veloped However these methods all require a user-specified support threshold Our algorithm does not need the user to provide any minimum support and in most cases runs fater than two efficient algorithms CHARM and CLOSET which in turn outperform Apriori substantially 7 81 Fu et al Z studied mining N most interesting item sets for every length 1 which is different from our work in several aspects 1 they mine all the patterns instead of only the closed ones  2 they do not have minimum length constraintssince it mines patterns at all the lengths some heuristics developed here cannot be applied and 3 their philosophy and methodology of FP-tree modification are also different from ours To the best of our knowledge this is the first study on mining topk frequent closed patterns with length constraint therefore we only compare our method with the two best known and well-performed closed pattern mining algorithms 5.2 Generation of association rules Although topk frequent itemsets could he all that a user wants in some mining tasks in some other cases sjhe wants to mine strong association rules from the mined topk fre quent itemsets We examine how to do this efficiently Items in the short transactions, though not contributing to the support of a topk itemset of length no less than mind may contribute to the support of the items in it Thus they need to be included in the computation which has minimal influence on the performance To derive cor rect confidence we have the following observations 1 The support of every Liternset is derived at the start of min ing 2 The set of topk closed itemsets may contain the items forming subsetjsuperset relationships and the rules involving such itemsets can be automatically derived 3 For rules in other forms, one needs to use the derived topk itemsets as probes and the known minsupport as threshold and perform probe constrained mining to find the support only related to those itemsets 4 As an alternative to the above one can set mind 2 which will derive the patterns readily for all the combinations of association rules 5.3 Pushing constraints into TFP mining Constraint-based mining 14.61 is essential to topk mining since users may always want to put constraints on the data and rules to be mined We examine how different kinds of constraints can be pushed into the topk frequent closed pattern mining deep into the TFP-mining process he succint constraints should be pushed deep to select only those itemsets hefore mining starts and the anti-monotonic Constraint should be pushed into the iterative TFP-mining process in a similar way as FP-growth Second for monotone constraints the rule will also be similar to that in traditional frequent pattern mining is if an itemset mined so far e.g okd satisfies a constraint sum 2 loo adding more items such as e still satisfies it and thus the constraints checking can be avoided in further expansion Third for convertible constraints one can arrange items in an appropriate order so that the constraint can be trans formed into an anti-monotone one and the anti-monotone constraint pushing can he applied First succinct and anti-monotone constraints can be pushed Interested readers can easily prove such properties for top k frequent closed pattern mining 6 CONCLUSIONS We have studied a practically interesting problem mining top-k frequent closed patterns of length no less than mind and proposed an efficient algorithm TFP with several opti mizations 1 using closednodexcount and descendantsum to raise mindupport before tree mining 2 exploring the topdown and bottom-up combined FP-tree mining to first mine the most promising parts of the tree in order to raise rninsupport and prune the unpromising tree branches and 3 using a special indexing structure and a novel closed pattern verification scheme to perform efficient closed pat tern verification Our experiments and performance study show that TFP has high performance In most cases it out performs two efficient frequent closed pattern mining algo rithms CLOSET and CHARM even when they are running with the best tuned minsuppwt Furthermore the method can be extended to generate association rules and to incor porate user-specified constraints Based on this study we conclude that mining topk fre quent closed patterns without minsupport should be more preferable than the traditional minsuppwt-based mining for frequent pattern nuning More detailed study along this direction is needed including further improvement of the performance and flexibility at mining topk frequent closed patterns as well as mining topk frequent closed sequential patterns or structured patterns Acknowledgements We are grateful to Dr Mohammed Zaki for providing the code and data conversion package of CHARM and promptly answering many questions 7 REFERENCES I R Agrawal and R Srikant Fast algorithm for mining 2 A W.-C. Fu R W.-W Kwong and J Tang Mining 3 J Han J Pei and Y Yin Mining frequent patterns 4 R Ng L V S Lakshmanan J Han and A Pang association rules VLDB'94 n-most interesting itemsets ISMIS'W without candidate generation SIGMOD'OO Exploratory mining and pruning optimizations of constrained esociations rules SIGMOD'SR Discovering frequent closed itemsets for association rules ICDT'99 161 J Pei J Ha and L V S Lakshmanan Mining frequent itemsets with convertible constraints ICDE'O1 7 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets DMKD'OO 8 M J Zaki and C J Hsiao CHARM An efficient algorithm for closed itemset mining SDM'O2 5 N Pasquier Y Bastide R Taouil and L Lakhal 218 


I Plenary Panel Session J Future Directions in Database Research  456 Chair Surajit Chaudhuri Microsoft Corporation Panelists Hector Garcia-Molina Stanford University Hank Korth, Bell Laboratories Guy Lohman IBM Almaden Research Center David Lomet Microsoft Research David Maier Oregon Graduate Institute I Session 14 Query Processing in Spatial Databases I Chair Sharma Chakravarthy University of Florida Processing Incremental Multidimensional Range Queries in a Direct Manipulation Visual Query Environment  458 High Dimensional Similarity Joins Algorithms and Performance Evaluation  466 S Hibino and E Rundensteiner N Koudas and K.C Sevcik Y Theodoridis E Stefanakis and T Sellis Cost Models for Join Queries in Spatial Databases  476 Mining Association Rules Anti-Skew Algorithms  486 J.-L Lin and M.H Dunham Mining for Strong Negative Associations in a Large Database of Customer Transactions  494 A Savasere E Omiecinski and S Navathe Mining Optimized Association Rules with Categorical and Numeric Attributes  503 R Rastogi and K Shim Chair: Anoop Singhal AT&T Laboratories S Venkataraman J.F Naughton and M Livny Remote Load-Sensitive Caching for Multi-Server Database Systems  514 DB-MAN A Distributed Database System Based on Database Migration in ATM Networks  522 T Hara K Harumoto M Tsukamoto and S Nishio S Banerjee and P.K Chrysanthis Network Latency Optimizations in Distributed Database Systems  532 I Session 17 Visualization of Multimedia Data I Chair Tiziana Catarci, Universita di Roma 223La Sapienza\224 W Chang D Murthy A Zhang and T.F Syeda-Mahmood Global Integration of Visual Databases  542 X 


The Alps at Your Fingertips Virtual Reality and Geoinformation Systeps  550 R Pajarola l Ohler P Stucki K Szabo and P Widmayer C Baral G. Gonzalez and T.C Son Design and Implementation of Display Specifications for Multimedia Answers  558 1 Session 18 Management of Objects I Chair: Arbee Chen National Tsing Hua University P Boncz A.N Wilschut, and M.L. Kersten C Zou B Salzberg, and R Ladin 0 Wolfson S Chamberlain S Dao L Jiang, and G. Mendei Flattening an Object Algebra to Provide Performance  568 Back to the Future Dynamic Hierarchical Clustering  578 Cost and Imprecision in Modeling the Position of Moving Objects  588 ROL A Prototype for Deductive and Object-Oriented Databases  598 A Graphical Editor for the Conceptual Design of Business Rules  599 The Active HYpermedia Delivery System AHYDS using the M Liu W Yu M Guo and R Shan P Lang W Obermair W Kraus and T Thalhammer PHASME Application-Oriented DBMS  600 F Andres and K. Ono S Chakravarthy and R Le S Mudumbai K Shah A Sheth K Parasuraman and C Bertram ECA Rule Support for Distributed Heterogeneous Environments  601 ZEBRA Image Access System  602 Author Index  603 xi 


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


