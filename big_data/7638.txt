Mining Asso ciation Rules An ti-Sk ew Algorithms 003 Jun-Lin Lin and Margaret H Dunham Departmen t of Computer Science and Engineering Southern Metho dist Univ ersit y Dallas T exas 75275-012 2 USA Abstract Mining asso ciation rules among items in a lar ge datab ase has b e en r e c o gnize d as one of the most imp ortant data mining pr oblems A ll pr op ose d appr o aches for this pr oblem r e quir esc anning the entir e datab ase at le ast or almost twic e in the worst c ase In this p ap er we pr op ose sever al te chniques which over c ome the pr oblem of data skew in the b asket data These te chniques r e duc e the maximum numb er of sc ans to less than 2 and in most c ases 014nd al l asso ciation rules in ab out1sc an Our algorithms employ prior know le dge c ol le cte d during the mining pr o c ess and/or via sampling to further r e duc e the numb er of c andidate itemsets and identify false c andidate itemsets at an e arlier stage 1 In tro duction Recen tly  data mining has attracted m uc h atten tion in the database comm unit y 3 One of the most inv estigated topics in data mining is the problem of disco v ering asso ciation rules o v er b asket data 1 6 8,9  Bask et data whic ht ypically con tains items purc hased b y a customer are collected at the p oin t-of-sales system in a retail business Eac h bask et data de\014nes a tr ansaction  With the adv ance of bar-co de tec hnology  businesses can e\013ectiv ely gather a v ast amoun t of bask et data Ho w ev er to e\013ectiv ely extract useful kno wledge o v er suc h bask et data to assist decision making is a ma jor c hallenge Mining asso ciation rules o v er bask et data w as 014rst in tro duced in Since then asso ciation rules ha v e b een applied to other databases as w ell for examples telecomm unication alarm data and univ ersit y course enrollmen t data 6  An asso ciation rule has three parts the he ad  the b o dy  and the c on\014denc e of the rule Both the head and the b o dy are a set of items F or example if 90 of the customers who purc hase A and B  also purc hase C and D  then it can b e represen ted as an asso ciation rule as follo ws f A B g!f C D g with conf idence  90 In order to generate reliable results the size of the database m ust b e large Th us an e\016cien t algo\003 This researc h is based up on w ork supp orted b y a Massiv e Digital Data System MDDS e\013ort sp onsored b y the Adv anced Researc h and Dev elopmen t Committee of the Comm unit y Managemen t Sta\013 rithm to disco v er the asso ciation rules m ust try to reduce I/O as m uc h as p ossible Man y algorithms ha v e b een prop osed in the literature 1 6,8,9  Among them the Partition algorithm 8  and the Sampling algorithm require the least amoun t of I/O to accomplish the task Both the P artition algorithm and the Sampling algorithm require t w o complete scans o v er the database in the w orst case and one complete scan in the b est case Also a v ariation of the P artition algorithm called SPINC 7  requires 2 n 000 1 n scans in the w orst case where n is the n um b er of partitions In this pap er w e prop ose and in v estigate a family of algorithms called A nti-Skew Counting Partition A lgorithms AS-CP A  Lik e SPINC AS-CP A only needs to scan the database once in the b est case and 2 n 000 1 n times in the w orst case Sev eral tec hniques ha v e b een incorp orated in to AS-CP A to reduce the p ossibilit yof w orst case b eha vior In case a random sample can b e dra wn a Random Sampling v ersion of AS-CP A denoted as RSAS-CP A outp erforms the Sampling algorithm whenev er the Sampling algorithm fails to 014nd all asso ciation rules during the 014rst scan RSAS-CP A has the same b est case p erformance i.e 1 scan as the Sampling algorithm In case a random sample can not b e dra wn without scanning the database from the b eginning a Sequen tial Sampling v ersion of AS-CP A denoted as SSAS-CP A can b e applied It is imp ortan t to note that our tec hniques emplo y prior kno wledge to remo v e false candidates in an earlier stage whic h in turn reduces the CPU and memory o v erhead The rest of this pap er is organized as follo ws Section 2 giv es a formal description of the problem and describ es previous w ork Sections 3 and 4 presen t AS-CP A and Sampling AS-CP A resp ectiv ely  Section 5 giv es a p erformance comparison among these algorithms and Section 6 concludes this pap er 2 Mining Asso ciation Rules This section giv es a formal description of the problem of mining asso ciation rules o v er bask et data mostly based on the description in 1  Let I  f i 1 i 2 i m g b e a set of literals called items  Let D b e a set of transactions Eac h transaction is a non-empt y subset of I  and is asso ciated with a unique iden ti\014er called TID  A set of items is called an itemset  An itemset with k items in it is called a k itemset Eac h itemset X 032I has a measuremen t of statistical signi\014cance 


in D  called supp ort  where suppor t  X D  equals the fraction of transactions in D con taining X  An asso ciation rule is an implication of the form X  Y  where X Y 032I  and X  Y    Eac h rule X  Y has a measuremen t of strength called c on\014denc e  where conf idence  X  Y  suppor t  X  Y D  suppor t  X D   The problem of mining asso ciation rules is to 014nd al l the asso ciation rules of the form X  Y that satisfy the follo wing t w o conditions 1 suppor t  X  Y D  025 min suppor t 2 conf idence  X  Y  025 min conf idence Here min conf idence and min suppor t are user sp eci\014ed This problem can b e decomp osed in to t w o subproblems 1 Find all the itemsets that ha v e supp ort ab o v e min suppor t  These itemsets are called the large itemsets 2 F or eac h large itemset X and an y Y 032 X  c hec k if rule  X 000 Y   Y has con\014dence ab o v e min conf idence  After 014nding all large itemsets the second subproblem can b e solv ed in a straigh tforw ard manner Ho wev er 014nding all large itemsets is not trivial b ecause the size of D and the n um b er of subsets of I can b e v ery large Th us most previous w ork fo cuses on ho w to 014nd all the large itemsets e\016cien tly  The rest of this pap er will also fo cus on this subproblem only  Sev eral algorithms ha v e b een prop osed on 014nding large itemsets 1 6  8 All previous approac hes to 014nding large itemsets construct a set of candidate itemsets and v erify if a candidate itemset is a large itemset Since the size of the database can b e v ery large most algorithms aim at reducing the n um ber of scans needed o v er the en tire database and the n um ber of candidate itemsets 2.1 Lev el-Wise Algorithms The idea b ehind lev el-wise algorithms is based on the follo wing observ ation If X is a large itemset then an y non-empt y subset of X is also a large itemset Let L k b e the set of all large k itemsets and C k be the set of candidate k itemsets Lev el-wise algorithms 4 6 s e L k to generate C k 1 so that C k 1 is a sup erset of L k 1  Then the algorithms construct L k 1 b y scanning the database once to c hec kif c is a large itemset for eac h c 2 C k 1  The same pro cess rep eats for the next lev el un til no large itemset is found As a result if the largest large itemset is a j itemset then the lev el-wise algorithms need to scan the database at most  j  1 times 2.2 P artition Algorithm The idea b ehind the P artition algorithm is based on lemma 1 Lemma 1 If X is a lar ge itemset in datab ase D  which is divide d into n p artitions p 1 p 2 p n  then X must b e a lar ge itemset in at le ast one of the n p artitions Pro of  Pro v eb y con trap ositiv e Assume that X is not a large itemset in an y of the n partitions and pro v e that X m ust b e a small itemset in D  2 The P artition algorithm divides D in to n partitions and pro cesses one partition in main memory at a time The algorithm 014rst scans partition p i  for i 1 to n  to 014nd the set of all lo cal large itemsets in p i  denoted as L p i  Then b y taking the union of L p i for i 1 to n  a set of candidate itemsets o v er D is constructed denoted as C G  Based on Lemma 1 C G is a sup erset of the set of all large itemsets in D  Finally  the algorithm scans eac h partition for the second time to calculate the supp ort of eac h itemset in C G and to 014nd out whic h candidate itemsets are really large itemsets in D Th us only t w o scans are needed to 014nd all the large itemsets in D  Only one scan is needed if all partitions ha v e iden tical lo cal large itemsets SPINC is a v ariation of the P artition algorithm prop osed in 7  Instead of constructing C G b y taking the union of L p i for i 1to n at the end of the 014rst scan as in the P artition algorithm it constructs C G incremen tally b y adding L p i to C G whenev er L p i is a v ailable SPINC starts coun ting the n um berofoccurrences for eac h candidate itemset c 2 C G as so on as c is added to C G  Th us after the 014rst scan if SPINC added the last candidate itemset to C G when pro cessing partition p j  then it only needs to scan up to partition p j 000 1 for the second scan Th us in the b est case only 1 scan is needed and in the w orst case 2 n 000 1 n scans are needed 2.3 Sampling Algorithm Using sampling to assist 014nding all asso ciation rules has b een discussed in 6 9 Among them the Sampling algorithm prop osed b y has the b est p erformance The Sampling algorithm 9  014rst tak es a random sample of the database D  and 014nds the set of large itemsets denoted as S  in the sample using a smaller min suppor t than the one the user sp eci\014ed Then the algorithm calculates the set of itemsets whic h are in the Bd 000  S  Bd 000  S  is the set of minim al itemsets X 032I that X is not in S  The algorithm scans D to c hec kif c is a large itemset in D  for eac h itemset c 2 S  Bd 000  S  If there is no large itemset in Bd 000  S  the algorithm has found all the large itemsets Otherwise the algorithm constructs a set of candidate itemsets denoted as C G  y expanding the negativ e b order of S  Bd 000  S  recursiv ely un til the negativ e b order is empt y  Finally  the algorithm scans D for the second time to c hec kif c is a large itemset for eac h c 2 C G In the b est case this algorithm needs only 1 scan o v er D  In the w orst case t w o scans are needed It should b e noted that the n um b er of candidate itemsets generated for the second scan can b e v ery large if a bad sample w as dra wn for the 014rst scan As a result the second scan can b e v ery ine\016cien t Also using a smaller min supp ort for the sample and expanding with the negativ e b order could result in a large C G  making the 014rst scan v ery ine\016cien t 


3 An ti-Sk ew Coun ting P artition Algorithms One of the ma jor problems of the P artition algorithm and the SPINC algorithm as w ell is data sk ew whic h refers to the irregularit y of data distribution o v er the en tire database D  An example from 8  s sev ere w eather conditions causing the sales of some items to increase rapidly only for a short p erio d of time Data sk ew can cause b oth algorithms to generate man y false candidate itemsets One w a ytoo v ercome data sk ew is to increase the randomness of data across all partitions 8  Ho w ev er this con\015icts with the goal of exploiting sequen tial I/O to sp eed up reading the database Ev en without data sk ew unless eac h item is distributed rather uniformly o v er D  and the size of eac h partition is large enough to capture this uniformness the c hance of a lo cal large itemset i.e a candidate itemset b eing a global large itemset can b e small and the n um b er of candidate itemsets generated can be v ery large It is ob vious that the ab o v e problems can b e mitigated with large partitions Ho w ev er this con\015icts with the main idea of the P artition algorithm pro cessing one partition in memory at a time to a v oid m ultiple scans o v er D from disk In what follo ws w e prop ose a v ariation of the SPINC algorithm called A nti-Skew Counting Partition A lgorithm AS-CP A  that mak es use of the cum ulativ e count of eac h candidate itemset to ac hiev e the illusion of a large partition Lik e the P artition Algorithm AS-CP A also divides the database D in to n disjoin t partitions and pro cesses one partition at a time A partition is a subset of transactions con tained in D  The notation sho wn in T able 1 is partly based on the notation in An itemset x is a lo c al large itemset in partition p i if suppor t  x p i  025 min suppor t An itemset x is a glob al large itemset if suppor t  x D  025 min suppor t  Lik e the SPINC algorithm AS-CP A adds L i to C G as so on as L i is a v ailable during the 014rst scan ASCP A starts coun ting the n um b er of o ccurrences for eac h candidate itemset c as so on as c is added to C G  Eac h candidate itemset c 2 C G has t w o attributes c:ag e con tains the partition n um b er when c w as added to C G  and c:count con tains the n um b er of o ccurrences of c since c w as added to C G A t the end of the 014rst scan w e already ha v e the n um b er of o ccurrences o v er D for eac h candidate itemset c 2 C G with c:ag e 1 th us w e can c hec kif c is a large itemset imm ediately  After that w e remo v e those itemsets with ag e 1 from C G  Then w e start the second scan on partition p 1  and coun t the n um b er of o ccurrence o v er p 1 for eac h itemset c 2 C G and add it to c:count A t this momen t w eha v e the the n um b er of o ccurrences o v er D for eac h candidate itemset c 2 C G with c:ag e 2 th us w e can c hec kif c is a large itemset and remo v e c from C G  Rep eat the same pro cedure for partitions p 2 p 3 p i un til C G is empt y  So far the description of AS-CP A is just the same as SPINC The main feature that separates AS-CP A from SPINC is that AS-CP A pro vides sev eral e\013ecDB i Set of all 1-itemsets in partition p i and their tidlists L i 1 Set of all lo cal large 1-itemsets in partition p i L i Set of all lo cal large itemsets not including 1-itemsets in partition p i L 1  i 1 Set of all lo cal large 1-itemsets in S j 1  i p j B i 1 Set of all b etter lo cal large 1-itemsets in partition p i B i Set of all b etter lo cal large itemsets not including 1-itemsets in partition p i C G Set of all global candidate itemsets L G Set of all global large itemsets T able 1 Notation tiv e tec hniques to 014lter out false candidate itemsets at an earlier stage This reduces the probabilit yof w orse case b eha vior W e discuss these tec hniques in the follo wing three subsections 3.1 Early Lo cal Pruning T o generate the set L i of all lo cal large itemsets in a partition p i  b oth the P artition algorithm and SPINC 014rst 014nd the set of all lo cal large 1-itemsets in p i and use a lev el-wise algorithm to 014nd all the lo cal large itemsets in p i  With e arly lo c al pruning w e tak ea similar but more e\016cien t approac h The idea of e arly lo c al pruning is as follo ws When reading a partition p i to generate L i 1 w e record and accum ulate the n um b er of o ccurrences for eac h item Th us after reading partition p i w e kno w b oth the n um b er of o ccurrences for eac h item in partition p i  and the n um b er of o ccurrences for eac h item in the big partition S j 1  2  i p j  With this information w e kno w b oth the set L i 1 of all lo cal large 1-itemsets in p i  and the set L 1 i 1 of all lo cal large 1-itemsets in the big partition S j 1  2  i p j W e de\014ne B i 1 as the set of all b etter lo cal large 1-itemsets in partition p i  where B i 1  L i 1  L 1  i 1  With e arly lo c al pruning w e use B i 1 instead of L i 1 to start the lev el-wise algorithm to construct the set B i of all b etter lo cal large itemsets in partition p i  Since the size of B i 1 is usually smaller than that of L i 1  B i can b e constructed faster than L i  With P artition algorithm Lemma 1 ensures that C G  S i 1  2  n L p i is a sup erset of the set L G of all global large itemsets With early lo cal pruning Lemma 2 sho ws that C G  S i 1  2  n B p i is also a sup erset of L G  Lemma 2 L et the datab ase D b e divide d into n p artitions p 1 p 2 p n L et C G  S i 1  2  n B i  Then C G is a sup erset of L G  Pro of  pro v eb y mathematical induction W e denote the set of all global large itemsets of a database D as L G  D  


Base case  D con tains only 1 partition Lemma 2 is true trivial Induction Hypothesis if D  S i 1  2  n p i  then S i 1  2  n B i is a sup erset of L G  D  That is L G  D  032 S i 1  2  n B i Induction Step  Consider a new database D 0  D p n 1  and pro v e that S i 1  2  n  n 1 B i is a sup erset of L G  D 0  Consider D 0 as a database with 2 partitions D and p n 1  According to Lemma 1 and the Induction Hyp othesis w eha v e L G  D 0  032 L G  D   L n 1 032 S i 1  2  n B i  L n 1 case 1 L n 1 1 032 L 1   n 1 1  B n 1 1  L n 1 1  L 1   n 1 1  L n 1 1   B n 1  L n 1   L G  D 0  032 S i 1  2  n  n 1 B i case 2 L n 1 1 is not a subset of L 1   n 1 1  there exists some 1-itemset l 2 L n 1 1 but not in L 1   n 1 1  Since l is not in L 1   n 1 1 an y sup erset of l is not in L G  D 0  Th us w e can remo v e all suc h l from L n 1 1  and ev en tually L n 1 1 will b ecome a subset of L 1   n 1 1  case 2 degrades to case 1 2 3.2 First Global An ti-Sk ew The 014rst glob al anti-skew tec hnique aims at remo ving p ossibly false global candidate itemsets during the 014rst scan o v er the database D  It can b e used with or without the early lo cal pruning tec hnique In what follo ws w e 014rst discuss ho w it is used without the early lo cal pruning tec hnique and then discuss ho w it can b e used with the early lo cal pruning tec hnique Lemma 3 L et the datab ase D b e divide d into n p artitions p 1 p 2 p n  Assume that itemset x is not alo c al lar ge itemset in either of the fol lowing two big p artitions S j 1  2  k 000 1 p j  and S j  k;k 1 k 2  i p j  If x is a lar ge itemset over D  x must b ea lo c al lar ge itemset in at le ast one of the fol lowing p artitions p i 1 p i 2 p n  Pro of  Lemma 3 is the direct result of Lemma 1 2 Without early lo cal pruning C G is de\014ned as S i 1  2  n L p i as in the P artition algorithm and the 014rst global an ti-sk ew tec hnique is based on Lemma 3 Note that for eac h c 2 C G  c is not a lo cal large itemset in S j 1  2  c:ag e 000 1 p j b ecause c w as added to C G when pro cessing partition p c:ag e  During the 014rst scan after pro cessing partition p i  for eac h itemset c 2 C G  c:count con tains the n um b er of o ccurrences of c in S j  c:ag e;c:ag e 1  i p j  Th us w e can c hec kif c is a lo cal large itemset in the big partition BI G  S j  c:ag e;c:ag e 1  i p j If c is not a lo cal large itemset in BI G  then c is a p ossibly false candidate itemset and the 014rst glob al anti-skew tec hnique remo v es c from C G If c is really a global large itemset in D  according to Lemma 3 c m ust b e a lo cal large itemset in at least one of the follo wing partitions p i 1 p i 2 p n  and th us c will b e added bac kto C G again Notice here w e can also remo v ean y sup erset s of c from C G when w e remo v e c from C G  The reason is as follo ws Since c w as added to C G when pro cessing partition p c:ag e  c is not a lo cal large itemset in S j 1  2  c:ag e 000 1 p j  This together with the fact that c is not a lo cal large itemset in S j  c:ag e;c:ag e 1  i p j  indicates that c is not a lo cal large itemset in S j 1  2  i p j  according to Lemma 1 Since s is a sup erset of c  s is not a lo cal large itemset in S j 1  2  i p j either If w e remo v e s from C G when pro cessing partition p i but s is actually a global large itemset in D  according to Lemma 1 s m ust b e a lo cal large itemset in at least one of the follo wing partitions p i 1 p i 2 p n  and th us s will b e added bac kto C G again T oa v oid remo ving and adding the same itemset rep eatedly and to reduce the added CPU o v erhead of the 014rst glob al anti-skew tec hnique this pro cedure is applied to eac h c 2 C G just once as so on as c is considered old enough That is if k partitions are considered large enough to capture the uniformness of the database D  then when c:count con tains the n um ber of o ccurrences of c in k partitions this pro cedure is applied to c  i.e after pro cessing partition p c:ag e  k 000 1  With early lo cal pruning C G is de\014ned as S i 1  2  n B p i  and the 014rst an ti-sk ew tec hnique is based on Lemma 4 b elo w Lemma 4 L et the datab ase D b e divide d into n p artitions p 1 p 2 p n  Assume that itemset x is not a b etter lo c al lar ge itemset in either of the fol lowing two big p artitions S j 1  2  k 000 1 p j  and S j  k;k 1 k 2  i p j  If x is a lar ge itemset over D  x must b eab etter lo c al lar ge itemset in at le ast one of the fol lowing p artitions p i 1 p i 2 p n  Pro of  According to Lemma 2 if c 2 C G is a global large itemset in D  then c m ust b e a better lo cal large itemset in at least one of the partitions of D  Lemma 4 is the direct result of Lemma 2 2 3.3 Second Global An ti-Sk ew The se c ond glob al anti-skew tec hnique is used to remo v e false global candidate itemsets during the second scan It can b e used with or without the earlier lo cal pruning and the 014rst global an ti-sk ew tec hniques In what follo ws w e 014rst sho who w the second an ti-sk ew tec hnique is used without the earlier lo cal pruning and the 014rst an ti-sk ew tec hniques Then w e sho who wto use the second an ti-sk ew tec hnique with the 014rst an tisk ew tec hnique Lemma 5 L et the datab ase D b e divide d into n p artitions p 1 p 2 p n  If itemset x is not a lo c al lar ge itemset in any of the fol lowing p artitions p 1 p 2 p k 000 1  and S j  k;k 1 k 2  n p j  S j 1  2  3  i p j  wher e i<k  then x is not a lar ge itemset in D  


Pro of  Since x is not a lo cal large itemset in an y of the follo wing partitions p 1 p 2 p k 000 1  x is not a lo cal large itemset in S j  i 1 i 2  k 000 1 p j  Imagine there are only t w o partitions in D  S j  k;k 1 k 2  n p j  S j 1  2  3  i p j and S j  i 1 i 2  k 000 1 p j  Lemma 5 is the direct result of Lemma 1 2 During the second scan after pro cessing partition p i  for eac h itemset c 2 C G  c:count con tains the n um b er of o ccurrences of c in S j  c:ag e;c:ag e 1  n p j  S j 1  2  i p j  Since c is added to C G when pro cessing partition p c:ag e  c is not a lo cal large itemset in an y of the follo wing partitions p 1 p 2 p c:ag e 000 1  Here w e assume the 014rst an ti-sk ew and early lo cal pruning tec hniques are not used According to Lemma 5 if c is not a large itemset in S j  c:ag e;c:ag e 1  n p j  S j 1  2  i p j  c is not a large itemset in D  and th us w e can remo v e c and all sup ersets of c from C G  A less restricted v ersion of Lemma 5 is needed in order to supp ort the com bination of the 014rst and se cond an ti-sk ew tec hniques With 014rst an ti-sk ew tec hnique a candidate itemset c can b e added to and remo v ed from C G man y times so that c migh t b e a local large itemset in one of the follo wing partitions p 1 p 2 p c:ag e 000 1  Th us w e can no longer apply Lemma 5 to supp ort the second an ti-sk ew tec hnique Lemma 6 sho ws that b oth an ti-sk ew tec hniques can co exist Lemma 6 L et the datab ase D b e divide d into n p artitions p 1 p 2 p n  Assume that at the end of the 014rst sc an an itemset c 2 C G has c:ag e  k  If c is not a lo c al lar ge itemset in S j  k;k 1 k 2  n p j  S j 1  2  3  i p j  wher e i k  then c is not a lar ge itemset in D  The pro of for Lemma 6 can b e found in 5  Both Lemmas 5 and 6 are based on Lemma 1 Since Lemma 1 to lo cal large itemsets is just the same as Lemma 2 to b etter lo cal large itemsets w e can deriv e from Lemma 2 in a similar fashion to pro v e that the second an ti-sk ew tec hnique can b e used with the early lo cal pruning tec hnique as w ell 3.4 Algorithms AS-CP A is a family of algorithms Eac h uses one or more tec hniques to impro v e its p erformance as sho wn in T able 2 Here w e only list the algorithm for ASCP A-12E AS-CP A has adopted some pro cedures from their coun terparts in the P artition algorithm 8  As in 2 w e assume that a transaction in the database is in the form of TID;i j i k i x   and the items in a transaction are in lexicographic order Name early lo cal pruning 1st global an ti-sk ew 2nd global an ti-sk ew AS-CP A-12 no y es y es AS-CP A-2 no no y es AS-CP A-12E y es y es y es AS-CP A-2E y es no y es T able 2 Algorithms pro cedure AS-CP A-12E 1 P  partition database D  2 n  Num b er of partitions 3 m  Num b er of transactions in D 4 k n um b er of partitions to b e considered as a big partition 5 for i 1to n 6 m  i   Num b er of transaction s in partition p i 7 C G   8 for i 1to n b egin  phase I 9 acc  i  m  i  10 for j  1 to  i 000 1 11 acc  j  acc  j  m  i  12 DB i  read and transp ose p i 2 P  13 add coun t C G DB i  14 B i 1  gen b etter large 1-itemsets DB i  15 B i  gen large itemsets B i 1  16 for eac h itemset l 2 B i 000 C G b egin 17 l:ag e  i 18 l:count  j l:tidlist j 19 C G  C G f l g 20 end 21 C G  014rst an ti sk ew C G  acc 1 i  22 end 23 C G  second an ti sk ew C G  acc  i n   phase II 24 L G  f c 2 C G j c:ag e  1 g 25 C G  C G 000 L G 26 i 1 27 while  C G 6    b egin 28 DB i  read and transp ose p i 2 P  29 add coun t C G DB i  30 for j  i 1 to n 31 acc  j  acc  j  m  i  32 i  i 1 33 C G  second an ti sk ew C G  acc  i n  34 L G  L G f c 2 C G j c:ag e  i g 35 C G  C G 000f c 2 C G j c:ag e  i g 36 end 37 return L G Initially  the database D is partitioned in to n partitions b y executing pro cedure partition database  and C G is empt y  During the phase I  the algorithm pro cesses one partition at a time for all partitions When pro cessing partition p i  the con ten tof p i is read and transformed in to DB i  whic h con tains a tidl ist data structure for eac h item in p i  Here the tidl ist of an item x con tains the TID of all the transactions in partition p i that con tains x Th us the length of the tidl ist of an item x  denoted as j x:tidl ist j  equals to the n um b er of o ccurrences of x o v er partition p i  With DB i  the algorithm then calls pro cedure add coun t to coun t the n um b er of o ccurrences for eac h itemset c 2 C G  and add it to c:count  1 1 The tidlist data structure is prop osed in 8  By taking the in tersection of the tidlist of all items in an itemset the length of the in tersection giv es the n um b er of o ccurrence s of the itemset 


In AS-CP A-12E w e use the e arly lo c al pruning tec hnique th us w e also accum ulate the n um ber of occurrences for eac h item and construct the set L 1  i 1 of all lo cal large 1-itemsets in S j 1  2  i p j in pro cedure add coun t  Then the algorithm calls pro cedure gen b etter large 1-itemsets to generate the set B i 1 of all the b etter lo cal large 1-itemsets and their associated tidl ist  Again due to the e arly lo c al pruning tec hnique the B i 1 generated here is L i 1  L 1  i 1 A t this stage the con ten tof DB i is no longer needed pro cedure add coun t C G DB i  1 L 1  i 1   2 for eac h x 2I b egin 3 x:count  x:count  j DB i x tidlist j 4 if x:count acc 1 025 min suppor t 5 L 1  i 1  L 1  i 1 f x g 6 end 7 for eac h c 2 C G b egin 8 num n um b er of o ccurrences of c in partition p i 9 c:count  c:count  num 10 end pro cedure gen b etter large 1-itemsets DB i  1 L 1 1   2 for eac h x 2I b egin 3 if j DB i x tidlist j acc  i  025 min suppor t 4 L i 1  L i 1 f x g 5 end 6 return L i 1  L 1  i 1 The algorithm then calls pro cedure gen large itemsets to generate the set of all the local b etter large itemsets in partition p i  denoted as B i  Then the algorithm starts to add itemsets from B i to C G  When adding an itemset l from B i to C G w e di\013eren tiate those in B i that are already in C G i.e old candidate itemsets and those in B i that are not y et in C G i.e new candidate itemsets If l is a old candidate do nothing since its n um b er of o ccurrences in partition p i has b een already added to l count when calling pro cedure add coun t If l is a new candidate w e assign partition n um ber i to l ag e  assign the n umb er of o ccurrences of l in p i to l count  and add l to C G  Then the algorithm calls pro cedure 014rst an ti sk ew to remo v e p ossibly false candidate itemsets and their sup ersets in C G  pro cedure 014rst an ti sk ew  C G  acc  i::j  1 for eac h c 2 C G with c:ag e  i 000 k 1 b egin 2 if c:count acc  c:ag e   min suppor t 3 C G  C G 000f s 2 C G j c 022 s g 4 end The same pro cess rep eats for the rest of the partitions then the algorithm en ters the phase II  Note that at this momen t for eac h candidate itemset c 2 C G  c:count con tains the n um b er of o ccurrences of c from partition p c:ag e to partition p n  During the phase II  the algorithm calls pro cedure second an ti sk ew to remo v e some false candidate itemsets in C G  Then the algorithm mo v es eac h c 2 C G with c:ag e 1 to L G  Then the algorithm reads in partition 1 and up dates the coun t for eac h itemset c 2 C G  The same pro cess rep eats un til C G is empt y  pro cedure second an ti sk ew  C G  acc  i::j  1 for eac h c 2 C G b egin 2 if c:count acc  c:ag e   min suppor t 3 C G  C G 000f s 2 C G j c 022 s g 4 end Note that the only complex pro cedure called b y CP Ais gen large itemsets  Since AS-CP A uses the same tidl ist data structure to coun t the n um ber of o ccurrences of an itemset as in P artition algorithm the same coun terpart in P artition algorithm can b e used in CP A with v ery little mo di\014cation Please refer 8  for gen large itemsets  4 Sampling An ti-Sk ew Coun ting P artition Algorithm In this section w e presen t a sequen tial sampling v ersion of AS-CP A called SSAS-CP A  that uses the 014rst few partitions as a sample to lo cate all large itemsets at an earlier stage With SSAS-CP A the user also sp eci\014es ho w man y partitions are used as the sample If the 014rst k partitions are used as sample then SSASCP A uses a min supp ort smaller than the one the user sp eci\014ed to determine the lo cal large itemsets for those partitions The rest of the algorithm is just the same as AS-CP A The idea of using a smaller min supp ort for the sample w as prop osed in the Sampling algorithm Ho wev er instead of calculating the negativ e b order of S to c hec k if all large itemsets ha v e b een found at the end of the 014rst scan as in SSAS-CP A simply executes pro cedure se c ond anti skew and c hec ks if there are an y itemsets in C G with age greater than 1 The Sampling algorithm in needs a second c omplete scan o v er D if it 014nds an y large itemsets not in S  SSAS-CP A needs a second p artial scan o v er D if it 014nds an y large itemsets with age greater than 1 There are sev eral adv an tages in SSAS-CP A First the sampling phase of SSAS-CP A is actually part of the 014rst scan and th us it do es not add extra o v erhead lik e the Sampling algorithm Second in some cases a random sample cannot b e dra wn without scanning the database D from the b eginning An example for this kind of database is a database with v arian t length transactions and there is no delimiter b et w een transactions In these cases a random sampling approac h could require an extra scan o v er the database and th us a sequen tial sampling approac h lik e SSAS-CP A is a b etter w a y to go In case a random sample can b e dra wn without to o m uc ho v erhead w e can simply use algorithm AS-CP A but 014rst add all lo cal large 


jD j n um b er of transactions j T j a v erage n um b er of items p er transaction j I j a v erage n um b er of items of maximal p oten tially large itemsets j L j n um b er of maximal p oten tially large itemsets N n um b er of items T able 3 P arameters Name j T j j I j jD j N T5.I2.100K 5 2 100K 1000 T10.I4.100K 10 4 100K 1000 T able 4 P arameter Settings itemsets with age  1 found in the sample to C G  W e denote the random sampling v ersion of AS-CP A as RSAS-CP A The algorithm for RSAS-CP A can b e found in 5  5 P erformance Results W eha v e implemen ted the v arious AS-CP A algorithms and examined their p erformance using a standard set of syn thetic test data 2  Since the second global an ti-sk ew tec hnique detects false candidate itemsets with little o v erhead it is treated as a required part of AS-CP A W e treat the 014rst global an ti-sk ew and the early lo cal pruning tec hniques as the optional parts of AS-CP A W e also apply the early lo cal pruning tec hnique to SPINC and the resulting algorithm is denoted as SPINC-E Belo ww e brie\015y examine some of the results The results of t w o data sets are discussed here T able 3 sho ws the notation for eac h parameter and T able 4 sho ws the name and parameter setting for eac h data set All algorithms in this exp erimen t divides the database in to 5 partitions T ables 5 and 6 sho wn um b er of scans for the v arious algorithms with eac h data set The columns are lab eled with the min supp ort v alue W ew ere not able to actually implemen t the Sampling algorithm as the sampling co de is proprietary sow eha v e sho wn a v alue of 1   This is b ecause the b est b eha vior for the Sampling algorithm is one scan The w orst is 2 The actual n um b er of scans required dep ends on the sampling as w ell as the data In 9  exp erimen ts w ere rep eated 100 times for eac h test database and the n um b er of scans w as rep orted as the a v erage of these The v alues plotted in this w ork w ere sligh tly ab o v e 1 The o v erall p ercen t of retriev als with misses w as 0.0038 When a miss o ccurs the database m ust b e read again completely  This requires another scan In addition the Sampling algorithm requires extra I/O to do the sampling Exp erimen ts done in 9  use a sample size from 20 to 80 of the database Th us coming up with an exact n um b er here is di\016cult As can b e seen though the sequen tial sampling AS-CP A algorithms has ab out the same p erformance as the Sampling algorithm based on n um b er of scans Certainly further w ork with more data and real data rather than syn0.0025 0.0050 0.0075 0.0100 P artition 2 2 2 1 SPINC 1  8 1  8 1  8 1 AS-CP A-2 1  8 1  8 1  2 1 AS-CP A-12 1  8 1  8 1  6 1 SPINC-E 1  8 1  8 1  8 1 AS-CP A-2E 1  8 1  8 1  2 1 AS-CP A-12E 1  8 1  8 1  6 1 SSAS-CP A-2 1 1 1 1 SSAS-CP A-2E 1 1 1 1 Sampling 1  1  1  1  T able 5 Num b er of Scans for T5.I2.D100K 0.0025 0.0050 0.0075 0.0100 P artition 2 2 2 2 SPINC 1  8 1  8 1  8 1  8 AS-CP A-2 1  8 1  8 1  4 1  2 AS-CP A-12 1  8 1  8 1  8 1  2 SPINC-E 1  8 1  8 1  8 1  8 AS-CP A-2E 1  8 1  8 1  4 1  2 AS-CP A-12E 1  8 1  8 1  8 1  2 SSAS-CP A-2 1 1 1 1 SSAS-CP A-2E 1 1 1 1 Sampling 1  1  1  1  T able 6 Num b er of Scans for T10.I4.D100K thetic data will help to determine the b est o v erall approac h The SSAS-CP A algorithms ho w ev er do app ear to b e quite promising Figure 1 sho ws the n um b er of global candidate itemsets generated b y the v arious algorithms with the t w o data sets The early lo cal pruning v ersions of ASCP A are not sho wn in Figure 1 since their n um ber of global candidate itemsets is ab out the same as their coun terpart without early lo cal pruning The early lo cal pruning tec hnique aims at reducing the n um ber of lo cal candidate itemsets not the global candidate itemsets During the 014rst phase scanning the 5 partitions the 014rst time AS-CP A-12 generates the smallest n umb er of global candidate itemset During the second phase AS-CP A-2 has the smallest Certainly since SPINC generates a smaller global candidate itemset size than the regular P artition algorithm our an tisk ew approac hes are also b etter than this Compared with non-sampling algorithms SSAS-CP A-2 has am uc h large n um b er of global candidate itemsets Although not sho wn w e kno w that the Sampling algorithm w ould ha v eam uc h larger n um b er of global candidate itemsets due to the fact that it adds those in the negativ e b order Notice that for these test data SSAS-CP A only had one scan in most case but less candidate itemsets w ere generated Using the n um b er of global candidate itemsets as a p erformance measure allo ws us to estimate CPU and memory utilization of the v arious algorithms Certainly more w ork is needed to in v estigate these further 


        0 200 400 600 800 1000 1200 1400 1600 1800 012345 Number of Candidate Itemsets Partition Number T5.I2.D100K, min_support = 0.0025 SPINC 1234 AS-CPA-12 AS-CPA-2 SSAS-CPA-2         0 5 10 15 20 25 30 35 40 45 50 012345 Number of Candidate Itemsets Partition Number T5.I2.D100K, min_support = 0.0075 AS-CPA-2 SPINC SSAS-CPA-2 1234 AS-CPA-12         0 5000 10000 15000 20000 25000 012345 Number of Candidate Itemsets Partition Number T10.I4.D100K, min_support = 0.0025 AS-CPA-12 AS-CPA-2 SPINC SSAS-CPA-2 1234         0 500 1000 1500 2000 2500 3000 3500 4000 012345 Number of Candidate Itemsets Partition Number T10.I4.D100K, min_support = 0.0050 AS-CPA-12 AS-CPA-2 SPINC SSAS-CPA-2 1234 Figure 1 Num b er of Global Candidate Itemsets 6 Conclusions and F uture W ork In this pap er w e prop ose sev eral tec hniques to sp eed up the pro cess of 014nding asso ciation rules among bask et data Our tec hniques emplo y prior or accum ulated kno wledge of the data pro cessed to prune false candidates at an early stage F uture w ork includes further p erformance studies of AS-CP A early iden ti\014cation of p ossibly large itemsets b efore scanning parallel v ersion of AS-CP A on a share-nothing arc hitecture and rule main tenance with AS-CP A after adding data to or remo ving data from the database References  R Agra w al T Imielinski and A N Sw ami Mining asso ciation rules b et w een sets of items in large databases In Pr o c e e dings of the A CM SIGMOD International Confer enc e on Management of Data  1993  R Agra w al and R Srik an t F ast Algorithms for Mining Asso ciation Rules in Large Databases In Pr o c e e dings of the 20th International Confer enc e on V ery L ar ge Datab ases  1994  M Chen J Han and P S.Y u Data mining An o v erview from a database p ersp ectiv e IEEE T r ansactions on Know le dge and Data Engine ering  8\(6 Decem b er 1996  U M F a yy ad G P  Shapiro P Sm yth and R Uth urusam y  editors A dvanc es in Know le dge Disc overy and Data Mining  AAAI Press 1996  J Lin and M H Dunham Mining asso ciation rules An ti-sk ew algorithms T ec hnical Rep ort 97-CSE-4 Southern Metho dist Univ ersit y  Computer Science Departmen t 1997 A vailable fr om http://www.se as.smu.e du  jun/ic de98.ps.gz   H Mannila H T oiv onen and A I V erk amo E\016cien t algorithms for disco v ering asso ciation rules In AAAI Workshop on Know le dge Disc overy in Datab ases KDD-94  1994  A Mueller F ast sequen tial and parallel algorithms for asso ciation rule mining A comparison T ec hnical Rep ort CS-TR-3515 Dept of Computer Science Univ of Maryland College P ark MD 1995  A Sa v asere E Omiecinski and S B Na v athe An e\016cien t algorithm for mining asso ciation rules in large databases In Pr o c e e dings of the 21st International Confer enc eonV ery L ar ge Datab ases  1995  H T oiv onen Sampling large databases for association rules In Pr o c e e dings of the 22nd International Confer enc eon V ery L ar ge Datab ases  1996  M J Zaki S P arthasarath y  W Li and M Ogihara Ev aluation of sampling for data mining of asso ciation rules T ec hnical Rep ort TR 617 Univ ersit yofRoc hester Computer Science Departmen t 1996 


FIGURE 5 Execution time and rules returned versus minimum coverage for the various algorithms FIGURE 6 Execution time of dense_0002 as minconf is varied for both data-sets. Minimum coverage is fixed at 5% on pums and 1% on connect-4 FIGURE 7 Maximum confidence rule mined from each data-set for a given level of minimum coverage   1 10 100 1000 10000 100000 0 10 20 30 40 50 60 70 80 90 Execution time \(sec Minimum Coverage connect-4 apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 1e+06 0 10 20 30 40 50 60 70 80 90 Number of Rules Minimum Coverage connect-4 apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 0 10 20 30 40 50 60 70 80 90 Execution Time \(sec Minimum Coverage pums apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 1e+06 1e+07 0 10 20 30 40 50 60 70 80 90 Number of Rules Minimum Coverage pums apriori_c  dense_0002   dense_002   dense_02    0 500 1000 1500 2000 2500 3000 3500 20 25 30 35 40 45 50 55 60 65 Execution time \(sec minconf pums  connect-4  1 10 100 1000 10000 100000 1e+06 20 25 30 35 40 45 50 55 60 65 Number of Rules minconf pums  connect-4    0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100 Highest Rule Confidence Minimum Coverage pums  connect-4 


8.2  Effects of minimum confidence The next experiment \(Figure 6\ws the effect of varying minconf while fixing minimp and minsup to very low values. With connect-4, we used a minimum coverage of 1%, and with pums, a minimum coverage of 5%. Minimp was set to .0002 with both data-sets. As can be extrapolated from the previous figures, the number of rules meeting these weak minimp and minsup constraints would be enormous As a result, with these constraints alone, Dense-Miner exceeds the available memory of our machine The efficiency of Dense-Miner when minimum confidence is specified shows that it is effectively exploiting the confidence constraint to prune the set of rules explored. We were unable to use lower settings of minconf than those plotted because of the large number of rules. As minconf is increased beyond the point at which fewer than 100,000 rules are returned, the run-time of Dense-Miner rapidly falls to around 500 seconds on both data-sets 8.3  Summary of experimental findings These experiments demonstrate that Dense-Miner, in contrast to approaches based on finding frequent itemsets achieves good performance on highly dense data even when the input constraints are set conservatively. Minsup can be set low \(which is necessary to find high confidence rules as can minimp and minconf \(if it is set at all\This characteristic of our algorithm is important for the end-user who may not know how to set these parameters properly. Low default values can be automatically specified by the system so that all potentially useful rules are produced. Refinements of the default settings can then be made by the user to tailor this result. In general, the execution time required by Dense-Miner correlates strongly with the number of rules that satisfy all of the specified constraints 9.     Conclusions We have shown how Dense-Miner exploits rule constraints to efficiently mine consequent-constrained rules from large and dense data-sets, even at low supports. Unlike previous approaches, Dense-Miner exploits constraints such as minimum confidence \(or alternatively, minimum lift or conviction\ and a new constraint called minimum improvement during the mining phase. The minimum improvement constraint prunes any rule that does not offer a significant predictive advantage over its proper sub-rules. This increases efficiency of the algorithm, but more importantly it presents the user with a concise set of predictive rules that are easy to comprehend because every condition of each rule strongly contributes to its predictive ability The primary contribution of Dense-Miner with respect to its implementation is its search-space pruning strategy which consists of the three critical components: \(1\functions that allow the algorithm to flexibly compute bounds on confidence, improvement, and support of any rule derivable from a given node in the search tree; \(2\proaches for reusing support information gathered during previous database passes within these functions to allow pruning of nodes before they are processed; and \(3\ item-ordering heuristic that ensures there are plenty of pruning opportunities. In principle, these ideas can be retargeted to exploit other constraints in place of or in addition to those already described We lastly described a rule post-processor that DenseMiner uses to fully enforce the minimum improvement constraint. This post-processor is useful on its own for determining the improvement value of every rule in an arbitrary set of rules, as well as associating with each rule its proper sub-rule with the highest confidence. Improvement can then be used to rank the rules, and the sub-rules used to potentially simplify, generalize, and improve the predictive ability of the original rule set References 1 w a l  R.; Im ie lin ski  T   a n d S w a m i, A. 1 9 9 3   M i n i ng As so ciations between Sets of Items in Massive Databases. In Proc of the 1993 ACM-SIGMOD Int\222l Conf. on Management of Data 207-216 2 raw a l R.; M a n n ila, H Sri k an t  R T o i v o n en  H.; an d  Verkamo, A. I. 1996. Fast Discovery of Association Rules. In Advances in Knowledge Discovery and Data Mining AAAI Press, 307-328 3 K Ma ng a n a r is S a n d Sri k a n t, R 19 97  P a rtia l Cl a ssif i cation using Association Rules. In Proc. of the 3rd Int'l Conference on Knowledge Discovery in Databases and Data Mining 115-118 4 a rd o  R. J 1 9 9 8  Ef f i c i en tly Min i n g  Lo n g  P a ttern s fro m  Databases. In Proc. of the 1998 ACM-SIGMOD Int\222l Conf. on Management of Data 85-93 5  Mi c h ae l J. A a n d  Lin o f f G  S 1 9 9 7  Data Mining Techniques for Marketing, Sales and Customer Support John Wiley & Sons, Inc 6 Bri n, S  M o t w a n i, R.; Ullm a n J.; a n d  Tsu r S. 19 9 7 Dyn a m i c  Itemset Counting and Implication Rules for Market Basket Data. In Proc. of the 1997 ACM-SIGMOD Int\222l Conf. on the Management of Data 255-264 7 h e n  W   W   1 9 9 5 F a st Ef fecti v e Ru le In d u ctio n   In  Proc. of the 12th Int\222l Conf. on Machine Learning 115-123 8 In tern atio n a l Bu sin e s s Mac h in e s   1 9 9 6  IBM Intelligent Miner User\222s Guide Version 1, Release 1 9 m e t tin e n M   Ma nn ila  P  Ro nk a i ne n  P   a n d V e rk a m o  A  I. 1994. Finding Interesting Rules from Large Sets of Discovered Association Rules. In Proc. of the Third Int\222l Conf. on Information and Knowledge Management 401-407 10  Ng   R  T    L a k s hm ana n   V   S    Ha n  J   an d P a ng A  1 9 9 8   Exploratory Mining and Pruning Optimizations of Constrained Association Rules. In Proc of the 1998 ACM-SIGMOD Int\222l Conf. on the Management of Data 13-24 11 Ry mo n  R 1 9 9 2   Search  t h ro u g h Sy s t e m atic S e t En u m era tion. In Proc. of Third Int\222l Conf. on Principles of Knowledge Representation and Reasoning 539-550 1  Sha f e r  J  A g r a w a l R   an d Me ht a M 19 98  SPR I N T   A  Scalable Parallel Classifier for Data-Mining. In Proc. of the 22nd Conf. on Very Large Data-Bases 544-555 13  S m y t he P  and  Go od man   R  M 19 92 An I n f o r m at i o n Th eo retic Approach to Rule Induction from Databases IEEE Transactions on Knowledge and Data Engineering 4\(4\:301316 14  S r i k a n t   R    V u  Q an d Ag r a w a l  R  19 97 M i ni ng  A ssoc i a tion Rules with Item Constraints. In Proc. of the Third Int'l Conf. on Knowledge Discovery in Databases and Data Mining 67-73 15 W e bb, G. I 1 9 9 5 OP U S An Ef f i c i e n t Adm i ssible Algo rit h m for Unordered Search. In Journal of Artificial Intelligence Research 3:431-465 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


