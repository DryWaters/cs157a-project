A Web-Programming Agent Ibrahim F Imam Computer Science Department Arab Academy for Science and Technology Cairo Egypt ifiO5@yahoo.com Abstract This paper presents an application for a webprogramming agent This paper presents an intelligent agent that assists the user to retrieve review and learn knowledge about web programming The agent has the ability to update its knowledge while searching for new information The agent utilizes two methodology for learning evaluates the importance of each term or piece of knowledge it knew and can learn association rules to associate pieces of 
knowledge together The learned associations is used to improve text evaluation Key words intelligent agent information retrieval webprogramming 1 Introduction These days technology is progressing very fast Researchers and Engineers are trying to utilize the discovered technology to maximize the ease and comfort of our life Since the discovery of the field of intelligence agents many applications are aimed for information retrieval These application raniges from mail handler and organizer to planet discoverer This paper presents an agent that can provide the user with daily news or information about a given field or subject The 
agent can learns new knowledge and provide it to the user as well This agent is very useful in many applications including but not limited to news education economy sports etc For example the agent can be used as a sport mediator The agent can searches the internet every day to retrieve updated sport information 2 Related Work In the past information was an expensive thing to get or find Today information is vast and available to almost everyone to the limit that no one can comprehend one tenth of the information available over the internet The 
problem now becomes how to handle this information utilize it for interesting tasks and how to find relevant information The internet provides information about almost every aspect in our life Even though we are discovering more and more knowledge every day the intemet makes it easier to get information about new discovery in no time Researchers are working since the 1 980s on developing systems that Information retrieval has been the concern of many researches since the 1960s 61 9 51  10 The development of information retrival systems requires 0-7803-8294-3/04/$20.00 2512004 IEEE knowledge and research on 
topic identifications 8 and 3 search algorithms 1 and 31 and semantic and syntax analysis 4 Topic identification is to identify the topic of a given text based on background knowledge Search algorithms for information retrival are quite different from search algorithms for data Algorithms ranges from very simple which searches for predefined key words to complex with searches for different words with the same meaning 3 The IIRA-01 Agent The paper presents an agent for retrieving information and learns while accomplishing its task Figure 1 shows a brief description of the different phases of 
the agent The first phase is concerned with teaching the agent about a certain subject The teaching is done through a set of key words related to the subject In the second ph the agent connects to the internet and starts searching for sites related to the subject of interest A site may not be directly relevant however a link inside this site could lead to a very relevant site e.g www.yahoo.com Therefore the agent should search for all sub-links in each site The agent evaluates the sites and saves the related links in a file 
called DNS file This phase can be substituted with a pre-defined DNS or by using a search engine e.g Google Insite Yahoo MSN etc to speed up the process In the third phase the text at each link in the DNS list is evaluated thoroughly to classify the links into relevant and irrelevant The irrelevant links are removed While retrieving and evaluating texts the agent learns in three different ways The agent updates its knowledge by learning the importance of each key word The agent learns new associations among its key words and evaluates the strength of 
these associations These associations are also used to evaluate every new text The agent can present the user with the information it retrieved about the subject of interest Teaching the Agent Traditional ways of teaching the agent the basic knowledge of its task require complex learning systems that can cope with different disciplines of learning 7 Initially we teach the agent a specific subject The agent should be aware of extensive set of vocabulary that covers the main terms and key words of the given subject In this paper we taught our agent the subject of 994 


category Average  final association 4 Networkbi Coding Key word category Figure 2 Key words categories and the sum of their frequency in final associations Table 1 Ex erments of links retrieval No Total Related  No otal lin Related  links links links 1 9 2 22 6 77 35 45 2 27 12 44 7 153 71 46 3 37 18 48 8 216 93 43 4 45 23 51 9 480 1 72 35 5 54 27 50 of Frequencyin 1  Web-Programming To avoid the problem of parsing prefixes and postfixes of words related to WebProgramming the vocabulary included many related words with possible prefixes and/or postfixes The list of key words is ordered ascendingly and stored in a text file Each key word after two days of search It was clear that an exhaustive search requires many days and may be weeks The agent ran in automatic search mode which performs the initial search through a DNS file The DNS file shall contain all links to relevant sites for future use The user may bypass this process and creates his/her own DNS file The agent can also work as a search engine 4.1 Learning Relationship of Classified Key Words When the training phase is done the association rules reflects relationships exists im many relevant texts to the agent domain We classified the key words into four distinct groups and labeled each group coding client/server networking and other The result shows that most Web-programming report focuses on coding key words Figure 2 The experiment can be repeated with different classifications 4.2 Relationship Between Retrieved and Relevant Sites The agent utilizes one of Lycos Goggle or Alta vista to retrieve sites relevant to its domain assuming no DNS 995 is given We ran an experiment to show the relationship between the links Google retrieves for the web programming subject and the links evaluated by the agent as relevant The agent searched for different words related to web programnmiig Table 1 shows 9 experimenits In eacih expcrimitent the total liniks is the number of links retrieved by Google the related links is the number of links evaluated by the agent as relevant to the subject and the percentage of relevant to retrieved links Word category  Word is given an initial weight of 0.5 These weights are updated by the agent to reflect the importance of each key word to Web-Programming This file is called the knowvlecke file The knowledge file is stored in a known folder to the agent to be used daily or as frequently as the agent runs The domain of the subject can be simply changed by removing the old knowledge file and inserting a new knowledge file with key words for a new subject Teaching the agent new knowledge Pages with related text i Figure 1 Illustration of the different phases of the retrieval agent 4 Experiments In this paper we selected the domain of web programming to illustrate the functions of the agent The agent started with 83 key words The agent searched the intemet for relevant sites to web programming We stopped the agent a search engine The agent is guided to use Google In every run Google retrieved tens of pages each page contains about 10 links to sites related to web programming The use of Google saved a significant amount of time The agent evaluates each link and saves the relevant ones into 


This result illustrates the performance of the agent algorithm for evaluating texts The irrelevant links appear due to the existence of common terms between different fields For example many terms are common among web programming networking and programming in general A word like proxy may be found in a text that handles networks design or programming a proxy server The agent overcomes this problem by evaluating each text using the weights of the key words and how many strong associations the text contains For example an association between two words as web and XML with a high weight will indicate that this page is related to the domain But if the word XML appears in the text alone its occurrence will increase which will affect the overall weight of the text 5 Conclusion Tllis paper introduced an application of information retrieval agent The application is concerned with web programming however the agent can perform the same task for any other subject The agents learned a set of key words related to web programming The agent can connect to the interent and search for relevant sites retrieves web pages related to the subject of interest and learns how to improve its finctionalities The agent creates its own Domain Name Server DNS The agent utilizes its DNS in all future usage The agent learns weights for the initial key words and utilizes these weights for evaluating new texts The domain of the agent can simply change by changing the knowledge file of the agent The paper presented an experiment and analysis of the performance of the agent We should in the future build more applications and improve the performance of the agent Environments Proceedings of the 21st International ACM SIGIR Conference on Research and Development in Information Retrieval Melbourne Australia 2 Brin S and Page L 1998 The Anatomy of a Large-Scale Hypertextual Web Search Engine Proceedings of the Seventh World Wide Web Conference WWW7 Brisbane Also in a special issue of the journal Computer Networks and ISDN Systems Volume 30 issues 1-7 3 Chakrabarti S van der Berg M Dom B 1999 Focused crawling a new approach to topic-specific Web resource discovery In Proceedings of 8th International World Wide Web Conference WWW8 Also in Computer Networks  ISDN Systems 31\(1 l-16 1640 4 Kleinberg J 1998 Authoritative Sources in a Hyperlinked Environment Proceedings of the ACMSIAM Symposium on Discrete Algorithms 5 Garfield E 1972 analysis as a tool in journal evaluation Science 178\(4060 1972 6 Garfield E 1955 Citation Indexes for Science A New Dimension in Documentation through Association of Ideas Science 122\(3159 108-111 1955 7 Gordon D.F 2000 Asimovian Adaptive Agents Journal of Artificial Intelligence Research Volume 13 pages 95-153 8 Imam I.F  Kodratoff Y 1996 Intelligent Adaptive Agents Al Magazine Al press September 9 Kessler M.M 1963 Bibliographic Coupling betwden Scientific Papers American Documentation 14\(1 1963 1'0 Small 1-1 1973 Co-citation in scicentific literature A new measure of the relationslhip between two documents Journal of the American Society for Information Science JASIS 24\(4 1973 References 1 Bharat K  Henzinger M.R 1998 Improved Algorithms for Topic Distillation in Hyperlinked 996 


In the follo wing w e presen t buc k eting algorithms that meet b oth of the ab o v e-men tioned requiremen ts  that is they do not compromise the optimalit yofthe optimized sets and ha v e time complexit y O  n  The output of the algorithms is the b buc k ets with their supp orts and con\014dences and this b ecomes the input to our dynamic programming algorithm in Section 4.2 F or optimized supp ort sets con tiguous v alues in the domain of the uninstan tiated attribute for whic h the con\014dence is greater than or equal to minConf are collapsed in to a single buc k et b y our buc k eting algorithm Eac h other v alue j whose con\014dence is less than minConf is assigned to a separate buc k et  the buc k et contains only the v alue j  In other w ords if v alues in in terv al  i j  are assigned to a buc k et then either 1 for all i 024 l 024 j  con\014dence of  l l  is at least minConf or 2 i  j and con\014dence of  i i  i s less than minConf  F or example let the domain of A 1 be f 1  2  6 g and con\014dences of 1 2 5 and 6 b e greater than or equal to minConf  while con\014dences of 3 and 4 b e less than minConf  Then our buc k eting sc heme generates 4 buc k ets the 014rst con taining v alues 1 and 2 the second and third con taining v alues 3 and 4 resp ectiv ely  and the fourth con taining v alues 5 and 6 It is straigh tforw ard to observ e that assigning v alues to buc k ets can b e ac hiev ed b y p erforming a single pass o v er the input data F urthermore the follo wing theorem can b e used to sho w that the ab o v e buc k eting sc heme preserv es the optimalit y of the computed optimized sets Theorem 4.1  L et S b e an optimize d supp ort set Then for any interval  i j  in S  it is the c ase that conf  i 000 1 i 000 1  minConf and conf  j 1 j   minConf  F rom the ab o v e theorem it follo ws that for an y set of con tiguous v alues eac h of whic h has con\014dence at least minConf  either all of the v alues are con tained in the optimized set or none of them are The reason for this is that if the optimized set con tained some of the v alues and not all the v alues then for some in terv al  i j  i n the optimized set either i 000 1or j 1 w ould b e one of the v alues with con\014dence at least minConf th us violating Theorem 4.1 Th us eac hin terv al in the optimized set computed prior to buc k eting either con tains all the v alues assigned to a buc k et or none of them As a result the optimized set can b e computed using the buc k ets instead of the initial domain v alues 4.2 Optimized Supp ort Algorithm In this subsection w e presen t a dynamic programming algorithm for the optimized supp ort problem The input to the algorithm is the b buc k ets generated b y our buc k eting sc heme in Section 4.1 along with their con\014dences and supp orts The problem is to determine a set of at most k non-o v erlapping in terv als suc h that the con\014dence of eac hin terv al is greater than or equal to minConf and supp ort of the set is maximized 4.2.1 In tuition Supp ose optSet i j l  denotes the optimized set for interv al  i j  con taining at most l non-o v erlapping interv als Th us for ev ery in terv al  p q  in the set 1 con\014dence is at least minConf  and 2 i 024 p 024 q 024 j  It is fairly straigh tforw ard to observ e that optSet i i l  for all l  1 k is  i i  if conf  i i  025 minConf  Otherwise it is   Similarly  for i<j  if conf  i j  025 minConf  then optSet i j l  for all l  1 k is  i j  On the other hand if conf  i j   minConf  then optSet for the in terv al b ounded b y i and j can b e computed from the optSet v alues for its subin terv als Theorem 4.2  If for i  j  c onf  i j   minConf  then the optimize d set optSet i j l  is 017 l 1  optSet i j 000 1  1  if sup optSet i j 000 1  1  sup optSet i 1 j 1  optSet i 1 j 1  otherwise 017 l 1  optSet i r  1  optSet r 1 j;l 000 1  wher e i 024 r<j is such that sup optSet i r  1  optSet r 1 j;l 000 1 is maximum The ab o v e theorem enables us to use dynamic programming in order compute optSet[1 b;k  4.2.2 Dynamic Programming Algorithm The dynamic programming algorithm for computing optSet i j l  is as sho wn in Figure 1 Before the algorithm is in v ok ed w e assume that conf  i j  and sup  i j  is precomputed for ev ery in terv al  i j  The arra y optSet is used to store the optimized sets for the v arious in terv als and the en tries of the arra y are initially set to   Pro cedure optSup1D 014rst c hec ks if optSet i j l  has already b een computed b yin v oking the function computed in Step 1 If so then the optimized set stored in optSet i j l  i s returned Otherwise the algorithm calculates optSet i j l  using the results of Theorem 4.2 It 014rst c hec ks the con\014dence of  i j  t o see 


pro cedure optSup1D i j l  b egin 1 if computed optSet i j l   true 2 return optSet i j l  3 if conf  i j  025 minConf f 4 optSet i j l   f  i j  g 5 return optSet i j l  6 g 7 tmpSet   8 if i j f 9 if l 1 10 tmpSet  maxS upS et optSup1D i j 000 1  1 optSup1D i 1 j 1 11 else 12 for r  i to j 000 1 do 13 tmpSet  maxS upS et tmpSet optSup1D i r  1  optSup1D r 1 j;l 000 1 14 g 15 optSet i j l   maxS upS et optSet i j l  tmpSet 16 return optSet i j l  end Figure 1 Algorithm for computing optimized supp ort set whether it is at least minConf  If this is the case then optSet i j l  i s the in terv al  i j  itself since it has the maxim um p ossible supp ort in  i j  Ho w ev er if the con\014dence of  i j  s less than minConf  then the t w o cases are when l  1 and l  1 F or l  1 the interv al w e are in terested m ust b e in either  i j 000  or  i 1 j  The function maxS upS et in Step 10 tak es t w o or more optSets as argumen ts and returns the set with the maxim um supp ort On the other hand if l  1 then one of the in terv als w e are in terested in m ust lie in  i r  and the other l 000 1m ust lie in  r 1 j  for some i 024 r  j  Th us optSet i j l  is set to the union of the optSets for the pair of in terv als with the maxim um supp ort Note that if optSup1D w as initially in v ok ed with parameters 1 b and k  then when Step 12 of the algorithm is executed the v alue of j is alw a ys b  In RS97  w e sho w that the time and space complexit y of our dynamic programming algorithm is O  b 2 k  and O  b 2  bk  resp ectiv ely  4.3 Divide and Conquer The dynamic programming algorithm for computing optimized supp ort sets presen ted in the previous section had time complexit y O  b 2 k  and space complexit y O  b 2  where b is the n um b er of input buc k ets In this section w e prop ose a divide and conquer algorithm that partitions the range consisting of b buc k ets in to subranges and uses the dynamic programming algorithm in order to compute optimized sets for eac h subrange It then com bines the optimized sets for the v arious subranges to deriv e the optimized set for the en tire range of buc k ets Since the input to the dynamic programming algorithm is a subrange whose size is smaller than that of the en tire range the divide and conquer approac h reduces the time and space complexit y of computing the optimized set for the en tire range The result is reduced execution times and memory requiremen ts th us allo wing our algorithms to execute in main-memory  Our exp erimen ts in Section 5 indicate that with the divide and conquer optimization execution times and storage needs of our algorithms increase linearly as opp osed to quadratically with the n um ber of input buc k ets In the follo wing subsections w e 014rst presen t the intuition underlying our divide and conquer approac h W e then presen t a sc heme with linear time complexit y for splitting the range of buc k ets in to subranges  this mak es the sc heme practical ev en for large b and for the case in whic h the supp ort and con\014dence information for the b buc k ets do es not 014t in main-memory  Finally w e sho who w the results for the subranges can b e com bined to yield the optimized supp ort set 4.3.1 In tuition W e 014rst describ e the in tuition underlying the optimization Supp ose for a buc k et p  1 024 p 024 b  it is the case that for ev ery in terv al  i j  con taining p  w eha v e conf  i j   minConf  Then since ev ery interv al in the optimized supp ort set m ust ha v e con\014dence at least minConf  w e can conclude that p do es not o ccur in the optimized set Consequen tly  ev ery in terv al in the optimized set m ust either be to the left of p or to the righ t of p none of the in terv als can span p  W e can th us indep enden tly compute optSet[1 p 000 1 l  and optSet p 1 b;l  for all 1 024 l 024 k  and then set optSet[1 b;k  to be optSet[1 p 000 1 l   optSet p 1 b;k 000 l  for the v alue of l bet w een 0 and k that results in maxim um supp ort optSet[1 p 000 1  0 and optSet p 1 b  are b oth trivially   Since the optimized set m ust ha v e0 024 r 024 k in terv als on the left and at most k 000 r in terv als on the righ tof p b y considering all v alues of l bet w een 0 and k for optSet[1 p 000 1 l   optSet p 1 b;k 000 l  optSet[1 b;k  i s guaran teed to b e an optimized set A generalization of the ab o v e idea is to 014rst compute p artition p oints  a partition p oin t is a buc k et with the prop ert y that ev ery in terv al con taining it has con\014dence less than minConf  Th us no in terv al in 


pro cedure computeP artition b egin 1 partP oin ts   2 earliest  b 1 3 j  b 4 for i  n umE\013ectiv e do wn to 1 do 5 while j 025 e\013ectiv e i  do 6 if conf e\013ectiv e i  j  025 minConf  f 7 earliest  e\013ectiv e i  8 break out of while-lo op 9 g else f 10 if j earliest 11 partP oin ts  partP oin ts f j g 12 j  j 000 1 13 g 14 return partP oin ts end Figure 2 Algorithm for generating partition p oin ts the optimized set can con tain a partition p oin t If p 1 p m 000 1 are the partition p oin ts in increasing order then these partition in terv al 1 b  n to m in terv als 1 p 1 000 1   p i 000 1 1 p i 000 1   p m 000 1 1 b  The m partitioned in terv als ha v e the prop ert y that ev ery in terv al in the optimized set is wholly con tained in a single partition The reason for this is that bet w een an yt w o adjacen t partitions there is a partition p oin t optSet[1 b;k  can then b e computed b y computing for ev ery partition  i j  and for 0 024 l 024 k  optSet i j l  and then c ho osing the com bination of optSets for the v arious partitions that has the maxim um supp ort 4.3.2 Computing P artition P oin ts In Figure 2 w e presen tan O  b  algorithm for computing the partition p oin ts p 1 p m 000 1  The k ey idea underlying the algorithm is as follo ws F or ev ery buc k et j w e 014rst compute the largest in terv al  i j  ending at j and with con\014dence at least minConf  A p oin t p i is a partition p oin tif for all j p i  the largest in terv al ending at j do es not con tain p i and no in terv al ending at p i has con\014dence that exceeds minConf  The largest in terv al with con\014dence minConf for all buc kets can be computed in linear time using recen t results from FMMT96b In FMMT96b the authors in tro duce the notion of e\013e ctive p oints  a buc k et s is e\013ectiv e if for all i  s  conf  i s 000 1  minConf  It is fairly straigh tforw ard to observ e that if  s j  is the largest in terv al with con\014dence exceeding minConf and ending at j  then s m ust be an e\013ectiv e p oin t Also for an e\013ectiv e p oin t s  if  s j  has con\014dence less than minConf  then for ev ery other i<s  conf  i j   minConf  W e are no w in a p osition to describ e ho w pro cedure computeP artition see Figure 2 utilizes the e\013ectiv e p oin ts in order to compute the partition p oin ts In FMMT96b  the authors sho who w e\013ectiv e p oin ts can b e computed in linear time in a single forw ard pass o v er the b buc k ets W e do not rep eat this here and assume that there are n umE\013ectiv e e\013ectiv e p oin ts that are stored in increasing order in the arra y e\013ectiv e The ab o v e-men tioned prop erties of e\013ectiv e p oin ts mak e them useful for e\016cien tly computing the largest interv al ending at buc k et j and with con\014dence at least minConf  only e\013ectiv e p oin ts preceding j need to b e scanned in rev erse order un til one is encoun tered sa y s  for whic h conf  s j  decreases b elo w minConf  Pro cedure computeP artition sim ultaneously scans b oth the input buc k ets as w ell as the e\013ectiv e p oin ts in the rev erse order The v ariable j k eeps trac k of the curren t buc k et b eing scanned while e\013ectiv e i  is the e\013ectiv e p oin t curren tly under consideration Finally  the v ariable earliest stores the earliest e\013ectiv e p oin t suc h that there exists an in terv al with con\014dence minConf b egining at earliest and ending at a buc k et greater than or equal to j  Th us if for buc k et j  j earliest then j is a partitioning p oin t see steps 10 and 11 since no in terv al ending at or after j has con\014dence minConf  When scanning the buc k ets and e\013ectiv e p oin ts in rev erse order for buc k et j  only e\013ectiv e p oin ts preceding it are candidates for the longest in terv al with con\014dence minConf and ending at j  F urthermore if conf e\013ectiv e i  j  025 minConf see Step 6 then w e next consider the e\013ectiv e p oin t immediately b efore it b y decremen ting i b y 1 for the longest in terv al ending at j  Also earliest is set to e\013ectiv e i  On the other hand if conf e\013ectiv e i  j   minConf  then w e simply consider the buc k et preceding j b y decremen ting j in Step 12 since the earliest buc k et for the longest in terv al with con\014dence minConf ending at j cannot b e b efore earliest if there are e\013ectiv e p oin ts bet w een e\013ectiv e i  and j  earliest stores the e\013ectiv e p oin t immediately follo wing e\013ectiv e i  The algorithm p erforms t w o passes o v er the data  one forw ard pass to compute the e\013ectiv e p oin ts and a rev erse pass during whic h the partition p oin ts are computed Th us the algorithm is e\016cien t and can b e used ev en if the data is to o large to 014t in main-memory  


pro cedure divideConquerSup b egin 1 for i  1 to m do f 2 optSetP art i    3 for l  1 to k do 4 optSetP art i l    optSup1D lower  i   upper  i  l  5 g 6 for i  0 to k do 7 optSet i    optSetP art[1 i  8 for i  2 to m do 9 for j  k do wn to 1 do 10 for p  0 to j do f 11 tmpSet  optSet j 000 p   optSetP art i p  12 if sup tmpSet  sup optSet j  then 13 optSet j    tmpSet 14 g 15 return optSet k  end Figure 3 Divide and conquer algorithm for optimized supp ort 4.3.3 Divide and Conquer Algorithm Once the m partitions are generated from the partitioning p oin ts then pro cedure divideConquerSup see Figure 3 can b e used to compute the optimized supp ort set Let lower  i  and upper  i  b e the b oundary buc k ets for partition i  and let b i be the n um ber of buc k ets in partition i  First in steps 1 through 5 the optimized set for eac h partition con taining at most 0   k in terv als is computed using optSup1D Pro cedure optSup1D i j l  returns the optimized supp ort set whose size is at most l for the in terv al consisting of buc k ets i through j  The optimized set of size l for partition i is stored in optSetP art i l  Ev en though in the for-lo op in steps 3-4 optSup1D is in v ok ed k times with the n um berofin terv als ranging from 1 to k  the complexit yof the for-lo op is still O  b 2 i k  b ecause the optimized sets for the v arious in terv als in partition i that are computed during an in v o cation of optSup1D can b e stored and shared b et w een the k in v o cations of optSup1D for the partition Th us for an arbitrary interv al in the partition and a maxim um size l for the optimized set the optimized set is computed only once the 014rst time it is required The optimized sets computed for the v arious partitions are then merged in steps 6{14 The merging process is carried out in successiv e steps  in step i  the optimized set for partition i is merged with the result of the merge of partitions 1   i 000 1 that is stored in optSet Th us at the end of step i  optSet j  stores the optimized set con taining at most j in terv als b elonging partitions 1 i  W e need to compute optSet j  for all 1 024 j 024 k since the optimized set con taining at most k in terv als for the 014rst i 1 partitions is obtained b y com bining during step i  1 optSet j  and optSetP art i 1 k 000 j  for the v alue of j that causes optSet k  to be maximized The complexit y of eac h step of the merge is th us O  k 2  since w e need to compute optSet for all v alues b et w een 1 and k  Since the n um b er of partitions is m  the complexit y of steps 6 14 b ecomes O  mk 2  Th us the o v erall complexit y of the algorithm b ecomes O  b 2 1    b 2 m  k  mk 2  where b 025 b 1    b m  If b max denotes the largest v alue among b 1 b m  then the complexit y b ecomes O  b 2 max mk  mk 2  In our exp erimen ts w e found that for a large n um ber of cases b max  b and since k  b  the divide and conquer results in substantial reductions in the computational complexit y of our dynamic programming algorithm whose original complexit y is O  b 2 k  In addition it also reduces the storage and memory requiremen ts of our dynamic programming algorithm from O  b 2 o O  b 2 max  5 Exp erimen tal Results In this section w e study the p erformance of our algorithms for computing optimized supp ort sets for the one attribute case In particular w e sho w that the buc k eting and divide and conquer optmizations mak e our dynamic programming algorithm highly scaleable F or instance w e can tac kle domains of sizes as high as 100,000 in a few seconds F urthermore mining as man y as 250 in terv als for 100,000 domain v alues can b e ac hiev ed in a matter of a few min utes W e also study the sensitivit y of the ab o v e t w o optimizations to the minim um con\014dence threshold In our exp erimen ts the data 014le is read only once at the b eginning of eac h algorithm in order to compute the supp ort and con\014dence for ev ery p oin t The time for this in most cases constitutes a tin y fraction of the total execution time of our algorithms Th us w e do not include the time sp en t on reading the data 014le in our results F urthermore note that the p erformance of our algorithms do es not dep end on the n um ber of tuples in the data 014le  it is more sensitiv e to the size of the attribute's domain n and the n um ber of in terv als k  Our exp erimen ts w ere p erformed on a Sun Ultra-2/200 mac hine with 512 MB of RAM and running Solaris 2.5 Syn thetic Datasets The asso ciation rule that w e exp erimen ted with has the form U  C 1  C 2 where U con tains 1 uninstan tiated attribute see Section 3 whose domain consists of in tegers ranging from 1 to n  


 0.01 0.1 1 10 100 1000 10000 1000 10000 100000 Execution time \(s Size of domain \(n minConf-0.75, k-50 No optimizations Only bucketing Bucketing, Divide & conquer a Scale-up with n n b m b max 500 195 80 6 1000 354 140 8 2000 738 293 8 5000 1829 743 10 7500 2759 1123 8 10000 3711 1518 12 50000 16321 6847 10 100000 32266 13512 12 b V alues of b  m and b max for di\013eren t input sizes Figure 4 V arying input size Ev ery domain v alue that is p oin t in one-dimensional space is assigned a randomly generated con\014dence b et w een 0 and 1 with uniform distribution Eac hv alue is also assigned a randomly generated supp ort bet w een 0 and 2 n with uniform distribution th us the a v erage supp ort for a v alue is 1 n  5.1 Buc k eting and Divide and Conquer In this subsection w e study the impro v emen ts in execution times that result due to the buc k eting and divide and conquer optimizations In Figure 4\(a w e plot the p erformance of three v arian ts of our algorithm as the domain size is increased from 500 to 100,000  1 with no optimizations 2 with only buc k eting and 3 with b oth buc k eting and divide and conquer W e use a log scale to represen tv alues along b oth axes Also in our exp erimen ts w e 014x the n um berofin terv als k at 50 and use a minim um con\014dence threshold of 0.75 F or optimized supp ort w e found that with no optimizations our dynamic programming algorithm to ok times excessiv eof 30 min utes for as few as 5000 v alues in the domains of the attributes On the other hand with b oth buc k eting and divide and conquer our algorithm to ok less than 15 seconds for domain sizes as high as 100,000 F rom the graphs it follo ws that the ma jor p ortion of the p erformance impro v emen t results due to divide and conquer Ev en though buc k eting do es reduce input size these reductions are fairly small for optimized supp ort ab out 5-6 Divide and conquer on the other hand partitions the original problem of size b in to m subproblems of size at most b max and has complexit y O  b 2 max mk  mk 2  F or n  100  000 and the optimized supp ort case divide and conquer splits the b  95000 buc k ets in to m  13000 partitions eac h of whose size is less than b max  20 Ob viously  since b max  b  k<<b and m<b  it follo ws that the computational complexit yof divide and conquer is m uc h smaller than O  b 2 k  the complexit y with only buc k eting The e\013ectiv eness of buc k eting and divide and conquer dep end on minConf v alues W e discuss this in more detail in Section 5.3 5.2 Scale-up with n Figure 4\(a also sho ws ho w the algorithms with and without optimizations scale with the input size n  Due to the quadratic complexit y of our algorithm w e found that as n doubles without optimizations the execution time increases almost four-fold This is in line with what w e exp ected The same holds when w e use only the buc k eting optimization With the divide and conquer strategy  the complexit y of our algorithm is O  b 2 max mk  mk 2  where m is the n um b er of partitions and b max is the size of the largest partition Th us if with increasing n  b max sta ys appro ximately the same and m increases linearly with n  then w e can exp ect the divide and conquer algorithm to exhibit a linear scale-up with increasing n  This is exactly what w e observ e for the optimized supp ort case The table in Figure 4\(b con tains the v alues of m and b max for increasing v alues of n  F or optimized supp ort b max sta ys in a narro w range b et w een 15 and 20 F urthermore m increases almost linearly with resp ect to n from 80 for n  500 to 13500 for n  100  000 Th us as n increases the cost of computing the optimized sets for eac h partition sta ys appro ximately the same and the algorithm only incurs a linear increase in the cost for computing optimized sets for all the partitions and then com bining them 5.3 Sensitivit y to minConf W e next study the senstivit y of our buc k eting and divide and conquer optimizations to minim um con\014dence v alues In Figure 5 for n  5000 and k  50 w e plot execution times for our algorithms as minConf is v aried from 0.2 to 0.9 Figure 5\(a plots the p erformance of our algorithms with and without optimizations for computing optimized supp ort sets First w e 014nd that without an y optimizations the p erformance of our dynamic program 


 0.1 1 10 100 1000 10000 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Execution time \(s Minimum confidence \(minConf n-5000, k - 50 No optimization Only bucketing Bucketing, Divide & conquer a Sensitivit yto minConf minConf b m b max 0.2 1620 1 1619 0.3 2166 1 2165 0.4 2489 1 2488 0.45 2553 1 2552 0.5 2539 1 2538 0.55 2451 136 172 0.6 2355 395 70 0.7 2063 708 14 0.8 1537 686 8 0.9 829 399 2 b V alues of b  m and b max for di\013eren t con\014dences Figure 5 V arying minConf ming algorithm b ecomes w orse as con\014dence increases The reason for this is that for optimized supp ort if the con\014dence of an in terv al is minConf or higher the optimized set for the in terv al is the in terv al itself and further recursiv e calls for the in terv al are not needed When minConf is small there are a large n um ber of suc hin terv als with con\014dence at least minConf  W e next turn our atten tion to the buc k eting and divide and conquer optimizations In the table in Figure 5\(b for a giv en con\014dence w e presen t the n um ber of buc k ets generated b y the buc k eting algorithm b  the n um b er of partitions generated b y the divide and conquer algorithm m  and the max size of a partition b max  F rom the table w e can mak e the follo wing three observ ations  1 as the con\014dence increases the n um ber of buc k ets input to our algorithm increases 2 for v alues of minConf that are lo w er than 0.5 the divide and conquer optimization has no e\013ect and 3 as the con\014dence increases b ey ond 0.5 for increasing con\014dence v alues the max partition size decreases The reason for P oin t 1 is that the buc k eting algorithm coalesces con tiguous v alues with con\014dence more than minConf and at lo w er minConf v alues there are a larger n um ber   0 20 40 60 80 100 120 140 160 180 50 100 150 200 250 Execution time \(s Size of optimized set \(k n-100000, minConf - 0.75, Bucketing, Divide & conquer optSup.75 Figure 6 Scale-up with k of suc hv alues P oin ts 2 an 3 can b e attributed to the fact that the n um b er of partition p oin ts increases as the con\014dence increases and the n um b er is 0 when minConf is less than 0.5 The ab o v e three p oin ts completely explain the b eha viour of our optimizations in Figure 5\(a With only buc k eting at lo w er minConf v alues due to the smaller n um berofbuc k ets and consequen tly the smaller input sizes the p erformance of our algorithm with buc k eting is m uc h b etter than without buc k eting Ho w ev er as the con\014dence increases the reductions in input size b ecome smaller and smaller and th us the buc k eting optimization has v ery little effect for large con\014dence v alues The p erformance of our algorithm with b oth buc k eting and divide and conquer enhancemen ts is more in teresting First since for con\014dence v alues b elo w 0.5 divide and conquer do es not generate an y partitions the p erformance of our algorithm with and without divide and conquer is the same Ho w ev er b ey ond 0.5 divide and conquer kic ks in and the execution times decrease as con\014dence increases  the smaller sizes of partitions is primarily resp onsible for this 5.4 Scale-up with k In order to determine ho w our algorithm with b oth the buc k eting and divide and conquer optimizations scales for increasing v alues of k w ev aried k bet w een 10 and 250 with a domain size of 100  000 and minConf  0.75 The results of our exp erimen ts for the optimized supp ort case are as sho wn in Figure 6 F rom the graphs it follo ws that the execution times increase sligh tly more than linearly as k is increased The reason for this is that the complexit y of our divide and conquer algorithm is O  b 2 max mk  mk 2  The 014rst term whic h is the complexit y of computing optmized sets for the m partitions increases linearly with k  Ho w ev er the second term whic h is the cost of com bin 


ing the results for the partitions to get the 014nal optimized set has complexit y that is quadratic in k  Th us w e 014nd that the increase in execution times for our algorithms is somewhere b et w een linear and quadratic for increasing k  On an a v erage w e found that computation times increase ab out 3-fold ev ery time k doubles Note that for v alues of k m uc h larger than b max  a ma jorit y of the time is sp en tincom bining the results for the v arious partitions Th us subsequen t increases in the v alue of k could result in quadratic increases in execution times when the divide and conquer optimization is emplo y ed 6 Concluding Remarks In this pap er w e generalized the optimized supp ort asso ciation rule problem b y p ermitting rules to contain upto k disjunctions o v er one uninstan tiated n umeric attribute W e presen ted a dynamic pr o gr amming algorithm for computing the optimized asso ciation rule and whose complexit yis O  n 2 k  where n is the n um ber of v alues in the domain of the uninstantiated attribute W e also presen ted t w o optimizations that signi\014can tly impro v e the computation time and memory requiremen ts of our algorithm The 014rst is a buc k eting algorithm that coalesces con tiguous v alues  all of whic hha v e con\014dence either greater than the minim um sp eci\014ed con\014dence or less than the minim um con\014dence The second is a divide and conquer strategy that enables us to split the original problem in to m ultiple smaller subproblems and then com bine the solutions for the subproblems W e exp erimen tally sho w ed that the t w o optimizations enable our dynamic programming algorithms to execute in main-memory for large v alues of the domain size n  With the optimizations our algorithms scale almost linearly with b oth the domain size n and the n um b er of disjunctions k  Ac kno wledgemen ts W ew ould lik e to thank Narain Gehani Hank Korth and Avi Silb ersc hatz for their encouragemen t Without the supp ort of Y eso ok Shim it w ould ha v e b een imp ossible to complete this w ork References AIS93 Rak esh Agra w al T omasz Imielinski and Arun Sw ami Mining asso ciation rules bet w een sets of items in large databases In Pr o c of the A CM SIGMOD Conferenc e on Management of Data  pages 207 216 W ashington D.C Ma y 1993 AS94 Rak esh Agra w al and Ramakrishnan Srik an t F ast algorithms for mining asso ciation rules In Pr o c of the VLDB Confer enc e  San tiago Chile Septem ber 1994 FMMT96a T ak eshi F ukuda Y asuhik o Morimoto Shinic hi Morishita and T ak esh T okuy ama Data mining using t w o-dimensional optimized asso ciation rules Sc heme algorithms and visualization In Pr o c of the A CM SIGMOD Confer enc e on Management of Data  June 1996 FMMT96b T ak eshi F ukuda Y asuhik o Morimoto Shinic hi Morishita and T ak esh T okuy ama Mining optimized asso ciation rules for n umeric attributes In Pr o c of the A CM SIGA CTSIGMOD-SIGAR T Symp osium on Principles of Datab ase Systems  June 1996 ORS98 B Ozden S Ramasw am y  and A Silb ersc hatz Cyclic asso ciation rules In Int'l Confer enc e on Data Engine ering  Orlando 1998 PCY95 Jong So o P ark Ming-Sy an Chen and Philip S Y u An e\013ectiv e hash based algorithm for mining asso ciation rules In Pr o c of the A CM-SIGMOD Confer enc e on Management of Data  San Jose California Ma y 1995 RS97 R Rastogi and K Shim Mining optimized asso ciation rules for n umeric attributes T ec hnical Rep ort 0112370971110-25 Bell Lab oratories Murra y Hill 1997 RS98 R Rastogi and K Shim Mining optimized asso ciation rule for categorical and n umeric attributes In Int'l Confer enc e on Data Engine ering  Orlando 1998 SON95 A Sa v asere E Omiecinski and S Nav athe An e\016cien t algorithm for mining asso ciation rules in large databases In Pr o c of the VLDB Confer enc e  Zuric h Switzerland Septem b er 1995 


 s_suppkey s_nationkey ps_partkey ps_suppkey ps_supplycost p_partkey p_name   l_partkey l_discount l_quantity l_orderkey l_suppkey l_extendedprice o_orderkey o_orderdate n_nationkey n_name p_partkey p_name   246\262 1 2 3 4 5 7 6 8 9 10,#11,#12,#13 14 15 16 17 18 1 2 Figure 11 Execution plan of TPC-D query 9 for transposed files 2 0 20 40 60 80 100 0 50 100 150 200 Time [s CPUusage NetSend NetRecv Disk 10 8 6 4 0 Throughput [MB/s CPU usage 8 9 10 13 Figure 12 Execution trace of TPC-D query 9 with transposed files 11 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


of I/O But the resulting speedup compared with the previous plans exceeds 2 which is quite satisfactory Table 3 shows the results of above right-deep rd left-deep ld and transposed file tp methods along with the reported results of other commercial systems for 100GB TPC-D query 9 Because our system lacks the software and maintenance price metrics the overall system price can\220t be determined accurately Hardware components themselves cost less than 0.5M We can observe that our system achieves fairly good performance Above all the execution time with the transposed files is twelve times as short as the most powerful commercial platform These results strongly support the effectiveness of the commodity PC based massively parallel relational database servers  System Exec Time Price Teradata on NCR 5100M 160 000 133MHz Pentium 20GB Main Memory 400 Disk Drives 953.3 17M Oracle 7 n DEC AlphaServer 8400 12 000 437MHz DECchip 21164 24GB Main Memory 84 Disk Drives 1884.9 1.3M Oracle 7 n SUN UE6000 24 000 167MHz UltraSPARC 5.3GB Main Memory 300 Disk Drives 2639.3 2.1M IBM DB2 PE on RS/6000 SP 306 96 000 112MHz PowerPC 604 24GB Main Memory 96 Disk Drives 2899.4 3.7M Oracle 7 n HP9000 EPS30 12 000 120MHz PA7150 3.75GB Main Memory 320 Disk Drives 7154.8 2.2M Our Pilot System 100 000 200MHz Pentium Pros 6.4GB Main Memory 100 Disk Drives rd 193.7 ld 177.2 tp 77.1 see text Table 3 Execution time of 100 GB TPC-D Q9 on several systems 12 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


4 Data mining 4.1 Association rule mining Data mining which is a recent hot research topic in the database field is a method of discovering useful information such as rules and previously unknown patterns existing behind data items It enables more effective utilization of transaction log data which have been just archived and abandoned Among the major applications of data mining is association rule mining so called 217\217basket analysis.\220\220 Each of the transaction data typically consists of a set of items bought in a transaction By analyzing them one can derive some association rule such as 217\21790 of the customers who buy both A and B also buy C.\220\220 In order to improve the quality of obtained rules a very large amount of transaction data have to be examined requiring quite a long time to complete First we introduce some basic concepts of association rule Let 000 000 001 000 1 001\000 2 001\002\002\002\001\000 000 002 be a set of items and 003 000 001 003 1 001\003 2 001\002\002\002\001\003 001 002 be a set of transactions where each transaction 003 002 is a set of items such that 003 002 004\000  n itemset 004 has support 005 in the transaction set 003 if 005  f transactions in 003 contain 004  here we denote 005 000 005\006\007\007\b\t 003 001 004 002 An association rule is an implication of the form 004 005 n  where 004\001 n 004 000  and 004 006 n 000 007  Each rule has two measures of value support and confidence  The support of the rule 004 005 n is 005\006\007\007\b\t 003 001 004 b n 002  The confidence 013 of the rule 004 005 n in the transaction set 003 means 013 of transactions in 003 that contain 004 also contain n  which can be written as 005\006\007\007\b\t 003 001 004 b n 002 f\005\006\007\007\b\t 003 001 004 002  For example let r 1 000 001 1 001 3 001 4 002  r 2 000 001 1 001 2 001 3 001 5 002  r 3 000 001 2 001 4 002  r 4 000 001 1 001 2 002  r 5 000 001 1 001 3 001 5 002 be the transaction database Let minimum  support and minimum confidence be 60 and 70 respectively First all itemsets that have support above the minimum support called large itemsets  are generated In this case the large itemsets are 001 1 002 001 001 2 002 001 001 3 002 001 001 1 001 3 002  Then for each large itemset 004  n association rule 004 t n 005 n 001 n 004 004 002 is derived if 005\006\007\007\b\t 003 001 004 002 f\005\006\007\007\b\t 003 001 004 t n 002 n minimum confidence  The results are 1 005 3 001 005\006\007\007\b\t 003 000 60 001 b\016\017 000\020\021\016\013\021 000 75 002 and 3 005 1 001 005\006\007\007\b\t 003 000 60 001 013\b\016\017 000\020\021\016\013\021 000 100 002  The most well known algorithm for association rule mining is the Apriori algorithm[1 We have studied several parallel algorithms for mining association based on Apriori One of these algorithms called HPA Hash Partitioned Apriori is discussed here Apriori first generates candidate itemsets and then scans the transaction database to determine whether each of the candidates satisfies the user specified minimum support and minimum confidence Using these results the next candidate itemsets are generated This continues until no itemset satisfies the minimum support and confidence The most naive parallelization of Apriori would copy the candidates over all the processing node and make each processing node scan the transaction database in parallel Although this works fine when the number of candidates is small enough to fit in the local memory of a single processing node memory space utilization efficiency of this method is very poor For large scale data mining the storage required for the candidates exceeds the available memory space of a processing node This causes memory overflow which results in significant performance degradation due to an excessive amount of extra I/Os HPA partitions the candidate itemsets among the processing nodes using a hash function as in the parallel hash join which eliminates broadcasting of all the transaction data and can reduce the comparison workload significantly Hence HPA works much better than the naive parallelization for large scale data mining The 022 th iteration pass 022  f the algorithm is as follows 1 Generate the candidate itemsets Each processing node generates new candidate itemsets from the large itemsets of the last  001 022 t 1 002 th iteration Each of the former itemsets contains 022 items while each of the latter itemsets contains 001 022 t 1 002 items They are called 022 itemsets and 001 022 t 1 002 itemsets respectively The processing node applies the hash function to each of the candidates to determine the destination node ID If the candidate is for the processing node itself it is inserted into the hash table otherwise it is discarded 13 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


 30 40 50 60 70 80 90 100 110 120 30 40 50 60 70 80 90 10 0 Execution Time [s Number of Nodes Figure 13 Execution time of HPA program pass 2 on PC cluster 2 Scan the transaction database and count the support count Each processing node reads the transaction database from its local disk 000 itemsets are generated from that transaction and the same hash function used in phase 1 s applied to each of them Each of the 000 itemsets is sent to certain processing node according the hash value For the itemsets received from the other nodes and those locally generated whose ID equals the node\220s ID the hash table is searched If hit its support count value is incremented 3 Determine the large itemset After reading all the transaction data each processing node can individually determine whether each candidate 000 itemset satisfy user-specified minimum support or not Each processing node sends large 000 itemsets to the coordinator where all the large 000 itemsets are gathered 4 Check the terminal condition If the large 000 itemsets are empty the algorithm terminates Otherwise the coordinator broadcasts large 000 itemsets to all the processing nodes and the algorithm enters the next iteration 4.2 Performance evaluation of HPA algorithm The HPA program explained above is implemented on our PC cluster Each node of the cluster has a transaction data file on its own hard disk Transaction data is produced using data generation program developed by Agrawal designating some parameters such as the number of transaction the number of different items and so on The produced data is divided by the number of nodes and copied to each node\220s hard disk The parameters used in the evaluation is as follows The number of transaction is 5,000,000 the number of different items is 5000 and minimum support is 0.7 The size of the data is about 400MBytes in total The message block size is set to be 16KBytes according to the results of communication characteristics of PC clusters discussed in previous section The disk I/O block size is 64KBytes which seems to be most suitable value for the system Note that the number of candidate itemset in pass 2 s substantially larger than for the other passes which relatively frequently occurs in association rules mining Therefore we have been careful to parallelize the program effectively especially in pass 2 so that unnecessary itemsets to count should not be generated 14 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


The execution time of the HPA program pass 2 is shown in figure 13 as the number of PCs is changed The maximum number of PCs used in this evaluation is 100 Reasonably good speedup is achieved in this application as the number of PCs is increased 5 Conclusion In this paper we presented performance evaluation of parallel database processing on an ATM connected 100 node PC cluster system The latest PCs enabled us to obtain over 110Mbps throughput in point-to-point communication on a 155Mbps ATM network even with the so-called 217\217heavy\220\220 TCP/IP This greatly helped in developing the system in a short period since we were absorbed in fixing many other problems Massively parallel computers now tend to be used in business applications as well as the conventional scientific computation Two major business applications decision support query processing and data mining were picked up and executed on the PC cluster The query processing environment was built using the results of our previous research the super database computer SDC project Performace evaluation results with a query of the standard TPC-D benchmark showed that our system achieved superior performance especially when transposed file organization was employed As for data mining we developed a parallel algorithm for mining association rules and implemented it on the PC cluster By utilizing aggregate memory of the system efficiently the system showed good speedup characteristics as the number of nodes increased The good price/performance ratio makes PC clusters very attractive and promising for parallel database processing applications All these facts support the effectiveness of the commodity PC based massively parallel database servers Acknowledgment This project is supported by NEDO New Energy and Industrial Technology Development Organization in Japan Hitachi Ltd technically helped us extensively for ATM related issues References  R Agrawal T Imielinski and A Swami Mining association rules between sets of items in large databases In Proceedings of ACM SIGMOD International Conference on Management of Data  pages 207--216 1993  R Agrawal and R Srikant Fast algorithms for mining association rules In Proceedings of International Conference on Very Large Data Bases  1994  A C Arpaci-Dusseau R H Arpaci-Dusseau D E Culler J M Hellerstein and D A Patterson High-performance sorting on Networks of Workstations In Proceedings of International Conference on Management of Data  pages 243--254 1997  D.S Batory On searching transposed files ACM TODS  4\(4 1979  P.A Boncz W Quak and M.L Kersten Monet and its geographical extensions A novel approach to high performance GIS processing In Proceedings of International Conference on Extending Database Technology  pages 147--166 1996 15 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


 R Carter and J Laroco Commodity clusters Performance comparison between PC\220s and workstations In Proceedings of IEEE International Symposium on High Performance Distributed Computing  pages 292--304 1995  D.J DeWitt and J Gray Parallel database systems  The future of high performance database systems Communications of the ACM  35\(6 1992  J Gray editor The Benchmark Handbook for Database and Transaction Processing Systems  Morgan Kaufmann Publishers 2nd edition 1993  J Heinanen Multiprotocol encapsulation over ATM adaptation layer 5 Technical Report RFC1483 1993  M Kitsuregawa M Nakano and M Takagi Query execution for large relations on Functional Disk System In Proceedings of International Conference on Data Engineering  5th pages 159--167 IEEE 1989  M Kitsuregawa and Y Ogawa Bucket Spreading Parallel Hash:A new parallel hash join method with robustness for data skew in Super Database Computer SDC In Proceedings of International Conference on Very Large Data Bases  16th pages 210--221 1990  M Laubach Classical IP and ARP over ATM Technical Report RFC1577 1994  D.A Schneider and D.J DeWitt Tradeoffs in processing complex join queries via hashing in multiprocessor database machines In Proceedings of International Conference on Very Large Data Bases  16th pages 469--480 1990  T Shintani and M Kitsuregawa Hash based parallel algorithms for mining association rules In Proceedings of IEEE International Conference on Parallel and Distributed Information Systems  pages 19--30 1996  T Sterling D Saverese D.J Becker B Fryxell and K Olson Communication overhead for space science applications on the Beowulf parallel workstaion In Proceedings of International Symposium on High Performance Distributed Computing  pages 23--30 1995  T Tamura M Nakamura M Kitsuregawa and Y Ogawa Implementation and performance evaluation of the parallel relational database server SDC-II In Proceedings of International Conference on Parallel Processing  25th pages I--212--I--221 1996  TPC TPC Benchmark 000\001 D Decision Support Standard Specification Revision 1.1 Transaction Processing Performance Council 1995 16 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


In accordance with 1910.97 and 1910.209 warning signs are required in microwave areas For work involving power line carrier systems this work is to be conducted according to requirements for work on energized lines Comments s APPA objects to the absolute requirement implied by the word ensure regarding exposure to microwave radiation and recommends revision of s l iii to read when an employee works in an area where electromagnetic radiation levels could exceed the levels specified in the radiation protection guide the employer shall institute measures designed to protect employees from accidental exposure to radiation levels greater than those permitted by that guide  I1 an employee must be stationed at the remote end of the rodding operation Before moving an energized cable it must be inspected for defects which might lead to a fault To prevent accidents from working on the wrong cable would require identification of the correct cable when multiple cables are present Would prohibit an employee from working in a manhole with an energized cable with a defect that could lead to a fault However if the cable cannot be deenergized while another cable is out employees may enter the manhole but must protect against failure by some means for example using a ballistics blanket wrapped around cable Requires bonding around opening in metal sheath while working on cable Underaround EIectrical Installations t Comments t This paragraph addresses safety for underground vaults and manholes The following requirements are contained in this section Ladders must be used in manholes and vaults greater than four feet deep and climbing on cables and hangers in these vaults is prohibited Equipment used to lower materials and tools in manholes must be capable of supporting the weight and should be checked for defects before use An employee in a manhole must have an attendant in the immediate vicinity with facilities greater than 250 volts energized An employee working alone is permitted to enter briefly for inspection housekeeping taking readings or similar assuming work could be done safely Duct rods must be inserted in the direction presenting the least hazard to employees and APPA recommends that OSHA rewrite section 7\regarding working with defective cables This rewrite would include the words shall be given a thorough inspection and a determination made as to whether they represent a hazard to personnel or representative of an impending fault As in Subsection \(e EEI proposes the addition of wording to cover training of employees in emergency rescue procedures and for providing and maintaining rescue equipment Substations U This paragraph covers work performed in substations and contains the following requirements Requires that enough space be provided around electrical equipment to allow ready and safe access for operation and maintenance of equipment OSHA's position A2-16 


is that this requirement is sufficiently performance oriented to meet the requirements for old installations according to the 1987 NEW Requires draw-out circuit breakers to be inserted and removed while in the open position and that if the design permits the control circuits be rendered inoperative while breakers are being inserted and removed stated in the Rules and requests that existing installations not be required to be modified to meet NESC APPA recommends that Section u 4 i which includes requirements for enclosing electric conductors and equipment to minimize unauthorized access to such equipment be modified to refer to only those areas which are accessible to the public Requires conductive fences around substations to be grounded Power Generation v Addresses guarding of energized parts  Fences screens, partitions or walls This section provides additional requirements and related work practices for power generating plants  Entrances locked or attended Special Conditions w  Warning signs posted  Live parts greater than 150 volts to be guarded or isolated by location or be insulated  Enclosures are to be according to the 1987 NESC Sections llOA and 124A1 and in 1993 NESC  Requires guarding of live parts except during an operation and maintenance function when guards are removed barriers must be installed to prevent employees in the area from contacting exposed live parts Requires employees who do not work regularly at the substation to report their presence Requires information to be communicated to employees during job briefings in accordance with Section \(c of the Rules Comments U APPA and EEI provide comments as follows Both believe that some older substations \(and power plants would not meet NESC as This paragraph proposes special conditions that are encountered during electric power generation, transmission and distribution work including the following Capacitors  Requires individual units in a rack to be short circuited and the rack grounded  Require lines with capacitors connected to be short circuited before being considered deenergized Current transformer secondaries may not be opened while energized and must be bridged if the CT circuit is opened Series street lighting circuits with open circuit voltages greater than 600 volts must be worked in accordance with Section q\or t and the series loop may be opened only after the source transformer is deenergized and isolated or after the loop is bridged to avoid open circuit condition Sufficient artificial light must be provided where insufficient naturals illumination is present to enable employee to work safely A2-17 


US Coast Guard approved personal floatation devices must be supplied and inspected where employees are engaged in work where there is danger of drowning Required employee protection in public work areas to include the following  Warning signs or flags and other traffic control devices  Barricades for additional protection to employees  Barricades around excavated areas  Warning lights at night prominently displayed Lines or equipment which may be sub to backfeed from cogeneration or other sources are to be worked as energized in accordance with the applicable paragraphs of the Rules Comments w APPA submits the following comments regarding this Special Conditions section Recommends that the wording regarding capacitors be modified to include a waiting period for five minutes prior to short circuiting and grounding in accordance with industry standards for discharging of capacitors For series street light circuits, recommends that language be added for bridging to either install a bypass conductor or by placement of grounds so that work occurs between the grounds Recommends modification of the section regarding personal floatation devices to not apply to work sites near fountains decorative ponds swimming pools or other bodies of water on residential and commercial property Definitions x This section of the proposed Rules includes definitions of terms Definitions particularly pertinent to understanding the proposal and which have not previously been included are listed as follows Authorized Employee  an employee to whom the authority and responsibility to perform a specific assignment has been given by the employer who can demonstrate by experience or training the ability to recognize potentially hazardous energy and its potential impact on the work place conditions and who has the knowledge to implement adequate methods and means for the control and isolation of such energy CZearance for Work  Authorization to perform specified work or permission to enter a restricted area Clearance from Hazard  Separation from energized lines or equipment Comments x The following summarizes the changes in some of the definitions which APPA recommends Add to the definition for authorized employee It the authorized employee may be an employee assigned to perform the work or assigned to provide the energy control and isolation function  Recommends that OSHA modify the definition for a line clearance tree trimmer to add the word qualified resulting in the complete designation as a qualified line clearance tree trimmer Recommends that OSHA modify the definition of qualified employee" to remove the word construction from the definition since it is felt that knowledge of construction procedures is beyond the scope of the proposed rule resulting in APPA's new A2-18 I 


wording as follows more knowledgeable in operation and hazards associated with electric power generation transmission and/or distribution equipment Recommends that OSHA add a definition for the word practicable and replace the word feasible with practicable wherever it appears in the proposed regulations and that practicable be further defined as capable of being accomplished by reasonably available and economic means OTHER ISSUES Clothing OSHA requested comments on the advisability of adopting requirements regarding the clothing worn by electric utility industry employees EEI has presented comments which indicates research is underway prior to establishing a standard for clothing to be worn by electric utility employees However EEI's position is that this standard has not developed to the extent that it could be included in the OSHA Rules Both APPA and EEI state that they would support a requirement that employers train employees regarding the proper type of clothing to wear to minimize hazards when working in the vicinity of exposed energized facilities Grandfathering Due to the anticipated cost impact on the utility industry of the proposed Rules requiring that existing installations be brought to the requirements of the proposed Rules both APPA and EEI propose that the final Rules include an omnibus grandfather provision This provision would exempt those selected types of facilities from modification to meet the new rules EEI states that if the grandfathering concept is incorporated that electric utility employees will not be deprived of proper protection They propose that employers be required to provide employees with a level of protection equivalent to that which the standard would require in those instances in which the utility does not choose to modify existing facilities to comply with the final standard Rubber Sleeves OSHA requests comments from the industry on whether it would be advisable to require rubber insulating sleeves when gloves are used on lines or equipment energized at more than a given voltage EEI states its position that utilities should continue to have the option of choosing rubber gloves or gloves and sleeves to protect employees when it is necessary to work closer to energized lines than the distances specified in the clearance tables Preemuting State Laws EEI requests that the final Rules be clear in their preempting state rules applicable to the operation and maintenance work rules for electric power systems. This is especially critical since some states now have existing laws which are more stringent than the proposed OSHA Rules Examples are 1 in California and Pennsylvania where electric utility linemen are prohibited from using rubber gloves to work on lines and equipment energized at more than certain voltages and 2 in California and Connecticut where the live line bare hand method of working on high voltage transmission systems is prohibited One utility Pacific Gas  Electric has obtained a variance from the California OSHA to perform live line bare-hand transmission maintenance work on an experimental basis Coiiflicts Between the Rilles and Part 1926 Subpart V Since many of the work procedures in construction work and operation and maintenance work are similar and difficult to distinguish between EEI requests that the final order be clear in establishing which rule has jurisdiction over such similar work areas A2-19 v 


IMPACTS ON COSTS AND ASSOCIATED BENEFITS In its introduction to the proposed rules OSHA has provided an estimate of the annual cost impact on the electric utility industry for the proposed des of approximately 20.7 million OSHA estimates that compliance with this proposed standard would annually prevent between 24 and 28 fatalities and 2,175 injuries per year The utilities which have responded to this proposed standard through their respective associations have questioned the claims both of the magnitude of the cost involved and the benefit to the industry in preventing fatalities and lost-time injuries Both EEI and APPA feel that the annual cost which OSHA estimates are significantly lower than would be realized in practice Factors which APPA and EEI feel were not properly addressed include the following OSHA has not accurately accounted for cost of potential retroactive impacts including retrofitting and modifying existing installations and equipment OSHA has not consistently implemented performance based provisions in proposed rules  many portions require specific approaches which would require utilities to replace procedures already in place with new procedures Estimates were based on an average size investor-owned utility of 2,800 employees and an average rural cooperative of 56 employees, which are not applicable to many smaller systems such as municipal systems OSHA has not adequately addressed the retraining which would be necessary with modifying long-established industry practices to be in accordance with the OSHA rules EEI claims that OSHA's proposed clearance requirements would not allow the use of established maintenance techniques for maintaining high voltage transmission systems and thus would require new techniques For an example of the cost which is estimated to be experienced as a result of the new Rules one of the EEI member companies has estimated that approximately 20,000 transmission towers would need to be modified to accommodate the required step bolts in the Rules at an estimated cost of 6,200,000 Additionally this same company estimates that the annual cost of retesting live line tools for its estimated 1,000 tools would be 265,000 Additionally, both EEI and APPA question the additional benefits which OSHA claims would result from implementation of the new Rules APPA questions the estimates of preventing an additional 24 to 28 fatalities annually and 2,175 injuries per year in that it fails to account for the fact that the industry has already implemented in large part safety measures which are incorporated in the Rules EEI and APPA also point out that many preventable injuries cannot be eliminated despite work rules enforcement and safety awareness campaigns since many such accidents which result in fatalities are due to employee being trained but not following the employer's training and policies PRESENT STATUS OF RULES According to information received from the OSHA office in February 1993 the final Rules are to be published no later than July 1993 and possibly as soon as March 1993 OSHA closed their receipt of comments in March 1991 and no further changes in the rules are thought possible A2-20 


CONCLUSION The OSHA 1910.269 which proposes to cover electric utility operation and maintenance work rules affects a multitude of working procedures as are summarized in this paper It is not possible at the present time to assess the final structure of the Rules as may be proposed in 1993 or subsequent years Since the comments from the utility associations APPA and EEI were made following the initial release of the proposed OSHA Rules in 1989 a significant amount of time has elapsed where other events have occurred which may affect the form of the final Rules The 1993 NESC went into effect in August 1992 and includes some of the requirements to which the commenters objected For example a significant requirement in the Part 4 of the 1993 NESC requires that rubber gloves be utilized on exposed energized parts of facilities operating at 50 to 300 volts This requirement is in conflict with EEl\222s proposed change to the OSHA Rules which would still allow working such secondary facilities without the use of rubber gloves Electric utilities are advised to review the January 31 1989 proposed operation and maintenance Rules as summarized in this paper and to review their procedures which would be affected by application of the Rules Many of the procedures proposed in the Rules provide valuable guidance in electric utilities\222 operation and maintenance activities Where the cost impact is not significant, it is recommended that utilities consider implementing such procedures in expectation of the Rules being published in the next few months Also it would be appropriate for electric utilities to review the 1993 edition of the NESC since there are portions of the Rules which have resulted in changes in the NESC These changes mainly occur in Part 4 Rules for the Operation of Electric Supply and Communications Lines and Equipment The concerns which the commenters have addressed regarding the cost impact and the resulting benefits experienced as a result of the promulgation of the Rules are real ones and must be addressed in the final Rules As a result this paper cannot present a conclusion regarding the full impact of the Rules The development of such Rules continue to be an ongoing matter and will undoubtedly require later analysis when the final rules are published A2-21 


