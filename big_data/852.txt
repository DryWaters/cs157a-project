 1 ASIC versus Antifuse FPGA Reliability John McCollum Actel Corporation 2061 Stierlin Ct Mountain View, CA 94043 650 John.mccolumm@actel.com  Abstract 227ASICs are generally accepted as the highest reliability for custom circuits as they do not have the overhead to personalize them wh en compared to an FPGA However, it is this overhead in FPGAs that allow them to be testable before personalization by the user, that makes them testable to a much higher degree than ASICs. Actual data from burn-in is compared between an ASIC and an RTAXS FPGA indicating that the FPGA is an order of magnitude more reliable than the ASIC 
12   T ABLE OF C ONTENTS  1  I NTRODUCTION 1  2  D EFECTS AND F AULT C OVERAGE 1  3  R ESISTIVE D EFECTS ON AN A CTUAL ASIC.....................1  4  FPGA S RTAXS 
 T ESTING 4  5  RTAXS  L IFE T EST R ESULTS 6  6  C ONCLUSIONS 6  R EFERENCES 11  1  I NTRODUCTION  It is commonly believed that ASICs represent the best in reliability when compared to FPGAs.  This seems to make 
sense as FPGAs use the same base process as a typical ASIC, but then add routing and programming overhead as well as additional technology such as Flash or Antifuses There is of course a major difference in testing and architecture which in fact changes the game 2  D EFECTS AND F AULT C OVERAGE  223MOS\224 has sometimes been used as a moniker meaning 223 M ostly O pens and S horts\224.  This is especially true in CMOS where most defects can be classified as gate shorts junction shorts, metal shorts or interconnect opens. Testing is therefore aimed at removing these defects from the population. For ASICs this necessitates the development of 
test vectors to test all the possible functions to verify that all the transistors are there and functioning. The standard military spec has been 95% fault coverage for test.  The failure rate of so called good die can then be estimated as  D = 1 - \(Test Yield 1 - Fault Coverage 1  which represents the probability of a defect occurring in the untested part of the die.  In Figure 1 the Percent Failure 1  1 978-1-4244-2622-5/09 25.00 \2512009 IEEE 2 IEEEAC paper #1420, Version 4, Updated January 18, 2009 Rate of an ASIC is plotted as function of die yield for different Fault Coverage 
 As can be seen in Figure 1 in page 2, a 95% fault coverage with a respectable 50% yield tran slates to a 3% failure rate This issue has of course lead to the development of scan chain technology to improve the fault coverage.  Scan chain technology is effective at catching junction shorts, poly shorts and metal shorts due to the low impedance nature of these defects.  This improves the detection of DC faults, but it is inadequate to locate AC faults 1  3  R ESISTIVE D EFECTS ON AN A CTUAL ASIC AC Faults are caused by resistive opens and shorts, which still manage to function, but are slower than a typical path 
To improve this ASIC manufacturers have resorted to quiescent leakage testing \(Iddq\o detect low leakage shorts and assembling die from good wafer areas to reduce the probability of a defect  3.1 Resistive Opens These techniques however do not detect the resistive opens Resistive opens are predominately poorly formed vias or contacts, which are very common in modern deep submicron technologies. In deep submicron technologies these are deep narrow wells w ith high aspect ratios which require the metal deposition to backfill the hole. With multilayer metal this makes this one of the leading candidates for defects.  LSI Logic Corporation published a study of this issue as it applies to a gate array at MAPLD 2005 
2 They performed a 24 hour burn-in on a production commercial product utilizing 99% fault coverage testing known good die area assembly, Iddq and Fmax testing After burn-in and repeating the Fmax testing shown in Figure 2, they discovered a 0.5% failure rate Analysis of the AC failures s howed defective via formation In the SEM in Figure 3, a void is evident where the tungsten did not deposit properly leaving a resistive via which subsequently failed in burn-in After adding an AC test vs Idd screening and then completing the 24 hour AC burn-in they were able to reduce that failure rate down to 0.2%.  They noted that the AC vector coverage was only 70%, so the actual defect rate is likely 0.3  


 2  Defective Good Die 0.01 0.10 1.00 10.00 100.00 0.30 0.50 0.70 0.90 Wafer Yield Pe rcen t Failure Rate 70 Fault Coverage 80 Fault Coverage 90 Fault Coverage 95 Fault Coverage 97 Fault Coverage 98 Fault Coverage 99 Fault Coverage  Figure 1 \226 Percent Defective ASIC Parts as a result of less than 100% Fault Coverage    O45 0.00 20.00 40.00 60.00 80.00 100.00 120.00 140.00 R125 30_4 R1 2530_ 5 R1253 0_6 R12 530_7 R 12530 _10 R125 30_13 R1 2530_ 14 R1253 0_15 R12 530_ 16 R12 530_1 8 R 12530 _19 R125 30_21 R1 2530_ 24 R1253 0_25 R12 530_2 6 R 12530 _27 R125 30_29 R1 2530_ 30 R1253 0_34 R12 530_3 5 R 12530 _39 R125 30_4 1 R1 2530 _43 R125 30_44 R1 2530_ 45 R 1253 0_48 R 12530 _52 R125 30_53 R1 2530_ 54 R1253 0_60 R12 530_6 1 R 12530 _62 R125 30_70 R1 2530_ 75 R1253 0_76 R12 530_1 515 R 12530 _153 7 R125 30_1 574 R1 2530 _1728 R135 15_64 1 R1 3515_ 651 R 1351 5_656 R13515_658 R135 15_69 3 R1 3517_ 622 R1351 7_675 R13517_692 R 13517 _704 R135 17_70 6 R1 3518_ 623 R1351 8_71 3 R13 518_ 714 R 1351 8_715 R13518_718 R 13519 _640 R135 19_64 3 R1 3519_ 662 R 1351 9_677 R13519_689 R 13520 _644 R1 3520_ 650 R1352 0_660 R13520_673 R 13520 _684 R137 76_1 R1 3776_ 2 R13776_3 R13776_4 R 1 377 6_5 Fmax \(MHz  Figure 2 \226 Burn-in Fmax Failures   


 3  Figure 3 - Resistive Unfilled W Via Furthermore to cause the resistive via to fail requires a high fault coverage AC stimulus in the burn-in oven, a very expensive requirement for small run ASICs 3.2 Resistive Shorts Another important reliability defect is gate oxide shorts Typically gate oxide shorts are in the range of 1k ohms and 20k ohms.  While gross failures can be caught by standard fault coverage testing, higher impedance shorts are often functional, but slow and can be unstable in the resistance of the short Iddq testing has been an appropriate method to detect these shorts, as shown in Figure 4 1 However, with modern scaled processes the sub th reshold transistor leakage swamps the sub 100uA leakage of a single gate short 2  Furthermore this does not catch gate oxide infant mortality Usually a 168 hour burn-in is required to capture gate oxide defects.  However to have a high confidence in finding an oxide defect would require a 100% toggle in burn-in Further, since ASICs are custom by their very nature, burnin boards must be custom, and, hence, lend themselves to all kinds of electrical over stress \(EOS\ey are not built with the planning of a flight board.  This has lead NASA to recommend to do functional burn-in on flight quality boards 3 but temperatures are often limited for flight boards so often a thousand hours or more of board testing is done to remove the infant mortality from the system       Figure 4 226 Current Path for Shorted Gate Oxide  


 4  Figure 4 RTAXS1000   4  FPGA S RTAXS  T ESTING  An RTAXS1000 FPGA as shown in Figure 5 is built on a standard 0.15u CMOS process to which an amorphous silicon antifuse is added.  As such it has all the same defect considerations as the ASIC, but with the addition of the antifuse.  Due to the fact that the FPGA is blank when shipped from the factory there have been numerous testing methods built in to the design for testing prior to personalization  In principle an FPGA is like an ASIC, a large number of universal logic modules \(U LM\mitted interconnect.  The ASIC would typically use a 2 input NAND gate as the ULM and a large amount of wire interconnect is programmed in design with the addition of vias and selection of the wire lengths.  In an FPGA the personalization of the interconnect is much more expensive so the ULM is inherently much larger to reduce the wire interconnect.  In the RTAXS the ULM is an eight input MUX that allows 4000 different logic functions coupled with an adjacent mast er/slave register.  The function of the ULM is determined by external stimulus of the various inputs, as well as the actual data paths.  As a result the actual amount of personalized interconnect is much lower for an FPGA than an ASIC.  This is the inherent reason why all FPGAs have much larger UL Ms than ASICs as there are extra transistors and signal lines added to the FPGA to allow testing of the blank FPGA.  Adding such testing to an ASIC would be impractical due to their fine grain nature  4.1 Fault Coverage FPGAs are fully tested before personalization.  All of the logic modules are tested in each state and all of the interconnect is verified to be continuous with no shorts. The key tests which allow Actel FPGA\222s to be fully testable are described below 1  A shift register circles the pe riphery of the chip that can be loaded and read back during testing.  The various shift register patterns are used to ensure that different portions of the internal circuits are fully functional 2  All vertical and horizontal tracks are tested for continuity and shorts 3  l horizontal and vertical pass transistors are tested for leakage and functionality 4  The clock buffers are fully tested by driving with the clock pins and reading the proper levels at the side of the array 5  There exist two special pins noted as Probe A and Probe B, which by use of the shift register, can be made to address the output of every ULM inside the array to ensure functionality. Every logic module in the blank FPGA is fully tested 6  Through Probe A and Probe B, numerous other functional tests are allowed on a blank FPGA such as verification of the Input and Output buffer on every I/O 7  Test modes exist to drive all output buffers to low, high or tri-state for testing. Thus Vol, Voh, Iol, Ioh standby current, and leakage tests are performed 8  There are two dedicated nets on the FPGA which are transparent to the customer but allow Actel to program antifuses connecting inputs and outputs of modules into what is known as the \223binning circuit\224. The programming is performed at conditions identical to those utilized by the programmer while programming a customer design. This allows Actel to ensure 


 5 functionality of programmed antifuses and guarantee speed performances of the FPGA 9  There are numerous tests to ensure that the programming circuitry is fully functional to ensure 90% programming yield. The programming circuitry is exercised at levels s lightly greater than the programming conditions used by the programmer 10  Special high voltage devices used to isolate the outputs of the low voltage logic from the elevated voltages during programming. Inputs gates of the logic modules are protected from programming voltages by raising Vcca.  This has the added benefit voltage stressing the low voltage logic and eliminating gate oxide infant mortality.  Subsequent to the voltage stress the logic is verified again.  Hence Antifuse FPGAs will have fewer gate oxide failures in an extended life test than an ASIC 11  There are tests to ensure that all antifuses are unprogrammed and to ensure reliability and quality of the antifuse in the FPGA. Thes e proven electrical screens to ensure antifuse reliability are performed prior to and after the tests that exercise the programming conditions of the FPGA. It is also important to note that the architectural design of the FPGA ensures the minimal stress on antifuses  In fact all the circuits ar e verified except whether a particular antifuse used in the customers design will program.  This can not be verified except at programming and results in less than 100% programming yield  Of course there is still the issue of a resistive via.  Most vias are redundant in the RTAXS.  However there is always the issue of the single via associated with the antifuse at the connection of and XY metal line intersection.  To double these vias would double the size of the chip, just as it would for an ASIC.  However in the RTAXS case this via will get stressed when the antifuse associated with it gets programmed with as much as 25mA.  In normal aperation a via is limited to a much smaller current, about 10 times less than the programming current.  If the via is defective it will fail during programming.  Actel has analyzed many programming failures and it is usually caused by a defective via, as shown in Figure 6 from a RTSXA programming failure.  In this case the antifuse programmed as can be seen, but the device failed the programming check.  The via was not properly etched and when programming current was passed the minimal metal area in contact with the antifuse top plate failed and b ecame an antifuse continuity failure  Hence if the via is defective, it fails during programming not in a customer\222s flight board  Figure 5 \226 Antifuse programming failure 4.2 Guaranteeing Programming In antifuse FPGAs it is not possible to test that every antifuse will program, therefore a small percentage will fail programming.  They are however guaranteed to be 100 functional once programmed.  This is achieved by: testing that the required antifuse is in fact programmed, that no nets are left floating, and that there are no shorts between nets The Actel programming algorith m serially identifies each antifuse requiring programming and applies a voltage in pulses to program the antifuse. A soak \223over program\224 step is performed to ensure that the antifuse is fully programmed and the resistance of the antifus e is uniform across the chip During programming, the programmer has the ability to ensure that each identified an tifuse to be programmed is fully programmed and only that antifuse is programmed else it identifies the part as a programming failure. There are numerous tests performed before and after every antifuse is blown to ensure correct functionality after programming. A standby-ICC measurement is recorded pre-programming and post programming to ensure that the chip ICC characteristics have not changed due to programming Through the extensive electrical tests and screens performed during regular production testing at Actel in combination with the extensive tests performed during programming Actel is able to ensure functionality and reliability of its programmed FPGA  


 6 One of the advantages of a standard product like an FPGA means that all of these tests get verified over time due to the fact that there are thousands of customers verifying the design and testing of the part. If anyone experiences any failure that is related to the product itself, it is corrected by improving the software, manufacturing, design or the testing 5  RTAXS  L IFE T EST R ESULTS  Another of the advantages of an FPGA is that many designs can be easily be used to test various aspects of the technology with 100% te stability for speed and functionality, including 100% ex ercising in the burn-in ovens. As a result several designs were used to test 100% of the features of the device as well as high stress designs to look for failures in the antifuse nets with one nanosecond accuracy. Following are the re sults of this testing  The base process/design \(AX Devices extensive testing by Actel a nd the Aerospace Corporation Actel data is shown in Table 1.  There were a total of 893 devices and \275 million total hours with no failures  Subsequently the Aerospace Cor poration setup a burn-in in which AX2000 parts were permanently soldered in the boards  utilizing a self checking design sensitive to a nanosecond  speed change.  The parts are periodically monitored by a computer so it is not necessary to remove the devices from the oven. The test is divided into three groups.  One is constant at 85C, one is at constant -55C and a third one that is cycled between -55C and +85C. This test is designed to run for three years and is still in the ovens The current number of hours on the boards is in the range of 14,000 hours.  The data is shown in Table 2  In this test of the AX2000 there are 38 boards with 20 parts on each in the life test.  Of the 760 devices under test, 14 did not start up correctly.  They lost communication with one device in the cold chamber and now four more in the temperature cycle chamber.  Additionally two devices showed issues with the SRAM at time zero in the cold oven Since these parts are commercial parts and were never tested at temperature, this is not considered a failure. That is a total loss of 21 parts \(3 from hot, 6 from cold, and 12 from cycling. While the final analysis has not been completed Aerospace Corporation believes these failures to be board related problems.  None of these have any logic or interconnect problems.  This totals 739 devices with a total of  9.3 million device hours  The RTAXS versions of this device have also under gone extensive testing with designs that also utilize 100% of the resources and design that can detect one nanosecond changes in timing.  As can be seen in the Table 3, some of the units were tested out to 6000 hours at 125C ambient Additionally some of these parts are a NASA test and are still in the ovens, going out to 6000 hours.  There are currently 2,842 parts with a total of over 3 million device hours. In these tests there have been two failures.  One was a contact failure and the other was a gate oxide failure in parts of the FPGA which are not part of the FPGA Core and are therefore similar to failures in gate arrays  Combining these tests gives a total of 4,474 parts with a combined total of 13.8 million device hours with two failures.  That is a 0.04% failure rate.  This is approximately an order of magnitude lower than that reported by LSI Logic Corporation for a commercial ASIC design after a 24 hour burn-in.  Additionally th e ASIC only has a 70% AC test coverage versus the FPGA with nanosecond resolution and 100% AC coverage  5.1 Post Programming Burn-in  Despite this excellent data, many customers want a method of burning in devices in which the antifuses have been programmed, or Post Programming Burn-in \(PPBI 1 To this end Actel has added this capability to the RTAXS 4000 and the new DSP family.  This is accomplished by adding an additional control line to l ogic module, which in addition to the normal testing circuitry allows the logic module output to be controlled independe nt of its inputs.  Thus all nets can be toggled at switching speeds independent of the design in the FPGA.  Thus only one burn-in board is required.  This in combination with the Dynamic Blank Burn-in 2 available on E-Flow part s means that the entire chip can be stressed independent of the design 6  C ONCLUSIONS  The nature of the RTAXS with the built in test circuitry which is not practical in a fi ne grain architecture of an ASIC, allows it to be tested and stressed much more fully than an ASIC. Additionally since the interconnect is used to program the antifuses at currents ten times higher than normal operation, followed by the programming verification tests in the programmer effec tively screens out the single defective vias. Further the voltages applied to program the antifuses are effective in rem oving the weak dielectrics that are susceptible to failure. The net result is the RTAXS FPGA is at least an order of magnitude more reliable than an ASIC. It is interesting to note that problems with printed circuit boards and handling issues out number actual part failures by about twenty to one   


 7 Product Units BI hours AF failures Other Failures Total hours T A C V CCA V V CCI V AX1000 129 1000 129000 125 1.8 3.6 AX1000 129 1000 129000 55 1.8 3.6 AX1000 77 1000 77000 125 1.6 3.6 AX1000 77 1000 77000 55 1.6 3.6 AX2000 38 1000 38000 125 1.6 3.6 AX2000 38 1000 38000 55 1.6 3.6 AX2000 22 1000 22000 125 1.65 3.6 AX2000 96 168 16128 110 1.8 3.6 AX2000 96 168 16128 110 1.9 3.6 AX2000 96 168 16128 110 1.9 3.6 AX2000 95 24 2280 110 2 3.6 Total 893 560664 AX Table 1 \226 AX Qualification Lot Summary     Table 2 \226 Aerospace Corporation Life Test \226 AX2000   


 8 Table 3 226 RTAXS Life Test Results  RTAXS Product Units Prog Yield BI hours AF failure s Other Failure s Total hours TA C Comments          RTAX2000 S 87  1000 0 2 85000 125 ESD RTAX1000 S 98  1000 0 4 94000 125 ESD RTAX1000 S 78  1000 0 1 77000 55 ESD RTAX1000 S 150  1000 0 6 144000 125 Cont failure due to BIB RTAX1000 S 28  1000 0 1 27000 125 improperly etched contact RTAX1000 S 120  6000 0 0 720000 125  RTAX2000 S 6 100% 1000 0 0 6000 125  RTAX2000 S 6 100% 1000 0 0 6000 125  RTAX1000 S 144  250 0 0 36000 55  RTAX1000 S 150  250 0 2 37000 55 Cont failure due to BIB RTAX2000 S 14 95 168 0 0 2352 125  RTAX2000 S 14 100 168 0 0 2352 125  RTAX2000 S 14 54 168 0 0 2352 125  RTAX1000 S 24  168 0 0 4032 125  RTAX1000 S 37 100% 1000 0 0 37000 125  RTAX1000 S 8 100% 1000 0 0 8000 125  RTAX2000 S 14 88 168 0 0 2352 125  RTAX2000 S 14 78 168 0 0 2352 125  RTAX2000 S 14  168 0 0 2352 125  RTAX2000 S 14 93 168 0 0 2352 125  RTAX250S 100 95 168 0 0 16800 125  RTAX1000 S 150  1000 0 2 148000 125 damage in metal layers only-EOS RTAX1000 S 148 250 0 0 37000 55  RTAX1000 S 150 250 0 0 37500 55  


 9 RTAX1000 S 150   1000 0 0 150000 125  RTAX2000 S 14 100% 168 0 0 2352 125  RTAX250S 6 100% 1000 0 0 6000 125  RTAX2000 S 58  1000 0 0 58000 125  RTAX2000 S 20  1000 0 0 20000 125  RTAX2000 S 58   168 0 0 9744 55  RTAX2000 S 20   168 0 0 3360 55  RTAX1000 S 24 100% 168 0 0 4032 125  RTAX2000 S 14 48% 168 0 0 2352 125  RTAX2000 S 6 86% 1000 0 0 6000 125  RTAX2000 6 100% 2000 0 0 12000 125  RTAX250S 100 93 168 0 0 16800 125  RTAX2000 S 79 100% 1000 0 0 79000 125  RTAX2000 S 14 88% 168 0 0 2352 125  RTAX2000 S 24 100% 1000 0 0 24000 125  RTAX2000 S 8 73% 2000 0 0 16000 125  RTAX2000 S 6 100% 2000 0 0 12000 125  RTAX2000 S 2 100 1000 0 1 1000 125 Gate oxide failure in buffer RTAX2000 S 8 100% 2000 0 0 16000 125  RTAX2000 S 14 93% 168 0 0 2352 125  RTAX2000 S 14 100% 168 0 0 2352 125  RTAX2000 S 11 92% 2000 0 0 22000 125  RTAX2000 S 11 92% 2000 0 0 22000 125  RTAX2000 S 8 93% 1000 0 0 8000 125  RTAX4000 S 75 72.5 5000 0 0 375000 125  RTAX4000 S 4 72.5 3000 0 0 12000 125  RTAX4000 S 24 80% 1000 0 0 24000 55  RTAX4000 S 24 80 1000 0 0 24000 125  RTAX2000 S 14 100% 168 0 0 2352 125  


 10 RTAX2000 S 14 88% 168 0 0 2352 125  RTAX4000 S 15 100% 1000 0 0 15000 125  RTAX2000 S 8 100% 2000 0 0 16000 125  RTAX1000 S 8 89% 1000 0 0 8000 125  RTAX1000 S 11 92% 2000 0 0 22000 125  RTAX2000 S 24 92% 1000 0 0 24000 125  RTAX2000 S 14 100% 168 0 0 2352 125  RTAX2000 S 14 88% 168 0 0 2352 125  RTAX4000 S 8 100% 168 0 0 1344 125  RTAX2000 S 82 95% 1500 0 0 123000 125  RTAX2000 S 82 95% 1500 0 0 123000 55  RTAX250S 82 98% 1500 0 0 123000 125  RTAX250S 82 98% 1500 0 0 123000 55           TOTAL 2842  TOTAL 3,057,244  LTOL 936  LTOL 507,604  HTOL 1906  HTOL 2,549,640    


 11 R EFERENCES     Reddy, M.K. and S.M. Reddy 223Detecting FET Stuck-Open Faults in CMOS Latc hes and Flip-Flops,\224 IEEE Design and Test of Computers Vol. 3 , No. 5 , pp. 17-26, October 1986   2 R Mad g e , M. Vilg is, a nd V. Bhide, "Achieving Ultra High Quality and Reliability in Deep Sub-Micron Technologies using Metal Layer Configurable Platform ASICs", MAPLD 2005    Kewal Sal u ja, \223Di g i t a l Sy st em Fundam e nt al s, Lect ure 11\224, Department of Electrical Engineering, University of Wisconsin Madison    Yu W e i  Papos ng  M oo Ki t Lee Peng W e ng Ng C h i n  Hu Ong, \223IDDQ Test Challenges in Nanotechnologies: A Manufacturing Test Strategy\224, Asian Test Symposium 2007. ATS apos;07. 16 th Volume , Issue , 8-11 Oct. 2007 Page\(s\211 \226 211  NASA GSFC Advi sory NA-GSFC 2004-06   Dan El ft m a nn, Sol o m on W o l d ay and M i nal  Sawant  New Burn In \(BI\ethodology for Testing of Blank and Programmed Actel 0.15 \265m RTAX-S FPGAs MAPLD 2005   M i nal Sawant Dan El ft m a nn,  W e rner van den Abeel an John McCollum, Solomon Wolday and Jonathan Alexander 223Post Programming Burn-in of Actel O.25um FPGA\222s\224 MAPLD 2002  B IOGRAPHY   John worked 2 years at Faichild R&D on bipolar switching performance specifically platinum dopedlife time control and the development of Ion Implantation.  He worked 15 years at Intel developing Intel's first bipolar PROM, Ion Implantation, the world's first 16K DRAM, as well as 64K and 256K DRAMs.  Mr. McCollum developed Intel's first dual layer metal CMOS technology for the 386 microprocessor.  He co-founded Actel and worked the last 20 years on process, antifuse and flash cell development and FPGA Architecture at Actel.  He holds over 50 patents   covering Process Technology, Antifuse and NVM technology, FPGA Architecture, Analog Processing and Radiation Hardening.  He has presented numerous papers at IEDM, MAPLD, CSME, SPWG, and the FPGA Symposium. He is currently a Fellow in the Technology Development Department 


Time Time 50 350   10 0                   10 1                   10 2 12.5 50 350   10 0                   10 1  12 expected from Figure 7, the width of the uncertainty region is compressed by the curvature of the monopulse response resulting in a detection-primitive with greater uncertainty than the variance admits.  A filter lag or so-called cluster tracking can easily result in a 5% or greater offset and degraded consistency.  After 300 seconds the curves peak up because the target is appr oaching a low-elevation beampoint limit.  This occurs anytime a target is tracked into the edge of the radar\222s field of re gard and can lead to radar-toradar handover difficulty         300 300 80  s D 2 k,1 k y    s D 2 k,1 k y   100 150 200 250 10 1                    10 5 1 0  Figure 8 - Consistency versus distance from beam center Monopulse Mismatch The next set of curves plotted in Figure 9 show the sensitivity of detection-primitive consistency to a mismatch in the monopulse slope.  All of these curves were generated using a linear monopulse response derived from the slope of the true monopulse response at beam center.  The slope of the 80% curve is 0.8 times th e beam-center slope; the 90 curve is 0.9 times the beam-center slope; and so on for 100%, 110% and 120%.  Again, the order of curves in the graph is the same as the legend order A steep slope tends to expand y I 222s uncertainty while a gentle slope tends to compress it.  An expanded uncertainty leads to a smaller consistency while a compressed uncertainty leads to a larger consistency.  This behavior can be observed in the family of curves in Figure 9.  Curves for the steeper slopes are on the botto m while curves for more gentle slopes are on top.  The notable feature of this set of curves is that the sensitivity to a mismatch in the monopulse slope is not very significant       100 150 200 250 10 1                    90 100 110 120  Figure 9 - Consistency versus monopulse mismatch Range-Bias Error The complex nature of the monopulse radar models presents ample opportunity to introduce errors in the software implementation.  One such e rror introduced in a \275 rangecell-width bias in the detection-primitive range which in turn resulted in a significant degradation to 2 9 k D The fact that 2 9 k D is measured in different coordinates compared to the bias made it difficult to determine which value or algorithm was to blame.  Examining the intermediate consistency values led directly to the error source A comparison between biased 2 1 k D  2 2 k D and 2 3 k D values and unbiased 2 2 k D values is shown in Figure 10.  The unbiased 2 2 k D is the bottom-most curve and the biased 2 3 k D is the top-most curve with a value around 80.  This large value for 2 3 k D indicates that there is a lot more uncertainty in the range measur ements compared to what is predicted by the range varian ce.  Since the range-variance calculation is easy to confirm, the problem must be in the algorithms that model or manipulate range A notable feature of Figure 10 is the sensitivity of the centroiding algorithm to range bias in the detection primitives.  The range bias is ba rely noticeable in the biased 2 1 k D and 2 2 k D curves.  Of course, if the unbiased 2 2 k D  curve existed as a baseline it would be relatively easy to spot the error 


Time Time Time 50 350   10 0                   10 1                   10 2 50 350   10 1                   10 0                   10 1 50 350   10 0                   10 1  13         Isolated No SNR Adjust  Figure 11 - Centroiding for isolated range cells Filter Tuning Now that the centroided m easurements are reasonably consistent, the parameters that govern track filtering can be examined.  As previously promised, the effects and corrections for atmospheric refr action and sensor bias have been disabled so that 2 8 k D can be analyzed using a sliding window.  Of course the full analysis would include these effects and 2 8 k D at each time step would be collected and averaged over many trials Plots of the effect of changing process noise in a nearlyconstant-velocity filter are shown in Figure 12 and Figure 13 for Cartesian position and velocity respectively.  In both figures, the plotted values have been divided by 3 so that the desired value is always 1.  Increasing the process noise up to a point should increase the updated uncertainty and reduce 2 8 k D values.  Except near th e end of the trajectory when the measurements are off of beam center, the curves in Figure 12 and Figure 13 appear inconclusive for this expected trend If 2 8 k D values are way out of range there are additional intermediate filter values that can be examined.  For example, the state extrapolati on algorithms can be examined by comparing the consistency of 1 210 Isolated With SNR Adjust 300 300 300 0.005 212 212 212 212 212 kkkk T kkkk D xhzSxhz 35        s D 2 k Range   D 2 k,2 biased D 2 k,1 biased D 2 k,2  Figure 10 - Range bias error in detection primitive Centroiding Algorithm From Section 3, assuming that the centroided-range uncertainty for an isolated range cell is the same as its detection-primitive uncertainty may be incorrect Collecting and plotting 2 3 k D values only from isolated range-cell measurements can be used to analyze such assumptions.  The plots in Figure 11 compare differences between the isolated-cell algorithm defined in Section 3, an algorithm that modifies the uncertainty based on the SNR in the isolated cell, and the 2 3 k D values from all measurements 34\was used to modify the range uncertainty for the upper line labeled Isolated with SNR Adjust    4 22  2 2  resRi o R R Rn bdp bm  s D 2 k,3 Range    s D 2 k,8 Position     212 1 can also be examined using \(35 The residual is also commonly used to determine the assignment cost  212 kk z  P  k  k1 with z k The consistency of the innovation covariance k T kkkkk RHPHS 100 150 200 250 10 1                    D 2 k,3 biased 100 150 200 250 10 2                    100 150 200 250 10 1                    0.5 50  Figure 12 \226 Position consistency, filter tuning example  r  t t 34 If the All Centroided curve \(middle\as the baseline doing nothing \(lower\imates the uncertainty and 33\imates the uncertainty.  Dividing by the square root of the observed SNR leads to a more consistent covariance; however, there is currently no statistical evaluation to justify it             210 210 1 1 1 2 All Centroided 


Time 50 350   10 1                   10 0                   10 1  14         300 0.005  s D 2 k,8 Velocity   100 150 200 250 10 2                    0.5 50  Figure 13 \226 Position consistency, filter tuning example 5  C ONCLUSION  Calculating and observing the behavior of covariance consistency at different levels  in the radar signal processing chain represents a very powerfu l tool that can be used to assess the accuracy and softwa re implementation of radar signal-processing algorithms.  Analyzing covariance consistency is applicable to radar systems both in the field and in simulations.  The primary challenge in both arenas comes down to properly accounting for the true target states that contribute to detections, detection primitives measurements, and state estimates For a fielded radar syst em, achieving covariance consistency is usually a s econdary consideration behind achieving and maintaining track s.  Indeed, until recently radar specifications did not even include requirements for covariance consistency.  Recent covariance consistency requirements stem from the fact that the use of radar systems in sensor netting applications is on the rise Currently the combined e ffects of off-beam-center measurements, atmospheric correction, bias correction clustering and centrioding, data association, and filtering on state covariance consistency throughout a target\222s trajectory are not well known.  This is particularly true for radars using wideband waveforms and multiple hypotheses or multiple frame trackers.  Numerical results presented here indicate that algorithms early in the radar signal processing chain can significantly degrad e covariance consistency and that some errors are better tolerated than others For a simulated target in a modeled system, truth relative to some global reference is known. However, transforming truth through different refere nce frames and accounting for changes that occur during various radar processing algorithms is not as simple as it appears.  The techniques in this paper help expose this hidden complexity and provide a framework for discussing and expanding the future development of covariance consistency techniques.  Such future developments include issues related to mapping truth through the convolution operation typically used to simulate wideband signal processing and the fast Fourier transforms typically used in pulse-Doppler processing.  Sophisticated tracking algorithms that carry multiple hypotheses, associate across multiple frames, or weight the association of multiple targets within a single frame pose significant challenges in properly associating truth with state estimates.  Additional work, including an investigation of track-to-truth assignment, is needed before covariance consistency techniques can be applied to these algorithms Another area that needs furthe r analysis is the use of a sliding window to approximate the covariance behavior expected during a set of Monte-Carlo trials.  Various timedependent variables such as the target\222s range and orientation, the transmit waveform, the radar\222s antenna patterns toward the target, missed detections, and false alarms could easily viol ate the assumption that measurement conditions are nearly stationary over the time of the window.  It is importa nt to understand the conditions when this assumption is violated Finally, the examples presented here included a relatively benign arrangement of targets.  Further analysis in dense target environments with the related increase in merged detections, merged measurements, and impure tracks is needed.  Further analysis for targets traveling over different trajectories is also needed Even so, the techniques presented here can be extended to many of these analyses R EFERENCES  1  S. Blackman and R. Popoli Design and Analysis of Modern Tracking Systems Artech House, 1999 2  Y. Bar-Shalom and X. R. Li Multitarget-Multisensor Tracking: Principles and  Techniques YBS Publishing, Storrs, CT, 1995 3  Y. Bar-Shalom, Editor Multi-target-Multi-sensor Tracking: Advanced Applications and  Vol. I Artech House, Norwood, MA, 1990 4  D. B. Reid, \223An Algorithm for Tracking Multiple Targets,\224 IEEE Trans. on Automatic Control Vol. 24 pp. 843-854, December 1979 5  T. Kurien, \223Issues in the Design of Practical Multitarget Tracking Algorithms,\224 in Multitarget-Multisensor Tracking Y. Bar-Shalom \(ed.\43-83, Artech House, 1990 6  R.P.S. Mahler, Statistical Multisource-Multitarget Information Fusion, Artech House, 2007 


 15 7  B.-N. Vo and W.-K. Ma, \223The Gaussian Mixture Probability Hypothesis Density Filter,\224 IEEE Trans Signal Processing Vol. 54, pp. 4091-4104, November 2006 8  B. Ristic, S. Arulampalam, and N. Gordon Beyond the Kalman Filter Artech House, 2004 9  Y. Bar-Shalom, X. Rong Li, and T. Kirubarajan Estimation with Applications to Tracking and Navigation, New York: John Wiley & Sons, pg. 166 2001 10  X. R. Li, Z. Zhao, and V. P. Jilkov, \223Estimator\222s Credibility and Its Measures,\224 Proc. IFAC 15th World Congress Barcelona, Spain, July 2002 11  M. Mallick and S. Arulampalam, \223Comparison of Nonlinear Filtering Algorithms in Ground Moving Target Indicator \(GMTI Proc Signal and Data Processing of Small Targets San Diego, CA, August 4-7, 2003 12  M. Skolnik, Radar Handbook, New York: McGrawHill, 1990 13  A. Gelb, Editor Applied Optimal Estimation The MIT Press, 1974 14  B. D. O. Anderson and J. B. Moore Optimal Filtering  Prentice Hall, 1979 15  A. B. Poore, \223Multidimensional assignment formulation of data ass ociation problems arising from multitarget and multisensor tracking,\224 Computational Optimization and Applications Vol. 3, pp. 27\22657 1994 16  A. B. Poore and R. Robertson, \223A New multidimensional data association algorithm for multisensor-multitarget tracking,\224 Proc. SPIE, Signal and Data Processing of Small Targets Vol. 2561,  p 448-459, Oliver E. Drummond; Ed., Sep. 1995 17  K. R. Pattipati, T. Kirubarajan, and R. L. Popp, \223Survey of assignment techniques for multitarget tracking,\224 Proc  on Workshop on Estimation  Tracking, and Fusion: A Tribute to Yaakov Bar-Shalom Monterey CA, May 17, 2001 18  P. Burns, W.D. Blair, \223Multiple Hypothesis Tracker in the BMD Benchmark Simulation,\224 Proceedings of the 2004 Multitarget Tracking ONR Workshop, June 2004 19  H. Hotelling, \223The generalization of Student's ratio,\224 Ann. Math. Statist., Vol. 2, pp 360\226378, 1931 20  Blair, W. D., and Brandt-Pearce, M., \223Monopulse DOA Estimation for Two Unresolved Rayleigh Targets,\224 IEEE Transactions Aerospace Electronic Systems  Vol. AES-37, No. 2, April 2001, pp. 452-469 21  H. A. P.  Blom, and Y. Bar-Shalom, The Interacting Multiple Model algorithm for systems with Markovian switching coefficients IEEE Transactions on Au tomatic Control 33\(8  780-783, August, 1988 22  M. Kendall, A. Stuart, and J. K. Ord, The Advanced Theory of Statistics, Vol. 3, 4th Edition, New York Macmillan Publishing, pg. 290, 1983 23  T.M. Cover and P.E. Hart, Nearest Neighbor Pattern Classification, IEEE Trans. on Inf. Theory, Volume IT-13\(1 24  C.D. Papanicolopoulos, W.D. Blair, D.L. Sherman, M Brandt-Pearce, Use of a Rician Distribution for Modeling Aspect-Dependent RCS Amplitude and Scintillation Proc. IEEE Radar Conf 2007 25  W.D. Blair and M. Brandt-Pearce, Detection of multiple unresolved Rayleigh targets using quadrature monopulse measurements, Proc. 28th IEEE SSST March 1996, pp. 285-289 26  W.D. Blair and M. Brandt-Pearce, Monopulse Processing For Tracking Unresolved Targets NSWCDD/TR-97/167, Sept., 1997 27  W.D. Blair and M. Brandt-Pearce, Statistical Description of Monopulse Parameters for Tracking Rayleigh Targets  IEEE AES Transactions, Vol. 34 Issue 2,  April 1998, pp. 597-611 28  Jonker and Volgenant, A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems, Computing, Vol. 38, 1987, pp. 325-340 29  V. Jain, L.M. Ehrman, and W.D. Blair, Estimating the DOA mean and variance of o ff-boresight targets using monopulse radar, IEEE Thirty-Eighth SSST Proceedings, 5-7 March 2006, pp. 85-88 30  Y. Bar-Shalom, T. Kirubarajan, and C. Gokberk 223Tracking with Classification-Aided Multiframe Data Association,\224 IEEE Trans. on Aerospace and Electronics Systems Vol. 41, pp. 868-878, July, 2005   


 16 B IOGRAPHY  Andy Register earned BS, MS, and Ph  D. degrees in Electrical Engineering from the Georgia Institute of Technology.  His doctoral research emphasized the simulation and realtime control of nonminimum phase mechanical systems.  Dr. Register has approximately 20 years of experience in R&D with his current employer, Georgia Tech, and product development at two early-phase startups. Dr. Register\222s work has been published in journals and conf erence proceedings relative to mechanical vibration, robotics, computer architecture programming techniques, and radar tracking.  More recently Dr. Register has b een developing advanced radar tracking algorithms and a software architecture for the MATLAB target-tracking benchmark.  This work led to the 2007 publication of his first book, \223A Guide to MATLAB Object Oriented Programming.\224  Mahendra Mallick is a Principal Research Scientist at the Georgia Tech Research Institute \(GTRI\. He has over 27 years of professional experience with employments at GTRI \(2008present\, Science Applications International Corporation \(SAIC Chief Scientist \(2007-2008\, Toyon Research Corporation, Chief Scientist 2005-2007\, Lockheed Martin ORINCON, Chief Scientist 2003-2005\, ALPHATECH Inc., Senior Research Scientist 1996-2002\, TASC, Principal MTS \(1985-96\, and Computer Sciences Corporation, MTS \(1981-85 Currently, he is working on multi-sensor and multi-target tracking and classification bas ed on multiple-hypothesis tracking, track-to-track association and fusion, distributed filtering and tracking, advanced nonlinear filtering algorithms, and track-before-detect \(TBD\ algorithms He received a Ph.D. degree in  Quantum Solid State Theory from the State University of New York at Albany in 1981 His graduate research was also based on Quantum Chemistry and Quantum Biophysics of large biological molecules. In 1987, he received an MS degree in Computer Science from the John Hopkins University He is a senior member of the IEEE and Associate Editor-inchief  of the Journal of Advances in Information Fusion of the International Society of Information Fusion \(ISIF\. He has organized and chaired special and regular sessions on target tracking and classific ation at the 2002, 2003, 2004 2006, 2007, and 2008 ISIF conferences. He was the chair of the International Program Committee and an invited speaker at the International Colloquium on Information Fusion \(ICIF '2007\, Xi\222an, China. He is a reviewer for the IEEE Transactions on Aerospa ce and Electronics Systems IEEE Transactions on Signal Pr ocessing, International Society of Information Fusion, IEEE Conference on Decision and Control, IEEE Radar Conference, IEEE Transactions on Systems, Man and Cybernetics, American Control Conference, European Signal Processing Journal and International Colloquium on Information Fusion ICIF '2007   William Dale Blair is a Principal Research Engineer at the Georgia Tech Research Institute in Atlanta, GA. He received the BS and MS degrees in electrical engineering from Tennessee Technological University in 1985 and 1987, and the Ph.D. degree in electrical engineering from the University of Virginia in 1998. From 1987 to 1990, he was with the Naval System Division of FMC Corporation in Dahlgren, Virginia. From 1990 to 1997, Dr Blair was with the Naval Surface Warfare Center, Dahlgren Division NSWCDD\ in Dahlgren, Virg inia. At NSWCDD, Dr Blair directed a real-time experiment that demonstrated that modern tracking algorithms can be used to improve the efficiency of phased array radars. Dr Blair is internationally recognized for conceptualizing and developing benchmarks for co mparison and evaluation of target tracking algorithms Dr Blair developed NSWC Tracking Benchmarks I and II and originated ONR/NSWC Tracking Benchmarks III and IV NSWC Tracking Benchmark II has been used in the United Kingdom France, Italy, and throughout the United States, and the results of the benchmark have been presented in numerous conference and journal articles. He joined the Georgia Institute of Technology as a Se nior Research Engineer in 1997 and was promoted to Principal Research Engineer in 2000. Dr Blair is co-editor of the Multitarg et-Multisensor Tracking: Applications and Advances III. He has coauthored 22 refereed journal articles, 16 refereed conference papers, 67 papers and reports, and two book chapters. Dr Blair's research interest include radar signal processing and control, resource allocation for multifunction radars, multisen sor resource allocation tracking maneuvering targets and multisensor integration and data fusion. His research at the University of Virginia involved monopulse tracking of unresolved targets. Dr Blair is the developer and coordinator of the short course Target Tracking in Sensor Systems for the Distance Learning and Professional Education Departmen t at the Georgia Institute of Technology. Recognition of Dr Blair as a technical expert has lead to his election to Fellow of the IEEE, his selection as the 2001 IEEE Y oung Radar Engineer of the Year, appointments of Editor for Radar Systems, Editor-InChief of the IEEE Transactions on Aerospace and Electronic Systems \(AES\, and Editor-in- Chief of the Journal for Advances in Information Fusion, and election to the Board of Governors of the IEEE AES Society,19982003, 2005-2007, and Board of Directors of the International Society of Information Fusion   


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


