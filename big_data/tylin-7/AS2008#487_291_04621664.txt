Information Driven Association Rule Hiding Algorithms 
Privacy is one of the most important properties an infor 
Igor Nai Fovino Joint Research Center Institute for the Protection and Security of the Citizen Via Enrico Fermi 1 Ispra Italy igor.nai@jrc.it Alberto Trombetta Insubria University Dipartimento di Informatica e Comunicazione Via Mazzini 5 Varese Italy alberto.trombetta@uninsubria.it 
Abstract 
mation system must satisfy A relatively new trend shows that classical access control techniques are not suf\336cient to guarantee privacy when datamining techniques are used Privacy Preserving Data Mining PPDM algorithms have been recently introduced with the aim of sanitizing the database in such a way to prevent the discovery of sensible information e.g association rules A drawback of such algorithms is that the introduced sanitization may disrupt the quality of data itself In this paper we introduce a new 
methodology and algorithms for performing useful PPDM operations while preserving the data quality of the underlying database 
1 Introduction 
Intense work in the area of data mining technology and in its applications to several domains has resulted in the development of a large variety of techniques and tools able to automatically extract knowledge from large amounts of data At the present moment these tools are the only effective 
way to extract useful knowledge from the increasing number of very large databases which cannot be manually analyzed due to their huge sizes However as with other kinds of useful technologies knowledge discovery processes can be misused For example malicious subjects may reconstruct sensitive information for which they do not have access authorization Usual data mining techniques can be deployed over non-sensitive data in order to infer private information For private information we mean any infor 
mation usually about the owner of the corresponding data that such owner is not willing to release Current security techniques are not suitable to prevent inferences of private information carried out through the use of such data mining techniques For this reason many research efforts have been devoted in order to seek privacy-preserving data mining PPDM from now on techniques i.e data mining techniques which yield useful information without violating the privacy of 
data owners As a result various database sanitization techniques have been proposed for hiding privacy-related items or patterns by removing some of the publicly available data or adding noise to them Some tests whose results are found in 4 7 s ho w that actually some problems occur when applying PPDM algorithms in contexts in which the meaning of sanitized data has a critical relevance More specifically these tests show that sanitization often introduces non-correct information Such phenomenon is due to the 
fact that current PPDM techniques do not take into account two relevant issues 
200 
Relevance of data Structure of the database 
 not all the information stored in the database has the same relevance and not all the information can be dealt in the same way  information stored in a database is strongly in\337uenced by the relationships between different data items e.g integrity constraints 
200 
These relationships are not always explicit We argue that the general problem of current PPDM algorithms is related to 
data quality 
 intended as the consistency between real-world data and their representations in 


an informations systems such as a database See Section 3 for a detailed discussion on the connection between privacy and data quality Of course in order to perform a data quality-aware sanitization any PPDM technique needs some additional information about the database to be sanitized In this paper we propose a suitable framework for expressing such additional information and deﬁne a corresponding data quality-aware sanitization technique 2 Related works Traditionally the association rule mining task consists in identifying a set of strong association rules Srikant and Agrawal in  e xplore and e xpand the i dea of interesting rule  Further Agrawal et and P a rk et al.[10 decompose the challenge of association mining into two main steps called large itemsets identiﬁcation and association rules identiﬁcation Atallah et al propose heuristic techniques for the modiﬁcation of data based on perturbations in order to avoid an unauthorized use of association rule mining Oliveira and Zaiane p ropose a heuristicbased framework for preserving privacy in mining frequent itemsets They focus on hiding a set of frequent patterns containing highly sensitive knowledge They propose a set of sanitization algorithms that only remove information from a transaction database also known in the statistical disclosure control area as non-perturbative algorithms unlike those algorithms that modify the existing information by inserting noise into the data referred to as perturbative algorithms The algorithms proposed by Oliveira and Zaiane rely on a item-restriction approach in order to avoid the addition of noise to the data and limit the removal of real data Evﬁmievski et al  propose a frame w o rk for mining association rules from transactions consisting of categorical items where the data has been randomized to preserve privacy of individual transactions while ensuring at the same time that only true associations are mined Another reconstruction-based technique is proposed by Rivzi and Haritsa  T he y p ropose a distortion m ethod to preprocess the data before executing the mining process Their goal is to ensure privacy at the level of individual entries in each customer tuple 3 The Data Quality Approach Any PPDM sanitization operating on a target database D has an effect on the data quality contained in it In order to preserve the quality of the database we propose to take into account the relevance and the intended use of the data contained in the DB represented as a function  DB  of the database D  Now by taking another little step in the discussion it is evident that the function  DB  has a different output for different databases This implies that a PPDM algorithm trying to preserve the data quality needs to know speciﬁc information about the particular database to be protected before executing the sanitization 3.1 Data Quality Concepts Traditionally data quality DQ is a measure of the consistency between data as stored in an information system and the same data as in the real world  In the context of PPDM DQ has a similar meaning with however an important difference The difference is that in the case of PPDM the real world is the original database and we want to measure how close the sanitized database is to the original one with respect to some relevant properties More in detail we consider relevant the parameters allowing to capture a possible variation of meaning or that are indirectly related with the meaning of the information stored in a target database Therefore we identify as most appropriate DQ parameters for PPDM Algorithms the following dimensions  Accuracy it measures the proximity of a sanitized value a to the original value a  In an informal way we say that a tuple t is accurate if the sanitization has not modiﬁed the attributes contained in the tuple t  Completeness it evaluates the percentage of data from the original database that are missing from the sanitized database Therefore a tuple t is complete if after the sanitization every attribute is not empty  Consistency it is related to the integrity constraints holding on the data and it measures how many of these constraints are still satisﬁed after the sanitization For a formal deﬁnitions of those parameters see 3.2 The Information Quality Model In the previous section we have presented a way to measure DQ in the sanitized database These parameters are however not sufﬁcient for the construction of a new PPDM algorithm aimed at DQ preservation It is necessary to provide a formal description of the information that the PDDM algorithm is intended to extract from the sanitized DB From now on we refer to such information as aggregate information and the relevance of DQ properties for each aggregate information for some aggregate information not all the DQ properties have the same relevance Moreover for each aggregate information it is necessary to provide a description of the relevance in term of consistency completeness and accuracy level required 


at the attribute level and the constraints the attributes must satisfy In order to achieve such goals we have adopted the Information Quality Model IQM presented in The Information Quality Model is composed in a bottom-up fashion by a number of components whose descriptions are detailed below Intuitively the task of these components is to provide useful information in order to nd out whether a given database can be sanitized e.g for hiding given association rules without incurring in a data quality loss The basic bricks of the IQM is the Data Model Graph DMG  whose task is to represent the attributes involved in an aggregate information and their constraints and Aggregation Information Schema AIS which measures the relevance of different aggregations Such information is grouped into what we call information Schema Set ASSET 3.3 The DQDB Algorithm distortion based algorithm In the Distortion Based Algorithms the database sanitization i.e the operation by which a sensitive rule is hidden it is obtained by the distortion of some values contained in the transactions supporting the rule partially or fully depending from the strategy adopted to be hidden In what follows we assume to work with categorical databases The sanitization strategy we adopt is similar to the one presented in the Codmine algorithms The rst operation required is the identiﬁcation of the rule to hide In order to do this we use the well known APRIORI algorithm  The output of this phase is a set of rules From this set a sensitive rule will be identiﬁed The next phase implies the determination of Zero  Impact Items if they exist associated with the target rule Figure 1 INPUT the IQM Schema A associated to the target database the rule Rh to be hidden OUTPUT the set possibly empty Zitem of zero impact items discovered 1 Begin 2 zitem   3 Rsup=Rh 4 Isup=list of the item contained in Rsup 5 While  Isup     6  7 res=0 8 Select item from Isup 9 res=Search\(Asset Item 10 if res==0 zitem=zitem+item 11 Isup=Isup-item 12  13.return\(zitem 14 End Figure 1 Zero Impact Algorithm If the Zero Impact itemset is empty this means no item exists contained in the rule that can be assumed to be non relevant for the DQ of the database In this case it is necessary to simulate what happens to the DQ of the database when a non zero impact item is modiﬁed The strategy we adopt in this case is the following a We compute how many steps are needed in order to downgrade the conﬁdence of the rule under the minimum threshold as it is possible to see from the algorithm code there is little difference between the case in which the item selected is part of the antecedent or part of the consequent of the rule b By inspecting the IQM we calculate the impact on the DQ in terms of accuracy and consistency for each items The Item Impact Rank Algorithm is reported in Figure 2 INPUT the IQM schema the rule Rh to be hidden the sup  Rh   the sup  ant  Rh   the Threshold Th OUTPUT the set Qitem ordered by Quality Impact 1 Begin 2 Qitem   3 Confa Sup  Rh  Sup  ant  Rh  4 Confp=Confa 5 Nstep if ant=0 6 Nstep if post=0 7 While  Confa>Th o 8  9 Nstep if ant 10 Confa Sup  Rh   Nstep if ant Sup  ant  Rh   Nstep if ant  11  12.While  Confb  T h o 13  14 Nstep if post 15 Confa Sup  Rh   Nstep if post Sup  ant  Rh   16  17 For each item in Rh do 18  19 if  item  ant  Rh   then N=Nstep if ant 20 else N=Nstep if post 21 For each AIS  Asset do 22  23 node=recover item\(AIS,item 24 Accur Cost node.accuracy Weight  N  25 Constr Cost=Constr surf\(N,node 26 item.impact item.impact   Accur Cost  AIS.Accur weight   Constr Cost  AIS.COnstr weight   27  28  29.sort by impact\(Items 30.Return\(Items End Figure 2 Item Impact Rank Algorithm for the Distortion Based Algorithm ant\(Rh indicates the antecedent component of a rule Finally the Sanitization algorithm works as follows 1 It selects all the transactions fully supporting the rule to be hidden 2 It computes the Zero Impact set and if there exist such type of items it randomly selects one of these items If this set is empty it calculates the ordered set of Impact Items and it selects the one with the lower DQ Effect 3 Using the selected item it modiﬁes the selected transactions until the rule is hidden 


The nal algorithm is reported in Figure 3 Given the algorithm for sake of completeness we try to give an idea about its complexity The search of all zero impact items assuming the items to be ordered according to a lexicographic order can be executed in N  O  lg  M  where N is the size of the rule and M is the number of different items contained in the Asset The cost of the Impact Item Set computation is less immediate to formulate However the computation of the sanitization step is linear  O   Sup  Rh     Moreover the constraint exploration in the worst case has a complexity in O  NC  where NC is the total number of constraints contained in the Asset This operation is repeated for each item in the rule Rh and then it takes  Items  O  NC   Finally in order to sort the Itemset we can use the MergeSort algorithm and then the complexity is  itemset  O  lg   itemset    The sanitization has a complexity in O  N  where N is the number of transaction modiﬁed in order to hide the rule  INPUT the Asset Schema A associated to the target database the rule Rh to be hidden the target database D OUTPUT the Sanitized Database 1 Begin 2 Zitem=DQDB Zero Impact\(Asset,Rh 3 if  Zitem     then item=random sel\(Zitem 4 else 5  6 Impact set=Items Impact ant\(Rh Threshold 7 item=best\(Impact set 8  9 Tr  t  D  t fully support Rh  10.While Rh.Conf  Threshold do 11.sanitize\(Tr;item 12 End Figure 3 The Distortion Based Sanitization Algorithm 4 Conclusions The problem of the sanitization impact on the quality of the database has been to the best of our knowledge never addressed by previous approaches to the PPDM We believe that the core of this problem has to be linked with the lack of information given to the PPDM algorithms about the meaning and the relevance of the information stored into the database On the basis of this consideration and starting from a previously done work of Bertino and Nai we have proposed and developed a suite of algorithms for association rule PPDM aimed by the scope of mitigating the DQ impact of the sanitization over a target database Preliminary test results show that such an approach is effective against the information corruption due to the PPDM sanitization The study is however far to be considered nished in fact we plan to extend our algorithms in the context of association rule PPDM blocking family Moreover we plan to apply the same approach in the context of clustering PPDM algorithms References 1 R A g r a w a l  T I m i e l i  nski and A Swami Mining association rules between sets of items in large databases In SIGMOD 93 Proceedings of the 1993 ACM SIGMOD international conference on Management of data  pages 207–216 New York NY USA 1993 ACM  R  A gra w al and R  S rikant F ast a lgorithms for m ining a ssociation rules In Proceeding of the 20 th International Conference on Very Large Databases Santiago Chile Morgan Kaufmann  June 1994  M  A tallah E Bertino A Elmag a rmid M  I brahim a nd V Verykios Disclosure limitation of sensitive rules In Proceedings of 1999 IEEE Knowledge and Data Engineering Exchange Workshop KDEX’99  pages 45–52 IEEE 1999  I  M  A uthor  A frame w o rk for e v a luating p ri v a c y preserving data mining algorithms Journal Data Mining and Knowledge Discovery  11\(2 September 1999  E Bertino a nd I F o vino Information d ri v e n e v a luation o f data hiding algorithms In 7 th International Conference on Data Warehousing and Knowledge Discovery  SpringerVerlag 2005  A  E vﬁmie v ski R Srikant R Agra w a l and J  Gehrk e Privacy preserving mining of association rules In KDD 02 Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining  pages 217–228 New York NY USA 2002 ACM  U  o f M ilan C omputer T echnology Institute Sabanci U ni v e r sity Codmine ist project 2002-2003 8 S R M O l i v e i r aa n d O  R  Z a  ane Privacy preserving frequent itemset mining In CRPIT 14 Proceedings of the IEEE international conference on Privacy security and data mining  pages 43–54 Darlinghurst Australia Australia 2002 Australian Computer Society Inc  K  O rr  Data quality and s ystems theory  Commun ACM  41\(2 1998  J S P a rk M S Chen a nd P  S Y u  A n e f f ecti v e h ashbased algorithm for mining association rules In SIGMOD 95 Proceedings of the 1995 ACM SIGMOD international conference on Management of data  pages 175–186 New York NY USA 1995 ACM  S J Rizvi a nd J R Haritsa Maintaining d ata p ri v a c y in association rule mining In VLDB 02 Proceedings of the 28th international conference on Very Large Data Bases  pages 682–693 VLDB Endowment 2002  R Srikant a nd R Agra w a l Mining generalized association rules In Proceedings of the 21 th International Conference on Very Large Data Bases  pages 407–419 Morgan Kaufmann 1995 


Header_Table 1-itemset Frequency a                        8 e                        5 b                        5 c                        7 f                        6 root b: 3 s a: 8 s c: 5 i e: 2 s b: 2 s c: 1 s e: 1 s e: 2 s null null null null null c: 1 i f: 1 s f: 2 s f: 1 s f: 1 i Figure 4 The final result   When the records are modified, the proposed maintenance algorithm first partitions the 1-itemsets in the modified records into four parts according to whether they are large or small in the original database and have positive or negative count \(or zero\ difference Each part is then processed in its own way. The Header_Table and the FUSP tree are correspondingly updated whenever necessary. The obtained tree complexity may not be optimal because the frequency order of items may not be kept. The effect is expected small since large count difference seldom happens when the amount of modified records is not large. The execution time of the proposed approach can, however be greatly improved. The proposed approach can thus achieve a good trade-off between execution time and tree complexity  6. References  1 R. A g r a w a l T  I m ie link s i a n d A  Swa m i M ining  association rules between sets of items in large database The ACM SIGMOD Conference pp. 207-216 Washington DC, USA, 1993 2 R. A g ra w a l, T  I m ie link s i a n d A  Sw a m i D a t a b a s e  mining: a performance perspective IEEE Transactions on Knowledge and Data Engineering Vol. 5, No. 6, pp 914-925, 1993 3 R. A g ra wa l a nd R Srik a n t   F a s t a l g o rithm f o r m i ning  association rules The International Conference on Very Large Data Bases pp. 487-499, 1994 4 R. A g r a w a l a nd R. Srik a n t M in ing se que ntia l pa tte r n s  The Eleventh IEEE International Conference on Data Engineering pp. 3-14, 1995  D W  Ch eun g  J Han  V  T   Ng an d C Y W o n g   Maintenance of discovered association rules in large databases: An incremental updating approach The Twelfth IEEE International Conference on Data Engineering pp. 106-114, 1996  H Ch en g  X  Yan an d J Han  In c S p an  i n crem en t a l  mining of sequential patterns in large database The ACM SIGKDD international conference on Knowledge discovery and data mining pp.527-532, 2004 7  J  H a n, J  P e i a n d Y  Y i n M ining f r e que nt pa tte r n s  without candidate generation The 2000 ACM SIGMOD International Conference on Management of Data pp. 112, 2000 8  T  P  H ong C  Y  W a ng a nd S S. T s e n g   I nc r e m e nta l  data mining for sequential patterns using pre-large sequences The International Multiconference on Systemics, Cybernetics and Informatics Vol. 14, pp. 543548, 2001 9  T  P  H ong C  W  L i n a n d Y   L  W u  I nc r e m e nta l l y  fast updated frequent pattern trees Expert Systems with Applications Vol. 34, Issue 5, pp. 2424-2435, 2008 10  M. Y  L i n a nd S. Y  L e e   I nc r e m e nta l up da te on  sequential patterns in large databases The Tenth IEEE International Conference on Tools with Artificial Intelligence pp. 24-31, 1998  11  C  W  L i n, T   P  H o ng W e nH s ia ng L u a nd W e nY a ng  Lin, ìAn Incremental FUSP-Tree Maintenance Algorithm The Eighth International Conference on Intelligent System  Design and Application 2008  
653 
653 


1  Br e a s t h e a l t h   h t t p    w w w  b r e a s t h e a l t h  c o m  a u   statisticsresearch/index.html  Accessed on 12 August 2008 2  Co o k e   D    O r d o n e z  C   E r n e s t  V   G a r c i a   E   Omiecinski Elyzabeth Krawczynska Russell Folks Cesar Santana, Levien de Braal, and N. Ezquerra. Data  mining of large myocardial perfusion SPECT MPS databases to improve diagnostic decision making S.H Lee J.E and Park J.S Analysis on risk factors for cervical cancer using induction technique Expert Systems with Applications, 27\(1\.  2004, 97-105 6  V i n n a k o t a  S   a n d  L a m   N  S  N   S o c i o e c o n o m i c  inequality of cancer mortality in the United States  a spatial data mining approach Int J Health Geogr 2006,  5- 9 7  W i k i p e d i a   h t t p    e n  w i k i p e d i a  o r g  w i k i   Bladder_cancer Accessed 20 August 2008 8  Ca n c e r  o r g   h t t p    w w w  c a n c e r  o r g  d o c r o o t  CRI/content/CRI_2_4_1X_What_are_the_key_statisti cs_for_bladder_cancer_44.asp Accessed, 20 August 2008 9  H e r z o g   T  J   N e w  a p p r o a c h e s  f o r  t h e  m a n a g e m e n t  of cervical cancer   2003 S22\226S27 17  We therefore concluded a rule for each cancer dataset and summarized as below All the rules were shown highest confidence We found the breast and prostate cancer rules are very simple Whereas rest of the rules contained several significant risk factors to predict an appropriate cancer  Relation:   bladder  Rule work_esposure 307 work_leather_industry 307 work_rubber_industry 307 work_cable_industry 307 work_printing_industry 001 bladder_cancer  Relation:   breast  Rule  gender female 307  age\(>40 307  family_history 001 breast_cancer  Relation:   cervical  Rule  poor_hygienic_practices 307  low_socioeconomic_status 307  low_vegetables 307  sexually_transmitted_infections,smoking 001  cervical_cancer  Relation:    lung  Rule  chemical_exposure 307  smoking 307  previous_lung_disease 307  urban_residence 307  family_history 001 cervical_cancer  Relation:  prostrate  Rule family_history 307 age \(>50 001  prostrate_cancer  Relation:   skin  Rule weak_immune_system 307 fragile_skin 307  exposure_to_environmental_hazards 307  previous_skin_cancer 001 skin_cancer  All the generated rules demonstrated high confidence Thus we conclude that these rules are highly acceptable Comparatively breast and prostrate cancer showed shorter rules comparing other cancer This research argues to consider more risk factors to be considered in the experimental dataset for both breast and prostate cancer to extract most significant risk factors   6.0 Conclusions  This study has demonstrated the use of association rule mining to identify some specific risk factors for a particular cancer problem In this research we employed three types of association rule mining algorithms to extract significant risk factor for a specific cancer Finally we suggested in our research to use Apriori algorithm for such type of task All the generated rules were showing highest confidence level in our experiment  We aim to extend this research by considering more risk factors in the raw data-set for each cancer problem in order for association rule mining algorithm to extract more useful and significant risk factors for a particular type of cancer   References  Gynecologic Oncology 90  1999, 38\22649 4  P e n d h a r k a r   P  C   J  A   Ro d g e r   G  J   Y a v e r b a u m   N  Herman and M Benner Association statistical mathematical and neural approaches for mining breat  cancer patterns 40\(5\, 1999 3  O r d o n e z   C   a n d  O m i e c i n s k i   E   D i s c o v e r i n g  association rules based on image content In Expert Systems with Applications   1999, 223\226232 5  H o   S  H    J e e    th th Journal of Nuclear Medicine IEEE Advances in Digital Libraries Conference ADL\22299 


 Kluwer Academic Publishers Springer, New York 1st edition, 2001 14  S c h e f f e r   T   F i n d i n g  A s s o c i a t i o n  Ru l e s  t h a t  T r a de Support Optimally Against Confidence th The Elements of Statistical Learning self_care_guide/Urogenital/Postate%20Cancer.pdf  Accessed, 25 August, 2008 11  A g r a w a l   R  T   I m i e l i n s k i     A   S w a m i   M i n i n g  association rules between sets of items in large databases, In Proceedings of the 1993 ACM SIGMOD international conference on Management of data  The Netherlands 42 2001 61-95  Ordonez C Association rule discovery with the train and test approach for heart disease predictio n 207\226 216 12 001 13  H a s t i e   T    R  T i b s h i r a n i     J  H   F r i e d m a n   Proceedings of the 5th European Conference on Principles and Practice of Knowlege Discovery in Databases\(PKDD'01 IEEE Transactions on Information Technology in Biomedicine, 10\(2\, 2006. 334 \226 343 001 Freiburg, Germany : SpringerVerlag, 2001. 424-435 15  F l a c h   P  A     L a c h i c h e   N   Co n f i r m a t i o n g u i d e d  discovery of first-order rules with Tertius 10  P h a r m a c y   h t t p    w w w  p h a r m a c y  g o v  m y    


 7. Conclusions  In this paper we have proposed an intelligent and efficient technique to reassess the distances between dynamic XML documents when one or all of the initially clustered documents have changed. After the changes, the initial clustering solution might become obsolete - the distances between clustered XML documents might have changed more or less depending on the degree of modifications \(insert update, delete\hich have been applied. Re-running full pair-wise comparisons on the entire set of modified documents is not a viable option, because of the large number of redundant operations involved Our proposed technique allows the user to reassess the pair-wise XML document distances, not by fully comparing each new pair of versions in the clustering solution, but by determining the effect of the temporal changes on the previously known distances between them. This approach is both time and I/O effective, as the number of operations involved in distance reassessing is greatly reduced  References  1  Beringer, J. and H\374llermeier, E., Online clustering of parallel data streams Data and Knowledge Engineering 58\(2\,  2006, 180-204 2  Catania, B. and Maddalena A., A Clustering Approach for XML Linked Documents, Proceedings of the 13th International Workshop on Database and Expert Systems Applications \(DEXA\22202\, IEEE 2002 3  Chen, M.S., Han, J. and Yu, P., Data Mining: An Overview from Database Perspective, IEEE Transactions on Knowledge and Data Engineering vol. 8, 1996, 866-883 4  Cormen, T., Leiserson, C. and Rivest, R Introduction to algorithms, MIT Press, 1990 5  Costa, G., Manco, G., Ortale, R. and Tagarelli, A., A tree-based Approach to Clustering XML documents by Structure, PAKDD 2004, LNAI 3202, 137-148 Springer 2004 6  Dalamagas, T., Cheng, T., Winkel, K.J. and Sellis, T 2004, Clustering XML documents by Structure SETN 2004, LNAI 3025, 112-121, Springer 2004 7  Ester, M., Kriegel, H.P., Sander, J., Wimmer,M. and Xu, X., Incremental Clustering for Mining in a Data Warehousing Environment, Proc.of the 24 th VLDB Conference, New York, USA, 1998 8  Garofalakis, M., Rastogi, R., Seshadri, S. And Shim K., Data Mining and the Web: Past, Present and Future Proceedings of WIDM 99 Kansas, US, ACM 1999 9  Mignet, L., Barbosa, D. and Veltri, P., The XML web : a first study, In Proceedings of the 12 th  International Conference on WWW, 500-510 2003   Nayak, R., Xu, S., XCLS: A Fast and Effective Clustering Algorithm for Heterogeneous XML Documents, In Proceedings of the 10 th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, Singapore, LNCS 3918, 2006   Rusu, L.I., Rahayu, W. and Taniar, D., A methodology for Building XML Data Warehouses International Journal of Data warehousing Mining, 1\(2 67-92, 2005   Rusu, L.I., Rahayu, W. and Taniar D.,  Maintaining Versions of Dynamic XML Documents, In Proceedings of the 6th International Conference on Web Information Systems Engineering, New York NY, USA, November 20-22, 2005, LNCS 3806   Rusu, L.I., Rahayu, W. and Taniar, D., Warehousing Dynamic XML Documents, In Proceedings of the 8 th  International Conference on Data Warehousing and Knowledge Discovery \(DaWaK 2006 LNCS 4081 Springer, 175-184, 2006   Shen, Y. and Wang, B., Clustering Schemaless XML documents, CoopIS / DOA/ODBASE 2003, LNCS 2888, 767-784, Springer 2003   Yoon, J. P., Raghavan, V., Chakilam, V., and Kerschberg, L., BitCube: A Three-Dimensional Bitmap Indexing for XML Documents J. Intel. Inf Syst 17, 2-3 \(Dec. 2001\, 241-254   XML data repository, online at http www.cs.washington.edu / research / projects / xmltk xmldata  
456 
456 


5 Related Work There exists extensive previous work on both the mining of software repositories and on the use of clustering algorithms in software engineering This discussion focuses on the most similar and recent work in the area of software evolution Mining Software Repositories Our technique was partially inspired by the work of Zimmermann et al and Y ing et al 17 on the mining of association rules in change history As described in Section 1 we sought to expand the technique to be able to recommend larger but less precise clusters of elements to guide program navigation Bouktif et al also investigated how to recommend cochanges in software development As opposed to the work cited above Bouktif et al used change patterns instead of association rules Also their approach does not attempt to reconstruct transactions and can consider associated 002les that were changed in different transactions ChangeDistiller is a tool to classify changes in a transaction into 002ne-grained operations e.g addition of a method declaration and determines how strongly the change impacts other source code entities Our approach uses similar repository analysis techniques but is focused on providing task-related information as opposed to an overall assessment of a system's evolution Finally repository mining can also be used to detect aspects in the code In this conte xt aspects are recurring sets of changed elements that exhibit a regular structure Aspects differ from the clusters we detect in the regular structure they exhibit which may not necessarily align with the code that is investigated as part of change tasks Clustering Analysis The classical application of clustering for reverse engineering involves grouping software entities based on an analysis of various relations between pairs of entities of a given version of the system Despite its long and rich history  e xperimentation with this approach continues to this day For example Andreopoulos et al combined static and dynamic information K uhn et al used a te xtual similarity measure as the clustering relation and Christl et al used clustering to assist iterative semi-automated reverse engineering The main dif ferences b e tween most clusteringbased reverse engineering techniques and the subject of our investigation is that the entities we cluster are transactions rather than software entities in a single version of a system For this reason our analysis is based strictly on the evolving parts of the system Both Kothari et al and V an ya et al 15 recently reported on their use of clustering to study the evolution of software systems The idea of using change clusters is the same in both works and ours but the purpose of the work is different Kothari et al use change clusters to uncover the types of changes that happened e.g feature addition maintenance etc during the history of a software system Vanya et al use change clusters which they call evolutionary clusters to guide the partitioning of a system that would increase the likelihood that the parts of the system would evolve independently In contrast we cluster transactions based on overlapping elements not 002les to recommend clusters to support program navigation as opposed to architectural-level assessment of the system Finally Hassan and Holt evaluated on 002ve open source systems the performance of several methods to indicate elements that should be modi\002ed together This study found that using historical co-change information as opposed to using simple static analysis or code layout offered the best results in terms of recall and precision The authors then tried to improve the results using 002ltering heuristics and found that keeping only the most frequently cochanged entities yielded the best results As opposed to our approach the evaluated 002ltering heuristics were only applied on entities recovered using association rules and not using clustering techniques The focus of their study was also more speci\002c as they recommend program elements that were strictly changed  as opposed to recommending elements that might be inspected by developers 6 Conclusion Developers often need to discover code that has been navigated in the past We investigated to what extent we can bene\002t from change clusters to guide program navigation We de\002ned change clusters as groups of elements that were part of transactions or change sets that had elements in common Our analysis of close to 12 years of software change data for a total of seven different open-source systems revealed that less than 12 of the changes we studied could have bene\002ted from change clusters We conclude that further efforts should thus focus on maximizing the quality of the match between the current task and past transactions rather than 002nding many potential matches Our study has already helped us in this goal by providing reliable evidence of the effectiveness of some 002ltering heuristics and useful insights for the development of additional heuristics Acknowledgments The authors thank Emily Hill and Jos  e Correa for their advice on the statistical tests and the anonymous reviewers for their helpful suggestions This work was supported by NSERC 
25 
25 
25 
25 
25 


References  B Andreopoulos A An V  Tzerpos and X W ang Multiple layer clustering of large software systems In Proc 12th Working Conf on Reverse Engineering  pages 79ñ88 2005  S Bouktif Y G Gu  eh  eneuc and G Antoniol Extracting change-patterns from cvs repositories In Proc 13th Working Conf on Reverse Engineering  pages 221ñ230 2006  S Breu and T  Zimmermann Mining aspects from v ersion history In Proc 21st IEEE/ACM Int'l Conf on Automated Software Engineering  pages 221ñ230 2006  A Christl R K oschk e and M.-A Store y  Equipping the re\003exion method with automated clustering In Proc 12th Working Conf on Reverse Engineering  pages 89ñ98 2005  D 020 Cubrani  c G C Murphy J Singer and K S Booth Hipikat A project memory for software development IEEE Transactions on Software Engineering  31\(6 465 2005  B Fluri and H C Gall Classifyi ng change types for qualifying change couplings In Proc 14th IEEE Int'l Conf on Program Comprehension  pages 35ñ45 2006  A E Hassan and R C Holt Replaying de v elopment history to assess the effectiveness of change propagation tools Empirical Software Engineering  11\(3 2006  D H Hutchens and V  R Basili System s tructure analysis Clustering with data bindings IEEE Transactions on Software Engineering  11\(8 1985  D Janzen and K De V older Na vig ating and querying code without getting lost In Proc 2nd Int'l Conf on AspectOriented Software Development  pages 178ñ187 2003  J K ot hari T  Denton A Shok ouf andeh S Mancoridis and A E Hassan Studying the evolution of software systems using change clusters In Proc 14th IEEE Int'l Conf on Program Comprehension  pages 46ñ55 2006  A K uhn S Ducasse and T  G  021rba Enriching reverse engineering with semantic clustering In Proc 12th Working Conf on Reverse Engineering  pages 133ñ142 2005  M P  Robillard T opology analysis of softw are dependencies ACM Transactions on Software Engineering and Methodology  2008 To appear  M P  Robillard and P  Mangg ala Reusing program in v estigation knowledge for code understanding In Proc 16th IEEE Int'l Conf on Program Comprehension  pages 202 211 2008  J Sillito G Murph y  and K De V older Questions programmers ask during software evolution tasks In Proc 14th ACM SIGSOFT Int'l Symposium on the Foundations of Software Engineering  pages 23ñ34 2006  A V an ya L Ho\003and S Klusener  P  v an de Laar and H van Vliet Assessing software archives with evolutionary clusters In Proc 16th IEEE Int'l Conf on Program Comprehension  pages 192ñ201 2008  N W ilde and M C Scully  Softw are reconnaissance Mapping program features to code Software Maintenance Research and Practice  7:49ñ62 1995  A T  Y ing G C Murph y  R Ng and M C Chu-Carroll Predicting source code changes by mining change history IEEE Transactions on Software Engineering  30\(9 586 2004  A Zeller  The future of programming en vironments Integration synergy and assistance In Proceedings of the 29th International Conference on Software Engineering The Future of Software Engineering  pages 316ñ325 2007  T  Zimmermann and P  W eiﬂgerber  Preprocessing C VS data for 002ne-grained analysis In Proc 1st Int'l Workshop on Mining Software Repositories  pages 2ñ6 May 2004  T  Zimmermann P  W eiﬂgerber  S Diehl and A Zeller  Mining version histories to guide software changes In Proc 26th ACM/IEEE Int'l Conf on Software Engineering  pages 563ñ572 2004 A Clustering Algorithm This algorithm is not sensitive to whether a given program element exists or not in a given version of a program For example if method m exists in one version it is considered a valid program element even if it is removed in a later version In the rest of this section we use the term program element to refer to the uniquely identifying representation of the element e.g a Java fully-quali\002ed name Let T be a transaction modeled as a set of program elements changed together during the history of a software system Let T be a sequence of transactions In this algorithm a cluster is also modeled as a set of elements 1 Input  T  A sequence of transactions 2 Parameter  M IN O VERLAP  A positive non-zero value indicating the minimum overlap between two transactions in a cluster 3 Var  C  A set of clusters initially empty 4 for all T i 2 T do 5 MaxOverlap  0 6 MaxIndex  000 1 7 for all C j 2 C do 8 if j C j  T i j  MaxOverlap then 9 MaxOverlap  j C j  T i j 10 MaxIndex  j 11 end if 12 end for 13 if MaxIndex   0  MaxOverlap 025 M IN O VERLAP  then 14 C MaxIndex   C MaxIndex  T i  15 else 16 NewCluster  T i 17 C  C  f NewCluster g 18 end if 19 end for 20 return C B Systems Analyzed System home pages last veri\002ed 7 May 2008 Ant ant.apache.org Azureus azureus.sourceforge.net Hibernate www.hibernate.org JDT-Core www.eclipse.org/jdt/core JDT-UI www.eclipse.org/jdt/ui Spring springframework.org Xerces xerces.apache.org 
26 
26 
26 
26 
26 


