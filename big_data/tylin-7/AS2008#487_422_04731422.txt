Method of Chinese Grammar rules automatically access based on mining association rules  Chao Tang Beijing University of Posts and Telecommunations Computer Science and Technology College Beijing, China danielxp2002@163.com Chen Liu Beijing University of Posts and Telecommunations Computer Science and Technology College Beijing, China lchen@bupt.edu.cn   Abstract Grammar rule set is often used in natural language processing. Usually, rule set can only be gained in linguistics materials artificially. In this paper, through study on the using states of grammar rules in natural language processing, we propose a method, with using a typical way of data mining mining association rules, of exploring Chinese grammar rules in real corpus environment. And we build Chinese grammar rule database using the rules explored by this method Compare the result in this method with the traditional artificial way, we find this method has better availability Keywords- chinese grammar;  data mining I   I NTRODUCTION  NLP\(Natural Language Process\stems such as information retrieval, text classification, and machine translation, are all closely related with syntax and lexical analysis. Since relationships is only considered between the neighboring words, Chinese lexical analysis usually can be achieved using the rule-based or statistic-based ways. While because Chinese grammar structure is relatively complex analysis of them not only needs to consider non-adjacent words, it is also necessary to consider the sub-structure, with make it is difficult to solve the problem using a simple statistical manner. Thus, Chinese grammar analysis are always involved with rule-based method. However according to some rules like Zapf Law be si d e s  s o m e  common rule there also contain a large number of noncommon rules. Common rules can be obtained by refer to the research results of linguists, but when facing so many noncommon rules and catchwords grammar rules which emerge continually, we need a method to find these rules automatically. This paper aims to study ways of Chinese grammar rules automatic acquisition using related methods in mining association rules, and make the rules available through optimization methods  II  THE  DEFINITION  OF  CHINESE  GRAMMAR  RULES For better expression of grammar rules and make the mining work easier, we made some form of expression provisions. In the academic study, modern Chinese is usually divided into some levels like morpheme, morpheme unit word, phrases, small sentence, and sentence[2 Sin ce w o r d  which is made of morpheme and morpheme unit is the smallest independent grammar units used in language, in order to facilitate treatment, this paper will also take the word as the smallest unit of grammar rules, and define grammar rule base on words: a grammar rule is a solid construction of grammar words and filled parts, in which grammar words are the stable parts that support framework of grammar rule, and filled parts are the variable composite in sentences use the grammar rule excepting the grammar words which do not affect the grammar rule, sometimes them can be empty. In accordance with the definition above grammar rule can be expressed with regular expression like follows grammar rule>:=<filled part>{<grammar word><filled part grammar word>:=<word filled part>:={<word There are two types of expression for typical grammar rules, one type use specific words, such as 001\000\011¾\034Ý\001\002\030!\001\002\001\001 002  001\000\014´\004\016\001\002\030\024\004¹\001\002\001\001\002Ä\001\000\001\002\001\001 stands for filled part 002 Another type use POS replace words of same kind, such as 001\000 pronoun 001\002 verb 001\002 noun 001\001  001\000 number + noun 001\001 etc. When analysis Chinese grammar, the latter type is relatively more general, therefore we mainly discuss mining of grammar rules in the latter type  III  M INING M ODEL D ESCRIPTION  As shown in Figure 1 below, grammar mining model consist of two knowledge base and three major steps In the model, raw corpus is the sentence set in real language environment, including a large number of sentence of varying length, which contains rich syntax information and is the main target of mining. Grammar rule base is used for grammar rules set storage. Because a lot of I/O operations are needed when using the base, it should be well designed and following points should specifically be noted   Rule redundancy – Rule redundancy can be divided into two categories: the rule-contained redundancy and dead-rule redundant. The existence of rule redundancy will affect base’s storage and efficiency. The grammar rule base should have redundant-clear function 
2008 International Symposium on Computer Science and Computational Technology 978-0-7695-3498-5/08 $25.00 © 2008 IEEE DOI 10.1109/ISCSCT.2008.68 265 


 Figure 1  Grammar Rules Mining System Model Rule inconsistent inspection – In addition to rule redundant, the rules in base may cause a number of inconsistencies, these rules may affect the normal operation of the system and the correctness of the results. Rule inconsistent includes conflict rules, redundant-regulation rules, cycle rule chain, and other types. The base should also have inconsistent-inspection and inconsistent-processing functions  Process of grammar rule mining from raw corpus, as shown in Figure 1, includes three main steps Pretreatment work like choose proper sentences from raw corpus base and lexical analysis Use association rule mining methods, and mine the original grammar association rules from corpus after step 1 Use automatic or manual verification methods, find the real grammar rules which is available from original rules set  A  Pretreatment on raw corpus We do statistics on a big set of raw corpus about length of sentence, and select one result as table 1 following The results above show clearly that: Chinese sentence usually contains 2 to 11 words, open-test on more sentences shows that sentences with these length occupy majority about 82%-90% of all. Thus we can restrict the mining object on the set of sentences which contains 2 to 11 words For the number of longer sentence which may contains longer syntax rules is so small that those sentences can be ignored. Then do lexical analysis, such as split words and mark POS, on the chosen sentences, and get POS string for further mining TABLE I  RELATIONSHIP  BETWEEN  LENGTH  OF  SENTENCE  AND  SENTENCES  COUNT length of sentence sentences counts 1 6 2 20 3 97 4 318 5 503 6 636 7 523 8 382 9 243 10 150 11 89 12 42 13 32 14 15 15 7 16 4  B  Association rules mining There exists similarity between grammar rule and other data about exist form[4  t h u s in th e o ry w e can f i n d  grammar rules from sentences through association rule mining methods just like from other data. Firstly, some association rule mining concepts are introduced here[5  I n  a  service-base D, for each object transaction T\(like POS string\ assuming X, Y as two different items in the transaction\(like words in the POS string\, Support or Confidence is often used to quantify the appearance possibility of the data. Support\(X\flects the probability that item X appears in base D. While Confidence\(X=>Y stands for that if there is a rule\(like grammar rule\ X=>Y how much is the probability that the transaction T which contains item X contains item Y as well. There are formulas of the two concepts support\(X=>Y\ = support\(X Y confidence\(X=>Y\=support\(X Y\support\(X 100 if support\(X=>Y\ual or larger than minsupport\(a number stands for minimum support, usually according to the number of transactions in the base\, and confidence\(X=>Y\ equal or larger than minconfidence\(defined just like minsupport\, then rule X=>Y is called strong rule. Strong rule is the final target an rusult in association rule mining work There are a lot of association rule mining algorithm, in which we choose Apriori algorithm which is used widely Apriori creates new designate item options with the big item set from last loop,  computes the support of options when scans data, and get big item set in the end. It overcome the waste of time and space appears in scanning data which AIS 
266 


and SETM algorithm do when they create designate item options, and do a better job than them Sentences in real language environment will be transformed to POS string, if take a POS string for a transaction T, sentences in the base can be regarded as transaction set D. Then grammar rules which use POS for word can be expressed as association rule, such as Example 1: association rule 1=m ==> 2=q stands for there is a quantity q after a number in position 1 of a sentence. This association rule expresses the grammar rule number + quantity Example 2: association rule 1=r ==> 2=v 8=n stands for the grammar rule “pronoun + verb…noun Examples above just reflect one to one relationship between association rule and grammar rule, but there are many problems in real mining work Sometimes several association rules reflects the same grammar rule, such as 1=m ==> 2=q and 2=q==> 1=m, they are seemed as difference rules when do mining, in fact they stands for the same grammar rule “number + quantity If range the mining result with the level of support or confidence, it will appear that many rules are combined, such as 1=n 2=n ==> 4=v, which really reflects the combination of rule “noun + noun” and “noun…verb”, and these rules have appeared in a high level rank Because of some POS itself has low support, related association rules ranks low in the result. If only high rank rules are selected, the final result probably is the collection of the most common rules and their combinations, which leads to poor result of data mining  C  Grammar rules mining In order to solve problems exist in association rule mining, further grammar rule mining should be done base on the result of association rule mining. Usually, there are many-to-one relationships between effective association rules and grammar rules, it means that one grammar rule may come from several association rules. Usefulness of association rule mining result has a close relationship with the sort parameters, besides confidence which is mentioned earlier, there are many sort parameters such as lift, leverage conviction and so on. Experiments show that leverage is more suitable for two-words rule mining, lift is for more than two words rules, while conviction and leverage are better choices for long sentences which contains long distance rules So we combine these parameters and use them in proper situation, raise the method of grammar rule mining base on association rule mining, as following steps  1  Filtering rules. Before sorting There are redundancy and interference in the result which need to be moved out. First of all, combine association results reflects the same grammar rule, leave only respective position relationship between words, then remove disturbers. Disturbers mainly rise from combination of general grammar rules. Thus if a rule contains only the other simple rules, remove it from the rule set  2  General grammar rule mining No matter which sort parameter is chosen, general rules always  appear in higher position of sort result, so we choose top 20 for target. If the sentence has less than 7 words leverage is chosen for sort parameter to mining two-word rule, then we put the result into an original rule set S. Lift is chosen for multi-word rules, and put the result into S if they do not in S. With the sentences have more than 7 words and less than 11 words, we first use lift to mine multi-rules, put the result into S if they do not in S. Then confidence is used for long sentences, we put the result into S if they do not in S  3  Rare grammar rule mining Rare grammar rule always appears after general and combination ones in the sort result, and usually has few words. We mainly choose sentences contain less than 7 words for mining and use straightly use leverage-sort for two-words rule mining while lift-sort for multi-words mining  Results of association rule mining transform to original grammar rule set S after process above. If grammar rule base already exists, through incremental-adding from S, it will be expanded gradually  IV  EXPERIMENTS  AND  RESULTS In order to study how much we can learn about grammar rule mining from association rule, some experiments are conducted to seek answer. We choose 87 modern Chinese grammar rules from linguistics monographs[6  8 research their using condition in real-life corpus, sort them with statistics possibility from high to low, and acquire a basic grammar rule-set. This set is used as normal set and compared with other experiment result. If a rule belongs to normal set, it is called effective rule. We design two experiments below, to test our mining method  Experiment 1: Effect on mining result with different corpus  Target: Research what difference the mining results show with corpus having different source background or different volume Corpus 1: 3072 sentences \(PKU sentences base Corpus 2: 836 sentences \(net page set A Corpus 3: 2753 sentences \(net page set B Corpus 4: 22156 sentences \(excerpt of novel Results in Table II TABLE II  EFFECTIVE  RULES  FROM  DIFFERENT  CORPUS Corpus 1 2 3 4 Sentences 3072 836 2753 22156 Effective rule 47 25 34 49   
267 


 Conclusion: Compare result on corpus with different volume, we can find that if corpus contains more sentences more effective rules will be found. While if the difference is corpus background, it show another view. Sentences in PKU base are selected, cover a large number of grammar rules while net pages and novel are relatively oral corpus, large number of sentences contain only minority general rules. But we find some new rules do not in normal rule-set, which reflects the fact that there are differences between catchphrases rule set and normal rule set Experiment 2:  different result mining on different length sentences Target: Research effect to mining result caused by different sentence length Corpus 1: 526 sentences contain less than 11 words Corpus 2: 328 sentences contain more than 11words Result : Corpus 1 contains 41 effective rules; corpus 2 contains 18 effective rules Conclusion: Long sentences show worse result than short ones, for they always contain combine rules of small ones this made both long and short rule difficult to be found. On the other hand, short sentences have advantage of number much more than long ones. Thus corpus collection should mainly base on short sentences  V  CONCLUSIONS  AND  SUMMARY Feasibility of the method of grammar rule mining base on association rule mining by our experiments. Technology about use data mining on natural language process also show it’s power by the experiments. Corpus from different background show different rule-set, so our method need corpus contains sentences with same background. If further work shows differences between rule-set, we can do better on combined corpus. In addition, through researches on experiment result, we find that use frequency of rules have statistic relationship with it’s length, further study may shows more about this. Another kind of mining mission use words instead of POS express grammar rules, this kind of work do help on not only grammar rule mining but also on high level natural language process work such as syntax or semantics, which has high research value too Feasibility of the method of grammar rule mining base on association rule mining by our experiments. Technology about use data mining on natural language process also show it’s power by the experiments. Corpus from different background show different rule-set, so our method need corpus contains sentences with same background. If further work shows differences between rule-set, we can do better on combined corpus. In addition, through researches on experiment result, we find that use frequency of rules have statistic relationship with it’s length, further study may shows more about this. Another kind of mining mission use words instead of POS express grammar rules, this kind of work do help on not only grammar rule mining but also on high level natural language process work such as syntax or semantics, which has high research value too  R EFERENCES   1  Christopher D.Manning, Hinrich Schutze, “Foundations of Statistical Natural Language Processing”, MIT Press, June 1999 2  Cheng shu ming, “Discuss on the modern Chinese grammar units Chinese Journal, 2006 vol.12 3  Sun yun chuan, Bie rong fang, “Study on refinement of produce-rule base”, Journal of Beijing Normal University \(Natural Science Aug.2003, Vol. 39 No. 4 4  Zheng xu ling, Zhou chang le, “Method of access Chinese semantic relation rule base on association rule mining”, Journal of Xiamen University \(Natural Science\, May 2007, Vol. 46 No. 3 5  Chen an, Chen ning, “Data mining technology and application Science Press, March 2006 6  Zhu de xi, “Study on modern Chinese grammar”, Commercial Press 1980 7  Fan kai tai, “Analysis on modern Chinese grammar”, East China Normal University Press, 2002 8  Practical modern Chinese grammar”, Huang cheng wen, Knowledge press, 2003    
268 


  Proceedings of the Sev e nth Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  K u nming, 12-15 J u ly 2008  S te p 3  B a sed on t h e quest i o nsr el at i ons hi p  m a ps o b t a i n e d  in S t ep 2 we can calculate t h e rele va nce degree bet w een co n c ep ts in qu estion s t h at t h e learn e r failed t h e qu estion  and t h en fai l ed the question w h ere   and  T h en, we ca n get the 223failure to failure\224 co nce p tsr el at i ons hi p t a bl e, as sh o w n  i n  T a bl e 9 F u rt he rm ore we can  cal cul a t e t h e re l e vance  deg r ee  bet w ee n c onc ept s i n q u est i ons t h at t h e que st i o n  is correctly learned a n d t h e que s tion  is correctly learn e d too. T h e n we ca n get the  223correct-to-c o rrect\224 concept s rel a t i onshi p t a bl e, as shown i n  T a bl e 10 i Q j Q  5 1 003  1  1  3 1 C C 003  0 5  1  4 1 C C 003  0 1 8  0  6  5 1 C C 003  0 2  0  6 6 7  1 2 C C 003  0 7 5  0  7 5  3 2 C C 003  0 5  1  4 2 C C 003  0 1 5  0  5  5 2 C C 003  0 2 6 7  0  6 6 7  1 3 C C 003  0 3  0  6  2 3 C C 003  0 4  0  8  4 3 C C 003  0 1 8  0  6  5 3 C C 003  0 2  0  4  1 4 C C 003  0 3  1  2 4 C C 003  0 2  0  6 6 7  3 4 C C 003  0 3  1  5 4 C C 003  0 2  0  6 6 7  1 5 C C 003  0 5  1  2 5 C C 003  0 5  0  5  3 5 C C 003  0 5  1  4 5 C C 003  0 3  1   T a bl e 10. C o rrect t o-correct concept s rel a t i onshi p t a bl e C o n c e p t s r e l a t i o n s h i p  R e l e v a n c e  d e g r e e  C o n f i d e n c e  2 1 C C 003  0  0  4 1 C C 003  0  0  5 1 C C 003  0 2  0  6 6 7  1 2 C C 003  0 5  0  5  4 2 C C 003  0  0  5 2 C C 003  0 2 6 7  0  6 6 7  1 3 C C 003  0  0  2 3 C C 003  0  0  4 3 C C 003  0  0  5 3 C C 003  0  0  1 4 C C 003  0  0  5 4 C C 003  0 2  0  6 6 7  1 5 C C 003  0 3  1  2 5 C C 003  0 4  1  4 5 C C 003  0 3  1  S te p 4  A s s u m e t h at t h e t h r e sh ol d val u e g i ven by t h e  us er is 0  35 th en  by co m b in in g t h e con cep ts-relatio n s h i p tab l es i e., T a bl e 9 a nd T a bl e 1 0  o b t a i n ed i n S t e p 3  we can ge t  t h e com b i n ed c once p t s r el at i o nshi p t a bl e as sho w n i n T a bl e 11    T a bl e 1 1 C o m b i n ed concept s rel a t i onshi p t a bl e Concepts rel a t i onship Relevance degree in the failure-to failure concepts rel a t i onship table Relevance degree in the  correct-to correct concepts rel a t i onship table Relevance degree in the  com b ined concepts rel a t i onship table Confidence in the com b ined concepts rel a t i onship table 2 1 C C 003  1  0    3 1 C C 003  0 5   0  5  1  4 1 C C 003  0 1 8  0    5 1 C C 003  0 2  0  2  0 2  0 6 7  1 2 C C 003  0 7 5  0  5  0 7 5  0 7 5  3 2 C C 003  0 5   0  5  1  4 2 C C 003  0 1 5  0    5 2 C C 003  0 2 6 7  0  2 6 7  0 2 6 7  0 6 7  1 3 C C 003  0 3  0    2 3 C C 003  0 4  0    4 3 C C 003  0 1 8  0    5 3 C C 003  0 2  0    1 4 C C 003  0 3  0    2 4 C C 003  0 2   0  2  0 6 7  3 4 C C 003  0 3   0  3  1  5 4 C C 003  0 2  0  2  0 2  0 6 7  1 5 C C 003  0 5  0  3    2 5 C C 003  0 5  0  4  0 5  0  5  3 5 C C 003  0 5   0  5  1  4 5 C C 003  0 3  0  3  0 3  1   Ass u m e that the m i nim u m  confide n ce gi ven by the user i s 0  7 5  de l e t i ng t h e rel a t i ons hi ps wh ose con f i d e n ce a r e  lo wer th an th e m i n i m u m co n f id en ce i.e.; < 0.75  we can g e t th e co m p leted co n c ep t relatio n s h i p table, as sh own i n  T a bl e 12  T a bl e 12. C o m p l e t e d concept s rel a t i onshi p t a bl e C o n c e p t s r e l a t i o n s h i p  R e l e vance degree Confide n c e 3 1 C C 003  0 5  1  1 2 C C 003  0 7 5  0  7 5  3 2 C C 003  0 5  1  3 4 C C 003  0 3  1  3 5 C C 003  0 5  1  4 5 C C 003  0 3  1   B a sed o n t h e com p l e t e d c once p t s r el at i o nshi p t a bl e sho w n i n T a bl e 12 we ca n con s t r uct t h e c once p t m a p, as shown i n Fi gure 3   3082 004 i Q j Q  T a bl e 9. Fai l u re-t o-fai l u re concept s rel a t i onshi p t a bl e Co n cep t s r e l a t i o n s h i p  R e l e v a n ce d e g r e e  Co n f i d ence 2 1 C C 001 001 i 5 1 001 001 j  j i 


  Proceedings of the Sev e nth Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  K u nming, 12-15 J u ly 2008   0 3 0 75 0.5 0 5 0.5 0 3 1 C 2 C 3 C 4 C 5 C  Fi gure 3 The com p l e t e d concept m a p  I t sh ou l d  b e noted t h at th e con cep t m a p constr u c ted b y  t h e m e t h o d p r esent e d i n  7  i s show n i n Fi gu re 4. B y  com p aring Fi gure 3 a n d Figure 4 we c a n see t h at t h e rel a t i ons hi ps and  in t h e co m p leted  conce p t m a p c onst r uct e d by t h e pr o pose d m e t h o d s h ow n i n  Fi gu re  3 have been rem oved  Fr om  t h e gra d e m a t r i x  G and th e qu estion s co n c ep ts m a p p i n g m a trix  QC   w e c a n s e e th at  the learners and  who c o rrectly ans w ered the  que st i o n   and th e q u e stio n  do not correctly ans w er th e qu estion It indicates tha t the learners  and  who c o rrectly learne d the concept and t h e conce p t  still failed to l earn th e co n cep t i.e., th e co n c ep t  sh ou l d no t b e  th e pre-requ ired con c ep t of th e con c ep t  and t h e c o ncept  The r efore  the c o ncept m a p con s t r uct e d by t h e p r op ose d m e t hod i s m o re reas ona bl e t h an t h e one const r uct e d by t h e m e t hod present e d i n 7   1 5 C C 003 2 5 C C 003 4 S 5 S 1 Q 2 Q 5 Q 4 S 5 S 1 C 2 C 5 C 5 C 1 C  2 C 0 3 0 3 1 0 5 0.5 0 15 0.4 1 C 2 C 3 C 4 C 5 C  Fi gu re 4 T h e co nce p t m a p co nst r uct e d  by Lee et al  222 s m e t hod [7   4 Concl u si ons In  t h i s  pa per  we ha ve p r ese n t e d  a ne w m e t h o d f o r au to m a tical ly con s tru c ting con c ep t m a p s  fo r adap ti v e  learning system s. The proposed m e th od is u s ing t h e a-p r i o ri algorithm 1] and consi d e r s t h e a ssoci ation rules t h at que st i ons a r e not co rrect l y a n swe r e d a n d a ssoci at i o n  rul e s that questions are c o rrectly ans w ere d  for constructing conce p t m a ps. It p r ovi des a u s ef ul  way t o  aut o m a ti cal ly co nstru c t con c ep t m a p s fo r ad aptive learning system s. It can o v erc o m e t h e dra w ba ck  of t h e m e t hod prese n t e d i n  7  and ca n provi d e m o re reas onab le resu lts fo r co n s t r u c ting  concept m a ps in adaptive learning system s Acknowledgements Th is wo rk was su ppo rted in p a rt  b y  t h e Nation a l Scien ce Council, Rep u b lic o f C h in a un der Gr an t N S C 95-2221-E-01 1-1 16-M Y 2 References 1   R Agrawal an d R. Srik an t  223Fast al g o rith m s fo r mining as soci ation rules\224 Proceedi ngs of the 20t h In tern ation a l Co nferen ce o n  Ve ry La rge Data base  Sant i a go, C h i l e pp. 487-499, 1994 2  S. M  B a i an d S  M  C h e n 223 A ut om at i cally constructing grade m e m b ershi p fun c tion s fo r stud en ts\222 evaluation for fuzzy gradi n g syste m s\224, Proc eedings of t h e 2 0 0 6 Wo rl d Aut o m a t i o n C o n g res s B uda pest   Hungary 2006 3   S. M. Bai and  S. M. C h en 223Ev a lu ating stud en ts\222 learning ac hi evem ent us ing fuzzy m e m b ership fun c tion s an d fu zzy ru les\224, Ex p e rt Syst e m s with  Appl i cat i ons, Vol  34, No. 1, pp. 339-410, 2008 4  S. M  B a i an d S  M  C h e n 223A  new ap p r oac h  f o r  au to m a tical ly co nstru c ting co n c ep t m a p s b a sed on fuzzy rules,\224  Proceedi n gs of the Twe n tieth In tern ation a l C o nferen ce o n  Industrial, E ngi neeri n g Oth e r App licatio n s of App l ied  In tellig en t  System s   Ky ot o, Japan, pp. 155-165, 2007 5  V C a rc hi ol o   A Lo n ghe u a n d M  M a l g eri  223A da pt i v e  fo rm ati v e pat h s i n a we bbas e d l ear ni n g en vi r onm ent 224  Ed ucat i onal  T echn o l o gy   Soci et y  V o l  5 N o  4  p p   64-75, 2002 6  G J H w an g, \223 A co nce p t u al m a p m odel fo r d e v e l o p i n g in t e llig en t tu toring system 224, Co m p u t ers Educat i on, Vol  40, No. 3, pp. 217-235, 2003  7   C  H  L e e  G   G  L e e  a n d Y  H  L e u  223 A p p l i c a t i o n o f  aut o m a t i call y  con s t r uct e d c o ncept m a p of l earni ng t o  conce p t u al  di agn o si s o f e-l e a r ni ng\224  Acce pt ed a n d t o  appear i n Expert Sy st em s wi t h Appl i cat i ons, 2008 8  J. D N ova k, \223Lea rni n g  creat i n g  and usi n g  k nowledg e: C o n c ep t m a p s as facilitativ e to o l s i n  sch ool s a n d co rp orat i ons 224, La wre n ce Erl b a u m  Associ at es, 1998 9  W. J Popham  Classroo m as sessm ent: W h at teachers  need t o k n o w  Pears o n Al l y n B aco n  p p  2 2 2 22 7 1999 1 0  L. A  Za deh   223Fuzzy set s 224 In fo rm at i on an d C o nt r o l   Vol  8, pp. 338-353, 1965   3083 


  Proceedings of the Sev e nth Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  K u nming, 12-15 J u ly 2008  al gori t h m   Our performance st udy shows  that DSCFCI  algorithm is ef ficient and ef fective  Refer e nces 1] C Gia nnel l a, J Ha n, J  Pei, et al. Mi ning fre quent p a ttern s i n  d a t a stream s at mu ltip le tim e g r an u l arities  G]. In  H Kar g up ta, A Jo sh i, K Siv a ku m a r  et al, ed s Next  Ge nerat i on Dat a M i ni ng C a m b ri dg e, M a ss  M I T Press, 2003  2 G S Mank u, R Mo t w an i. App r ox im a t e f r e q u e n c y cou n t s ove r st ream i ng dat a  C    The 28 th Int 222 l Confere n ce on V e ry Lar g e Data Bases \(VL D B 2002 HongKong, 2002 3  Aras u A M a nk u G S  A p pr oxi m a t e co unt s a n d  q u a n tiles ov er slid in g wi n d o w s. In Pro c eed ing s  o f  th e 23 rd ACM S I G M OD SI G AC T S I G A R T  Sym posi u m on Pri n ci pl es o f  Dat a base Sy st e m s. Pari s France  AC M Press, 2004 4 Datar M, Gio n i s A, In d y k P   Mo t w an i  R. Main tain in g str eam stat is tics o v e r slid in g  w i ndow s In  Proceedi ngs of the 13 th A nnu al ACMSIA M  Sy m p o s iu m on  Discrete Al g o rith m s San Fran cisco   USA  AC M Press, 2002 5 N Pas quier  Y Bastide  R T a ouil, et al Discoveri n g freq u e n t clo s ed item s e t s fo r asso ciatio n ru les[C]. In   Beeri C, et al, eds  Proc of the 17 th I n t 222 l Co nf  on Dat a base Theory B e rl i n  Spri nger V e rl ag, 1999  6  W a n g Jian yon g. Clo s et sear ch ing fo r t h e b e st st rat e gi es f o r m i ni ng f r e que nt cl ose d i t e m s et s. In  Pr oc. N i n t h A C M SIGK DD In t\222 1 Co nf  on  K now ledg e D i scov er y an d  D a t a Min i n g  W a shi ngt on,DC Aug.2003    280 


association mining The Likelihood Ratio Test fails to extract features also belonging to common vocabulary and it makes the extraction dependent on the feature position in the sentence leading to low recall The dBNP and bBNP based methods yield low recall due to the fact that the product features do not occur with the article the in front of them very often The Association Mining approach returns all frequent nouns which decreases precision Our results suggest that the choice of algorithm to use depends on the targeted dataset If it consists of mainly on-topic content the results of Table 10 indicate that the Association Mining algorithm is better suited for this task due to its high recall If the dataset consists of a mixture of onand off-topic content our results suggest that the Likelihood Ratio Test based algorithm would perform better due to its ability to distinguish and 002lter out the off-topic features For future work we plan to extend the Likelihood Ratio Test methods especially the dBNP based approach by other determiners such as a or this  which should increase the recall of this method Another possibility which we will investigate regards the BNP patterns The current Likelihood Ratio Test approach is not capable of dealing with discontinuous feature phrases for example in 5 the quality of the pictures is great the feature would be picture quality  This problem could be addressed by introducing wildcards in the BNP patterns We will also investigate whether there are any methods in order to calculate an optimal threshold for the candidate feature extraction in order to increase the recall of the Likelihood Ratio Test based algorithm We plan to investigate whether a deeper linguistic analysis e.g with a dependency parser can improve the feature extraction Acknowledgements The project was funded by means of the German Federal Ministry of Economy and Technology under the promotional reference 01MQ07012 The authors take the responsibility for the contents The information in this document is proprietary to the following Theseus Texo consortium members Technische Universit  at Darmstadt The information in this document is provided as is and no guarantee or warranty is given that the information is 002t for any particular purpose The above referenced consortium members shall have no liability for damages of any kind including without limitation direct special indirect or consequential damages that may result from the use of these materials subject to any liability which is mandatory due to applicable law Copyright 2008 by Technische Universit  at Darmstadt References  R Agra w al and R Srikant F ast algorithms for mining association rules Proc 20th Int Conf Very Large Data Bases VLDB  1215:487–499 1994  K Bloom N Gar g and S Ar g amon Extracting a ppraisal expressions In HLT-NAACL 2007  pages 308–315 2007  R Bruce and J W iebe Recognizing subjecti vity a case study in manual tagging Natural Language Engineering  5\(02 1999  K Da v e S La wrence and D Pennock Mi ning the peanut gallery opinion extraction and semantic classi\002cation of product reviews In Proceedings of the 12th International Conference on World Wide Web  pages 519–528 New York NY USA 2003 ACM  T  Dunning Accurate methods for the statistics of surprise and coincidence Computational Linguistics  19\(1 1993  O Feiguina and G Lapalme Query-based summ arization of customer reviews In Canadian Conference on AI  pages 452–463 2007  C Fellbaum Wordnet An Electronic Lexical Database  MIT Press 1998  A Ferraresi Building a v ery lar ge corpus of english obtained by web crawling ukwac Master's thesis University of Bologna Italy 2007  M Gamon A Aue S Corston-Oli v er  and E Ringger  Pulse Mining customer opinions from free text In Proceedings of the 6th International Symposium on Intelligent Data Analysis IDA-2006  Springer-Verlag 2005  N Glance M Hurst K Nig am M Sie gler  R Stockton and T Tomokiyo Deriving marketing intelligence from online discussion In Proceedings of the 11th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining  pages 419–428 New York USA 2005 ACM  M Hu and B Liu Mining opinion features in customer reviews In Proceedings of 9th National Conference on Arti\002cial Intelligence  2004  N K obayashi K Inui K T atei shi and T  Fukushima Collecting evaluative expressions for opinion extraction In Proceedings of IJCNLP 2004  pages 596–605 2004  S Morinag a K Y amanishi K T ateishi and T  Fukushima Mining product reputations on the Web In Proceedings of KDD-02 8th ACM International Conference on Knowledge Discovery and Data Mining  pages 341–349 Edmonton CA 2002 ACM Press  A.-M Popescu and O Etzioni Extracting product features and opinions from reviews In Proceedings of HLT-EMNLP-05 the Human Language Technology Conference/Conference on Empirical Methods in Natural Language Processing  pages 339–346 Vancouver CA 2005  H Schmid T reetagger a language independent part-ofspeech tagger Institut fur Maschinelle Sprachverarbeitung Universitat Stuttgart  1995  J W iebe R Bruce and T  O'Hara De v elopment and use of a gold-standard data set for subjectivity classi\002cations In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics  pages 246–253 Association for Computational Linguistics Morristown NJ USA 1999  J Y i T  Nasuka w a R Bunescu and W  Niblack Sentiment analyzer Extracting sentiments about a given topic using natural language processing techniques In Proceeding of ICDM-03 the 3ird IEEE International Conference on Data Mining  pages 427–434 Melbourne US 2003 IEEE Computer Society 
160 
151 


Figure 4 Expected and real number of extracted patterns using two promoter sequence datasets Horizontal axis minimum support vertical axis number of patterns 
85 
85 


a frequency constraint and according to the structure of the dataset These proposals are all based on a global analytical model i.e an interesting approach that needs however to develop complex and speci\036c models As a result they cannot be easily extended to handle complex conjunctions of constraints to incorporate different symbol distributions or different semantics for pattern occurrences To the best of our knowledge no method has been proposed to estimate the number of patterns satisfying a constraint while avoiding to develop a global analytical model Our approach requires only to know how to compute for a given pattern its probability to satisfy the constraint this can be obtained in many situations and it remains ef\036cient in practice by adopting a pattern space sampling scheme 6 Conclusion Using constraints to specify subjective interestingness issues and to support actionable pattern discovery has become popular Constraint-based mining techniques are now well studied for many pattern domains but one of the bottlenecks for using them within Knowledge Discovery processes is the extraction parameter tuning This is especially true in the context of differential mining where domain knowledge is used to provide different datasets to support the search of truly interesting patterns From a user perspective a simple approach would be to get graphics that depict the extraction landscape i.e the number of extracted patterns for many points in the parameter space We developed an ef\036cient technique based on pattern space sampling that provides an estimate on the number of extracted patterns This has been applied to non trivial substring pattern mining tasks and we demonstrated by means of many experiments that the technique is effective It provides reasonable estimates given execution times that enable to probe a large number of points in the parameter space Notice that domain knowledge is also exploited here when selecting the distribution model Future directions of work include to adapt the approach to other pattern domains and to different constraints Another interesting aspect to investigate is the use of more sophisticated sampling schemes e.g that could b e incorporated in the approach when more complex syntactical constraints are handled e.g a grammar to specify the shape of the patterns Acknowledgments This work is partly funded by EU contract IQ FP6-516169 Inductive Queries for Mining Patterns and Models and by the French contract ANR-MDCO14 Bingo2 Knowledge Discovery For and By Inductive Queries We thank Dr Olivier Gandrillon from the Center for Molecular and Cellular Genetics CNRS UMR 5534 who provided the DNA promoter sequences References  J F  Boulicaut L De Raedt and H  M annila e ditors Constraint-Based Mining and Inductive Databases  volume 3848 of LNCS  Springer 2005  C  B resson C K e ime C F a ure Y  Letrillard M  B arbado S San\036lippo N Benhra O Gandrillon and S GoninGiraud Large-scale analysis by SAGE revealed new mechanisms of v-erba oncogene action BMC Genomics  8\(390 2007  L  C ao and C  Z hang Domain-dri v e n actionable kno wledge discovery in the real world In Proceedings PAKDDÕ06 volume 3918 of LNCS  pages 821–830 Springer 2006  G  D ong and J  L i Ef 036cient mining of emer ging patterns discovering trends and differences In Proceedings ACM SIGKDDÕ99  pages 43–52 1999  F  Geerts B  G oethals and J  V  d en Bussche T ight upper bounds on the number of candidate patterns ACM Trans on Database Systems  30\(2 2005  U  K eich and P  A  P e vzner  S ubtle motifs de\036ning the limits of motif 036nding algorithms Bioinformatics  18\(10 2002  S  K ramer  L De Raedt and C  Helma M olecular f eature mining in HIV data In Proceedings KDDÕ01  pages 136 143 2001  L  L hote F  Rioult and A  S oulet A v e rage number of frequent closed patterns in bernouilli and markovian databases In Proceedings IEEE ICDMÕ05  pages 713–716 2005  I  M itasiunaite a nd J.-F  B oulicaut Looking for monotonicity properties of a similarity constraint on sequences In Proceedings of ACM SACÕ06 Data Mining  pages 546–552 2006  I Mitasiunaite and J F  Boulicaut Introducing s oftness i nto inductive queries on string databases In Databases and Information Systems IV  pages 117–132 IOS Press 2007  I Mitasiunaite C Rigotti S Schicklin L  M e yniel J F  Boulicaut and O Gandrillon Extracting signature motifs from promoter sets of differentially expressed genes Technical report LIRIS CNRS UMR 5205 INSA Lyon France 2008 23 pages Submitted  G Ramesh W  M aniatty  a nd M J Zaki F easible itemset distributions in data mining theory and application In Proceedings ACM PODSÕ03  pages 284–295 2003  F  Zelezn  y Ef\036cient sampling in relational feature spaces In Proceedings ILPÕ05  volume 3625 of LNCS  pages 397 413 Springer 2005 
86 
86 


