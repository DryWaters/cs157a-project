A n  I m p r o v e d  M e t h o d  f o r  M i n i n g  G e n e r a l i z e d  F r e q u e n t  I t e m s e t s  B a s e d  o n    Yu Xing Mao, Bai Le Shi Department of Computer and Informati on Technology, Fudan University, Shanghai 200433 CHINA myuxing, bshi}@fudan.edu.cn  Abstract  Mining generalized association rules is closely related to the taxonomy\(is-a hierarchy\ data which exists widely in retail, geography, biology and financial domains. If we use traditional method to mine the generalized association rules, it becomes inefficient because the itemsets will be huge along with the items and levels of taxonomy increasing, and it also wastes lots of time to calculate the support of redundant or unnecessary itemsets. In this paper, we proposes a new efficient method call CBP to partition the transaction database into several smaller ones level by level using correlation of itemsets, which make the mining more efficient by reducing the scanning size of transaction database. By experiments on the real-life transaction database, the results show that our CBP_based algorithms outperform the well-known algorithms  1. Introduction  Unlike traditional association rules [1   researches perform on single level data, generalized association rules researches study the taxonomy data Besides the prototypical application of supermarket basket data, an example of financial application also has the taxonomy structure. In Figure 1, the financial transaction encode  with 4 bits. The 0*** represents the first bit of transaction code, and means the personal transaction. The 01** represents the first two bits of transaction code, and means the personal transactions of credit card. The 015* represents the first three bits of transaction code, and means the personal transaction of credit card query. The leaf node 0154 represents the complete transaction code, and means the personal transaction of querying the balance of a credit card The semantic of mining generalized association rule in this area is to find the association of customers behavior, such as “80% customers will query their balance after withdrawing cash”. Through analysis the association behavior of customers, we can provide more specific financial products and services to specific customers  Figure 1 Taxonomy data of financial transaction code If we use traditional algorithms [1   i rectly   we will meet with several problems. Firstly, the inefficiency of the algorithms will be outstanding. In Figure 1, traditional methods only mine single level frequent itemsets from the leaf items. When mining the generalized frequent itemsets, we have to mine all the items including the leaf items and generalized items which usually doubled the number of leaf items Secondly, the traditional algorithms will waste lots of time to calculate the support of redundant or unnecessary itemsets After the generalized association rules first introduced by R. Strikant d Jiaw e i Han 5], th e recently optimization of the algorithms have two strategies. On 7 is t o o p ti m i ze th e m i n i ng algorithms by using efficient way of lattice traversal or support counting, and another [8 9 i s t o redu ce t h e size of items or transactions. We follow the second direction and find that the frequency of different itemsets occurs together is different in practice. In Figure 1 for example, 01** and 02** are all personal credit card transactions, so maybe 80% customers do these two transactions together. While only 0.01 customers do the transactions like 01** and 09 together since 09** is the personal check transaction which has weak relation to 01** which is the personal credit card transaction. This situation widely exists in the Correlation Between Items 
International Symposium on Computer Science and its Applications 978-0-7695-3428-2/08 $25.00 © 2008 IEEE DOI 10.1109/CSA.2008.25 56 
International Symposium on Computer Science and its Applications 978-0-7695-3428-2/08 $25.00 © 2008 IEEE DOI 10.1109/CSA.2008.25 56 


other retail transaction databases like the supermarket basket data In this paper, we propose a new efficient algorithm called CBP_SLAR to mine the generalized frequent itemsets from taxonomy data. We partition the transaction database into several smaller ones level by level using the correlation of items. It makes the mining more efficient by reducing the scanning size of transaction database. In addition, we also propose several optimizing rules to prune the redundant generalized frequent itemsets, which can reduce many unnecessary itemsets. By experiments on the real-life transaction database, the results show that our CBP_based algorithms outperform the well-known Apriori_based algorithms without any loss This paper is organized as follows. Section 2 is problem statements and term definitions related to the generalized frequent itemsets. In Section 3, the correlation theory and optimized pruning rules are given. The CBP_SLAR algorithms are  illustrated in Section 4. In Section 5, by experiments, we compared our algorithms with the well-known algorithms. The study is concluded in the last section  2. Problem Statement  Suppose transaction database D= { t 1 t 2 t n  t i is any single transaction, and itemset s I= { I 1 I 2 I m  I a  is a subset of I  In this paper, we present a transaction set including itemset I a  as D a t i I a   t i t i   D Next we depict it with a directed acyclic graph  Definition 1  Taxonomy Tree d Subtree We use the DAG=\(V E\ to depict V is the set of all item nodes, E is the set of all edges, v 0 represents the root node, each node represents a generalized item For any simple path P=\(v 0 v 1  v n then 1 n-1 is the parent item of v n noted parent\(v n In contrast, v n is the child item of v n-1 noted child\(v n-1  2 0 v n-1 are the ancestor items of v n noted ancestor\(v n In contrast, v 1  v n are the descendant  items of v 0 noted descendant\(v 0  3\ For any items x, y, z, if x and y are the children of z, then x and y are brother items 4\he depth of item v refers to the level of v located in taxonomy data 5\ If depth\(v i equals to depth\(v j we call v i and v j  sibling items 6\ An item without a child item called a leaf item 7\ The items, except root and leaf items, are called interim items or generalized items 8\imilarly subtree x is noted as G V E V is the set of x and their descendants, E is the set of edges  between x and all the items of its descendants For simplification, the subtre e mentioned in this paper only means the items x and all its descendants  Definition 2 Generalized frequent itemsets In a transaction database, the support of any generalized item g equals to |D g D|, and if |D g D| is great than then g is frequent in D, where is the  minimum support threshold defined by user. |D g is the number of transactions contains g  Definition 3 Generalized item and item set To generalized item g i and generalized item set G if   g j   G g i equals to g j or g i is the ancestor of g j then we say g i  belongs to G noted as g i    G to generalized item set G 1 and G 2 if  g i   G 1 then g i   G 2 we say G 1   G 2   Definition 4  Generalized Association Rules with Taxonomy  1\he same level generalized association rules SL AR\ {A => B | A, B   A  B A, B   and   g 1  A  g 2  B, depth\(g 1 depth\(g 2  2\he cross level generalized association rules  CLAR\ {A => B | A, B   A  B and    g 1   A    g 2  B, ancestor\(g 1 g 2 or ancestor \(g 2 g 1 and  g 1  A  g 2  B, depth\(g 1   depth\(g 2   In this paper, we only apply our method to SLAR Definition 5 Strong generalized association rule  The confidence of generalized association rule A => B is the percentage of transaction numbers in transaction database D, which contain generalized item A and B to the transaction numbers of the generalized item B That is conditional probability P B| A noted as confidence A => B which equals to P B| A If P B| A  then we say A => B is a strong  association rule is the minimum confidence defined by the user   In this paper, we still use the support-confidence measurement system and the different minimum supports for different levels of taxonomy  3. Correlation theory  3.1 The correlation between the items There are a variety of measures can be used normalized measures are preferable. Therefore, we choose the Jaccard coefficien m e a s u r e th e  relationship among generalized itemsets Definition 6 Correlation of generalized items g i and g j   The correlation between two generalized items is defined as        ij ij ij i j ij gg gg ij gg g g gg DD DD cor g g DD D D DD      
57 
57 


Formula 3-1  The definition means the two items have a strong correlation if they occur together many times. From the definition, we have the following property Property 1  If cor g i g j   minsup then {g i g j is not frequent  Definition 7 The relationship between generalized item g i and g j   With generalized items g i and g j if cor g i g j 0 then we can say that generalized item g i  is non-related with generalized item g j expressed as   t i g i g j  t i g i   I g j  I t i  D}. If cor g i g j  mincor then we can say that generalized item g i is weak-related with generalized item g j except that we can say generalized item g i is related with generalized item g j Totally, we define min sup min sup 0     ij non related cor g g weak related related      012  Formula 3-2  To avoid the loss of generalized frequent itemsets we choose the minimum support of all levels as the minimum correlation, that is min min sup min   1   cor ll depth   Of course, we can choose some bigger mincor and partition the taxonomy data more efficiently. However, it will cause some loss of frequent itemsets. Therefore, it is the trade-off of efficiency and loss  Definition 8 The relationship between Subtree G i and subtree G j  For subtree G i and G j rooted with g i and g j  separately, if cor g i g j 0 then we can say subtree G i and G j are non-related when cor g i g j  mincor then we can say subtree G i and G j are weakrelated otherwise the two subtrees are related   To partition the generalized items into different clusters, we have to calculate the correlation between two set of generalized itemsets. Here we give the definition of item-cluster Definition 9 Item-Cluster We call a set of related  generalized itemsets as an item-cluster  Definition 10 The correlation between two item clusters C i and C j  There are many ways to evaluate the correlation between two item-clusters We use the underlying  function    max    qir j ij qr pCpC COR C C cor p p    Formula 3-3  The correlation COR\(C i C j between two itemclusters C i and C j is the maximal value among all elements of these two item-clusters. When there exists a pair of elements of two item-clusters related, we can say these two item-clusters are related 3.2 How to partition the transaction database based on correlation between items Our method partitions the transaction database along with the generation of frequent generalized itemsets using the top-down and recursion policy. It includes four major steps Step 1  Choose the start level for partition The partition can start from any level l of the taxonomy Usually, it starts from the level 2 Step 2 Counting the support of 1-itemsets and prepruning For each generalized item of level l we generate the frequent 1-itemsets at first based on the partitioned transaction databases of level l 1, then prune all the descendants of the items, which are not frequent Step 3 Counting the support of 2-itemsets and forming the item-Clusters During the generating all the frequent k-itemsets of level l we use the formula 31 and 3-3 to calculate the correlation between items or item-clusters. we use hierarchical clustering method to form the item-clusters of level l such as C 1 C 2 C 3 C m  Step 4 Partitioning the transaction database According to the result of the item-clusters of level l  we partition the transaction database D into several smaller transaction databases such as D 1 D 2 D 3 D m and each D i with only items in C i left, where 1 j m Repeating the step 2 to step 4 on the level l 1 until the upper level of leaf level of taxonomy, then we can get all the frequent generalized itemsets level by level Because it partitions the transaction databases base on the level l so the number of partition transaction database of level l 1 is usually more than that of level l and the size of each partition transaction database of level l 1 are usually smaller than that of level l It will benefit the counting the support of itemsets by reducing the scan I/O time of transaction database  3.3 The optimized pruning rules based on the correlation theory Beside we can use the correlation theory of item to partition the transaction database, we also propose some optimized pruning rules, which also will benefit our algorithms Theorem 1  If generalized itemsets g i and g j are nonrelated, then any descendants of subtree rooted with g i  and g j are non-related  Corollary 1  If generalized itemsets g i and g j are weak-related, then any descendants of subtree rooted with g i and g j are weak-related  
58 
58 


Theorem 2  If the generalized itemsets g i and g j are weakly related. Then g i and g j cannot form frequent itemsets  Corollary 2 If generalized itemsets g i and g j are weakly related, then any descendants of subtrees rooted with g i and g j cannot form frequent itemsets  Optimization rule 1  Any generalized frequent itemsets cannot contain the ancestor and descendant nodes at the same time  Optimization rule 2  Any generalized itemsets containing two non-related subtrees’ nodes \(or weakrelated subtrees’ nodes\ cannot be frequent  Optimization rule 3 If a generalized itemsets has one subset, the subset’s ancestor cannot form frequent itemsets, and thus this generalized itemsets cannot be frequent   4. Algorithms for mining generalized frequent itemset  Our algorithm includes two parts: the CBP_SLAR is the main algorithm to mine generalized frequent itemsets and the CBP is the procedure called by the CBP_SLAR to partition the transaction database based on the correlation of items 4.1. Algorithm CBP_SLAR The CBP_SLAR algorithm generates the generalized frequent itemsets from the top to bottom of the taxonomy tree level by level using different level minimum support  The main algorithm follows Input  D\(1\- the original transaction database; D- the partitioned transaction database the taxonomy tree minsup  l the minimum support of level l Dstack- the stack keeps the partitioned transaction database Fstack- the stack keeps the mark of level; Fstack- the stack keeps the frequent itemsets Output LL l The frequent generalized itemsets of level l  1  Push D\(1\ to Dstack; Push 1 to FStack 2  for Dstack do 3  Pop up D from Dstack 4  Pop up l from FStack 5   if depth l   then  6  pop up L l 1,1\ from Kstack 7  end if  8   Apriori_gen L l k\ntil L l k-1 9  add all L l k\L l  10  if L l 2  then  11  Compute COR C i C j  12   CBP  COR\(C i C j  13  end if  14  LL l  k L l k 15  end for  4.2 Algorithm CBP CBP is the key function to partition the transaction database and called by the main algorithms CBP_SLAR  Algorithm 4.2  CBP COR\(C i C j  Partitions the transaction database according to the correlation between item-clusters C i and C j  Input D- the transaction database the taxonomy tree; COR\(C i C j the correlation of item-clusters C i  and C j  mincor the minimum correlation of itemclusters Output Dstack- the partitioned transaction database  1  Initialize item-Cluster 2  for Max\(COR\(C i C j  012  mincor  3  Merge C j to C i  4  Recompute COR\(C i C j  5  end for  6  for each item-cluster do  7  partition D by item-clusters 8  push D into DStack 9  partition L l 1\y item-clusters 10  push L l 1\to KStack 11  Push l 1 into FStack 12  end for   5. Performance study  To evaluate the efficiency of CBP_SLAR algorithm we develop all the algorithms in JAVA language. We tested and compared it with the well-known algorithms on a HP DL380 G3 server with duo Xeon 2.8 GHZ CPU and 2GB main memory running Windows Server 2003. The databases DB1I1 and DB2I2 we used for the experiment are real-life financial transaction databases from a large commercial bank. The characteristics of two transaction databases are summarized in Table 5.1 Table 5.1 Real-life transaction databases parameters Databases Number of Transactions Number of Items Max Length of items Level o f Taxonomy Distribution of items DB1I1 About 250K 165 8 5 Balanced DB2I2 About 360K 165 10 5 Skewed In Table 5.1, the DB2I2 is the item-skewed transaction database where the proportion of some items is very high, while the items in DB1I1 are itembalanced relatively. Our experiments work on both transaction databases. Because ML_series algorithms have several varieties, we select the ML_TML1 as the representation to compare with our CBP_SLAR algorithms 
59 
59 


Group one - DB1I1 used We first examine the algorithms with respect to the different minimum support thresholds with different Geometric Proportion GP\. For example, if we use 0.4 0.2 0.1 and 0.05 at four levels with GP equals to 2. Here 0.05 is the minimum support of the fourth level, and 0.1% is the minimum support of the third level, which equals to 0.05% multiply GP \(equals to 2\, and 0.2% is the minimum support of the second level, and 0.4% is the minimum support of the first level.. We let mincor  always equals to the minimal support of the leaf level according to the definition  Figure 4  Performance study by different minimum support with GP=1.5 on DB1I1   Figure 5  Performance study by different minimum support with GP=1.1 on DB1I1 In Figure 4, we use GP=1.5, the curves show that CBP_SLAR runs faster than the ML_TML1 by 32% to 48% without loss of frequent itemsets. In Figure 5, we use GP=1.1, the curves show that CBP_SLAR runs faster than the ML_TML1 by 40% to 58%. The above Figures show two interesting features. First, when the minimum support decreases, the CBP_SLAR will be more efficient compared with the ML_TML1. Second when the geometric proportion of minimum support decreases, the CBP_SLAR will be more efficient compared with the ML_TML1. All these features of CBP_SLAR are desirable in mining more weak association rules in large transaction databases Group two - DB2I2 used We test CBP_SLAR with the same minimal support setting on DB2I2, which is item- skewed. The curves of Figure 6 and Figure 7 show that CBP_SLAR runs faster than the ML_T2L1 by 12% to 24% and 26% to 32% respectively. From the Figures 6 and 7, we can see the same performance advantage of CBP_SLAR but the efficiency is reduced compare with Figure 4 and 5. That is because the DB2I2 is item- skewed and many items are tightly correlated, and cannot be partitioned. Therefore, the performance of CBP_SLAR is more efficient on the balanced data than on the skewed data  Figure 6  Performance study by different minimum support with GP=1.5 on DB1I2  Figure 7  Performance study by different minimum support with GP=1.1 on DB1I2 Group three - DB1I1 used Now we examine the algorithms in relevant to the number of transactions in the database. In Figure 8, we use the 50k, 100k, 150k 200k and 250k transactions with the minimum support being \(0.84, 0.56, 0.38, 0.25, GP=1.5\ The curves show that CBP_SLAR runs faster than the ML_TML1 by 28% to 52%. In Figure 9, we use the 50k, 100k 150k, 200k and 250k transactions with the minimum support being \(0.33, 0.30, 0.28, 0.25, GP=1.1\. The curves show that CBP_SLAR runs faster than the ML_TML1 by 38% to 58%. These two Figures show that the curve of CBP_SLAR is more flat than the curve of ML_TMl1 when the number of transaction is increasing. Therefore, we can draw a conclusion that CBP_SLAR has relatively good “scale-up” behavior which is desirable in the processing of large transaction databases using relatively smaller minimum supports 
60 
60 


 Figure 8  Performance study by different transaction numbers with GP=1.5 on DB2I1  Figure 9  Performance study by different transaction numbers with GP=1.1 on DB2I1  6. Conclusions and future work  To improve the efficiency of mining generalized frequent itemsets from taxonomy data, this paper propose a new method call CBP_SLAR algorithms to mine generalized itemsets. First, we gave the correlation theory, which is used to partition the transaction database into several smaller ones level by level and reduce the scanning size of transaction database. secondly, we also propose several optimizing rules to prune the redundant generalized itemsets which can reduce the amount of unnecessary itemsets By experiments on the real-life transaction database the results show that our CBP_based algorithms outperform the well-known ML_series algorithms by improving at least 30% to 40% without loss of the frequent itemsets In addition, our algorithms will be more efficient with the transactions increasing and minimum supports descending, and will do benefit to the large taxonomy data in most practice applications with either itemskewed or item-balanced characters. As the universal method, CBP_based algorithm can be used to other research areas with taxonomy such as maximum or closed generalized frequent itemsets, which will be our future work  References  1 A g ra w a l, R., Im ie linsk i, T a n d Sw a m i A Mini ng  association rules between sets of items in large databases. In Proceedings of the SIGMOD. Washington DC. Pages 207 216. 1993 2 A g ra wa l, R. a n d Srik a n t, R  Fa st A l g o rithm s  f o r m i ning  association rules. In Proceedings of the VLDB Conference Santiago, Chile. 1994 3 H a n, J  a nd P e i, J  Mi ning f r e que nt pa tte r n s by pa tte r n  growth: Methodology and implications. SIGKDD Explorations 2, pages 14–20. 2000 4 Sr ik a n t, R  a n d A g r a w a l R   Mi ning g e ne r a liz e d  association rules. In proceedings of the 21 st VLDB Conference Zurich, Switzerland. 1995 5 Jia w e i  Ha n,Yong jia n Fu Disc ov e r y o f Multiple L e v e l  Association Rules from Large Databases. In Proceedings of the 21 st VLDB Conference. Zurich, Switzerland. Pages 420431. 1995 6 J o c h e n H i p p  A ndr e a s My k a R udig e r w i r t h ul r i c h  guntzer. A new algorithm for faster mining of generalized association rules. In proceedings of the 2 nd PKDD. Pages 7482, Nantes, France. 1998  Kri t s ad a S r i p h aew  T h an aru k T h eer a m u n k o n g  A n e w method for finding generalized frequent itemsets in generalized association rule mining. In proceedings of the 7 th  ISCC. 2002  8 Y  J  T s ay J  Y  C h ia ng C B A R a n e f f i c i e n t m e thod f o r  mining association rules, Knowledge-Based Systems. Pages 99-105. 2005  9 H u ng P in C h iu Y i T s ung T a ng a n d K u nL i n H s ie h A  Cluster-Based Method for Mining Generalized Fuzzy Association Rules. In proceedings of the 1 st ICICIC. 2006  10 K unk le D  Zha n g D  C o o p e r m a n G  Mining Fr e que n t  Generalized Itemsets and Generalized Association Rule Without Redundancy. Journal of Computer Science and Technology 23\(1\. Pages 77-102. 2008 11 Rich ard O. D u d a an d  P e ter E Hard   P a ttern  Classification and Scene Analysis. A Wiley-interscience Publication, New York. 1973  
61 
61 


7 5769 l00 97.67 l005 I00 Avg Accuracy ______ __95.59 95.98 89.5 8954 BSTC/RCBT To keep comparisons fair we ran SVM and randomForest on the same genes selected by our entropy discretization except with their original undiscretized gene expression values SVM was run with its default radial kernel We ran randomForest 10 times with its default 500 trees for ALL LC and OC and its accuracy was constant For PC we had to increase randomForest's number of trees to 1000 before its accuracy stabilized over the 10 runs Table III contains the number of class 0/1 samples in the clinically determined training set the number of genes selected by our entropy discretization and our experimental results As shown in this table the overall average accuracies of BSTC and RCBT are again best at about 96 each When compared against RCBT SVM and randomForest on the individual tests we can see that BSTC is alone in having 100 accuracy on the majority of datasets However BSTC's performance on the preliminary AML/ALL dataset test is relatively poor This is likely due to over fitting Every error BSTC made mistook a class 0 AML test sample for a class 1 ALL test sample i.e all errors were made in this same direction And the ALL training data has both i about 2.5 times as many class 1 samples as class 0 samples and ii a small number of total samples/genes When the training set is more balanced and the number of samples/genes is larger we can expect that cancellation of errors will tend to neutralize/balance any over fitting effects in BSTC And BSTC is a method meant primarily for large training sets where CAR-mining is prohibitively expensive As we will see below in Section V-B.1 BSTC s performance is much better for larger AML/ALL training set sizes B Holdout Validation Studies Holdout validation studies make comparisons less susceptible to the choice of a single training dataset and provide performance evaluations that are likely to better represent program behavior in practice We next present results from a thorough holdout validation study completed using 100 different training/test sets from each of the ALL LC PC and OC data sets For these holdout validation tests we benchmark BSTC against Top-k/RCBT because i BSTC/RCBT perform hest in our preliminary experiments ii Top-k/RCBT is the fastest/most accurate CAR-based classifier for microarray data and iii we are interested in BSTC's CAR-related vs Topk/RCBT s CAR based scalability For the holdout validation study we generated training sets of sizes 40 60 and 80 of the total samples Each training set was produced by randomly selecting samples from the original combined dataset We then used the standard R dprep package's entropy minimized partition 17 to discretize the selected training samples Finally the remaining dataset samples were used for testing the two classifiers after rule/BST generation on the randomly selected training data For each training set size we produced 25 independent tests In addition to these training sets we created an additional 25 1-x/0-y tests To create these tests we chose training data by randomly selecting x class 1 samples and y class 0 samples to be used as training data As before the remaining samples were then used to test both classifiers For each dataset the x and y values are chosen so that the resulting 25 classification tests have the exact same training/test data proportions as the single related dataset test reported in section V-A For each training set size we plot our results using a boxplot Boxplot Interpretation Each boxplot that we show in this section can be interpreted as follows The median of the measurements is shown as a diamond and a box with boundaries is drawn at the first and the third quartile The range between these two quartiles is called the inter-quartile range IQR Vertical lines a.k.a whiskers are drawn from the box to indicate the minimum and the maximum value unless outliers are present If outliers are presents the whiskers only extend to 1 5 x IRQ The outliers that are near i.e within 3 x IRQ are drawn as an empty circle and further outliers are drawn using an asterisk 1 ALIJAML ALL Experiment Figure 4 shows the classification accuracy for the ALL/AML dataset As can be seen in this figure BSTC and RCBT have similar accuracy across the ALL/AML tests as a whole BSTC outperforms RCBT in terms of median and mean accuracy on the 40 and 80 training set sizes while RCBT has better median/mean accuracy on the 1-27/0-11 training size tests And both classifiers have the same median on the 60 training set size Over the 100 ALL/AML tests we see that BSTC has a mean accuracy of 92.13 while RCBT has a mean accuracy of 91.39 they are very close It's noteworthy that BSTC is 100 accurate on the majority of 80 training size tests However BSTC appears to have slightly higher variance than RCBT on all but the 40 training tests Considering all the results together both BSTC and RCBT have essentially equivalent classification accuracies on the ALL/AML dataset 2 Lung Cancer LC Experiment The results for the Lung Cancer dataset are reported in Figure 5 Here again both BSTC and RCBT have similar classification behavior RCBT has higher mean and median accuracies on the 40 and 60 tests while BSTC outperforms RCBT on the 1-16/0-16 tests Meanwhile both classifier have the same median on the 80 training test Over all 100 LC tests we find that BSTC has a mean accuracy of 96 32 while RCBT has a mean accuracy of 97.08 again they are very close As before BSTC is alone in having 100 accuracy more 1068 TABLE III USINC GIVEN RES Ul TS TRAINING DATA  Class I  Class 0 Genes random Training Training After BSTC RCBT SVM Forest Dataset Samnples Samnples Discr Ac or Accr Ac o Accuracy ALL 27 11 866 82.35 91 18 91 18 85.29 LC 16 16 2173 100 97.99 93.29 99.33 PC 52 50 1554 100 97.06 73.53 73.53 OC 133 


TABLE IV AVERACGE RUN TIMLES FOR THE PC TESTS IN SEC-NI t INDICATES nl WAS LOWERED TO 2 Training Median  Mean 260 Near outliers  25/25t T O Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 80 Training 1 27/0-11 Training 1.0 1.0 1 0]0.95 0.9 0.9 0.7 0.7 0.850.8 0.80.80.80.750.7 0.70.70.7 650.6 0.60.60.6Holdout Validation Results 25/25t cJ CZ C O Far outliers 40 Training 60 Training 80 Training 1-16/0-16 Training h0 1.0 T 1.0 1.00.95 0.9 0.9 0 0.8 0.8 0.80.80.8LC Holdout Validation Results THE PC TESTS THAT RCBT FINISHED Training BSTC RCBT 40U 75.08 79.27 60 78.18 85.45 80 84.98 1-52/0 50 81.65%o 1069 cJ CZ C 5.06 120.63 21.32 RCBT  7110  7200  7200  RCBT DNF 0 P 00 24/25 t per training set test Finally the  RCBT DNF column gives the number of tests RCBT was unable to finish in  the cutoff time over the number of tests for which Top-K finished mining rule group upper bounds Explanation for varying nd values Run time cutoffs were necessary to mitigate excessive holdout validation CARmining times Even with a cutoff of 2 hours these 100 PC experiments required about 11 days of computation time with most experiments not finishing For the 80 and 1-52/0-50 training set sizes RCBT with nl  20 failed to finish lower bound rule mining for all 50 tests within 2 hours Thus RCBT's nl parameter was lowered from the default value of 20 to 2 in an attempt to improve its chances of completing tests Not surprisingly decreasing nl i.e mining fewer lower bound rules per Top-k rule group decreases RCBT s runtime However RCBT was still unable to finish lower bound rule mining for any tests Classification Accuracy Figure 6 contains accuracy results for BSTC on all four Prostate Cancer test sets Prostate Cancer boxplots for RCBT weren't constructed for training set sizes that RCBT was unable to complete all 25 tests within the time cutoffs In contrast BSTC was able to complete each of the 100 PC classification tests in less than 6 seconds Table V contains mean accuracies for the PC dataset with 40 60 80 and 1-52/0-50 training For each training set the average accuracies were taken over the tests RCBT was able to complete within the cutoff time Hence the 40 row means were taken over all 25 results Since RCBT was unable to complete any 80 or 1-52/0-50 training size tests we report these BSTC means over all 25 tests RCBT has slightly better accuracy then BSTC on 40 training For 60 training TABLE V MEAN AcCURACIES FOR 40 60 80 1.52/0.50 BSTC 3 4.93 5.78 5.57 Top0.09 BSTC F a RCBT BSTC RCBT BSTC RCBT b c Fig 5 BSTC RCBT BSTC RCBT b c ALL BSTC RCBT a Fig 4 BSTC RCBT d then half the time for any training set size see Figure 5 d However RCBT has smaller variance for 3 of the 4 training set sizes Therefore as for the ALL/AML data set both BSTC and RCBT have about the same classification accuracy on LC 3 Prostate Cancer PC Experiment RCBT begins to run ilnto a comiputational difficulties on PC's larger training set sizes This is because before using a Top-k rule group for classification RCBT must first mine nt lower bound rules for the rule group RCBT accomplishes rule group lower bound mining via a pruned breadth-first search on the subset space of the rule group's upper bound antecedent genes This breadthfirst search can be quite time consuming In the case of the Prostate Cancer PC dataset all 100 classification tests 25 tests for each of the 4 training set sizes generated at least one top10 rule group upper bound with more than 400 antecedent genes Due to the difficulties involved with a breadth-first search over the subset space of a several hundred element set RCBT began suffering from long run times on many PC classification tests Table IV contains four average classification test run times in seconds for each PC training size The BSTC column run times reflect the average time required to build both class 0 and class I BSTs and then use them to classify all the test samples Each Top-k column run time is the average time required for Top-k to mine the top 10 covering rule groups with minimum support 0.7 for each training set Table IV's RCBT column gives average run times for RCBT using a time cutoff value of 2 hours for all the training sets For each classification test if RCBT was unable to complete the test in less than the cutoff time it was terminated and it s run time was reported as the cutoff time Hence the BSTC RCBT d RCBT column gives lower bounds on RCBT s average run time 


TESTS IN SECOND t INDICATES nl WAS LOWERED TO 2 Training BSTC Top-k RCBT 7 OC Holdout Validation Results RCBT outperforms BSTC on the single test it could finish by more then 7 although it should be kept in mind that RCBT's results for the 24 unfinished tests could vary widely Note that BSTC's mean accuracy increases monotonically with training set size as expected At 60 training BSTC's accuracy behaves almost identically to RCBT's 40 training accuracy see Figure 6 4 Ovarian Cancer OC Experiment For the Ovarian Cancer dataset which is the largest dataset in this collection the Top-k mining method that is used by RCBT also runs into long computational times Although Top-k is an exceptiounally fast CAR group upper bound miner it still depends on performing a pruned exponential search over the training sample subset space Thus as the number of training samples increases Top-k quickly becomes computationally challenging to tune/use Table VI contains four average classification test run times in seconds for each Ovarian Cancer\(OC training size As before the second column run times each give the average time required to build both class 0/1 BSTs and then use them to classify all test's samples with BSTC Note that BSTC was able to complete each OC classification test in about 1 minute In contrast RCBT again failed to complete processing most classification tests within 2 hours Table VI's third column gives the average times required for Top-k to mine the top 10 covering rule groups upper bouhnds for each training set test with the same 2 hour cutoff procedure as used for PC testing The fourth column gives the average run times of RCBT on the tests for which Topk finished mining rules also with a 2 hour cutoff Finally the  RCBT DNF column gives the number of tests that RCBT was unable to finish classifying in  2 hours each THE OC TESTS THAT RCBT FINISHED Training BSTC RCBT 40 92.05 97.66 60 95.75 96.73 80 94 12 98.04 1-133/077 9380 96.12 1070 cJ CZ C 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 BSTC RCBT d Median Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 0.90.80.70.6BSTC RCBT a 80 Training 1-52/0-50 Training 0.9DNFI 0.80.70.6BSTC RCBT b 1 u0.9DNFI 0.80.70.6BSTC RCBT  RCBT DNF 40 30.89 0.6186 273.37 0/25 60 61.28 41.21  5554.37 19/25 80 71.84  1421.80  7205.43 t 21/22 TIMES FOR THE OC 9 Mean 0 Near outliers  Far outliers 1.01 11 01 1.0 d Fig 6 PC Holdout Validation Results BSTC RCBT a Fig 0.80.8 0.8BSTC RCBT BSTC RCBT b c c i DNF cJ CZ C 40 Training 60 Training 80 Training 1-133/0-77 Training 0.95 DNF DNF DNF 0.9 0.90.90.90.85 0.8 BSTC RCBT TABLE VI AVERAGE RUN 1 133/0-77 70.38  1045.65  6362.86 t 20/23 over the number of tests for which Top-k finished Because RCBT couldn't finish any 80 or 1-133/0-77 tests within 2 hours with nl  20 we lowered nl to 2 Classification Accuracy Figure 7 contains boxplots for BSTC on all four OC classification test sets Boxplots were not generated for RCBT with 60 80 or 1-133/0-77 training since it was unable to finish all 25 tests for all these training set sizes in  2 hours each Table VII lists the mean accuracies of BSTC and RCBT over the tests on which RCBT was able to produce results Hence Table VII's 40 row consists of averages over 25 results Meanwhile Table VII's 60 row results are from 6 tests 80 contains a single test's result and 1-133/0-77 results from 3 tests RCBT has better mean accuracy on the 40 training size but the results are closer on the remaining sizes   4 difference over RCBT's completed tests Again RCBT's accuracy could vary widely on its uncompleted tests CAR Mining Parameter Tuning and Scalability We attempted to run Top-k to completion on the 3 OC 80 training and 2 OC 1-133/0-77 training tests However it could not finish mining rules within the 2 hour cutoff Top-k finished two of the three 80 training tests in 775 min 43.6 sec and 185 min 3.3 sec However the third test ran for over 16,000 mnm  11 days without finishing Likewise Top-k finished one of the two 1-133/0-77 tests in 126 min 45.2 sec but couldn't finish the other in 16,000 min  11 days After increasing Top-k's support cutoff from 0.7 to 0.9 it was able to finish the two unfinished 80 and 1-133/0-77 training tests in 5 min 13.8 sec and 35 min 36.9 sec respectively However RCBT with nl 2 then wasn't able to finish lower bound rule mining for either of these two tests within 1,500 min Clearly CAR-mining and parameter tuning on large training sets is TABLE VII MEAN AcCU1ACIES FOR 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


