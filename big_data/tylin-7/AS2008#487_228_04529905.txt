A Chain of Text-mining to Extract Information in Archaeology Ahmed Amrani 1 Vicken Abajian 2 Yves Kodratoff 1  and Oriane Matte-Tailliez 3  1 LRI, CNRS UMR 8623, University Paris 11 Orsay, France 2 Informatics Laboratory, General Directorate of Antiquities and Museum in Syria Damascus, Syria 3 Kalamoon University, Faculty of Engineering Damascus, Syria ahmed.amrani@lri.fr, vickenabajian@yahoo.com oriane.matte@heartofdamascus.org, yves.kodratoff@lri.fr   Abstract In this paper, we describe a Text-Mining system used 
to extract archaeological knowledge from specialized texts. Our approach relies on two basic principals. First, we use userfriendly tools enabling experts to transfer easily their knowledge Second, our tools include inductive algorithms to reduce the experts’ workload Electronic Archaeology; Text-mining; Terminology Information Extraction I  I NTRODUCTION    In this paper, we present a project that aims to extract knowledge by information extraction process from the now huge amount of electronic literature: it is thus relative to textmining. Completion of archaeological knowledge would help 
archaeologists to elucidate some questions and infer archaeological assumptions to discover new features and relations in various domains of Archaeology/Cultural Heritage With this intention, we will use and develop an original approach for a complete software chain of semi-automatic information extraction from texts. This chain  includes the following steps: Gathering the corpus, text standardization Part-of-Speech \(PoS\ tagging, co-reference resolution terminology extraction, classification of concepts establishment of extraction patterns, application of association rules, and validation of extracted information 
The effective processing of technical texts requires the cooperation of field experts in order to improve the way to simulate understanding of the text content. The development of interactive visualization tools including inductive modules will make it possible to facilitate the work of the expert and limit the intervention of data processing specialists. For each step such software is currently developed and this software will be completed in the course in this project. The validation of the text processing system thus built can be done only by the effective use of extracted information. The developed methods 
of extraction are the extraction of patterns and of association rules This project can fit \(like a module\n our global project earchaeology It falls under a will of cooperation between various disciplines for better apprehending the complexity of the subject in its whole. With this intention, it will bring together experts of the field \(archaeologists/Museum specialists\, computer engineers and Data-Mining specialists The main objectives of the project are: extracting information on Archaeology from e-literature  and modelling knowledge from large data on archaeology \(e-archaeology 
II  S CIENTIFIC CONTENT AND METHOLOGY  This work concerns the theoretical and practical challenges carried by the automatic exploitation of an abundant scientific literature and in particular i\ the extraction of information from full-length texts and ii\ integration of this information in archaeological models The volume of published archaeological research, and therefore the underlying archaeological knowledge base, is expanding at an increasing rate. The abundance of literature motivates an intensive pursuit for effective text-mining tools Such tools are expected to help to uncover the information 
contained in large and unstructured bodies of text. Text mining is a process of data analysis that leads to the discovery of heretofore unknown information, or to answer questions the answer of which is not currently known [1   Significant progress has been made in applying text mining to named entity recognition, text classification, terminology extraction, relationship extraction, and hypothesis generation Several research groups are constructing integrated flexible text mining systems to find the nuggets of information most relevant and useful for specific analysis tasks 
In order to extract knowledge from archaeology texts, we use the following methodology [2,3,4  s e v e r a l  r e la te d s t e p s  a r e  used: text standardization, lexicon building, PoS tagging terminology extraction, co-reference resolution, detection of user-defined concepts, information extraction and knowledge discovery. Our text mining tools are adapted to technical texts The whole chain of treatment we deal with is shown in the Fig. 1       


Figure 1  Our global chain of text processing in view of text mining III  S TANDARDIZATION  Standardization is also often called text cleaning. This step is extremely tedious and requires a strong man-machine interaction. It consists of several types of treatments highly dependent upon the specialty and the type of information to extract. A detailed description of the task of standardization is given in [5  Th e  g o al o f th i s  ta s k  i s t o  lim it th e  e r r o r s in th e  following steps of the text processing and to prepare the ultimate step of knowledge extraction by reducing the complexity of the used vocabulary. The standardization includes the following operations   Recognizing a sentence as a textual entity. This task consists in identifying the sentence elements within a text. A key element for this task is determining the role of the dot. Indeed, in addition to ending a sentence, the dot can have several roles. It can be used in an abbreviation, for example Mr and Dr or in a decimal number   Suppression of not useful parts of a text   Correction of orthographic mistakes   Acronym acquisition and disambiguation   Reduction of synonymy. For example: The most frequent form is chosen between co-direction and codirection  Standardization is obviously specific to each domain and only the expert is able to decide what is to keep, transform or delete IV  P ART OF S PEECH TAGGING  The next step is relative to the PoS tagging \(grammatical tagging\. PoS tagging consists in associating each word with its grammatical tag, according to its morphology and context There are two main approaches for PoS tagging: the linguistic and the data-driven The linguistic approach consists of coding the necessary knowledge in a set of rules written by a linguist. Although the linguistic approach produces high quality taggers, it is a time consuming one In a data-driven approach, a machine learning method is applied to learn a tagging model from an annotated corpus Several data-driven approaches have been applied to PoS tagging. Among them, Inductive Logic  Progra mming [6  Transformation Based Learning [7   an d d eci s i o n tr ees  l ear n i n g  8  Wh a t e v er  t h e t e ch n o l o g y o n w h i c h t h e y a r e b a s e d   t h e data-driven taggers obtain very satisfactory results. However the construction of these taggers requires the availability of large annotated corpus. Acquiring such a corpus is expensive and time consuming and it is often the bottleneck to build a tagger for a new domain Most of the available taggers are learned from general corpora. Their good performance can be explained because test corpora are of similar type to training corpora. The problem is that the accuracy of general taggers drops down dramatically when applied to specialized corpora and high quality taggers are not available in specialized domains such as biology or archeology The solution we propose is to adapt a general tagger to a specialized corpus [9  We th u s u s e B r ill  s ta g g e r a s  th e s t ar t i n g  point of our system. We apply the ordered list of Brill’s rules to our corpus  Then our tagging software, ETIQ, enables the expert to display the results of Brill's tagging and allows him to add specialized rules in a simple and interactive way The strategy we propose is described as follows 


In a preliminary stage, we supplement Brill's lexicon by a specialized lexicon, i.e., a list of specialized words, where each one is followed by its possible tags “Tab. 1”. Then, we apply Brill's lexical rules. To help the expert to detect the tagging errors, the system ETIQ gives him the possibility of visualizing groups of words of similar morphological features, and their tags “Fig. 2”. Reacting to the detected errors, the expert inserts adequate lexical rules to correct these errors. We present below an example of lexical rule Assign the ‘adjective’ tag to the words having suffix ‘al  When the expert estimates that the tagging carried out during lexical stage brings no more improvement, contextual rules may be used. These rules change word tags according to the word and its context \(i.e., the word form, its tag, and neighboring words and their tags\. Similarly to the lexical module, the expert can, in an interactive way, look for words according to their forms, tags, and morphological criteria. This enables to visualize the contexts and to detect the errors. The expert can thus correct these errors by inserting specialized contextual rules We observe that it is sometimes very difficult to write down correction rules because the expert is unable to take into account all the possible exceptions. We thus propose the following strategy [10  U s i n g o u r f r ie n d ly u s e r i n te r f ac e   t h e  expert annotates examples containing the ambiguous words. He will assign to the target word of each example \(or group of examples\ the corresponding PoS tag. These annotated examples allow learning automatically correction rules by using the propositional rule induction algorithms PART [1  and RIPPER [12  T h e o b t a i n e d  r u l e s  a r e t h en c o n v er t e d  i n  t h e ETIQ format and are inserted at the end of the contextual rule list. We present below an example of an induced correction rule If the current word is a plural noun and the precedent word tag does not belong to any of the groups Noun and Verb, and the next word is a singular noun then the current word must be tagged VBZ \(Verb, 3rd person singular present  In order to reduce the number of examples to annotate, we can use an active learning strategy [13  I nde e d  b y us i n g a c t i v e  learning [14   t h e  l e a r ni ng s y s t e m s e l e c t s w h i c h e x a m pl e s  t o  annotate, instead of asking blindly the human to annotate the whole set of examples TABLE I  E XAMPLES OF WORDS EXTRACTED FROM THE SPECIALIZED LEXICON  Word Part-of-Speech tag ceramic NN \(Noun Islamic JJ \(Adjective Serail NNP \(Proper noun archaeological JJ Hellenistic JJ   Figure 2  ETIQ: Example of visualizing words and their tags V  T ERMINOLOGY EXTRACTION  Once a correct PoS tagging is obtained, we are able to extract doublets or triplets of successive words, called collocations. The PoS tagging is an important step, since we used grammatical patterns to extract the collocations. The collocations correspond to a specific PoS-tag series: NounPreposition-Noun, Noun-Noun, Adjective-Noun. The relevant collocations i.e collocations which are instances of a concept are called “terms We use the E XIT software [16  t h a t  ite r a t i v e ly e x tr ac ts  complex terms. The E XIT system uses an iterative approach to extract complex terms \(for example detailed studies of Islamic levels and study of Islamic period settlement with binary or ternary terms \(for example Islamic levels and Islamic period settlement The E XIT system is also based on statistical measures to order extracted collocations. Many statistical criteria exist to order collocations [17,18  The reduction of errors in PoS-tagging will have a direct effect on the quality of the terminology [10 F o r ex am p l e, i f a noun is taken for a verb, the term that contains this noun will be omitted, and conversely, if a verb is taken for a noun, a term containing this verb will be extracted in an automatic way though it is not relevant VI  C ONCEPT RECOGNITION  Once the terminology is completed, we use it to recognize the occurrences of concept in a text, by clustering the terms into classes, each term being a linguistic instance of a concept We are currently building a system \(ACT\ [3,19 o f co n cep t recognition in texts using the terms, the words, their tags and the context of each word or term. In our system, a syntactic relation among words may be also an instance of a concept The concept definition is entirely in the hands of the field expert. This process is domain specific. The expert defines 


what the interesting concepts are “Tab. 2”. Then, he associates the concept instances to their appropriate concept TABLE II  E XAMPLES OF FIELD CONCEPTS DEFINED BY THE EXPERT  Field concepts CONSTRUCTION PERIOD SITE METHOD SOIL  The next step in the process of concept categorization is the inductive phase. In this phase, the existing categorization of concepts is supplemented automatically [20 O n ce th e ex p e r t  has provided the set of interesting concepts, and as many as possible instances of these concepts, the induction algorithm will supplement automatically this categorization by adding new instances for each concept VII  I NFORMATION EXTRACTION  After concept categorization, we are able to extract information from the corpus. In addition to conceptual classification, the extraction module requires, as input, a PoS tagged corpus including the terminology and field entities \(for example, different categories of pottery Information Extraction methods are used to identify relations between types of objects. The goal of these methods is to transfer knowledge from unstructured data, the literature to a structured form that can be included in a database or knowledge base Our information extraction system, EXTRACT, helps experts to extract information in convivial and interactive way   T h e  e x pe r t  c a n m a ke r e que s t s us i n g  a  c o m b i n a t i o n o f  words, terms and/or concepts. The goal of the requests is to find relevant co-occurrences of concepts in sentences, and thus to discover relations between concepts. The requests are based on the potential instances of concepts and the entities connecting the concepts \(verbs, coordinating conjunction,…etc.\. To define requests the expert use a dedicated language, which facilitates the writing of expressive rules by combining conditions using the logical operators and  or and not To express conditions, we can use the following linguistic constraints   PoS tags \(for example plural noun  adjective etc   Groups of PoS tags. \(for example, the group of nouns contains the tags singular Nouns  plural nouns and singular Proper nouns    Morphological features based on regular expressions For example, concept instances ending with the suffix al The regular expressions are pre-coded, the expert can insert them easily via a dedicated interface   The membership of the concept instance to a particular concept. This constraint is established by using a conceptual classification “Tab. 3 By using our Information Extraction tool, the expert can, in an interactive way, apply a request to look for co-occurrences of concept instances in the sentences. This enables him to visualize the request results and he can thus improve it in a simple and interactive way TABLE III  E XTRACT OF CONCEPTUAL CLASSIFICATION   E ACH TERM IS ASSOCIATED TO A CONCEPT  Terms Concepts Grand_Serail CONSTRUCTION Mamluk_monument CONSTRUCTION Mamluk_ribat CONSTRUCTION Roman_buildings CONSTRUCTION acropolis-like_hill CONSTRUCTION main_streets CONSTRUCTION port CONSTRUCTION Hellinistic PERIOD Islamic_levels PERIOD Islamic_period_settlement  PERIOD Roman_origin PERIOD early_Abbasid_times PERIOD Rue_Allenby SITE Rue_Trablous SITE Rue_Weygand SITE Souk_Tawileh SITE Souk_al-Jamil SITE Souks_area SITE area in early_Abbasid_times  SITE archeological_investigations  WORK archeological_studies WORK studies_of_Islamic_levels  WORK excavation WORK study_of_Islamic_period_settlement WORK limestone_bedrock SOIL limestone_promontory  SITE line_of_the city_wall  SITE Pottery OBJECT Ceramic_ware_type_series  OBJECT urban_excavation METHOD sampling_methodologies METHOD  VIII  K NOWLEDGE DISCOVERY  We can also perform knowledge discovery. The definition of the concepts makes it possible to rewrite the corpus in a more compact way by replacing all different occurrences of a concept by the name of this concept [3   Th e n w e s ear ch  association rules [22 b e t w een co n cep t s  Th e  d i s c o v e r e d  knowledge will have the form concept A  concept B After that experts will validate the knowledge obtained Instead of seeking information with key words like it is usually done, we search for precise information \(for instance there is a weak interaction between object A and object B This information is collected by finite-state machines built according to the questions of the researchers. In fact considering the diversity of the language, to answer only one question, one needs thousands of finite-state machines, which is currently impossible to realize, whereas our tools summarize intelligently" these thousands of automats in a very small number. The validation will be done on two levels. The first level of validation will consist in a direct expertise on the quality of extracted information which will result in an 


effective use into phase of constitution of the archaeology knowledge. The second level of validation will consist in automatically integrating the information extracted by the software chain we developed, then to appraise the quality of the results so obtained CONCLUSION AND FUTURE WORK  Knowledge and information extraction from archaeological texts requires the application of a complete process of text mining. Our text mining process is composed of the following steps: Standardization, PoS tagging, terminology extraction concept recognition, information extraction and knowledge discovery. The quality of each step depends strongly on the preceding steps The processing of specialized texts requires the cooperation of field experts and thus the use of convivial software allowing an effective work. The use of inductive methods is an important aspect in our text mining process. Indeed, for each step of the process, a specific inductive method is applied Within the framework of the e-archaeology project, we will continue to adapt and to improve the treatments carried out in each step For the standardization, we will develop an interactive software which would make it possible to the expert to apply standardization algorithms, to visualize their effects, and to insert easily correction rules For PoS tagging, the first task will be the acquisition of a specialized lexicon starting from the corpus and external resources. Then, the specialized rules will be acquired by using our tagging software ETIQ In our system, the concept instances may be terms or syntactic relationships. The extraction of syntactic relationships will be improved by using our PoS tagger results as input of the syntactic parser The process of concept categorization is a important subtask in information extraction. We will continue to improve our inductive, convivial and cooperative method to effectively include the expert in this process The validation of the data processing system can be done by the effective use of the information extracted from texts R EFERENCES  1  M. A. Hearst, “Untangling Text Data Mining”,  37 th Annual Meeting of the Association for Computational Linguistics, University of Maryland June 20-26, 1999, pp. 3-10 2  A. Amrani, J. Azé, T. Heitz, Y. Kodratoff, and M. Roche, “From the texts to the concepts they contain: a chain of linguistic treatments”, Text REtrieval Conference \(TREC'04\, National Institute of Standards and Technology, Gaithersburg Maryland USA, 2004, pp. 712-722 3  M. Roche, J. Azé, O. Matte-Tailliez, and Y. Kodratoff, “Mining texts by association rules discovery in a technical corpus”, Proceeding of IIS IIPWM’04 \(Intelligent Information Processing and Web Mining Springer Verlag series “Advances in soft Computing”, Poland, 2004, pp 89-98 4  Y. Kodratoff, A. Dimulescu, and A. Amrani. “Man-Machine Cooperation in Retrieving Knowledge From Technical Texts”. AAAI 2005 Fall Symposium Series. Mixed-Initiative Problem Solving Assistants, 2005. Washington, USA, pp. 69-74 5  T. Heitz, “Modélisation du prétraitement des textes“, International Conference on Statistical Analysis of Textual Data, vol. 1, Besançon France,  pp. 499-506 6  J. Cussens, “Part-of-speech tagging using Progol”, S. Dzeroski et N Lavrac, Eds., Proceedings of the 7 th International Workshop on ILP Vol.1297,  1997, pp. 93–108 7  E. Brill, “Some Advances in Transformation-Based Part of Speech Tagging”, Proceedings of the twelfth national conference on Artificial intelligence, 1994, Washington, United States, pp. 722–727 8  L. Marquez and H. Rodriguez, “Part-of-Speech Tagging Using Decision Trees”, Proceedings of the 10 th European Conference on Machine Learning, 1998, 25–36 9  A. Amrani, Y. Kodratoff, and O. Matte-Tailliez, “A Semi-automatic System for Tagging Specialized Corpora”, 8 th Pacific-Asia Conference on Knowledge Discovery and Data Mining \(PAKDD 2004\, LNAI 3056 Springer-Verlag, may 2004, Sydney, Australia, pp. 670-681 10  A. Amrani, Y. Kodratoff, and O. Matte-Tailliez, “Inductive improvement of Part-of-Speech tagging and its effect on a terminology of molecular biology”, In the 18 th Canadian Conference on Artificial Intelligence \(AI’2005\, May 2005, Victoria, Canada. Lecture Notes in Artificial Intelligence, Springer-Verlag, pp. 366-376 11  E. Frank and I. H. Witten, “Generating Accurate Rule Sets Without Global Optimization”, Shavlik, J. Eds., Proceedings of the 15th International Conference on Machine Learning, Madison, Wisconsin 1998, pp. 144-151 12  W. Cohen, “Fast Effective Rule Induction”, Proceedings of the 12 th  International Conference on Machine Learning \(ICML\, 1995, pp. 155123 13  A. Amrani, Y. Kodratoff, and O. Matte-Tailliez,  “Induction de règles de correction pour l’étiquetage morphosyntaxique de la littérature de biologie en utilisant l’apprentissage actif“. TALN 2005 \(Traitement Automatique des Langues Naturelles\, Dourdan, France, 2005, pp. 385390 14  D. Lewis and J. Catlett, “Heterogeneous Uncertainty Sampling for Supervised Learning”, 11 th International Conference on Machine Learning \(ICML\., pp. 148-156 15  A. McCallum and K. Nigam, “Employing EM and Pool-Based Active Learning for Text Classification”. International Conference on Machine Learning, 1998, pp. 350-358 16  M. Roche, T. Heitz, O. Matte-tailliez, and Y. Kodratoff,  “EXIT: un système itératif pour l’extraction de la terminologie du domaine à partir de corpus spécialisés”,  Journée Internationales d’Analyse Statistique des Données Textuelles \(JADT’04\, Volume 2, louvain-la-Neuve Belgique, 2004, pp. 946-956 17  L. Nerima, V. Seretan, E. Wehrli, “Creating a multilingual collocations dictionary from large text corpora”, Proceedings of Conference of the European Chapter of the Association for Computational Linguistics EACL\,  2003, 131-134 18  T. E. Dunning, “Accurate Methods for the Statistics of Surprise and Coincidence”, Computational Linguistics, Volume 19 \(1\ , 1993, pp. 61 74 19  Y. Kodratoff, “Comparing Machine Learning and Knowledge Discovery in DataBases: An Application to Knowledge Discovery in Texts Machine Learning and Its Applications, 2001, pp. 1-21 20  Y. kodratoff, “Induction Extentionnelle: définition et application l’acquisition de concetps à partir de textes", Extraction et gestion des connaissances \(EGC'04\, 2004, pp. 247-252 21  A. Amrani and O. Matte-Tailliez, “Information extraction on Crohn’s disease from texts by interactive visualization”, Journées Ouvertes de Biologie, Informatique et Mathématiques \(JOBIM’05\, Lyon, France, 68 July 2005 22  R. Agrawal, T. Imielinski, A. Swami, “Mining Association Rules between Sets of Items in Large Databases”, International Conference on Management of Data  archive ACM SIGMOD, Washington, D.C United States, pp. 207-216  


In other words, an item would only appear at most one time in the i th row during a single round. In this way, we have few linked lists including n items if the hash function H is sufficiently uniform. The total memory required is again 000\013\000\014 On  In summary, the time an d space complexity of the candidate-generation process in our SERIT are asymptotically the same as BTH’s 4. experiment results In this section, we report the results of testing our SERIT on several data sets. Compar ing with algorithm TAPER and BTH, SERIT is proved to be effective. Bounded with a small false-negative tolerance 000W as 0.005, SERIT can not only prune the uncorrelated pairs efficiently to generate a small candidate set and save computi ng resources; but also cover the shortcoming of BTH, still achieving small running time when 000T is relatively small The experiment bases on both synthetic and real data sets We choose the famous “retail” data set, which contains data from a retail store and can be downloaded from FIMI website. The tool mentioned in [2 is also use d h e re  to generate three synthetic data sets S 1 S 2 and S 3 Table 3 lists the characteristics of these data sets. And table 4 shows the values of parameter k and t in algorithm SERIT and BTH for different 000T   T ABLE 3 THE CHARACTERISTICS OF DATA SETS  Data Set Num of Items Num of Records retail 16470 88163 S 1 20589 51316 S 2 33052 51292 S 3 42522 51337 All the experiments were performed on a personal computer, with a 2.0 GHz CPU and 512 Mbytes of memory running the windows XP operating system In section 5.1, we give th e executing time of the three algorithms, including candidate-generation running time and overall running time. We then show in section 5.2 the scalability of SERIT T ABLE 4   THE VALUES OF PARAMETER K AND T   Algorithm SERIT Algorithm BTH 000T  k t k t 0.8 7 8 2 10 0.7 11 6 2 18 0.6 14 6 2 38 0.5 17 6 2 47 0.4 28 4 2 204 0.3 30 4 2 370 0.2 110 4 2 2940 0.1 410 2 2 50000 4.1. The executing time  All the three algorith ms need two steps: 1\ generation of the candidate set 2\refinement. As a result, there are two types of executing time, candidate-generation running time and overall running time, which may present different properties. Although an algorithm executes efficiently during candidate-generation stage, it may produce a large candidate set inducing the incr ease of overall executing time The coefficient thresholds are sp lit to two parts with 0.4 as a dividing point. Figure 5 illust rates the candid ate-generation time, while Figure 6 shows the overall executin g time, both based on “retail” data set The corresponding executing time on S 1 is presented in Figure 7 and 8. Note that when 0.1 000T 000  we find that the executing time including the two types\ of BTH is too long, much longer than that of other algorithms Moreover, it is also much longer than the time when 0.2 000T 000  For the rationality of experiment and brevity to present, we don’t illustrate the execu ting time of BTH with 0.1 000T 000  From Figure 5~8\(a\s small executing time when 0.4 000T 000d And the advantage becomes more obvious when 000T  is smaller. Oppositely, the running time of BTH increases sharply with the decrease of 000T This is mainly due to the very large numbers of min-hash values caused by large t. Although BTH merely chooses two items in the same equivalence class to generate a candidate pair and the method in SERIT seem s a little complex, SERIT is still faster due to the r eason mentioned above When 0.5 000T 000t BTH computes less min-hash values, and consequentially it has smaller running time. SERIT is slightly slower than BTH in most cases, but always faster than TAPER. This is determin ed by the property of the probability function used in SERIT: With a big 000T the item pair whose correlation coefficient is smaller than 000T would be chosen into the candidate set in a relatively high probability \(compared with BTH\In this way, SERIT may have a slightly bigger candidate set than BTH From further analysis, the other two is much powerful than TAPER to remove pairs unwanted. As a result of the large candidate set generated in TAPER, in the refinement step more time is required. As shown in Figure 7 \(b\d 8 b\ the execution of TAPER can be one order of magnitude slower Moreover, comparing the experimental results on “retail with S 1 we can conclude that the size of item set has greater influence on TAPER than on SERIT and BTH. The running time increases obviously with the growth of item set 
424 
418 


0.10 0.15 0.20 0.25 0.30 0.35 0.40 0 100 200 300 400 500 600 BTH TAPER SERIT Execution Time \(sec Threshold  Fig 5\(a\ candidate-generati on time on “retail” with 0.4 000T 000d  0.50 0.55 0.60 0.65 0.70 0.75 0.80 5 10 15 20 25 30 Execution Time \(sec Threshold BTH TAPER SERIT  Fig 5\(b\ candidate-generati on time on “retail” with 0.5 000T 000t  0.10 0.15 0.20 0.25 0.30 0.35 0.40 0 100 200 300 400 500 600 BTH TAPER SERIT Execution Time \(sec Threshold  Fig 6\(a\ overall executing time on “retail” with 0.4 000T 000d  0.50 0.55 0.60 0.65 0.70 0.75 0.80 20 40 60 80 100 120 140 160 180 200 Execution Time \(sec Threshold BTH TAPER SERIT  Fig 6\(b\ overall executing time on “retail” with 0.5 000T 000t  0.10 0.15 0.20 0.25 0.30 0.35 0.40 0 100 200 300 400 500 600 BTH TAPER SERIT Execution Time \(sec Threshold  Fig 7\(a\ candidate-generation time on S 1 with 0.4 000T 000d  0.50 0.55 0.60 0.65 0.70 0.75 0.80 5 10 15 20 25 30 35 40 45 Execution Time \(sec Threshold BTH TAPER SERIT  Fig 7\(b\ candidate-generation time on S 1 with 0.5 000T 000t  0.10 0.15 0.20 0.25 0.30 0.35 0.40 0 100 200 300 400 500 600 700 800 900 BTH TAPER SERIT Execution Time \(sec Threshold  Fig 8\(a\ overall executing time on S 1 with 0.4 000T 000d  0.50 0.55 0.60 0.65 0.70 0.75 0.80 50 100 150 200 250 300 350 400 450 Execution Time \(sec Threshold BTH TAPER SERIT  Fig 8\(b\ overall executing time on S 1 with 0.5 000T 000t  4.2. The scalability of algorithm SERIT We use synthesized data sets list in Table 3 to examine the scalability of SERIT from the vi ew point of ex ecution time They have different sizes of item set. Figure 9 shows that the execution time increases with the number of items. This conclusion is consistent with th e complexity analysis shown in section 3 
425 
419 


2.0 2.5 3.0 3.5 4.0 4.5 40 60 80 100 120 140 160 Execution Time \(sec Num of items              X 10 4 Thre0.1 Thre0.3 Thre0.5 Thre0.7  Fig 9. scalability of algorithm SERIT 5. Conclusion and future work In this paper, given a coefficient threshold, we design and implement the algorithm SERIT which can find the correlated item pairs through efficient pruning in a massive market-basket data set. The method proposed by Zhang et al   o re suitable when 000T is relatively big. Hence, a new problem is generated when 000T is small. Our SERIT introduces a new probability functio n to select candidate pairs Experimental results show that the space and time complexity are asymptotically the same as BTH’s. In ad dition, with a relatively small 000T  0.4 000d eds a much smaller and more reasonable kt 000  than BTH. Theref ore, SERIT achieves much larger saving of computational resources  There are several potential dir ections in future research First, we will examine whether it is more efficient to combine SERIT and TAPER. However, the two different pruning methods need different pre-proces sings. In particular, we have to consider the trade-off between a more complex pre-processing and efficiency Second, we only considered pairs of items in this paper How about a correlated item set Also, there exist several other cr iterions to measure correlation association\, which may have peculiar properties  A CKNOWLEDGMENT  This work is supported by the National High-Tech Research and Development Plan of China \("863" plan\nder Grant No  2006AA01Z451 and No. 2007AA01Z474 6. References   Hui Xiong, Shashi Shekhar, P. N. Tan, and Vipin Kumar Exploiting a support-based upper bound of Pearson’s correlation coefficient for efficiently identi fying strongly correlated pairs KDD’04, pp. 334–343, August 22-25, 2004, USA   J i an Zh ang and J o an F e ig e nbaum, “Findind highly correlated pairs efficiently with powerfu l pruning”, CIKM’06, pp. 152–161 November 5-11,2006, USA  Hui Xiong, Mark Brodie a nd Sheng Ma, “TOP-COP: mining TOP-K strongly correlated pairs in large database”, ICDM’06, pp 1162–1166  R.Agrawal  T. Im i e linski   a nd A. S w am i   M ining associa tion rules between sets of items in large databases SIGMOD’93, pp 207-216  C. Jerm aine, “ Th e com putational complexity of high-dimensional correlation s earch”, ICDM’01, pp. 249-256  C.Bucila, J Gehrke, D  Kif e r and W. M. White, “ Dualminer: a dual-pruning algorithm for itemsets with constrains”, SIGKDD’02 pp. 241-272  E. Cohen   S ize-estim at i on fram e work with appl ic atio ns to transitive closure and reachability Journal of Computer and System Sciences, 1997, pp. 441-453  C. Jermaine, “Play i ng hide-a nd-seek with correlations SIGKDD’03, pp. 559-564  S  Brin R  M o tw ani and C S ilvers t ein   B e y ond m a rket bas k ets   Generalizing association rules to correlations”, SIGMOD’97, pp 265-276  Yu Li Miroslav, and  Kubat, “Searching for high-su pport itemsets in itemset trees”, Intellig ent Data Analysis, , March, 2006 Volume 10, Issue 2, pp. 105-120  H. T. R e ynolds The an aly s is of cross-classifications”, The Free Press, New York, 1977   Toon Cad e rs, B a rt Goeth a ls and S. Jaroszewicz, “Mining rank-correlated sets of numerical attributes KDD’06, August 20-23 2006, USA, pp. 96-105  C. K. S Leung, and Qu amru l l. Khan, “Efficient mining of constrained frequent patterns fro m Streams”, IDEAS’06, pp. 61-68  Motwani  E Cohe n, M Da ta r, S. Fujiwa re A Gionis P Indyk, J. Ullman, and C.Yang, “Finding interesting associations without support pruning”, IEEE Transactions on Knowledge and Data Engineering \(special issue  W. DuMouchel and D. Preg ibon Empirical bayes screening for multi-item associations”, KDD’01, pp. 67-76  S  J a ros zew icz and D a n A  Simovici, “ Inte restingness of frequent itemsets using Bayesi an networks as background knowledge”, SIGKDD’04, USA, pp. 178-186  
426 
420 


TESTS IN SECOND t INDICATES nl WAS LOWERED TO 2 Training BSTC Top-k RCBT 7 OC Holdout Validation Results RCBT outperforms BSTC on the single test it could finish by more then 7 although it should be kept in mind that RCBT's results for the 24 unfinished tests could vary widely Note that BSTC's mean accuracy increases monotonically with training set size as expected At 60 training BSTC's accuracy behaves almost identically to RCBT's 40 training accuracy see Figure 6 4 Ovarian Cancer OC Experiment For the Ovarian Cancer dataset which is the largest dataset in this collection the Top-k mining method that is used by RCBT also runs into long computational times Although Top-k is an exceptiounally fast CAR group upper bound miner it still depends on performing a pruned exponential search over the training sample subset space Thus as the number of training samples increases Top-k quickly becomes computationally challenging to tune/use Table VI contains four average classification test run times in seconds for each Ovarian Cancer\(OC training size As before the second column run times each give the average time required to build both class 0/1 BSTs and then use them to classify all test's samples with BSTC Note that BSTC was able to complete each OC classification test in about 1 minute In contrast RCBT again failed to complete processing most classification tests within 2 hours Table VI's third column gives the average times required for Top-k to mine the top 10 covering rule groups upper bouhnds for each training set test with the same 2 hour cutoff procedure as used for PC testing The fourth column gives the average run times of RCBT on the tests for which Topk finished mining rules also with a 2 hour cutoff Finally the  RCBT DNF column gives the number of tests that RCBT was unable to finish classifying in  2 hours each THE OC TESTS THAT RCBT FINISHED Training BSTC RCBT 40 92.05 97.66 60 95.75 96.73 80 94 12 98.04 1-133/077 9380 96.12 1070 cJ CZ C 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 BSTC RCBT d Median Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 0.90.80.70.6BSTC RCBT a 80 Training 1-52/0-50 Training 0.9DNFI 0.80.70.6BSTC RCBT b 1 u0.9DNFI 0.80.70.6BSTC RCBT  RCBT DNF 40 30.89 0.6186 273.37 0/25 60 61.28 41.21  5554.37 19/25 80 71.84  1421.80  7205.43 t 21/22 TIMES FOR THE OC 9 Mean 0 Near outliers  Far outliers 1.01 11 01 1.0 d Fig 6 PC Holdout Validation Results BSTC RCBT a Fig 0.80.8 0.8BSTC RCBT BSTC RCBT b c c i DNF cJ CZ C 40 Training 60 Training 80 Training 1-133/0-77 Training 0.95 DNF DNF DNF 0.9 0.90.90.90.85 0.8 BSTC RCBT TABLE VI AVERAGE RUN 1 133/0-77 70.38  1045.65  6362.86 t 20/23 over the number of tests for which Top-k finished Because RCBT couldn't finish any 80 or 1-133/0-77 tests within 2 hours with nl  20 we lowered nl to 2 Classification Accuracy Figure 7 contains boxplots for BSTC on all four OC classification test sets Boxplots were not generated for RCBT with 60 80 or 1-133/0-77 training since it was unable to finish all 25 tests for all these training set sizes in  2 hours each Table VII lists the mean accuracies of BSTC and RCBT over the tests on which RCBT was able to produce results Hence Table VII's 40 row consists of averages over 25 results Meanwhile Table VII's 60 row results are from 6 tests 80 contains a single test's result and 1-133/0-77 results from 3 tests RCBT has better mean accuracy on the 40 training size but the results are closer on the remaining sizes   4 difference over RCBT's completed tests Again RCBT's accuracy could vary widely on its uncompleted tests CAR Mining Parameter Tuning and Scalability We attempted to run Top-k to completion on the 3 OC 80 training and 2 OC 1-133/0-77 training tests However it could not finish mining rules within the 2 hour cutoff Top-k finished two of the three 80 training tests in 775 min 43.6 sec and 185 min 3.3 sec However the third test ran for over 16,000 mnm  11 days without finishing Likewise Top-k finished one of the two 1-133/0-77 tests in 126 min 45.2 sec but couldn't finish the other in 16,000 min  11 days After increasing Top-k's support cutoff from 0.7 to 0.9 it was able to finish the two unfinished 80 and 1-133/0-77 training tests in 5 min 13.8 sec and 35 min 36.9 sec respectively However RCBT with nl 2 then wasn't able to finish lower bound rule mining for either of these two tests within 1,500 min Clearly CAR-mining and parameter tuning on large training sets is TABLE VII MEAN AcCU1ACIES FOR 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


