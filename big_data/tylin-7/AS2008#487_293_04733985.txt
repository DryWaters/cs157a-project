Bounding and Estimating Association Rule Support from Clusters on Binary Data Carlos Ordonez Kai Zhao Zhibo Chen University of Houston Department of Computer Science Houston TX 77204 USA Abstract The theoretical relationship between association rules and machine learning techniques needs to be studied in more depth This article studies the use of clustering as a model for association rule mining The clustering model is exploited to bound and estimate association rule support and conﬁdence We rst study the efﬁcient computation of the clustering model with K-means we show the sufﬁcient statistics for clustering on binary data sets is the linear sum of points We then prove itemset support can be bounded and estimated from the model Finally we show support bounds fulﬁll the set downward closure property Experiments study model accuracy and algorithm speed paying particular attention to error behavior in support estimation Given a sufﬁciently large number of clusters the model becomes fairly accurate to approximate support However as the minimum support threshold decreases accuracy also decreases The model is fairly accurate to discover a large fraction of frequent itemsets at different support levels The model is compared against a traditional association rule algorithm to mine frequent itemsets exhibiting better performance at low support levels Time complexity to compute the binary cluster model is linear on data set size whereas the dimensionality of transaction data sets has marginal impact on time 1 Introduction Association rules a re a v ery popul ar dat a mi ni ng technique 15 17 w h i c h h a v e p roduced a s i gni  cant body of research There is research on efﬁcient algorithms to discover association rules 3 32 16 38  u s i ng i t e m t axonomies 35 creat i n g i t e ms et s u mmari es 8 i n corporat ing constraints 40 creat i n g s ummari es 10  20  a nd applying them in different domains 9 11 29 among ot her topics Unfortunately association rules lack a basic model to understand their relationship to properties from the data set In this work we defend the idea of using clustering 7 26 12 a s a model t o d i s co v e r a s s o ci at i o n r ul es  Association rules is a well s tudied topic but most research has proposed efﬁcient algorithms 24 16 32 or v a ri ants like quantitative 36  c ons trained  25  4 0   t axonomybased 35 among ot hers  S t udyi ng t h ei r r el at i ons hi p t o classical machine learning techniques has received less attention Using a model to discover them nding ways to summarize them proposing metrics to approximate them or explaining their validity using alternative techniques are aspects that have not received enough attention Our point of view is that clusters represent a more fundamental pattern than association rules from which we can generate them The mathematical rela tionship between clustering and association rules is impor tant and interesting because clustering on one hand looks for global patterns producing disjoint subsets of points whereas association rules uncover many local patterns referring to overlapping subsets of points That is both techniques have different goals This is a summary of our contributions We rst introduce a one-pass K-means clustering algorithm based on simpliﬁed sufﬁcient statistics for binary data to build a model that summarizes a set of transactions as clusters We prove clusters can be used to bound and approximate association rule metrics Based on the clustering model three association metrics are introduced lower upper and average support These new metrics provide bounds and an estimation for association support Also our metrics exhibit the subset downward closure property which allows efﬁcient bottom-up search algorithm that can work in main memory We derive three metrics for association rule conﬁdence reusing support bounds lower upper and average conﬁdence We perform an extensive experimental with real and synthetic data sets We study error behavior varying the number of clusters and accuracy to discover a large set of frequent itemsets at different support thresholds This is an outline of the article Basic deﬁnitions on clustering and association rules are presented in Section 2 Sec 
2008 IEEE International Conference on Data Mining Workshops 978-0-7695-3503-6/08 $25.00 © 2008 IEEE DOI 10.1109/ICDM.Workshops.2008.49 609 
2008 IEEE International Conference on Data Mining Workshops 978-0-7695-3503-6/08 $25.00 © 2008 IEEE DOI 10.1109/ICDMW.2008.47 609 


tion 3 explains how to exploit a clustering model to bound an estimate itemset support Section 4 discusses experiments studying error on support estimation speed and scalability Closely related work is presented in Section 5 Section 6 concludes the article 2 Deﬁnitions We give a uniﬁed set of deﬁnitions for association rules 2 3 32 and c lus t ering  7  28  Items act as dimens ion subscripts for matrix manipulation 2.1 Association Rules Let D   T 1 T n  beasetof n transactions on d items where  T i   d crucial to develop an efﬁcient distance computation 26 and I   1  2 d  each integer identiﬁes one item Each subset X of items s.t X 002I is called an itemset  An itemset X containing p items is called a p itemset X   i 1  i 2  i p  wherei l 003I The probability of appearance for one itemset X 002I  is called support and it is deﬁned as s  X   T i  X 002 T i  n That is the fraction of transactions in D that contain X Inan association rule X 004 Y  X Y 005I and X 006 Y  007  X is called the antecedent and Y is called the consequent X 004 Y has a measure of reliability called conﬁdence given by c  X 004 Y  s  X 010 Y  s  X   The standard association problem is to nd all itemsets that have support above a minimum support threshold 002  The standard problem of mining association rules is to nd all rules X 004 Y s.t s  X 004 Y  011 002 and c  X 004 Y  011 003  2.2 Clustering In the context of clustering D is analyzed as a binary data set with d dimensions K-means partitions the data set into k disjoint groups based on a Euclidean distance similarity metric The clustering model consists of matrices W C R  which contain the weights the centroids means and the radiuses variances respectively for each cluster and a partition of D into k subsets where k is user-speciﬁed Matrices C and R are d  k and W is a k  1 matrix To refer to to the j th cluster we use the j subscript i.e C j R j  Therefore C j is the j th cluster centroid R j is the j th diagonal variance matrix and W j is the weight of cluster j  Subscripts are used as follows Subscript i is used to scan transactions i 003 1  2 n   Notice i alone is a subscript to index transactions but i j is item j  Subscript j is used to refer to one cluster j 003 1  2 k   Finally subscript h is used for dimension h 003 1  2 d  Let D 1 D 2 D k be the k disjoint subsets from D induced by clusters s.t D j 006 D g  007 for j 012  g The diag  is a T i Items 1 45 2 34 3 12 4 2 5 12 6 345 7 1 8 34 T i 12345 1 00011 2 00110 3 11000 4 01000 5 11000 6 00111 7 10000 8 00110 Figure 1 Input transactions d 5 n 8  W  002 0  5 0  5 003 C  004 005 005 005 005 006 0  00 0  75 0  00 0  75 0  75 0  00 1  00 0  00 0  50 0  00 007 010 010 010 010 011 Figure 2 Clustering model d 5 k 2  transformation operator to transform a vector into a diagonal matrix or vice versa The T superscript indicates vector and matrix transposition Example 2.1 In Figure 1 we present a small input database D where d 5 n 8  Figure 2 presents a clustering model where k 2  Finally Figure 3 presents an intuitive model summary Items are grouped according to their cluster centroid value The last column shows a basic itemset represented by each cluster which can be used to derive itemset subsets Thus the clustering summary can derive all frequent itemsets that can be obtained from D  Parameter 004 is used to lter out long itemsets obtained from each cluster it will be explained in more detail in Section 3 3 Bounds on Support Based on Clusters Clustering is used as a statistical model for association rules In this section we introduce an efﬁcient algorithm to cluster binary data sets and a family of association rules metrics based on a clustering model C j range for items Itemsets jW j 0.75-1 0.5-0.75 0.25-0.5 at 004 0  5 10.5 34 5 345 20.5 12 12 Figure 3 Clusters summary and itemsets 
610 
610 


3.1 Clustering Binary Data Sets Since we are trying to use clusters as a model to discover associations the rst task is clustering transactions K-means 7 31 23 17 i s u s e d t o c lus t er trans actions  F rom a scalability point of view It is feasible to derive incremental K-means clustering versions 26 t h at can get a good s o lution in one pass or a few passes over a large data set The K-means algorithm used in this work is an improvement of the incremental K-means algorithm for binary streams 26 In this s ection w e p res e nt the m ain f eatures that make it suitable to cluster binary data sets and introduce useful theoretical results K-means is signiﬁcantly accelerated with sparse distance computation sparse matrix addition and simpliﬁed sufﬁcient statistics The K-means algorithm 26  lik e o th er scalab le clu s ter ing algorithms 7 44  e x p l o its su f  cien t s tatistics wh ich are summaries of D 1 D 2 D k represented by matrices L Q N that contain k sums of points k sums of squared points and number of points per cluster respectively L is a d  k matrix such that each column L j is computed as L j  012 x i 002 D j x i  Each column of Q is a diagonal matrix Q j  Q j  012 x i 002 D j x i x T i  Finally matrix N is k  1 which counts the number of points per cluster i.e N j   D j   The update formulas for W C R based on sufﬁcient statistics are W j  N j n 1 C j  1 N j L j 2 R j  1 N j Q j  1 N 2 j L j L T j 3 The sufﬁcient statistics can be simpliﬁed for clustering binary data In fact they are simpler than the sufﬁcient statistics required for clustering numeric data Lemma 3.1 Let D 1 D 2 D k be k subsets representing a partition of D  Then the sufﬁcient statistics required for computing C R W are only N and L because L  Q  Proof Based on the fact that x i is a binary vector 012 x i  012 x i x T i  Therefore L  Q  002 This result states that we only need to update the model based on Equation 1 and Equation 2 That is matrix R j can be obtained from C j  R j  diag  C j   C j C T j 4 More importantly storage space for clustering results is reduced to one half Therefore matrix R is not needed to estimate bounds for support and conﬁdence We can now derive a simple criterion to evaluate cluster quality Theorem 3.1 R lj has a minimum if and only if C lj 0 or C lj 1 Also R lj has a maximum if and only if C lj 1  2  Proof The result follows from R j  diag  C j   C j C T j and the rst derivative conditions for entry R lj to have a maximum 002 Theorem 3.1 states that it is best C j entries are close to either 0 or 1 with the ideal case when C lj 0 or C lj 1  3.2 One Pass K-means for Binary Data When D isasparsematrixand d is high Euclidean distance is expensive to compute In a typical binary transaction data set only a few dimensions have non-zero values Therefore we precompute a distance from every C j to the null vector  0  To that purpose we deﬁne the following k dimensional vector 002 j  005   0 C j  Let 005 ij  005  x i C j   Then 005 ij 002 j  d 013 l 1   x i  l 003 0  x i  l  C lj  2  C 2 lj  5 This optimized distance co mputation decreases time complexity but it does not affect quality of results A similar idea is applied to update L Q matrices in a sparse fashion after a transaction cluster membership is determined Sparse matrix addition updates only the closest cluster dimensions where the point dimension value is 1 Based on the improvements described in the previous section we present a one pass K-means algorithm This is a high-level description The input is the set of transactions D   T 1 T 2 T n  and k  the desired number of clusters as deﬁned in Section 2 The output is 003  W C   describing the model and a partitioning of D into k subsets A constant 006 is used to seed C based on d and k The global statistics  and 004 can be quickly computed from a sample Standard deviations are computed as 007 ll  013 004 ll  The cluster membership step is executed for every transaction  n times Since this is the most frequently executed step the optimization for sparse matrix operations is essential to achieve good performance These sparse operations include sparse distance computation for 005 and sparse matrix addition to update L  005 ij is efﬁciently computed using 002 j  The M step is periodically executed every n/\010 transactions  010 times W C are updated using Eq 1 and Eq 2 Dimensions items are ranked within each cluster by their value in C j to make output easier to understand 3.3 Bounds on Support The following theoretical results are important to use the model to obtain bounds on association support The support 
611 
611 


Input D   T 1 T 2 T n  and k  Output 002  W C  002 002 1   dk  003 002 log  n  FOR j 1 TO k DO  Initialize  C j 002   002r diag  004  W j 002 1 k 003 j  005   0 C j  C T j C j L j 002 C j N j 002 1 ENDFOR FOR i 1 TO n DO FOR j 1 TO k DO  distances  005 j 002 005  T i C j   sparse operation  ENDFOR Let J be s.t 005 J 003 005 j j 1 k L J 002 M J  x i  sparse operation  N J 002 N J 1 IF i mod  n/\003 0  THEN  update model  FOR j 1 TO k DO C j 002 M j N j W j 002 N j  012 k J 1 N J 003 j 002 C T j C j ENDFOR ENDIF ENDFOR Figure 4 One pass K-means for binary data of an itemset in one cluster  s  X D j   must be less or equal than the minimum support of any item as shown in 32 Lemma 3.2 Let D j be one cluster from D Let X   i 1  i 2  i p  be some p itemset Let C j be the mean of transactions that belong to cluster j Then s  X D j  014 min  C j  i 1   C j  i 2   C j  i p   Proof sketch Induction on p  002 The following theorem links clustering results with associations Let the upper bound of s  X  be deﬁned as follows u  X  k 013 j 1 W j min  C j  i 1   C j  i 2   C j  i p   6 Theorem 3.2 Let D be a set of n transactions Let C W be the matrices containing the means and weights respectively on some clustering of D into k subsets of transactions D 1 D 2 D k Let X be any p itemset given by X   i 1  i 2  i p  Then s  X  014 u  X   Proof sketch By Lemma 3.2 we can sum k inequalities corresponding to each subset D j  s  X D j  014 u  X D j  to get s  X  014 u  X  on D  002 Theorem 3.2 asserts C and W can be used to mine all potential frequent itemsets thereby eliminating the need to scan D  Nevertheless the theorem does not guarantee that only true frequent itemsets are above the minimum support threshold We may nd some false-positive itemsets whose exact support is actually lower than that indicated by the model The following result based on Theorem 3.2 is applied to prune the lattice search space if associations are being mined from the clustering model using u  Nowwe introduce a lower bound for the support of an association l  X  k 013 j 1 W j max 0  1 p 013 l 1  C j  i l  1  7 where max  is used to get a non-negative lower bound per cluster Theorem 3.3 Let X C W  D 1 D k  be as stated in Theorem 3.2 Let X be a p itemset X   i 1  i 2  i p   Then l  X  014 s  X   Proof sketch The sum in Equation 7 computes a lower bound on the fraction of transactions that do not contain X for cluster j  002 Since u  and l  are guaranteed bounds on the exact support of any association their average is a good estimator a  X  l  X  u  X  2  8 3.4 Downward Closure Property The following result proves our metrics preserve the downward closure property e.g a subset of a frequent itemset is frequent Theorem 3.4 Let X be a p itemset and let Y 005 X  Y   h 1  h q   q<p  Then 1 l  X  014 l  Y  2 u  X  014 u  Y  3 a  X  014 a  Y   Proof The proof follows by considering the extra items in X  which may decrease the bounds on Y  002 Theorem 3.4 guarantees a complete set of associations with respect to any of the metrics using the standard bottomup search algorithm This result generalizes bottom-up search algorithms to work with a clustering model instead ofthedataset As explained above upper support  u  ssafeinthe sense that it produces a superset of all frequent itemsets but it may generate infrequent false positive itemsets In general a subset of C j entries close to zero indicate itemsets that are unlikely to be frequent To solve such limitation we propose to use a 004 parameter which is used to lter out itemsets unlikely to be above 002 i.e not frequent clusters whose upper support for one speciﬁc itemset fall below 002 
612 
612 


do not contribute to the support estimation used for pruning Therefore we get an adjusted upper support for pruning below 002  The spectrum for 004 is At one e x t r eme 004 0 does not eliminate any itemset all frequent itemsets are discovered but there are additional itemsets whose support is actually below the 002 threshold At the other extreme 004 1 allows only dimensions with zero variance in each cluster to participate in itemset generation in this case all itemsets that are generated are frequent there are zero false positives but there may be many itemsets whose support was actually above the 002 threshold there are false negatives 4 Experimental Evaluation We rst discuss our experimental setup and data sets Experiments study two major aspects First we study relative error on support estimation and model accuracy to discover frequent itemsets varying the number of clusters and the minimum support threshold Second we evaluate speed and scalability 4.1 Setup Experiments were performed on a workstation with a 2.4 GHz CPU 2 GB of memory and 160 GB on disk The algorithms were programmed in the C language The data sets were stored as text les The clustering model matrices were stored on a binary le after their computation The model was loaded into main memory before mining frequent itemsets Each experiment was repeated ve times and average measurements are reported In general elapsed times are reported in seconds 4.2 Data Sets Experiments use real and synthetic data sets The real data sets were obtained from the UCI Machine Learning Repository 4 a n d ar e w id ely u sed i n t h e r e sear ch liter a tu r e on association rule mining Synthetic transaction data sets were created running the well-known IBM transaction data generator 3  A ll d a ta sets wer e tr an sf o r m e d a n d co n v er ted to transaction format itemset representation as required by our proposal Table 1 provides a summary of data sets We now explain how synthetic data sets were generated with the IBM transaction data generator 3  Th is g e n e r a to r has seven parameters indicating our notation in parenthesis number of transactions D  n  number of items  d  number of patterns P frequent itemsets average transaction length T average pattern length I average correlation corr and average rule conﬁdence conf Testﬁlesarenamed after the parameters with which they were created The standard way 3 15 i s t o u s e T  I a nd D t o l abel  l es s i nce those are the three most common parameters to change and Table 1 Data sets Data set type dn Chess real 75 3196 Mushroom real 127 8124 Vo t e s real 17 435 T10I4D100k synthetic 100 100k T10I4D1M synthetic 100 1000k the ones which play a more important role in performance e.g T10I4D100k The transaction les we used had defaults D=1M  n 1000k d  100  T=10 I=4 P=100 corr 0  75 and conf 0  75  4.3 Clustering Model Accuracy Deafult Parameters Setting Each data set has a different optimal setting for k Therefore there is no default value for k  On the other hand we set 004 0  2  Recall 004 is used to lter out itemsets unlikely to be frequent Relative error on support was used to measure accuracy For an itemset X it was computed as e s  X   a  X   s  X     s  X    where average support a  X  is obtained from the clustering model and s  X  is the exact support value as computed from D  We focus on analyzing error for support estimation The problem of choosing the best k is called model selection 23 A l o w k produces a simple but probably inaccurate model a high k will produce a more complex but accurate model We made a few runs to determine a good k for the real data sets considering that when k is too high some clusters tend to have almost zero points Therefore we set k high enough so that relative error reached less than 10 and we did not get zero-weight clusters In general there was a slight variation in the cluster model quality from run to run We selected the run that produced best clustering results out of ve runs in order to estimate support bounds This is a reasonable approach because in practice a data mining user picks the best model Error varying the number of clusters Figure 5 analyzes error behavior varying k Theleft graph shows error behavior for real data sets at 002 0  2  whereas the right graph analyzes error for the synthetic data set for two 002 values We discuss results for real data sets As can be seen error rapidly decreases as k grows The trend indicates asymptotic behavior Despite the difference in data set sizes and dimensionality in all cases error goes below 5 when k reaches 80 On the other hand error is 
613 
613 


0 0.05 0.1 0.15 0.2 0.25 0.3 0 20 40 60 80 100 Error k Real data set Chess  Mushroom   Votes  0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0 100 200 300 400 500 600 700 800 Error k Synthetic data set tau=0.10  tau=0.05  Figure 5 Support estimation error increasing k  high when k is low We now turn our attention to the synthetic data set where we built models with much higher k  In this case error shows a slower rate of decrease The trend also seems asymptotic We can also see there is a clear gap between 002 0  05 and 002 0  10  error roughly grows 100 when 002 decreases by 50 Interestingly enough results are much better for real data sets than for the synthetic data set Error decreasing minimum support Figure 6 shows error growth as 002 decreases The left graph analyzes error for real data sets We can see error growth shows different behavior for each data set Error growth is slow for the Votes data set Error grows almost linearly for the Chess data set Error grows fast for the Mushroom data set The trend indicates we need to build more accurate models with higher k for Mushroom probably not for Chess and not necessary for the Votes data set The right graph analyzes error growth for the synthetic data set Theﬁrstmodelat k  800 is twice as accurate as k  400  We can see error grows linearly for high 002 values but it start growing faster at 002 0  1  The trend indicates the growth is not linear but it does not seem exponential 4.4 Comparing Speed and Scalability We rst compare our proposal versus the standard algorithm to mine association rules We then study time complexity and scalability Comparing clustering and A-priori Table 2 compares the efﬁci ency of the model with the standard A-priori algorithm We must stress our proposal does not intend to substitute fast association rule algorithms Table 2 Comparing model and A-priori 002 from model clustering+model A-priori 0.20 1 1672 24 0.15 1 1672 43 0.10 1 1672 156 0.05 3 1674 645 0.02 11 1683 3347 0.01 36 1708 14806 32 16 b u t w e i ncl ude t h es e c ompari s ons t o pro v i d e a rel ative performance benchmark We used the synthetic data with n 1 M  The clustering model used had the number of clusters set to k  100  These times include the time to compute exact support on a nal pass The rst column varies 002 the minimum support threshold The second column shows the time to discover frequent itemsets from the model excluding the time to compute the model The third column adds the time to compute the clustering model and the time to mine frequent itemsets Finally the fourth column shows the time for the traditional algorithm As can be seen the clustering model r epresents an efﬁcient mechanism to produce all frequent itemsets assuming the model is already tuned and stored That is we assume the model is computed a few times or even once The second column shows clustering the data set takes most of the time In this case the standard algorithm is faster at high support levels but the model becomes faster at low support levels Notice the third column represents a pessimistic case in which the model is recomputed every time Finally we can see the A-priori algorithm suffers scalability problems due to the exponential growth of patterns The basic reason the model is faster is because it uncovers long itemsets and it is efﬁciently manipulated in main memory 
614 
614 


0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0 0.1 0.2 0.3 0.4 0.5 0.6 Error tau Real data set Chess  Mushroom   Votes  0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0 0.05 0.1 0.15 0.2 Error tau Synthetic data set k=400  k=800  Figure 6 Support estimation error decreasing 002  0 100 200 300 400 500 600 700 0 200 400 600 800 1000 1200 1400 1600 Time in seconds n x 1000 Data set size d= 100  d=1000  0 50 100 150 200 250 300 0 20 40 60 80 100 120 140 160 Time in seconds k Number of clusters d= 100 n=100k  d=1000 n=100k  Figure 7 Time complexity for clustering large data sets with K-means 
615 
615 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


