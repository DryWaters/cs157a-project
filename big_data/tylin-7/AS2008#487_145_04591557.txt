Hybrid Strategies for Attribute Relation Learning from Candidates  Kui Fu, Guihua Nie, Huimin Wang Department of Electronic Business, Wuhan University of Technology, Wuhan, China fukui & niegh}@whut.edu.cn, huiminwangbj@126.com  Abstract Attribute relation learning is important, but has been few studied. This paper proposes hybrid strategies for attribute relation acquisition from candidate attributes. The composition of candidate attributes is firstly analyzed and subdivided into three types: non-attribute vocabularies, invalid attribute and valid attribute. Secondly, the HowNet-based filtering strategy is presented which filters out the nonattribute vocabularies and invalid attributes from the candidates using the knowledge of “is-a” relations and attribute-host relations described by attribute sememe in HowNet. Thirdly, the pruning strategy based on domain concept tree is proposed to further perfect the associations between a concept and its candidate attributes. We define some pruning rules through which some redundant, unreliable, even wrong attributes can be discarded from candidates and some lost attributes can be recalled. Our results about attribute relation learning show the efficiency of our hybrid strategies 1. Introduction Concept intension is the aggregation of common attributes of all instances belonging to the concept Therefore, it is very important to identify attributes of a concept for the understanding of concept. Besides attribute relation has an important role in many ontology-based computer application systems. But comparing with is-a relation, researchers pay little attention to attribute relation. Almost all-existing ontology learning tools and systems don’t provide support for attribute relation learning Classification of attributes varies with the difference of understanding. According to Pustejovsky’s Generative Lexicon theory  t h e r e a r e f o ur  t y pe s of attributes: the Formal Role, the Constitutive Role, the Telic Role and the Agentive Role. In order to identify concept attributes, Poesio and Almuhareb[2 f i r s t l y  collected candidate attributes using text patterns. They considered that the training data in the experiment could be classified into six type categories: qualities parts, related-objects, activities, related-agents and non-attributes. And then, they constructed attribute classifier using four types of information morphological information, an attribute model, a question model, and an attributive-usage model. These works are positive attempts about attribute knowledge But all the above categories of attribute are broad, and could meet the requirement of attribute relation acquisition of ontology construction This paper proposes hybrid strategies for attribute relation acquisition from candidate attributes. The candidate attributes are collected using the text-pattern method implemented in our ontology learning system OntoFactory [3  T h e H o w N e t b as e d f ilte r i n g s t r a te g y  is presented which filters out the non-attribute vocabularies and invalid attributes from the candidates The pruning strategy based on domain concept tree is proposed, so that some redundant, unreliable, even wrong attributes can be removed from candidates and some lost attributes can be recalled The rest of this paper is organized as follows. In Section 2, we review HowNet and attribute knowledge in HowNet. We also introduce our ontology learning system, OntoFactory. Section 3 details filtering strategy of candidate attributes based on HowNet Pruning strategy of attribute relations based on domain concept tree is introduced in Section 4. Experimental results are provided in Section 5. Finally, we present conclusions of our work 2. Background 2.1 HowNet and its attribute knowledge HowNet is an online common-sense knowledge base exposing inter-conceptual relations and interattribute relations of concepts as connoted in lexicons of Chinese and their English equivalents [4   Th e essential concept is sememe which broadly speaking refers to the smallest basic semantic unit that cannot be reduced further and is used to define the other words that are not the sememe. These sememes have been classified into five types \(event, thing, attribute, value of attribute and sub-feature\ that comprise the fundamental layer of the knowledge base The way understanding attribute in HowNet is any one object necessarily carries a set of attributes Similarities and differences between the objects are determined by the attributes they each carry. There will be no object without attributes. The relationship between the attributes and their host is unbending. The attributes simply come with the host and vice versa [4   There exist attribute sememe and attribute value sememe in HowNet. Attribute sememes are subdivided 
Annual IEEE International Computer Software and Applications Conference 0730-3157/08 $25.00 © 2008 IEEE DOI 199 
Annual IEEE International Computer Software and Applications Conference 0730-3157/08 $25.00 © 2008 IEEE DOI 10.1109/COMPSAC.2008.21 199 
Annual IEEE International Computer Software and Applications Conference 0730-3157/08 $25.00 © 2008 IEEE DOI 10.1109/COMPSAC.2008.21 199 
Annual IEEE International Computer Software and Applications Conference 0730-3157/08 $25.00 © 2008 IEEE DOI 10.1109/COMPSAC.2008.21 199 


into seven types: appearance, measurement, property relationship, situation, quantity property and quantity They are hierarchically organized according to taxonomic relations among attributes. The hierarchical tree is shown in Fig.1 Fig. 1 The hierarchical tree of attribute sememe  Attributes are necessarily defined in terms of the possible classes of host in HowNet, which is to annotate the attribute-host relation. An example of attribute sememe’s definition is given in Fig.2. We can find that the host of attribute “fatness” is concept animal” or “human” from the attribute sememe’s definition given in Fig.2. The character “&” is the symbol used to label the attribute-host relation Fig. 2 The form of attribute sememe’s definition  2.2 OntoFactory OntoFactory is an system for automatic acquisition of domain ontologies from web pages, domain dictionaries, general ontology and other sources It  supports the language of English and Chinese. The tasks of ontology learning in OntoFactory include domain concept acquisition, is-a relation learning attribute relation learning and ontology instance acquisition. Concept learning is achieved using statistical, linguistical approaches and short-term information extraction techniques. Is-a relations are mainly learned through the method based on self-study pattern and based on the discovery of web directories Attribute relation acquisition is achieved on the basis of results of concept learning and is-a relation learning The method of text-pattern attribute acquisition is implemented in OntoFactory system 3. Filtering  of Candidate Attributes Based on HowNet Candidate attributes of a concept can be collected using the text-pattern method implemented in our ontology learning system, OntoFactory. Acquired candidates consist of many other vocabularies, besides attributes. The goal of filtering of candidate attributes is to separate attributes from the others Candidates are composed of attribute vocabularies and non-attribute vocabularies. Attribute vocabularies can be further divided into valid attributes and invalid attributes. Valid attributes are attribute vocabularies between which and their host concept there exists the attribute-host relation described in HowNet. Invalid attributes are attribute vocabularies that don’t satisfy the attribute-host relation. The composition of candidates discussed above is shown in Fig.3 Candidate Attributes   Fig. 3  Composition of Candidate Attributes  Non-attribute Vocabularies Attribute Vocabularies  Valid Attributes  Invalid Attributes  Filtering of candidate attributes based on HowNet is to filter out non-attributes vocabularies and invalid attributes from candidates using the knowledge of attribute sememe in HowNet. The filtering process includes two phases: non-attribute vocabularies filtering phase and invalid attributes filtering phase The first phase is to discard non-attribute vocabularies from candidate attributes, and then attribute vocabularies are left. The second phase is to delete invalid attributes from attribute vocabularies, and then only valid attributes satisfying the attribute-host relation is left. Our goal is reached after the two-phase filtering. Filtering of non-attribute vocabularies and invalid attributes are discussed below in detail 3.1 Filtering of Non-attribute Vocabularies Candidate attributes include known vocabularies and unknown vocabularies in HowNet from the perspective of HowNet. For a known vocabulary, we may directly determine whether a candidate is an attribute vocabulary or not according to the definition of attribute sememe in HowNet. For an unknown vocabulary, the determination process is relatively complicated. The idea is to measure the similarity between the candidate and existing attribute vocabulary in HowNet, and to make determination according the similarity. If the value of similarity is smaller than an experiential threshold value, the candidate is view as a non-attribute vocabulary. If the value of similarity is bigger than or equal to the experiential threshold value, the candidate is view as an attribute vocabulary. Besides, we can identify the attribute type that the candidate belongs to, according to the most similar attribute descending by the value of similarity and “is-a” relations of attribute sememe described in HowNet From the above basic idea of filtering of nonattribute vocabularies, we can find that it is the key to 
200 
200 
200 
200 


measure the similarity between the unknown vocabulary and existing attribute vocabularies in HowNet. We firstly search the candidate in other semantic dictionaries, such as WordNet, Synonyms dictionary. If the candidate is found in above dictionaries, then the similarity is measured using the semantic similarity algorithm proposed in [5  e l s e t h e  similarity is calculated using the literal similarity algorithm proposed in [6  3.2 Filtering of Invalid Attributes After filtering of non-attribute vocabularies, there are only left attribute vocabularies in candidate attributes. The next work is to filter out invalid attributes from attribute vocabularies, which is based on the previous work Same as non-attribute vocabularies, attribute vocabularies can also be divided into two types: known vocabularies and unknown vocabularies in HowNet from the perspective of HowNet. Respectively filtering of invalid attributes includes the processing of known candidates and unknown candidates. For a known attribute vocabulary in HowNet, we can match the attribute vocabulary and the concept with the attribute-host relation described by the attribute sememe in HowNet. If the match succeeds, the attribute vocabulary is determined as a valid attribute of the respective concept, or else as an invalid attribute For an unknown attribute vocabulary in HowNet, due to recording of its most similar attribute and identification of attribute type, we can determine whether it is an invalid attribute or not according to its most similar attribute 4. Pruning of Attribute Relations After filtering of candidate attributes, some refinement and pruning work should be done in order to further perfect the associations between a concept and its candidate attributes. In some cases, some redundant, unreliable, even wrong attributes need to be discarded from candidates. In other cases, some lost attributes need to be recalled through rules. We solve these problems using our pruning method based on domain concept trees, which is detailed as below 4.1 Generation of Domain Concept Tree with Attributes The domain concept tree with attributes is the basis of our pruning method. The generation process of domain concept tree with attributes consists of two steps: the generation of domain concept tree and the mapping of candidate attributes In order to generate the domain concept tree, we should first make use of the knowledge of ontology concepts and “is-a” relations among them, which are learned by OntoFactory system. The generation rule of domain concept tree is defined as below Rule I Let H c  c i  c is-a c i  c  i 1,2 n be the is-a” relations aggregation, where c is the hypernym concept c i  is the hyponym concept. When generating the domain concept tree, we create a tree node for each concept, and let the tree node containing concept c as the parent node, other tree nodes as child nodes The mapping of candidate attributes is to map all candidate attributes into the domain concept tree. It needs to find the tree node containing the host concept of candidate attributes in the domain concept tree, and then to insert candidate attributes into the tree as the found tree node’s attributes. An example of domain concept tree with attributes is shown in Fig. 4 publications Fig. 4  A domain concept tree with attributes paper publications magazine\(price,publisher,intrudction,Vol electronic eublications\(price,publisher book\(price,publisher,intrudction cyclopaedia\(price,publisher,intrudction  CD\(price,publisher,singer’s name VCD\(price,publisher,film’s name video tape\(price,publisher,singer’s name music tape     4.2 Pruning Rules Based on Concept Tree After Generation of domain concept tree with attributes, we can prune the attribute relations in the domain concept tree using the leaf-root methods. Our pruning methods are implemented based on the following three pruning rules Rule II If candidate attribute aggregations of each hyponym concept of concept c contain candidate attribute a then attribute a is determined as an attribute of concept c and should be recalled Rule III If candidate attribute aggregations of concept c and its most hyponym concepts contain candidate attribute a then attribute a is determined as an attribute of concept c and so attribute a should be added to candidate attribute aggregations of other hyponym concepts which don’t contain attribute a  Rule IV If attribute a only exists in candidate attribute aggregations of concept c and few hyponym concepts, then attribute a is an unreliable attribute and should be discarded from candidate attribute aggregations of concept c and its all hyponym concepts The following is some explanations of applying above rules to prune attribute relation with the Fig. 4 Due to all hyponym concepts of “paper publications have the attributes “publisher”, “introduction” and price”, we can infer that attributes “publisher introduction” and “price” are the attribute of concept paper publications” according to RULE II. Owing to concept “electronic publications” and its most 
201 
201 
201 
201 


hyponyms have the attribute “price” and “publisher we can determine that the hyponym “music tape” has the high probability of having the attribute “price” and publisher” according to RULE III, though no attribute is learned in the learning process 5. Experimental Results We selected online consumed product sold in electronic markets, such as www.alibaba.com.cn and www.hc360.com, as our experiment domain. We recently acquired concepts and is-a relations between these concepts using our ontology learning system OntoFactory. After constructing and refining the ontology, there are left 2,137 concepts. Then we collected candidate attributes for each concept using the text-pattern method implemented in OntoFactory The number of all candidate attribute relations is 19,954. We threw out every candidate attribute with the appearing frequency less than 15; this reduced the number of candidate attribute relations to 3,898 Considering complexity of work, 1,588 candidate attribute relations were randomly selected and handlabeled as testing data set We do two experiments in turn: filtering of candidate attributes based on HowNet and pruning of attribute relations based on domain concept tree. In experiments on filtering of candidate attributes based on HowNet, we let the threshold value of similarity be 0.45 experientially.  We filtered out non-attribute vocabularies and invalid attributes from the testing data set using the filtering strategy introduced in Section II. In experiments on pruning of attribute relations, we let the threshold value of RULE II be 0.67 experientially. Then, we pruned attribute relations on the basis of filtered results. The experimental results are given in TABLE I  TABLE I EXPERIMENTAL RESULTS  Phase Extracted relations  Precision Recall F-value  1  1,953 814 2,335 0.417  0.349 0.380  1,137 785 2,335 0.690  0.336 0.452 1,404 1007 2,335 0.717  0.431 0.539 From TABLE I, it is observed that the precision increases and the recall decreases through the initial phase to the filtering phase. According to our filtering strategy, non-attribute vocabularies and invalid attributes are discarded from candidate attributes Therefore, the precision is improved. At the same time a few correct attribute relations may be incorrectly discarded also; this result in the decrease of the recall In the pruning phase, the precision continues to increase and has a value of 0.717. This is because some correct attribute relations were recalled according to pruning RULE II and RULE III, the same as the recall During the pruning phase, only a few attribute relations may be deleted from candidates and the effect can be ignored In general, the F-value always increases during the process of all experiments. It is indicated that attribute relations are improved using our hybrid strategies 6. Conclusion In order to further perfect attribute relation learning from candidate attributes collected using text-pattern method, HowNet-based filtering strategy and pruning strategy based on domain concept tree are proposed in this paper. Candidate attributes are classified into nonattribute vocabularies, invalid attribute, and valid attribute. Non-attribute vocabularies can be filter out from candidates by using of the knowledge of attribute sememe in HowNet and similarity measures between the candidate and existing known attribute in HowNet And then attribute vocabularies are only left in candidates. Invalid attributes can be filter out from attribute vocabularies using of the knowledge of attribute-host relation in HowNet. After filtering of candidate attributes, some refinement and pruning rules are defined based on domain concept tree with attributes, so that some redundant, unreliable, even wrong attributes can be removed from candidates and some lost attributes can be recalled Acknowledgement This research is supported by the National Natural Science Foundation of China under Grant NO.70572079 and National Key Project of Scientific and Technical Supporting Programs under Grant NO.2006BAH02A08 References 1  H Pustejovsky, J. \(1995 The generative lexicon MIT Press 2  Abdulrahman Almuhareb, Massimo Poesio Finding Attributes in the Web Using a Parser Proceedings of the Corpus Linguistics 2005 Conference, Birmingham, July 14-17,2005 3  Kui Fu, Guihua Nie Product Ontology Learning from Web Pages Proceedings of 2007 International Coference on Innovation & Management. Yamaguchi, Japan. Dec.56,2007 4  Zhendong Dong, "HowNet", http://www.keenage.com unpublished 5  Jian Wu Zhaohui Wu, Ying Li, Shuiguang Deng Web Service Discovery Based on Ontology and Similarity of Words Chinese Journal of Computers, 2005,28\(4 6  Yihua Zhu Automatic Recongnition of Synonym in Construction of Intelligent Search Engine Dissertation   Nanjing: Nanjing Agricultural University, China. 2001  correc t  r elations Total correc t  relations Extracte d  Initial p hase Filtering phase Pruning phase 
202 
202 
202 
202 


using the denition given in Equation 3 F  x             0 if  x  a  or  x  d   x  a   b  a  if x   a b  1 if x   b c   d  x   d  c  if x   c d  3 Table 2 shows the results of applying the Fuzzy function Eq 3 for diferent values of risk_zone and the returned value represents the degree of membership to a risk zone for towns that are in such range For example for T 1 with risk_zone of 8 km the membership function returns a highest degree of risk equals to 1 TABLE II T RANSACTIONS D EGREE OF MEMEBERSHIP TO F UZZY SETS  USING F X  IN E Q 3 Fuzzy sets T 1 T 2 T 3 T 4 T 5  risk_degree High 1 0 0.44 0 0  risk_degree Medium 0 0.23 0.56 1 0  risk_degree Low 0 0.77 0 0 1 Another way to estimate the degree of membership consists on to dene a threshold   this value indicates whether a transaction belongs to some set For instance working with  0  3  those transactions with a value less than equal to 0.3 do not belong to a given fuzzy set then their degree of membership will be 0 E Association Rules Extraction The Association Rules are obtained from the spatial datawarehouse using Apriori algorithm which it is part of WEKA A collection of tools that implement decision trees tables and rule learners,etc By using the data cube is possible to generate the association rules like the one represented in expression 4 This rule helps to make the right decision with respect to the evacuation route that exist in towns close to Popocatepetl volcano in case of an eruption COUNTY _ IN _ ROU T E  xRISK _ DEGREE  High ROAD _ STATUS  Good support   SELECTED _ ROU T E  x support conf idence 4 Suppose we consider unpaved road near a town with high population and high probability of seismic activity attributes together with an advisory of evacuation this situation could create a bottle neck so we expected that our model will return an alternative evacuation route INEG’s le containing Popocateptl seismic activity was processed using the Apriori algorithm in order to get the association rules The parameters of the Apriori algorithm are dened as follows Condence of 0.9 high expectation Support minimum 0.01 0.05 0.1  b as ed on t h es e v al ues w e can r e duce t he number o f r ul es and redundancy See Figure 7 Typically it is desirable to generate only rules from frequent itemsets that are well-represented in the data The minimum Fig 7 Association rules from Weka frequent itemset support is a user-specied percentage that limits the number of frequent itemsets produced by the model A frequent itemset must appear in at least this percentage of all the transactions if it is to be used as a basis for rules TABLE III N UMBER OF RULES VS  MINIMUN SUPPORT Support Number of rules Iterations 0.5 3 10 0.1 7 18 0.05 73 19 0.01 197 20 Table 3 shows that keeping high condence equals to 0.9 and varying support we can control the number of rules and working with 10 the number of rules is reasonable It is important to mention that instead of working with the nine attributes,that the le denes were selected only ve attribute zone that represents the three risk zones previously dened ashes_affect represents the level of ashes on the ground volcanic_material represents the level of fallen volcanic material such as incandescent stones ow_path represents different trajectories for lava ow and ow_danger that represents the danger level This reduction of the dimensionality in terms of attributes allows to the algorithm to nd the most interesting patterns V C ONCLUSIONS This work proposes a framework to model the geography according how it is presented in the real world taking advantage of the new information technologies The concepts in which the multidimensional model is based on and the 130 


Fuzzy Logic are complemented for the generation of the Fuzzy Data Cube Data Mining techniques are applied to the information stored in the FDC in order to extract the set of association rules which dene the behavior of spatial objects in their environment Fuzzy sets dened in the FDC allow to model spatial objects whose attributes have a certain degree of membership Spatial and non spatial attributes can support the information related to vague regions Fuzzy Logic is crucial in modeling vague regions since their borders are not sharply dened In the application presented here Fuzzy Logic helps us to determine the degree of membership of the different towns to each risk zone and then can determine which evacuation road could be the most suitable one Therefore Association Rules obtained from FDC will assist on the decision making process for instance when it is necessary to choose among different available evacuation routes it has to be considered quality measures such as condence and support that association rules provide A CKNOWLEDGMENT Authors want to thank Facultad de Ciencias de la Computación Computer Science Dept for the help and support in making this wortk possible Part of this result is in part due to National Council for Science and Technology CONACyT for supporting Jos Tecuapacho R EFERENCES  P  A  B ur r ough and A  U  F r a nk Concepts and paradigms in spatial information Are current geographic information systems truly generic  International Journal of Geogra phical Information Systems 1995 2 J  H er nández J  R  Q u intana and C  F er r i  Introducci—n a la Miner’a de Datos  Capítulos 1 2 y 9 Pearson Prentice Hall 2005 3 M  J  S om ode villa Fuzzy MBRs Modeling for Reasoning about Vague Regions  PhD Thesis Tulane University 2003  L  Y ubao Y  J i an The Computation of Semantic Data Cube  The 4th International Conference on Grid and Cooperative Computing pp 573578.Beijing China Nov 30-Dec 3 2005 5 A  R eda K  M e hm et Integrating Fuzziness into OLAP for Multidimensional Fuzzy Association Rules Mining Third IEEE International Conference on Data Mining pp 469-475.Nov 2003  R ongqin L   W e nzhong S  X iaom ei Y  and G uangyuan L  Mining Fuzzy Spatial ConÞguration Rules Methods and Applications  ISPRS Workshop on Service and Application of Spatial Data Infrastructure.XXXVI pp319323.2005 7 J  M  M olina a nd J  G a r c a TŽcnicas de An‡lisis de Datos Aplicaciones Pr‡cticas utilizando Microsoft Excel y Weka Art Universidad Carlos III Madrid España.pp539-562.2004  M icr o s o ft 2005 OLAP and data mining functionality  Available http://www.microsoft.com/spanish/msdn  M er cedes V ittur i ni S ilvia Cas t r o and S er gio M ar tig  2 005 Modelos de Datos Espaciales  VII Workshop de Investigadores en Ciencias de la Computación.pp39-62.Argentina Available:http://dc.exa.unrc.e du.ar/wicc/papers/Otros/42.pdf  M e ter F is her  Boolean and Fuzzy Regions Department of Geography University of Leicester Leicester,UK.2001  Car l os M o lina F er nández Impresici—n e Incertidumbre en el Modelo Multidimensional Aplicaci—n a la Miner’a de Datos PhD Thesis Universidad de Granada.2005  Bonif acio M ar tín and A lfr e do Sanz Redes Neuronales y Sistemas Difusos Chapter 7 and 8 Alfaomega Ra-Ma 2001 131 


Since the attribute determination algorithm has determined that the attribute Sno in Table 0, the attribute Cno in Table 1, and the attributes <Sno Cno> in Table 2 embrace the double-connective association rule student\(Sno 010 1 course\(Cno 010 2 study\(Sno, Cno\he connective determination algorithm make the relational matrix shown in Fig. 4 according to the binary relationship table of Table 2   C1 C2 C3 C4 S1   T  T  F  F S2   T  F  T  F S3   T  F  F  F S4   F  T  F  F S5   T  F  F  T   Fig. 4 The relational matrix made from Table 2  Fig. 4 is made like this: Table 2 has the tuple S1, C1>, then at the cross of the row S1 and the column C1, a T is filled; Table 2 does not have tuple S1, C3>, then at the cross of the row S1 and the column C3, a F is filled Suppose the cardinality of student\(Sno\s M, in this example 5, i.e. S1 to S5; the cardinality of course\(Cno\n this example 4, i.e. C1 to C4 The algorithms for DCAR1 through DCAR6 are as follows The algorithm for DCAR1 If in Fig. 4 there is M*cf 1 rows, N*cf 2 columns submatrix, in which all elements are Ts, then DCAR1 holds The algorithm for DCAR2 If in Fig. 4 there is at least one column, in which there are at least M*cf 1 Ts, then DCAR2 holds The algorithm for DCAR3 If in Fig. 4 at least M*cf 1 rows have Ts, then DCAR3 holds The algorithm for DCAR4 If in Fig. 4 there is at least one row, in which there are at least N*cf 2 Ts, then DCAR4 holds The algorithm for DCAR5 If in Fig. 4 at least N*cf 2 columns have Ts, then DCAR5 holds The algorithm for DCAR6    DCAR6   DCAR3  DCAR5     DCAR2  DCAR4   DCAR1 Fig. 5 The complement lattice formed by DCAR1 through DCAR6 
277 
277 


000\003 000\\000L\000J\000\021\000\031\000\003\000\003\000&\000R\000Q\000Q\000H\000F\000W\000L\000Y\000H\000\003\000G\000H\000W\000H\000U\000P\000L\000Q\000D\000W\000L\000R\000Q\000\003\000D\000O\000J\000R\000U\000L\000W\000K\000P\000\003 Start Call DCAR1 000D\000O\000J\000R\000U\000L\000W\000K\000P  DCAR1 holds 002  Call DCAR2 000D\000O\000J\000R\000U\000L\000W\000K\000P  Output DCAR1,2,3,4,5,6 End DCAR2 holds 002  Output DCAR2,3,6 Call DCAR3 000D\000O\000J\000R\000U\000L\000W\000K\000P  DCAR3 holds 002  Output DCAR3,6 Call DCAR4 000D\000O\000J\000R\000U\000L\000W\000K\000P  DCAR4 holds 002  Call DCAR5 000D\000O\000J\000R\000U\000L\000W\000K\000P  Output DCAR4,5,6 End DCAR5 holds 002  Call DCAR6 000D\000O\000J\000R\000U\000L\000W\000K\000P  Output DCAR5,6 End DCAR6 holds 002  Output DCAR6 End Error Y N N Y Y N N Y Y N N Y 
278 
278 


If in Fig. 4 there is at least one T, then DCAR6 holds DCAR1 through DCAR6 forms a complement lattice shown in Fig. 5 In Fig. 5, the lower rule implies the upper rule That is, if DCARj is reachable from DCARi via an ascending path, and DCARi holds, then DCARj holds Because DCAR1 through DCAR6 satisfies Fig 5, their algorithms can be merged into one algorithm called connective determination algorithm, shown in Fig. 6 Suppose cf 1 80%, cf 2 75%. In Fig. 4, for the column of C1, there are M*cf 1 5*80%=4 elements whose values are T \(namely, S1, S2, S3, S5 Therefore, DCAR2: course\(Cno 004 1  student\(Sno 003 1  study\(Sno, Cno\olds. From Fig. 5, we know that DCAR3 and DCAR6 also hold. In Fig. 4, there are at least N*cf 2 4*75%=3 columns which have value T \(namely, in the column of C1 there is S1, in the column of C2 there is S1, in the column of C3 there is S2, in the column of C4 there is S5 therefore DCAR5: course\(Cno 003 1  student\(Sno 004 1  study\(Sno, Cno  VI. CONCLUDING REMARKS 1\ Double-connective association rule mining is different from single-connective association rule mining. The former mines the association among the primary keys of the two entity tables and the primary key of the binary relationship table. The latter mines the association between frequent item sets 2\. 4 is different from data cubes in data warehouses. The elements in Fig. 4 are T or F. The elements in the data cubes are data 3\The differences between double-connective association rule and database query are that, first, the query information in databases are predeterminate while the information to be mined by double-connective association rule is not predeterminate, it is implied. Secondly, database query needs to write SQL statements, while double-connective association rule mining is automatic. Thirdly, the information obtained by database query is quantitative, while the information obtained by double-connective association rule mining is qualitative such as “for many”, “there are some  REFERENCES 1 Ji a w ei H a n   M i ch eli n e K a m b er   D a t a  M i n i n g C onc ep t s  a nd Techniques, Higher Education Press, Beijing, 2001, Morgan Kaufmann Publishers, 2000 2 A  G  Ha m i lt on  L o gi c for M a th em a t i c ia ns R evi s ed E d i t i o n   Cambridge University Press, 1988, Tsinghua University Press Beijing, 2003 3 X unw e i Z h o u   Br ie f I ntr o du c t io n  to  Mu t u al l y I nve r s is tic Logic”, 1999 European Summer Meeting of the Association for Symbolic Logic, Utrecht, The Netherlands, August 1-6 1999 4 u n w ei Zh ou F i r s t leve l exp l i c i t m u lt ip le i ndu ct i v e composition”, 2005 Spring Meeting of the Association for Symbolic Logic, The Westin St. Francis Hotel, San Francisco CA. USA, March 25-26, 2005 5 A b rah a m S i lb ers c ha t z  Hen r y  F  Kort h  S S u da rs ha n Dat a b a s e  System Concepts \(Fourth Edition\, Higher Education Press Beijing, 2002, McGraw-Hill Companies, 2002  
279 
279 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


