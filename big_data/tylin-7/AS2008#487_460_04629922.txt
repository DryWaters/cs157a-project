MINING ASSOCIATION RULES WITH SYSTOLIC TREES Song Sun and Joseph Zambreno Dept of Electrical and Computer Engineering Iowa State University Email  sunsong zambreno  iastate.edu ABSTRACT Association Rules Mining ARM algorithms are designed to nd sets of frequently occurring items in large databases ARM applications have found their way into a variety of elds including medicine biotechnology and marketing This class of algorithm is typically very memory intensive 
leading to prohibitive runtimes on large databases Previous attempts at acceleration using custom or reconìgurable hardware have been limited as many of the signiìcant ARM algorithms were designed from a software developerês perspective and have features e.g dynamic linked lists recursion that do not translate well to hardware In this paper we look at how we can accomplish the goal of association rules mining from a hardware perspective We investigate a popular tree-based ARM algorithm FP-growth and make use of a systolic tree structure which mimics the internal memory layout of the original software algorithm while achiev 
ing much higher throughput Our experimental prototype demonstrates how we can trade memory resources on a software platform for computational resources on a reconìgurable hardware platform in order to exploit a ne-grained parallelism that was not inherent in the original ARM algorithm 1 INTRODUCTION Much effort has been put into looking for highly efìcient ARM algorithms 1 Apriori narro ws the range of candidate frequent itemset by applying the principle that a superset of items must not be frequent unless any subset of it is frequent Through repeated iterations Apriori determines 
the n length frequent itemset in the n th iteration Two drawbacks prevent a more widespread adoption of Apriori The rst is that frequent itemsets consume large amounts of time and memory to compute The other drawback is that repeated scans of the database put a lot of pressure on I/O devices and lead to intolerable performance overhead The FP-growth algorithm solv es the second problem of Apriori by reading the database only twice By storing all transactions in a compact tree frequent itemsets are explored 
by recursively traversing the tree and performing a bruteforce enumeration If the tree is predictably large the entire database can be projected to produce smaller trees In addition to searching for more efìcient algorithms researchers have looked into parallel computing and hardware implementations 4 5 In this paper we look at this problem from a hardware perspective and propose a new hardware architecture based on a systolic tree structure The remainder of this paper is organized as follows Section 2 describes related work Section 3 explores FP-growth algorithms in more depth The 
systolic tree and the candidate itemset dictation are discussed in Section 4 Section 5 is the implementation evaluation Section 6 is the conclusion 2 RELATED WORK A parallel Apriori algorithm version is proposed in In 8 the FP-growth algorithm is implemented on PC clusters Though parallel computing platforms can improve the processing speed of software algorithms measured speedup has not scaled with the number of processors In scaling ef ciency with number of processors is only 13.4/16  0.84 and 22.6/32  0.71 respectively In only 2 times speedup 
was achieved when using 8 nodes An advantage FPGAs have over parallel computing platforms is the ability to parallelize algorithms at the instruction-level granularity as opposed to a higher level module-level granularity The poor scaling of software-based algorithms has sparked research into hardware-based methods for data mining A parallel implementation of the Apriori algorithm on FPGAs was rst done in Due to the processing time in v olv ed in reading the transactional database multiple times the hardware implementation was only 4 times faster than the fastest software implementation They further explored the paral 
lelism and developed a bitmapped CAM architecture which provides 24 times performance gain over the software version Achie ving a better speedup using Apriori will be difìcult due to the nature of the algorithm which must read the entire database once for every item in the worst case FPgrowth algorithms are an alternative to Apriori-based solutions Conclusions from several independent evaluations in978-1-4244-1961-6/08/$25.00 ©2008 IEEE 143 


Table 1  Transactional Database  ID Items ID Items 1 B,C,D 5 A,B,C 2 B,C 6 A,B,C 3 A,C,D 7 A,B,D 4 A,C,D dicate that FP-growth is the current optimal association rules mining algorithm 11 While softw are solutions e x ist we do not know of any existing hardware implementations of the FP-growth algorithms 3 FP-GROWTH ALGORITHM The FP-growth algorithm is composed of two phases In the rst phase the FP-tree is built up The frequent itemset is generated in the second phase All transactions in the database are represented compactly in the FP-tree If one transaction is a preìx of another transaction they share the same path in the FP-tree As transactions share more paths the tree is denser In the worst case the number of nodes in the tree is 2 n where n is the number of items in the database If the FP-tree is too large to t into memory the projection method can be used to di vide the tree into smaller more manageable portions The smaller subdatabases can be mined in multiple processors or processed sequentially in one device Table 1 shows an example of a small transactional database which is composed of four ordered items  A,B,C,D   Each row in the table corresponds to a transaction Figure 1 shows an FP-tree that contains all the information of the transactions in the database The number in each node represents the times the preìx from the root item to it occur in the transaction database The level of a node is the number of the nodes from it to the root The higher the level of a node the larger length of the preìx it represents Because two transactions can only share a path when one is a preìx of another the number of the higher level nodes is never greater than that of the lower level nodes The FP-growth algorithm extracts the frequent itemset in a recursive enumeration manner Starting from the highest level of the FP-tree toward the root the itemsets ending with item D are checked rst This problem is further divided into several subproblems of nding frequent itemset ending with  C,D    B,D  and  A,D   Each subproblem ending with deìned itemsets is further divided into smaller problems by building its conditional FP-tree and 12 discuss the operation on conditional FP-tree in detail A:5 B:3 C:2 B:2 C:2 D:1 C:2 D:2 D:1 control Fig 1  The FP-tree Data Structure 4 OUR APPROACH 4.1 Systolic Tree It is not always practical or efìcient to directly translate a software algorithm into a hardware architecture Our approach is to build the tree based on the maximum node degree estimation When the actual node degree at some point in the tree exceeds the estimated node degree some frequent itemset will not be found Suppose the number of items in the database is n  the estimated maximum node degree estimation is K and the estimated depth of the systolic tree is W  Each node in the static tree structure has K children The total number of nodes in the tree is K W  K W  1    K 1  K  K W  1 K  1  When K is large the number of children for each node is large which in turn requires each node to have a large number of interfaces This will make the inner structure of each node very complex To simplify the complexity of the node we assign two instead of K interfaces to each node One of the two interfaces is dedicated to the connection with its rst child the other one is connected to its nearest sibling To further illustrate the difference between FP-tree and systolic tree architecture consider the FP-tree data structure shown in Fig 1 This FP-tree is different from the one deìned in in that it has a control node The input and output of the whole systolic tree passes through the control node The input can be data or control signals and the output is usually deìned by the designer The letter inside each node represents the item and the number beside the node is the number of times that the item emerges in the transaction database The dashed lines indicate the connections in the original FP-tree The solid lines show the actual connections of the nodes in the systolic tree Figure 2 shows the static systolic structure where K 2 and W 3 Each node in the systolic tree architecture is also referred to as a processing element PE Each PE has its local data structure and corresponding operations upon receiving signals from outside There are three kinds of pro144 


 Control B:2 C:2 D:1 C:2 D:2 A:5 B:3 C:2 D:1    2 1 3 4 5 6 7 89 10 11 12 13 14 Fig 2  The Systolic Tree Architecture cessing elements in this gure The root PE is the control node discussed above The PEs in the rightmost column are the counting nodes which are speciìcally used for frequent itemset dictation which we will talk about later The third kind of processing elements are the general PEs Each node in Fig 1 has a counterpart in the general processing elements in Fig 2 but the converse does not hold Each general PE has one input from its parent and two outputs to its child and siblings respectively Each PE only has a connection with its leftmost child If it has to send the data to its rightmost child the data will be passed to its leftmost child and then through its siblings passing through all children on the way The general processing elements which do not contain any item are empty If the items in one transaction are transferred into the architecture in an ascending order any PE must contain a smaller item than that of its children However this does not guarantee that the node item in the systolic tree is always greater than its left-side siblings 4.2 Systolic Tree Creation Each PE in the systolic tree has three modes WRITE mode SCAN mode and COUNT mode The last two modes will be discussed in later sections When the tree is in building phase PEs are in WRITE mode An item is loaded into the control PE each cycle which in turn transfer each item into the general PEs If the item is already contained in a PE the corresponding count value will be increased Otherwise an appropriate empty PE will be located for it The algorithm for WRITE mode in each PE is given in Fig 3 The input of the algorithm is an item t The match ag is set when the item in PE matches t The Inpath ag is not set when the PE does not contain any item of the current transaction For example the PE under the control PE in Fig 2 should not contain the item B in a new transaction  A,B,C  After all items are sent to the systolic tree a control signal that states the termination of an old transaction and the start of a new one is sent to the control PE The signal will be broadcasted to all PEs which reinitialize themselves for the next transaction The initialization includes resetting match and Algorithm  WRITE mode\(item t  match  0 InPath  1 1 if P E is empty then store the item t  count  1 match  1 stop forwarding  2 if  tisinPE  and  InPath 1 then match  1 count  stop forwarding  3 if  match 0 then forward t to the sibling  InPath  0 else forward t to the children Fig 3  WRITE Mode Algorithm Inpath ags in the rst line of Fig 2 Letês illustrate the creation of systolic tree with an example shown in Fig 2 In order to clearly differentiate PEs a number in light scale is placed in the top-right corner Suppose a new transaction A,B,D is to be added into the systolic tree A control signal which indicates the WRITE mode is rst broadcasted from the control PE Then the transaction A,B,D is sent to the control PE sequentially When PE1 receives A the step 3 in Fig 3 is triggered and A is forwarded to PE2 The Inpath ag in PE1 is set to 0 Step 2 is triggered in PE2 The count value in PE2 is increased by 1 The match ag is set to 1 PE2 stops forwarding A to other neighbors While item A is sent to PE2 by PE1 the second item B is sent to PE1 by the control node Step 3 is triggered in PE1 Item B is sent to PE2 Since the match ag is set to 1 the item B is sent to PE5 Next step 3 is triggered in PE5 B is then sent to PE6 where it is not further forwarded The count value is increased by 1 in PE6 In a similar way the item D is sent to PE14 4.3 Candidate Itemset Dictation The FP growth algorithm generates frequent itemsets by recursive enumeration However a recursive implementation is impractical in an FPGA implementation The approach used in systolic tree architecture is what we call candidate itemset dictation When we want to check whether a given itemset is frequent or not it is sent to the systolic tree The number of the itemset will be obtained in the output of the systolic tree after some clock cycles The dictation must be performed after the systolic tree is built When the tree is in itemset dictation phase PEs are in SCAN mode In our systolic tree structure there is only one path trac145 


 Algorithm  SCAN mode\(item t  open the bottom door  match  0 IsLeaf  0 1 if P E is empty then stop forwarding  2 if  tisinPE  and  Bottom door is open  then match  1 IsLeaf  1 forward t to the sibling  3 if t  the item in P E then IsLeaf  0 close the bottom door  forward t to the sibling  4 if t  the item in P E then IsLeaf  0 forward t to the sibling  forward t to the child if the bottom door is open  Fig 4  SCAN Mode Algorithm ing back from any PE to the control PE since each PE has a unique parent The main principle of dictation is that any path containing the queried candidate itemset will be reported to the control node Note that such path may contain more items than the queried itemset To clarify the dictation algorithm we deem there are two doors in each PE The right door is always open The bottom door is locked when no data should be sent to the children The door policy is described in Fig 4 The IsLeaf ag is set if PE matches the last item in the queried candidate itemset The PE with IsLeaf set is responsible for reporting the number of the candidate itemset to the counting PEs Since the item is sent one by one the ag IsLeaf is cleared if another item in the candidate itemset which is larger than the stored item passes through the PE If the input item is smaller than the stored item the bottom door should be closed The rationale behind this is that the item in the child can never be larger than that of its ancestor in a path Whenever a bottom door in a PE is closed the path passing through it will never contain the candidate itemset However if the input item is larger than the stored item it should be forwarded to all open doors The rationale is that the path may contain items which are not in the candidate itemset and the candidate itemset is contained in the path Letês illustrate the whole dictation process with the example shown in Fig 5 Suppose the threshold support count is four Now we want to check whether the candidate itemset  B,D  is frequent A control signal which indicates the SCAN mode is rst broadcasted from the control PE Then the item B is sent to the control PE When PE1 receives B the step 2 in Fig 4 is triggered and B is forwarded to PE2 Control B:2 C:2 D:1 C:2 D:2 A:5 B:3 C:2 D:1    1 2 34 5 6 78 91011 12 13 14 Fig 5  SCAN Mode Example Upon receiving the item B the step 4 is triggered The bold lines show the track of the item B PE5 closes the bottom door and forwards the item to PE6 The bottom door in PE5 is locked At this moment both PE1 and PE6 have the IsLeaf ag set Now consider the item D which is sent one clock cycle later after the item B is sent in a pipelined fashion When PE1 receives the item D the step 4 is triggered in Fig 4 The IsLeaf ag is cleared and D is forwarded to PE3 and PE2 The dashed lines show the path taken by item D Notice that since the bottom door of PE5 is locked PE11 will not receive the item D The IsLeaf ag in PE6 is cleared upon receiving D In the end the IsLeaf ag is set in PE7 and PE14 As will be explained in the following section these two PEs report their item count to the counting node The item count value in both PE7 and PE14 is one The total count of candidate itemset  B,D  is two which is less than four Therefore this candidate itemset is not frequent 4.4 Candidate Itemset Count Computation Once all items in a candidate itemset are sent to the systolic tree a control signal signifying the COUNT mode is broadcasted to the whole systolic tree The architecture of the systolic tree will change accordingly with response to the COUNT mode signal Looking back at Fig 2 we have shown that the rst childês input interface is always connected to its parent while others accept input from the rst child in WRITE and SCAN mode In COUNT mode all PEs receive input from its leftmost neighbor except the rst PE in each row The PEs with IsLeaf ag set are ready to send their own item count and forward the count from their leftmost neighbor in a pipelined fashion when all PEs are in COUNT mode The counting PEs on the rightmost column always enter the COUNT mode earlier than the general PEs This is different from the operation used in WRITE mode or SCAN mode where the items are sent one by one in each clock cycle The COUNT mode control signal sent to the counting PEs by the control PE should at least be delayed  KW  W  cycles after it is sent to the general PEs The 146 


 Algorithm  COUNT mode\(int number  CountSent  0 if  CountSent 0 and  IsLeaf 1 then CountSent  1 forward  count  number  to its neighbor  else forward  number  to its neighbor  Fig 6  COUNT Mode Algorithm COUNT mode algorithm in each PE is listed in Fig 6 The input of the algorithm is an integer sent by its left neighbor The count variable is the number of the locally stored item The CountSent ag is set when the local number has been reported to the counting PEs Only the PEs with the IsLeaf ag set can report its local count value while other PEs transfer the number sent by the left neighbor to the right neighbor The counting PEs transfer the number sent by its left and bottom neighbors to its top neighbor The control PE adds up all number sent by the counting PEs and sends it to the output signal 4.5 Candidate Itemset Generation The FP-growth algorithm uses a divide and conquer approach to generate frequent itemset by searching the tree in a bottomup fashion From the hardware perspective the candidate generation method used in Apriori is more suitable for dictation There are several candidate generation procedures The F k  1  F k  1 method is well suited for systolic tree A new candiate itemset with k items is dictated only if it is generated from two frequent itemsets with k  1 items whose rst k  2 items are identical This approach will decrease the number of candiate itemset dramatically 5 IMPLEMENTATION EVALUATION 5.1 Simulated Result The systolic tree architecture was simulated and synthesized in ISE 9.1.03i on target board Xilinx Virtex5 XC5VLX330 The performance and area requirement results in Table 2 are extracted from the post place and route report The total number of PEs includes control PE and counting PEs The biggest circuit in the table is 25 of the device since the area usage is exponentially related to the value of W and K while the clock frequency decreases very slightly with the increase of W and K  The control signal and data width is eight bits in our system The throughput is around 3Gbps Remember that to build the systolic tree the items in transactional database are sent to the tree one by one in each clock cycle The time required to build the systolic tree is  B  throughput seconds where  B  is the size of the database in bits 5.2 Mining Time Comparison To nd all frequent itemsets we can dictate all candidate frequent itemsets If a transaction database has n items the total number of candidate frequent itemsets is 2 n A database can be divided into multiple sub-databases with a smaller number of items These sub-databases can be mined in a parallel manner The parallel manner is also called Parallel Projection where all sub-databases are mined simultaneously The time for creating systolic tree and transfering data through I/O device is usually trivial compared with the time for mining To check the support count of a candidate itemset the C items are sent to the systolic tree in a pipelined fashion It takes C cycles in the WRITE mode The time in SCAN mode is for an item propagated from the control node to the furthest node Since each node has K degrees and the depth of the tree is W  it takes  K  1  W cycles In COUNT mode the support count is collected from those nodes where the last item in the candidate itemset resides This includes the time for the general node propagating the support count to the counting node in the same row and the time for the counting node to propagate the support count to the control node The runtime for the former case is determined by the characteristic of the database Suppose the the database has n frequent items Not every candiate itemset will be produced by the F k  1  F k  1 method with equal probability In general the probability of a candiate with k items is C k n 2 n  Each node in the systolic tree has the equal possibility to be the reporting nodes The average time to dictate the 2 n candidates for the former case is 1 2 n   n k 1 n k C k n  The latter in the worst case is W cycles which is the time for the bottom counting node to propagate the item to the control node The time in this part is negligible compared with that of the former case computed above For an arbitrary transactional database with n frequent items it can always be projected into multiple sub-databases with N frequent items by Parallel Projection Since these subdatabases are mined in parallel the time required for mining is solely determined by the size of the systolic tree If the size of the tree N is equal to the number of frequent items n  i.e K  W  N  n  the number of clock cycles for mining are 2 n   n k 1 n k C k n  Based on the simulated result we compare the mining time of systolic tree with FP-growth algorithm in Fig 7 The FP-growth algorithm is from The mining time of the software algorithm is collected from a PC with Pentium D 3GHz CPU 2GB RAM The benchmark is chess.dat from which is prepared from the UCI datasets and PUMSB This database has 75 items and 3196 transactions In our experiments we change the support count threshold to get 147 


Table 2  Experimental Results  Design Parameters K=2 K=3 K=4 W=3 W=4 W=3 W=4 W=3 W=4 Total PEs 18 35 43 125 88 345 Total Slices 648\(1.25 1267\(2.44 1548\(2.99 4616\(8.90 3169\(6.11 12674\(24.25 Clock Freq MHz 427.533 425.532 421.941 412.031 402.253 363.769 7 8 9 10 11 12 13 0 2 4 6 8 10 12 14 16 18 20 Number of Frequent Items Logarithm of time in microseconds   Software\(FP-growth Hardware\(Systolic Tree Fig 7  Hardware and Software Mining Time Comparison different numbers of frequent items Note that the run time of the FP-growth algorithm is closely related to the size of the FP-tree while the run time of the systolic tree implementation is only determined by the number of frequent items It can be observed that the threshold size of the systolic tree must be no more than 11 in order to be faster than FP-growth algorithm When the size of the systolic tree is 10 the mining speed is 24 times faster than FP-growth Our future work will shrink the size of the systolic tree to shorten the dictation time thus increasing the threshold size of the tree 6 CONCLUSION In this paper we proposed a systolic tree hardware architecture for association rules mining Similar to the FP-growth algorithm our architecture only requires two database reads Our preliminary experiments show that with the careful selection of the size of the systolic tree the mining time can be greatly accelerated compared to current software approaches 7 REFERENCES  R Agra w a l and R Srikant F ast algorithms for mining association rules in Proceedings of the 1994 International Conference on Very Large Data Bases VLDB  1994  J Han J Pei Y  Y in and R Mao Mining frequent patterns without candidate generation A frequent-pattern tree approach Data Mining and Knowledge Discovery  2004  S K otsiantis and D Kanellopoulos  Association rules mining A recent overview in GESTS International Transactions on Computer Science and Engineering Vol.32  2006  A Choudhary  R  Narayanan B Ozisik yilmaz G Memik J Zambreno and J Pisharath Optimizing data mining workloads using hardware accelerators in Proceedings of the Workshop on Computer Architecture Evaluation using Commercial Workloads CAECW  2007  R Narayanan B Ozisik yilmaz J Zambreno G Memik and A Choudhary Minebench A benchmark suite for data mining workloads in Proceedings of the IEEE International Symposium on Workload Characterization IISWC  2006  J Zambreno B Ozisik yilmaz G Memik and A Choudary  Performance characterization of data mining applications using minebench in Proceedings of the Workshop on Computer Architecture Evaluation using Commercial Workloads CAECW  2006  Y  Y e and C.-C Chiang  A parallel apriori algorithm for frequent itemsets mining in Proceedings of the Fourth International Conference on Software Engineering Research Management and Applications  2006  I Pramudiono and M Kitsure g a w a   P arallel FP-gro wth on PC cluster in Proceedings of the Seventh PaciìcAsia Conference of Knowledge Discovery and Data Mining\(PAKDD03  2003  Z K.Bak er and V  K.Prasanna Ef cient hardw are data mining with the Apriori Algorithm on FPGAs in Proceedings of the 13th Annual IEEE Symposium on Field-Programmable Custom Computing Machines FCCM  2005    A n a rchitecture for ef cient hardw are data mining using reconìgurable computing systems in Proceedings of the 14th Annual IEEE Symposium on Field-Programmable Custom Computing Machines FCCM  2006  A Ghoting G Buehrer  S  P arthasarathy  D  Kim A Nguyen Y.-K Chen and P Dubey Cache-conscious frequent pattern mining on a modern processor in Proceedings of the 31st International Conference on Very Large Databases  2005  P N T an M Steinbach and V  K umar  Introduction to Data Mining  Addison Wesley 2005  F  Coenen The LUCS-KDD implementation of the FP-growth algorithm  website 2003 http://www.csc.liv.ac.uk  frans KDD/Software/FPgrowth/fpGrowth.html#downloading  B Goethals Frequent Itemset Mining Dataset Repository  website http://ìmi.cs.helsinki.ì/data 148 


0 10 20 30 40 50 60 3210.75 minimum support execution time \(minutes GMFI GMAR BASIC Cumulate  Fig. 10 Mining time in DENSE databases  7.  Conclusions  Through several comprehensive experiments, we found that the FCET and IFECT can save a larger amount of storage spaces than Apriori, MaxEclat, and CHARM in both SPARSE databases and DENSE databases. Since the FCET stores fewer elements for a long pattern, when matched with GMFI/GMAR algorithm, it also revealed efficient execution time than BASIC and CUMULATE in mining generalized association rules The time complexity to find the maximal itemsets is O\(log2n\ where n is the total number of maximal itemsets. For a long pattern, we used a partition tree to count the SUB_TID of itemsets, and then got their merged results. Although the memory required for the FCET is still exponentially large, through limiting the size of maximal itemsets and the size of clustering to a reasonable memory requirement, we do save a large amount of storage spaces, especially in dense databases  7.    Conclusions  Through several comprehensive experiments, we found that the FCET and IFECT can save a larger amount of storage spaces than Apriori, MaxEclat, and CHARM in both SPARSE databases and DENSE databases. Since the FCET stores fewer elements for a long pattern, when matched with GMFI/GMAR algorithm, it also revealed efficient execution time than BASIC and CUMULATE in mining generalized association rules The time complexity to find the maximal itemsets is O\(log 2 n\ where n is the total number of maximal itemsets For a long pattern, we used a partition tree to count the SUB_TID of itemsets, and then got their merged results Although the memory required for the FCET is still exponentially large, through limiting the size of maximal itemsets and the size of clustering to a reasonable memory requirement, we do save a large amount of storage spaces especially in dense databases 8. References  1 g ra w a l, T  Im ieli n s k i an d A  S w a m i M in i n g  association rules between sets of items in large databases,î Proc. ACM International Conference on Management of Data \(1993\, pp. 207-216 2 g ra w a l an d R. Srik an t F a s t alg o rit h m s f o r  mining association rules,î Proc. 20th International Conference on Very Large Data Bases \(1994\ pp.487499  J i aW e i Han  J i an  P e i an d YiW en Yi n   Mi n i n g frequent patterns without candidate generation,î Proc ACM International Conference on Management of Data 2000\p. 1-12 4 J  S   Pa r k  M  S  C h en  an d P S  Y u     A n ef f e c t iv e hash-based algorithm for mining association rules,î Proc ACM International Conference on Management of Data 1995\p.175-186 5 A  Sa va se r e  E  O m i e c i ns ki  a n d S N a va t h e    A n  efficient algorithm for mining association rules in large databases,î  Proc. 21st International Conference on Very Large Data Bases \(1995\ pp.432-443 6 Y i n-F u H u a ng a n d  Chi e h M i ng W u   M i n i n g  generalized association rules using pruning techniques Proc. IEEE International Conference on Data Mining 2002\p.227-234 7 a k i a n d C.J  Hsia o   E ff icie n t al g o rith m s f o r  mining closed itemsets and their lattice structure, î  IEEE Transactions on Knowledge and Data Engineering, vol 17, no. 4, April \(2005\p. 462-478   Bu rdick  M. C a li m l i m  an d J. Geh r k e  M AF I A a maximal frequent itemset algorithm for transactional databases,î Proc. 17th International Conference on Data Engineering, \(2001\p.443-452  a k i S  P a rth a s a rat h y   M. Ogih ara, a n d W. Li   New algorithms for fast discovery of association rules Proc. 3rd ACM International Conference on Knowledge Discovery in Databases and Data Mining, \(1997\pp 283-286 10 M  J  Za ki a n d K  G o ud a  Fa s t ve r t i c a l  mi ni ng usi n g  diffsets,î Proc. 9th ACM International Conference on Knowledge Discovery and Data Mining, Aug. \(2003   Mam o u l i s D  W. C h e u ng an d W. L i a n    Similarity search in sets and categorical data using the signature tree,î Proc. 19th International Conference on Data Engineering, \(2003 12 R. Srik a n t a n d R Ag ra w a l   M i n i n g g e n e ralized  association rules,î Proc. 21st International Conference on Very Large Data Bases, \(1995\.407-419    
571 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


