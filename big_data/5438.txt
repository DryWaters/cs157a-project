A New Temporal Measure for Interesting Frequent Itemset Mining Asem Omari Assistant Professor Department of Computer Science Jerash Private University Jerash, Jordan Asem_omari@yahoo.com   Abstract Frequent itemset mining assists the data miner in searching for strongly associated items \(and transactions large transaction databases. Since the number of frequent itemsets is usually extremely large and unmanageable for a human user, methods for mining interesting rules have been proposed to define meaningful and summarized representations of them. Furthermore, many measures have been proposed in the literature to determine the interestingness of the rule. In this paper, we introduce a new temporal measure for interesting frequent itemset mining. This measure is based on the idea that interesting frequent itemsets are mainly covered by many recent transactions. This measure reduces the cost of searching for frequent itemsets by minimizing the search interval. Furthermore, this measure can be used to improve the search strategy implemented by the Apriori algorithm Keywords-Association Rule Mining, Frequent itemset mining, temporal data mining, temporal interestingness measure I. INTRODUCTION Data mining is the process of finding interesting information from large databases. The authors in [11] list the most challenging problems in data mining research. One of those problems is mining sequence data and time series data. A fundamental problem for mining association rules is mining frequent itemsets. In a market basket transaction dataset frequent itemset mining is the process of searching for itemsets that a group of customers likely to purchase in a given visit to the store. In a previous approach [3; 4], the authors studied the behavior of frequent itemsets with respect to time. This is done through mining for frequent itemsets in different time periods Top level goal of temporal analysis is to filter for interesting 


frequent itemsets. We argue that interesting frequent itemsets are mainly covered by many recent transactions For example, in a transactions database in an electronics store, before two years a 500MB MP3 player was frequently sold, nowadays most of the people buy MP3 players with at least 2GB capacity. From that, when mining the whole transactions dataset, we may get an interesting rule saying that a percentage of customers are interested in buying 500MB MP3 players. This rule is not anymore interesting because people are not anymore interested in such small capacity MP3 players. So we can say that this rule is too old to be interesting. On the other hand, a rule that says that a good percentage of people are interested in buying 2GB MP3 players is still interesting as it can be taken into account in the decision making process.  This rule after a period of time is also going to be non-interesting This paper is structured as follows. In section 2, we introduce the problem of association rule mining. Then, in section 3, we discuss some related work. We present our new measure to mine for interesting frequent itemsets in section 4. Experimental work is introduced in section 5 Then, in section 6, we discuss some possible application fields of our approach. Finally, in section 7, we summarize the main contributions of our paper and identify possible future directions to extend this work  II. ASSOCIATION RULE MINING Association Rule Mining captures the relationships between different items. An association rule finds the notion that two different items occur together in a transactions database. An Association Rule has the form X Y , where X and Y are sets of items and have no items in common. Given a database of transactions D  where each transaction T D is a set of items, X Y denotes that whenever a transaction T contains X then there is a probability that it contains Y too. The rule X Y holds in the transactions set T with confidence c if c % of transactions in T that contain X also contain Y. The rule has support s in T if s of all transactions in T contains both X and Y. Association rule mining is finding all association rules that have support and confidence values greater than or equal a user-specified 


minimum support \(minsup minconf _____________________________________ 978-1-4244-5265-1/10/$26.00 2010 IEEE two steps: finding frequent itemsets and then extracting interesting rules from the frequent itemsets A. The Apriori Algorithm The Apriori algorithm is one of the efficient association rule mining algorithms [8].  It generates all frequent itemsets called also large itemsets, by making multiple passes over the transactions database D. The algorithm makes a single pass over the data to determine the support of each item which results in the set of 1-itemsets. Next, the algorithm will iteratively generate new candidate k-itemsets using the frequent \(k  1 additional pass over the data set is made to count the support of the candidates.  After counting their supports, the algorithm eliminates all candidate itemsets whose support count is less than minsup. The algorithm eliminates some of the candidate k-itemsets using the support-based pruning strategy. If any subset of the k-itemset I is not frequent then I is pruned. The algorithm terminates when there is no new frequent itemsets generated. Association rules are generated by generating all non-empty subsets of each frequent itemset and outputs its rule if its confidence is >= minconf. That rules are called interesting association rules  III. RELATED WORK Temporal data mining [6] is the process of mining databases that have time granularities. The goal is to find patterns which repeat over time and their periods, or finding the repeating patterns of a time-related database as well as the interval which corresponds to the pattern period [13]. The work presented in [5] studies the problem of association rules that exist in certain time intervals and thus display regular cyclic variations over time. Since their introduction in 1993 in [1], hundreds of new scalable methods have been proposed to solve the problem of mining frequent itemsets Typically, while a too high support threshold leads to generate only commonsense patterns \(or none the itemsets having a low support, or dealing with high correlated data, may generate an explosive number of results 


often hard to examine for a user In order to solve this problem, several methods were proposed to compress \(or summarize itemsets, i.e. to find a concise representation [10] of the whole collection of patterns. In [7], the authors present an algorithm that utilizes the transaction time interval of individual customers and that of all customers to find out when and who will buy products, and what items of products they will buy. The work in [9] presents a visualization technique to allow the user to visually analyze the rules and their changing behaviors over a number of time periods. This enables the user to find interesting rules and understand their behavior easily Beside support and confidence, the authors in [12] present a survey of the available interestingness measurements in the literature such as Conviction, Lift, Piatetsky-Shapiro Coverage, Correlation, and Odds ratio. All those measures are defined using probabilities. As far as we know, there is no previous work that takes the temporal aspects into account in the process of searching for interesting frequent itemsets IV. TEMPORAL FREQUENT ITEMSET MINING Supposing that we have a dataset that represents the log of some website. The web log consists of a set of records R that represent user requests. Every request consists of a client IP address, the requested URL, and the time stamp t when the URL is requested. Most websites implement user tracking in some way, mostly using cookies. This makes it possible to map the client IP address to some user ID, which is unique for a user. This avoids difficulties with users whose IP address changes over time. A session is created by taking consecutive requests of a particular user with small waiting time   between the consecutive requests and a maximum session time length . We consider the time stamp of the first request of a session to be the time stamp of the session. As a result, we get the session which is a sequence of visited URLs with a time stamp. The set of all sessions may include multiple sessions of the same users The set of unique URLs of all sessions in forms the set of items waitt maxt 


S S I used for frequent itemset mining in the next step The sessions in S are transformed to transactions by removing the duplicate URLs which may appear several times during a single session. So, we have a one to one mapping from the set of sessions S  to the set of transactionsT The set FI include all frequent itemsets f I , which are covered by more than minsup transactions. The frequent itemsets can be found from T by the Apriori algorithm Those frequent itemsets represent URLs that are frequently visited together by the users. In order to include the temporal information into our analysis, for each frequent itemset f FI  the set of covering transactions   Since a transaction t T  corresponds to exactly one session in s S , we can trace back the time stamp of s once we know t . So, we construct a time series for each frequent set f FI as follows. First, all covering transactions set of time stamps of the session corresponding to all  is ordered and forms the time series fs corresponding to the frequent itemset f . Note that the length of  fs  equals the support of f f FI length c At median  fs  minsup The set of time series for all frequent itemsets is denoted byTS Definition: A frequent itemset A  is interesting if where  is the current time,  is the ordered time series of the frequent itemset ct As A , and  is a positive number that represents a time threshold. That means finding all frequent itemsets from the transactions within time interval \f 


ct t c with respect to minimum support gives a super set of all interesting frequent itemsets Figure 1 shows an example of interesting and noninteresting frequent itemsets. The frequent itemsets {b, d and {a, b, c} are not interesting because the median is outside the time interval\f c  ctt It is also clear that the frequent itemset {a, b, c} is too old to be interesting. On the other hand, {a, e, f} is interesting because the median of its time series is within the determined interval.  A necessary condition for interesting frequent itemset A is    pQuantile 1 2 with p A   minsup sup Where is the  of the time series of set A The  of the ordered set of values is a number puantileQ x such that a proportion p of the set are less than or equal to x  Figure 1. Interesting and non-interesting frequent itemsets For example, the  \(called also the quartile an ordered set of values is a value 0.25Quantile x  such that 25% of the values of the set are below that value, and 75% of the values are  above that value. The Quantile  \(same as the median values are less than or equal to it and half are greater than or 


equal to it. The usage of the above condition means finding all frequent itemsets from the transactions within the time interval 0.5 f ct t minsup c 2 with respect to minimum support minsup , gives a super set of all frequent itemsets.  This is because when the frequent itemset A is interesting, then and because the median is the central values of the frequent itemset, then we need at least half of the interval\f c ct t  If the itemset is interesting, then it should definitely be frequent. Therefore, supporting transactions should be in the interval 2minsup f ct ct . This condition can be used either as a preprocessing step to search for frequent itemsets within the determined interval, or as an extension to the Apriori algorithm to prune non-interesting frequent itemsets. Figure 2 is an example of applying this condition In this example, we have the minsup = 10. When using this condition, the search for frequent itemsets will be within the interval \f c ct t with minsup = 10/2 = 5 From that, we get a super set of all interesting frequent itemsets. The 3-candidate frequent itemset with minsup = 5 is {a,b,e}. On the other hand, the 3-candidate frequent itemsets with minsup =10 are {a,b,c} and {a,b,e Candidates for interesting frequent itemsets are \(interesting frequent itemsets are underlined a}, {b}, {c}, {e a,b}, {a,c}, {a,e}, {b,e 


a,b,e On the other hand, all frequent itemsets are a}, {b}, {c}, {e a,b}, {a,c}, {a,e}, {b,c}, {b,e a,b,c}, {a,b,e  Figure 2. An example of using the necessary condition Using this method, we reduced the cost of searching for frequent itemsets. This method can be used to improve the search strategy implemented by the Apriori algorithm. A time series fs  of a frequent itemset f  is an ordered sequence of time stamps of the covering transactions. This set will be helpful in finding out what kinds of changes are occurring in which time periods. This can give an indication of the behavior of users with respect to time. We can find why some pages are frequently visited and why others not Through that, we can get a better view about our pages, and also about the users visiting our website. We can predict the future behavior of users depending on the periodical behavior of the users that we have already extracted V. EXPERIMENTAL WORK For the experiments we used a dataset that contains the preprocessed and   filtered sessionized data for the main DePaul CTI Web server \(http://www.cs.depaul.edu data is   based on a random sample of users visiting this site for a 2 week period during April of 2002. Each session begins with a line of the form SESSION #n \(USER_ID = k Where n in the session number and k is the USER_ID There may be multiple   consecutive sessions corresponding to the same USER_ID \(repeated users with a dashed line. Within a given session, each line corresponds to one page view access. Each line in a session is a tab delimited sequence of 3 fields: time stamp, page view accessed, and the referrer. The time stamp represents the number of seconds relative to January 1, 2002 In order to illustrate what we made in our experimental work, let us take a sample of three sessions that could be found in the web log file SESSION #1 \(USER_ID = 11 9374553 /news/default.asp /news 9374590 /people/search.asp?sort=pt /news/default.asp 


9374610 /people/facultyinfo.asp people/search.asp?sort=pt 9374685 /news/default.asp /people/facultyinfo.asp 9374720 /courses/ /news/default.asp  SESSION #2 \(USER_ID = 22 9185108 /admissions/ /programs 9185138 /news/default.asp /news/default.asp  SESSION #3 \(USER_ID = 33 9226945 /people/search.asp?sort=pt /people 9226975 /people/facultyinfo.asp people/search.asp?sort=pt 9227072 /advising/ /courses 9227098 /people/search.asp?sort=pt /news/default.asp  After preprocessing which includes the removal of redundant URLs within every session, considering the time stamp of the first request in every session as the time stamp of the session, and the removal of the referrer we get SESSION #1 \(USER_ID = 11 9374553 /people/search.asp?sort=pt people/facultyinfo.asp news/default.asp courses  SESSION #2 \(USER_ID = 22 9185108 /admissions news/default.asp  SESSION #3 \(USER_ID = 33 9226945 /people/search.asp?sort=pt people/facultyinfo.asp advising news/default.asp Then we build the set of items which consists of the set of unique URLs of all sessions Set of items I = {/people/search.asp?sort=pt people/facultyinfo.asp news/default.asp, /courses/, /admissions/, /advising We then used the Apriori algorithm to mine for frequent itemsets. We set the minimum support to be 20% which is 


equivalent to minimum support count equal to 2 Candidate 1-itemset news/default.asp \(support count =3 people/search.asp?sort=pt\( support count =2 people/facultyinfo.asp\(support count =2 Candidate 2-Itemset news/default.asp, /people/search.asp?sort=pt}\(support count = 2 news/default.asp, /people/facultyinfo.asp}\(support count 2 people/search.asp?sort=pt, /people/facultyinfo.asp support count= 2 Candidate 3-Itemset news/default.asp, /people/search.asp?sort=pt people/facultyinfo.asp\(support count= 2 and because the available dataset was not suitable to run our experiments, the main goal of our future work is to find a real life representative dataset that fits our experimental needs or use one of the available dataset generators to generate a suitable one Candidate 3-Itemset represents the frequent itemset f . The set of covering transactions for the frequent itemset f  are in session #1 and session #3 which have the time  stamps 9374553 and 9226945 respectively. The time stamps are then ordered to get the time series corresponding to the frequent itemset f :  REFERENCES 1] Agrawal, R., Imieliski, T., and Swami, A. Mining association rules between sets of items in large databases. In: SIGMOD NY, USA, ACM Press \(1993 207-216 S = {9226945, 9374553 The experimental results we have got were not representative enough to reflect the applicability of our approach. Such bad results were expected because the dataset we used was not the one we need. On one hand because the time interval of the transactions is too small \(2 weeks other hand, the transactions represent user sessions in a university website, and usually the transactions recorded in the web log file of university websites are not more than course registration, search for an assignment, etc.  The dataset we need to run an effective experiment should have 


a long time interval for example a dataset that represents customers transactions in a retail website within two years As far as we know, such dataset does not exist especially when we talk about time stamped datasets. This lack of such datasets amplifies the need to develop time stamped transactional datasets generators which is discussed in [2 2]  Asem Omari, Regina Langer, and Stefan Conrad. TARtool: A Temporal Dataset Generator for Market Basket Analysis. In Proceedings of the 4th International Conference on Advanced Data Mining and Applications \(ADMA 2008 2008. Springer Lecture Notes in Artificial Intelligence \(LNAI August 2008 3] Asem Omari. Data Mining for Improved Website Design and Enhanced Marketing.  In Yukio Ohsawa and Katsutoshi Yada editors, Data Mining for Design and Marketing,  volume 5 of Chapman Hall/CRC Data Mining and Knowledge Discovery Series, chapter  6. Chapman Hall/CRC, First edition, November 2008 4] Asem Omari, Alexander Hinneburg, and Stefan Conrad Temporal Frequent Itemset Mining. In Proceedings of the Knowledge Discovery, Data Mining and Machine  Learning workshop \(KDML 2007 5]  B. Ozden, S. Ramaswamy and A. Silberschatz: Cyclic Association Rules. In: ICDE  98: Proceedings of the  Fourteenth International Conference on Data Engineering,  Washington, DC USA, IEEE Computer Society \(1998 412421 6] C. Antunes and A. Oliveira: Temporal Data Mining: An Overview. In: Proceedings of the Workshop on Temporal Data Mining, of Knowledge Discovery and Data Mining KDD01, San Francisco, USA \(2001 VI. APPLICATION FIELDS This method can be applied in different fields. One application field is in search engine log files for example to find out the most frequently searched keywords in the last time period. Another application field is in web usage mining for example to find out the most visited web pages in the last 3 months in some website. It can also be applied to a transaction dataset in a physical store or business to find out the most frequently bought products or used services in the last time period. Any other problem that needs to study the behavior of some items with respect to time can be a good application field 


7] Ding-An Chiang, Shao-Lun Lee, Chun-Chi Chen, and MingHua Wang. Mining Interval Sequential Patterns. International Journal of Intelligent Systems, 20\(3 359373 8] J. Han and M. Kamber: Data Mining Concepts and Techniques Morgan Kaufmann Publishers, San Francisco \(2001 9] Kaidi Zhao and Bing Liu. Visual Analysis of the Behavior of Discovered Rules. In Workshop Notes in ACM SIGKDD-2001 Workshop on Visual Data Mining, San  Francisco, CA, 2001 10] Mannila, H., Toivonen, H.: Multiple uses of frequent sets and condensed representations. In: KDD, Portland, USA \(1996 pp\(189-194 11] Q. Yang and X. Wu: 10 Challenging Problems in Data Mining Research. Volume 5, World Scientific Publishing Company International Journal of Information Technology and Decision Making \(2006 597604  VII. SUMMARY AND FUTURE WORK In this paper, we presented a new measure to mine for interesting frequent itemsets. This measure is based on the idea that interesting frequent itemsets are mainly covered by many recent transactions.  This measure reduces the cost of searching for frequent itemsets by minimizing the search interval. Furthermore, it can be used to improve the search strategy implemented by the Apriori algorithm 12] Sheikh, L.M. Tanveer, B. Hamdani, M.A. Interesting Measures for Mining Association Rules. In proceedings of the 8th International Multi-topic Conference INMIC, 2004. pp \(641 644 13] W. Lin, M. A. Orgun and G. Williams: An Overview of Temporal Data Mining. In:  Proceedings of the 1st Australian Data Mining Workshop, Canberra, Australia,   University of Technology, Sydney \(2002 8390  


processed, until all attributes in A have been exhausted and we get the final fuzzy version of the dataset E. At the end, all attributes would have categorical values for each record in the fuzzy dataset E. Thus, by applying the aforementioned pre-processing, given any dataset D with initial crisp attributes \(set A fuzzy records. And each of these is further iteratively converted to generate more fuzzy records, until each crisp attribute has been taken into account and we get our final fuzzy dataset E  VI. COUNTING IN FUZZY ASSOCIATION RULES Crisp ARM algorithms calculate support of itemsets in various ways Record-by-record counting; as in Apriori Counting using tidlists; for example, ARMOR Tree-style counting; as in FPGrowth In this section, we describe how counting is done in various fuzzy ARM algorithms using membership functions and how our pre-processing technique can be used to generate fuzzy datasets which can be used by any fuzzy ARM algorithm Table I. t-norms in Fuzzy sets  t-norm TM\(x, y x, y TP\(x, y TW\(x, y x + y ? 1, 0  A. Counting in Fuzzy Apriori The first pass of Apriori counts item occurrences to determine the large 1-itemsets. Any subsequent pass k consists of two phases. First, the large itemsets found in the k-1 the kth pass. Next, the database is scanned and the supports of candidate itemsets are counted. In any pass k, each record is selected in a sequential manner and the supports for the candidate itemsets, occurring in that particular record, are increased by one. Thus, the counting in Apriori is done in a record-by-record manner Fuzzy Apriori is a modified version of the original Apriori algorithm, and can deal with fuzzy records. Fuzzy 


Apriori counts the support of each itemset in a manner similar to the counting in Apriori; the only difference is that it calculates sum of the membership function corresponding to each record where the itemset exists. Thus the support for any itemset is its sum of membership functions over the whole fuzzy dataset. This calculation is done with the help of a suitable t-norm \(see Table I We generated the fuzzy dataset required for Fuzzy Apriori using our pre-processing methodology. The crisp dataset \(FAM95 sections 4 and 5, and the resultant fuzzy dataset was used as input to the Fuzzy Apriori algorithm. More details of how FPrep was used for pre-processing before Fuzzy Apriori can be found in [21] \(though the pre-processing methodology used in [21] is not explicitly names as FPrep  B. Counting in Fuzzy ARMOR Each record in the dataset is marked by a unique number called transaction id \(tid order. A tid-list of an itemset X is an ordered list of TIDs of transactions that contain X. ARMOR is based on the Oracle algorithm and is totally different from Apriori in that it calculates the support of each itemset by creating its tidlist and counting the number of tids in the tidlist. The count of any itemset is equal to the length of its corresponding tidlist The tidlist of an itemset can be obtained as the intersection of the tidlists of its mother and father \(for example, ABC is generated by intersecting AB and BC started off using the tidlists of frequent 1-itemsets In a similar manner, Fuzzy ARMOR also creates the tidlist for each itemset by intersecting the tidlists of its mother and father itemsets. And for each tid in the tidlist, it calculates the membership function  \(again using a suitable t-norm support for an itemset is thus the sum of the membership functions associated with each tid in its tidlist We have also developed an initial implementation of Fuzzy ARMOR [21]. This algorithm uses the same fuzzy dataset as input as that was used for Fuzzy Apriori. There is no change, whatsoever, made to this fuzzy dataset after it was generated initially \(for Fuzzy Apriori processing technique. Even though Fuzzy Apriori and Fuzzy 


ARMOR operate in different ways and process data differently, the fuzzy dataset created using our preprocessing technique can be used as input for both the algorithms. This is because the fuzzy dataset is generated in a standard manner of fuzzy data representation \(as described in section 5 ARM algorithm. More details of how FPrep was used for pre-processing before Fuzzy ARMOR can be found in [21 C. Counting in Fuzzy FPGrowth FPGrowth uses a compact data structure, called frequent pattern tree \(FP-tree structure and stores quantitative information about frequent patterns. Only frequent length-1 items will have nodes in the tree, and the tree nodes are arranged in such a way that more frequently occurring nodes will have better chances of sharing nodes than less frequently occurring ones. FP-treebased pattern fragment growth mining starts from a frequent length-1 pattern, examines only its conditional pattern base constructs its \(conditional recursively with such a tree. The support of any itemset can be calculated from its conditional pattern base and from the nodes in the FP-tree, which correspond to the itemset Fuzzy FPGrowth also works in a similar manner by constructing an FP-tree, with each node in the tree corresponding to a 1-itemset. In addition, each node also has a fuzzy membership function  corresponding to the 1itemset contained in the node. The membership function for each 1-itemset is retrieved from the fuzzy dataset while constructing the FP-tree, and the sum of all membership function values for the 1-itemset is its support. The support for a k-itemset \(where k ? 2 corresponding to the itemset by using a suitable t-norm  VII. RELATED WORK 3] describes the current status and future prospects of applying fuzzy logic to data mining applications. In [4] and 5], the authors discuss two facets of fuzzy association rules namely positive rules and negative rules, and describe briefly a few rule quality measures, especially for negative rules. The authors in [6] take this discussion further by describing in detail the theoretical basis for various rule quality measures using various t-norms, t-conorms, S 


implicators, and residual implicators. [8] and [9] illustrate quality measures for fuzzy association rules and also show how fuzzy partitioning can be done using various t-norms, tconorms, and implicators. The authors in [8] go a step further and do a detailed analysis of how implicators can be used in the context of fuzzy association rules Last, [7] and [10] take diametrically opposing stands on the usefulness of fuzzy association rules. The authors of [7 do a data-driven empirical study of fuzzy association rules and conclude that fuzzy association rules, after all, might not be as useful as thought to be. But the authors of [10 defended the usefulness of fuzzy association rules, by doing more experimental work, and then corroborating their stand through the successful results of their empirical research In addition to the fuzzy clustering based methodology briefly mentioned in [7], [19] and [20] describe methodologies for generating fuzzy partitions \(using nonfuzzy hard clustering original dataset into a fuzzy form. [19] uses k-Medoids CLARANS  CURE for the same. The hard clusterings so generated are then used to derive the fuzzy partitions. In such cases, where hard clustering is used, typically the middle point of each fuzzy partition is taken as reference \(membership  = 1 with respect to which the memberships for other values belonging to that partitions are calculated. [22] goes even a step further, and uses Multi-Objective Genetic Algorithms in the process for finding fuzzy partitions. Such methodologies which use hard clustering, or non-fuzzy methods are one way to obtain fuzzy versions of original datasets before any fuzzy ARM can ensue. But, with FPrep we use only fuzzy methods, fuzzy clustering to be more specific, in order to ensure consistency, and to have the notion of fuzziness maintained throughout. The main motive behind doing so is to ensure that any processing preceding the actual fuzzy ARM process, also involves fuzzy methods. Thus, the whole end-to-end process, right from the moment the processing of original crisp dataset starts till the time the final frequent itemsets are generated, involves only fuzzy methods and is holistic in nature  VIII. EXPERIMENTAL RESULTS 


The experimental results of FPrep as compared to other such non-fuzzy methods, on the basis of various parameters, are described below  A. Results from First Dataset We have tested FPrep against the automated methods for generating fuzzy partitions proposed in [19], [20]. These use hard clustering algorithms CLARANS \(k-Medoids CURE respectively. The main tangible metric to compare our approach to the ones proposed in [19], [20] is the time taken for execution. And, the dataset used for doing so is the USCensus1990raw dataset http://kdd.ics.uci.edu/databases/census1990 has around 2.5M transactions, and we have used nine attributes present in the dataset, of which five are quantitative and the rest are binary. The attributes, with their respective number of unique values, on which the evaluation was done, are as follows Age - 91 unique values Hours  100 unique values Income1  55089 unique values Income2  13707 unique values Income3  4949 unique values  Using each of the three methodologies being evaluated three fuzzy partitions were generated for each of these attributes. The results are illustrated in fig. 7, which has the y-axis in log10 form for ease of perusal.  The same are also available in Table II. As far as speed is concerned, for attributes having very low number of unique values \(~ 100 there is no big difference among the three methods. FPrep and CURE perform five times better than CLARANS for the attributes Age and Hours, both of which have around 100 unique values. But, the real differences become apparent for higher number of unique values. For attribute Income3, with 4949 unique values, we see that FPrep is nearly nine times faster than CURE, and nearly 2672 times faster than CLARANS, and for attribute Income2, with 13707 unique values, it is 27 times faster than CURE, and 13005 times faster than CLARANS. For attribute Income1, having 55089 unique values, FPrep is 46 times faster than CURE. No comparison was done with CLARANS for this attribute, as 


the time needed for execution exceeded 100000 seconds Thus, from this analysis we see that FPrep, which uses FCM clustering, clearly outperforms the CLARANS and CURE based methods on the basis of speed. The execution times for CLARANS and CURE mentioned in fig. 7 and Table II do not include the time required to create fuzzy sets, and calculate the membership value  for each numerical data point in every fuzzy set for the numerical attribute under consideration. These times also do not take into account the time required to transform crisp numerical attributes to fuzzy attributes, and derive the fuzzy dataset from the original crisp dataset The fuzzy partitions generated for each of the five numerical attributes for the USCensus1990raw dataset are shown in Table III. Coincidentally, generating three fuzzy partitions for each numerical attribute seemed a perfect fit In addition to the superior speeds achieved by FPrep, as illustrated in fig. 7 and Table II, Table III indicates the semantics and the quality of the fuzzy partitions generated by FPrep. Moreover, the number of frequent itemsets generated by a fuzzy ARM algorithm \(like fuzzy ARMOR and fuzzy Apriori minimum support threshold, is illustrated in fig. 8   Fig. 7. Algorithm, numerical attribute comparison based on speed \(log10 seconds   Fig. 8. Number of frequent itemsets for various minimum support values  B. Results from Second Dataset We have also applied FPrep on the FAM95 dataset http://www.stat.ucla.edu/data/fpp transactions. Of the 23 attributes in the dataset, we have used the first 18, of which six are quantitative and the rest are binary. For each of the six quantitative attributes, we have generated fuzzy partitions using FPrep. A thorough analysis with respect to execution times, has already been performed on the USCensus1990raw dataset \(which is manifolds bigger in size than the FAM95 dataset both on the basis of number of transactions and number of unique values for numerical 


attributes dataset has been done solely to provide further evidence of the quality and semantics of the fuzzy partitions generated by FPrep. The details of the same are in Table IV. In this case, the number of fuzzy partitions is different for different numerical attributes. Thus, the number and type of fuzzy partitions to be generated is totally dependent on the attribute under consideration. A graphical representation of the fuzzy partitions generated for the attribute Age has already been provided in fig. 5, and clearly shows the Gaussian nature of the fuzzy partitions. The nature and shapes of fuzzy partitions for the rest of the attributes are also similar. Last, the number of frequent itemsets generated for different minimum support values is illustrated in fig. 8  C. Analysis of Results With FPrep, we can analyze and zero in on the number and type of partitions required based on the semantics of the numerical attributes, which the methods detailed in [19 20] do not necessarily facilitate. Then, FPrep, backed by FCM clustering, takes care of the creating the fuzzy partitions, especially assigning membership values for each numerical data point in each fuzzy partition. In section 8.A we have already shown that FPrep is nearly 9 to 44 times faster than the CURE-based method, and 2672 to 13005 times faster than the CLARANS-based method. FPrep is not only much faster than other related methods, but also generates very high quality fuzzy partitions \(Table III and IV much user-intervention. We have created a standard way of representing any fuzzy dataset \(converted from any type of crisp dataset efficacy of the same is corroborated by the successful implementation of Fuzzy Apriori and Fuzzy ARMOR on the fuzzy dataset \(converted from crisp version of FAM95 dataset an initial implementation of Fuzzy ARMOR, are very encouraging. FPrep, when used in conjunction with these fuzzy ARM algorithms, generates a pretty good number of high-quality frequent itemsets \(fig. 8 frequent itemsets generated for a particular minimum support is same, irrespective of the fuzzy ARM algorithm 


used IX. CONCLUSIONS In this paper we have highlighted our methodology, called FPrep, for ARM in a fuzzy scenario. FPrep is meant for seamlessly and holistically transforming a crisp dataset into a fuzzy dataset such that it can drive a subsequent fuzzy ARM process. It does not rely on any non-fuzzy techniques and is thus more straightforward, fast, and consistent. It facilitates user-friendly automation of fuzzy dataset 1 0 1 2 3 4 5 Age - 91 Hours - 100 Income3 4949 Income2 13707 Income1 55089 Ti m e lo g1 0 se co nd s Numerical Attribute - Number of Unique Values FCM CURE CLARANS 0 500 1000 1500 2000 2500 


3000 0.075 0.1 0.15 0.2 0.25 0.3 0.35 0.4 N o o f F re qu en t I te m se ts Minimum Support USCensus1990 FAM95 generation through FCM, and subsequent steps in preprocessing with very less manual intervention and as simple and straightforward manner as possible. This methodology involves two distinct steps, namely creation of appropriate fuzzy partitions using fuzzy clustering and creation of fuzzy records, using these partitions, to get the fuzzy dataset from the original crisp dataset FPrep has been compared with other such techniques, and has been found to better on the basis of speed. We also illustrate its efficacy on the basis of quality of fuzzy partitions generated and the number of itemsets mined by a fuzzy ARM algorithm which is preceded by FPrep. This preprocessing technique provides us with a standard method of fuzzy data \(record that it is useful for any kind of fuzzy ARM algorithm irrespective of how the algorithm works. Furthermore, this pre-processing methodology has been adequately tested with two disparate fuzzy ARM algorithms, Fuzzy Apriori and Fuzzy ARMOR, and would also work fine with other fuzzy ARM algorithm REFERENCES 1] Zadeh, L. A.: Fuzzy sets. Inf. Control, 8, 338358 \(1965 2] Chen G., Yan P., Kerre E.E.: Computationally Efficient Mining for Fuzzy Implication-Based Association Rules in Quantitative Databases. International Journal of General Systems, 33, 163-182 


2004 3] Hllermeier, E.: Fuzzy methods in machine learning and data mining Status and prospects. Fuzzy Sets and Systems. 156, 387-406 \(2005 4] De Cock, M., Cornelis, C., Kerre, E.E.: Fuzzy Association Rules: A Two-Sided Approach. In: FIP, pp 385-390 \(2003 5] Yan, P., Chen, G., Cornelis, C., De Cock, M., Kerre, E.E.: Mining Positive and Negative Fuzzy Association Rules. In: KES, pp. 270-276 Springer \(2004 6] De Cock, M., Cornelis, C., Kerre, E.E.: Elicitation of fuzzy association rules from positive and negative examples. Fuzzy Sets and Systems, 149, 7385 \(2005 7] Verlinde, H., De Cock, M., Boute, R.: Fuzzy Versus Quantitative Association Rules: A Fair Data-Driven Comparison. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics 36, 679-683 \(2006 8] Dubois, D., Hllermeier, E., Prade, H.: A systematic approach to the assessment of fuzzy association rules. Data Min. Knowl. Discov., 13 167-192 \(2006 9] Dubois, D., Hllermeier, E., Prade, H.: A Note on Quality Measures for Fuzzy Association Rules. In: IFSA, pp. 346-353. Springer-Verlag 2003 10] Hllermeier, E., Yi, Y.: In Defense of Fuzzy Association Analysis IEEE Transactions on Systems, Man, and Cybernetics - Part B Cybernetics, 37, 1039-1043 \(2007 11] Agrawal, R., Imielinski, T., Swami, A.N.: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Record, 22, 207216 \(1993 12]  Agrawal, R., Srikant, R.: Fast Algorithms for Mining Association Rules. In: VLDB, pp. 487-99. Morgan Kaufmann \(1994 13] Han, J., Pei, J., Yin, Y.: Mining Frequent Patterns without Candidate Generation. In: SIGMOD Conference, pp. 1-12. ACM Press \(2000 14] Han, J., Pei, J., Yin, Y., Mao, R.: Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach. Data Mining and Knowledge Discovery, 8, 5387 \(2004 15] Pudi V., Haritsa J.R.: ARMOR: Association Rule Mining based on Oracle. CEUR Workshop Proceedings, 90 \(2003 16] Dunn, J. C.: A Fuzzy Relative of the ISODATA Process and its Use in Detecting Compact, Well Separated Clusters. J. Cyber., 3, 32-57 1974 17] Hoppner, F., Klawonn, F., Kruse, R, Runkler, T.: Fuzzy Cluster Analysis, Methods for Classification, Data Analysis and Image Recognition. Wiley, New York \(1999 


18] Bezdek J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Kluwer Academic Publishers, Norwell, MA \(1981 19] Fu, A.W., Wong, M.H., Sze, S.C., Wong, W.C., Wong, W.L., Yu W.K. Finding Fuzzy Sets for the Mining of Fuzzy Association Rules for Numerical Attributes. In: IDEAL, pp. 263-268. Springer \(1998 20] Kaya, M., Alhajj, R., Polat, F., Arslan, A: Efficient Automated Mining of Fuzzy Association Rules. In: DEXA, pp. 133-142. Springer \(2002 21] Mangalampalli, A., Pudi, V. Fuzzy Association Rule Mining Algorithm for Fast and Efficient Performance on Very Large Datasets In FUZZ-IEEE, pp. 1163-1168. IEEE \(2009 22] Kaya, M., Alhajj. Integrating Multi-Objective Genetic Algorithms into Clustering for Fuzzy Association Rules Mining. In ICDM, pp. 431434. IEEE \(2004  Table II. Algorithm, numerical attribute comparison based on speed \(seconds  Algorithm Age - 91 Hours - 100 Income3 - 4949 Income2 - 13707 Income1 - 55089 FCM 0.27 0.3 3.13 6.28 79.4 CURE 0.25 0.25 28.67 163.19 3614.13 CLARANS 1.3 1.34 8363.53 78030.3 Table III. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions Age Old Middle Aged Young Hours More Average Less Income1 High Medium Low Income2 High Medium Low Income3 High Medium Low  Table IV. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions AGE Very old Around 25 Around 50 Around 65 Around 35 HOURS Very High Zero Around 40 Around 25 INCHEAD Very less Around 30K Around 50K Around 100K INCFAM Around 60K Around 152K Around 96K Around 31K Around 8K TAXINC Around 50K Around 95K Around 20K Very less FTAX Around 15K Very less Around 6K Very high Around 33K  


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


