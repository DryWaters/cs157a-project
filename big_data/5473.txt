Privacy-Preserving Distributed Association Rule Mining Based on the Secret Sharing Technique Xinjing Ge  LiYan   Jianming Zhu   Wenjie Shi   School of Information Central University of Finance and Economics Beijing 100081 China xjge@163 com  School of Information Renmin University of China Beijing 100872 China  School of Information Central University of Finance and Economics Beijing 100081 China  School of Foreign Studies Central University of Finance and Economics Beijing 100081 China Abstract Due to privacy law and motivation of business interests privacy is concerned and has become an important issue in data mining This paper explores the issue of privacypreserving distributed association rule mining in vertically partitioned data among multiple parties and proposes a collusion-resistant algorithm of distributed association rule mining based on the Shamirês secret sharing technique which prevents effectively the collusive behaviors and conducts the computations across the parties without compromising their data privacy Additionally analyses with regard to the security efìciency and correctness of the proposed algorithm are given Keywords association rule mining privacy security secret sharing I I NTRODUCTION Privacy-preserving data mining PPDM has been studied extensively and applied widely In this paper we are particularly interested in the mining of association rule in a scenario where the data is vertically distributed among different parties To mine the association rule these parties need to collaborate with each other so that they can jointly mine the data and produce results that interest all of them However Due to privacy law and motivation of business interests the parties may not trust any other parties and do not want to reveal her own portion of the data although realizing that combining their data has some mutual beneìt Secure Multi-party Computation SMC technique introduced by is an important approach to achie v e pri v a c y preserving data mining and privacy preserving protocols or algorithms are designed based on cryptographic techniques and secure multi-party computations for PPDM 15  Ho we v e r  most of these pri v a c y preserving data mining algorithms were based on the assumption of a semi-honest environment where the participating parties always follow the This work was supported by the Key Project of Chinese Ministry of Education No109016 the Beijing Natural Science Foundation No 4082028 the National Natural Science Foundation of China No 60970143 and the Central University of Finance and Economics 2009 Graduate Research Innovation Fund protocol and never try to collude As mentioned in previous works on privacy-preserving distributed mining it is rational for distributed data mining that the participants are assumed to be semi-honest but the collusion of parties for gain additional beneìts can not be avoided Reference sho wed the participants have a tendency to collude by using a game theoretic framework and conducting equilibrium-analyses of existing PPDM algorithms So there has been a tendency for privacy preserving data mining to devise the collusion resistant protocols or algorithms and recent research have addressed this 5 8 9 Reference 4 rstly proposed an approach to conduct privacy preserving decision tree classiìcation based on the Shamirês secret sharing technique In this paper we provide a secure and efìcient distributed association rule mining algorithm based on the Shamirês secret sharing technique instead of decision tree classiìcation The major contributions of this paper are as follows 1 We present a new and efìcient solution to distributed association rule in vertically partitioned data 2 We provide a secure algorithm of the distributed association rule mining which can prevent effectively the collusion of parties by employing the Shamirês secret sharing technique and discuss the algorithm with regard to its security efìciency and correctness The rest of the paper is organized as follows Section 2 present an overview of the related work Subsequently Section 3 present the technical preliminaries We then present the deìnition of the distributed mining of association rule and the secure algorithm in Section 4 Finally we present the conclusion and future work in Section 5 II R ELATED W ORK In privacy-preserving data mining many methods were proposed to preserve the privacy of the data Data perturbation technique rst proposed in represents one common approach in privacy preserving data mining where the original private dataset is perturbed and the result is released for data analysis Data perturbation 345 


includes a wide variety of techniques as follows additive multiplicative matrix multiplicative k-anonymization microaggregation categorical data perturbation data swapping data shufîing T ypically  there e xists a trade-of f o f privacy/accuracy in data mining On one hand perturbation must not allow the original data records to be adequately recovered on the other hand it must allow patterns in the original data to be mined For example applied data perturbation to conduct association rule mining Secure Multi-party Computation SMC technique proposed in is an alternati v e approach to achie v e pri v a c y preserving data mining and has been proved that there is a secure multi-party computation solution for any polynomial function This approach though appealing in its generality and simplicity is highly impractical for large data sets Based on the idea of secure multiparty computation privacyoriented protocols were designed for the privacy-preserving collaborative data mining For example presented the component scalar product protocol for privacy-preserving association rule mining over vertically partitioned data in the case of two parties Reference proposed a homomorphic cryptographic approach to tackle collaborative association rule mining among multiple parties proposed a e f cient and practical protocol for privacy-preserving association rule mining based on identity-based cryptography which has an additional advantage that no public key certiìcate is needed Game theory now has been applied to privacy preserving data mining and is a relatively new area of research For example Reference gued that lar ge-scale multi-party PPDM can be thought of as a game where each participant tries to maximize its beneìt by optimally choosing the strategies during the entire PPDM process and proposed a cheap-talk based protocol to implement a punishment mechanism to offer a more robust process III T ECHNICAL P RELIMINARIES A Shamirês secret sharing technique Shamirês secret sharing allo ws a dealer D t o distribute a secret value among n peers such that the knowledge of any peers is required to reconstruct the secret The method is described in Algorithm 1 Algorithm 1  Shamirês secret sharing algorithm Require Secret value v s  P  Set of parties P 1 P 2   P n to distribute the shares k  Number of shares required to reconstruct the secret 1 Select a random polynomial q  x  a k  1 x k  1    a 1 x 1  v s  where a k  1  0 q 0  v s  2 Choose n publicly known distinct random values x 1 x 2   x n such that x i  0  3 Compute the share of each peer p i  where share i  q  x i   4 for i 1 to n do 5 Send share i to peer p i  6 end for Shamirês method is theoretically secure in order to construct the secret value v s  at least k shares are required to determine the random polynomial q  x  of degree k  1  so the complete knowledge of up to k  1 peers does not reveal any information about the secret B The Deìnition of the Association rule mining The association rule mining can be deìned as follows let I   i 1 i 2   i m  be a set of items DB be a set of transactions where each transaction T is an itemset such that T  I  An itemset X with k items called k-itemset Given an itemset X  I  a transaction T contains X if and only if X  T  An association rule is an implication of the form X  Y where X  I,Y  I and X  Y    The rule X  Y has support s in the transaction database DB if s  of transactions in DB contain X  Y  The association rule hold in the transaction database DB with conìdence c if c  of transactions in DB that contain X also contains Y  The problem of mining association rule is to nd all rules whose support and conìdence are higher than certain user speciìed minimum namely thresholds support and conìdence With this framework we consider mining Boolean association rule the absence or presence of an attribute is represented as 0 or 1  so transactions are strings of 0 and 1  IV P RIVACY P RESERVING D ISTRIBUTED M INING OF A SSOCIATION R ULE IN V ERTICALLY P ARTITIONED D ATA A Problem Deìnition In this paper we consider the distributed mining of association rule that is the mining environment is distributed Let us assume that a transaction database DB is vertical partitioned among n parties namely P 1 P 2   P n  where DB  DB 1  DB 2    DB n DB i  DB j    for  i j  n DB i resides at party P i 1  i  n   This means 1 all the data sets contain the same number of transactions let N denote the total number of transactions for each data set 2 The identities of the jth for j  1 N   transaction in all the data sets are the same  the assumptions can be achieved by pre-processing the data sets DB 1 DB 2   DB n  and such a pre-processing does not require one party to send her data set to other parties Now all parties want to conduct distributed association rule mining on the concatenation of their data sets in which no party is willing to disclose its raw data set to others because of the concern about their data privacy so we formulate the following privacy-preserving distributed association rule mining problem in vertically partitioned data Problem Deìnition Party P 1 P 2   P n have private data set DB 1 DB 2   DB n respectively and DB i  DB j    for  i j  n  The data set DB 1 DB 2   DB n forms a database DB  namely DB  DB 1  DB 2  DB n  which is actually the concatenation of DB 1 DB 2   DB n  Let N denote the total number of transactions for each data set the n parties want to conduct association rule mining on DB  DB 1  DB 2  DB n and to nd the association rules with support and conìdence being greater than the given thresholds 346 


During the mining of association rule we assume all parties follow the protocol and the object of the paper is to propose a protocol of distributed association rule mining in vertically partitioned data based on the Shamirês secret sharing  The protocol can pre v ent ef fecti v ely the collusive behaviors and conduct the computations across the parties without compromising their data privacy Simultaneously the security of the protocol refers to semantic security B Distributed Association rule mining Algorithm In order to learn association rules one must compute conìdence and support of a given candidate itemset Given the values of the attributes are 1 or 0  to nd out whether a particular itemset is frequent we only count the number of records denote c.count  where the values for all the attributes in the itemset are 1 if c.count 012 Ns  then the candidate itemset is the frequent itemset The following is the algorithm to nd frequent itemsets 1 L 1   large 1-itemset  2 for  k 2 L k  1     k  3 C k  apriori-gen  L k  1  4 for all candidates c  C k do begin 5 if all the attributes in c are entirely the same party that party independently compute c.count 6 else collaboratively compute c.count We will show how to compute it in Section 4.3 7 end 8 L k  L k  c  c.count 012 minsup  9 end 10 Return L   k L k  In step 3 the function C k  apriori-gen  L k  1  can generate the set of candidate itemsets C k  which is discussed in Given the counts and frequent itemsets we can compute all association rules with support 012 minsup  In the procedure of association rule mining step 1  3  6 and 8 require sharing information In step 3 and 8  we use merely attribute names in step 1  to compute large 1-itemset  each party elects her own attributes that contribute to large 1itemset  where only one attribute forms a 1-itemset  there is no computation involving attributes of other parties Therefore data disclosure across parties is not necessary At the same time since the nal result L   k L k is known to all parties step 1  3 and 8 reveal no extra information to either party However to compute c.count in step 6  a computation accessing attributes belonging to different parties is necessary How to conduct these computations across parties without compromising each partyês data privacy is the challenge we are faced with If all the attributes belong to the same party then c.count  which refers to the frequency counts for candidates can be computed by this party If the attributes belong to different parties they may rst construct vectors for their own attributes For example for some candidate itemset party P i have attributes a 1 a 2   a p  then party P i can construct vector A i  the jth element denote A ij   p k 1 a k in vector A i  Subsequently they apply our secure protocol to obtain c.count  which will be discussed in Section 4  3  C Privacy-Preserving Algorithm to Collaboratively Compute c.count The fact that the distributed parties jointly compute c.count without revealing their raw data to each other presents a great challenge In this section we show how to privately compute c.count based on Shamirês secret sharing algorithm and the privacy preserving summation of secrets in the case of multiple parties without revealing the secret values to others Without loss of generality assuming party P i has a private vector A i i  1 n   and we use A ij to denote the jth element in vector A i  the value of A ij is the attribute value of the P i in the jth transaction of the database Given that the absence or presence of an attribute is represented as 0 or 1 the value of A ij is equal to 0 or 1  for example A i 1  0  1  1    0 T  Firstly parties P 1 P 2   P n determine the degree k  k  n  1 of a polynomial respectively that is going to be used in Shamirês secret sharing They also agree on m 012 n publicly known distinct random values X  x 1 x 2   x m  For the sake of simplicity and without loss of generality we will assume k  n  1 and m  n  So for arbitrary transaction j in database DB  each party P i chooses a random polynomial q i  x  of degree k and q i 0  A ij  then computes the share of each party P t including itself as share  A ij P t  q i  x t  t  1  2   n  At the end every party P i sends out the shares such that share  A ij P t  is sent to party P t  Secondly each party P i gets the shares q 1  x i  q 2  x i    q n  x i  from other parties and adds up all the shares S  x i  q 1  x i  q 2  x i     q n  x i   then sends this intermediate result to all parties Thirdly each party P i can compute the sum of secret values A ij using the results received during the second phase Note that each party P i uses a random polynomial with degree k  n  1 and constant term A ij  and therefore the sum of all these polynomials results in another polynomial S  x  q 1  x  q 2  x     q n  x  of degree k with the constant term equal to the sum of all secret values A ij  The results computed during the second computation phase correspond to the values of S  x  at points x 1 x 2   x n So each party P i will have line equations                        b n  1 x n  1 1  b n  2 x n  2 1    b 1 x 1  n  i 1 A ij  S  x 1  b n  1 x n  1 2  b n  2 x n  2 2    b 1 x 2  n  i 1 A ij  S  x 2   b n  1 x n  1 n  b n  2 x n  2 n    b 1 x n  n  i 1 A ij  S  x n  1 Through solving the above equations we can determine all the coefìcients of S  x   Since the constant term of S  x  is equal to  n i 1 A ij  each party will learn the sum of all secret values but not the individual ones 34 7 


Fourthly party P i acquires the sum  n i 1 A ij of secret values if the  n i 1 A ij  n  it means the values of P 1 P 2   P n in transaction j of database are all 1  so the transaction j supports the association rule then let m j 1  otherwise m j 0  At last continuing for all transactions each party P i acquires m j 1 or 0  and computes c.count   N j 1 m j  So assuming all parties follow the algorithm and do the computations honestly we summarize the whole process in Algorithm 3 Algorithm 3 Privacy-Preserving Algorithm to Collaboratively Compute c.count Require P  Set of parties P 1 P 2   P n  A ij  Secret value of P i  X  A set of n publicly known random values x 1 x 2   x n  k  Degree of the random polynomial k  n  1  1 For each transaction j 1 to N do 2 For each party P i   i 1  2   n  do 3 Select a random polynomial q i  x  a n  1 x n  1    a 1 x 1  A ij 4 Compute the share of each party P t  where share  A ij P t  q i  x t  5 For t 1 to n do 6 Send share  A ij P t  to party P t 7 Receive the shares share  A ij P t  from every party P t  8 Compute S  x i  q 1  x i  q 2  x i     q n  x i  9 For t 1 to n do 10 Send S  x i  to party P t 11 Receive the results S  x i  from every party P t  12 Solve the set of equations to nd the sum  n i 1 A ij of secret values 13 If the  n i 1 A ij  n  let m j 1  otherwise m j 0  14 Each party computes c.count   N j 1 m j  Example  Assume that there are 4 parties P 1 P 2 P 3 P 4 with secret values A 1 j 1 A 2 j 0 A 3 j 1 A 4 j 1 respectively for arbitrary transaction j in database DB  and that they want to decide whether the transaction j supports the association rule without revealing their values to each other At rst they decide on a polynomial degree k 3 and m 4 publicly known distinct random values X 2  3  5  6  Each party P i then chooses a random polynomial q i  x  of degree k 3 whose constant term is the secret value A ij  P 1 picks q 1  x  x 3 3 x 2 2 x 1 and computes the shares for other parties such that the share of party P t  share  A 1 j P t  q 1  x t   where x t is the t th element of X  Thus the shares computed by P 1 are as follows share  A 1 j P 1  q 1 2  25  share  A 1 j P 2  q 1 3  61 share  A 1 j P 3  q 1 5  211  share  A 1 j P 4  q 1 6  337 Similarly other parties P 2 P 3 P 4 pick random polynomials q 2  x  x 3  x 2 6 x 0  q 3  x  x 3 2 x 2 0 x 1  q 4  x  x 3  x 2 3 x 1 and compute the shares for other parties  The following gives the shares of all other parties q 2 2  24 q 2 3  54 q 2 5  180 q 2 6  288 q 3 2  19 q 3 3  49 q 3 5  181 q 3 6  295 q 4 2  27 q 4 3  73 q 4 5  291 q 4 6  487 During the second phase each party adds up all the shares received from other parties and then sends this result to all other parties That is party P i computes S  x i  q 1  x i  q 2  x i  q 3  x i  q 4  x i  and sends to all other parties At the third computation phase each party P i will have the 4 values of polynomial S  x  q 1  x  q 2  x  q 3  x  q 4  x  b 3 x 3  b 2 x 2  b 1 x  b at X 2  3  5  6 with the constant term equal to the sum of all secret values So each party P i can get liner equations        8 b 3 4 b 2 2 b 1  b 95 27 b 3 9 b 2 3 b 1  b  237 125 b 3 25 b 2 5 b 1  b  863 216 b 3 36 b 2 6 b 1  b  1407 2 and get b 3 through solving the above liner equations so  4 i 1 A ij  b 3  Because the  4 i 1 A ij  b 3  it means the values of P 1 P 2 P 3 P 4 in transaction j of database DB are not all 1 so the transaction j does not support the association rule then let m j 0  D Analysis of the Privacy-Preserving Algorithm to Collaboratively Compute c.count 1 Correctness Analysis Assuming party P i has a private vector A i  so for arbitrary transaction j in database DB  party P i has a private value A ij  according algorithm 3 the sum  n i 1 A ij of secret values is the constant term of the sum polynomial S  x  q 1  x  q 2  x     q n  x  so we need to solve the liner equations 1  noting there are n unknown coefìcients and n equations and that the determinant of coefìcient D           x n  1 1 x n  2 1  x 1 1 x n  1 2 x n  2 2  x 2 1              x n  1 n x n  2 n  x n 1          3 is the Vandermonde determinant When D   1  j  i  n  x i  x j   0  that is x i   x j  the equations has a unique solution and each party P i can solve the set of equations and determine the value of  n i 1 A ij  however it cannot determine the secret values of the other parties since the individual polynomial coefìcients selected by other parties 348 


are not known to P i If A 1 j A 2 j   A nj are all equal to 1  that is  n i 1 A ij  n  this means the transaction has the whole attributes and supports the association rule we let m j 1  Otherwise if some attributes of A 1 j A 2 j   A nj are not equal to 1 that is  n i 1 A ij   n  this means the transaction has not the whole attributes and does not support the association rules we let m j 0  To compute the number of transactions which support the association rule we only count the number of m j 1  then c.count   N j 1 m j  so the algorithm 3 can compute c.count correctly under the condition of all parties doing the computations honestly during the mining of association rules 2 Complexity Analysis Assuming there are N transactions and n parties the communication cost is from step 5 6 and step 9 10 of algorithm 3 so the communication cost of algorithm 3 is 2 Nn  n  1  The following contribute to the computational cost of each transaction 1 the generation of the random polynomial q i  x  i 1  2   n from step 3 2 the total number of n 2 computations on the share of each party from step 4 3 the total number of n  n  1 additions from step 8 4 the computational cost of solving the set of equations to nd the sum  n i 1 A ij of secret values from step 12 5 computational cost of letting m j 1 or 0 according the sum  n i 1 A ij from step 13 Compared to the other techniques such as commutative encryption and secure multi-party computations the algorithm proposed by our paper does not have to add to computation and communication cost but our algorithm can prevent effectively the collusive behaviors and conduct the computations across the parties without compromising their data privacy so our algorithm is efìcient and practical 3 Security Analysis Proposition 1 Algorithm 3 is semantically secure for the netw ork attack ers Proof  The network attackers listening to the network trafìc of the parties cannot learn any useful information such as the private values or the sum of those values expect for all the shares and the intermediate values However these values cannot be used to determine the coefìcients of the sum polynomial and each partyês secretly random polynomial without knowing random values x 1 x 2   x n by which the intermediate results are calculated and the shares and the intermediate values can not be used to infer whether the transaction supports the association rule Proposition 2 Algorithm 3 is semantically secure and can prevent effectively the collusive behaviors from the collaborative parties under the condition of the number of the collusive parties t<n  1  Proof  Firstly algorithm 3 is semantically secure for the collaborative parties Compared with the network attackers the collaborative parties know random values x 1 x 2   x n  At algorithm 3 P i computes the value of its polynomial at n points as shares and then keeps one of these shares for itself and sends the remaining n  1 shares to other parties so if the collaborative party gets all other parties shares and intermediate values through listening to the network trafìc of the parties except for the value of the corresponding sum polynomial at n different points he can get for example the value of that party P  i s secretly random polynomial at n  1 different point x 1   x i  1 x i 1   x n  And because the degree of each party P  i s secretly random polynomial is k  n  1 and each party P  i s secretly random polynomial has n unkown coefìcients in order to compute the coefìcients of the corresponding party P  i s secretly random polynomial and get the party P  i s private value the value at n different points is needed so party P  i s private value can not be achieved Secondly algorithm 3 can prevent effectively the collusive behaviors for the collaborative parties under the condition of the number of the collusive parties t<n  1  If there are n  1 parties collude for example P 2   P n  they can get the value s  x 1  of sum polynomial at x 1 and q 1  x 2    q 1  x n  which is the value of party P  1 s secretly random polynomial at n  1 different points x 2   x n  Noting that S  x i  q 1  x i  q 2  x i    q n  x i   they can compute q 1  x i  S  x i   q 2  x i   q n  x i   then can conclude the party P  1 s private value through solving the following liner equations        a n  1 x n  1 1  a n  2 x n  2 1    a 1 x 1  A 1 j  q  x 1  a n  1 x n  1 2  a n  2 x n  2 2    a 1 x 2  A 1 j  q  x 2   a n  1 x n  1 n  a n  2 x n  2 n    a 1 x n  A 1 j  q  x n  4 In a word the algorithm is semantically secure and can prevent effectively the collusive behaviors V C ONCLUSION The issue of privacy preserving distributed association rule mining is concerned by this paper In particular based on the Shamirês secret sharing technique the study focuses on how multiple parties can conduct distributed association rule mining in their joint database of the vertically partitioned private data in which we propose a collusion-resistant algorithm which computes whether one transaction supports the association rule in order to nd the frequent itemsets Indepth analysis of correctness complexity and security about our algorithm is given and the result shows that the algorithm is efìcient practical and secure However our algorithm should be used with care in practice in that the algorithm is designed for the model in which all parties follow the algorithm and conduct computation honestly where collusive behaviors will be deterred effectively due to the collusion resistant features In some practical scenarios where participants may deviate from the algorithm therefore techniques like zero-knowledge proofs should be employed to prevent malicious behavior yet this generic approach might be rather inefìcient and add considerable overhead to each step of the algorithm So nding an efìcient algorithm designed for adversary model in privacy-preserving distributed association rule mining is our future research focus R EFERENCES  R Agra w al T  Imielinski and A Sw ami Mining association rules between sets of items in large databases Proceedings of ACM SIGMOD Conference on Management of Data pp 207Ö216 1993 34 9 


 R Agra w a l and R Srikant F ast algorithms for mining association rules  Proceedings of 20th International Conference on Very Large Data Bases pp.12Ö15 1994  R Agra w a l and R Srikant Pri v a c y-preserving data mining  Proceedings of the ACM SIGMOD Conference on Management of Data pp.439Ö450 2000  F  Emekci O D Sahin D Agra w a l and A El Abbadi Pri v a c y preserving decision tree learning over multiple parties Data and Knowledge Engineering vol.63 pp.348Ö361 2007  X J Ge and J M Zhu Collusion-resistant protocol for pri v a c y preserving distributed association rule mining Proceedings of the 11th International Conference on Information and Communications Security pp.359Ö369 2009  O Goldreich Secure multi-party computation  http://www wisdom weizmann.ac.il 1998  O Goldreich F oundations of cryptography   Class notes T echnion University Spring 1989  W  Jiang C Clifton and M Kantarcioglu T ransforming semi-honest protocols to ensure accountability Data and Knowledge Engineering vol.65 pp.57Ö74 2008  H Kar gupta K Das and K Liu Multi-party  pri v a c y-preserving distributed data mining using a game theoretic framework PKDD vol.4702 of Lecture Notes in Computer Science Springer pp.523Ö531 2007  K Lin C Giannella and H Kar gupta  A surv e y of attack techniques on privacy-preserving data perturbation Privacy-Preserving Data Mining Models and Algorithms pp.275Ö299 2008  Y  Lindell and B Pinkas Pri v a c y preserving data mining  Journal of Cryptology vol.15\(3 pp.177Ö206 2002  P  P aillier  Public-k e y cryptosystems based on composite de gree residuosity classes Advances in Cryptography-EUROCRYPTê99 pp.223Ö238 Prague Czech Republic 1999  S J Rizvi and J R Haritsa Pri v a c y-preserving association rule mining Proceedings of 28th International Conference on Very Large Data Bases pp 682-693 2002  A Shamir  Ho w t o share a secret  Communications of the A CM vol.22\(11 pp.612Ö613 1979  J V aidya and C W  Clifton Pri v a c y preserving association rule mining in vertically partitioned data Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 2002  F  W u  J  Q  Liu and Sh Zhong  A n e f cient protocol for pri v ate and accurate mining of support counts Pattern Recognition Letters vol.30\(1 pp.80Ö86 2009  A C Y ao Protocols for secure computations  Proceedings of the 23rd Annual IEEE Symposium on Foundations of Computer Science,IEEE Press New York 1982  J Zhan S Matwin and L W  Chang Pri v a c y-preserving collaborati v e association rule mining Journal of Network and Computer Applications vol.30\(3 pp.1216Ö1227 August 2007 350 


modify the related items as elements pop-up from the queue constantly. Finally, traverse the interested items table, output the frequency of each item, get the frequent itemsets according to the given minimum support degree and excavate the association rule which has the minimum confidence from the frequent itemsets. Child1 and Child2 are as mentioned above. The full mining flaw chart is shown in Fig. I B. c # code archive the algorithm 1 public long Count\(int length long sum = 1 for \(int j = O;j < length;j sum *=2 return sum public string[] GetSubSet\(string set 424 int length = set.Length List<string> subsets = new List<string>O for \(long i = 1; i < Count\(length string s for \(int j = 0; j < set. Length; j long k = i & CountG if \(k != 0  subsets.Add\(s return subsets.ToArray 2 public string Intresting\(string sl, string s2 interested items in the record char[] delimiter = sI.ToCharArray string result for \(int i = 0; i < delimiter. Length; i bool b = s2.Contains\(delimiter[i].ToString if \(b if \(!result.Contains\(delimiter[i].ToString result += delimiter[i return result 3 interested items public static bool Check\(string sl, string s2 bool b = false  


for \(int i = 0; i < sl.Length; i b = s2.Contains\(sl [i].ToString if \(b return !b 4 public static void Sub\(int count, List<Str> str for \(int i = 0; i < str.Count; i if \(str[i].1 < count str[i].Istrue = false  5 public static int GetCount\(string s, List<Str> str bool b = false int count = 0 for \(int i = 0; i < str.Count; i if \(\(s = str[i].SI  b = true count = str[i].I break  if \(!b return 0 else return count V. PERFORMANCE EVALUATION OF IMPROVED ALGORITHM The algorithm introduced in this paper can greatly improve the mining efficiency. Firstly, the improved scan the database only once; secondly, it reduces a lot of unnecessary operations. The Improved mining algorithm has lower time complexity as it has explicit object. We test the traditional algorithm and the improved one based on the same situation that all the data are from Table I, and lead to the results showed on Fig. 2 and Fig. 3. Fig. 2 and Fig. 3 showed that improved algorithm improved a lot on time. When it comes to the case that the transaction database is huge and the bigger interested items, the improved algorithm runs much better. Figure 3 showed operate times to complete the excavation when the interest items are different \(A, AB ABC 3.5 


2.5 1------1.5 0.5 The number of scanning the database C 1 mp"ovcd a 1 go.' i thrn Trad it i ona 1 81 'orithrns Figure 2. The comparison chart of scanning the database 50 45 40 35 30 25 20 15 10 5 o 2 The number of interested items Figuire 3. The comparison chart of different interested Item VI. CONCLUSION We reach a conclusion after an analysis on Apriori algorithm: \(l 2 reduce unnecessary duplication of effort. Experiments showed that the algorithm in this paper can get the target to improve the efficiency. The algorithm also has its disadvantages, as we need larger memory to shore the flag bit when interested items increase ACKNOWLEDGMENT This work is supported by National Natural Science Foundation of China \(Grant No. 60773008 of Aerospace Information Security and Trust Computing of Ministry of Education., National Science Foundation for Post-doctoral Scientists of China, Natural Science Foundation ofHubei Province REFERENCES 1] T.Uno, M.Kiyomi,and H.Arimura. Lcm ver.2:Efficient mlmng algorithms for frequent/closed/maximal itemsets. In B. Goethals,M. J Zaki,and R.Bayardo,editors, Proceedings of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations \(FIMI'04 


volume 126 of CEUR Workshop Proceedings, Brighton, UK, 2004 2] B.R'acz.nonordtp:An FP-growth variation without rebuilding the FPtree .In B.Goethals, M.J.Zaki,and R.Bayardo, editors, Proceedings ofthe IEEE ICDM Workshop on Frequent Itemset Mining Implementations \(FIMI'04 Proceedings, Brighton, UK, 2004 3] Adhikari A, Rao P R.A framework for synthesizing arbitrary Boolean expressions induced by frequent itemsets[C] .3rd Indian Intemat Conference on Artificial Intelligence, 2007 4] M.Kuramochi and G.Karypis.Frequent subgraph discovery. In Proceedings of the first IEEE International Conference on Data Mining, pages313-320, 2001 5] M. Elhadef, K. Abrougui, S. Das, Nayak, A parallel Probabilistic system- level fault diagnosis approach for large multiprocessor systems, Processing Letters, 1\(2006 6] L.Schmidt-Thieme.Algorithmic features of eclat.ln B.Goethals,M J.Zaki,and R.Bayardo, editors,Proceedings of the IEEEICDM Workshop on Frequent Itemset Mining Implementations FIMI'04 Brighton,UK, 2004 7] Eberhart R C,Dobbins R W.Neural Network PC Tools:A Practical Guide[M].Academic Press, 1 990 8] Wu X,Wu Y,Wang Y,et al.Privacy-aware market basket data set generation:A feasible approach for inverse TABLE III. THE INTERESTED TABLE ID Has Itemses Flag Frequenc Pass mark The number The marked The Children accesse y of item sets number Address d int int string Bit[] int Sign int int *p 0 0 ABC 0000000 0 null 3 6 1, 2, 3 1 0 AB 0000000 0 null 2 5 4, 5 2 0 AC 0000000 0 null 2 4 4, 6 3 0 BC 0000000 0 null 2 3 5, 6 4 0 A 0000000 0 null 1 3 null 5 0 B 0000000 0 null 1 2 null 6 0 C 0000000 0 null 1 1 null 425 End Table is empty >----N-----i< in access line is 1 Figure 1 The improved algorithm flowchart 426 


A. Constrained Tree Edit Distance CTED [10] is based upon the idea of transforming one hierarchy into another via certain local editing operations The distance between two hierarchies is the minimum total number of operations needed for transforming the one into the other. Three modification operations are considered: A node i is deleted by transmitting its children to its parent and removing i from the hierarchy. Inserting is the complement of deleting: Inserting i as a child of j makes i the parent of some subset of the children of j. Changing a node means changing its label. In order to make computation in polynomial time possible, the constraint that disjoint subtrees have to be mapped to disjoint subtrees is included in the definition [10], [16 The original definition does not include any normalization For two hierarchies with q labels each, 2q is the highest possible CTED  corresponding to deleting all labels of the first hierarchy and inserting all labels of the second hierarchy We normalize the CTED by 12q which results in distances in 0, 1 B. Taxonomic Overlap TO [11] compares the nodes of two hierarchies according to how many ancestors and descendants they share. We describe a version adapted to our experiments. Define semantic cotopy as SC\(i,H i j Given two hierarchies H1 and H2, the corresponding overlap for i ? L is defined as O\(i,H1, H2 SC\(i,H1 i,H2 SC\(i,H1 i,H2 Note that this is a symmetric expression in H1 and H2 with values in [0, 1]. The TO of H1 and H2 is the average of all overlaps TO\(H1, H2 1 q  i?L O\(i,H1, H2 For two identical hierarchies, TO is 1, while it is 0 if H1 and H2 are totally unrelated. Since the two other hierarchy 


proximity measures used in the experiments are distances with smaller values indicating higher similarity TO? := 1? TO to simplify comparisons C. LCA-Path Tree Distance Since in a hierarchy a parent is a more general concept than any of its children, the locations of the same label i in two hierarchies H1 and H2 can be compared by how far one has to travel up the chain of ancestors in both hierarchies until a common ancestor is found. For a label i ? L the depth dH\(i 0 dH\(i pH\(i a1, . . . , av the ancestors of i in H1, such that a1 = pH1\(i a2 = pH1\(a1 i common ancestor of i in H1 and H2, lca\(i,H1, H2 first ancestor of i in H1 which is also an ancestor of i in H2 j? := min  j | j ? {1, . . . , v} such that aj ? AH2\(i  lca\(i,H1, H2 Since 0 is an ancestor of all labels in all hierarchies, the LCA always exists. Note that usually lca\(i,H1, H2 lca\(i,H2, H1 We use the LCA to measure how far apart the positions of a label in both hierarchies are: If a label i has the same parent in both hierarchies it is regarded as being in the same position, otherwise the distances of i to its LCA in both hierarchies are used to measure how far apart both locations are. The cost for i is defined as d\(i,H1, H2 2 k=1 dHk\(i lca\(i,H1, H2 D\(i,H1, H2  0, if pH1\(i i d\(i,H1, H2 8 This can be seen as the length of the path from i to lca\(i,H1, H2 i,H1, H2 As D is not symmetric, the complete LCA-path distance 


between H1 and H2 is the mean value D\(H1, H2 1 W  i?L 1 2  D\(i,H1, H2 i,H2, H1  0 1 2 3 a 0 2 1 3 b Fig. 3. Sample hierarchies where W is a normalization factor: The worst case for a misplaced label i is that lca\(i,H1, H2 8 i,H1, H2 i i holds accordingly for D\(i,H2, H1 i?L dH1 \(i i H1, H2  D. Comparison All three hierarchy proximity measures presented above reflect different concepts of tree similarity. It is thus not surprising that CTED and TO*, although they perform well in the domains they were designed for, may produce counterintuitive results when used in the field of HE. This is demonstrated by the hierarchies shown in Figure 3: Despite the fact that the relationships between the nodes in both hierarchies are completely different, their CTED and TO are 0.33 and 0.44, respectively. Their LCAPD, on the other hand, is 1.00. Thus only LCAPD marks both hierarchies with the highest possible distance Some hierarchies used in our experiments contain nodes with exactly one child \(single-child labels that TO* does not distinguish between such labels and their children, in the following sense: Given a hierarchy H and a single-child node i, we denote with H ? the hierarchy 


that is obtained from H by swapping i and its child. Then TO?\(H,H CTED or LCAPD V. EXPERIMENTS We tested the proposed data mining system with four multi-label classifiers: ML-FAM, ML-ARAM, ML-kNN and BoosTexter on three real-world datasets with increasing complexity from the text-mining field. In our experiments we compared extracted hierarchies to the original hierarchy by means of three hierarchy proximity measures: LCAPD CTED, and TO The experimental setup of ML-FAM and ML-ARAM had the following parameter values: choice parameter 0.0001; learning rates ?a,b = 1.0 for fast learning; vigilance parameters ?a = 0.9 and ?b = 1.0. The parameter t was chosen to be 0.05 for the 20 newsgroups dataset and 0.02 for the other datasets. The number of voters V was set to 9 Following [8], we used 10 nearest neighbors \(k = 10 Laplace smoothing \(s = 1 BoosTexter was trained using 500 boosting rounds as in 8] and the threshold for converting rankings into multi-labels was set to 0 [9 A. Multi-label Classification Performance Measures We used a large set of performance measures for evaluation of the MC experiments: First, two example-based measures for multi-label predictions: Accuracy \(A measure \(F  of the predicted labels are actually present while F-measure is the harmonic mean of precision and recall calculated on the per-instance basis. The larger the A and F values, the better the MC performance Then two label-based measures were calculated on the basis of binary counts for each label: the numbers of true positives, true negatives, false positives, and false negatives We used the micro-averaged version of F1 with the binary measures counted on the whole test set. The perfect performance corresponds to F1 = 1. Additionally, micro-averaged precision and recall were used for computing the Area Under a Precision-Recall Curve \(AUPRC  precision corresponds to the proportion of predicted labels in the test set that are correct and recall to the proportion of labels that are correctly predicted. AUPRC has been claimed 


to be a well-suited performance measure for MC tasks where the number of negative instances significantly exceeds the number of positive instances [6]. Another advantage of AUPRC is its global nature and independence of a certain threshold value. The closer the AUPRC value is to 1, the better the performance Since these measures are based on the comparison of multi-labels, they depend on a transformation from rankings to classes. As a contrast we also used four wellknown ranking-based performance measures: One-Error OE RL C cision \(AP  for all ranking-based performance measures except AP. OneError evaluates how many times the top-ranked label is not in the set of proper labels of the instance. Ranking Loss is defined as the average fraction of pairs of labels that are ordered incorrectly. Coverage evaluates how far we need, on average, to go down the list of labels in order to cover all the proper labels of the instance. Average Precision evaluates the average fraction of labels ranked above a particular label i ? mt which actually are in mt And finally, the special hierarchical loss function H-loss 2] were utilized. Following [3], normalized costs were calculated: ci := 1/|c\(p\(i i ? L i of all direct children of i. Hierarchical loss \(H-loss consider mistakes made in subtrees of incorrectly predicted labels and penalizes only the first mistake along the path from the root to a node. The smaller the H-loss value, the better the performance B. 20 Newsgroups Dataset We modified the popular single-label dataset 20 Newsgroups [18], [19] by considering eight additional labels corresponding to the intermediate levels of the hierarchy faith, recreation, recreation/sport, recreation/vehicles, politics, computer, computer/hardware, science. This dataset is a collection of almost 20,000 postings from 20 newsgroups sorted by date into training \(60 40 data were preprocessed by discarding all words appearing only in the test documents and all words found in the stop word list [20]. Afterwards, all but the 2%-most-frequent words were eliminated to reduce the dimensionality. Documents were represented using the well-known TF-IDF \(Term 


Frequency  Inverse Document Frequency scheme [19]. The TF-IDF weights were then normalized to the range of [0, 1]. Conversion to TF-IDF and normalization were performed separately for training and test data. This resulted in the 1,070-dimensional dataset with 11,256 training instances, 7,493 test ones and 28 labels To test the performance of two HE algorithms, we first extracted hierarchies from the True Test Multi-Labels \(TTML and calculated the corresponding proximity measures. Both algorithms successfully extracted the original hierarchy We studied the performance of the multi-label classifiers and their ability to infer the class hierarchies in the presence of only partly available hierarchical information. We performed a series of HE experiments with multi-labels having a decreasing number of inserted non-leaf labels describing the levels in the hierarchy. We randomly removed such labels from 20%, 30%, and 40% of the training instances leaving them single-labeled. The results for predicted test multilabels are shown in Table I where the bold face marks the best classifier, and the first column \(left result of HE by Voting and the second \(right Thresholding \(referred as GT Comparing classification performance, one can see that the ART-based networks are superior to both the other classifiers in terms of most performance measures and that ML-FAM slightly outperforms ML-ARAM. Taken together they win on at least 6 and at most 8 out of 9 evaluation measures. BoosTexter has the second best performance, but its predictive power degrades more quickly with the increase in the number of single-label instances. The poorest MC results were shown by ML-kNN, its performance decreased very fast with any reduction in the number of multi-labels For example, F1 decreased by 15% while removing 40% of labels instead of 30%. It is also interesting to note that when trained on the dataset with 40% removed labels, ML-FAM and ML-ARAM significantly outperformed ML-kNN trained on the original dataset with all labels The hierarchy proximity measures confirm the good quality of predictions produced by the ART-based networks: The hierarchies were correct extracted by both HE algorithms of Section II-B even with 40% removed labels. The predictions of ML-kNN were the worst: The Voting variant of the HE 


algorithm could not extract the correct hierarchy with 30 assigning five labels incorrectly to the root label. None of the HE algorithms could extract the correct hierarchy in the absence of 40% multi-labels. With 40% and Voting, the number of labels falsely assigned to the root was 13, while with GT it was only three. For BoosTexter, Voting assigned two labels wrongly to the root label in the experiment with TABLE I 20 NEWSGROUPS ALL, -20%, -30% AND -40% RESULTS Measure all 20% 30% 40%ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN BoosT A 0.635 0.638 0.429 0.549 0.613 0.633 0.383 0.456 0.596 0.619 0.322 0.412 0.563 0.591 0.255 0.387 F1 0.694 0.696 0.565 0.677 0.675 0.688 0.528 0.604 0.662 0.677 0.469 0.566 0.640 0.657 0.392 0.542 F 0.691 0.692 0.480 0.605 0.671 0.688 0.429 0.507 0.658 0.676 0.364 0.465 0.630 0.652 0.296 0.441 OE 0.221 0.220 0.336 0.222 0.259 0.236 0.387 0.275 0.273 0.259 0.415 0.301 0.301 0.291 0.434 0.316 RL 0.100 0.098 0.124 0.073 0.108 0.110 0.128 0.077 0.098 0.103 0.132 0.079 0.101 0.106 0.135 0.082 C 4.188 4.168 6.080 4.164 4.397 4.446 6.184 4.286 4.246 4.340 6.326 4.328 4.334 4.463 6.397 4.379 AP 0.789 0.790 0.677 0.778 0.774 0.782 0.657 0.758 0.769 0.774 0.645 0.747 0.759 0.764 0.638 0.740 AUPRC 0.775 0.772 0.618 0.749 0.743 0.727 0.581 0.691 0.733 0.722 0.555 0.671 0.715 0.708 0.535 0.660 H-loss 0.103 0.123 0.121 0.094 0.102 0.098 0.124 0.108 0.106 0.103 0.132 0.117 0.115 0.111 0.145 0.122 Wins 1 5 0 3 1 6 0 2 2 6 0 1 2 6 0 1 LCAPD 0 0 0 0 0 0 0 0 0 0 0.12 0 0.05 0 0 0 0.51 0.26 0.17 0 CTED 0 0 0 0 0 0 0 0 0 0 0.14 0 0.07 0 0 0 0.50 0.21 0.21 0 TO* 0 0 0 0 0 0 0 0 0 0 0.11 0 0.05 0 0 0 0.39 0.18 0.15 0 30% removed labels and and six labels in the experiment with 40% removed labels. GT resulted in zero distances in the both cases. Assigning more labels to the root creates more shallow and wider hierarchies \(trivial case as stated before The good hierarchy extraction with ART networks demonstrates the system robustness  even with strongly damaged data the system can rebuild the original hierarchy C. RCV1-v2 Dataset The next experiment was based on the tokenized version of 


the RCV1-v2 dataset introduced in [21]. Only the topics label set consisting of 103 labels arranged in a hierarchy of depth four is examined here. Documents of the original training set of 23,149 were converted to TF-IDF weights and normalized Afterwards the set was splitted in 15,000 randomly selected documents as training and the remaining as test samples In this case, the Voting variant of HE applied to the TTML resulted in the LCAPD, CTED and TO* values 0.12, 0.15 and 0.13, respectively. The corresponding values of the GT variant are 0.05, 0.07 and 0.05. The poor performance of the Voting method is due to the fact that for the TTML only very high threshold values succeed in removing enough noise The Voting results are thus dominated by bad hierarchies extracted for all but the highest thresholds The classification and HE results for this dataset are shown in Table II. ML-ARAM has better performance results on this data set in all points than ML-FAM except for RL being the best of all classifiers in terms of the multi-label performance measures. BoosTexter is the best in terms of all ranking measures For both HE algorithms the distances of BoosTexter are the best, those of ML-FAM second, followed ML-ARAM and ML-kNN. All three distance measures correlate. Interesting is also that for ML-kNN the distance values obtained by both HE methods are almost the same The hierarchy extracted by GT from the TTML has much lower distances values as compared with the hierarchies extracted by both methods from predicted multi-labels. This reflects a specific problem of HE, since only a small fraction of the incorrectly classified multi-labels can prevent building of a proper hierarchy. For example, 16.5% of misassigned labels in the extracted hierarchy are responsible for about 80% of LCAPD calculated from the predictions of MLARAM. This large part of the HE error is caused by only 4% of the test data. Under these circumstances the other distances behave analogically. Most labels were not assigned making them trivial edges, but six labels were assigned to a false branch. This can happen when labels have a strong correlation and in the step Hierarchy Construction of the basic algorithm the parent is not unique in the confidence matrix. BoosTexters results suffer less from this problem because it generally sets more labels for each test sample 


Both HE algorithms behaved similarly on the predictions of the ART networks. They constructed a deeper hierarchy than the original one and wrongly assigned the same 11 labels to the root node. The higher distances come from Voting assigning more labels to the wrong branch. For MLkNN both algorithms again create very similar hierarchy trees, both misassigned 28 labels to the root label. For BoosTexter it was seven with Voting and eight with GT Voting produced a deeper hierarchy here D. WIPO-alpha Dataset The WIPO-alpha dataset1 comprises patent documents collected by the World Intellectual Property Organization WIPO ments. Preprocessing was performed as follows: From each document, the title, abstract and claims texts were extracted stop words were removed using the list from [20] and words were stemmed using the Snowball stemmer [22]. All but the 1%-most-frequent stems were removed, the remaining stems were converted to TF-IDF weights and these were normalized to the range of [0, 1]. Again, TF-IDF conversion and normalization were done independently for the training and the test set. The original hierarchy consists, from top to bottom, of 8 sections, 120 classes, 630 subclasses and about 69,000 groups. In our experiment, only records from the sections A \(5802 training and 5169 test samples H \(5703 training and 5926 test samples 1http://www.wipo.int/classifications/ipc/en/ITsupport/Categorization dataset/wipo-alpha-readme.html August 2009 TABLE II RCV1-V2 RESULTS Measure ARAM FAM kNN BoosT A 0.748 0.731 0.651 0.695 F1 0.795 0.777 0.735 0.769 F 0.805 0.787 0.719 0.771 OE 0.077 0.089 0.104 0.063 RL 0.087 0.086 0.026 0.015 C 11.598 11.692 8.563 5.977 AP 0.868 0.860 0.839 0.873 AUPRC 0.830 0.794 0.807 0.838 H-loss 0.068 0.077 0.097 0.081 Wins 4 0 0 5 LCAPD 0.29 0.22 0.25 0.20 0.34 0.34 0.21 0.18 


CTED 0.32 0.23 0.28 0.22 0.38 0.37 0.24 0.20 TO* 0.27 0.18 0.22 0.17 0.31 0.30 0.21 0.17 document in the collection has one so-called main code and any number of secondary codes, where each code describes a group the document belongs to. Both main and secondary codes were used in the experiment, although codes pointing to groups outside of sections A and H were ignored. We also removed groups that did not contain at least 30 training and 30 test records \(and any documents that only belonged to such small groups 7,364 test records with 924 attributes each and a label set of size 131 In this case, the Voting variant of the HE algorithm applied to the TTML resulted in the LCAPD, CTED and TO* values of 0.13, 0.12 and 0, respectively. GT showed the same values. Remarkable are the TO* distances, which are equal to 0. This is due to the fact that the WIPO-alpha hierarchy contains 16 single-child labels that are not partitioned by the true multi-labels: whenever a single-child label j is contained in a multi-label, so is its child, and vice versa. It is therefore theoretically impossible to deduce from the multilabels which of them is the parent of the other. As a result the HE algorithms often choose the wrong parent, resulting in higher LCAPD and CTED values. TO*, as described above is invariant under such choices The results obtained on the WIPO-alpha dataset are shown in Table III. The classification performance of the ART-based networks on this dataset is slightly worse than that of BoosTexter. Mostly in the terms of OE, RL, C, AP, AUPRC, and H-loss measures BoosTexter is better because its rankings are better and it assigned more labels to each sample. But the ART networks have better HE results because their predicted labels are more consistent with the original hierarchy. MLkNN has the worst classification results and distance values again. The reason for the high relative difference between LCAPD as well as CTED and TO* obtained for the ART networks or BoosTexter as compared to the results of the other datasets is because most of the labels were assigned in the right branch but not exactly where they belong Both HE algorithms extracted the same hierarchy from the predictions of ML-ARAM and a very similar hierarchy for ML-FAM. About 5% labels were assigned wrongly to the 


root label in the hierarchies of the ART networks. For MLTABLE III WIPO-ALPHA\(AH Measure ARAM FAM kNN BoosT A 0.588 0.590 0.478 0.564 F1 0.694 0.691 0.614 0.693 F 0.682 0.682 0.593 0.679 OE 0.052 0.057 0.110 0.042 RL 0.135 0.136 0.056 0.025 C 25.135 25.269 22.380 11.742 AP 0.790 0.785 0.724 0.802 AUPRC 0.720 0.684 0.688 0.762 H-loss 0.090 0.093 0.149 0.079 Wins 1 2 0 6 LCAPD 0.16 0.16 0.17 0.17 0.32 0.38 0.21 0.21 CTED 0.18 0.18 0.19 0.19 0.38 0.53 0.27 0.27 TO* 0.05 0.05 0.07 0.07 0.24 0.32 0.08 0.08 kNN both HE methods wrongly assigned about the half of the labels and about 20% of total labels were assgined to the root label. Here, GT extracted a much worse hierarchy as shown by CTED being 0.15 higher for GT than for Voting For BoosTexter both HE methods built the same hierarchy and no label was wrongly assigned to the root. All extracted hierarchies were one level deeper than the original one Although Voting produced worse hierarchies than GT on two previous datasets, this time its distance values were comparable or even better. In comparison to Voting, GT has higher values for all distances on the multi-labels of MLkNN. Voting has the advantage of being a much simpler method and of being more dataset independent. Still the tree distances have the same ranking order for all classifiers for both HE methods VI. CONCLUSION In this paper we studied Hierarchical Multi-label Classification \(HMC tive was to derive hierarchical relationships between output classes from predicted multi-labels automatically. We have developed a data-mining-system based on two recently proposed multi-label extensions of the FAM and ARAM neural networks: ML-FAM and ML-ARAM as well as on a Hierarchy Extraction \(HE algorithm builds association rules from label co-occurrences 


and has two modifications. The presented approach is general enough to be used with any other multi-label classifier or HE algorithm. We have also developed a new tree distance measure for quantitative comparison of hierarchies In extensive experiments made on three text-mining realworld datasets, ML-FAM and ML-ARAM were compared against two state-of-the-art multi-label classifiers: ML-kNN and BoosTexter. The experimental results confirm that the proposed approach is suitable for extracting middle and large-scale class hierarchies from predicted multi-labels. In future work we intend to examine approaches for measuring the quality of hierarchical multi-label classifications REFERENCES 1] M. Ruiz and P. Srinivasan, Hierarchical text categorization using neural networks, Information Retrieval, vol. 5, no. 1, pp. 87118 2002 2] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni, Incremental algorithms for hierarchical classification, The Journal of Machine Learning Research, vol. 7, pp. 3154, 2006 3] , Hierarchical classification: combining Bayes with SVM, in Proceedings of the 23rd international conference on Machine learning ACM New York, NY, USA, 2006, pp. 177184 4] F. Wu, J. Zhang, and V. Honavar, Learning classifiers using hierarchically structured class taxonomies, in Proceedings of the 6th International Symposium on Abstraction, Reformulation And Approximation Springer, 2005, p. 313 5] L. Cai and T. Hofmann, Hierarchical document categorization with support vector machines, in Proceedings of the thirteenth ACM international conference on Information and knowledge management ACM New York, NY, USA, 2004, pp. 7887 6] C. Vens, J. Struyf, L. Schietgat, S. Dz?eroski, and H. Blockeel Decision trees for hierarchical multi-label classification, Machine Learning, vol. 73, no. 2, pp. 185214, 2008 7] E. P. Sapozhnikova, Art-based neural networks for multi-label classification, in IDA, ser. Lecture Notes in Computer Science, N. M Adams, C. Robardet, A. Siebes, and J.-F. Boulicaut, Eds., vol. 5772 Springer, 2009, pp. 167177 8] M. Zhang and Z. Zhou, ML-kNN: A lazy learning approach to multilabel learning, Pattern Recognition, vol. 40, no. 7, pp. 20382048 2007 9] R. Schapire and Y. Singer, BoosTexter: A boosting-based system for text categorization, Machine learning, vol. 39, no. 2, pp. 135168 


2000 10] K. Zhang, A constrained edit distance between unordered labeled trees, Algorithmica, vol. 15, no. 3, pp. 205222, 1996 11] A. Maedche and S. Staab, Measuring similarity between ontologies Lecture notes in computer science, pp. 251263, 2002 12] G. Carpenter, S. Martens, and O. Ogas, Self-organizing information fusion and hierarchical knowledge discovery: a new framework using ARTMAP neural networks, Neural Networks, vol. 18, no. 3, pp. 287 295, 2005 13] A.-H. Tan and H. Pan, Predictive neural networks for gene expression data analysis, Neural Networks, vol. 18, pp. 297306, April 2005 14] G. Carpenter, S. Grossberg, N. Markuzon, J. Reynolds, and D. Rosen Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps, IEEE Transactions on Neural Networks, vol. 3, no. 5, pp. 698713, 1992 15] Y. Freund and R. Schapire, A decision-theoretic generalization of online learning and an application to boosting, Journal of computer and system sciences, vol. 55, no. 1, pp. 119139, 1997 16] K. Zhang and T. Jiang, Some MAX SNP-hard results concerning unordered labeled trees, Information Processing Letters, vol. 49 no. 5, pp. 249254, 1994 17] G. Tsoumakas and I. Vlahavas, Random k-labelsets: An ensemble method for multilabel classification, Lecture Notes in Computer Science, vol. 4701, p. 406, 2007 18] K. Punera, S. Rajan, and J. Ghosh, Automatic construction of nary tree based taxonomies, in Proceedings of IEEE International Conference on Data Mining-Workshops. IEEE Computer Society 2006, pp. 7579 19] T. Joachims, A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization, in Proceedings of the Fourteenth International Conference on Machine Learning. Morgan Kaufmann Publishers Inc. San Francisco, CA, USA, 1997, pp. 143151 20] A. McCallum, Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering, 1996 http://www.cs.cmu.edu/ mccallum/bow 21] D. Lewis, Y. Yang, T. Rose, and F. Li, RCV1: A new benchmark collection for text categorization research, The Journal of Machine Learning Research, vol. 5, pp. 361397, 2004 22] M. Porter, Snowball: A language for stemming algorithms, 2001 http://snowball.tartarus.org/texts/introduction.html 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


