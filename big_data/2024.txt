Fast Online Dynamic Association Rule Mining Yew-Kwong Woon Wee-Keong Ng Amitabha Das N any an g Techno logic a1 University Nanyang Avenue Singapore 639798 SINGAPORE davidwyk@singnet.com.sg wkn@acm.org asadas@ntu.edu.sg Abstract At present there are no association rule mining algo rithms that are suitable for use in electronic commerce because they do not consider that new products are in troduced and old ones are retired frequently and they as sume that support thresholds do 
not change In this paper a new algorithm called Fast Online Dynamic Association Rule Mining FOLDARM is introduced for mining in eke tronic commerce It uses a novel tree structure known as a Support-Ordered Trie Itemset SOTrieIT structure to hold pre-processed transactional data It allows FOLDARM to generate large 1 itemsets and 2-itemsets quickly without scanning the database In addition the SOTrielT structure can be easily and quickly updated when transactions are added or removed It also stores data that is independent 
of the support threshold and thus can be used for mining with varying support thresholds without any degradation in per formance Experiments have shown that FOLDARM out pelforms Apriori a classic mining algorithm by up to two orders of magnitude 100 times 1 Introduction Since the introduction of the Apriori algorithm l in 1994 there has been sustained interest in researching new association rule mining algorithms that can perform more efsciently However to the best of our knowledge, these ex isting algorithms are not designed 
for use in a fast-changing and widely distributed environment like the Internet In electronic commerce where a huge number of transactions can arrive at a virtual store from all over the world 24 hours a day, transaction databases are expected to be updated fre quently Thus, existing algorithms that are designed to mine static databases that are not expected to change will not be able to perform as well with databases that are constantly changing There are some algorithms that can perform in cremental mining, which means that they can improve min ing speed by reusing past mined information 
However in the next section we will see that such algorithms cannot cope with databases with very frequent updates In addition in a highly competitive setting like elec tronic commerce companies will need to constantly intro duce new products and remove unpopular products to sat isfy the increasingly demanding needs of the now empow ered customer This means that the number and type of unique items in the database will change very often Unfor tunately existing algorithms assume that unique items are xed and thus each time items are added or 
removed, the algorithms must mine the database from scratch and discard valuable past mined results Finally given the dynamism and volatility of transac tional data in electronic commerce companies cannot pre dict a suitable support threshold to set for the mining pro cess Using too high a threshold may result in too many unimportant rules while too low a threshold may result in certain important rules being passed over Therefore there is a need to mine the database with several different support 
thresholds before an optimal threshold can be determined This critical need has not been effectively tackled by current algorithms Recently we have introduced an innovative algorithm called Rapid Association Rule Mining RARM 2 to mine static databases efEciently In this paper a new algo rithm called Fast Online Llynamic Association Rule Min ing FOLDARM is proposed as an extension of RARM so that association rule mining can be performed more ef ciently in electronic commerce Like RARM, FOLDARM constructs a new data structure called Support-Ordered 
Trie Itemset SOTrieIT This trie-like tree structure stores the support counts of all I-itemsets and 2-itemsets in the database All transactions that arrive are pre-processed all 1 itemsets and 2-itemsets are extracted from each transac tion The extracted information will be used to update the SOTrieIT without the need for prior knowledge of the sup port threshold This structure is sorted according to the sup port counts of each node in descending order FOLDARM 0-7695-1393-WO2 17.00 0 2002 IEEE 278 


uses SOTrieIT to quickly discover large 1-itemsets and 2 itemsets without scanning the database The need to gener ate candidate 1 itemsets and 2-itemsets constitutes the main bottleneck in large itemset generation, as observed in 3 Therefore by eliminating this need FOLDARM achieves signifcant speed-ups Subsequently it applies the Apriori algorithm to obtain larger-sized itemsets Moreover, unlike RARM FOLDARM allows transactions and unique trans actional items to be added and removed without the need to destroy the current SOTrieIT and rebuild it It is now clear to see how FOLDARM obtains its name it is fasr because the SOTrieIT is constructed in an incre mental manner it is online because users can vary the sup port threshold while maintaining the same performance it is dynamic because the unique set of items which we deEne as the Universal Itemset can be changed and be easily ac commodated in the SOTrieIT Experiments have been con ducted to study the performance of FOLDARM and com pare it against Apriori  11 FOLDARM is found to be up to 100 times faster than Apriori The rest of the paper is organized as follows The next section reviews related work Section 3 gives a description of the problem while Section 4 presents the new tree struc ture Section 5 describes the FOLDARM algorithm Time and space complexity of the new structure will be examined in Section 6 Performance evaluation is discussed in Section 7 Section 8 compares the salient features of FOLDARM with existing algorithms to explain its edge over them Fi nally the paper is concluded and recommendations for fu ture work are made in Section 9 2 Related Work The Apriori algorithm I is the Erst successful algo rithm for mining association rules Since its introduction it has popularized the task of mining association rules and sparked off many research papers It introduces a method to generate candidate itemsets ck in a pass k using only large itemsets Lk-1 in the previous pass The idea rests on the fact that any subset of a large itemset must be large as well Hence ck can be generated by joining Lk-1 and deleting those that contain any subset that is not large This would result in a signifcantly smaller number of candidate item sets being generated After Apriori the Direct Hashing and Pruning DHP algorithm  is the next most widely used algorithm for the effcient mining of association rules It employs a hash technique to reduce the size of candidate itemsets and the database This amounts to signifcant speed-ups because the dominating factor in the generation of large itemsets is the size of the candidate itemsets DHP has signiEcant speed improvements due to the reduced size of the candi date itemsets generated However it incurs additional over heads due to the need to do hashing and to maintain a hash table After some experiments 3 it is concluded that the hash technique should only be applied during the genera tion of candidate 2-itemsets to achieve speed-ups of up to 3 times against Apriori The Fast Update FUP algorithm 4 is an incremental algorithm which makes use of past mining results to speed up the mining process Its successor the Fast Update Two FUP2 algorithm 5 is a faster version and generalization of it By setting bounds for the support counts of candidate itemsets it is able to reduce the size of Ck and improve its efEciency By re-using past mining information FUP2 reduces the number of candidate sets and hence achieves a speed improvement of up to 2 times over Apriori However when the size of the updates exceeds 40 of the original database Apriori performs better Incremental mining is brought to another new level when the Adaptive algorithm  is introduced This algorithm is not only incremental but also adaptive in nature By in ferring the nature of the incremental database it can avoid unnecessary database scans Experiments have shown that it can perform up to seven times faster than Apriori Unlike all the discussed algorithms the Continuous As sociation Rule Mining Algorithm CARMA 7 allows the user to change the support threshold and continuously dis plays the resulting association rules with support and conf dence bounds during the Erst scan or phase During the sec ond phase it determines the precise support of each item set and extracts out all the large itemsets With the sup port lattice CARMA can readily compute large itemsets for varying support thresholds However experiments reveal that CARMA only performs faster than Apriori at support thresholds of 0.25 and below Finally the Frequent Puttern-growth FP-growth algo rithm 8 is a most recent association rule mining algorithm which achieves impressive results It uses a compact tree structure called a Frequent Pattern-tree FP-tree to store information about large 1 itemsets This compact struc ture also removes the need for database scans and it is con structed using only two scans In the Erst database scan large 1-itemsets L1 are obtained and sorted in support de scending order In the second scan, items in the transac tions are Erst sorted according to the order of L1 These sorted items are used to construct the FP-tree FP-growth then proceeds to recursively mine FP-trees of decreasing size to generate large itemsets without candidate generation and database scans It does so by examining all the condi tionalpattern bases of the FP-tree which consists of the set of large itemsets occurring with the suffx pattern Condi tional FP-trees are constructed from these conditional pat tern bases and mining is carried out recursively with such trees to discover large itemsets of various sizes However since the construction and use of the FP-trees are complex 279 


the performance of FP-growth is reduced to be on par with Apriori at support thresholds of 3 and above It only achieves signiEcant speed-ups at support thresholds of 1.5 and below Moreover it is only incremental to a certain ex tent depending on the FP-tree watermark validity support threshold The features and performance of the discussed algo rithms are presented in Figure 1 To sum up, none of the algorithms are suitable for use in electronic commerce be cause there is not one that is incremental supports dynamic thresholds and at the same time performs fast enough for online queries Moreover all of them assume that the uni versal set of unique items do not change. Should the unique items change mining must be done from afresh and past mined information cannot be reused In contrast, our pro posed algorithm FOLDARM possesses all the essential features of an ideal mining algorithm for electronic com merce 3 Problem Description The problem of mining association rules is described as follows Let the universal itemset I  alla2  a be a set of literals called items Let D be a database of transac tions, where each transaction T contains a set of items such that T 5 I An itemset is a set of items and a k-itemset is an itemset that contains exactly k items For a given itemset X C I and a given transaction T T contains X if and only if X C T Let ux be the support count of an itemset X which is the number of transactions in D that contain X Let s be the support threshold and ID be the number of transactions in D An itemset X is large or frequent if g  Dl x s An association rule is an implication of the form X j Y where X C_ I Y 5 I and X n Y  0 The association rule X  Y holds in the database D with conxdence c if no less than c of the transactions in D that contain X also contain Y The association rule X  Y has support s in D if gxUy  Dl x s For a given pair of conEdence and support thresholds the problem of mining association rules is to discover all rules that have conEdence and support greater than the cor responding thresholds For example in a computer hard ware shop the association rule Digital Camera  Printer means that whenever customers buy digital cameras they also buy printers c of the time and this trend occurs s of the time This problem consists of Ending large item sets Erst and then generating association rules from the large itemsets We will only address the Erst sub-problem of End ing large itemsets because it is much more computationally expensive and thus it is the main bottleneck in association rule mining 4 Data Structure Though our new data structure the SOTrieIT is de scribed in detail in our previous work 2 we will briay describe it here again for completeness 4.1 A Complete TrieIT The database D of transactions is stored in a forest of lexicographically-ordered tree nodes known as Trie Item set TrieIT\Let the set of items I  ul a2    aN be ordered so that for any two items al E I,a E I 1  i,j f N a  a3 if and only if i  j Dehition 1 Complete TrieIT A complete TrielT is a tree structure such that every tree node w is a 2-tuple wel w where we E I is the label of the node and wC is a support count Since every tree node corresponds to some item a E I for brevity, we also use w to refer to a tree node that corresponds to al E I The following conditions hold I Let C\(wz be the ordered set of children nodes of wz 2 Given a node w let Wk Wk+l    wt-l\(l  k 6 i  1 be the set of nodes on the path from the root to the parent of w then w is a count of the itemset ak ak+l     a in the database Hence the sup port count of any k-itemset can be obtained by follow ing a set of nodes to a depth of k fC\(W  0 then C\(WJ c WL+l W2+2  1 WN Let Wi be a complete TrieIT whose root node has label a Then the D is stored in a set of complete TrietTs denoted I by W where W C_ Wi  Wz     WN For every transaction that arrives the complete TrieIT needs to be updated with the powerset of the transaction items The amount of storage space needed by the complete TrieIT scales exponentially with respect to the number of unique items Hence with its expensive computation and storage requirements the complete TrieIT is not a practical data structure We will discuss a better alternative in the next section 4.2 Support-Ordered Trie Itemset This new design builds on the ideas presented in the pa per on DHP 3 In that paper it is discovered that genera tion of large 2-itemsets is the main performance bottleneck Using a hashtable DHP is able to improve performance sig niEcantly by reducing the size of the candidate 2-itemsets Similarly this approach seeks to End a data structure that allows for quick generation of large 2-itemsets without the 280 


Algorithm Incremental Apriori X DHP X FUP2 J CARMA X FP-growth X TID Items 1 Dynamic support threshold Speed-up against Apriori X 1 X 3 X 2 X 10 s I 1.5 J 2 s I 0.2 Figure 1 Summary of Algorithms Figure 2 An example transaction database with N  4 heavy processing and memory requirements of the complete TrieIT The solution is a 2-level support-ordered tree which is called a SOTrielT Support-Ordered Trie Itemset Deenition 2 SOTrieIT TrielT except the following A SOTrielT has the same properties as the complete 1 Let Y be a SOTrielT whose root node has label ai Let Y be a set of SOTrielTs which stores the support counts of all I-itemsets and 2-itemsets in D such that 2 The nodes are sorted according to their support counts I In other words the set of SOTrieITs only keeps a record of all 1-itemsets and 2-itemsets contained in a transaction Therefore much computation time is saved as compared to the case with the complete TrieIT where the power set of transaction items is extracted The Erst-level nodes represent 1 itemsets while second-level nodes represent 2 itemsets The resultant structure will be much smaller than the multi-level complete TrieIT Henceforth we shall use the term SOTrieIT to denote a set of SOTrieITs By keeping track of the support counts of all 1-itemsets and 2-itemsets SOTrieIT allows both large 1-itemsets and 2-itemsets to be found very quickly This is because there is no need to scan the database which could be very large Instead only the small SOTrieIT is scanned Moreover as the SOTrieIT is sorted according to the support counts of the itemsets only part of the structure needs to be scanned y c Y,,Y2  YlV in descending order from the lep Example Figure 3 represents the fully constructed SOTrieIT for the example transaction database in Figure 2 To illustrate how nodes are created let us examine what happens when a new transaction arrives Note that only 1 itemsets and 2-itemsets are extracted from the transactions When both transaction 100 and 200 arrive the nodes created are shown in Figures 3\(a\and 3\(b Notice that in Figure 3\(b\the node wc under the ROOT node comes before the node WA This is because the nodes are sorted according to their support counts and wc has a higher support count than WA When transaction 300 arrives the following itemsets areextracted A}, {B},{C A,B A,C},{B,C Figure 3\(c shows the resultant SOTrieIT when this transaction is processed When transaction 400 arrives the following itemsets are extracted B D C D The SOTrieITs are updated in a similar fashion for transaction 400 as seen in Figure 3\(d A B C D A B A c A 01 B c Correctness We need to show that with a SOTrieIT, the support counts of all 1-itemsets and 2-itemsets can be cor rectly obtained without scanning the database Let T be a transaction of size s and T  bl b2    b The 1-itemsets that are extracted and used to build W are  bl  bz      b and the 2-itemsets extracted are by where 0  z  s and z  y  s These itemsets update counts in the SOTrieITs accordingly Every item set increments or decrements the support count of its corre sponding tree node depending on whether the transaction is added or deleted At any point in time W contains all the support counts of all 1-itemsets and 2-itemsets that appear in all the transactions Hence there is no longer any need to scan the database during the generation of large 1-itemsets and 2-itemsets 5 Algorithm FOLDARM 5.1 Pre-processing Figure 4 shows the pre-processing steps taken whenever a transaction is added or deleted. For every transaction that arrives 1-itemsets and 2-itemsets are rst extracted from it For each itemset the SOTrieIT Y will be traversed in or der to locate the node that stores its support count. Support counts of 1-itemsets and 2-itemsets are stored in Erst-level and second-level nodes respectively Therefore this traver sal requires at most two redirections that makes it very fast 281 


1 Let Y be a set of SOTrieITs 2 for k  1 IC  2 IC do begin 3 4 5 6 7 8 9 Obtain all k-itemsets of the transaction and store them in Ck foreach itemset X E Ck do begin Traverse Y to locate nodes along the path that represents X if such a set of nodes exists in Y then Increment or decrement support count of the leaf node depending on the nature of update if its support count falls to 0 remove node and its child nodes if any Sort the updated node according to its new support count in descending order Create a new set of nodes with support count of 1 that represent a path to X Insert nodes into Y according to their suppor counts in descending order from the left 11 else 12 13 14 endif 15 endfor 16 endfor Figure 4 Pre-processing Algorithm Y will then be sorted level-wise from left to right according to the support counts of the nodes in descending order If such a node does not exist, it will be created and inserted into Y accordingly Similarly Y is then sorted after such an insertion For deletions the steps are similar except that the support counts of the affected nodes are decremented and nodes are deleted if their support counts fall to zero 5.2 Update of Universal Itemset Figure 5 shows how the SOTrieIT is updated when the universal itemset is changed In algorithms like FP-growth that use a similar data structure to store itemset information the structure must be rebuilt to accommodate updates to the universal itemset In our case the SOTrieIT can be easily updated to accommodate the new changes 1 if a new item i is added to the universal itemset 2 3 else if an item j is removed from the universal itemse do nothing because the SOTrieIT will be updated the moment a transaction with i arrives traverse the SOTriet to remove all nodes and their child nodes if any that contain j Figure 5 Universal ltemset Update Algorithm 5.3 Mining of large itemsets Figure 6 shows the steps taken when the mining process is started The SOTrieIT Y is rst traversed to discover large 1 itemsets and 2-itemsets. In our approach depth-Erst search is used starting from the leftmost Erst-level node As Y is sorted according to support counts the traversal can be stopped the moment a node is found not to satisfy the minimum support threshold After large 1-itemsets and 2 itemsets are found the algorithm proceeds to discover other larger itemsets using the Apriori algorithm Example To illustrate the mining algorithm we use the same transaction database found in Figure 2 and the SOTrieIT structure in Figure 3\(d Suppose the support threshold is set at 75 Then the minimum support count to qualify an itemset to be large is 3 Figure 7 shows the traversal path taken in obtaining the large I-itemsets and 2-itemsets The bold numbers on the arrows denote the se quence with which the SOTrieIT is traversed During the generation of the Erst two large itemsets the moment a Erst-level node with a support count lower than 3 is encoun tered, the rest of its siblings and subtrees are not scanned But when a second-level node is found not to have satisEed the minimum support count only its subsequent siblings will be ignored In this case at the Efth traversal when the node that represent itemset A B is found to have a support count of less than 3 the node that represent item set A D will not be explored The Enal large 1-itemsets and 2-itemsets found are L1  A  B  C and Lz  A C B C and the total number of traversals is 9 out of a maximum IO The scenario changes favorably 282 


1 Let N be the qth child node of parent node p 2 Let NC be number of child nodes under p 3 Let I be the itemset represented by node n 4 for 1 J  NCROOT x do begin 5 Let X  N$OoT 6 8 9 11 endfor 12 endif 13 endfor 14 Run Apriori from its third iteration to End if FX 3 ID x s then begin 7 Add Ix to L1 for y 1  y  NCx  y do begin if ONC 2 Dl x s then 10 Add IN to Lz the rest of the large itemsets Figure 6 Mining Algorithm ROOT D\(1 C\(3 B\(2 D\(1 C\(3 D\(1 Figure 7 Traversal path of the SOTrielT at a support threshold of 75 when we increase the support threshold slightly The next example demonstrates more clearly the advantage of order ing the SOTrieIT by the support counts of nodes For a minimum support threshold of SO the minimum support count needed is 4 Figure 8 shows the traversal path taken in obtaining the large 1-itemsets and 2-itemsets The FOLDARM algorithm stops traversing the SOTrieIT at the third traversal when it encounters the item A which has a support count of 3 This is because all other nodes that come after Erst-level node A will have a support count of 3 or less Therefore there will not be any more large itemsets in the rest of the SOTrieIT The algorithm terminates after only 3 traversals and the only large itemset is C The above examples illustrates the usefulness of the SOTrieIT once constructed it can be used for varying sup port thresholds In addition in a best case scenario where the minimum support threshold is high it may only need one traversal to discover all large itemsets while in a worst case scenario it may only need a number of traversals whose cost is defnitely lesser than that of scanning a large D\(1 C\(3 B\(2 D\(1 C\(3 D\(1 Figure 8 Traversal path of the SOTrielT at a support threshold of 80 database Time and space complexity issues will be ex plored further in the next section 6 Time and Space Complexity 6.1 Pre-processing Time Complexity The amount of time to pre-process a transaction depends on the amount of time to extract 1 itemsets and 2-itemsets from the transaction to traverse the SOTrieIT to increment the support counts of the respective nodes and to create new nodes in the SOTrieIT for items that are not encountered yet For a transaction of size s only 223Cl 221C2 itemsets are pre-processed Hence its complexity is O\(s2 As the SOTrieIT is only two levels deep it takes at most two links to reach the desired node Suppose it also takes one unit of time to move over one link it will take a maximum of 2 x 222Cl 222C2 units of time to move to all the nodes required by a transaction of size s Space Complexity In a database with N unique items there will be N Erst-level nodes in the SOTrieIT For each Erst-level node since the SOTrieIT is created in a trie-like manner it will contain only items that are lexicographically larger than itself The Erst-level node who has the largest number of child nodes is the one that has the Erst position in a set of lexicons It will have N  1 child nodes Sub sequent Erst-level nodes will have one less child node than the previous one Therefore, for N unique items a max imum of only x nodes inclusive of both Erst-level and second-level nodes are needed to store the entire pre processing information Hence its complexity is O\(N2 N 6.2 Mining of large itemsets This section discusses the time complexity of the mining phase as compared to that of Apriori Space complexity will not be mentioned because this phase also involves the SOTrieIT whose space complexity is already discussed 283 


Symbol Meaning Number of unique items Number of transactions Number of maximal potentially large itemsets Average size of the transactions Average size of the maximal potentially large itemsets Figure 9 Defnition of Parameters Time Complexity To compare the time complexity of FOLDARM and Apriori we shall focus only on the scan ning process to obtain the support counts of itemsets This is enough to see the vast improvement of FOLDARM over Apriori For each pass of the Apriori algorithm there is a need to scan the entire database regardless of the desired support threshold Suppose the database is of size a and the average size of each transaction is b Then Apriori takes O\(ab units of time to scan the database at each pass For the Erst two passes of FOLDARM only the SOTrieIT Y needs to be scanned. According to Section 6 I Y has x nodes and since each node has one link from its parent in the worst case it will take at most 2 x 5 units of time to traverse the entire structure where N  a In addition, the time needed also depends on the desired support threshold which would further reduce the number of traversals Therefore the average amount of scanning time for the Erst two passes will be far less than O\(ab N N 7 Performance Evaluation This section evaluates and compares the relative perfor mance of the Apriori and FOLDARM algorithms by con ducting experiments on a Pentium-111 machine with a CPU clock rate of 1.7 GHz 256 MB of main memory and run ning on a Windows 2000 platform The algorithms are im plemented in Java and hence large memory requirements of the Java Virtual Machine prevented us from scaling up the experiments Future experiments will be conducted to tackle this issue The SOTrieIT structure is implemented using a combination of integer arrays and Eles Implemen tation details are omitted due to the lack of space In spite of extra &le U0 requirements, FOLDARM maintains its im pressive performance Figure 9 shows the various parame ters used and their meanings The method used for generating synthetic data is similar to the one used in l To describe an experiment we the notation Tw.1z.Ny.D modiEed from the one used in l where w is the average size of transactions II is the average size of maximal potentially large itemsets y is the number of unique items and z is the size of the database We added the y parameter to represent the databases in a clearer man ner The databases used here are similar to those in SI The Erst one is T25.IlO.NlK.DlOK which is denoted as Dl while the second is T25.120.NlOK.DlOOK which is denoted as D2 The following sections analyze the performance of FOLDARM as compared to the algorithms introduced in Section 2 in different scenarios 7.1 Static Databases Figure 10 shows the execution times excluding pre processing time of FOLDARM\for the two different static databases of both Apriori and FOLDARM The databases are termed static because they are not expected to change over time From the graphs it can be quickly observed that FOLDARM outperforms Apriori in all situations In Figure 10\(a FOLDARM maintains a steady speed-up of about 10 times for support thresholds ranging from 3 to 1.5 in VI However when the support threshold falls below 1.5 the speed-up is signifcantly reduced At a support thresh old of OS FOLDARM only manages a speed-up of 1.2 times The situation changes dramatically in D2 Figure 10\(b uses a log scale for the time axis because of the vast dif ference between the execution times of FOLDARM and Apriori. FOLDARM performs at least 80 times faster than Apriori for support thresholds ranging from 3 to 2 Its performance peaks at a support threshold of 1.5 where it performs more than 160 times faster than Apriori This speed-up falls to 54 times at a support threshold of 0.5 Explanation The poor performance of FOLDARM in VI especially at lower support thresholds is due to the fact that more larger-sized frequent itemsets exist at lower sup port thresholds and FOLDARM uses the Apriori algorithm to discover large k-itemsets for k  3 Hence the com putation savings in the Erst two iterations are insigniEcant compared to the huge computation costs needed in obtain ing larger frequent itemsets Interestingly in larger databases like D2 FOLDARM performs exceptionally well The obvious vast improve ment of FOLDARM in 232 can be explained by Figure 11 which shows the number of candidate k-itemsets generated during the mining of Dl and  for a support threshold of 0.5 From Figure 11 it is clear that the main difference in candidate generation between 21 and V2 is in the num ber of candidate 2-itemsets generated Thus by eliminating the need for candidate 2-itemset generation FOLDARM achieves a much greater speed-up in D2 as it contains more than 6 times the number of candidate 2-itemsets as com pared to VI As for higher support thresholds, FOLDARM is able to outperform Apriori by up to two orders of magni tude because in D2 the maximum size of the large itemsets IC is much lower than that of VI If k increases indef 284 


1 8e+006 16e+006 14e+006 O 800000 e GOMXX D1  x  D2 f3   4m00 200000 Ork initely the performance of FOLDARM will eventually be reduced to that of Apriori However, we can conclude from the experiments that as databases and universal itemsets in crease in size k will decrease and thus FOLDARM will scale up very well against Apriori This Ending is particu larly crucial in an electronic commerce where transactions database size\are expected to arrive by the thousands and a wide variety of products \(universal itemset size is available for purchase Another important point to note is that after the Erst 2 passes FOLDARM actually uses Apriori to mine larger fre quent itemsets without relying on the SOTrieIT structure Despite of the fact that FOLDARM only differs from Apri ori during the Erst 2 passes it is much faster and scalable this congrms that candidate itemset generation during the Erst 2 passes constitutes the main bottleneck in Apriori FP-growth is currently the fastest algorithm for min ing static databases However due to the lack of time FP-growth is not implemented but its performance against FOLDARM can be evaluated using Apriori as a basis for relative comparisons The experiments conducted in  81 re port an overall improvement of only an order of magnitude for FP-growth over Apriori In addition the performance of FP-growth is on par with Apriori for support thresholds ranging from 3 to 1.5 in Dz The poor performance of FP-growth can be attributed to the cost in recursively con structing FP-trees Hence signiEcant speed-ups can only be noticed in lower support thresholds when Apriori can not cope with the exponential increase in candidate itemset generation This is undesirable because we want to mine databases efkiently at a wide range of support thresholds instead of only at low support thresholds FOLDARM over comes this limitation of FP-growth and consistently outper forms Apriori at all support thresholds and it can even per form up to two orders of magnitude faster than Apriori   2   __    __    9  p  9 Q 7.2 Dynamic Databases A dynamic database is one with frequent updates trans actions are added and removed frequently In 5 the FUP2 algorithm is presented to make use of past mining results to mine new transactional updates more efkiently Due to time constraints FUP2 is not implemented for comparison studies with FOLDARM As before with the results in its performance analysis section, we can easily assess its per formance as compared to FOLDARM with the performance of Apriori as our reference point In a database of the type TIO.14.NlK.DlOOK with an addition and deletion of 5000 transactions it is found that FUP2 performs only about twice as fast as Apriori as seen in the experiments in 5 This is its best performance against Apriori through the use of past mined knowledge In ad dition it is discovered that when the size of the updates of a database exceeds 40 of the its original size, Apriori can even outperform FUP2 Both Apriori and FOLDARM mine a dynamic database as if it were a static database because they do not make use of past results Since FOLDARM per forms up to 100 times faster than Apriori in a much larger database we can safely conclude that FOLDARM will def initely outperform FUP2 by a wide margin Explanation In spite of its ability to reuse mined results and hence reduce the number of candidate itemsets gener ated FUP2 still performs slower than FOLDARM because it needs to scan the database during the generation of large 1-itemsets and 2-itemsets FOLDARM can quickly do so simply by scanning the SOTrieIT structure which is de nitely many times smaller than the database In situations when the updates are high, the performance of FUP2 drops because the updated database becomes so different from the original one that past mining results are not helpful in determining new large itemsets In processing useless old information, precious computation time is wasted On the other hand FOLDARM does not need to retain past min ing results. Moreover, frequent database updates do not af fect it because it always mines the database from scratch Note that the SOTrieIT structure does not need to be con structed from scratch when the database is updated because it is continuously updated each time a transaction is added or retired 7.3 Dynamic Support Threshold CARMA is currently the only algorithm that allows the user to modify the support threshold on the ay We will once again use the results presented in 7 as a form of compar ison In a database of the form T10.14.NlOK.D100K it is found that Apriori outperforms CARMA for support thresh olds of 0.5 and above It is only when the support thresh 285 


D1 T25110NlKDlOK Apnon x  350 FOLOARM 0 300 I 250 c 200 F 150 100 50 0 e E 11    0 x x __-__ 0 4 9 __ x _ __    c e 1 E F 150 j Suppon Threshold  a 0 D2 T25.120.NlOK.Dl00K 0 4 m Apnon x FOLOARM 0 100000 1 0.5 1 15 2 2.5 3 Suppon Threshold 46 b Figure 10 Execution times for two databases of the form Tw.1s.Ny.D where w is the average size of transactions 3 is the average size of maximal potentially large itemsets y is the number of unique items and z is the size of the database at varying support thresholds olds are at 0.25 and below that CARMA begins to outper form Apriori only by less than 1.5 times On the other hand FOLDARM consistently outperforms Apriori by wide mar gins at various support thresholds and supports dynamic support thresholds This is because the SOTrieIT can be reused for mining at different support thresholds without additional computation Explanation The poor performance of CARMA is at tributed to its need to maintain a lattice of potentially large itemsets It will be faster only when the user does not need a precise set of large itemsets because in this case CARMA does not need to re-scan the database FOLDARM per forms much faster because the SOTrieIT stores threshold independent information and thus its performance will not be affected even if the user changes the support thresholds frequently to obtain an optimal threshold 7.4 Dynamic Universal itemset As none of the algorithms discussed takes into consider ation of the fact that unique items in the database will vary it is not possible to conduct experiments for comparison However we can approximate the most probable results by examining the characteristics of each algorithm When new transactions with new items are added to a database or when old transactions with obsolete items are retired all the discussed algorithms would have to mine the updated database from scratch. Therefore their performance against FOLDARM in such a scenario can be deduced from the pre vious sections When the universal itemset is changed, the SOTrieIT can be easily updated as seen in Figure 5 Hence it should retain its performance edge and outperform all the algorithms seen in the previous sections 7.5 Pre-processing and Storage Requirements As pre-processing is carried on a transaction at the mo ment it arrives in the database it is distributive by nature and thus will not burden a system excessively FOLDARM spends an average of only 180 ms and 250 ms in pre processing a single transaction found in 731 and D2 respec tively This amount of time is insigniEcant considering that it will result in major speed-ups in the mining process This requirement should not be taken into consideration in com paring the performance of FOLDARM and Apriori because pre-processing is done outside of the actual mining process itself The SOTrieIT structure resides in both memory and fles As primitive integer arrays are employed in memory for storing the Erst-level nodes the SOTrieIT only takes up only 2 KB and 14 KB in 271 and Dz respectively Second level nodes grow exponentially with respect to N as seen in Section 6.1 and as such they cannot be stored in memory Instead they are stored in Eles which are named after the labels of their parents These Eles take up approximately 2 MB and 53 MB for 731 and 732 respectively Therefore, it can be concluded that by distributing the SOTrieIT structure among memory and Eles scalability is ensured as hard disk space is currently in the realm of tens of gigabytes 8 Comparison of features This section focuses on the salient features of FOLDARM and elaborates on why it is more suitable for 286 


use in electronic commerce as compared to existing algo rithms discussed in Section 2 For an association rule min ing algorithm to be useful in electronic commerce it must have the following capabilities 1 General Incremental Mining Support The algorithm must obtain association rules quickly that react the latest changes to the transaction database at a certain point in time This can only be achieved if the algo rithm can perform general incremental mining which means that past mining results are exploited during the mining of a database which has many additions and deletions of transactions since the last mining opera tion that was carried out on it 2 Dynamic Threshold Support The fast-changing na ture of electronic commerce prevents the prediction of a suitable support threshold for the mining process Hence the algorithm must allow the user to change the support thresholds of the mining process until an opti mum value is found without signigcant performance degradation In other words it must be able to create and reuse mining information that is independent of the support threshold In a highly competitive environment like electronic commerce new products must be introduced frequently and old ones be retired to satisfy the changing and demand ing needs of the empowered customer Therefore the algorithm must not assume that items in the database remain xed. It must be able to reuse past mining re sults efgciently regardless of changes made to the set of unique items in the database 3 Dynamic Universal itemset Support Apriori I DHP 3 and FP-growth 8 all attempt to improve mining speed but do not satisfy any of the above criteria FUP  FUP2 5 and the Adaptive algorithm 6 satisfy the Erst criteria but do not take into consideration the other two CARMA fulflls the second criteria but does not satisfy the other two and is extremely slow Finally no algorithms exist to date that satisEes the third criteria With the SOTrieIT FOLDARM satisges all three criteria and even performs faster than all algorithms 9 Conclusions The increasing popularity of electronic commerce presents new challenges to association rule mining Due to the easy availability of huge amount of transactional data there is a urgent need for faster algorithms to mine such rich data We have proposed a new algorithm called FOLDARM which uses an efscient novel structure known as the SOTrieIT By simply eliminating the need for candi date I-itemset and 2-itemset generation, FOLDARM is able to achieve signigcant speed-ups Experiments have shown that FOLDARM is much faster than Apriori By using Apri ori as a basis for comparisons FOLDARM is proven to be faster than some prominent existing algorithms In addition as FOLDARM can maintain its performance while the sup port threshold and universal itemset change it is ideal for use in electronic commerce. Therefore though there are ad ditional pre-processing and storage requirements they are both insignigcant and worthwhile considering the advan tages that they reap As databases and their universal item sets grow in size it will be more difgcult to maintain the SOTrieIT Hence parallel versions of the SOTrieIT and the FOLDARM algorithm need to be researched to meet the rising demands of mining larger databases References I R Agrawal and R Srikant Fast algorithms for mining association rules In Proc 20th tnt Con on Very Large Databases pages 487499 Santiago Chile 1994 2 A Das W K Ng and Y K Woon Rapid association rule mining In Proc 10th Int Con on Information and Knowledge Management Atlanta Georgia 2001 3 J S Park M S Chen and P S Yu Using a hash based method with transaction trimming for mining as sociation rules IEEE Trans. on Knowledge and Data Engineering 9\(5 September 1997 4 D W Cheung J Han V T Ng and C Y Wong Maintenance of discovered association rules in large databases An incremental updating technique In Proc 12th Int Con on Data Engineering pages 106-1 14 New Orleans, Louisiana 1996 5 D W Cheung S D Lee and B. Kao A general in cremental technique for maintaining discovered associ ation rules In Proc. 5th tnt Con on Database Sys tems for Advanced Applications pages 185-1 94 Mel bourne Australia 1997 6 N L Sarda and N V Srinivas An adaptive algorithm for incremental mining of association rules In Proc 9th Int Con5 on Database and Expert Systems pages 240-245 Vienna Austria 1998 7 C Hidber Online association rule mining In Proc ACM SIGMOD tnt Con on Management of Data pages 145-154 Philadephia Pennsylvania USA 1999 8 J Han J Pei and Y Yin Mining frequent patterns without candidate generation In Proc ACM SIGMOD Int Con on Management of Data pages 1-12 Dallas Texas 2000 287 


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


