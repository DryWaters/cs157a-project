Modeling and Predicting the Helpfulness of Online Reviews Yan g L i u 1  Xiangji Huang 2  Aijun An 1 and Xiaohui Yu 2 1 Department of Computer Science and Engin eering York University Toronto Canada  yliu aan  cse.yorku.ca 2 School of Information Technology York University Toronto Canada  jhuang xhyu  yorku.ca Abstract Online reviews provide a valuable resource for potential customers to make purchase decisions However the sheer volume of available reviews as well as the large variations in the review quality present a big impediment to the effective use of the reviews as the most helpful reviews may be buried in the large amount of low quality reviews The goal of this paper is to develop models and algorithms for predicting the helpfulness of reviews which provides the basis for discovering the most helpful reviews for given products We rst show that the helpfulness of a review depends on three important factors the reviewer’s expertise the writing style of the review and the timeliness of the review Based on the analysis of those factors we present a nonlinear regression model for helpfulness prediction Our empirical study on the IMDB movie reviews dataset demonstrates that the proposed approach is highly effective 1 Introduction The increasing impact of the I nternet has dramatically changed the way that people shop for goods More and more people are now gravitating to reading products reviews prior to making purchasing decisions Such reviews have become an indispensable component of e-commerce Websites such as Amazon http://www.amzon.com and they are also available through dedicated Websites such as CNET http://www.cnet.com and IMDB http://www.imdb.com While reading reviews can help the potential customers make informed decisions in many cases the large quantity of reviews available for a product can be overwhelming and actually impede the customers ability to evaluate the product This is further aggravated by the fact that the quality of the online reviews tends to be very uneven ranging from excellent detailed opinions to simple repetition of product speci\036cations to in the worst case pure spams As a consequence potential consumers have to sift through a large number of reviews in order to form an unbiased judgment regarding the product To alleviate this problem many Websites are now allowing readers of a review to indicate whether they think that review is helpful by voting for or against it and a tally or score is provided in the form of  100 out of 150 people found the following review helpful  The reviews can be sorted according to their helpfulness using those scores Although this is certainly an improvement in the right direction there are still important issues to be addressed For example  For newly posted reviews most likely no vote or only a few votes have been cast and therefore identifying their helpfulness is dif\036cult  Presenting the reviews ranked by their user-voted helpfulness scores may create situations of monopoly in that only the highest ranked reviews get viewed leaving no opportunities for the newly published yet unvoted reviews to show up on users radar  In some cases reviews can be incorrectly labeled as helpful or not helpful due to spam voting 8  In these scenarios it will be highly desirable to have a way to predict the helpfulness of the given reviews The predicted helpfulness scores can then be used to address the above problems either directly or indirectly by combining with existing user votes if there is any This paper is concerned with the problem of automatically evaluating the helpfulness of reviews and consequently identifying the most helpful reviews for a particular product Previous research on review mining has focused on answering questions like  What do people think of the product  3 16 1   How would users evaluation affect the sales of a certain product  1 5 1  a n d  How to understand and summarize the reviews with minimum human efforts  7 20  b u t f e w e x p licitly co n s id er th e p r o b l em of evaluating the quality of reviews which is signi\036cantly 
2008 Eighth IEEE International Conference on Data Mining 1550-4786/08 $25.00 © 2008 IEEE DOI 10.1109/ICDM.2008.94 443 
2008 Eighth IEEE International Conference on Data Mining 1550-4786/08 $25.00 © 2008 IEEE DOI 10.1109/ICDM.2008.94 443 


different from the well-studied problem of sentiment classi\036cation and opinion extraction In this paper we take a principled approach to tackling this important problem by developing a novel model for predicting helpfulness of reviews The model is based on a thorough analysis of some major factors that may affect the helpfulness of a review including the areas of expertise of the reviewer the writing styles the timeliness of the reviews the length of the reviews etc We provide a detailed analysis of those factors and explain their effects on the helpfulness of reviews We then develop a non-linear regression model that takes all important factors into consideration serving as a basis for helpfulness prediction Extensive experiments were conducted on the IMDB dataset demonstrating the effectiveness of the proposed approach To make our discussions and results more concrete in this paper we use movie reviews in the past two years 2006-2007 collected from the IMDB Website as a case study However our approach is general enough to be easily adapted to handling other types of online reviews To summarize we make the following contributions in this paper  We carefully analyze the possible factors that might affect the helpfulness of reviews and identify three most in\037uential ones namely reviewer expertise writing style and timeliness  We develop a mathematical model that is able to capture all of the three important factors for helpfulness prediction  We conduct extensive experiments on a movie dataset to verify the effectiveness of our approach The rest of the paper is organized as follows In Section 2 we de\036ne the prediction problem and provide a detailed analysis of the major factors that affect the helpfulness of reviews In Section 3 we propose a regression model based on radial basis functions Experimental results are presented in Section 4 Section 5 provides a review of related work Section 6 concludes this paper and discusses directions for future work 2 Problem Deﬁnition and Observations In this section we 036rst formally de\036ne the problem of helpfulness prediction and then analyze the factors that may affect the helpfulness of a review which will provide the basis for the proposal of the model in the next section 2.1 Problem deìnition The goal of this research is to develop a model that can accurately predict the helpfulness of a review For a given review its helpfulness H is de\036ned as the expected fraction of people who will 036nd the review helpful That is H is a number falling in the range 0    and greater values of H imply higher helpfulness As in any prediction tasks the prediction model will be obtained based on available training data which consist of reviews and related product information Let the set of reviewers authors of the reviews concerned be U thesetof movies be M  the set of reviews be D  then each review can be represented as a quadruple R  u d m t  where u 002U denotes the reviewer d 002D represents the review m 002M represents the movie for which the review is written and t indicates the number of days elapsed from the movie release to the time the review is published For each movie in M  assume that the genres it falls in are also available The helpfulness of a review in the training data can be approximated by the tally attached to that review which takes the form of  x out of y people found the following review helpful  That is H  x y  As an effective indicator of the public opinions this evaluation metric has also been widely adopted in previous product review helpfulness studies 19 T o m ai nt ai n t he rob u s t nes s of t h e p redi ct i o n model in this study we only consider reviews with at least 10 votes i.e y 003 10  2.2 Observations In order to develop an effective model for helpfulness prediction we must carefully analyze the important factors that may affect a reviewís helpfulness rating To this end we have examined the reviews on several popular Websites including CNET Amazon and IMDB and conducted preliminary experiments to evaluate the various factors involved Our efforts reveal that the following are among the most important factors 1 Reviewer Expertise  Product reviews often involve personal experience thoughts and concerns Also it is common that different reviewers demonstrate expertise on different types of products For example reviewers fond of science 036ctions are likely to be familiar with and produce good reviews on sci-\036 movies like Star Wars and The Matrix butmaybelesspro\036cient in writing reviews for American Zeitgeist and An Inconvenient Truth  which fall into the category of documentaries Those preferences and expertise might be well re\037ected through reviews they compose which we must take into consideration when building the prediction model 2 Writing Style  Due to the large variation of the reviewers background and language skills the online reviews are of dramatically different qualities Some 
444 
444 


0 10 20 30 40 50 60 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8   Day Helpfulness   Pirates of the Caribbean  Casino Royale  Figure 1 An example of review helpfulness vs time of review reviews are highly readable and therefore tend to be more helpful whereas some reviews are either lengthy but with few sentences containing authorís opinions or snappy but 036lled with insulting remarks Simply ignoring such differences in readability an d style may produce misleading estimates of the review quality A proper representation of such difference must be identi\036ed and factored into the prediction model 3 Timeliness  In addition to the available review content most online reviews are also associated with a particular time stamp which indicates when the review is posted In general the helpfulness of a review may signi\036cantly depend on when it is published For instance research shows that a quarter of a motion pictureís total revenue comes from the 036rst two weeks 4 which m eans a timely r e v ie w m ight be es pecially valuable for users seeking opinions about the movie As a concrete example Figure 1 shows the average helpfulness of reviews versus the time the reviews are published number of days since the release for two movies Pirates of the Caribbean Dead Man’s Chest and Casino Royal  The average helpfulness numbers presented here are 14-day moving averages in order to smooth-out the short-term irregularities and show the overall trend It is evident that the general trend is that the average helpfulness of movie reviews declines as time passes by In addition some previous studies made similar observations that product reviews written early tend to get more user attention on e-commerce websites such as Amazon 8  T his f urther con\036rms our hypothesis that timely reviews are usually more helpful We have also considered other possible factors that may affect the helpfulness values e.g length of the review polarity of the review the average rating of all reviews on the movie etc However none of them shows clear correlation with the value of helpfulness and the detailed examination is available elsewhere 12  O ther f actors  s u ch as s e rv er side weblogs indicating how many users read but did not respond to the helpfulness ques tion might also facilitate the prediction However they are not considered in our study due to the data availability issue 3 Predicting Helpfulness Based on the observations from the previous section we propose a model that accounts for these three important factors Once trained this model can be used for predicting the helpfulness of a given review In the following discussions we will use the IMDB movie data as a case study although the model can be easily applied to other types of review data Since radial basis functions RBF are used in the modeling of both the expertise factor and the writing style factor in the next subsection a brief introduction of RBF is in order After that we will analyze how to model each factor mentioned in the previous section and then present the nonlinear regression model with all those factors incorporated followed by a description of the training algorithm 3.1 Radial basis functions Function approximation is an important component to solving prediction problems de\036ned over both continuous and discrete spaces A powerful function approximator will not only accurately represent a value for a state it has experienced but also generalize values to nearby states it has not experienced before The most common type of approximator is the linear approximator It has the bene\036t of being straightforward and involving lower computational cost but it is obviously unreliable if the true relation between the inputs and the output is nonlinear One then has to rely on non-linear approximators such as RBF Radial basis functions have the advantage of being much simpler than other popular function approximators such as multilayer perceptron neural networks but still serving as a universal function approximator They are generally used when local properties of the functional relationship needs to be captured as is the case in the modeling of reviewer expertise and writing style Due to its high 037exibility radial basis functions have been widely used in many areas including 036nance and image processing 2 A radial basis function is a real-valued function whose value depends only on the distance of the input vector x from some center point   In the most general form the RBF 002  x    002  f 002  x    T 002  1  x    003 where f is the function used Gaussian Cauchy etc and 002 is the metric The term  x    T 002  1  x    represents the distance between the input x and the center  in the metric de\036ned 
445 
445 


by 002  Here we choose the distance metric to be Euclidean In this case 002  003 2 I for some scalar radius 003  Hence 002  x    002  f 004  x    T  x    003 2 005  1 The function f can take various forms In this study we choose the commonly used Gaussian RBF f  y  e  y  and y    x    T  x    002 2 where 003 is also called the spread of the RBF Intuitively the further away x is from the center   the smaller the function value is and the function peaks at the center when x    In addition the value of the spread 003 determines the tightness of the RBF i.e how fast the function value falls off when the input x gets further away from the center Multiple RBFs can be combined to build up function approximations of the form g  x  k 006 i 1 a i 002  x   i  002 i   2 where the approximation function g  x  is represented as a weighted sum of k radial basis functions each with a different center  i  a metric 002 i  and a weight a i  Such function approximation models are sometimes referred to in the literature as radial basis function networks Figure 2 shows an example of using 3 radial basis functions to approximate a function In this example one would like to 036t a function to the scattered data points Although in our model for helpfulness prediction we will deal with multi-dimensional input for illustration purpose this example deals with onedimensional input The 036tted function represented by the solid line can be obtained by taking the weighted sum of the three individual RBFs Fitting the data with the function involves determining the centers and spreads of the RBFs as well as the weight of each RBF 3.2 Modeling expertise As discussed in Section 2 the helpfulness of a review depends in part on the level of expertise of the reviewer on the product movie concerned For example for a given reviewer if his past reviews on a certain set of movies denoted by A  are rated very high while his reviews on some other movies denoted by B  are very low then we have reasons to expect that a new review by this reviewer will be considered more helpful if the movie concerned is more similar to the movies in A than to those in B  In order to quantify the similarity we 036rst need to choose the right features to represent each movie To this end we use the genres provided by IMDB to represent each movie As an example the movie Casino Royale is labeled by IMDB as Action Adventure and Thriller which can be used to represent the movie for our purpose 000 3 000 2 000 1 0 1 2 3 000 0.2 0 0.2 0.4 0.6 0.8 1 1.2 1.4   Input x Function value f\(x Figure 2 Using radial basis functions for function approximation The solid line represents the tted function and the three RBFs are plotted with dotted lines Formally each movie is represented by a m dimensional vector x  x 1 x 2 x m  where m is the number of different genres available for all movies Each dimension corresponds to one genre and x i 1 004 i 004 m  takes the value of 1 l  if the movie belongs to the corresponding genre where l is the number of genres the movie falls into and 0 otherwise Note that due to the normalization factor l  007 m i 1 x i 1  The next step is to measure the similarity of a given movie to movies that have been reviewed by the same reviewer and relate this measure to the helpfulness score We choose to approximate the relationship using RBFs If we were to predict the helpfulness of a review based solely on the reviewer expertise factor then we would 036t the following regression model on the training data  H 1  k 1 006 i 1 u i 002  x   i 003 i   3 where  H 1 is the estimated helpfulness score x is the feature vector representing the movie k 1 is the number of centers in the RBF network  i and 003 i are the center and spread of the i th RBF respectively and u i is the weight of the i th RBF Since we represent each movie using a feature vector based on its genres each center can be considered as corresponding to one cluster of movies that are similar to each other in terms of their genres The helpfulness of a given movie is thus the weighted sum of the distance between the movie to those centers In this way the reviewerís expertise on different clusters of movies can be naturally captured in that similar movies will have similar distances to the centers and therefore have similar helpfulness scores 
446 
446 


3.3 Modeling writing style A previous study 19 has s ho w n t h at t h e l i ngui s t i c s t yl e can be a very good indicator of the utility of the review In fact shallow syntactical features like part-of-speech provide more predicting powers than deeper features at the lexical level Thus we choose to label the part-of-speech of the words contained in the reviews with a 036xed set of tags using LingPipe 1  a suite of Java libraries for the linguistic analysis of natural language For each review we parse it using the LingPipe tagger and count the number of words with each tag Those counts are further normalized by dividing them with the word count of the review The resulting numbers form a vector denoted by y  with each number corresponding to one dimension This vector y is used as a representation of the review for the purpose of modeling writing styles We again use a radial basis function network to model the relationship bet ween the feature vector y and the helpfulness of the review with each RBF explaining part of the functional relationship and the weights indicating the contribution of each RBF Formally if we were to predict the helpfulness solely based on the writing style the regression model we would like to use is  H 2  k 2 006 i 1 v i 004  y  002 i 005 i   4 where  H 2 is the estimated helpfulness v i  002 i and 005 i are the weight center and the spread of the i th RBF respectively and k 2 is the number of RBFs Of course the writing style is only one of the factors affecting the helpfulness of a review Therefore the model in Equation 4 will be combined with other factors in the complete model we propose 3.4 Modeling timeliness Our analysis in Section 2 has shown that there is a strong correlation between the helpfulness of a review and when it is published Having observed the trend for a large number of movies we hypothesize that the helpfulness of a movie review is subject to exponen tial decay with respect to time Therefore we propose the following model for movie reviews if the prediction of helpfulness were to be done only based on the timeliness  H 3  e  003  t  t 0  d  5 where  H 3 is the estimated helpfulness t 0 is the release time of the movie t is the time when the review is published and 006 and d are parameters in the model to be estimated Intuitively 006 controls the rate of decay in the helpfulness as we move further away in time from the movie release 1 http://alias-i.com/lingpipe 3.5 The complete model Now that we have built the regression model for each individual factor we are ready to propose the complete model that incorporates all of the above factors The idea is to consider the helpfulness score a weighted sum of the three individual models as shown below  H  p k 1 006 i 1 u i 002  x   i 003 i  q k 2 006 i 1 v i 004  y  002 i 005 i  r  e 025 002  t 025 t 0  d  where p  q and r are the weights of the three components Note that the above equation can be further simpli\036ed as the weights p  q and r can be absorbed by the individual components For example p 007 k 1 i 1 u i 002  x   i 003 i  can be rewritten as 007 k 1 i 1 u 002 i 002  x   i 003 i  where u 002 i  p  u i and r  e  003  t  t 0  d can be rewritten as w  e  003  t  t 0  where w  r  e d  Therefore the model can be written in a more concise form  H  k 1 006 i 1 u i 002  x   i 003 i  k 2 006 i 1 v i 004  y  002 i 005 i  w  e 025 002  t 025 t 0  6 where the notations u i and v i are overloaded for the sake of brevity with them actually referring to u 002 i and v 002 i as de\036ned above The model given in Equation 6 makes it possible to capture all of the factors discussed in this section with the weights  u i  k 1 i 1   v i  k 2 i 1 and w controlling the contribution of each factor to the helpfulness score 3.6 Parameter estimation We now develop the algorithm that can be used to estimate the model parameters based on the training data movie reviews Assume that the training data consists of N reviews and for each review j 1 004 j 004 N   x j  y j and t j can be obtained as well as the true helpfulness score H j  The set of parameters in the model include 1 the weights  u i  k 1 i 1   v i  k 2 i 1 and w  2 the centers   i  k 1 i 1 and  002 i  k 2 i 1  3 the spreads  003 i  k 1 i 1 and  005 i  k 2 i 1 and 4 the decay rate 006  The values of k 1 and k 2 are supplied by the user The goal of training is to estimate the parameters such that the sum of squared error SSE between the true values and the model output values is minimized i.e we would like to minimize 007  1 2 N 006 j 1 010 2  7 
447 
447 


where 010  H j   H j  The optimization can be done through the method of steepest descent By computing the partial derivatives of Equation 7 we can apply the following rules to iteratively update the values of the parameters as follows Let  011 u  011 v  011 w   be the user-de\036ned learning rate for parameters  u i  v i  w   in the model 1 For the weights we have u new i  u old i  004 u 005\006 005u i  u old i  004 u N 006 j 1 007\010  x j   old i 002 old i   v new i  v old i  004 v 005\006 005v i  v old i  004 v N 006 j 1 007\011  y j  002 old i 012 old i   w new  w old  004 w 005\006 005w  w old  004 w N 006 j 1 007e 025 002 old  t j 025 t 0 j   2 For the centers we have  new   old  004  005\006 005    old  2 004   old N 006 j 1 007\010  x j   old i 002 old i  x j   old i  002 old  2  002 new  002 old  004 002 005\006 005 002  002 old  2 004 002 002 old N 006 j 1 007\011  y j  002 old i 012 old i  y j  002 old i  012 old  2  3 For the spreads let 012  1 002 2 and 013  1 012 2  and we have 013 new  013 old  004 003 005\006 005\013  013 old  004 003 u old i N 006 j 1 007\010  x j   old i 013 old i    x j   old i  T  x j   old i  014 new  014 old  004 004 005\006 005\014  014 old  004 004 v old i N 006 j 1 007\011  y j  002 old i 014 old i    y j  002 old i  T  y j  002 old i  4 Finally for the decay rate 006 wehave 003 new  003 old  004 002 005\006 005\003  003 old  004 002 w old N 006 j 1  t j  t 0 j  e 025 002  t j 025 t 0 j  3.7 Proliìc vs non-proliìc reviewers Recall that in modeling the reviewer expertise as described in Section 3.2 we rely on the genres of the movies the reviewer has commented and the corresponding helpfulness scores This requires suf\036cient past reviews of the reviewer in order to achieve meaningful results In reality some reviewers may have written only a few or no reviews or the reviews a reviewer has written may not be present due to data availability issues We therefore make the distinction between proli\036c reviewers and non-proli\036c reviewers and revise the model correspondingly We call a reviewer a proli\036c reviewer if the number of reviews authored by him/her in the data set exceeds a certain threshold T and non-proli\036c otherwise For proli\036c users we simply use the model described in Section 3.5 whereas for non-proli\036c ones we need to drop the 036rst term regarding reviewer expertise in the model as we do not have suf\036cient grounds to make meaningful inference in that regard In that case the model becomes  H  k 2 006 i 1 v i 004  y  002 i 005 i  w  e  003  t  t 0   8 Note that since the above model does not involve any information regarding individual reviewers a common model can be trained for all of the non-proli\036c reviewers The parameter estimation can be done using the update formulae presented in Section 3.6 It is worth pointing out that the distinction between proli\036c and non-proli\036c reviewers is due to data availability we do not assume that the reviews written by proli\036c reviewers are more helpful than those written by non-proli\036c reviewers 4 Empirical Study We conducted extensive experiments on the IMDB data set to evaluate the effectiveness of the proposed prediction model and study the behavior of the model as we change the user-tunable parameters 4.1 Experiment settings The movie review data set was obtained from the publicly accessible IMDB Website Speci\036cally we collected the reviews for 504 movies released in the United States during the period from January 6 2006 to November 21 2007 We intentionally selected the time that is not very close to the present time in the hope that the voting of helpfulness has stabilized as less and less reviews are expected to appear as time increases across the whole time span To model reviewer expertise we also collected the genre labels for each movie In total 94  919 reviews were collected and the number of rev iew entries collected for each movie ranges from 2  152 for Superman Returns  t o 2 for Absolute Wilson  Thos e r e v i e w s w e re pos t e d by 56,588 different reviewers Note that we only collected reviews posted by reviewers from the US as it helps to ensure the consistency in the release time it is common for a movie to be released on different dates in different countries The total number of genres involved are 27 
448 
448 


0 500 1000 1500 2000 2500 0 100 200 300 400 Number of Reviews Number of Movies a Distribution of the number of reviews 0 10 20 30 0 50 100 150 200 250 300 Genre Number of Movies b Distribution of the number of movies per genre Figure 3 The distribution properties of the data Figure 3\(a and b show the distributions of the number of reviews available for movies and the number of movies per genre respectively To ensure the robustness of the prediction model we only use the reviews with at least 10 votes Also for the purpose of training and testing only the reviews with a helpfulness score available i.e reviews with a label of the form  x out of y people found this review helpful  are used The number of such movie reviews is 22,819 The movie information genres for each movie and the review data are indexed using Apache Lucene 2 For each review its feature vectors are obtained as described in Section 3 and we use 10-fold cross validation to evaluate our approach 4.2 Evaluation We evaluate the effectiveness of the proposed model using two metrics as we anticipate that the model will be used in different ways First the model can be used to predict the helpfulness of reviews directly so we would like to measure the deviation of the predicted value from the true value We call this a prediction problem Second the model can be also used to help retrieve only those reviews that are considered helpful i.e the reviews having a predicted helpfulness higher than a certain threshold We call this a classi\036cation or retrieval problem Two metrics which were used in previous literatures 19 5  a re adopted to e v aluate the p redication accurac y in thos e two scenarios respectively namely the Mean Squared Error MSE for the prediction problem and the F-measure for the classi\036cation problem Speci\036cally for each review in the test set we make a prediction for its helpfulness and compute the squared deviation between the predicated value and the true helpfulness MSE is de\036ned as the sum of all the deviations divided by the total number of predictions That is 2 http://lucene.apache.org MSE  1 n n 006 i 1  H i   H i  2  9 where n is the number of reviews in the test set Note that lower MSE values indicate higher prediction accuracy To measure the performance using F-measure we consider a review as helpful if its helpfulness score is greater than a given threshold 014  In our experiments we set 014 0  5  4.3 Parameter selections In the prediction model there are several user-chosen parameters that provide the 037exibility to 036ne tune the model for optimal performance They include the threshold T to separate proli\036c users and non-proli\036c users the number of RBFs in the RBF network k 1 and k 2  and a threshold 014 determining whether a review is helpful We now study how the choice of these parameter va lues affects the prediction accuracy 4.3.1 Effect of T Recall that in Section 3 we use a threshold T to distinguish a proli\036c reviewer from a non-proli\036c reviewer based on how many reviews in the data are authored by that reviewer We train different models for the two types of reviewers as discussedinSection3 With 036xed values of k 1 and k 2  k 1 3 and k 2 10  we vary T  and observe the changes in accuracy Similar trends can be observed for other values of k 1 and k 2 As shown in Table 1 as T increases from 10 to 30  the prediction performance improves in both F-measure for the classi\036cation problem and MSE for the prediction problem  and at T 30  it achieves the best accuracy with Fmeasure=0.7116 and MSE=0.0332 This implies that accumulating more reviews for a given author allows our model to better capture the effects that in\037uence the helpfulness which leads to more accurate predictions In addition the accuracy for proli\036c reviewers is much superior to that for non-proli\036c reviewers across different values of T  indicating the effectiveness of the reviewer expertise factor in the model for proli\036c reviewers 4.3.2 Effects of k 1 and k 2 We then vary the values of k 1 and k 2  with T 036xed at 30 to study how the number of RBFs affects the prediction accuracy on proli\036c reviewers We do not consider non-proli\036c reviewers in this experiment as the features describing reviewer expertise are not available for non-proli\036c reviewers and thus k 1 is not required for the corresponding model The effect of k 2 is similar on the two types of reviewers and therefore only the results on the proli\036c reviewers are presented here 
449 
449 


T MSE F-measure  of Reviews  of Reviewers 10 P 0.0486 0.6717 2378 109 N 0.0768 0.4307 20441 17266 15 P 0.0392 0.6748 1814 65 N 0.0632 0.4307 21005 17310 20 P 0.0386 0.6886 1258 33 N 0.0668 0.4364 21561 17342 25 P 0.0354 0.6989 1079 25 N 0.0661 0.4363 21740 17350 30 P 0.0332 0.7116 912 19 N 0.0658 0.4365 21907 17356 Table 1 Effect of T  N and P refer to nonproliﬁc and proliﬁc reviewers respectively We 036rst vary the value of k 1  and observe from Figure 4\(a and b that there is a large improvement in accuracy when k 1 increases from 1 to 2 and the model achieves its best performance with k 1 3  This implies that introducing multiple components to ana lyze the reviewer expertise can greatly improve the prediction accurary However after k 1 past a thresold the accuracy tends to decrease This might be due to over-\036tting the training data with more RBFs Nonetheless the accuracy remains stable for a wide range of k 1 values indicating the insensitivity of the model with respect to the choice of k 1 values It is also worth noting that the trend in accuracy remains the same regardless of the choice of k 2  1 2 3 4 5 6 7 8 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 k 1 MSE   k 2 5 k 2 10 k 2 15 a MSE 1 2 3 4 5 6 7 8 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 k 1 F\025measure   k 2 5 k 2 10 k 2 15 b F-measure Figure 4 Effect of k 1 on proliﬁc reviewers Similarly we 036x the values of T and k 1 andvary k 2 from 1 to 12  As shown in Figure 5 a and b there is also an optimal choice of k 2  which is 10 Similar to the case of 0 2 4 6 8 10 12 0 0.05 0.1 0.15 0.2 0.25 k 2 MSE   k 1 2 k 1 3 k 1 4 a MSE 0 2 4 6 8 10 12 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 k 2 F\025measure   k 1 2 k 1 3 k 1 4 b F-measure Figure 5 Effect of k 2 on proliﬁc reviewers 30 40 50 60 70 80 0 0.2 0.4 0.6 0.8 1 002 F\025measure   Prolific Non\025prolific Figure 6 Effect of 014 k 1  the accuracy remains quite stable over a wide range of k 2  which again demonstrates that the model is not sensitive to the choice of parameter values 4.3.3 Effect of 014 In classifying a review as helpful or not helpful we use a threshold 014  Figure 6 shows the effect of the value of 014 on the accuracy Clearly smaller 014 values tend to lead to better accuracy This is as expected because a larger 014 means less reviews can be classi\036ed as helpful and is therefore more restrictive making accurate cl assi\036cations more dif\036cult 4.4 E\003ects of individual factors In our study three factors that may affect the review helpfulness are considered and we propose a non-linear re 
450 
450 


gression model to incorporate them into one model Here we study how the three factors affect the prediction of helpfulness individually That is how would the model perform if we choose to use only one of the factors for prediction In Section 3 we discussed three models Equations 3 4 and 5 corresponding to the three factors For the experiments we train the three individual models as presented in Section 3 with the corresponding feature vectors and measure the accuracy of each one In particular we let k 1 3 and k 2 10 in this experiment and the results are shown in Table 2 Component MSE F-measure reviewer expertise 0.1912 0.5179 writing style 0.1937 0.2433 timeliness 0.1004 0.6482 all of the three 0.0332 0.7116 Table 2 Individual effects on prediction Apparently considering the timeliness factor only yields the best results in both MSE and F-measure among the three factors This coincides with our intuition that a timely review can be very helpful for customers to evaluate the product of interest Only considering the writing style gives the worst performance of the three implying that it has less predictive power compared with reviewer expertise and timeliness Note that the results obtained by considering only one factor are not as good as considering all the factors together 4.5 Comparison with alternative method To demonstrate the effectiveness of our proposed model we compare it against a baseline model that use linear regression LR For each review we obtain the feature vectors  x  y t  corresponding to each factor in the same way as described in Section 3 and concatenate them together to form one vector r  Then the linear regression model can be written as  H l  003 T r  b where 006 is the coef\036cient vector and b is the intercept This model can be 036t to the training data using standard linear least squares method We conducted a series of experiments with different T values and compare the performance of the LR model with our proposed method As shown in Figure 7 it is clear that our proposed method is much more accurate than the LR model for both proli\036c and non-proli\036c reviewers 5 Related Work 5.1 Review mining With the rapid growth of online reviews automatic review mining has attracted a lot of research attention Early work in this area was primarily focused on determining the 10 15 20 25 30 35 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 T MSE   RBF Prolific Linear Prolific RBF Non\025pro Linear Non\025pro a MSE 10 15 20 25 30 35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 T F\025measure   RBF Prolific Linear Prolific RBF Non\025pro Linear Non\025pro b F-measure Figure 7 Comparison with linear regression model semantic orientation of reviews Among them some of the studies attempt to learn a positive/negative classi\036er at the document level 16 15 w h i l e ot hers w o rk at a 036 ner l e v el and use words as the classi\036cation subject 18  Pushing further from the explicit two-class classi\036cation problem Liu et al 7 b u ild a frame w o rk to compare c onsumer opinions of competing products using multiple feature dimensions Liu et al 11 a s s u me t h at s e nt i m ent consists of multiple hidden aspects and use a probability model to quantitatively measure the relationship between sentiment aspects and reviews Our method departs from classic review mining approaches in that ultimately we want to examine the importance of these opinions which is a new and important research problem In some sense  determining the sentiment and helpfulness of reviews a re orthogonal to each other and could be modeled independently One recent work that is closely related to our study attempts to examine the economic impact of the online reviews 5 That approach mainly focuses on quantifying the extent of which the textual content especially the subjectivity of each review affects product sales on a market such as Amazon while our method aims to build a more fundamental model for review helpfulness prediction Another relevant study in this 036eld analyzes spams that exist in online reviews 8  I n p a r ticular their work presents a categorization of review spams and proposes some novel strategies to detect different types 
451 
451 


of spams Our work can be considered complimentary to that work in that the spam 036ltering model can be used as a preprocessing step in our approach 5.2 Authority and importance mining Identifying the quality of Web documents has received a lot of attention particular ly because of its application to search engines PageRank and HITS are two popular link-based ranking algorithms to determine the importance of web pages 10 14 The H ITS a l gori t h m i s b as ed on the observation that a good hub usually points to good authorities and a good authority usually points to good hubs The Pagerank algorithm doesnít distinguish hub and authority pages Instead it estimates the importance of the web pageís neighbours and the authority of the page is considered proportional to this value Motivated by this idea various algorithms have been proposed to discover the authorities or leaders in the Web domain 9 17 13  Note that our approach is different from above methods in that we use semantic inform ation of web document rather than link structures for evaluating the helpfulness of online reviews 6 Conclusions and Future Work In this paper we have considered the important problem of predicting the helpfulness of reviews We provided a detailed analysis of the major factors affecting the helpfulness of a review and proposed a nonlinear model based on radial basis functions for helpfulness prediction Extensive experiments on the IMDB data set have con\036rmed the effectiveness of the proposed model Our study in this paper has focused on the movie reviews but our approach is general enough to be easily adapted to other domains as well For example if we would like to handle product reviews on Amazon or CNET we can simply replace the genres of movies with the categories of products and the writing style and timeliness can still be modeled in the similar way as described in Section 3 This study presents the 036rst step in modeling the helpfulness of reviews For future work we plan to study the related ranking problem i.e how do we rank the reviews based on the helpfulness One way to do this is to rank the reviews based on their predicted helpfulness but we can also develop a model to directly predict the set of most helpful reviews Another possible direction for future work is to incorporate existing votes as an indicator of the future helpfulness and build an adaptive model which can automatically update the predication value of helpfulness as new reviews come in Besides we also plan to incorporate collaborative 036ltering methods such as 6  t o h el p b ui l d a p er sonalized helpfulness prediction model References 1 N  A r c hak A  G hose and P  G  I pei r ot i s  S ho w m e t he money deriving the pricing power of product features by mining consumer reviews In KDD  pages 56ñ65 2007 2 J  C  C arr  R K Beatso n  J B Ch errie T  J M itch e ll W  R Fright B C McCallum and T R Evans Reconstruction and representation of 3D objects with radial basis functions In SIGGRAPH 01  pages 67ñ76 2001 3 K  D a v e S  L a w r ence and D  M  P ennock Mi ni ng t h e peanut gallery opinion extraction and semantic classi\036cation of product reviews In WWW  pages 519ñ528 2003 4 C  D el l a r o cas N  F  A w a d and X  M  Z hang E xpl or i n g t he value of online reviews to organizations Implications for revenue forecasting and planning In ICIS  pages 379ñ386 2004 5 A  G hose a nd P  G  I p ei r o t i s  D esi gni ng no v e l r e v i e w r anki ng systems predicting the usefulness and impact of reviews In ICEC  pages 303ñ310 2007 6 T  H of mann and J  P uzi c ha L at ent c l a ss model s f o r c ol l a borative 036ltering In IJCAI  pages 688ñ693 1999 7 M  H u a nd B  L i u Mi ni ng and s ummar i zi ng cust omer r e views In KDD  pages 168ñ177 2004 8 N  J i ndal a nd B  L i u O p i n i o n s pam a nd anal ysi s  I n WSDM  pages 219ñ230 2008 9 P  J u r c z y k a n d E Ag ic h t e i n  Disc o v e rin g a u th o r itie s i n q u e stion answer communities by using link analysis In CIKM  pages 919ñ922 2007  J M K l ei nber g  A ut hor i t a t i v e sour ces i n a hyper l i n k e d e nvironment J ACM  46\(5\:604ñ632 1999 11 Y  Liu  X Hu an g  A An  a n d X Y u  A RS A a s en timen taware model for predicting sales performance using blogs In SIGIR  pages 607ñ614 2007  Y  L i u X  H u ang A  A n  a nd X  Y u  M i n i n g h el pf ul ness of online reviews Technical Report CSE-2008-05 Department of Computer Science and Engineering York University 2008  S  N a kaj i m a J T a t e mur a  Y  H i no Y  H a r a  a nd K  T a naka Discovering important bloggers based on analyzing blog threads In WWW  2005  L  Page S  B r i n R  Mot w ani  and T  W i nogr ad T he pager ank citation ranking Bringing order to the web Technical report Stanford Digital Library Technologies Project 1998  B Pang and L  L ee A s ent i m ent a l e ducat i on sent i m ent analysis using subjectivity summarization based on minimum cuts In ACL  page 271 2004  B  Pang L  L ee and S  V ai t hyanat h an T humbs up S entiment classi\036cation using machine learning techniques In EMNLP  2002  X  S ong Y  C h i  K  H i no and B  T seng I d ent i f yi ng opi ni on leaders in the blogosphere In CIKM  pages 971ñ974 2007  P  D  T u r n e y  T humbs up or t humbs do w n  semant i c or i entation applied to unsupervised classi\036cation of reviews In ACL  pages 417ñ424 2001  Z  Z h ang a nd B V a radaraja n Utility scorin g of product reviews In CIKM  pages 51ñ57 2006  L  Z huang F  Ji ng and X  Y  Z hu Mo vi e r e v i e w m i n i n g a nd summarization In CIKM  pages 43ñ50 2006 
452 
452 


 11 This model specifies that covariates act multiplicatively on time t r than on the hazar d function.  That is, we assume a baseline hazard function to exist and that the effects of the covariates are to alter the rate at which an individual proceeds along the time axis.  In other words, the covariates z accelerates or decelerates the time to failure Kalbfleisch and Prentice 2002, Lawless 2003  It should be pointed out that the distribution-based regression models for exponential and Weibull distributions in the previous section are th e special cases of both PHM and AFT.  This correspondence is not necessarily true for models based on other distribu tions. Indeed, two-parameter Weibull distribution has the uniq ue property that it is closed under both multiplication of fa ilure time and multiplication of the hazard function by an arbitrary nonzero constant Lawless 2003, Kalbfleisch & Prentice 2002, Klein Moeschberger 2003  2.6. Counting Process and Survival Analysis   In the previous sections, we introduced censoring and survival analysis models th at can handle the censored information; however, we did not discuss how the censored information is processed.  Accommodating and maximally utilizing the partial information from the censored observations is the most challenging and also the most rewarding task in survival anal ysis.  This also establishes survival analysis as a unique fiel d in mathematical statistics Early statistical inferences for censored data in survival analysis were dependent on asymptotic likelihood theory Severini 2000\ Cox \(1972, 1975\ proposed partial likelihood as an extension to classical maximum likelihood estimation in the context of hi s proportional hazards model as a major contribution. Asymptotic likelihood has been and still is the dominant theory for developing survival analysis inference and hypothesis testing methods \(Klein and Moeschberger 2003, Severini 2000\. There are many monographs and textbooks of survival analysis containing sufficient details for applying survival analysis \(Cox and Oakes 1984, Kalbfleisch and Prentice 1980, 2002, Lawless 1982, 2003, Klein and Moeschberger, 2003\. A problem with traditional asymptotic lik elihood theory is that the resulting procedures can become very complicated when handling more complex censoring mechanisms \(Klein Moeschberger 2003\. A more elegant but requiring rigorous measure-theoretic probability theo ry is the approach with counting stochastic processes and the Martingale central limit theorems.  Indeed, this approach was used by Aalen 1975\ to set the rigorous mathematical foundation for survival analysis, and later further developed and summarized by Fleming and Harrington \(1991\, Andersen et al. \(1993\several research papers.  In reliability theory Aven and Jensen \(1999\ dem onstrated such an approach by developing a general failure model, which we briefly introduced in Section 1.2. However, the counting process and Martingale approach require measure theoretic treatments of probability and st ochastic processes, which is often not used in engineering or applied statistics.  A detailed introduction of the topic is obviously beyond the scope of this paper, and we only present a brief sketch of the most important concepts involved.  Readers are referred to the excellent monographs by Andersen et al. \(1993 Fleming and Harrington \(1991\ Aven and Jensen \(1999\ for comprehensive details, and Kal bfleisch and Prentice \(2002 Klein and Moeschberger \(2003\, Lawless \(2003\ for more applicationñoriented treatments The following discussion on this topic is drawn from Klein and Moeschberger \(2003  A counting stochastic process N  t  t 0 possesses the properties that N  0 ro and N  t   with probability one. The sample paths of N  t ht continuous and piecewise constant with jumps of size 1  step function In a right-censored sample, \(we assume only right censoring in this section N i  t  I  T i t  i   which keep the value 0 until individual i fails and then jump to 1  are counting processes. The accumulation of N i  t ocess     1 t N t N n i i is again a counting process, which counts the number of failures in the sample at or before time t   The counting process keeps track of the information on the occurrences of events,   for instance, the history information such as which individual was censored prior to time t and which individual died at or prior to time t as well as the covariates information This accumulated history information of the counting process at time t is termed filtration at time t denoted by F t For a given problem F t  rests on the observer of the counting process.  Thus, two observers with different recordings at different times will get different filtrations.  This is what Aven and Jensen 1999\ referred to as different information levels or the amount of actual available information about the state of a system may vary  If the failure times X i and censoring times C i  are independent,  then the probability of an event occurs at time t given the history just prior to t  F t\n be expressed as  t T if dt t h t C t X dt t C dt t X t P F dt t T t P i i i i i i r t i i r          1     t T if F dt t T t P i t i i r 0   1    51  Let dN  t be the change in the process N  t over a short time interval    t t t Ignoring the neglig ible chance of ties 1   t dN if a failure occurred and 0   t dN otherwise  Let Y  t denote the number of individuals with an observation time T i t Then the conditional expectation of dN  t   dt t h t Y F dt t C dt t X t with ns observatio of number E F t dN E t i i i t              52 


 12 The process  t  Y  t  h  t  is called the intensity process  of the counting process.  It is a stochastic process that is determined by the information contained in the history process F t via Y  t  Y  t  records the number of individuals experiencing the risk at a given time t   As it will become clear that   t is equivalent to the failure rate or hazard function in tr aditional reliability theory, but here it is treated as a stochastic process, the most general form one can assume for it. It is this generalization that encapsulates the power that counting stochastic process approach can offer to survival analysis  Let the cumulative intensity process H  t be defined as   t t ds s t H 0 0      53  It has the property that  E  N  t  F t  E  H  t  F t  H  t   54  This implies that filtration F t, is known, the value of Y  t  fixed and H  t es deterministic H  t is equivalent to the cumulative hazards in tr aditional reliability theory  A stochastic process with the property that its expectation at time t given history at time s < t is equal to its value at s is called a martingale That is M  t martingale if           t s s M F t M E s  55  It can be verified that the stochastic process         t H t N t M 56  is a martingale, and it is called the counting process martingale The increments of the counting process martingale have an expected value of zero, given its filtration F t That is  0      t F t dM E 57 The first part of the counting process martingale [Equation 56  N  t non-decreasing step function.  The second part H  t smooth process which is predictable in that its value at time t is fixed just prior to time t It is known as the compensator  of the counting process and is a random function. Therefore, the martingale can be considered as mean-zero noise and that is obtainable when one subtracts the smoothly varying compensator from the counting process  Another key component in the counting process and martingale theory for survival analysis is the notion of the predictable variation process of M  t oted by    t M It is defined as the compensator of process   2 t M  The term predictable variation process comes from the property that, for a martingale M  t it can be verified that the conditional variance of the increments of M  t  dM  t  h e inc r em ents of   t M That is          t M d F t dM Var t  58  To obtain      t F t dM Var  one needs the random variable N  t  variable with probability   t of having a jump of size 1 at time t The variance of N  t   t  1 t   since it follows b i nom ial distribution    Ignoring the ties in the censored data 2  t  is close to zero and Var  dM  t  F t    t  Y  t  h  t   This implies that the counting process N  t on the filtration F t behaves like a Poisson process with rate  t    Why do we need to convert the previous very intuitive concepts in survival analysis in to more abstract martingales The key is that many of the sta tistics in survival analysis can be derived as the stochastic integrals of the basic martingales described above The stochastic integral equations are mathematically well structured and some standard mathematical techniques for studying them can be adopted  Here, let   t K be a predictable  process An example of a predictable process is the process   t Y Over the interval 0 to t the stochastic integral of su ch a process, with respect to a martingale, is denoted by     0 u dM u K t It turns out that such stochastic integrals them selves are martingales as a function of t and their predictable variation process can be found from the predictable variation process of the original martingale by           0 2 0 u M d u K u dM u K t t 59 The above discussion was drawn from Klein and Moeschberger \(2003 also provide examples of how the above process is applied. In the following, we briefly introduce one of their exampl es ó the derivation of the Nelson-Aalen cumulative hazard estimator  First, the model is formulated as           t dM dt t h t Y t dN  60  If   t Y is non-zero, then               t Y t dM t d t h t Y t dN 61  Let   t I be the indicator of whether   t Y  is positive and define 0/0 = 0, then integrating both sides of above \(61 One get 


 13                     0 0 0 u dM u Y u I u d u h u I u dN u Y u I t t t 62  The left side integral is the Nelson-Aalen estimator of H t           0  u dN u Y u I t H t 63  The second integral on the right side           0 u dM u Y u I t W t 64  is the stochastic integral of the predictable process      u Y u I with respect to a martingale, and hence is also a martingale  The first integral on the right side is a random quantity    t H   t du u h u I t H 0        65  For right-censored data it is equal to   t H  in the data range, if the stochastic uncertainty in the   t W is negligible Therefore, the statistic    t H is a nonparametric estimator of the random quantity    t H   We would like to mention one more advantage of the new approach, that is, the martingale central limit theorem.  The central limit theorem of martingales ensures certain convergence property and allows the derivations of confidence intervals for many st atistics.  In summary, most of the statistics developed with asymptotic likelihood theory in survival analysis can be derived as the stochastic integrals of some martingale.  The large sample properties of the statistics can be found by using the predictable variation process and martingale central limit theorem \(Klein and Moeschberger \(2003  2.7. Bayesian Survival Analysis  Like many other fields of statistics, survival analysis has also witnessed the rapid expansion of the Bayesian paradigm.  The introduction of the full-scale Bayesian paradigm is relative recent and occurred in the last decade however, the "invasion" has been thorough.  Until the recent publication of a monograph by Ibrahim, Chen and Sinhaet 2005\val anal ysis has been either missing or occupy at most one chapter in most survival analysis monographs and textbooks.  Ibrahim's et al. \(2005\ changed the landscape, with their comprehensive discussion of nearly every counterp arts of frequentist survival analysis from univariate to multivariate, from nonparametric, semiparametric to parametric models, from proportional to nonproportional hazards models, as well as the joint model of longitudinal and survival data.  It should be pointed out that Bayesian survival analysis has been studied for quite a while and can be traced back at least to the 1970s  A natural but fair question is what advantages the Bayesian approach can offer over the established frequentist survival analysis. Ibrahim et al 2005\entified two key advantages. First, survival models are generally very difficult to fit, due to the complex likelihood functions to accommodate censoring.  A Bayesi an approach may help by using the MCMC techniques and there is available software e.g., BUGS.  Second, the Bayesian paradigm can incorporate prior information in a natural way by using historical information, e.g., from clinical trials. The following discussion in this subsection draws from Ibrahim's et al. \(2005  The Bayesian paradigm is based on specifying a probability model for the observed data, given a vector of unknown parameters This leads to likelihood function L   D   Unlike in traditional statistics is treated as random and has a prior distribution denoted by   Inference concerning is then based on the posterior distribution which can be computed by Bayes theorem d D L D L D              66 where is the parameter space  The term    D is proportional to the likelihood    D L  which is the information from observed data, multiplied by the prior, which is quantified by   i.e          D L D 67 The denominator integral m  D s the normalizing constant of    D and often does not have an analytic closed form Therefore    D often has to be computed numerically The Gibbs sampler or other MCMC algorithms can be used to sample    D without knowing the normalizing constant m  D xist large amount of literature for solving the computational problems of m  D nd    D   Given that the general Bayesian algorithms for computing the posterior distributions should equally apply to Bayesian survival analysis, the specification or elicitation of informative prior needs much of the atte ntion.  In survival analysis with covariates such as Cox's proportional hazards model, the most popular choice of informative prior is the normal prior, and the most popular choice for noninformative is the uniform Non-informative prior is easy to use but they cannot be used in all applications, such as model selection or model comparison. Moreover, noninformative prior does not harness the real prior information. Therefore, res earch for informative prior specification is crucial for Bayesian survival analysis  


 14 Reliability estimation is influenced by the level of information available such as information on components or sub-systems. Bayesians approach is likely to provide such flexibility to accommodate various levels of information Graves and Hamada \(2005\roduced the YADAS a statistical modeling environment that implements the Bayesian hierarchical modeli ng via MCMC computation They showed the applications of YADES in reliability modeling and its flexibility in processing hierarchical information. Although this environment seems not designed with Bayesian surviv al analysis, similar package may be the direction if Bayesian survival analysis is applied to reliability modeling  2.8. Spatial Survival Analysis  To the best of our knowledge spatial survival analysis is an uncharted area, and there has been no spatial survival analysis reported with rigor ous mathematical treatment There are some applications of survival analysis to spatial data; however, they do not address the spatial process which in our opinion should be the essential aspect of any spatial survival analysis. To develop formal spatial survival analysis, one has to define the spatial process first  Recall, for survival analysis in the time domain, there is survival function   R t t T t S  Pr   68  where T is the random variable and S  t e cumulative probability that the lifetime will exceed time t In spatial domain, what is the counterpart of t One may wonder why do not we simply define the survival function in the spatial domain as   S  s  Pr S s  s R d  R > 0  69  where s is some metric for d-dimensional space R d and the space is restricted to the positive region S is the space to event measurement, e.g., the distance from some origin where we detect some point object.  The problem is that the metric itself is an attribute of space, rather than space itself Therefore, it appears to us that the basic entity for studying the space domain has to be broader than in the time domain This is probably why spatial process seems to be a more appropriate entity for studying  The following is a summary of descriptions of spatial processes and patterns, which intends to show the complexity of the issues involved.  It is not an attempt to define the similar survival function in spatial domain because we certainly unders tand the huge complexity involved. There are several monographs discussing the spatial process and patterns \(Schabenberger and Gotway 2005\.  The following discussion heavily draws from Cressie \(1993\ and Schabenberger and Gotway \(2005  It appears that the most widely adopted definition for spatial process is proposed in Cressie \(1993\, which defines a spatial process Z  s in d-dimensions as    Z  s  s D R d  70  Here Z  s denotes the attributes we observe, which are space dependent. When d = 2 the space R 2 is a plane  The problem is how to define randomness in this process According to Schabenberger and Gotway \(2005 Z  s an be thought of as located \(indexed\by spatial coordinates s  s 1  s 2  s n  the counterpart of time series Y  t  t   t 1  t 2  t n   indexed by time.  The spatial process is often called random field   To be more explicit, we denote Z  s  Z  s   to emphasize that Z is the outcome of a random experiment  A particular realization of produces a surface    s Z  Because the surface from whic h the samples are drawn is the result of the random experiment Z  s called a random function  One might ask what is a random experiment like in a spatial domain? Schabenberger and Gotway \(2005\ offered an imaginary example briefly described below.  Imagine pouring sand from a bucket on to a desktop surface, and one is interested in measuring the depth of the poured sand at various locations, denoted as Z  s The sand distributes on the surface according to the laws of physics.  With enough resources and patience, one can develop a deterministic model to predict exactly how the sand grains are distributed on the desktop surface.  This is the same argument used to determine the head-tail coin flipping experiment, which is well accepted in statistical scie nce. The probabilistic coinflip model is more parsimonious than the deterministic model that rests on the exact \(perfect\but hardly feasible representation of a coin's physics. Similarly deterministically modeling the placement of sand grains is equally daunting.  However, th e issue here is not placement of sand as a random event, as Schabenberger and Gotway 2005\phasized.  The issue is that the sand was poured only once regardless how many locations one measures the depth of the sand.  With that setting, the challenge is how do we define and compute the expectation of the random function Z  s Would E  Z  s   s  make sense  Schabenberger and Gotway \(2005\further raised the questions: \(1\to what distribution is the expectation being computed? \(2\ if the random experiment of sand pouring can only be counted once, ho w can the expectation be the long-term average  According to the definition of expectation in traditional statistics, one should repeat the process of sand pouring many times and consider the probability distributions of all surfaces generated from the repetitions to compute the expectation of Z  s is complication is much more serious 


 15 than what we may often reali ze. Especially, in practice many spatial data is obtained from one time space sample only. There is not any independent replication in the sense of observing several independent realizations of the spatial process \(Schabenberger and Gotway 2005  How is the enormous complexity in spatial statistics currently dealt with? The most commonly used simplification, which has also been vigorously criticized, is the stationarity assumption.  Opponents claim that stationarity often leads to erroneous inferences and conclusions.  Proponents counte r-argue that little progress can be made in the study of non-stationary process, without a good understanding of the stationary issues Schabenberger and Gotway 2005  The strict stationarity is a random field whose spatial distribution is invariant under translation of the coordination.  In other word s, the process repeats itself throughout its domain \(Schabenberger and Gotway 2005 There is also a second-order or weak\ of a random field  For random fields in the spatial domain, the model of Equation \(70  Z  s  s D R d  is still too general to allow statistical inference.  It can be decomposed into several sub-processes \(Cressie 1993             s s s W s s Z  D s  71  where  s  E  Z  is a deterministic mean structure called large-scale variation W  s is the zero-mean intrinsically stationary process, \(with second order derivative\nd it is called smooth small-scale variation   s is the zero-mean stationary process independent of W  s and is called microscale variation  s  is zero-mean white noise also called measurement error. This decomposition is not unique and is largely operational in nature \(Cressie 1993\he main task of a spatial algorithm is to determine the allocations of the large, small, and microscale components However, the form of the above equation is fixed Cressie 1993\, implying that it is not appropriate to sub-divide one or more of the items. Therefore, the key issue here is to obtain the deterministic  s  but in practice, especially with limited data, it is usually very difficult to get a unique  s   Alternative to the spatial domain decomposition approach the frequency domain methods or spectral analysis used in time series analysis can also be used in spatial statistics Again, one may refer to Schabenberger and Gotway \(2005  So, what are the imp lications of the general discussion on spatial process above to spatial survival analysis?  One point is clear, Equation \(69 S  s  Pr S s   s R d is simply too naive to be meaningful.  There seem, at least, four fundamental challenges when trying to develop survival analysis in space domain. \(1\ process is often multidimensional, while time pr ocess can always be treated as uni-dimensional in the sense that it can be represented as  Z  t  t R 1  The multidimensionality certainly introduces additional complications, but that is still not the only complication, perhaps not even the most significant 2\One of the fundamental complications is the frequent lack of independent replication in the sense of observing several independent realizations of the spatial process, as pointed out by Cressie \(1993\\(3\e superposition of \(1 and \(2\brings up even more complexity, since coordinates  s of each replication are a set of stochastic spatial process rather than a set of random variables. \(4\n if the modeling of the spatial process is separable from the time process, it is doubtful how useful the resulting model will be.  In time process modeling, if a population lives in a homogenous environment, the space can be condensed as a single point.  However, the freezing of time seems to leave out too much information, at least for survival analysis Since the traditional survival analysis is essentially a time process, therefore, it should be expanded to incorporate spatial coordinates into original survival function.  For example, when integrating space and time, one gets a spacetime process, such as          R t R D s t s Z d   where s is the spatial index \(coordinate t is time.  We may define the spatial-temporal survival function as   S  s  t  Pr T t  s D  72  where D is a subset of the d dimensional Euclidean space That is, the spatial-temporal survival function represents the cumulative probability that an individual will survive up to time T within hyperspace D which is a subset of the d dimensional Euclidian space.  One may define different scales for D or even divide D into a number of unit hyperspaces of measurement 1 unit  2.9. Survival Analysis and Artificial Neural Network   In the discussion on artificial neuron networks \(ANN Robert & Casella \(2004\noted, "Baring the biological vocabulary and the idealistic connection with actual neurons, the theory of neuron networks covers: \(1\odeling nonlinear relations between explanatory and dependent explained\ariables; \(2\mation of the parameters of these models based on a \(training\ sample."  Although Robert & Casella \(2004\ did not mentioned survival analysis, their notion indeed strike us in that the two points are also the essence of Cox s \(1972\roportional Hazards model  We argue that the dissimilarity might be superficial.  One of the most obvious differences is that ANN usually avoids probabilistic modeling, however, the ANN models can be analyzed and estimated from a statistical point of view, as demonstrated by Neal \(1999\, Ripley \(1994\, \(1996 Robert & Casella \(2004\What is more interesting is that the most hailed feature of ANN, i.e., the identifiability of model structure, if one review carefully, is very similar to the work done in survival anal ysis for the structure of the 


 16 Cox proportional hazards model. The multilayer ANN model, also known as back-propagation ANN model, is again very similar to the stratified proportional hazards model  There are at least two advantages of survival analysis over the ANN.  \(1\val analysis has a rigorous mathematical foundation. Counting stochastic processes and the Martingale central limit theory form the survival analysis models as stochastic integral s, which provide insight for analytic solutions. \(2\ ANN, simulation is usually necessary \(Robert & Casella \(2004\which is not the case in survival analysis  Our conjecture may explain a very interesting phenomenon Several studies have tried to integrate ANN with survival analysis.  As reviewed in the next section, few of the integrated survival analysis and ANN made significant difference in terms of model fittings, compared with the native survival analysis alone The indifference shows that both approaches do not complement each other.  If they are essentially different, the inte grated approach should produce some results that are signifi cantly different from the pure survival analysis alone, either positively or negatively Again, we emphasize that our discussion is still a pure conjecture at this stage   3   B RIEF C ASE R EVIEW OF S URVIVAL A NALYSIS A PPLICATIONS     3.1. Applications Found in IEEE Digital Library  In this section, we briefly review the papers found in the IEEE digital library w ith the keyword of survival analysis  search. The online search was conducted in the July of 2007, and we found about 40 papers in total. There were a few biomedical studies among the 40 survival-analysis papers published in IEEE publications. These are largely standard biomedical applications and we do not discuss these papers, for obvious reasons  Mazzuchi et al. \(1989\m ed to be the first to actively  advocate the use of Cox's \(1 972\ proportional hazards model \(PHM\in engineering re liability.  They quoted Cox's 1972\ original words industrial reliability studies and medical studies to show Cox's original motivation Mazzuchi et al \(1989 while this model had a significant impact on the biom edical field, it has received little attention in the reliability literature Nearly two decades after the introducti on paper of Mazzuchi et al 1989\ears that little significant changes have occurred in computer science and IEEE related engineering fields with regard to the proportional hazards models and survival analysis as a whole  Stillman et al. \(1995\used survival analysis to analyze the data for component maintenance and replacement programs Reineke et al. \(1998 ducted a similar study for determining the optimal maintenance by simulating a series system of four components Berzuini and Larizza \(1996 integrated time-series modeling with survival analysis for medical monitoring.  Kauffman and Wang \(2002\analyzed the Internet firm su rvival data from IPO \(initial public offer to business shutdown events, with survival analysis models  Among the 40 survival analysis papers, which we obtained from online search of the IEEE digital library, significant percentage is the integration of survival analysis with artificial neural networks \(ANN In many of these studies the objective was to utilize ANN to modeling fitting or parameter estimation for survival analysis.  The following is an incomplete list of the major ANN survival analysis integration papers found in IEEE digital library, Arsene et 2006 Eleuteri et al. \(2003\oa and Wong \(2001\,  Mani et al 1999\ The parameter estimation in survival analysis is particular complex due to the requirements for processing censored observations. Therefore, approach such as ANN and Bayesian statistics may be helpful to deal with the complexity. Indeed, Bayesian survival analysis has been expanded significantly in recent years \(Ibrahim, et al. 2005  We expect that evolutionary computing will be applied to survival analysis, in similar way to ANN and Bayesian approaches  With regard to the application of ANN to survival analysis we suggest three cautions: \(1\he integrated approach should preserve the capability to process censoring otherwise, survival analysis loses its most significant advantage. \(2\ Caution should be taken when the integrated approach changes model struct ure because most survival analysis models, such as Cox's proportional hazards models and accelerated failure time models, are already complex nonlinear models with built-in failure mechanisms.  The model over-fitting may cause model identifiability  problems, which could be very subtle and hard to resolve 3\he integrated approach does not produce significant improvement in terms of model fitting or other measurements, which seemed to be the case in majority of the ANN approach to survival analysis, then the extra complication should certainly be avoided. Even if there is improvement, one should still take caution with the censor handling and model identifiability issues previously mentioned in \(1\ and \(2  3.2. Selected Papers Found in MMR-2004  In the following, we briefly review a few survival analysis related studies presented in a recent International Conference on Mathematical Methods in Reliability, MMR 2004 \(Wilson et al. 2005  Pena and Slate \(2005\ddressed the dynamic reliability Both reliability and survival times are more realistically described with dynamic models. Dynamic models generally refer to the models that incorporate the impact of actions or interventions as well as thei r accumulative history, which 


 17 can be monitored \(Pena and Slate 2005\he complexity is obviously beyond simple regression models, since the dependence can play a crucial ro le. For example, in a loadsharing network system, failure of a node will increase the loads of other nodes and influences their failures  Duchesne \(2005\uggested incorporating usage accumulation information into the regression models in survival analysis.  To simplify the model building Duchesne \(2005\umes that the usage can be represented with a single time-dependent covariate.  Besides reviewing the hazard-based regression models, which are common in survival analysis, Duchesne 2005\viewed three classes of less commonly used regression models: models based on transfer functionals, models based on internal wear and the so-called collapsible models. The significance of these regression models is that they expand reliability modeling to two dimensions. One dimension is the calendar time and the other is the usage accumulation.  Jin \(2005\ reviewed the recent development in statisti cal inference for accelerated failure time \(AFT\odel and the linear transformation models that include Cox proportional hazards model and proportional odds models as special cases. Two approaches rank-based approach and l east-squares approach were reviewed in Jin \(2005\. Osbo rn \(2005\d a case study of utilizing the remote diagnostic data from embedded sensors to predict system aging or degradation.  This example should indicate the potential of survival analysis and competing risks analysis in the prognostic and health management PHM\ since the problem Osborn \(2005 addressed is very similar to PHM. The uses of embedded sensors to monitor the health of complex systems, such as power plants, automobile, medical equipment, and aircraft engine, are common. The main uses for these sensor data are real time assessment of th e system health and detection of the problems that need immediate attention.  Of interest is the utilization of those remote diagnostic data, in combination with historical reliability data, for modeling the system aging or degradation. The biggest challenge with this task is the proper transformation of wear  time The wear is not only influenced by internal \(temperature, oil pressures etc\xternal covariates ambient temperature, air pressure, etc\, but also different each time   4  S UMMARY AND P ERSPECTIVE   4.1. Summary  Despite the common foundation with traditional reliability theory, such as the same probability definitions for survival function  S  t  and reliability  R  t  i.e  S  t  R  t well as the hazards function the exact same term and definition are used in both fields\rvival analysis has not achieved similar success in the field of reliability as in biomedicine The applications of survival analysis seem still largely limited to the domain of biomedicine.  Even in the sister fields of biomedicine such as biology and ecology, few applications have been conducted \(Ma 1997, Ma and Bechinski 2008\ the engin eering fields, the Meeker and Escobar \(1998\ monograph, as well as the Klein and Goel 1992\ to be the most comprehensive treatments  In Section 2, we reviewed the essential concepts and models of survival analysis. In Section 3, we briefly reviewed some application cases of survival analysis in engineering reliability.  In computer science, survival analysis seems to be still largely unknown.  We believe that the potential of survival analysis in computer science is much broader than network and/or software reliability alone. Before suggesting a few research topics, we no te two additional points  First, in this article, we exclusively focused on univariate survival analysis. There are two other related fields: one is competing risks analysis and the other is multivariate survival analysis The relationship between multivariate survival analysis and survival analysis is similar to that between multivariate statistical analysis and mathematical statistics.  The difference is that the extension from univariate to multivariate in survival analysis has been much more difficult than the developm ent of multivariate analysis because of \(1\rvation cens oring, and \(2\pendency which is much more complex when multivariate normal distribution does not hold in survival data.  On the other hand, the two essential differences indeed make multivariate survival analysis unique and ex tremely useful for analyzing and modeling time-to-event data. In particular, the advantages of multivariate survival analysis in addressing the dependency issue are hardly matched by any other statistical method.  We discuss competing risks analysis and multivariate survival analysis in separate articles \(Ma and Krings 2008a, b, & c  The second point we wish to note is: if we are asked to point out the counterpart of survival analysis model in reliability theory, we would suggest the shock or damage model.   The shock model has an even longer history than survival analysis and the simplest shock model is the Poisson process model that leads to exponential failure rate model The basic assumption of the shock model is the notion that engineered systems endure some type of wear, fatigue or damage, which leads to the failure when the strengths exceed the tolerance limits of th e system \(Nakagawa 2006 There are extensive research papers on shock models in the probability theory literature, but relatively few monographs The monograph by Nakagawa \(2006\s to be the latest The shock model is often formulated as a Renewal stochastic process or point process with Martingale theory The rigorous mathematical derivation is very similar to that of survival analysis from counting stochastic processes  which we briefly outlined in section 2.6  It is beyond the scope of the paper to compare the shock model with survival analysis; however, we would like to make the two specific comments. \(1\Shock models are essentially the stochastic process model to capture the failure mechanism based on the damage induced on the system by shocks, and the resulting statistical models are 


 18 often the traditional reliability distribution models such as exponential and Weibull failure distributions.  Survival analysis does not depend on specific shock or damage Instead, it models the failure with abstract time-to-event random variables.  Less restrictive assumptions with survival analysis might be more useful for modeling software reliability where the notions of fatigue, wear and damage apparently do not apply.  \(2\hock models do not deal with information censoring, the trademark of failure time data.  \(3\ Shock models, perhaps due to mathematical complexity, have not been applied widely in engineering reliability yet.  In contrast, the applications of survival analysis in biomedical fields are much extensive.  While there have been about a dozen monographs on survival analysis available, few books on shock models have been published.  We believe that survival analysis and shock models are complementary, a nd both are very needed for reliability analysis, with shock model more focused on failure mechanisms and the survival analysis on data analysis and modeling  4.2. Perspective   Besides traditional industrial and hardware reliability fields we suggest that the following fields may benefit from survival analysis  Software reliability Survival analysis is not based on the assumptions of wear, fatigue, or damage, as in traditional reliability theory.  This seems close to software reliability paradigm. The single biggest challenge in applying above discussed approaches to softwa re systems is the requirement for a new metric that is able to replace the survival time variable in survival analysis. This "time" counterpart needs to be capable of characterizing the "vulnerability" of software components or the system, or the distance to the next failure event. In other words, this software metric should represent the metric-to-event, similar to time-toevent random variable in survival analysis.  We suggest that the Kolmogorov complexity \(Li and Vitanyi 1997\n be a promising candidate. Once the metric-to-event issue is resolved, survival analysis, both univariate and multivariate survival analysis can be applied in a relative straightforward manner. We suggest that the shared frailty models are most promising because we believ e latent behavior can be captured with the shared fra ilty \(Ma and Krings 2008b  There have been a few applications of univariate survival analysis in software reliability modeling, including examples in Andersen et al.'s \(1995\ classical monograph However, our opinion is that without the fundamental shift from time-to-event to new metric-to-event, the success will be very limited. In software engineering, there is an exception to our claim, which is the field of software test modeling, where time variable may be directly used in survival analysis modeling  Modeling Survivability of Computer Networks As described in Subsection 2.3, random censoring may be used to model network survivability.  This modeling scheme is particularly suitable for wireless sensor network, because 1\ of the population nature of wireless nodes and \(2\ the limited lifetime of the wireless nodes.  Survival analysis has been advanced by the needs in biomedical and public health research where population is th e basic unit of observation As stated early, a sensor networ k is analogically similar to a biological population. Furthermore, both organisms and wireless nodes are of limited lifetimes. A very desirable advantage of survival analysis is that one can develop a unified mathematical model for both the reliability and survivability of a wireless sensor network \(Krings and Ma 2006  Prognostic and Health Management in Military Logistics PHM involves extensive modeling analysis related to reliability, life predictions, failure analysis, burnin elimination and testing, quality control modeling, etc Survival analysis may provide very promising new tools Survival analysis should be able to provide alternatives to the currently used mathematical tools such as ANN, genetic algorithms, and Fuzzy logic.  The advantage of survival analysis over the other alternatives lies in its unique capability to handle information censoring.  In PHM and other logistics management modeling, information censoring is a near universal phenomenon. Survival analysis should provide new insights and modeling solutions   5  R EFERENCES   Aalen, O. O. 1975. Statistical inference for a family of counting process. Ph.D. dissertation, University of California, Berkeley  Andersen, P. K., O. Borgan, R D. Gill, N. Keiding. 1993 Statistical Models based on Counting Process. Springer  Arsene, C. T. C., et al. 2006.  A Bayesian Neural Network for Competing Risks Models with Covariates. MEDSIP 2006. Proc. of IET 3rd International Conference On Advances in Medical, Signal and Info. 17-19 July 2006  Aven, T. and U. Jensen. 1999. Stochastic Models in Reliability. Springer, Berlin  Bazovsky, I. 1961. Reliability Theory and Practice Prentice-Hall, Englewood Cliffs, New Jersey  Bakker, B. and T.  Heskes. 1999. A neural-Bayesian approach to survival analysis. IEE Artificial Neural Networks, Sept. 7-10, 1999. Conference Publication No 470. pp832-837  Berzuini, C.  and C. Larizza 1996. A unified approach for modeling longitudinal and failure time data, with application in medical mon itoring. IEEE Transactions on Pattern Analysis and Machine Intelligence. Vol. 18\(2 123 


 19 Bollob·s, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSSí03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et MathÈmatiques AppliquÈes de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


