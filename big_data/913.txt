Modifying Fuzzy Association Rules with Linguistic Hedges Qiang Wei\222 Guoqing Chen School of Economics  Management, Tsinghua University Beijing 100084 China weiq, chengq} @em.tsinghua.edu.cn Geert Wets Faculty of Applied Economic Sciences Limburg University 3 590 Diepenbeek Belgium geert.wets@luc.ac.be Abstract This paper introduces linguistic hedges in data mining 
especially in association rules mining based on our recent work on fuzzy taxonomic structures where fmzy rules like 223Expensive cosmetics 3 Tropical fruits 224 can be dealt with In order to express the decision knowledge more naturally the notion of fuzzy association rules with linguistic hedges is presented such as 223very expensive goo 
3 sort of fruit\224 Furthermore a method of mining all possible fuzzy association rules with a particular pool of linguistic hedges is discussed 1.Introduction Data mining is one of the emerging fields of information processing Recently, a lot of attention has been paid to the discovery of association rules  1,2,4 71 An 
example of such rules is 223Apple3Pork\224 e.g customers who bought apples turned to buy pork In particular Srikant and Agrawal 6 introduced the notion of generalized association rules so as to discover the association upon all levels of presumed exact taxonomic structures e.g FruimMeat In many real-world applications however the taxonomic structures may not be crisp This issue is 
deemed to be of particular interest to high-level decision-makers. Thus in 7 we presented the notion of fuzy taxonomic structures and extended the approach to discovering generalized association rules in which each child-node may belong to its parent node in a partial degree \(between 0 and 1 Generally speaking each interior node non-leaf node can be regarded as a fuzzy 
set or linguistic term\represented by its child nodes In this way certain hzzy association rules can be represented and discovered such as \223Tropical FrubFresh Meat\224, where tropical hit and fresh meat are both fuzzy terms. This allows managers or decision-makers to express higher-level e.g., abstract, conceptual\concepts and knowledge, as well as in a more natural manner Here Dsupport and Dconfidence are used 
as measures in the mining process Dsupport of a hzzy rule XSY means the 223number\224 e.g Zcount of the transactions containing XuY and Dconfidence of XJY  Dsupport\(X=SY I Dsupport\(X 7 A set that contains k items is called a k-candidate itemset and if Dsupport exceeding the threshold min-support the itemset is called 
a k frequent itemset Only the rules with Dsupport and Dconfidence exceeding the threshold min-support and min-confidence are regarded usehl In this paper we further consider using linguistic Partly supported by the National Science Foundation of China No 79925001 222 Corresponding author 0-7803-6274-8/00/$10.00 O 2000 IEEE 397 


hedges modifiers to modi the fuzzy items in the fuzzy taxonomic structures such as 223very expensive goods\224 223almost young people\224. There are two reasons that linguistic hedges are worth being considered in mining fuzzy association rules First with the operation of linguistic hedges on the items the discovered knowledge is more understandable and closer to human language For the decision-makers especially the top managers this type of knowledge may be more often used and meaningful Second it can enrich the semantics of association rules and make the discovered rules more granular For example we can get the rules like 223Apple 3 Jeans\224 223Expensive Apple s Cool Jeans\224, and 223Very Expensive Apple More-or-less Cool Jeans\224 2.Main ideas and Problem Statement Linguistic hedges, such as very more-or-less, sort of are not themselves modeled by fuzzy sets as the primary terms are, but rather are modeled as operators acting on the fuzzy sets representing the primary terms r31 Consider a hedge operator H which can be used to deal with a number of linguistic hedges Let F\(U be the set of all fuzzy set on U and HA be a hedge operator with h E 0 001 then H is a mapping from F\(U to F\(U\such that VAEF\(U HA A  A E F\(U orva E U pH,\(A a  bA E OJI When h  1 H reduces the membership degrees for the elements of the fuzzy set being modified, which are called concentration operators while h  1 H increases the membership degrees for the elements of the fuzzy set being modified which are called dilation operators For example H is referred to as a  concentration operator for hedge 223very\224 semantically Given a linguistic hedge h  very and a fuzzy term node w  expensive electronics   l/air-conditioner 0.6/mobile O.S/television     then hw  very expensive electronics   l/air-conditioner, 0.36/mobile 0.25/television    Since in the fuzzy taxonomic structures an interior node could be expressed as a fuzzy set on its child nodes 7 the interior node could be modified in forms of hedges with the same child-nodes. Then if we apply each hi E H onto the items \(nodes in the taxonomic structures we can derive all the fuzzy sets of the modified items with linguistic hedges i.e h,w\222s In so doing, the modified items could be added into the original structures and form the new fuzzy taxonomic structures Apparently the methods proposed in 7 could be applied in a straightforward fashion. However it is worth noting that such methods can be considerably improved since a great number of itemsets can be pruned in the mining process due to certain properties which will be discussed in the next section Now mining fuzzy association rules with linguistic hedges is to discover all the possible rules with any possible linguistic hedges in the transaction set. Three basic inputs are as follows A A transaction set T as an original data source B Pre-specified fuzzy taxonomies FG associated with transaction set T Concretely 6 71 let FG be a directed graph on the items I  i i  i An edge in FG represents a fuzzy is-a relationship which means along with each edge there exists a partial degree p with which the child-node on this edge belongs to its parent-node on this edge, where 0 S p I 1 Figure 1 shows an example Vegetable dishes Exoensive electronics Figure 1 An example of fuzzy taxonomic structures C. A set \(or pool of linguistic hedges \(modifiers H  h I h is a hedge operator represented by H H 398 


could be specified by users or decision-makers or obtained from their past querying records which reflects either their interests or their behaviors An example is web mining to trace customers\222 interests and behaviors in relation to relevant product web sites This may be particularly meaninghl for companies to gaidsustain their competitive advantages in e-business environments some combinations of specific hedges and specific items are not correct linguistically such as 223higher than fresh fruits\224 An example of a match table is as follows Items Table 1 An example of a match set 3.Building New Fuzzy Taxonomies B New fuzzy taxonomic structures This process consists of constructing a match table and the corresponding taxonomic structures A Match table When applying the hedges in H onto the items in FG not all the linguistic hedges can be applied on every node First semantically, none of the leaf-nodes can be operated on any linguistic hedges Each leaf-node is basic item of the taxonomic structures and is not regarded as a fuzzy set on other items Second some linguistic hedges cannot be applied onto certain items. Usually 223very\224 can be applied onto any nouns with adjective, but cannot be applied onto nouns directly. However 223sort of\222 can be applied onto both nouns and nouns with adjective Typically, the pool of linguistic hedges H may contain adjectives which can be used to modify nouns andor adverbs which can be used to modify adjectives To facilitate the meaningful linguistic modification a match table will be constructed based on FG and H Four steps are identified 1 Divide the items in I into two sets, nouns and nouns with adjective 2 Divide the hedges in H into two sets adjectives and adverbs Since some hedges can be used as both adjectives and adverbs such as \223sort of\222 they may appear in both sets 3 Match each item with corresponding hedges according to the above principle 4 Filter the match table say by users or experts. This is because that With the match table and H we can construct the fizzy sets of all the new modified items by which we can add all the new items into original structures FG and form new structures called FG\222 For example given figure 1 and table 1 we can obtain new fuzzy taxonomic structures as follows \(partly Vegetable dishes a pq Figure 2 An example of FG\222 For each item node w to be modified a new node hw such as 223very expensive electronics\224 is labeled and then constructed as a hzzy set by connecting it to the corresponding elements where the respective degrees are modified according to H 4.Mining and computational complexity In this section, some key steps of mining will be discussed Based on the method in 7 the membership degree that each leaf-node belongs to its ancestor-node 399 


is first computed Then the candidate itemsets and frequent itemsets will be generated. Next for all the frequent itemsets, the Dconfidence of association rules will be computed by which the final rules are derived Notably generating the frequent itemsets is the most crucial and time-consuming step In this regard there are two ways to optimize this step A Optimization I Straightforward mining without any optimization may produce correct results but inefficiently due to the fact that the number of items in FG\222 is much bigger than the number of items in FG Generally each interior-node will be expanded to m+l nodes if there are on average m linguistic hedges in H which are operated on each node Then, there will be nx\(m+l nodes in FG\222 if there are n nodes in FG regardless the leaf-nodes In order to simplify the analysis we assume min support  0 which means all possible combination of items are frequent itemsets and should be counted And we also do not eliminate the combination that the child-node and its parent-node are in the same itemset Thus, theoretically we will generate 2\224x\(m+l frequent itemsets and should scan the database for 2 224\222+\222 times if using the straightforward mining strategy directly on FG\222 Semantically, however in FG\222 many items are usually modified items with different linguistic hedges based on a same original item in FG which we call both them and the original item to be in a class of items For example 223very expensive electronics\224 223relatively expensive electronics\224 and 223expensive electronics\224 are in one class of items Then when combining different items into an itemset we cannot combine any two items of the same class into one itemset because it is meaningless Therefore when we generate itemsets on FG\222 we can only select one item from specific class of items New Algorithm Then the number of frequent itemsets that we can generate is m+2 Under the same assumption the scan of database in mining fuzzy generalized association rules on FG without linguistic hedges is 2\224 Then we compare these three situations in figure 3 where A presents using the old algorithm on FG\222 B presents using new algorithm on FG\222 C presents using old algorithm on FG  Figure 3 Comparison of the three situation For m  2  210g2\(m+2  2n\222og2\(m+2 and m 2 0 2\224   2n\222og2\(m+2 5 2\224\(\222\224+\222 it appears that with the enlargement of m the space between A and B and space between B and C are bigger and bigger but the speed of increment of the space between B and C is slower and slower which means that using New Algorithm can reduce the computational complexity compared with A Especially when m  0 representing that no linguistic hedges are applied\the three situations are the same B Optimization I1 Further optimization may be made for the algorithm in the mining process First in a class of items, only one is the basic item node and others result from operating onto the basic item by H The basic item in the class can also be regarded as operated by HI According to the definition of HA given two items i and i in one class, which are operated by HA H respectively, and XI  A then the degree that each child-node belongs to i is less than the degree that the same child-node belongs to i Given two itemsets A={i i  ip and B={ill i\222z  i\222 where ij and i\222j are in the same class of items which are operated by Hij HA respectively, and 400 


hj  1 I j I p then it is clear that the degree that t E T supports A is greater than or equal to the degree that t supports B according to the definition of Dsupport in 7 Then Dsupport of A is greater than or equal to that of B Next with the New Algorithm we could sort the class of items lexicographically, and for each class we sort the items in the ascending order of h This can guarantee that in generating candidate itemsets the itemset with smaller Dsupport appears after the itemset with larger Dsupport Consequently the 1 frequent itemsets and k+l-frequent itemsets for k  0 could be generated as follows 1 When we generate I-frequent itemsets we can find if an itemset composed of one item i is not a frequent itemset, then any itemset composed of other items in the same class to which i belongs is not a frequent itemsets either For example if 223sort of fresh fruit\222 is not a frequent itemset clearly 223very fresh fruit\224 is not a frequent itemset either In this way we can filter such itemsets to reduce the time in scanning of the database 2 After generating k+l candidate itemsets based on k-frequent itemsets we should compute Dsupport of each candidate itemset to eliminate the non-frequent itemsets If we find an itemset such as A that is not a frequent itemset, then we can scan the rest of the set of k+l candidate itemsets and eliminate all the itemsets such as B with respect to A for they are not frequent itemsets either This results in the set of k 1 frequent itemsets Ongoing studies for the algorithm optimization include further theoretical explorations of itemsets properties and related pruning strategies as well as more detailed analysis of experiments with both synthetic and real data 5 Conclusions In this paper linguistic hedges have been incorporated into fizzy association rules to facilitate the representation and discovery of more meaningful knowledge in a natural manner This is also deemed to be of particular interest for web mining in e-business environments In dealing with the problem new fuzzy taxonomic structures could be constructed from the original one by directly applying the linguistic hedges onto the appropriate basic nodes Based on the straightforward mining strategy further optimizations have been discussed to reduce the number of candidatehequent itemsets for the classes of items as well as for certain itemsets with smaller Dsupport References R Agrawal T Imiclinski A Swami Mining Association Rules between Sets of Items in Large Databases Proc. of ACM SIGMOD Conf Washington DC USA May 1993 R Agrawal H Mannila R Srikant H Toivonen A Inkeri Verkamo Fast Discovery of Association Rules in Advances in Knowledge Discovery and Data Mining AAA1 Press/The MIT Press 1996 Guoqing Chen Fuzzy Logic in Data Modeling semantics, constraints and database design Kluwer Academic Publishers, Boston 1998 J Han Y Fu Discovery of Mult@le-leveI Association Rules from Large Databases Proceedings of the 21st International Conference on Very Large Databases Zurich, Switzerland September 1995 Savasere E Omiecinski S Navathe An Eflcient Algorithm for Mining Association Rules in Large Databases Proceedings of the VLDB Conference Zurich Switzerland, September 1995 R Srikant R Agrawal Mining Generalized Association Rules Proc of the 21\221 VLDB Conf Zurich Swizerland 1995 Qiang Wei Guoqing Chen Mining Generalized Association Rules with Fuzzy Taxonomic Structures NAFIPS 99 New York 1999 40 1 


 Root I:5 2 I:4 4 I:2 1 I:1 4 I:1 5 I:1 3 I:1 5 I:1 3 Item Head of Node-link I 5 I 1 I 2 I 3 I 4   I 6                 I:1 6  R u R s I-A A Figure 1 An example of an FP-MPIS-tree number of transactions is large These repeated patterns are processed only once with 000\001\002  From our experiments this mechanism can greatly reduce the overall running time 6.3.2 Calculating Pro\336t with the FP-MPIS-tree In the de\223nition of the pro\223t of an item selection 000 000 000 001 001 see De\223nition 6 we need to compute the number of transactions containing some selected items 002 000 andanyiteminset 003 000 the value of 004 000 002 001 005\003 000 001  where 002 001 003 001 and 003 000 004 002 005 001 This is computed for many selections for each iteration hence the ef\223ciency is important For this task we use the FP-MPIStree data structure In the FP-MPIS-tree we divide the items into two sets 002 005 001 and 001 Set 001 corresponds to items selected while 002 005 001 contains those not selected The items in set 002 005 001 are inserted into FP-MPIS-tree near to the r oot Similar to the FP-tree the ordering of items in each set in the FP-MPIS-tree is based on the frequencies of items An example is shown in Figure 1 In the 223gure the set of selected items is 001 002 006 002 000 005\002 001 005\002 002 007 and e set of unselected items is 002 005 001 002 006 002 003 005\002 004 005\002 005 007  To compute 004 000 002 001 005\003 001  we 223rst look up the horizontal linked list dotted links in Figure 1 of item 002 001 in the FP-MPIStree For each node 006 in the linked list we call the function parseFPTree 006\005 003  The function returns a count we add up all the counts returned from e nodes 006 and it is the value of 004 000 002 001 005\003 001  Function parseFPTree 007\005 003  computes the number of transactions containing item 002 001 and at least one item in 003 in the path from root of FP-tree to 007  Starting from e node 007  we traverse the tree upwards towards the root of the FP-tree until we d a node b containing one element in set 003 or we hit the root node If b exists the count stored in node 007 is returned The call of function parseFPTree 007\005 003  is quite ef\223cient as we do not need to traverse downwards from node 007  This is because all nodes below node 007 are selected items no item in 003 will be found below 007  A further re\223nement for the FP-MPIS-tree is to insert only transactions that contain both selected and non-selected items For transactions with only selected items the pro\223t for each selected item is simply given For transactions with only nonselected item the pro\223t contribution will be zero This re\223nement can greatly reduce the size of the FP-MPIS-tree Note also that the FP-MPIS-tree is built from the FP-tree 000\001\002 and not from the original database 6.3.3 Item Bene\336t Update In each iteration after we remove item 002 002  we need to check the selection t 000 for each item 002 000 in b 000 If t 000 contains item 002 002  it should be updated because item 002 002 has been removed also a new item 002 003 will be lected to be included into t 000 As t 000 is changed the bene\223ts n 000 also have to be updated Let t 000 000 t\006 002 002 007 be the selection before we remove item 002 002 while t 000 000 t\006 002 003 007 be the selection after we have removed item 002 002 and added item 002 003 in the selection t 000  We can do the item bene\223t update y scanning only those transactions 002 containing at least one of item 002 002 and item 002 003 Let 000 000 000 000 001\005 002 001 be the pro\223t of the item selection 001 generated by transactions in 002 The item bene\223t is updated n 000 n n 000 003 000 000 000 000 t 000 000 t\006 002 003 007 005 002 001 005 000 000 000 000 t 000 000 t 006 002 002 007 005 002 001  The computation of 000 000 000 000 001\005 002 001 can be done in a similar manner as 000 000 000 001 001 but 000 000 000 000 001\005 002 001 considers only transactions 002  instead of all transactions As there are fewer transactions in 002 compared to the whole database the update can be done very ef\223ciently 7 Empirical Study We use the Pentium IV 1.5 GHz PC to conduct our experiment Frontline System Solver is used to solve the QP problem All algorithms other than QP are implemented in C/C The pro\336tability is in terms of the percentage of the total pro\223t in the data set We compare our methods with HAP and the naive approach The naive approach simply calculates the pro\223ts generated by each item for all transactions and select the 013 items with the greatest pro\223ts Several synthetic data sets and a real data set are to be tested in our experiments We have tried a number of quadratic programming tools including LINDO TOMLAB GAMS BARON OPTRIS WSAT Frontline System Solver MOSEK and OPBDP We choose Frontline System Solver Premium Solver Premium Solver Platform 1 becaus e i t performs the bes t out of thes e solvers 7.1 Synthetic Data Set In our experiment we use the IBM synthetic data generator in 2 t o g en erate th e d ata set with th e f o llo win g p aram eters same as the parameters of 26 1,000 i t e ms  10,000 t r ans actions 10 items per transaction on average and 4 items per frequent itemset on average The price distribution can be approximated by a lognormal distribution as pointed out in 15 We use the same settings as 26 That i s  10 of i t e ms ha v e the low pro\223t range between 0.1 and 1 80 of items have the medium pro\223t range between 1 and 5 and 10 of items have the high pro\223t range between 5 and 10 7.2 Real Data Set The real data set is obtained from a large drug store in Canada over a period of 3 month In this data set there are 26,128 items and 193,995 transactions On average each transaction contains 2.86 items About 40 of the transactions contain a single item 22 contain 2 items 13 contain 3 items the percentages for increasing sizes decrease smoothly and there are about 3 of the transactions with more than 10 items The greatest transaction size is 88 items In this data set the pro\223t distribution of items is shown in the Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


following table Pro\223t Range Proportion Pro\223t Range Proportion 0-$0.1 2.03 5-$10 10.43 0.1-$1 25.05 10-$100 7.75 1-$5 54.59 100-$400 0.15 7.3 Results r Synthetic Data In the 223rst experiment we have the same setup as in 26 but the pro\223t follows lognormal distribution The result is shown in Figure 2 In the 223gure it is noted that the pro\223tability lines for MPIS Alg QP and HAP are overlapping and the execution-time line for HAP is slightly greater than that for naive For pro\223tability we observe that for the data set the naive approach gives the lowest pro\223tability among all algorithms This is because the naive approach does not consider any cross-selling effect Naturally the pro\223tabilities of all algorithms increase when the number of items selected increases From the graph of the execution time against the selection size the execution time of MPIS Alg increases from 0 selection reaching a maximum when about half the items are selected and then decreases afterwards Here the execution time depends on two factors The 223rst factor is related to the complexity of each iteration If there are more items to be selected the bene\223t calculation is more complex and updates to the bene\223t are more likely The initial increase is related to the 223rst factor The second factor is related to the number of iterations in the algorithm When 000  the number of items selected increases the number of items to be removed in the iteration step decreases Thus the number of iterations decreases if 000 is large compared with 001  The 223rst factor is dominant when the selection is below 50 but the second factor becomes dominant when the selection is larger than 50 The quadratic programming approach QP used in the chosen Solver uses a variant of the Simplex method to determine a easible region and then uses the methods described in 13 to 223nd the solution As the approach uses an iterative step based on e current state to determine the next step the execution time is quite 224uctuating as the execution time is mainly dependent on the problem or which state the algorithm is in HAP is an iterative approach to 223nd the authority weight of each item The formula for the update of the authority weight is in the form 002 000 003\002 where 002 is a vector of dimension 001 representing the authority weight of 001 items and 003 is an 001 000 001 matrix used in HAP to update the authority weight In our experiment we observed that the authority weights converge rapidly QP takes the longest execution time compared with other algorithms Naive gives the shortest execution time as there are only simple operations HAP gives the second shortest execution time for this small synthetic data set We note that the number of iterations involved are quite small MPIS Alg has the second greatest execution time but it scales much better with increasing number of items where it can outform HAP many folds see the next subsection 7.4 Results r Real Data Set With the drug store data set we have conducted similar experiments as with the synthetic data However the Quadratic Programming QP Solver 1 does not handl e m ore t han 2000 variables In the real data set there are 26,128 variables i.e items hence it is not possible to experiment with our QP tool The results of the experiments are shown in Figure 3 In the results HAP gives the lowest pro\223tability The reason is as follows In the dataset there are some items with zero-pro\223t and high authority weight described in Section 3 yielding a low estimated total pro\223t of the item selection Suppose item 004 000 has zero pro\223t it is likely a good buy and hence can lead to high support If there are suf\223cient number of purchases of other item says item 004 001  with item 004 000 and if item 004 000 usually occur in e transactions containing item 004 001  the con\223dence of the rule 004 001 001 004 000 is quite high This creates a high authority weight for item 004 000  Items like 004 000 would lead to smaller profitability for HAP MPIS Alg gives a greater pro\223tability than naive approach in the real data set For instance if 000 000\001\002 005 003\002\001  the difference in pro\223tabilities between these two approaches is 2 In the real data set the l pro\223t is equal to 1,006,970 The difference in 2 pro\223tability corresponds to 20,139.4 which is a signi\223cant value If J=8709 the difference in pro\223tabilities between the two approaches is about 8 which corresponds to 80,557.6 On average the execution time of HAP is 6.5 times slower than MPIS Alg when the problem size is large HAP requires 6 days to 223nd the item selection while MPIS Alg requires about 1 day to 223nd the solution Since item selection is typically performed once in a while only when a store should update the types of products it carries the execution time is acceptable Though the naive method is much faster the pro\223t gain consideration from MPIS Alg would make it the better choice for an application The execution time of HAP increases signi\223cantly when the number of items increases compared with MPIS Alg In HAP a cross-selling matrix 006 is updated iteratively The matrix is of the order 001 000 001  For the real data set 001 000 001\004 005 005\001\006 and 001 000 will be very large Let 002 be e 001 000 005 vector representing the authority weight of each item In HAP there is a process to update 003\002 iteratively where 003 000 006 002 006 Thismatrix multiplication of matrix 003 with vector 002 is highly costly Let us consider the memory required for matrix 003  If double data type 8 bytes is used for storage of each entry then the matrix requires a memory size of about 5.08GB If 224oat data type 4 bytes is required then about 2.5GB memory is required This large matrix cannot 223t into the physical memory g a t of disk accesses for virtual memory Since the matrix 003 is sparse a hash data structure can be used so that only non-zero entries are stored We have adopted the hash structure for the real data set and fouud that less than 5MB memory is needed Our results in Figure 3 are based on this enhanced hashing approach However the computatio n with this reduced size is still very massive We have also tried other sets of experiments where not all Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


the items are considered but only those above a minimum support threshold of 0.05 or 0.1 are considered However the resulting pro\223tabilities are much lower than those shown in Figure 3 For instance if J  500 and min-support  0.05 the pro\223tability of naive and MPIS Alg is about 1.3 But if all items are considered the pro\223tability of those approaches is about 25 This is explained by the existence of items that generate h pro\223ts but which are not purchased frequently enough to be counted given the support thresholds  Na ve i Na ve i Figure 2 The Synthetic Data Set Na ve i Na ve i Figure 3 The drug store data set 8 Conclusion One of the applications of association rule the maximalpro\223t item selection problem with cross-selling effect MPIS is discussed in this paper We propose a modeling by the loss rule which is used in the formulation of the total pro\223t of the item selection We propose both a quadratic programming approach and a heuristical approach to solve the MPIS problem We show by experiments that these methods are ef\223cient and highly effective We believe that much future work can be done The heuristical method can be enhanced with known methodologies such as hill climbing Expert knowledge can be included in the methods and the de\223nition of the problem can be changed in different ways to re\224ect different user environments Acknowledgements We would like to thank M.Y Su for his generous help in providing the source codes for the HAP solution and his other advices We also thank Ping Quin of DBMiner Technology for providing the real dataset This research is supported by the Hong Kong RGC Earmarked Grant UGC REF.CUHK 4179/01E References 1 F ro n tlin e s y s tems so lv er  h ttp www so lv er co m 2 R  A gr a w al  I bm synt het i c dat a gener at or  http://www.almaden.ibm.com/cs/quest/syndata.html 3 R  A g r a w al T  I milien s k i  a n d Sw ami M i n i n g asso ciatio n r u l es between sets of items in large databases In SIGMOD  1993  R  A gra w al and R  S r i kant  F ast al gori t h ms for m i n i n g a ssoci ation es In VLDB  1994  J  E  B easl e y  H euri st i c al gori t h ms for t he unconst rai ned bi nary quadratic programming problem In chnical report the Management School Imperial College London  Dec 1998  T  B l i s chok E v ery t ransact i o n t el l s a s t o ry  I n Chain Store Age Executive with Shopping Center Age 1 3  pages 50\20557 1995 7 T  B r i j s  B  G oet hal s  G  S wi nnen K V a nhoof  a nd G W e t s  A data mining framework for optimal product selection in retail supermarket data The generalized profset model In SIGKDD  2000 8 T  B r i j s  G  S wi nnen K V a nhoof  a nd G W e t s  U si ng associ ation rules for product assortment decisions A case study In SIGKDD  1999  M R Gare y and D S  Johnson Computers and intractability A guide to the theory of np-completeness In Freeman  1979  J Han J P e i  and Y  Y i n Mi ni ng f r e quent pat t e r n s w i t hout candidate generation In SIGMOD  2000  S  Hedber g T h e dat a gol d r ush I n BYTE October  pages 83\205 99 1995  Hiller and L ieber man Introduction to operations research In McGraw Hill Seventh Edition  2001 13 B V  Ho h e n b a lk e n  A 223n ite a l g o r ith m t o m a x imiz e c e r ta in pseudoconcave functions on polytopes In Mathematical Programming 8  1975  R  Hor s t  P  M Par dal os and N  V  T hoai  I nt r oduct i o n t o global optimization In Kluwer Academic Publishers Second Edition  2000  J C  Hul l  Opt i ons fut u res and o t her deri v a t i v es In Prentice Hall International Inc 3rd Edition  1997  L  D Iasemi di s P  Pardal os J C S ack el l ares and D S  S h i au Quadratic binary programming and dynamical system approach to determine the predictability of epileptic seizures In Journal of Combinatorial Optimization Kluwer Academic  pages 9\20526 2001  J Kl ei nber g C  Papadi mi t r i ou and P  R agha v a n A m i c r o economic view of data mining In Knowledge Discovery Journal  1998  J M Kl ei nber g Aut hor i t a t i v e sour ces i n a hyper l i n k e d e n v i ronment In Proc ACM-SIAM Symp on Discrete Algorithms  1998 Also in JACM 46:5 1999  S  J L e on L i near al gebr a w i t h appl i cat i ons I n Prentice Hall Fifth Edition  1998 20 J Lu o  K R P a ttip ati an d P K W illett A s u b o p timal s o f t d ecision pda method for binary quadratic programming In Proc of the IEEE Systems Man and Cybernetics Conference  2001  H Manni l a  M et hods and pr obl ems i n dat a mi ni ng I n Proc of Int Conf on Database Theory  1997  H Manni l a  H  T oi v onen and A I  V e r kamo E f 223 c i ent al gorithms for discovering association rules In KDD  1994  V  S a f r ono v a nd M Par ashar  O pt i m i z i n g w eb ser v e r s usi n g page rank prefetching for lustered accesses In World Wide Web Internet and b Information Systems Volume 5 Number 1  2002  S  S a hni  C omput at i onal l y r e l a t e d p r obl ems I n SIAM J Comput 3  pages 262\205279 1974 25 J Ullman  L ectu re n o t es o n search in g t h e web  h ttp wwwdb.stanford.edu ullman/mining/mining.html  K W a ng and M Y  S u  I t e m sel ect i o n b y 216 hubaut hor i t y 216 p r o 223 t ranking In SIGKDD  2002 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


I Plenary Panel Session J Future Directions in Database Research  456 Chair Surajit Chaudhuri Microsoft Corporation Panelists Hector Garcia-Molina Stanford University Hank Korth, Bell Laboratories Guy Lohman IBM Almaden Research Center David Lomet Microsoft Research David Maier Oregon Graduate Institute I Session 14 Query Processing in Spatial Databases I Chair Sharma Chakravarthy University of Florida Processing Incremental Multidimensional Range Queries in a Direct Manipulation Visual Query Environment  458 High Dimensional Similarity Joins Algorithms and Performance Evaluation  466 S Hibino and E Rundensteiner N Koudas and K.C Sevcik Y Theodoridis E Stefanakis and T Sellis Cost Models for Join Queries in Spatial Databases  476 Mining Association Rules Anti-Skew Algorithms  486 J.-L Lin and M.H Dunham Mining for Strong Negative Associations in a Large Database of Customer Transactions  494 A Savasere E Omiecinski and S Navathe Mining Optimized Association Rules with Categorical and Numeric Attributes  503 R Rastogi and K Shim Chair: Anoop Singhal AT&T Laboratories S Venkataraman J.F Naughton and M Livny Remote Load-Sensitive Caching for Multi-Server Database Systems  514 DB-MAN A Distributed Database System Based on Database Migration in ATM Networks  522 T Hara K Harumoto M Tsukamoto and S Nishio S Banerjee and P.K Chrysanthis Network Latency Optimizations in Distributed Database Systems  532 I Session 17 Visualization of Multimedia Data I Chair Tiziana Catarci, Universita di Roma 223La Sapienza\224 W Chang D Murthy A Zhang and T.F Syeda-Mahmood Global Integration of Visual Databases  542 X 


The Alps at Your Fingertips Virtual Reality and Geoinformation Systeps  550 R Pajarola l Ohler P Stucki K Szabo and P Widmayer C Baral G. Gonzalez and T.C Son Design and Implementation of Display Specifications for Multimedia Answers  558 1 Session 18 Management of Objects I Chair: Arbee Chen National Tsing Hua University P Boncz A.N Wilschut, and M.L. Kersten C Zou B Salzberg, and R Ladin 0 Wolfson S Chamberlain S Dao L Jiang, and G. Mendei Flattening an Object Algebra to Provide Performance  568 Back to the Future Dynamic Hierarchical Clustering  578 Cost and Imprecision in Modeling the Position of Moving Objects  588 ROL A Prototype for Deductive and Object-Oriented Databases  598 A Graphical Editor for the Conceptual Design of Business Rules  599 The Active HYpermedia Delivery System AHYDS using the M Liu W Yu M Guo and R Shan P Lang W Obermair W Kraus and T Thalhammer PHASME Application-Oriented DBMS  600 F Andres and K. Ono S Chakravarthy and R Le S Mudumbai K Shah A Sheth K Parasuraman and C Bertram ECA Rule Support for Distributed Heterogeneous Environments  601 ZEBRA Image Access System  602 Author Index  603 xi 


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


