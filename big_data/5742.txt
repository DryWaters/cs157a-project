An Algorithm for Cost-Effectively Storing Scientific Datasets with Multiple Service Providers in the Cloud Dong Yuan 1 Xiao Liu 3 Lizhen Cui 2 Tiantian Zhang 2 Wenhao Li 1 Dahai Cao 1 Yun Yang 4,1   1 Centre for Computing Engineering and Software Systems Swinburne University of Technology Melbourne, Australia dyuan, wli, dcao}@swin.edu.au 2 School of Computer Science and Technology Shandong University Jinan, China clz@sdu.edu.cn zhangtthappy@gmail.com 3 Shanghai Key Laboratory of 
Trustworthy Computing Software Engineering Institute of East China Normal University Shanghai, China xliu@sei.ecnu.edu.cn 4 School of Computer Science and Technology Anhui University Hefei, China yyang@swin.edu.au  
The proliferation of cloud computing allows scientists to deploy computation and data intensive applications without infrastructure investment, where large generated datasets can be flexibly stored with multiple cloud service providers. Due to the pay-as-you-go model, the total application cost largely depends on the usage of computation, storage and bandwidth resources and cutting the cost of cloud-based data storage becomes a big 
Abstract 
 
Keywords-cloud computing; scientific application; datasets storage 
1 
concern for deploying scientific applications in the cloud. In this paper, we propose a novel algorithm that can automatically decide whether a generated dataset should be 1\ stored in the current cloud, 2\ deleted and re-generated whenever reused or 3 transferred to cheaper cloud service for storage. The algorithm finds the trade-off among computation, storage and bandwidth costs in the cloud, which are three key factors for the cost of storing generated application datasets with multiple cloud service providers. Simulations conducted with popular cloud service providersê pricing models show that the proposed algorithm is highly cost-effective to be utilised in the cloud I 
 
 I NTRODUCTION  With the rapid growth of e-science, domain scientists increasingly rely on computer systems to conduct their research 5   26 e  g. cl ust e r gri d a nd HP C  H i g h  Performance Computing\stems. In recent years, cloud computing is emerging as the latest parallel and distributed computing paradigm which provides redundant, inexpensive and scalable resources on demand to user requirements [1   The emergence of cloud computing offers a new way for deploying scientific applications. IaaS \(Infrastructure as a Service\ is a very popular way to deliver services in the cloud  w h e r e th e heter o g e n e ity of c o m p uting sy ste m s 3 6   o f one service provider can be well shielded by virtualisation 
technology. Hence, scientists can deploy their applications in unified cloud resources such as computing, storage and network services without any infrastructure investment, and only pay for their usage according to the pay-as-you-go model However, along with the convenience brought by using ondemand cloud services, users have to pay for the resources used which can be substantial. Especially, nowadays scientific applications are getting more and more data intensive     Yun Yang is the corresponding author of this paper  w h e r e ge ner at ed  da t a s e t s ar e o f t e n gi gab y t e s  t e r a b y t e s, or even petabytes in size. As reported by Szalay et al. in [2   science is in an exponential world and the amount of 
application data will double every year over the next decade and future. These generated data contain important intermediate or final results of computation, which may need to be stored for reuse [7 nd sh ar i ng [8  He nc e c u t t i ng t h e co st  of cloud-based data storage in a pay-as-you-go fashion becomes a big concern for deploying scientific applications in the cloud In the cloud, users have multiple options to cope with the large generated application data. As excessive storage and processing power can be obtained on-demand from commercial service providers, users can either store all data in the cloud and pay for the storage cost, or delete them and pay for the computation cost to regenerate them whenever they are reused Furthermore, as cloud computing is such a fast growing market 
more and more different cloud service providers with costeffective storage solutions appear [3   Th is ph en om en on a l l o w s  users to transfer the generated application data to cheaper services for storage with paying for the incurred bandwidth cost. Hence, in the cloud, users can flexibly store their data with different storage strategies which also lead to different total costs correspondingly. In light of this, a good storage strategy should be able to balance the usage of computation storage and bandwidth resources in the cloud, which are three key factors for the cost of storing generated application data Existing work [3  on ly in v estig at es th e t r a d eof f b etw een  computation and storage within one cloud service provider where bandwidth cost has not been considered 
In this paper, by investigating the trade-off among computation, storage and bandwidth, we propose a novel costeffective algorithm for storing the generated application datasets in the cloud. We utilise a Data Dependency Graph DDG\ to represent generated application data in the cloud and design the novel T-CSB algorithm which can calculate the Trade-off among Computation, Storage and Bandwidth \(TCSB\ in the cloud. The T-CSB algorithm can be utilised to cost-effectively store the generated application data with multiple service providers in the cloud The remainder of this paper is organised as follows. Section II presents a motivating example of scientific application and 
2013 IEEE 9th International Conference on e-Science 978-0-7695-5083-1/13 $25.00 © 2013 IEEE DOI 10.1109/eScience.2013.34 285 


A Motivating Examples B Problems Analysis 
 
  
analyses the research problems. Section III introduces some preliminaries and data storage cost model in the cloud. Section IV presents our novel T-CSB algorithm in detail. Section V describes our experimental results for evaluation. Section VI discusses the related work. Section VII summarises our conclusions and points out future work II M OTIVATING E XAMPLES AND P ROBLEMS A NALYSIS  In this Section, we introduce a real world application in Structural Mechanics which generates large intermediate data with various sizes, and analyse the problems of storing them in the cloud Finite Element Modelling \(FEM\an important and widely used method for impact test of objects, where classic applications are split Hopkinson pressure bar test, gas gun impact test, drop hammer test, etc. In the Faculty of Engineering and Industrial Sciences, Swinburne University of Technology, researchers of the Structural Mechanics Research Group conduct FEM simulations of Aluminium Honeycombs under dynamic out-of-plane compression to analyse the impact behaviour of the material and structure. In their research numerical simulations of the dynamic out-of-plane compression are conducted with ANSYS/LS-DYNA software which is a powerful FEM tool for modelling non-linear mechanics of solids, fluids, gases and their interaction. The FEM application has four major steps as shown in Figure 1   Figure 1. Overview of FEM application From Figure 1, at beginning, based on the researchers design, the object with special structure \(i.e. the honeycombs structure in this example\ for FEM analysis is generated in the Object Modelling step. Then, researchers specify more detailed parameters of the object model in the FEM Initiation step, e.g material of the object and elements for modelling. Based on the well-defined model, researchers can run different FEM simulations according to requirements of the experiment, e.g speed of the compression and time interval for recording data This is the most time consuming and important step in the FEM application, which also generates the largest volume of data as simulation results. Depending on the speed of the compression the computation time of this step varies from several hours to around one hundred hours, while depending on the time interval for recording data, the size of generated data varies from gigabytes to hundreds of gigabytes. These data are very important for researchers, based on which the simulation results can be demonstrated in various ways for analysis As researchers often need to run different simulations, large volume of the generated results data are accumulated as time goes on. However, due to the capacity limit of the local storage system, researchers can only store the recently generated results. Whenever they want reuse or re-analyse the results of pervious simulations, they have to re-run the simulation from beginning to regenerate the data, which is not efficient. Hence researchers consider of migrating the FEM application to the cloud where the storage bottleneck can be avoided in a costeffective way The storage limitation would not be the case in the cloud because the commercial cloud service providers can offer virtually unlimited storage resources. But, due to the pay-asyou-go model in the cloud, cost is one of the most important factors that users would care about. In order to make good use of the redundant cloud resources from different service providers, we need to design a smart algorithm to greatly reduce the cost of storing large generated application data in the cloud. However, designing this algorithm is not an easy job where the following two issues need to be carefully investigated 1\ All the resources in the cloud carry certain costs. No matter how we dealt with the generated data \(e.g. storing, regenerating or transferring\ we have to pay for the corresponding resources used. Different data vary in size, and have different re-generation costs and usage frequencies, e.g data generated in the FEM application in Figure 1; therefore, it is most likely not cost effective to store all the generated data in the cloud. Intuitively, some heuristics can be applied for reducing the cost of storing the generated data. For example we can delete the less frequently used data which have large size but small re-generation cost, and re-generate them whenever reused. Also, for the less frequently used data which have large size and huge re-generation cost, we can transfer them to cheaper places for storage, e.g. to other cloud storage systems, or even out of cloud to usersê own spare storage devices. Hence, there is a trade-off among computation, storage and bandwidth in the cloud which can minimise the cost of storing the generated application data. However, finding this trade-off is not easy, as data in the cloud have dependencies \(i.e complex generation relationships\ and this is also the key issue for cost-effective data storage in the cloud 2\ The best trade-off among computation, storage and bandwidth may not be the best strategy for storing the generated application data. When the deleted data are needed the regeneration not only imposes computation cost, but also causes a time delay, e.g. Step 3: FEM Simulation in Figure 1 sometimes takes several days to finish. It is also the same for data being transferred to other places are needed to be transferred back. Depending on the different time constraints of applications [20 user sê t o l e r a nce  o f t h i s de l a y  m a y d i ffe r dramatically. Therefore, for some applications, users preferences on storage are needed to be investigated. However for some application, users do not concern about waiting for them to become available, hence they may delete or transfer the 
286 


002 
003 003\004\003\005\003\003 
i 
   
focus on the first research issue only not only dataset\(s 
kikjjikji ddddddddd 
  
rarely used data to reduce the overall application cost Therefore, this issue is not the focus of this paper In this paper, we  We design an algorithm which can find the trade-off among computation, storage and bandwidth in the cloud, and thus be utilised for cost-effective data storage with multiple cloud service providers III S CIENTIFIC D ATASETS S TORAGE IN C LOUDS  In this section, we first present some preliminaries including a classification of application data in the cloud and the important concept of DDG \(Data Dependency Graph Then we present the data storage cost model which represents the trade-off among computation, storage and bandwidth in the cloud In general, there are two types of data stored in the cloud original data and generated data 1 are the data uploaded by users, for example, in scientific applications they are usually the raw data collected from the devices in the experiments. For these data users need to decide whether they should be stored or deleted since they cannot be regenerated by the system once deleted As cost of storing is fixed, they are  considered in the scope of this paper 2 are the data newly produced in the cloud while the applications run. They are the intermediate or final computation results of the applications, which can be reused in the future. For these data, their storage can be decided by the system since they can be regenerated if their provenance is known. Hence, our work is applied to the  in the cloud that can automatically decide the storage status of generated datasets in applications. In this paper, we refer as  DDG \(Data Dependency Graph\ [33 is  a d i r ect e d acy cli c graph \(DAG\ which is based on data provenance in scientific applications. All the datasets once generated in the cloud whether stored or deleted, their references are recorded in DDG In other words, it depicts the generation relationships of datasets, with which the deleted datasets can be regenerated from their nearest existing preceding datasets. Figure 2 depicts a simple DDG, where every node in the graph denotes a dataset. We denote dataset in DDG as Furthermore pointing to means that is used to generate   pointing to and means that is used to generate and  based on different operations and pointing to means that and are used together to generate   Figure 2. A simple Data Dependency Graph \(DDG To better describe the relationships of datasets in DDG, we define a symbol which denotes that two datasets have a generation relationship, where means that is a predecessor dataset of in DDG. For example, in Figure 2ês DDG, we have    etc Furthermore is transitive, i.e    In a commercial cloud computing environment, service providers have their cost models to charge users. In general there are three basic types of resources in the cloud computation, storage and bandwidth. Popular cloud services providersê cost models are based on these types of resources  For example, Amazon cloud servicesê prices are as follows 2  0.10 per CPU instance hour for the computation resources 0.15 per Gigabyte per month for the storage resources 0.12 per Gigabyte bandwidth resources for data downloaded from Amazon via Internet In this paper, we facilitate our datasets storage cost model in the cloud as follows  where the total cost of the datasets storage is the sum of which is the total cost of computation resources used to regenerate datasets which is the total cost of storage resources used to store the datasets, and which is the total cost of bandwidth resources used for transferring datasets To utilise the datasets storage cost model, we assume that the application be deployed in one cloud service 3 denoted as  and there be different cloud services, denoted as for storing the generated datasets in the cloud. For a dataset in DDG denoted as we define its attributes as follows  where denotes the generation cost of dataset from its direct predecessors in the cloud denotes the cost per time unit \(i.e. storage cost rate storing dataset in cloud service Especially  denotes the cost rate of storing in the cloud service where the application is deployed    2 The prices may fluctuate from time to time according to market factors.  As this paperês focus is cost-effectiveness, to simplify the problem, we assume that the same types of computation resources are used for generatio and regeneration of datasets, and the same types of storage resources are used for storing datasets 3 We assume that the application only run with one cloud service due to the following two reasons: 1\ Some applications contain dedicate commercial software, e.g. the ANSYS/LS-DYNA software in the FEM application introduced in Section II. Due to the license restriction, these kinds of software cannot be freely installed in different service providersê resources in the cloud 2\ Migrating applications, especially scientific applications to a cloud service is a complex process. In order to take advantage of on-demand cloud services software in the applications usually need second development to facilitate the dynamic scale up and down in the cloud 4 These atrributes were introduced in our prior work   ba s e d on w h i c h  we incorperate bandwidth cost of data transfer into the original definitions. If needed, please refer to our prior work 
for more detailed description of these attributes 
33 
  
003 003 003 003 003 003 003 
DDGd i DDGd i 
002 002 
A Application Data and DDG Original data original data Generated data generated data generated data d i d 1 d 2 d 1 d 2 d 2 d 3 d 5 d 2 d 3 d 5 d 4 d 6 d 7 d 4 d 6 d 7 d i d j d i d j d 1 d 2 d 1 d 4 d 5 d 7 d 1 d 7 B Datasets Storage Cost Model Cost Computation Storage Bandwidth c 1 m c 1 c 2  c m  d 1 d 2 d n  d  d i c s y i,1 d i 
Cost = Computation + Storage + Bandwidth x i y i,s z i,s f i v i provSet i CostR i  4 x i y i,s 
287 


006  006  
ii VV 
  
002 002\004\003\003   
002 
 002 012 007\007  
i i i i i i i 
is is i i i i ver ver CTG d d DDG d d 
ji jijki ij s j d provSet ki k d provSet d d d genCost d z xx 
denotes the transfer cost of dataset from service provider to especially  is a flag which denotes the storage status of dataset  Specifically  1,2  represents that dataset is stored in cloud service and represents that dataset is deleted denotes the usage frequency, which indicates how often is used denotes the set of stored provenance that are needed when regenerating dataset If we want to regenerate we have to find its direct predecessors which may also be deleted or stored in other cloud services is the set of the nearest stored predecessors of in the DDG.  Hence the generation cost of is     1 As we can see from formula \(1\ the regeneration cost of is two folds: 1\e bandwidth cost of transferring s stored provenance datasets to which is the cloud service that the application is deployed, and 2\e computation cost of regenerating in  is s cost rate, which means the average cost per time unit of dataset in the cloud. The value of  depends on the storage status of where    0   2 Hence, the total cost rate of storing a DDG is the sum of of all the datasets in it, which is We further define the storage strategy of a DDG as which denotes the storage status of datasets in the DDG. Formally which is the set of every dataset's attribute  indicating the cloud service in which is stored. We denote the cost rate of storing a DDG with the storage strategy as Sum of Cost Rate\, where  3 Based on the definition above, different storage strategies lead to different cost rates for the application. This cost rate, i.e cost per time unit, represents the cost-effectiveness of storage strategies, which incorporates the trade-off among computation storage and bandwidth costs in the cloud. In next section, we will present the design of our T-CSB algorithm for costeffective datasets storage based on this trade-off model IV C OST E FFECTIVE D ATASETS S TORAGE A LGORITHM IN M ULTIPLE C LOUD S ERVICES  In the section, we first briefly introduce the philosophy of the novel T-CSB algorithm, and then describe the detailed steps of the algorithm in order to find the trade-off among computation, storage and bandwidth costs for storing generated datasets with multiple cloud services In this paper, we design the T-CSB algorithm that can find the minimum cost storage strategy for storing datasets of linear DDG with multiple cloud storage services. Linear DDG means a DDG with no branches, where each dataset in the DDG only has one direct predecessor and successor except the first and last datasets. The minimum cost storage strategy found by the algorithm represents the best trade-off among computation storage and bandwidth costs in the cloud. Given a general DDG the T-CSB algorithm can be utilised in every linear segment of the DDG respectively and thus find the trade-off for storing datasets with multiple cloud services The basic idea of the T-CSB algorithm is to construct a Cost Transitive Graph \(CTG\ased on the linear DDG. First for every dataset in the DDG, we create a set of vertices in the CTG representing different storage services where the dataset can be stored. Next, we design smart rules for adding edges to the CTG and setting weights to them. Based on rules, we guarantee that in the CTG, the paths from the start vertex to the end vertex have a one-to-one mapping to the storage strategies of the DDG, and the length of every path equals to the cost rate of the corresponding storage strategy in the cloud. Then we can use the well-known Dijkstra shortest path algorithm \(or Dijkstra algorithm for short\ to find the shortest path in the CTG, which in fact represents both the minimum cost storage strategy for datasets of the DDG with multiple storage services and the best trade-off among computation, storage and bandwidth costs in the cloud Given a linear DDG with datasets   and  cloud services   for storage. The T-CSB algorithm has the following four steps Create vertices for the CTG. First, we create the start and end vertices, denoted as and Then, for every we create a vertex set  where is the number of cloud services in which can be stored. Hence represents dataset storing in cloud service  Add directed edges to the CTG. For every In other words for any two vertices we create an edge between them. Formally      Especially, for we add out-edges to all other vertices in the CTG, and for we add in-edges from all other vertices in the CTG Set weights to edges in the CTG. The reason we call the graph Cost Transitive Graph is because the weights of its edges are composed of the cost rates of datasets. For an edge   we denote its weight as    which is defined as the sum of cost rates of and the datasets 
003 003 
002 
003 
DDGd i 
z i,s z i,1 f i  f i v i provSet i CostR i 
     
ii F fd DDG 
007 010 
007\007 007\007 007 002\004 002 004\003 
F DDGd i i RCost SCR 
i f ss m 
007\007 007 007 007\007 002\004 002 004\003 007\007  
DDGd i i RCost 
002 
      
2,1   we add out-edges to all vertices in the set of   belonging to different datasetsê vertex sets \(i.e 
end ver is i s e ver ver is i s ver ver 
002 
d i c s c 1 0 d i  d i c s 0 d i d d d i provSet i d i d  d i d i c 1 d i c 1 d d CostR d CostR F f i d i F SCR A Overview of T-CSB \(Tr ade-off among Computation Storage and Bandwidth\ Algorithm B Detailed Steps in the T-CSB Algorithm d 1 d 2 d n m c 1 c 2 c m ver start ver end m d i ver i,s d i  c s  start ver d i 
  
     
004 005 006 
ii i i i is i is i i s g enCost d v f d is deleted CostR z vy fs disstoredinc miiii verververV is i s i i i i is i s ver ver CTG d d DDG d d e ver ver 
002 
CTG ver si 
CTG verver sisi 
002 007\007 011  
003 
Step 1 Step 2 Step 3 
  
002  
  
288 


      
 min  For the given linear DDG with datasets  min of its CTG is the minimum cost rate for storing the datasets in the DDG, and the corresponding storage strategy is represented by the vertices that  min  traverses In Figure 3, we demonstrate a simple example of constructing CTG for a DDG with two datasets 
003\003 
       
    
d i d i d i d i d i d i d i d i d i provSet d 1 d 2 d n m c 1 c 2 c m d 1 d 2 m n m mn m 2 n 2 O\(n 2  O\(m 2 n 4  O\(m 2 n 2  O\(m 2 n 4  A Simulation Setup and Evaluation Method 
003 003 
  
kk i k i hh i h k is is is i is is k h k d d DDG d d d d d DDG d d d ver ver z v y z xx v 
012 007 007 007\007 007 002\004\003\003 007\007 007 007\007 002\004\003\003   006  006 
between and supposing that only and are stored with corresponding cloud services and the rest of datasets between  and are all deleted. Formally 4 Since we are discussing linear DDG, for the datasets between  and  is the only dataset in their s. Hence we can further derive  Find the shortest path of the CTG. From the above construction steps, we can clearly see that the CTG is an acyclic oriented graph. Hence we can use the Dijkstra algorithm to find the shortest path from to The Dijkstra algorithm is a classic greedy algorithm to find the shortest path in graph theory. We denote the shortest path from to as   and  cloud storage services   the length of  and  different cloud services for the storage                           Figure 3. Example of constructing CTG for DDG Next, we analyses the efficiency of our T-CSB algorithm As introduced above, for a linear DDG with datasets and  cloud services for storage, we need to create vertices in the CTG. Hence the number of edges in the CTG is in the magnitude of Since the time complexity of calculating the longest edgeês weight is the worst case time complexity for creating the CTG is Next, the time complexity of the Dijkstra algorithm is Hence the total time complexity of the T-CSB algorithm is  V E VALUATION  In this section, we demonstrate simulation results conducted on Amazon cloud. First, we introduce our simulation setup and evaluation method. Then, we present general simulation results and evaluate the cost effectiveness of our algorithm As Amazon is a well-known and widely recognised cloud service provider, we conduct experiments on Amazon cloud using on-demand services for simulation. We implement our algorithm in Java programming language and run the algorithm on the virtualised EC2 instance with the Amazon Linux Image to evaluate its cost effectiveness and efficiency. We choose the standard small instance \(m1.small\o conduct the experiments because it is the basic type of EC2 CPU instances, which has a stable performance of one ECU 5  To evaluate the cost effectiveness of our T-CSB algorithm for multiple cloud storage services, we compare it with different representative storage strategies for one cloud service provider, which are as follows Store all datasets strategy, in which all generated datasets of the application are stored in the cloud Store none datasets strategy, in which all generated datasets of the application are deleted after being used Cost rate based strategy reported in [3 3 5 in  w h ich  w e  store datasets in the cloud by comparing their own generation cost rate and storage cost rate Local-optimisation based strategy reported in  which we only achieve the localised optimum of the tradeoff between computation and storage in the cloud Next, we assume that the scientific application be deployed in Amazon cloud using EC2 service 6 0.1 per CPU instance hour\ for computation and S3 service \($0.15 per gigabyte per month\ for storage. To utilise our T-CSB algorithm, we assume that generated datasets can be transferred to another two cloud services for storage with the prices: Storage Service One: $0.1 per gigabyte per month for storage and $0.01 per gigabyte for outbound 7 data transfer and Storage Service Two: $0.05 per gigabyte per month for storage and $0.06 per gigabyte for outbound data transfer. We only use the above prices as representatives, as many cloud service providers \(e.g. GoGrid 8 Rackspace 9 Haylix 10 and Amazon Glacier 11 etc.\ave similar pricing models    5 ECU \(EC2 Computing Unit\ is the basic unit defined by Amazon to measure the compute resources. Please refer to the following address for details http://aws.amazon.com/ec2/instance-types  6 Amazon cloud service offers different CPU instances with different prices where using expensive CPU instances with higher performance would reduce computation time. There exists a trade-off of time and cost [1 w h ich i s  different to the trade-off of computation and storage described in this paper hence is out of this paperês scope 7 At present, most cloud storage services only charge on the outbound data transfer, while inbound data transfer is usually free 8 GoGrid http://www.gogrid.com  9 Rackspace http://www.rackspace.com  10 Haylix http://www.haylix.com  11 Amazon Glacier http://aws.amazon.com/glacier  
     
   
Step 4 
kk i k i kk i k i is i s ik d d DDG d d d is i is k k d d DDG d d d ver ver CostR CostR z v y genCost d v 
012 007 007\007 007\007 007 007\007 002\004\003\003 002\004\003\003 006   
end start ververP end start ververP end start ververP 
 
      
start ver end ver start ver end ver 
289 


 
The simulations are conducte DDG with datasets of random s i usage frequencies. In the experim e large DDGs with different numb e random size from 1GB to 100GB random, from 10 hours to 100 ho u again random, from once per mont h to run our T-CSB algorithm, we p a linear DDG segments with 50 dat a the T-CSB algorithm Based on the above settings, we DDGs with different number of da t rates \(i.e. average daily cost\ of st o shows the increases of the daily c o the number of datasets grows i n illustrates detailed datasets storage different storage strategies From Figure 4, we can see that t store all datasetsé strategies are investigating the trade-off betwee n  12 The impact of DDG partition on cost-effe c strategy has been investigated in our prior w o  
Strategies  DDGs Cost Rate based Strategy LocalOptimisation based Strategy 100 datasets 200 datasets 300 datasets 500 datasets 700 datasets 1000 datasets T-CSB algorithm with Two Additional Storage Services T-CSB algorithm with Additional Haylix Storage T-CSB algorithm with Additional Glacier Storage 
Figure 4 C TABLE I D ET A Deleted Stored S3 Deleted Stored S3 64 36 57 43 133 67 118 82 203 97 176 124 334 166 286 214 466 234 406 294 644 356 577 423   To further demonstrate the practicality algorithm, we adapt real cloud service p r models and use them as the additional clou d respectively in the simulation. Specifically 1\mazon Glacier. Glacier is an ex t storage service that provides secure and du r data archiving and backup. The pricing model is: $0.01 per gigabyte per month for storage 0 for outbound data transfer from Glacier 2\ Haylix cloud storage. Haylix is a l e IaaS cloud service provider, who provide storage with fast access for local Australia n transfer over the Internet is often expensive a n in general, some cloud service providers cooperate with network Infrastructure provid e to provider dedicate connection service \(e Connect\sting the data transfer speed cloud. Hence, we use the pricing models of H Direct Connect in our simulation, i.e. $0.12 month for storage in Haylix, $0.046 p er giga b data transfer from Haylix C ost-effectiveness comparison of different storage strategies A ILED DATASETS STORAGE STATUS OF DIFFERENT STORAGE STRATEGIES  Deleted Stored S3 Stored Service1 Stored Service2 Deleted Stored S3 Store d Hayli x 43 0 29 28 57 38 5 93 0 38 69 118 72 10 149 0 69 82 173 110 17 223 0 98 179 286 187 27 324 0 150 226 404 262 34 428 0 182 390 573 379 48 of our T-CSB r ovidersê pricing d storage service t remely lo w cost r able storage for for using Glacier 0 02 per gigabyte e ading Australian s reliable cloud n users. As data n d relatively slow e.g. Amazon e rs \(e.g. Equinix g. AWS Direct in and out of the H aylix and AWS per gigabyte per b yte fo r outbound d  x  Deleted Stored S3 Stored Glacier 5 0 95 29 0 171 29 0 271 50 0 450 67 0 633 103 0 897 d o n randomly generated i zes, generation times and e nts, we randomly generate e r of datasets, each with a T he generation time is also u rs. The usage frequency is h to once pe r year. In order a rtition the large DDGs into a sets 12 on which we apply run evaluation strategies on t asets and calculate the cost o ring the datasets. Figure 4 o st of different strategies as n the DDG, and Table I status of the DDGs under t he çstore none dataseté and very cost ineffective. By n computation and storage   c tiveness and efficiency of storage o rk  
34 
 
B Simulation Results 
290 


  
the çcost rate based strategyé and çlocal-optimisation based strategyé can smartly choose to store or delete the datasets in one cloud storage service \(as shown in Table I\, thereby largely reducing the cost rate for storing datasets with one cloud service provider. If more cloud storage services are available as shown in Figure 4, the simulation of çT-CSB algorithm with two additional storage servicesé demonstrates further reduction of the cost rate by taking bandwidth cost into account. Table I shows the number of datasets transferred and smartly stored in two representative cloud storage services with our T-CSB algorithm. Furthermore, how much cost can be reduced depends on the price of available storage services. In the simulation of çT-CSB algorithm with additional Haylix storageé, although some datasets are transferred to Haylix for storage \(as shown in Table I\, the cost rate only drops slightly comparing to the çlocal-optimisation based strategyé \(as shown in Figure 4\. This is because the price of Haylix is not much cheaper than Amazon S3 cloud. In contrast, in the simulation of T-CSB algorithm with additional Glacier storageé, our T-CSB algorithm significantly reduces the cost rate \(as shown in Figure 4\ by transferring datasets to Glacier 13 for storage \(as shown in Table I From the above simulation, we can see that for different price models of cloud storage services, our T-CSB algorithm can always store the datasets accordingly, even in the situation that the price difference is minor \(e.g. the simulation of çTCSB algorithm with additional Haylix storageé\. Hence our TCSB algorithm is very effective in reducing the cost \(i.e. costeffective\ for storing generated application datasets with multiple service providers in the cloud VI R ELATED W ORK  Today, research on scientific applications in the cloud becomes popular [1    30 Co m p ar i ng t o  t h e  traditional computing systems, e.g. cluster, grid and HPC systems, a cloud computing system has cost benefits in various aspects [4   Wi th A m azon c l ou ds  c o s t m ode l an d  BO I N C  volunteer computing middleware, the work in [1 an aly s es  th e cost benefits of cloud computing versus grid computing. The work by Deelman et al. [11 a l s o  a p p lies Am azon  cl ou ds   c o s t  model and demonstrates that cloud computing offers a costeffective way to deploy scientific applications. The work mentioned above mainly focuses on the comparison of cloud computing systems and the traditional distributed computing paradigms, which shows that applications running in the cloud have cost benefits. However, our work focuses on reducing cost for running application in the cloud This paper is mainly inspired by the research in the area of scheduling, in which much work focuses on reducing various costsé for applications [2 sy st e m s [3 1 o r d a t a ce ntr e networks T h e d i ffer e n c e i s t h a t  sc h e d u l i ng a i m s  at  improving resource utilisation whilst our work investigates the trade-off among computation, storage and bandwidth costs which is a unique issue in cloud computing due to the pay-asyou-go model. Another important foundation for our work is the research on data provenance. Due to the importance of data    13 Data stored in Glacier usually need 3 to 5 hours to become available when users retrieve them. As analysed in Section II.B, usersê delay tolerance is out of the scope of this paper. Hence we only focus on the cost in the simulation provenance in scientific applications, many works about recording data provenance of the system have been done [9   Recently, research on data provenance in cloud computing systems has also appeared M o r e spe c ific a lly  Ost e r w e il e t  al e s e n t ho w t o ge ner a t e a da ta  de r i v a t i o n g r ap h  fo r execution of a scientific workflow. Foster et al. [1 pr o p o s e  the concept of virtual data in the Chimera system, which enables the automatic regeneration of datasets when needed Our DDG is based on data provenance, which depicts the dependency relationships of all the generated datasets in the cloud. With DDG, we can manage where the datasets are stored or how to regenerate them As the trade-off among computation, storage and bandwidth is an important issue in the cloud, much research has already embarked on this issue to a certain extent. First plenty of research has been done with regard to the trade-off between computation and storage. The Nectar system [15 is  designed for automatic management of data and computation in data centres, where obsolete datasets are deleted and regenerated whenever reused in order to improve resource utilisation. In [11  De elm an et al  p r es en t th at st o r in g so m e  popular intermediate data can save the cost in comparison to always regenerating them from the input data. In [2 A d am s e t  al. propose a model to represent the trade-off of computation cost and storage cost. In [33   th e au th o r s  pr o p o s e th e C T T SP algorithm that can find the best trade-off between computation and storage in the cloud, based on which a highly cost-effective and practical strategy is developed for storing datasets with one cloud service provider [3  How e v e r th e a b ov e w o rk  di d n o t  consider bandwidth cost into the trade-off model. In [6 B a li g a  et al. investigate the trade-off among computation, storage and bandwidth in the infrastructure level of cloud systems, where reducing energy consumption is the main research goal. In [3   Agarwala et al. transform application data to certain formats and store them with different cloud services in order to reduce storage cost in the cloud, but data dependency and the option of data regeneration are not considered in their work. In this paper we propose the T-CSB algorithm which can find the best tradeoff among computation, storage and bandwidth costs for storing datasets of linear DDG in the cloud. This algorithm can be utilised for cost-effectively storing generated application datasets with multiple service providers in the cloud VII C ONCLUSIONS AND F UTURE W ORK  In this paper, we have investigated the unique features of storing large volume of generated scientific datasets with multiple cloud service providers in the cloud. Towards achieving the cost-effectiveness, we have proposed a T-CSB Trade-off among Computation, Storage and Bandwidth algorithm to find the minimum cost storage strategy for datasets of linear DDG, which also represents the best trade-off among three key factors \(computation, storage and bandwidth for the cost of data storage in the cloud.  This algorithm can be utilised for cost-effectively storing generated application datasets with multiple service providers in the cloud. General simulations indicate that our T-CSB algorithm is very effective in reducing cost for cloud storage In our current work, we assume that the storage of one cloud service provider have a unified price. However, in the 
291 


pp. 1-5 2009 3 S  Ag a r wa la  D Ja d a v  a n d L  A B a t h e n   i C o st a l e  Ad a p t i v e  C o st  Optimization for Storage Clouds," in pp. 436-443, 2011 4 M  Ar mb ru st  A Fo x   R  Gri f fi t h  A D  J o s e p h   R  Ka t z  A Kon wi n sk i   G. Lee, D. Patterson, A. Rabkin, I. Stoica, and M. Zaharia, "A View of Cloud Computing vol. 53, pp. 50-58, 2010 5 M   S  Av i l a Ga rc i a  X  Xi on g A E   T r e f e t h e n   C   C r i c h t on  A T s u i   a n d  P. Hu, "A Virtual Research Environment for Cancer Imaging Research in pp. 1-6 2011   J  Ba l i g a R W   A y re  K   H int o n and R  S   T u ck er  G reen c lo u d  computing: Balancing energy in processing, storage, and transport vol. 99, pp. 149-167, 2011   R. B o s e and J   F r ew   L in e a ge R e tr i e va l  for S c i e n tific Data Pro c es s ing  A  Survey vol. 37, pp. 1-28, 2005   A   Bu r t o n and A  T r elo a r P ub l i s h M y D a ta  A  C o m p os iti o n of  Serv i c e s  from ANDS and ARCS," in pp. 164-170, 2009   P. Ch e n  B Pl a l e, and M S A k tas  T em por a l  re pres e n t a ti o n  fo r s c i e n tific data provenance," in pp. 1-8, 2012   Y  Cui, H  W a ng and X   Ch e n g C hann el A llocati o n  in W i reles s  Data Center Networks," in pp. 1395-1403, 2011   E  Deelm an  G  S i n g h M L i v n y  B  B e rr im an, and J   G o o d  T h e C o s t  of  Doing Science on the Cloud: the Montage Example," in pp. 1-12, 2008   I  F o s t er J   Vo ck ler M. W i l d e   and Z. Yo n g  C him er a  A V ir tu a l  D a ta System for Representing, Querying, and Automating Data Derivation," in pp. 37-46, 2002   I  F o s t er, Z. Y o n g I   Ra icu and S  L u  C lo ud C o m putin g and G r id  Computing 360-Degree Compared," in pp. 1-10, 2008   S K  G a rg   R  Bu y y a and H   J   Si egel  T i m e and C os t T r ad e Off  Management for Scheduling Parallel Applications on Utility Grids vol. 26, pp. 1344-1355, 2010   P. K  G unda L   R a v indr an ath C A  T h ekkath, Y  Y u   and L  Z huan g  Nectar: Automatic Management of Data and Computation in Datacenters," in pp. 1-14, 2010   X. H u an g  Z  L u o and B  Yan C y b er infr a s t r uctu re  and e-S ci e n c e  Application Practices in Chinese Academy of Sciences," in pp. 348-354 2011   M  H u m p h r ey  N  B e e k w i l d er J   L   G ood a l l  and M B E r c a n Calibration of watershed models using cloud computing," in pp. 1-8, 2012   G  J u ve E. Deelm an, K  V a hi  and G   M e h t a, "D ata S h a r in g O p ti o n s  for  Scientific Workflows on Amazon EC2," in pp. 1-9, 2010   D. K ond o   B. J a v a di  P Ma lec o t, F   Capp ello and D  P  A n d e r s o n   C o s t Benefit Analysis of Cloud Computing versus Desktop Grids," in pp. 1-12, 2009 20  X  L i u  Z  Ni  D  Yu a n  Y  J i a n g Z   Wu  J  C h en   a n d  Y  Ya n g   A N o v e l Statistical Time-Series Pattern based Interval Forecasting Strategy for Activity Durations in Workflow Systems vol. 84, pp. 354-376, 2011   B  L uda s c h e r   I   A ltint as  C B e r k ley  D H i gg in s  E J a eger, M  J o n e s  and E. A. Lee, "Scientific Workflow Management and the Kepler System pp. 1039Ö1065 2005   K  K  Munis w am y R e dd y  P  Mack o  and M. Selt zer   P rove nanc e  for th e  Cloud," in pp. 197-210, 2010   H Ng u y en and D A b r a m s o n  W ork W a y s   I n t e r acti ve W o r k flow b a s e d  Science Gateways," in pp. 1-8, 2012   L  J  Os t e rw ei l  L  A   C l a r k e A   M E llis o n   R  Po d o roz hn y A  W i s e   E   Boose, and J. Hadley, "Experience in Using A Process Language to Define Scientific Workflow and Generate Dataset Provenance," in pp. 319-329, 2008   J  Q iu, J  Ekana y ak e  T  G una r a thn e   J   Y Ch o i, S  H  Ba e, H  L i  B   Zhang, Y. Ryan, S. Ekanayake, T.-L. Wu, A. Hughes, and G. Fox Hybrid Cloud and Cluster Computing Paradigms for Life Science Applications vol. 11, 2010 26  X  Su  Y M a   H  Ya n g   X C h a n g  K  N a n  J  Xu  a n d  K N i n g   An Op en Source Collaboration Environment for Metagenomics Research," in pp. 7-14, 2011 27  A   S  S z al ay a n d J   G r ay    S c i e n ce i n a n  Ex po ne n t i a l  W o r l d    vol 440, pp. 23-24, 2006   S. Toor  M. S a b e s a n, S  H o lm gre n and T   R i s c h, "A S c a l ab le A r chit ecture for e-Science Data Management," in pp. 210-217, 2011   D W a r n e k e and O  K a o  E x p loiting Dy na m i c R e s o u r c e A llocati o n  for  Efficient Parallel Data Processing in the Cloud vol. 22, pp. 985-997, 2011   Y. Y a n g K  L i u, J   Ch e n X  L i u, D   Yuan and H   J i n   A n A l gorithm  in SwinDeW-C for Scheduling Transaction-Intensive Cost-Constrained Cloud Workflows," in pp. 374-375, 2008  L   Yo un g Ch oo n and A  Y Z o m a y a   E n e rgy C o ns ci o u s Sch e du l in g for  Distributed Computing Systems under Different Operating Conditions vol. 22, pp. 13741381, 2011   D Y u an  Y   Y a n g  X   L i u  and J  Ch e n   A C o st-Effecti ve S t r a t e gy  for Intermediate Data Storage in Scientific Cloud Workflows," in  pp. 1-12, 2010   D Y u an Y   Y a n g   X   L i u and J  Ch e n  O nd e m a nd Minim um C os t  Benchmarking for Intermediate Datasets Storage in Scientific Cloud Workflow Systems vol 71, pp. 316-332, 2011 34  D  Yu a n   Y  Ya n g X L i u  W  L i  L   C u i   M  Xu  a n d  J   C h en    A Hi gh l y Practical Approach towards Achieving Minimum Datasets Storage Cost in the Cloud vol 24, pp. 1234-1244, 2012 35  D  Yu a n   Y   Ya n g  X L i u  G Z h a n g  a n d  J  C h e n    A Da t a D e p e n d e n c y  Based Strategy for Intermediate Data Storage in Scientific Cloud Workflow Systems vol. 24, pp. 956-976, 2012   M. Z a h a r i a, A  K o n w ins ki A  D   J o s e ph R. K a t z and I  S t o ica  Improving MapReduce Performance in Heterogeneous Environments," in pp. 29-42, 2008  
real world, the price of cloud storage is different according to different usages. In the future, we will incorporate more complex pricing models in our datasets storage cost model Furthermore, methods for forecasting dataset usage frequency can be further studied, with which our T-CSB algorithm can be adapted to different types of applications more easily A CKNOWLEDGMENT  The research work reported here is partly supported by Australian Research Council under DP110101340 and LP130100324, Shanghai Knowledge Service Platform Project No. ZF1213. We are also grateful for the discussions on Finite Element Modelling application with Dr. S. Xu from Faculty of Engineering and Industrial Sciences, Swinburne University of Technology R EFERENCES    A m a zo n C l o ud S e rv ic es    http://aws.amazon.com  2  I  A d a m s  D  D  E  L o n g  E  L   M i l l e r   S  P a s u p a t h y  a n d  M  W  S t o r e r   Maximizing Efficiency by Trading Storage for Computation," in 
Workshop on Hot Topics in Cloud Computing \(HotCloud'09 IEEE International Conference on Cloud Computing \(CLOUD2011 Communication of the ACM 7th International Conference on E-Science \(e-Science2011 Proceedings of the IEEE ACM Computing Surveys 5th International Conference on E-Science \(eScience'09 8th International Conference on E-Science \(eScience2012 IEEE INFOCOM 2011 ACM/IEEE Conference on Supercomputing \(SC'08 14th International Conference on Scientific and Statistical Database Management, \(SSDBM'02 Grid Computing Environments Workshop \(GCE'08 Future Generation Computer Systems 9th Symposium on Operating Systems Design and Implementation \(OSDI'2010 7th International Conference on E-Science \(e-Science2011 8th International Conference on E-Science \(e-Science2012 ACM/IEEE Conference on Supercomputing \(SC'10 23th IEEE International  Parallel & Distributed Processing Symposium IPDPS'09 Journal of Systems and Software Concurrency and Computation: Practice and Experience 8th USENIX Conference on File and Storage Technology  FAS T'10 8th International Conference on E-Science \(eScience2012 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering Journal of BMC Bioinformatics 7th International Conference on E-Science \(e-Science2011 Nature 7th International Conference on EScience \(e-Science2011 IEEE Transactions on Parallel and Distributed Systems 4th International Conference on E-Science \(eScience2008 IEEE Transactions on Parallel and Distributed Systems 24th IEEE International Parallel & Distributed Processing Symposium \(IPDPS'10 Journal of Parallel and Distributed Computing IEEE Transactions on Parallel and Distributed Systems Concurrency and Computation: Practice and Experience 8th USENIX Symposium on Operating Systems Design and Implementation \(OSDI'2008 
292 


Jorda Polo, David Carrera, Yolanda Becerra, Malgorzata Steinder  and Ian Whalley. Performance-driven task co-scheduling for  mapreduce environments. In Network Operations and Management  Symposium \(NOMS\2010 IEEE, pages 373 Ö380, 19-23 2010 12 K. Kc and K. Anyanwu, çScheduling hadoop jobs to meet deadlines  in 2nd IEEE International Conference on Cloud Computing  Technology and Science \(CloudCom\, 2010, pp. 388 Ö392 13 Xicheng Dong, Ying Wang, Huaming Liao çScheduling Mixed Real time and Non-real-time Applications in MapReduce Environment  In the proceeding of 17th International Conference on Parallel and  Distributed Systems. 2011, pp. 9 Ö 16 14 Xuan Lin, Ying Lu, J. Deogun, and S. Goddard. Real-time divisible  load scheduling for cluster computing. In Real Time and Embedded  Technology and Applications Symposium, 2007. RTAS ê07. 13th  IEEE pages 303 Ö314, 3-6 2007 15 HDFS  http://hadoop.apache.org/common/docs/current/hdfsdesign.html  16 Chen He, Ying Lu, David Swanson. çMatchmaking : A New  MapReduce Scheduling Techniqueé. In the proceeding of 2011  CloudCom, Athens, Greece, 2011, pp. 40 Ö 47 17 Matei Zaharia, Dhruba Borthakur, Joydeep Sen Sarma and Khaled  Elmeleegy, Scott Shenker, and Ion Stoica, çDelay scheduling: a  simple technique for achieving locality and fairness in cluster  schedulingé. In the proceedings of the 5th European conference on  Computer systems, 2010.  pp 265-278 18 Zhuo Tang, Junqing Zhou, Kenli Li, and Ruixuan Li "A MapReduce  task scheduling algorithm for deadline constraints.", Cluster  Computing, Vol. 15,  2012 19 Eunji Hwang, and Kyong Hoon Kim. "Minimizing Cost of Virtual  Machines for Deadline-Constrained MapReduce Applications in the  Cloud." Grid Computing \(GRID\, 2012 ACM/IEEE 13th  International Conference on. IEEE, 2012 20 Micheal Mattess, Rodrigo N. Calheiros, and Rajkumar Buyya  Scaling MapReduce Applications across Hybrid Clouds to Meet Soft  Deadlines." Technical Report CLOUDS-TR-2012-5, Cloud  Computing and Distributed Systems Laboratory, the University of  Melbourne, August 15, 2012 21 
 
11 
                
Chen He, Ying Lu, David Swanson. çReal-Time Application Scheduling in Heterogeneous MapReduce Environments Technical Report TR-UNL-CSE2012-0004, University  of Nebraska-Lincoln, 2012 Available: http://cse apps.unl.edu/facdb/publications/TR-UNL-CSE20120004.pdf 22 T. Condie, N. Conway, P. Alvaro, J. M. Hellerstein, K  Elmleegy, and R. Sears. çMapreduce Onlineé. In NSDI 2010 23 A. D. Ferguson, P. BodÌk, S. Kandula, E. Boutin, and R  Fonseca. çJockey: Guaranteed Job Latency in Data Parallel Clusters. In EuroSys, 2012 24 G. Wang, A. R. Butt, P. Pandey, and K. Gupta. çA Simulation Approach to Evaluating Design Decisions in MapReduce Setupsé. In MASCOTS 2009 25 H. Herodotou and S. Babu. Profiling, çWhat-if Analysis and Cost-based Optimization of MapReduce Programs In VLDB 2011 26 H. Herodotou, F. Dong, and S. Babu. çNo One \(Cluster Size Fits All: Automatic Cluster Sizing for Dataintensive Analyticsé. In SoCC 2011  
1544 
1544 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


