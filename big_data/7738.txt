An Adaptive Algorithm for Incremental Mining of Association Rules N.L Sarda N V Srinivas Department of Computer Science and Engineering Indian Institute of Technology Bombay Mumbai India nls@cse.iitb.ernet.in Abstract The association rules represent an important class of knowledge that can be discovered from data warehouses Current research efforts are focused on inventing ef\002cient ways of discovering these rules from large databases As databases grow the discovered rules need to be veri\002ed and new rules need to be added to the knowledge base Since mining afresh every time the database grows is inef\002cient algorithms for incremental mining are being investigated Their primary aim is to avoid or minimize scans of the older database by using the intermediate data constructed during the earlier mining In this paper we present one such algorithm We make use of large and candidate itemsets and their counts in the older database and scan the increment to 002nd which rules continue to prevail and which ones fail in the merged database We are also able to 002nd new rules for the incremental and updated database The algorithm is adaptive in nature as it infers the nature of the increment and avoids altogether if possible multiple scans of the incremental database Another salient feature is that it does not need multiple scans of the older database We also indicate some results on its performance against synthetic data 1 Introduction Data mining which is also referred to as knowledge discovery in databases is a process of nontrivial extraction of implicit previously unknown and potentially useful information from data in a database Data mining has recently attracted considerable attention from database user community as they realize that this information locked inside the large organizational databases built over many years can provide information and knowledge for enhancing their organization's effectiveness and competitiveness The process of data mining provides knowledge in the form of rules and patterns based on statistical analysis of data The process is challenging because the source databases from which the knowledge is extracted are large and growing The knowledge itself is time-varying as some rules and patterns may hold now but not in future or vice-versa The mining techniques must scale well to handle very large and growing databases and should permit ef\002cient maintenance of extracted knowledge One of the most studied data mining problems is mining for association rules Given a collection of items and a set of records i.e transactions each of which contain some number of items from the given collection the association rules indicate af\002nities that exist among the collection of items These af\002nities can be expressed by rules such as 22462  of all the records that contain items A B and C also contain items D and E.\224 The speci\002c percentage of occurrences is called the con\002dence factor of the rule A database may throw up a very large number of association rules Much work has been done in the 002eld of 002nding association rules 1  2  3   8 6  These e f f orts are d irected at devising algorithms to mine the rules ef\002ciently in large databases They commonly require multiple scans of the given database As databases grow over time there is a need to undertake mining again for maintaining i.e verifying rules discovered earlier and also for discovering new rules However it has been realized that applying the proposed algorithms on the updated database i.e the older and the incremental database together may be too costly Researchers are now investigating ways by which rule maintenance can be done by processing the incremental part separately and scanning the older database only if necessary To achieve this the incremental mining algorithms generally plan to use intermediate information collected during earlier mining process This strategy for mining is also re 


ferred to as incremental mining Comparatively there is not enough work in the 002eld of incremental mining 4  5  7 Maintaining sets of discovered rules poses several challenges because the underlying data changes over time and therefore the current rules may become invalid while new rules may exist undetected New rules may be valid only for the incremental database or for the whole database If the purpose of the incremental mining is just to validate the existing rules then it is a trivial process which can be done in one pass of the incremental database we just have to count the support of all the large itemsets of the original database in the incremental database However there is always a need for 002nding new rules both in the increment and the updated database Since mining for association rules is costly we would like to somehow use the information that was found during the earlier mining process both to maintain and generate the rules An Adaptive algorithm is proposed in this paper that makes effective use of the information discovered during the earlier process of mining Moreover the knowledge of the type of incremental database is also used by the algorithm for improving its performance further Experiments have been conducted to study the performance of the Adaptive algorithm The remaining of the paper is organized as follows A brief review of the related work is given in Section 2 Different types of incremental data are identi\002ed in Section 3 Section 4 contains description of the proposed Adaptive algorithm The test data generation and performance studies are given in Section 5 Finally Section 6 contains concluding remarks 2 Background and Related Work The task of mining association rules can be formally stated as follows  Let I  f i 1 i 2 i m g be a set of literals called items Let D be a set of transactions where each transaction T is a set of items such that T 2 I  We say that a transaction T contains X  a set of some items in I if X 2 T An association rule is an implication of the form X  Y w h e r e X 032 I Y 032 I and X  Y 036 TheruleX  Y holds in the transaction set D with con\002dence c if c of transactions in D that contain X also contain Y The rule X  Yhas support s in the transaction set D if s of the transactions in D contain X  Y The problem of mining association rules is to generate all association rules that have support and con\002dence greater than the user-speci\002ed minimum support and minimum con\002dence respectively In all the different algorithms given in the literature the problem of discovering association rules is decomposed into two subproblems 2  017 Find all sets of items  itemsets  that have support above the minimum support The support for an itemset is the number of transactions that contain the itemset Itemsets with minimum support are called large itemsets 017 Use the large itemsets to generate the desired rules For every large itemset l  002nd all non-empty subsets of l  For every such subset a  output a rule of the form a   l-a  if the ratio of support  l  to support  a  is at least minconf  Since it is easy to generate association rules if the large itemsets are available major research efforts have been focused on 002nding ef\002cient algorithms to compute large itemsets An itemset is called k-itemset if the number of items in the itemset is k  We denote as L k the set of large k-itemsets Candidate itemsets are those which are potentially large itemsets Let C k denote the set of candidate k-itemsets The Apriori algorithm 3 mak e s m u ltip le p a sses o v e r the database to 002nd all large itemsets In the 002rst pass the algorithm simply counts item occurrences to determine large 1-itemsets A subsequent pass say pass k consists of two phases First the large itemsets L k 000 1 found in the k1 pass are used to generate the candidate itemsets C k  using the apriorigen function This function 002rst joins L k 000 1 with itself the joining condition being that the lexicographically ordered 002rst k-2 items are the same Next it deletes all those itemsets from the join result that have some k-1 subset that is not in L k 000 1  yielding C k  The algorithm now scans the database For each transaction it determines which of the candidates in C k are contained in the transaction and increments the count of those candidates To ef\002ciently determine which of the candidates in C k are contained in a transaction it uses a data structure called hash tree  Toivonen  has p ropos ed a s ampl i n g b as ed al gori t h m for mining association rules They take a random sample of the database 002nd large itemsets and then verify the results with the whole database They make use of negative border to 002nd candidate itemsets Since we also use this concept we will de\002ne it below Negative border 8  Gi v e n R as a s et of items and a collection S 022 P R of sets  P R represents the powerset of R closed with respect to the set inclusion relation the negative border B 000  S f S consists of the minimal itemsets X 022 R not in S  For example let R  A,B  Fandassume the collection S of frequent itemsets in some relation r and some speci\002ed minimum support s s f A g  f B g  f C g  f F g  f A,B g  f A,C g  f A,F g  f C,F g  f A,C,F g  The negative border for the above collection B 000  S  ff B,C g  f B,F g  f D g  f E gg  The intuition behind the concept of negative border is that given a closed collection S of sets that are frequent the negative border contains the 224closest\224 itemsets 2 


that could be frequent too Let DB be the original database and db be the incremental database The problem of incremental mining is to 002nd the set L DB  db of large itemsets in the updated database DB  db  We may also be interested in 002nding the set L db of large itemsets in db  In 4 C h eung et al des c ri be t h e F as t U pdat e A l gori t h m FUP for incrementally maintaining association rules from large databases Basically the framework of FUP is similar to that of Apriori It contains a number of iterations We will brie\003y review FUP algorithm so that we can compare it with the algorithm proposed in this paper Let L k denote the set of all sizek large itemsets in DB  and L 0 k represent the set of all large k itemsets in DB  db  C k is the set of sizek candidate itemsets in the k th iteration of FUP Let X suppor t D  X suppor t d and X suppor t UD denote support counts of an itemset X in DB  db and DB  db  respectively The main steps in the FUP algorithm are  1 Scan db for all itemsets X 2 L 1  and update its support count X suppor t UD  Remove those itemsets X for whom the condition X suppor t UD s 002  D  d  is true In the same scan the candidate set C 1 is created to store all size-one itemsets which are not in L 1 Their support in db is also found in the same scan The itemsets in C 1 whose X suppor t d is less then s 002 db are deleted as they cannot be large in the updated database 2 Scan DB to update the support of each X 2 C 1 By checking their support count new large itemsets from C 1 are found By combining these with those identi\002ed in L 1  the set of all size-one large itemsets L 0 1 is generated 3 The above steps are repeated for all the later iterations until no large itemset is found In iteration k  C k is generated by 002rst using the join-based apriori-gen function 3 o n L 0 k 000 1 and then pruning C k by removing those itemsets which are already present in L k  Although FUP algorithm makes multiple scans of db and DB it achieves signi\002cant ef\002ciency because it avoids recomputations for itemsets which were already found large during miming of DB and it signi\002cantly reduces number of new candidate sets through a simple pruning strategy It also prunes the given databases by removing transactions containing items that do not have a chance of appearing in large itemsets The performance studies on the FUP algorithm 4 sho w signi\002cant improvement compared to the association algorithms applied to the updated database afresh For small support levels the performance improvement is 3 to 6 times and for large itemsets it is 2 to 3 times The studies also show that there is 98 to 95 reduction in number of candidate itemsets at each iteration Another observation is that the performance gain reduces when the increment db is large in size In 5 th e F UP alg o r ith m h as b een e x ten d e d to handle insertions as well as deletions from the original database A temporal windowing technique for incremental maintenance of association rules is proposed in 7 Thei r a pproach is based on the premise that transactions outside auser-de\002ned time window are too old to contribute towards association rules of current interest They also de\002ne a strong support threshold and near strong threshold levels and also the corresponding strong and near strong con\002dence levels for mining strong and near-strong association rules These near-strong rules have the potential to become strong association rules during the next time window Consequently their update algorithm retires old and outdated transactions and carries out mining using the incremental database only As they note in their conclusions the time window and the deviations from strong support for de\002ning near string support may be hard to de\002ne Their technique where applicable can lead to substantial performance improvement in maintaining the contemporary association rules 3 Types of Incremental databases The database used in mining for knowledge discovery is dynamic in nature The existing data may be updated and new transactions may be added over time we will however assume that the database represents history and hence changes are of append type only The knowledge discovered from these databases is also dynamic The changes represent changing policies of the organization which,in fact may be based on knowledge obtained from past mining or changes in behavior of the customers or due to seasonal nature of demands The objective of incremental mining is to avoid re-learning of rules from the old data 7 and utilize knowledge that has already been discovered The new transactions representing new happenings in the organization are accumulated over a period of time The incremental database db collected during time period p is periodically merged with the original database DB representing history up to time T The period p will be chosen such that suf\002cient number of new transactions are accumulated to indicate changing patterns if any Let A T and A T  p represent rules discovered from DB and the updated database DB+db up to time T and T+p for a given level of support and con\002dence A T  p may contain some new rules called winners  Some rules from A T  called losers  may be absent in A T  p  However we must note that the increment db may itself contain some interesting patterns which may be a consequence of new business strategies or seasonal customer demands We also need to 002nd the rules A p which hold only in the increment Note 3 


that these may be missed in A T  p  Thus for rulebase maintenance we need an approach where both the increment database db and the updated database DB+db are mined Our objective in mining is still to avoid multiple scans of the two databases by utilizing intermediate information obtained in the earlier mining of DB The cost of maintaining the rules can be reduced considerably once we know the type of incremental database being mined We visualize the following types of incremental databases  1 The incremental database can be considered as a sample of the of the original database In this case we do not expect any signi\002cant differences between the patterns in the original and the updated databases An example of such a database is the Treatment database describing drugs used for curing diseases Here changes in treatments are infrequent 2 The incremental database has more or less similar patterns to that of the original database Essentially the original patterns may still exist but there may be a few new patterns An example could be the Point of Sale database During the mining of the original database we may have observed a few associations Subsequently we may have laid out new strategies These might have resulted in new associations We expect the updated database to re\003ect them 3 The incremental database may not necessarily be a good sample of the original database The patterns found in the incremental and the original database may be entirely different An example of such an incremental database is one containing patterns of seasonal nature In the next Section we will describe an algorithm for incremental mining which adapts itself for better performance by observing the type of incremental data that it is mining 4 Adaptive Algorithm 4.1 De\002nitions and Notations 017 The set of candidate itemsets in the original database is denoted by C DB where C DB  S i  n i 1 C DB i  017 The set of large itemsets in the original database is denoted by L DB where L DB  S i  n i 1 L DB i  017 The set of itemsets which are candidate itemsets but not large in the original database is denoted by C L  017 Set of itemsets which are large in the incremental database is denoted by L db where L db  S i  p i 1 L db i  4.2 Approach Initially we assume the incremental database to be a good sample of the original database i.e the incremental database is of type 1 We start counting the support of all the candidate itemsets C DB  of the original database C DB is obtained by combining the large itemsets in DB with its negative border After counting the support of the itemsets in C DB  which gives us L db  the algorithm applies the following rules to 002nd out whether more scans of the incremental database are required 017 Rule 1  If the set of large itemsets in the incremental database L db  is a subset of L DB  then no more scans of either the incremental database or the original database is required 017 Rule 2  If any of the local large itemsets X 2L db is in C L then there is a possibility that we have missed out some local large itemsets In this case we have to scan the incremental database again Let us look at an example Let us suppose f A g  f B g  f C g  f D g  f E g  f AB g  f AC g  f BC g  f ABC g are the candidate itemsets in the original database of which f A g  f B g  f C g  f AB g  f AC g and f BC g are large in the original database As noted above the candidate itemsets include large itemsets and the negative border Now if the the set of local large itemsets L db is a subset of L DB  then we don't have to scan anymore either the incremental database or the original database But instead if f D g turns out to be large in the incremental database then there is a possibility that we have missed out some large itemsets like f AD g  f BD g  f CD g etc To 002nd out the missed local large itemsets we have to rescan the incremental database This is the time we can infer the nature of incremental database If the local large itemsets belonging to C L are not too many we can generate all possible itemsets which could be large in the incremental database and count their support in one more scan Thus we may totally require two scans of the incremental database For instance in the above example if only D turns out to be the local large itemset then we will count the support of the following in the next scan f AD g  f BD g  f CD g  f ABD g  f ACD g and f BCD g Instead if the local large itemsets belonging to C L are plenty in number then we switch to level wise algorithm to mine the incremental database for remaining large itemsets For example in the above case if both f D g and f E g turn out to be local large itemset and the candidate itemsets generated by these itemsets is above some threshold then we switch to the level wise algorithm i.e in the kth pass or scan of the incremental database support for candidate k itemsets is counted Here we can optimize by not counting candidate k itemsets whose support has already been counted Thus 4 


in the second pass of the incremental database support for f AD g  f BD g  f CD g  f AE g  f BE g  f CE g  f DE g is counted If f AD g and f BD g turn out to be large locally then in the third pass support for f ABD g is counted and so on 4.3 Algorithm Based on the above discussion the adaptive algorithm is presented as follows Input  1 DB  the original database of size D 2 L DB k  the set of all large k itemsets in DB where k 1   r  and N k  the negative border of L DB k 3 db  an increment database of size o d 4 s  the minimum support threshold and 5 z  threshold on number of new candidate itemsets Output  1 The set of large-itemsets which are large when both the original and the incremental databases are taken into consideration 2 The set of large-itemsets which are large only in the incremental database Steps 1 Build level-wise hash trees for C DB  ie for large itemsets in DB and their negative border 2 Count support for itemsets in hash trees in the incremental database db 3 Find those which are large ie L db  4 If L db 022L DB then output L db as no new large itemsets found in db and exit 5 Generate candidate itemsets upto the threshold z  build hash trees for them and in one scan of db obtain large itemsets in db ie L db  Output these and go to step 7 if the threshold was not crossed 6 Starting from the level at which we stopped in the above step for each level until no new large itemsets are found compute candidate itemsets using apriorigen  scan db and obtain large itemsets We obtain L db in multiple scans of db Output these 7 Scan DB to get further counts of itemsets in L db to 002nd new large itemsets in the combined database 5 Comparison and Performance Studies In Section 2 a technique proposed by Cheung et al 4 f o r in cremen tally main tain in g a sso ciatio n r u l es w a s d escribed Their approach also utilizes what has already been learnt about large itemsets to reduce the cost of updating the discovered rules However in their approach all the rules that are found are those which are applicable when both the original and the incremental databases are taken to-gether In our approach apart from 002nding such rules we also 002nd rules which are applicable in the incremental database alone Moreover we achieve both the tasks in generally two scans of the incremental database and only one scan of the original database Note that we need only one scan of the incremental database when the increment has similar patterns as the original database Let L be the set of large itemsets in the time period T  t 0 and l be the set of large itemsets in the time period t 0 alone The set l of large itemsets can be a result of new business strategies The set l may continue to be large in the time period t 1 t 2   In this case the set l will remain undetected by Cheung et al's approach because the itemsets in l do not have enough support when both the original and the incremental database are taken into consideration The advantages of our approach when compared to the one proposed by Cheung et al 4 a re as fol l o w s  017 Since the process of 002nding rules in the updated database is separated from 002nding rules that are applicable for incremental database alone we can postpone the scan of the original database 017 Each rule can be time-stamped with the interval of the incremental database to indicate its temporal validity 017 When we decide to retire some old data we do not have to re-learn the rules afresh as we have the rules valid for the incremental databases 5.1 Generation of test data The original database is generated using the same technique introduced in 3 The i ncrement al dat a bas e can be generated in the following different ways  1 A random sample of the original database is taken 2 A set of itemsets S  is chosen as would-be-largeitemsets  The itemsets in S are not large in DB but it is intended to make these itemsets large in db After choosing S  a sample of transactions db is taken from DB To make the itemsets in S large in db items present in S are added to the transactions in db with some probability that depends on how many of these items a transaction already contains 3 A database of size  D+d  is 002rst generated using the technique given in 3  T he 002 r s t D transactions are used as DB and the next d transactions form db 5.2 Performance of the Adaptive algorithm Experiments have been performed with the original database of about 70,000 transactions and incremental databases of size 1k 5k 10k The performance of the adaptive algorithm was compared with the Apriori algorithm 5 


3 The A priori w a s r un on the d atabas e o f  D+d ransactions and also on d transactions The time taken in the two cases was added up Since we are 002nding large itemsets in  D+d swellasin d  the above comparison is justi\002ed The Speedup ratio S was calculated as follows  S   time f or Apr ior i D  d  time f or Apr ior i d  time f or Adaptiv e D  d In our experiments the speed-up ratios ranged from 4.2 to 7.6 when the test data was generated using method 1 above and from 3.0 to 6.3 when method 2 was used It was observed in the experiments that the size of the largest itemset was 7 while the number of scans of the incremental database required was only 4 The number of scans of the original database was 0 or 1 The speed-up ratios obtained in our simulation results may appear lower than those reported for the FUP 4 a l gori t h m The r eas on coul d be the nature of test data and the details of coding We also note that we did not include pruning of the databases in our algorithm We stress here that since we do not scan original database more than once and that only the incremental database is scanned generally twice only it will be scanned multiple times only when it is signi\002cantly different from the original database in the rules it contains we expect to have better performance for the adaptive algorithm 6 Conclusions The real-world databases from which useful patterns and rules are mined are dynamic in nature Periodically the organizational database is updated and it may become necessary to carry out the mining process again on the updated database Since mining is a costly activity typically requiring multiple database scans process of incremental mining is proposed by researchers to maintain the rules discovered during previous mining processes In this paper we have proposed an adaptive algorithm for incremental mining We have tried to categorize different types of increments Primarily the increments could be representing similar business trends as before or signi\002cantly different trends The algorithm adapts itself by 002rst 002nding the nature of the increment It then decides whether to scan the original database for updating the rules which were obtained in earlier mining processes The algorithm not only updates rules discovered from the original database but also mines rules which may be present in the increment alone These rules may be due to new business decisions or due to changing customer preferences or seasonal trends It is important to extract them explicitly as new trends to help in business decisions It must be noted that these rules found from the increment alone may not have the required support in the updated database References  R  A gra w al T  I mielinski and A  S w a mi Database mining A performance perspective In IEEE Trans Knowledge and Data Engineering  1993 2 R  A g r a w al T  I mielin sk i an d A  S w a mi M in ing association rules between sets of items in large databas es In Proc ACM SIGMOD  1993  R  A gra w al and R  S ri kant h F a s t al gori t h ms for mining association rules in large databases In Proc 20th Int'l Conf Very Large Data Bases  1994  D a v i d W  C h eung J i a w ei H a n V i ncent T  N g and C.Y.Wong Maintenance of discovered association rules in large databases An incremental updating approach In 12th IEEE International Conference on Data Engineering  1996  D a v i d W  C h eung S  D  Lee and B  K ao A general incremental technique for maintaining discovered association rules In Proceedings of Database Systems for Advanced Applications DASFAA'97  Melbourne Australia pp 185194 1997 6 J o n g So o P ark  Min g Sy an Ch en  a n d Ph ilip S Yu An effective hash-based algorithm for mining association rules In Proceedings of ACM SIGMOD conference  San Jose California 1995  C hri s P  R a i n s f ord Muk e s h K  Mohani a and John F Roddick A temporal windowing technique for the incremental maintenance of association rules In 8th International Database Workshop Data Mining Data Ware housing and Client/Server Databases  1997  H  T oi v onen S a mpl i n g l ar ge dat a bas e s f or as sociation rules In Proceedings of Very Large Data Bases Conference  Mumbai India pp 134-145 1996 6 


Since we can only decrease the value of the subtractive term by such a transformation, we have not decreased the value of the expression Now, given and it is easy to show that  and Because the expression is anti-monotone in and and monotone in we can replace  with  with and with without decreasing its value We are now left with an expression identical to the expression in the theorem, except for occurring in place of Taking the derivative of this expression with respect to and solving for 0 reveals it is maximized when Note that for any rule derivable from  must fall between and Given this restriction on the equation is maximized at  We can therefore replace with without decreasing its value. The resulting expression, identical to that in the theorem statement, is thus an upper-bound on  To apply this result to prune a processed group  Dense-Miner sets to since the required supports are known. Computing a tight value for  where is the item in that minimizes this support value\ is not possible given the support values available in the candidate set of and its ancestors. Dense-Miner therefore sets to an upperbound on as computed by the following function when has a parent and where denotes the single item within the itemset  otherwise This computation requires only the value of which was previously computed by the parent, and the supports of candidate set members   and in order to compute  In applying theorem 5.6 to prune an unprocessed group Dense-Miner computes as above. For it lacks the necessary support information to compute so instead it computes a lowerbound on the value as described in section 5.3 5.5  Bounding support The value of is comparatively easy to compute because support is anti-monotone with respect to rule containment. For Dense-Miner simply uses the value of Other anti-monotone constraints, e.g those discussed in [1  c a n b e e x p l oi te d w i th s i m ila r  e a s e  6.     Item ordering The motivation behind reordering tail items in the Generate-Next-Level function is to, in effect, force unpromising rules into the same portion of the search tree. The reason this strategy is critical is that in order for a group to be prunable every sub-node of the group must represent a rule that fails to satisfy one of the constraints. An arbitrary ordering policy will result in a roughly even distribution of rules that satisfy the constraints throughout the search tree, yielding little pruning opportunities We experimented with several different ordering policies intended to tighten the bounds provided by the pruning functions. The strategy we found to work best exploits the fact that the computations for and both require a value and the larger the value allowed for the tighter the resulting bound. The idea then is to reorder tail items so that many sub-nodes will have a large value for This is achieved by positioning tail items which contribute to a large value of last in the ordering since tail items which appear deeper in the ordering will appear in more sub-nodes than those tail items appearing earlier. We have found that the tail items which contribute most to this value tend to be those with small values for This can be seen from Observation 5.4 which yields a larger lower-bound on when the value of summed over every tail item is small. The policy used by Dense-Miner is therefore to arrange tail items in decreasing order of  7.     Post-processing The fact that Dense-Miner finds all frequent, confident large-improvement rules and places them into follows from the completeness of a set-enumeration tree search and the correctness of our pruning rules, as established by the theorems from Section 5. Dense-Miner must still post-process because it could contain some rules that do not have a large improvement Removing rules without a large improvement is non-trivial because improvement is defined in terms of all the proper sub-rules of a rule, and all such rules are not necessarily generated by the algorithm. A naive post-processor for removing rules without a large improvement might, for every mined rule, explicitly compute its improvement by generating and testing every proper sub-rule. Because Dense-Miner is capable of mining many long rules, such an approach would be too inefficient Instead, the post-processor first identifies some rules that do not have a large improvement by simply comparing them to the other rules in the mined rule set It compares each rule to every rule such that and  If ever it is found that then rule is removed because its improvement is not large This step alone requires no database access, and removes almost all rules that do not have a large improvement To remove any remaining rules, the post-processor performs a set-enumeration tree search for rules that could potentially prove some rule in does not have a large improvement. The main difference between this procedure and the mining phase is in the pruning strategies applied For this search problem, a group is prunable when none of its derivable rules can prove that some rule in lacks a large improvement. This is determined by either of the following conditions rr s a 0 263 y  y 263b  b 243 a  y  b  a  0 b  b y  y x  x x  x  y 2 y b   gx  sup hg  C 310  minsup x  x minmax y 2 y b minsup  sup hg  C 310    x  x  x imp r  g y sup hg  tg  c 330  310\310  b sup hg  i m 226  i m 330 c 330   310  i m hg  g b sup hg  i m 226  i m 330 c 330   310  f b g  min f b g p sup hg p  i 330 c 330   310     g g p i hg  hg p  226 f b g  245  f b g p  hg  hg  C 310 hg p  hg p  C 310 sup hg p  i 330 c 330   310  g b y sup hg  tg  c 330  310\310  usup g  usup g  sup hg  C 310  uconf g  uimp g  y sup hg  tg  c 330  310\310  243 y sup hg  tg  c 330  310\310  sup hg  tg  c 330  310\310  sup hg  i 330 c 330   310  sup hg  tg  c 330  310\310  sup hg  i 330 c 330   310  sup hg  i 330 c 330   310  R R R r 1 R 316 r 2 r 2 R 316 r 2 r 1 314 conf r 1  conf r 2  226 minimp  r 1 R g R 


225 There exists no rule for which  225 for all rules such that  After groups are processed, any rule is removed if there exists some group such that and Because the search explores the set of all rules that could potentially prove some rule in does not have a large improvement, all rules without a large improvement are identified and removed Our post-processor includes some useful yet simple extensions of the above for ranking and facilitating the understanding of rules mined by Dense-Miner as well as other algorithms. The improvement of a rule is useful as an interestingness and ranking measure to be presented to the user along with confidence and support. It is also often useful to present the proper sub-rule responsible for a rule\222s improvement value. Therefore, given an arbitrary set of rules, our post-processor determines the exact improvement of every rule, and associates with every rule its proper subrule with the greatest confidence \(whether or not this subrule is in the original rule set\le-sets that are not guaranteed to have high-improvement rules \(such as those extracted from a decision tree\, the sub-rules can be used to potentially simplify, improve the generality of, and improve the predictive ability of the originals 8.     Evaluation This section provides an evaluation of Dense-Miner using two real-world data-sets which were found to be particularly dense in [4  1 The first data-set is compiled from PUMS census data obtained from It consists of 49,046 transactions with 74 items per transaction, with each transaction representing the answers to a census questionnaire. These answers include the age, taxfiling status, marital status, income, sex, veteran status, and location of residence of the respondent. Similar data-sets are used in targeted marketing campaigns for identifying a population likely to respond to a particular promotion. Continuous attributes were discretized as described in  though no frequently occurring items were discarded. The second data-set is the connect-4 data-set from the Irvine machine learning database repository It consists of 67,557 transactions and 43 items per transaction This data-set is interesting because of its size, density, and a minority consequent item \(\223tie games\224\ accurately predicted only by rules with low support. All experiments presented here use the \223unmarried partner\224 item as the consequent with the pums data-set, and the \223tie games\224 item with the connect-4 data-set; we have found that using other consequents consistently yields qualitatively similar results Execution times are reported in seconds on an IBM IntelliStation M Pro running Windows NT with a 400 MHZ Intel Pentium II Processor and 128MB of SDRAM. Execution time includes runtime for both the mining and post-processing phases The minsup setting used in the experiments is specified as a value we call minimum coverage where In the context of consequent constrained association rule mining, minimum coverage is more intuitive than minimum support, since it specifies the smallest fraction of the population of interest that must be characterized by each mined rule 8.1  Effects of minimum improvement The first experiment \(Figure 5\hows the effect of different minimp settings as minsup is varied. Minconf in these experiments is left unspecified, which disables pruning with the minimum confidence constraint. The graphs of the figure plot execution time and the number of rules returned for several algorithms at various settings of minimum support Dense-miner is run with minimp settings of .0002, .002, and 02 \(dense_0002, dense_002, and dense_02 respectively We compare its performance to that of the Apriori algorithm optimized to exploit the consequent constraint \(apriori_c This algorithm materializes only those frequent itemsets that contain the consequent itemset The first row of graphs from the figure reveals that apriori_c is too slow on all but the greatest settings of minsup for both data-sets. In contrast, very modest settings of minimp allow Dense-Miner to efficiently mine rules at far lower supports, even without exploiting the minconf constraint. A natural question is whether mining at low supports is necessary. For these data-sets, the answer is yes simply because rules with confidence significantly higher than the consequent frequency do not arise unless minimum coverage is below 20%. This can be seen from Figure 7 which plots the confidence of the best rule meeting the minimum support constraint for any given setting 2 This property is typical of data-sets from domains such as targeted marketing, where response rates tend to be low without focusing on a small but specific subset of the population The graphs in the second row of Figure 5 plot the number of rules satisfying the input constraints. Note that runtime correlates strongly with the number of rules returned for each algorithm. For apriori_c, the number of rules returned is the same as the number of frequent itemsets containing the consequent because there is no minconf constraint specified. Modest settings of minimp dramatically reduce the number of rules returned because most rules in these data-sets offer only insignificant \(if any\ predictive advantages over their proper sub-rules. This effect is particularly pronounced on the pums data-set, where a minimp setting of .0002 is too weak a constraint to keep the number of such rules from exploding as support is lowered. The increase in runtime and rule-set size as support is lowered is far more subdued given the larger \(though still small\inimp settings 1 Both data-sets are available in the form used in these experiments from http://www.almaden.ibm.com/cs/quest rR 316 hg  r 314 conf r   uconf g  226 minimp 263 rR 316 hg  r 314 rR 316 g hg  r 314 conf r   conf hg   226minimp  R http://augustus.csscr.washington.edu/census/comp_013.html http://www.ics.uci.edu/~mlearn/MLRepository.html 2 The data for this figure was generated by a version of Dense-Miner that prunes any group that cannot lead to a rule on the depicted support/confidence border. This constraint can be enforced during mining using the confidence and support bounding techniques from section 5 minimum coverage minsup sup C  244  


FIGURE 5 Execution time and rules returned versus minimum coverage for the various algorithms FIGURE 6 Execution time of dense_0002 as minconf is varied for both data-sets. Minimum coverage is fixed at 5% on pums and 1% on connect-4 FIGURE 7 Maximum confidence rule mined from each data-set for a given level of minimum coverage   1 10 100 1000 10000 100000 0 10 20 30 40 50 60 70 80 90 Execution time \(sec Minimum Coverage connect-4 apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 1e+06 0 10 20 30 40 50 60 70 80 90 Number of Rules Minimum Coverage connect-4 apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 0 10 20 30 40 50 60 70 80 90 Execution Time \(sec Minimum Coverage pums apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 1e+06 1e+07 0 10 20 30 40 50 60 70 80 90 Number of Rules Minimum Coverage pums apriori_c  dense_0002   dense_002   dense_02    0 500 1000 1500 2000 2500 3000 3500 20 25 30 35 40 45 50 55 60 65 Execution time \(sec minconf pums  connect-4  1 10 100 1000 10000 100000 1e+06 20 25 30 35 40 45 50 55 60 65 Number of Rules minconf pums  connect-4    0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100 Highest Rule Confidence Minimum Coverage pums  connect-4 


8.2  Effects of minimum confidence The next experiment \(Figure 6\ws the effect of varying minconf while fixing minimp and minsup to very low values. With connect-4, we used a minimum coverage of 1%, and with pums, a minimum coverage of 5%. Minimp was set to .0002 with both data-sets. As can be extrapolated from the previous figures, the number of rules meeting these weak minimp and minsup constraints would be enormous As a result, with these constraints alone, Dense-Miner exceeds the available memory of our machine The efficiency of Dense-Miner when minimum confidence is specified shows that it is effectively exploiting the confidence constraint to prune the set of rules explored. We were unable to use lower settings of minconf than those plotted because of the large number of rules. As minconf is increased beyond the point at which fewer than 100,000 rules are returned, the run-time of Dense-Miner rapidly falls to around 500 seconds on both data-sets 8.3  Summary of experimental findings These experiments demonstrate that Dense-Miner, in contrast to approaches based on finding frequent itemsets achieves good performance on highly dense data even when the input constraints are set conservatively. Minsup can be set low \(which is necessary to find high confidence rules as can minimp and minconf \(if it is set at all\This characteristic of our algorithm is important for the end-user who may not know how to set these parameters properly. Low default values can be automatically specified by the system so that all potentially useful rules are produced. Refinements of the default settings can then be made by the user to tailor this result. In general, the execution time required by Dense-Miner correlates strongly with the number of rules that satisfy all of the specified constraints 9.     Conclusions We have shown how Dense-Miner exploits rule constraints to efficiently mine consequent-constrained rules from large and dense data-sets, even at low supports. Unlike previous approaches, Dense-Miner exploits constraints such as minimum confidence \(or alternatively, minimum lift or conviction\ and a new constraint called minimum improvement during the mining phase. The minimum improvement constraint prunes any rule that does not offer a significant predictive advantage over its proper sub-rules. This increases efficiency of the algorithm, but more importantly it presents the user with a concise set of predictive rules that are easy to comprehend because every condition of each rule strongly contributes to its predictive ability The primary contribution of Dense-Miner with respect to its implementation is its search-space pruning strategy which consists of the three critical components: \(1\functions that allow the algorithm to flexibly compute bounds on confidence, improvement, and support of any rule derivable from a given node in the search tree; \(2\proaches for reusing support information gathered during previous database passes within these functions to allow pruning of nodes before they are processed; and \(3\ item-ordering heuristic that ensures there are plenty of pruning opportunities. In principle, these ideas can be retargeted to exploit other constraints in place of or in addition to those already described We lastly described a rule post-processor that DenseMiner uses to fully enforce the minimum improvement constraint. This post-processor is useful on its own for determining the improvement value of every rule in an arbitrary set of rules, as well as associating with each rule its proper sub-rule with the highest confidence. Improvement can then be used to rank the rules, and the sub-rules used to potentially simplify, generalize, and improve the predictive ability of the original rule set References 1 w a l  R.; Im ie lin ski  T   a n d S w a m i, A. 1 9 9 3   M i n i ng As so ciations between Sets of Items in Massive Databases. In Proc of the 1993 ACM-SIGMOD Int\222l Conf. on Management of Data 207-216 2 raw a l R.; M a n n ila, H Sri k an t  R T o i v o n en  H.; an d  Verkamo, A. I. 1996. Fast Discovery of Association Rules. In Advances in Knowledge Discovery and Data Mining AAAI Press, 307-328 3 K Ma ng a n a r is S a n d Sri k a n t, R 19 97  P a rtia l Cl a ssif i cation using Association Rules. In Proc. of the 3rd Int'l Conference on Knowledge Discovery in Databases and Data Mining 115-118 4 a rd o  R. J 1 9 9 8  Ef f i c i en tly Min i n g  Lo n g  P a ttern s fro m  Databases. In Proc. of the 1998 ACM-SIGMOD Int\222l Conf. on Management of Data 85-93 5  Mi c h ae l J. A a n d  Lin o f f G  S 1 9 9 7  Data Mining Techniques for Marketing, Sales and Customer Support John Wiley & Sons, Inc 6 Bri n, S  M o t w a n i, R.; Ullm a n J.; a n d  Tsu r S. 19 9 7 Dyn a m i c  Itemset Counting and Implication Rules for Market Basket Data. In Proc. of the 1997 ACM-SIGMOD Int\222l Conf. on the Management of Data 255-264 7 h e n  W   W   1 9 9 5 F a st Ef fecti v e Ru le In d u ctio n   In  Proc. of the 12th Int\222l Conf. on Machine Learning 115-123 8 In tern atio n a l Bu sin e s s Mac h in e s   1 9 9 6  IBM Intelligent Miner User\222s Guide Version 1, Release 1 9 m e t tin e n M   Ma nn ila  P  Ro nk a i ne n  P   a n d V e rk a m o  A  I. 1994. Finding Interesting Rules from Large Sets of Discovered Association Rules. In Proc. of the Third Int\222l Conf. on Information and Knowledge Management 401-407 10  Ng   R  T    L a k s hm ana n   V   S    Ha n  J   an d P a ng A  1 9 9 8   Exploratory Mining and Pruning Optimizations of Constrained Association Rules. In Proc of the 1998 ACM-SIGMOD Int\222l Conf. on the Management of Data 13-24 11 Ry mo n  R 1 9 9 2   Search  t h ro u g h Sy s t e m atic S e t En u m era tion. In Proc. of Third Int\222l Conf. on Principles of Knowledge Representation and Reasoning 539-550 1  Sha f e r  J  A g r a w a l R   an d Me ht a M 19 98  SPR I N T   A  Scalable Parallel Classifier for Data-Mining. In Proc. of the 22nd Conf. on Very Large Data-Bases 544-555 13  S m y t he P  and  Go od man   R  M 19 92 An I n f o r m at i o n Th eo retic Approach to Rule Induction from Databases IEEE Transactions on Knowledge and Data Engineering 4\(4\:301316 14  S r i k a n t   R    V u  Q an d Ag r a w a l  R  19 97 M i ni ng  A ssoc i a tion Rules with Item Constraints. In Proc. of the Third Int'l Conf. on Knowledge Discovery in Databases and Data Mining 67-73 15 W e bb, G. I 1 9 9 5 OP U S An Ef f i c i e n t Adm i ssible Algo rit h m for Unordered Search. In Journal of Artificial Intelligence Research 3:431-465 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


