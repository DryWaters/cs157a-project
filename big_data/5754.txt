GPU Accelerated Item-Based Collaborativ e Filtering for Big-Data Applications Chandima Hewa Nadungodage Yuni Xia Department of Computer Information Science Purdue School of Science IUPUI Indianapolis, USA chewanad, yuxia}@iupui.edu  John Jaehwan Lee Department of Electrical Computer Engineering Purdue School of Engineering Technology IUPUI Indianapolis, USA johnlee@iupui.edu Myungcheol Lee Choon Seo Park Big-Data Software Platform Research Department Software Research Laboratory  Electronics & Telecommunications Research Institute, Korea  mclee, parkcs}@etri.re.kr  Abstract Recommendation systems are a popular marketing strategy for online service providers. These systems predict a customer’s future preferences from the past behaviors of that customer and the other customers.  Most of the popular online stores process millions of transactions per day; therefore providing quick and quality recommendations using the large amount of data collected from past transactions can be challenging. Parallel processing power of GPUs can be used to accelerate the recommendation process. However, the amount of memory available on a GPU card is limited; thus, a number of passes may be required to completely process a large-scale dataset. This paper proposes two parallel item-based  recommendation algorithms implemented using the CUDA platform. Considering the high sparsity of the user-item data we utilize two compression techniques to reduce the required number of passes and increase the speedup. The experimental results on synthetic and real-world datasets show that our algorithms outperform the respective CPU implementations and also the naïve GPU implementation which does not use compression Keywords - recommendation systems; GPU; CUDA collaborative filtering; big-data I  I NTRODUCTION  Internet-based service providers such as Amazon and Netflix widely use Collaborative Filtering \(CF\ to provide personalized recommendations based on past behaviors of the users. CF-based recommender systems can be divided into two categories; the user-based CF and the item-based CF 1 2 u e t o t h e l a r g e v o l u m e of t r a n sac t i o ns ge ner a t e d b y t h e online stores, providing quick and quality recommendations is a challenging task Graphics Processing Units \(GPUs\ are designed to handle highly parallel workloads and can execute thousands of concurrent threads. With the introduction of CUDA \(Compute Unified Device Architecture 1 platform, a general purpose parallel computing architecture, GPU computing has become more and more popular in general-purpose, large-scale, data mining applications [3 P a r a l l e l  pr oc e s si ng  p o w e r o f G P U s  can be used to speed up the CF process [6  H o w e ve r  t h e amount of memory available on a GPU card is limited  1 http://www.nvidia.com/object/cuda_home_new.html therefore, a number of runs may be required to completely process a large-scale dataset. This limits the performance gain that can be achieved by using GPUs. In most scenarios, the user-item data is very sparse because there are large numbers of users and large variety of items but each customer only purchases a relatively small number of items. Thus representing the user-item data in a compressed format can significantly reduce the memory requirements and the processing time. The existing CUDA implementations [6-8  of user-based CF systems do not consider the sparsity of the user-item data Various studies have shown that the item-based CF methods outperform the user-based CF methods in many scenarios [1    2     9     1 0   D u e t o its  ab ility t o sc ale t o l a r g e  numbers of users and its predictive accuracy item-based CF is used by many large-scale online service providers such as Amazon, Google, YouTube, and Netflix. Some recommendation algorithms compute recommendations based on the user ratings of the items. However, most of the times, user ratings are not available but only the purchase information. In such situations, Deshpande and Karypis [1  have shown that the item-based CF approach combined with the conditional probability-based similarity measure produces higher-quality recommendations than the user-based CF approach The conditional probability-based similarity of two items depends on the co-occurrence of those two items in the transaction history. Thus, it is only required to know whether the user has purchased an item or not \(i.e. the user rating of the item is not necessary\ This property of conditional probability-based similarity measure enables us to compress the user-item data and significantly reduces the memory requirements, thereby making it a good candidate for GPU implementation. In this paper we propose two GPU accelerated item-based CF algorithms, using conditional probability as the similarity measure. Considering the high sparsity of the user-item data, we utilize compression techniques to reduce the required number of passes to completely process large-scale datasets. Data compression also allows us to reduce the number of computations required in populating the item-item similarity matrix. The experiments 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 175 


show that the proposed GPU implementations are very efficient in processing large-scale datasets in a timely manner and achieved up to 30X and 125X speedups compared to the respective CPU implementations and up to 5X and 18X speedups compared to the naïve GPU implementation which does not use data compression II  R ELATED W ORK  There is very limited work on using GPUs to accelerate recommendation system applications. Kato and Hosino [6  proposed a CUDA implementation of user-user  k nearest neighbor search for user-based CF. Li et al. [7 r op os ed  a  social network-aware top-N recommender system using GPUs. This approach also uses user-based CF and requires social network information to produce recommendations Both above work do not consider the sparsity of the user-item matrix, and thus, require a number of passes to process largescale datasets using GPUs. Zhanchun and Yuying [8 a l s o  proposed a GPU accelerated user-user similarity calculation method, emphasizing on addressing the accuracy limitation of user-based CF techniques by using implied similarity and default values to fill the missing values. This might increase the accuracy of the recommendation process, but still require a number of passes to completely process large-scale datasets as the amount of memory available in GPU is limited Our proposed methods consider the sparsity of user-item data and use compression techniques to reduce the number of runs required to process large-scale datasets in GPU, thereby increasing the possible speedup. Furthermore, our methods are based on item-item CF which is proven to be more accurate and scalable than user-user CF methods    2   1 0 III  P RELIMINARIES  A  Item-based Collaborative Filtering Let n denote the number of distinct users and m denote the number of distinct items in the dataset. Then, this dataset can be represented by an n  m binary matrix R that will be referred to as the user-item matrix, such that R i,j is one if the i th user has purchased the j th item, and zero otherwise. Let N denote the number of recommendations that need to be computed for a particular user. The topN recommendation problem is formally defined as follows Given the user-item matrix R and the set of items X ui  purchased by a given user u i  i 1 to n identify a set of items X ri  X ui  X ri and X ri   N which is most likely to be purchased by the user u i  Item-based CF process consists of two stages. The first stage is to compute the similarities between the different items; this produces an m  m similarity matrix S Each column j in S is then sorted to get the topk similar items for a particular item j The second stage is to derive the topN recommendations for a particular user u i using the sorted similarity matrix S and the items already purchased by the user X ui The focus of this paper is on accelerating the first stage i.e item-ite m similarity calculation\ using GPUs. Since the similarity of each pair of items <I i I j can be individually calculated without depending on the other items, this stage can be highly parallelized using GPUs B  Conditional Probability-Based Similarity Calculation The similarity between a pair of items i and j can be calculated based on the conditional probability of purchasing one of the items given that the other has already been purchased. The conditional probability of purchasing item j  given that item i has already been purchased [P\(j|i  i s de fi ned as the number of users that purchase both items i and j divided by the total number of users that purchased i If P\(j|i\ is directly used as the similarity measure, each item i will tend to have high similarities with the frequently purchased items such as milk or bread\, thus those items will appear on the top of the recommendation list. According to Kitts et al. [10  th is  problem can be corrected by penalizing the frequently purchased items by dividing P\(j|i\ with a quantity that depends on the frequency of item j thus avoiding them dominating the recommendations In the original form, the rows of the user-item matrix R correspond to the binary purchase information, in which case the similarity calculation gives equal weight to all users. It is shown in [1 th at th e c o p u r ch asin g in f o r m atio n d e r i v e d f r o m  the users that have bought fewer items is a more reliable indicator for the similarity of two co-purchased items than the information derived from the users that tend to buy a large number of items. By scaling each row of R to be of unit length we can make users that have purchased fewer items contribute higher weight to the similarity measure than the users that have purchased more items. Based on these facts, Deshpande and Karypis prop o s e d t h e fo l l o w i ng fo r m ul a  t o co m p ut e  the similarity between two items using the normalized useritem matrix R           1 Here  and 1. The numerator in \(1\ is the sum of the corresponding entries of the j th column in the matrix R, for the set of the users who have purchased both items i and j In the remainder of the paper we refer this as “weighted co-occurrence frequency IV  P ROPOSED A PPROACH  A  CUDA Implementation From a programmer’s point of view, the CUDA programming model is a collection of threads running in parallel. A CUDA program consists of a host program running on the host CPU, and one or more parallel kernel functions executed on a GPU. Each kernel is executed by multiple blocks, and each block contains multiple threads. We split the similarity matrix computation into three steps and use a separate GPU kernel function for each step. First, Kernel 1 computes the weighted item-item co-occurrence frequencies numerator in \(1\\ for possible pairs of items. Then, using that information Kernel 2 computes the item-item similarity according to \(1\. Finally, Kernel 3 sorts each column of the similarity matrix to find topk similar items for each item Before starting the GPU computation, input data \(useritem matrix\ needs to be transferred from the host memory to the GPU’s memory which is called the global memory If the 176 


GPU does not have enough global memory space to hold the entire input dataset, multiple iterations are required to complete processing the entire dataset. When the user-item matrix is very large, this will have an effect on the runtime of the algorithm and reduce the performance gain. To address this issue, we propose two memory efficient implementations for step 1 \(weighted co-occurrence frequency calculation The first method, GPU_BP_CF \(GPU accelerated BitPacked Collaborative Filtering\, uses bit packing to compress the data to reduce the memory requirement and increase the speedup. The second method, GPU_CMP_CF \(GPU accelerated Compact Collaborative Filtering\, uses a compact format of the user-item matrix which only stores non-zero elements. When user-item matrix is sparse, this method also reduces the memory requirement and gives significant speedup B  GPU_BP_CF Algorithm In this implementation we do not normalize the rows of R beforehand. Each column of the user-item matrix is packed into n 32 n is the number of users 2 32-bit integers; such bitpacking reduces the size of input data by 32 times, making it possible to store more transactions in the GPU memory at a time; thus reducing the number of iterations require to process the entire dataset.  We also compute the weight of each user w u which is the reciprocal of the number of items purchased by the user\ and transfer this information separately to the GPU to be used for row normalization To facilitate coalesced memory access, the user-item matrix is stored in the transposed \(item-user\ormat in GPU memory. Each column of the user-item matrix is packed into 32-bit integers and then stored as a row in the item-user matrix. Fig. 1 depicts the bit packing process and computing the co-occurrence frequency of the pair of items I i and I j  When the data sparsity is high b it packing allows to efficiently rule-out the users who have not purchased the interested pair of items without checking the individual bits. This is done using bitwise AND operation and significantly improves the processing time. After performing the bitwise AND operation we only need to unpack the entries with non-zero values Then we multiply the non-zero bits with weight of the corresponding user \(w u d take the summation to get the weighted co-occurrence frequency  Figure 1  GPU_BP_CF algorithm - compressing the user-item matrix using bit-packing  2 If n is not a multiple of 32, add padding bits of zeros to the end 1  Kernel 1 – Weighted co-occurrence frequency calculation As opposed to the serial CPU implementation, in GPU we can compute the co-occurrence frequency of multiple pairs of items in parallel. We divide the item-item similarity matrix into m x B y two dimensional blocks, and each block is divided into T x x T y threads. The value of T x needs to be a multiple of 32 to utilize full power of coalesced memory access. Then value of B y is given by m T y Each row in each thread block computes the co-occurrence frequency of the pair of items <I x  I y where I x  b x and I y  b y  T y  t y here b x  b y is the block index and t y is the row index within the block. Each row of threads reads the corresponding T x elements \(or columns from the bit-packed item-user matrix and computes the weighted sum in parallel.  This is repeated n 32 T x times until all the elements of the corresponding row are processed Fig. 2 illustrates the block and thread layout for Kernel 1 implementation of GPU_BP_CF algorithm  Figure 2  GPU_BP_CF Algorithm - thread layout for Kernel1 computing weighted co-occurrence frequency of items in parallel 2  Kernel 2 – Compute the similarity values Once Kernel 1 is executed for all grids, we have the weighted co-occurrence frequencies for all pairs of items Using this information Kernel 2 computes the similarity values for each pair of items in parallel. The similarity matrix is divided into B x B blocks of T x T threads. The value of T needs to be a multiple of 32 to achieve coalesced global memory access. Then value of B is given by m T. Each thread computes the similarity of the pair of items <I x I y according to \(3\. Item indices are given by I x b x  T  t x and I y b y  T   t y where b x  b y is the block index and \(t x t y the thread index within the respective block 3  Kernel 3 – Sort the similarity matrix Once the item-item similarities are computed, we need to find the top-k similar items for each item. Kernel 2 sorts columns of the similarity matrix in parallel to get topk similar items for each item. Each column is only required to be sorted partially until we get the topk items. The value of k is relatively small compared to the number of items. Therefore in Kernel 3 we use the partial insertion sort algorithm implemented by Garcia et al. [4   177 


C GPU_CMP_CF Algorithm In this algorithm we compress the user-item matrix by storing only the non-zero entries. As shown in Fig. 3 we use two arrays to store the compressed data. In the data array for each user, we store the corresponding item numbers of the items purchased by that user. Row boundary for each user is maintained in a separate indices array of size n 1\here n  is the number of users. For each user we record the corresponding transaction starting index of the data array For the last user we need to store the transaction ending index as well. Using this compact format significantly reduces the data volume; thus, reducing the number of iterations required to process the entire dataset. Moreover, if the data is uncompressed, it is required to compute the co-occurrence frequency of all possible pairs of items. When we use the compact format, it is only required to count the item pairs that actually occurred in the transactions. Here also we do not normalize the rows of R beforehand. The number of items purchased by a particular user can be found using the indices array and will be used for normalization at the time of weighted co-occurrence frequency calculation  Figure 3  Compressing the user-item matrix into compact data and indices arrays  Kernel 1 – Weighted co-occurrence frequency calculation Fig. 4 depicts the block and thread layout for Kernel 1 in GPU_CMP_CF algorithm. The data array is divided into G  grids. The number of grids depends on the available GPU memory. The GPU loads and processes one grid at a time Using the compact format reduces the number of iterations required. Each grid is divided into 1 x B one dimensional blocks which work in parallel. Each block is divided into T x x T y threads. The value of T x needs to be a multiple of 32 to utilize the full power of coalesced memory access. Then the value of B is given by n T y Each row of threads in each thread block processes a corresponding user of the data array given by the index u  b y  T y  t y where b y is the block index and t y is the thread index within the respective block. For a particular user index u row length \(i.e. the number of items purchased by the user \(C u is found using the indices array Each thread t x finds the co-occurring items I y for the item I x from the set of items purchased by the user u and atomically increases the weighted co-occurrence frequency count of the respective index <I x I y in the item-item similarity matrix by 1/C u  Kernel 1 is executed on all G grids, one grid at a time before proceeding to the other kernels. Implementations of  3  http://www.flixster.com  Kernel 2 and Kernel 3 are similar to what is described in the previous section  Figure 4  GPU_CMP_CF Algorithm - thread layout for Kernel1 computing weighted co-occurrence frequency of items in parallel V  R ESULTS AND O BSERVATIONS  We compared the performance of our parallel algorithms with the respective serial implementations on the CPU and with a naïve GPU implementation which does not use data compression. The CPU version of the bit-packed algorithm is named as CPU_BP_CF and the CPU version of the compact matrix algorithm is named as CPU_CMP_CF. The naïve GPU implementation is named as GPU_UNCMP_CF The GPU versions of the algorithms are implemented using C++ and CUDA, and CPU versions are implemented in C++. All the experiments were carried out on a server running 64bit Fedora 17 with an Intel Xeon 3.3GHz CPU and 64GB memory, and an NVIDIA GeForce GTX680 GPU card with 2GB memory, CUDA runtime version 5.0, and compute capability 3.0. We evaluated the performance and scalability of the proposed algorithms using synthetic datasets of different volumes and sparsity. We also tested our algorithms on real-world datasets derived from the movie rental website Flixster 3 The runtimes reported here are the total runtimes for the respective versions, including the time to read input data from the database and the data transfer time between the CPU and the GPU A  Runtime comparison with data volume To test the runtime variation of the algorithms according to the increasing number of items, we fixed the number of users to 100K and increased the number of items from 5000 to 10000. The sparsity of the datasets is set to 90 k is set to  First we compared the runtime of the proposed GPU implementations with the respective serial CPU implementations. Fig. 5 presents the runtime comparison and Fig. 6 presents the respective CPU/GPU speedups. It is visible that the runtime of the CPU versions increases rapidly with the increasing number of items, whereas the runtime of the GPU versions scales well with the increasing number of items. The GPU_CMP_CF algorithm has above 20X speedup compared to its CPU counterpart, and the speedup slightly increases with the increasing number of items. The GPU_BP_CF algorithm has above 60X speedup compared to 178 


its CPU counterpart, and there is more noticeable increase in the speedup \(up to 80X\ with the increasing number of items  Figure 5  Runtime vs number of items [sparsity 90%, #users 100K   Figure 6  CPU/GPU speedup vs number of items [sparsity 90%, #users 100K  Next we compared the runtime of the proposed GPU implementations with the runtime of the GPU_UNCMP_CF implementation which does not use any data compression According to Fig. 7, it is visible that the runtimes of the proposed GPU implementations scale well with the number of items in the dataset and outperform the GPU_UNCMP_CF implementation which does not use any data compression  Figure 7 Comparison of GPU implementations - runtime vs number of items [sparsity 90%, #users 100K  We also compared the runtime variation of the algorithms according to the increasing number of users by fixing the number of items to 10K and increasing the number of users from 50K to 100K. The sparsity of the datasets is set to 90 k   In these tests also, the GPU_CMP_CF algorithm achieved up to 20X speedup compared to its CPU counterpart, and the GPU_BP_CF algorithm achieved up to 80X speedup compared to its CPU counterpart. We have omitted the graphs due to the space limitations B  Runtime comparison with data sparsity This section presents the runtime variation of the algorithms according to the sparsity of the user-item matrix The number of users in the dataset is set to 100K and the number of items is set to 10K. The sparsity of the dataset is varied from 97% to 80 k is set to 50  Fig 8 presents the runtime comparison of the proposed GPU implementations with the respective serial CPU implementations and Fig. 9 presents the respective CPU/GPU speedups. It is visible that runtime of the CPU versions increases rapidly with the increasing density of the dataset whereas runtime of the proposed GPU versions scales well with the increasing density of the dataset. The speedup achieved by the GPU_CMP_CF algorithm increased from 20X to 30X whereas speedup of the GPU_BP_CF algorithm increased from 40X-125X  Figure 8  Runtime vs sparsity of the dataset [#items 10K, #users 100K   Figure 9  CPU/GPU speedup vs sparsity of the dataset [#items 10K users 100K  We also compared the runtime of the proposed GPU implementations with the runtime of the GPU_UNCMP_CF implementation which does not use data compression, at different sparsity levels of the dataset. Fig. 10 depicts the runtimes of the three GPU implementations against the sparsity of the dataset. When the sparsity of the dataset is over 80%, it is visible that the proposed GPU implementations outperformed the GPU_UNCMP_CF implementation and achieved 2X-18X speed increase      0 1000 2000 3000 4000 5000 6000 7000 8000 5000 6000 7000 8000 9000 10000 Runtime \(s Number of items GPU_CMP_CF   GPU_BP_CF CPU_CMP_CF   CPU_BP_CF 0 10 20 30 40 50 60 70 80 90 5000 6000 7000 8000 9000 10000 Speedup Number of items CPU_CMP_CF/GPU_CMP_CF CPU_BP_CF/GPU_BP_CF     0 50 100 150 200 250 300 5000 6000 7000 8000 9000 10000 Runtime \(s Number of items GPU_CMP_CF   GPU_BP_CF   GPU_UNCMP_CF     0 2000 4000 6000 8000 10000 12000 14000 16000 18000 97 95 93 90 87 85 83 80 Runtime \(s Sparsity of the dataset GPU_CMP_CF   GPU_BP_CF CPU_CMP_CF   CPU_BP_CF 0 20 40 60 80 100 120 140 97 95 93 90 87 85 83 80 Speedup Sparsity of the dataset CPU_CMP_CF/GPU_CMP_CF CPU_BP_CF/GPU_BP_CF 179 


It is also noticeable that the GPU_CMP_CF algorithm is faster than the GPU_BP_CF algorithm when data sparsity is above 90% and vice versa when data sparsity below 90%. The reason behind this is, in the compact algorithm we only compute the similarity for the item pairs that actually co-occur in the transactions.  When the data is very sparse, the average number of items purchased by a user is low; thus, the required number of co-occurrence frequency computations is low. In the bit-packed implementation, although we rule out most of the irrelevant users using bitwise AND operations, it is still required to consider all possible pairs of items. When the data get denser, the number of co-occurring pairs of items increases; thus, the performance gain realized by the bit-pack implementation increases compared to the performance gain realized by the compact implementation  Figure 10  Comparison of GPU implementations - runtime vs data sparsity items 10K, #users 100K  To evaluate the performance of the proposed algorithms on real-world data, we used the version of the Flixster dataset 4  prepared by Mohsen Jamali W e o n l y  co nsi d er ed t h e  users that have watched at least two movies. The original dataset consists of around 109K such users and for these users we extracted five different datasets with the number of items varying from 10K to 20K. The sparsity of these datasets is around 99%. The results obtained on Flixter datasets confirmed our observations on the synthetic datasets Displaying a similar behavior to the experiments on the synthetic datasets, here also the speedup achieved by the proposed GPU implementations increased with the number of items in the dataset.  The GPU_CMP_CF algorithm achieved up to 28X speedup and the GPU_BP_CF algorithm achieved up to 36X speedup compared to their respective CPU counterparts. Both algorithms also outperformed the GPU_UNCMP_CF algorithm which does not use data compression. We have omitted the graphs due to the space limitations VI  C ONCLUSION  In this paper we proposed two GPU accelerated similarity calculation algorithms for item-based collaborative filtering systems. Considering the sparsity of large-scale user-item data, and limited memory available on GPUs, we use compression techniques to reduce the data volume. By doing  4 http://www.cs.ubc.ca/~jamalim/datasets/flixster.zip so, we reduce the number of iterations required to completely process large-scale datasets and also reduce the number of computations required for item-item similarity calculation thereby increasing the speedups achieve by the GPU implementations. The experimental results show that our proposed parallel GPU implementations outperform not only the respective serial CPU implementations but also the naïve GPU implementation which does not use data compression The proposed algorithms scale well with the increasing volume of the user-item data and produce results in a timely manner, making them applicable for large-scale recommendation systems A CKNOWLEDGMENT  This work was supported by the IT R&D program of MSIP/KEIT, Korea. [10041709, Development of Key Technologies for Big Data Analysis and Management based on Next Generation Memory   R EFERENCES  1  M. Deshpande and G. Karypis, “Item based top-n recommendation algorithms,” ACM Transactions on Information Systems, 22:143–177, 2004 2  B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, “Item-based collaborative filtering recommendation algorithms,” ACM International conference on World Wide Web \(WWW\, 285295, 2001 3  D. Steinkraus, I. Buck, and P.Y. Simard, “Using GPUs for machine learning algorithms,” IEEE International Conference on Document Analysis and Recognition, 1115–1120, 2005 4  V. Garcia, E. Debreuve, and M. Barlaud, “Fast k nearest neighbor search using GPU,” IEEE Computer Vision and Pattern Recognition Workshops \(CVPRW\, 1–6, 2008 5  R. Farivar, D. Rebolledo, E. Chan, and R. Campbell, “A parallel implementation of k-means clustering on GPUs International Conference on Parallel and Distributed Processing Techniques and Applications \(PDPTA\, 340–345 2008 6  K. Kato and T. Hosino, “Solving k-Nearest Neighbor Problem on Multiple Graphics Processors,” IEEE/ACM International Conference on Cluster, Cloud and Grid Computing, 769-773 2010 7  R. Li, Y. Zhang, H. Yu, X. Wang, J. Wu, and B. Wei, “A social network-aware top-N recommender system using GPU ACM/IEEE joint conference on Digital libraries \(JCDL\, 287296, 2011 8  Z. Gao, and L. Yuying, "Improving the Collaborative Filtering Recommender System by Using GPU," IEEE International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery \(CyberC\, 2012 9  G. Linden, B. Smith, and J. York, "Amazon.com Recommendations: Item-to-Item Collaborative Filtering," IEEE Internet Computing, vol. 7, no. 1, 76-80, 2003 10  B. Kitts, D. Freed, and M. Vrieze, “Cross-sell: A fast promotion-tunable customer–item recommendation method based on conditional independent probabilities,” ACM SIGKDD International Conference, 437–446, 2000 11  M. Jamali and M. Ester, "A Matrix Factorization Technique with Trust Propagation for Recommendation in Social Networks,” ACM Conference on Recommender Systems RecSys\, 135-142, 2010      0 50 100 150 200 250 300 97 95 93 90 87 85 83 80 Runtime \(s Sparsity of the dataset GPU_CMP_CF   GPU_BP_CF   GPU_UNCMP_CF 180 


 
    
 Figure 7  It is not always enough to see how fast a program runs but also the cost of runn ing it. Especially when running on cloud environments where there is a fee for everything and costs can grow quickly. The next section compares the three parsers CPU usage and therefore the cost to run the parser and are shown in Fig. 9 The Parser CPU Cost by File Size chart, shown in Fig. 10 correlates with the Parser Performance chart. It shows the total time spent running the job by adding together the CPU utilization for each of the CPU's. The Fig. 9 chart shows that the parsers CPU usage is fairly close for the smaller file sizes but not for the larger files. There seems to be a dip at the 90 CPU test for all the parsers. Th e dip correlates to the performance graph of each of the processors counts. This may indicate there is a diminishing return as the CPU's increase for each file size Reviewing both charts it is very clear that Jena utilizes the most CPU than the other two The NXParser seems to be the least greedy when utilizing co mpute resources. Therefore the NXParser would be the least expensive to run in most cases Fig 10. shows CPU utilization per parser based on different number of processors. This last chart shows CPU utilization from a slightly different perspectiv e. The chart is similar to the CPU utilization by file size as it shows the same dip in the 90 CPU boundaries. This chart may help to strengthen the what is seen in Fig 9 
Parser Performance - NXParser  Figure 8 Parser Performance - Jena  Figure 9 Parser CPU Cost by File Size  Figure 10 Parser CPU Cost by File Size The Any23 parser as shown in Fig. 6 was in the middle of almost all permutations except it was faster in the 4.7 GB, 10 CPU and 15.9 GB, 64 and 90 CPU runs. Strangely, it was much slower in the 4.7 GB, 90 CPU run. For the smallest file it is slow at first, gains mo mentum and then quickly slows down again. Apparently the overhead of CPU's slows it down. For the medium file size, it optimizes performance at the 24 CPU count. For the large file, it improves over the initial progression of CPU count but falters slightly at the 90 CPU count The NXParser as shown in Fig. 7 and Any23 parser had very similar performance characteris tics which make it difficult to see which parser would work better for particular scenarios It appears the NXParser parser was faster more times than Any23. The NXParser also has the best CPU usage index as noted in the next section. The NXParser shows normal time increases for the small file but speeds up when using 90 CPU's The optimal time on the medium size file is 24 CPU's and does not take advantage of more CPU's. The large file performance improves as expected with more CPU's until the use of 90 CPU's where there is a minor dip The Jena parser as shown in Fig 8, for the most part, was the slowest of all and used the most CPU time, especially in the 15.9 GB file runs. Oddly, it was the best performer in the 0.7 GB, 64 CPU run. For the small file it progressed negatively as the CPU count increased. On the medium file size performance increased as the number of CPU's increased until the 90 CPU count. Finally, Jena took full advantage of throwing CPU's at it 
250 
250 


                    
VI C ONCLUSIONS AND F UTURE W ORK  It is clear that the NX Parser is the most efficient and may be the best for applications needi ng no validation. The Any23 parser is comparable to the NX Parser in performance but not in efficiency In order to understand why Jena is the slowest, the Jena parser has a larger codebase and is performing validations while parsing. Any23 and th e NX Parser are doing simple format validations before writing to the destination output Computing speed, dataset size and quality of output are the critical parameters to be consid ered when determining an application's needs. Usually we want processing to be instantaneous regardless of th e dataset size but this is not realistic. If the application has extremely large datasets and quality is critical, then we will have to give up speed and use the Jena processor. For some extremely large datasets, Jena may not be feasible. On the other hand, if the application with extremely large datasets is not s ubject to quality requirements or data validation has already been performed on the dataset e.g. the Web Data Commons files, then the NX Parser or Any23 are recommended. A pre-validation step using a specialized format checker may be required for some applications that require both quality and performance when using the NX Parser or Any23 It is also clear that there is acceptable speedup when using the Amazon EMR with the smaller file sizes, i.e. 0.7 and 4.7 GB. At the large 15.9 GB file size, the performance seemed to level off and the results were similar or worse than the smaller files. However, it is not clear where the best or optimal cost to performance curve might be. That is up to more testing of the RDFParseParallelMain program which could be done in some future work. But it is interesting to witness the various performance curves within the scenarios of file size and CPU count. The charts can be used for each parser as a starting point when the file size is known. Thi s could prevent overspending of CPU usage for any given test Hopefully, this work has provided a rich set of information about tackling Big Data and Semantic Web challenges. More specifically, when your challe nge is to query large RDF datasets, the determination of which parser to use may now be much easier. This work has been an exhausting challenge. But with any activity, there is always more to do. Next we will highlight some of the areas where future investigation could be focused This work did not have the time or budget to perform extensive testing. Even though the testing program and methodology are very effec tive and produced reasonable results, more testing could solidify the observed trends. Larger dataset sizes would put higher stress on the parsers and CPU's Using more and faster CPU's could result in a better understanding of the speedup curve. CPU analysis could also indicate where there is a diminishing return. In other words, the use of different CPU models coul d give the optimal approach for running certain application types Even though this work was performed on pre-validated datasets, un-validated datasets coul d be tested and, along with the above observations, a be tter heuristic could be developed for the determination of use for a variety of applications. In other words, there may be combinations of parsers and validators that could run in a sequence of steps optimized for a particular application. And, there are many CPU combinations that could be considered for the best result. For example, a very large dataset requiring an av erage level of validation might optimize at 200 large stan dard, first-generation instances. A chart could be developed based on the application or scenario type, as well R EFERENCES   
 
IEEE DanaC 2012 IEEE How Hadoop Map/Reduce works 
Retrieved October 2012, from Web Data Commons http://webdatacommons.org  Retrieved October 2012, fr om Common Crawl http://commoncrawl.org  Retrieved February 2013, from Amazon EC2 https://aws.amazon.com/ec2  Retrieved February 2013, from Amazon EMR http://aws.amazon.com/elasticmapreduce  Retrieved October 2012, from Apache Any23: http://any23.apache.org  Retrieved 02 2013, from Apache Jena:  http://jena.apache.org/index.html  Retrieved October 2012, from NX Parser https://code.google.com/p/nxparser/ redirected from http://sw.deri.org/2006/08/nxparser  Retrieved April 2013, from WikiPedia: Big Data http://en.wikipedia.org/wiki/Big_data  Retrieved February 2013, from Amazon S3: http://aws.amazon.com/s3  Retrieved October 2012, from RDF NQuads http://sw.deri.org/2008/07/n-quads  Retrieved November 2013, from Apache Jena Tutorial http://jena.apache.org/tutorials/rdf_api.html  Retrieved October 2013, from Any23 Incubator http://incubator.apache.org/projects/index.html#graduated  Retrieved October 2012, from NxParser Deri http://semanticweb.org/wiki/DERI_Galway  Retrieved October 2012, from Nxparser Yars  Mohammad Farhan Husain, e a. \(2010\Intensive Query Processing for Large RDF Graphs Using Cloud Computing   Raffael Stein, e. a. \(2009\mber Nine  Bugiotti, F. \(2012\ Data Management in the Amazon Cloud   Hannes Mühleisen, e. a. \(2012\Web Data Commons … Extracting Structured Data from Two Large Web Corpora   Ho, R. \(2008, 12 16 Retrieved April 2013, from DZone: http://architects.dzone.com/articles/how-hadoopmapreduce-work  Retrieved January 2013, from Apache Hadoop http://hadoop.apache.org  
251 
251 


a Outside View b Inside View Figure 13  Mock cave test site Tunnel extends approximately 300m Cave 003oor is covered in rocky material to emulate planetary terrain Site contains surface terrain and tunnel inside building Exploration Performance Experiments A comparison of exploration performance between distributed centralized and uncoordinated task allocation was conducted using a 2-robot team of 2D mapping robots For the uncoordinated runs tasks were randomly assigned to a robot and the robot randomly decided whether to keep the task not taking into account any costs associated with that robot's performance of the task Maps at a resolution of 0.05 meters per pixel were built from 5 runs of each type Each run lasted 15 minutes The operator indicated tasks that should be performed and the system assigned these tasks to a robot In 3 out of 5 runs for each set the 002rst selected task was in the direction of the bridge in the other 2 it was in the direction of the dead-end to the right of the starting position in Figure 12 The same operator selected tasks for all runs Tables 1 2 and 3 show results for each run Percent explored is the percentage of the explorable area as determined from the ground truth model that the robot team explored Unique to total is the ratio of the area explored by a single robot to the total area explored by the team This metric gives a sense of how much overlapping work the robots are doing The average percent explored for runs with a 002rst task in the direction of the bridge was 68 and for runs with the 002rst task in the direction of the dead-end 67 The average ratio of unique to total explored for these cases was 0.45 and 0.44 respectively The average over all runs was 67 of explorable area covered and a ratio of 0.45 unique to total explored area Figure 14 shows merged maps for the runs with the largest and smallest explored areas in this experiment Table 1  2D Mapping Results Distributed Run Bridge 1st  Explored Unique:Total 1 1 80 0.57 2 1 52 0.43 3 1 94 0.45 4 0 97 0.76 5 0 60 0.37 Mean 77 0.52 Std Dev 20 0.15 Table 2  2D Mapping Results Centralized Run Bridge 1st  Explored Unique:Total 1 1 75 0.58 2 0 52 0.46 3 1 55 0.29 4 0 49 0.15 5 1 62 0.59 Mean 59 0.41 Std Dev 10 0.19 Table 3  2D Mapping Results Uncoordinated Run Bridge 1st  Explored Unique:Total 1 1 51 0.20 2 0 54 0.26 3 1 73 0.49 4 0 87 0.65 5 1 68 0.42 Mean 67 0.41 Std Dev 15 0.18 These results show high performance variation within run sets and do not show signi\002cant difference between sets The direction of the 002rst assigned task did not signi\002cantly affect results Limited navigation and path planning capabilities on individual robots common for all runs likely introduces signi\002cant randomness If the robots could more reliably complete their assigned tasks differences between task allocation strategies would likely become more evident Exploration of larger areas could also make differences clearer as more tasks would need to be assigned The lack of signi\002cant differences between allocation methods is somewhat encouraging however It indicates that distributed task allocation the method believed to be most promising for planetary missions does not perform any worse than other methods in early tests The uncoordinated method while by far the simplest would fail once robots with different capabilities are introduced Failure would occur for example if a 3D modeling task were assigned to a 2D mapping robot Mapping and Modeling Experiments An experiment including both 2D mapping and 3D modeling was conducted at the patio test site In this experiment the two 2D mapping robots were operated as described in section 6 A mapped area was then selected by the operator for 3D modeling and the 3D modeling robot was sent to complete that task There was no time limit on the run Figure 15 shows 9 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


