    Abstract 227Data mining is a very active and rapidly growing research area in the field of co mputer science. Its goal is to obtain useful knowledge for users from a database. Association rule mining from a database is one of the most well-known data mining techniques. In general, a large number of if-then rules are extracted by specifying minimum support and confidence levels. They are, however, too complicated as knowledge for users to understand many rules at one time. Multiobjective genetic fuzzy rule selection from Pareto-optimal and near Pareto-optimal rules is a promising approach which can obtain an accurate and simple rule set by considering the accuracy maximization and the complexity minimization. In this paper we propose two extensions of multiobjective genetic fuzzy rule selection for designing more accurate fuzzy rule-based classifiers. One extension is to add compatible rules with misclassified patterns into candida te rules for genetic fuzzy rule selection. The other is to tune membership functions after genetic fuzzy rule selection. We examine the effects of these extensions through computational experiments on imbalanced data sets I  I NTRODUCTION  SSOCIATION rule mining is one of the most well-known techniques in data mining. It finds all rules that satisfy the given minimum confidence and minimum support levels A large number of rules are generated from a database. From the view point of human users, it must be very difficult to understand all the extracted rules at one time. That is, some pruning process is needed to choose a small number of interesting rules. A number of rule evaluation criteria which quantify the interestingness or goodness of a rule have been proposed. For example, gain, variance, chi-squared value entropy gain, gini, laplace, lift and conviction. It was shown in [1] that the best rule according to any of the above mentioned criteria is included in the Pareto-optimal rules with respect to the maximization of confidence and support. In some recent studies [2   i ning has been applied to the design of accurate fuzzy rule-based classifiers In  we proposed genet i c fuzzy rul e sel ect i on from t h e Pareto-optimal rules to obtai n simpler but more accurate classifiers than classifiers with all Pareto-optimal rules Our genetic fuzzy rule selection is a simple two stage method for designing fuzzy classifi 9 In t h e fi rst  stage, a large number of short fuzzy rules are generated. The  Y. Nojima, Y. Kaisho, and H. Ishibuchi are with the Department of Computer Science and Intelligent Syst ems, Osaka Prefecture University Sakai, Osaka 599-8531, JAPAN phone: +81-72-254-9198; fax: +81-72 254-9915; e-mail: nojima@cs.osakafu-u.ac jp, kaisho@ci.cs.osakafu-u.ac.jp hisaoi@cs.osakafu-u.ac.jp short rule means that the rule has a few antecedent conditions   9  Pareto-optim al a nd near Pareto-optimal rules are chosen among the generated rules based on the dominance relation with respect to the confidence and support of each rule. The chosen rules are referred to as candidate rules. In the second stage, we apply a genetic algorithm to a combinatorial optimization of the candidate rules. We use two criteria accuracy maximization and co mplexity minimization Although we successfully obtai ned simple and accurate classifiers by our method, th e accuracy was not always high for some data sets, especially class imbalanced data sets In this paper, we propose two extensions of our genetic fuzzy rule selection for designi ng more accurate classifiers One extension is to add compatible rules with misclassified patterns into candidate rules for genetic fuzzy rule selection The other extension is to tune membership functions after genetic fuzzy rule selection We examine the effects of the proposed extensions through computational experiments on imbalanced data sets This paper is organized as follows. In Section II, we briefly explain fuzzy if-then rules for pattern classification problems and Pareto-optimal rules. We also explain genetic fuzzy rule selection for the design of fuzzy rule-based classifiers. In Section III, we propose the above-mentioned two extensions of genetic fuzzy rule selection We examine the effects of the proposed extensions through computational experiments in Section IV. Finally, we conclude this paper in Section V II  A SSOCIATION R ULE M INING AND G ENETIC F UZZY R ULE S ELECTION FOR D ESIGNING F UZZY C LASSIFIERS  A  Fuzzy If-then Rules Let us assume that we have m training \(i.e., labeled patterns x p   x p 1 205 x pn  p 1, 2, \205 m from M classes in the n dimensional continuous pattern space where x pi is the attribute value of the pth training pattern for the ith attribute  i 1, 2 n For the simplicity of explanation, we assume that all the attribute values ha ve already been normalized into real numbers in  For our n dimensional pattern classification problem, we use fuzzy rules of the following type Rule R q If x 1 is A q 1 and  ...  and x n is A qn  then Class C q with CF q   1 where R q is a rule label x  x 1 205 x n  n dimensional pattern vector A qi is an antecedent fuzzy set i 1, 2, \205 n  C q is a class label, and CF q is a rule weight Accuracy Improvement of Genetic Fuzzy Rule Selection with Candidate Rule Addition and Membership Tuning Yusuke Nojima Member, IEEE Yutaka Kaisho, and Hisao Ishibuchi Senior  Member, IEEE  A 978-1-4244-8126-2/10/$26.00 ©2010 IEEE 


   We simultaneously use multiple fuzzy partitions with different granularities in fuzzy rule generation. We use four homogeneous fuzzy partitions in Fig. 1 \(i.e., 14 triangular fuzzy sets\e also use a dom an antecedent fuzzy se t to represent a don\222t care condition. That is, we use the 15 anteceden t fuzzy sets in total  0 1 1 Membership Input value 01 1 Membership Input value 01 1 Membership Input value 01 1 Membership Input value S 2 L 2 S 5 ML 5 SM 5 L 5 M 5 S 4 ML 4 SM 4 L 4 S 3 L 3 L 3  Fig. 1.  Four fuzzy partitions used in our computational experiments The consequent class C q and the rule weight CF q of each fuzzy rule R q can be specified in a heuristic manner by compatible training patterns w ith the antecedent part of R q  For details 11  Since we use the 15 antecedent fuzzy sets for each attribute of our n dimensional pattern classification problem, the total number of possible fuzzy rules is 15 n For high-dimensional data sets, the total number of possible fuzzy rules becomes quite large. Moreover, it is very difficult to intuitively understand long fuzzy rules with many antecedent conditions Thus, we generate short fuzzy rules with only a small number of antecedent conditions. In this paper, we examine short fuzzy rules of length L max or less \(e.g L max 3\n order to generate understandable fuzzy rules as candidates in fuzzy rule selection. It should be noted that the length of a fuzzy rule is defined by the numbe r of its antecedent conditions excluding don\222t care This limitation on the rule length also leads to short computation time for rule generation. It is much faster than GA-based optimization and tuning processes Instead of specifying the maximum rule length, identifying the relevant variables for each data set may be a good alternative approach \(e.g., [12 B  Pareto-optimal and Near Pareto-optimal Rules In the data mining field, two rule evaluation criteria called confidence and support are frequently used to measure the goodness of rules. Confidence and support of rules for data with quantitative attributes are calculated by using fuzzy sets Let us assume that we have m training patterns x p The confidence of the rule R q for class h is defined as follows m p p h p q q p q h conf 1 Class   Class  x x A A x A 2 where  p q x A is the membership value of the pattern x p to the antecedent part A q  A q 1  A qn It is calculated with the product operator as  1 1 pnA pAp x x qn q q x A 3 where  qi A is the membership value to the A qi  The confidence conf   A q Class h e ratio of compatible patterns with both the antecedent part A q and the consequent class h to compatible patterns with the antecedent part A q The confidence conf   A q C q measures the validity of the linguistic association rule A q C q The confidence can be viewed as the fuzzy conditional probability of class C q  In the same manner, support of the rule R q for class h is defined as follows m h supp h p q p q Class  Class  x x A A 4 The support supp  A q Class h e ratio of compatible patterns with both the antecedent part A q and the consequent class  h to the given m training patterns. The support supp  A q C q measures the covered rate of training patterns by the linguistic association rule A q C q In the same manner, the coverage covr  A q Class h lated as h h p q m h covr p q Class  Class  x A x A 5 where m h is the number of patterns in the class h In this paper we use coverage instead of support. This is because the range of support values is often very different among the classes for imbalance data sets We generate all the possible rules of length L max or less. If the confidence and coverage of the generated rules are smaller than the minimum confidence and coverage levels they are removed. Among the remaining rules, Pareto optimal and near Pareto-optimal rules with respect to the maximization of confidence a nd coverage are extracted In order to handle not only Pareto-optimal rules but also near Pareto-optimal rules Pareto-optimal rules using a dominance margin are defi Fi g 2 shows a Pareto-optimal rules and \(b\ear Pareto-optimal rules in the confidence-coverage space. Each circle represents a rule. A red line in Fig. 2 \(a\ndicates a Pareto front  Confidence Coverage  conf covr Confidence Coverage  a\-optimal rules               \(b\-optimal rules Fig. 2.  Example of the distribution of Pareto-optimal and near Pareto-optimal rules with the dominance margin  


   A rule R i is said to be dominated by another rule R j when at least one of the following two conditions are satisfied covr  R i  covr  R j  covr and conf   R i  conf   R j  conf 6 covr  R i  covr  R j  covr and conf   R i  conf   R j  conf 7 where conf and covr are the margins for confidence and coverage, respectively. To adjust the ranges between confidence and coverage covr is calculated as conf k k k k covr Rconf Rconf Rcovr Rcovr min\\(max min\\(max 8 When a rule R i is not dominated by any other rules in the sense of 7 R i as an Pareto-optimal rule. We remove weak Pareto-optimal rules with the highest confidence \(i.e., 1.00\all coverage as in [8] because good results were obtained by removing them i.e., to reduce the search space for rule selection C  Classifier Design by Genetic Fuzzy Rule Selection In order to obtain simple and accurate classifiers, we use multiobjective genetic fuzzy rule selection. Let us assume that we have already extracted N rules by the association rule mining technique explained in the last section. Each binary string of length N is represented as N ssssS  321 where s i   1 and s i   0 mean that the ith candidate rule is included in and excluded from the rule set S respectively Since any subsets of the N rules are represented by binary strings, we can use almost all evolutionary multiobjective optimization algorithms. In this paper, we use the NSGA-II algorithm ance EMO algorithm. Let P be the current population in NSGA-II The outline of NSGA-II can be written as follows 1 P Initialize P  2 while a termination condition is not satisfied do  3 P 222 = Selection P  4 P 222\222 = Genetic Operations P 222 5 P Replace P P 222\222 6 end  while  7 return non-dominated solutions P   First, an initial population is gene rated in Line 1. In Line 3 parent individuals \(i.e P 222\are selected from the current population P The standard binary tournament selection is usually used to choose a pair of parent individuals. In Line 4 an offspring population P 222\222 is generated from the parent population P 222 by uniform crossover and biased mutation. The biased mutation changes 0 to 1 with a small probability and 1 to 0 with a large probability to decrease the number of 1\222s in each offspring. In Line 5, the best individuals are chosen from the merged population P P 222\222 population P   We use the following two objec tives to find an accurate and simple fuzzy classifier S  f 1  S  The arithmetic average of the classification rate for each class by S  f 2  S  The number of selected fuzzy rules in S  Genetic rule selection is formulated as follows Maximize f 1  S inimize f 2  S 9 We use a single winner-based i.e., winner-take-all classification method. A single winner rule is identified by using the product of the comp atibility grade and the rule weight CF of each fuzzy rule. Each CF value is not changed during genetic rule selection Since we use the single winner-based classification method some rules may be used for the classification of no training patterns. Whereas the existence of such an unnecessary rule in the rule set S has no effect on the first objective \(i.e., the arithmetic average of the classi fication rate for each class deteriorates the second objective. Thus we remove all the unnecessary rules responsible for the classification of no training patterns before calculating the second objective III  T WO E XTENSIONS FOR A CCURACY I MPROVEMENT  We propose two extensions of genetic fuzzy rule selection candidate rule addition and membership function tuning A  Candidate Rule Addition For class imbalanced data, the classification accuracy is often very worse. The reason may be that the necessary rules for correctly classifying pattern s of a minority class are not extracted as candidate rules for genetic fuzzy rule selection This means that there is no possibility to improve the accuracy for the minority class even if we found the optimal combination of candidate rules To handle this issue, we propose a candidate rule addition procedure which adds compatible rules with misclassified patterns into candidate rules. The procedure is as follows Step 1 A number of rules are generated from numerical data under the condition on the maximum rule length, the minimum confidence and coverage levels. Let us refer to the generated rules as R  Step 2 Pareto optimal rules are extracted from R Let us refer to these rules as R Pareto  Step 3 Pre-classification of all the training patterns is performed by R Pareto  Step 4 Rules compatible with misclassified patterns are extracted from R  R Pareto A single rule is extracted for each misclassified pattern. Let us refer to the additional rules as R add  We use R Pareto and R add as candidate rules of our genetic fuzzy rule selection As a rule addition criterion, we examine the following two cases in this paper EC For each misclassified patte rn, we add a rule with the 


   same class as that of the pattern and with the highest compatibility grade to the pattern ECW: For each misclassified patte rn, we add a rule with the same class as that of the pattern and with the highest product value of the compa tibility grade and the rule weight B  Membership Function Tuning A number of membership function tuning techniques have been already proposed in the litera ture [14]-[17]. In this paper we employ genetic lateral tuni 15 aft e r genet i c rul e  selection. We show the example of lateral tuning in Fig. 3 The vertex of the membership function is originally on x 1/3 in the unit i.e., it corresponds to SM 4 in the fourth granularity in Fig. 1\ex of the membership function is laterally movable between 1/6 and 1/2 in genetic lateral tuning Genetic lateral tuning does not lose the structure of the linguistic labels initially defi ned because the prespecified shape of the membership functions is preserved and the vertexes of the adjoining membership functions are not overlapped, see more details in 15    01  21  1/3 1/6  Fig. 3.  The movable area of a membership function We apply a single-objective genetic algorithm with real coding to the genetic lateral tuning. We tune the set of 14 fuzzy sets for each attribute in the classifier obtained by genetic fuzzy rule selection That is, each string length is 14 the number of attributes The objective function in the genetic lateral tuning is the same as f 1  S  tuning procedure. Each square plot represents a non dominated classifier obtained by genetic rule selection. We perform the genetic lateral tuning for each non-dominated classifier obtained by genetic rule selection. Each circle represents a newly obtained classifier by the genetic lateral tuning from the classifier of the closed square. A closed circle means the best classifier among the new classifiers. After the genetic lateral tuning for all the non-dominated classifiers, we choose a classifier with the best training data accuracy An initial population is generate d by uniformly distributed random values within each movable area. One individual has the same values as the original vertex position. Tournament selection, BLX crossover 0.1\form mutation are used as genetic operations in our experiments The CF value of each rule is speci fied as the original one and not changed by the genetic lateral tuning. The adaptation of the CF value during the genetic lateral tuning can be interesting alternative method Rule 1 Rule 2 Rule 3 Rule 4 Class 1 Class 2 Class 3 Error Number of rules Class 4  Fig. 4.  Illustration of genetic lateral tuning IV  C OMPUTATIONAL E XPERIMENTS  In our computational experiments, we used two-class data sets shown in Table I. Some of the data sets are generated from original data sets with more than two classes available from UCI Machine Learning Repository. Table I shows the number of patterns, the number of attributes, and the imbalance ratio for each two-cl ass data. \223bal1\224 was also examined, but we eliminated th is data set because we could not generate any minority cla ss rules under the condition of the maximum rule length specified in this experiment. The imbalance ratio is the number of patterns of a majority class divided by that of a minority class  TABLE  I D ATA S ETS USED IN O UR C OMPUTATIONAL E XPERIMENTS  Data set Number of patterns Number of attributes Imbalance ratio bal0 625  4  1.17 bal2 625  4  1.17 glass1 214  9  1.82 pima 768  8  1.87 glass0 214  9  2.06 newt2 215  5  2.31 haberman 306  3  2.78 newt1 215  5  5.14 newt0 215  5  6.17 glass5 214  9  6.40 glass2 214  9  11.6 glass3 214  9  15.5 glass4 214  9  22.8  For rule extraction, we speci fied the minimum confidence and coverage levels as 0.60 and 0.01, respectively. We examined the effects of four specification of the dominance margin conf  conf 0.00, 0.01, 0.05, 0.10. The covr was calculated by \(8 The settings of computational experiments are as follows Genetic Fuzzy Population size: 200 The number of generations: 1000 


   Crossover probability: 0.9 Mutation probability p m 0 1 N and p m 1 0  N the number of candidate rules Lateral Tuning Population size: 100 The number of generations: 100 Crossover probability: 1.0 Mutation probability: 1 L  L the length of a string We examined the following six approaches Pareto Genetic rule selection with Pareto optimal rules EC Genetic rule selection with Pareto optimal rules and additional rules accord ing to the membership degree ECW Genetic rule selection with Pareto optimal rules and additional rules according to the product of the membership degree and the rule weight Pareto Pareto with genetic lateral tuning EC EC with genetic lateral tuning ECW ECW with genetic lateral tuning We performed the three times of the ten-fold cross validation procedure \(3 10CV\e chose the classifier with the best training accuracy after ge netic fuzzy rule selection in the case of Pareto, EC, ECW, and after genetic lateral tuning in the case of Pareto*, EC*, ECW A  Experimental Results with 0.0 Table II shows the average training data accuracy over 30 runs for each data set and the average training data accuracy over all the data sets. Although the average training data accuracy over all the data sets may be unfair statistics, we use this value for rough comparison among six approaches. Bold face indicates the best value in each row From Table II, we can see a clear positive effect of candidate rule addition comparing Pareto with EC and ECW We can also see further improvement by genetic lateral tuning. The best training da ta accuracy was improved from 82.58 to 89.91 on average According to the recommendation by Dem\232ar  we performed the Freidman\222s test t o t e st t h e nul l  hy pot hesi s that all the approaches work equivalently on average Regarding the training data accuracy in Table II, the Friedman\222s test rejected the null hypothesis. Each average rank is shown in Table II. A smaller rank value is better  TABLE  II T RAINING D ATA A CCURACY OF THE O BTAINED C LASSIFIERS    0.0 Pareto EC ECW Pareto* EC* ECW bal0 93.51 96.33 97.92 96.23 97.50 98.72 bal2 93.50 96.42 98.03 96.17 97.74 98.88 glass1 71.02 77 72 79.28 81.79 88.80 89.18 pima 76.00 79.35 80.69 78.96 81.04 82.29 glass0 66.52 69.91 69.84 76.05 74.77 74.36 newt2 95.79 97.78 97.95 99.82 99.75 99.86 haberman 60.67 63.85 63.80 69.88 73.34 72.62 newt1 94.82 97.69 97.75 97.68 99.32 99.36 newt0 98.66 98.85 98.85 99.80 99.75 99.76 glass5 95.24 97.78 97.94 97.14 98.05 98.05 glass2 55.56 56 45 56.45 56.11 58.42 58.55 glass3 89.04 91 54 95.41 93.32 95.42 97.75 glass4 83.18 94.88 94.80 93.62 99.51 99.51 Average 82.58 86.04 86.82 87.43 89.49 89.91 Ave. Rank 6.00 4 15 3.54 3.77 2.15 1.38  Since the Friedman\222s test rej ected the null hypothesis, we also performed pair-wise comparisons like Nemenyi\222s test  Hol m 222s t e st 21 Shaffer\222s t e st 22 and B e rgm a nn and Hommel W e used t h e open source code  TABLE  III A DJUSTED P VALUES OBTAINED BY N EMENYI 222 S T EST   H OLM 222 S T EST   S HAFFER 222 S T EST  AND B ERGMANN H OMMEL P ROCEDURE FOR THE T RAINING D ATA A CCURACY OBTAINED IN OUR C OMPUTATIONAL E XPERIMENTS  i Hypothesis Unadjusted p  p Neme  p Holm  p Shaf  p Berg  1 Pareto vs. ECW 3.2 x 10 10 4.8 x 10 9 4.8 x 10 9 4.8 x 10 9 4.8 x 10 9  2 Pareto vs. EC 1.6 x 10 7 2.4 x 10 6 2.2 x 10 6 1.6 x 10 6 1.6 x 10 6  3 EC vs. ECW 0 0002 0.0024 0.0021 0.0016 0.0016 4 Pareto vs. ECW 0 0008 0.0119 0.0095 0.0080 0.0056 5 Pareto* vs. ECW 0.0012 0.0173 0 0127 0.0116 0.0081 6 Pareto vs. Pareto 0.0024 0.0355 0 0237 0.0237 0.0142 7 ECW vs. ECW 0 0033 0.0500 0.0300 0.0237 0.0142 8 EC vs. EC 0.0064 0.0963 0.0514 0.0449 0.0385 9 Pareto vs. EC 0 0119 0.1781 0.0831 0.0831 0.0475 10 Pareto* vs. EC 0 0277 0.4156 0.1663 0.1663 0.0831 11 ECW vs. EC 0.0592 0.8876 0.2959 0.2367 0.1183 12 EC* vs. ECW 0 2945 1.0000 1.0000 1.0000 1.0000 13 EC vs. ECW 0.4017 1.0000 1.0000 1.0000 1.0000 14 EC vs. Pareto 0 6002 1.0000 1.0000 1.0000 1.0000 15 ECW vs. Pareto 0 7532 1.0000 1.0000 1.0000 1.0000 


   developed by Garc\355a and Herrera Table III shows the adjusted p values of those statistical tests. The statistical results also show the effectiveness of the proposed candidate rule addition and genetic lateral tuning Table IV shows the average number of rules in the obtained classifiers. From Ta ble IV, we can see that each classifier has a very small number of rules. There are three interesting observations. One is that the number of rules was increased when we compared Pa reto with EC and ECW. This means that the necessary rules were successfully added to the candidate rule set and selected by genetic rule selection Another interesting observation is that the number of rules was reduced after genetic lateral tuning. This is a positive side effect of genetic lateral tuning. The other interesting observation is that the number of rules decreased as the imbalance rate became larger. We need further investigation in a future study  TABLE  IV N UMBER OF F UZZY R ULES IN THE O BTAINED C LASSIFIERS    0.0 Pareto EC ECW Pareto* EC* ECW bal0 8.3 14.8 25 5 6.5 12.3 24.5 bal2 8.0 14.8 27 5 4.6 12.5 25.8 glass1 6.0 10.3 11.2 5.1 8.8 9.8 pima 7.7 12.5 15 4 7.1 11.4 13.9 glass0 3.0 2.8 2.8 2.9 2.6 2.4 newt2 5.3 6.8 6 6 4.6 5.2 5.4 haberman 7.3 8.4 8.1 6.6 7.5 7.2 newt1 3.9 4.4 4 5 3.4 4.2 4.5 newt0 3.8 3.8 3 8 2.4 2.1 2.2 glass5 3.6 4.6 5.3 2.7 4.5 5.0 glass2 3.6 3.5 3.5 2.7 2.9 2.9 glass3 3.2 4.1 5.1 3.1 3.8 4.8 glass4 3.7 4.6 4.3 2.7 3.6 3.0   TABLE  V T EST D ATA A CCURACY OF THE O BTAINED C LASSIFIERS    0.0 Pareto EC ECW Pareto* EC* ECW bal0 88.35 91.51 89.22 92.26 92.68 89.79 bal2 91.58 92.43 91.07 94.07 93.03 91.68 glass1 63.97 68.35 67.65 72.19 76.62 74.45 pima 73.99 72.14 73.51 74.19 73.36 72.28 glass0 62.85 68.10 68.04 70.75 73.40 72.53 newt2 94.77 92.48 92.98 97.09 96.44 95.57 haberman 55.48 57 30 56.75 59.76 60.20 60.37 newt1 95.28 95 56 94.86 95.19 97.04 96.67 newt0 96.14 93.36 95.03 96.31 96.03 96.59 glass5 93.53 93 05 93.14 92.05 91.10 93.69 glass2 49.62 49 15 48.90 49.40 50.44 50.56 glass3 80.71 81.26 83.59 81.76 87.00 82.32 glass4 69.46 82 17 78.99 72.17 88.32 91.58 Average 78.13 79.76 79.52 80.55 82.74 82.16 Ave. Rank 4.54 4.31 4.62 3.00 2.23 2.31 Table V shows the average test data accuracy over 30 runs for each data set and the averag e test data accuracy over all the data sets as Table II. From Table V, it seems that there is a less effect of candidate rule addition comparing the training data accuracy in Table II. On the other hand, we can see that genetic lateral tuning can improve the generalization ability for the test data. We performed the statistical tests in the same manner as the training data accuracy. The Friedman\222s test rejected the null hypothesis. Thus, we performed Nemenyi\222s test, Holm\222s test, Shaffer\222s test, and Bergmann-Hommel procedure. Table VI shows the adjusted p values of those statistical tests. There are clear statistically significant differences between Pareto and all the approaches with genetic lateral tuning. We can also see that the combinational use of candidate rule addition and genetic lateral tuning can improve the generalization ability on the test data very well e.g., see the p values of Pareto vs. ECW* and Pareto vs EC B  Comparison among Several the Dominance Margins We examined the effects of the dominance margin. Figs. 5 and 6 show the overall training data accuracy and test data accuracy over all the data sets. While a larger value improved the training data accuracy thanks to a larger number of candidate rules, the test da ta accuracy was not improved by the larger value. Comparing the results in the same we can see a clear improvement by genetic lateral tuning for both training and test data accuracy  Pareto EC ECW Pareto EC ECW  0.00 0.01 0.05 0.10 80 84 88 92 Accuracy  Fig. 5.  Average training accuracy over all the data sets Pareto EC ECW Pareto EC ECW  0.00 0.01 0.05 0.10 76 79 82 85 Accuracy  Fig. 6.  Average test accuracy over all the data sets 


    Figs. 7 and 8 show the average rank for the training data accuracy and the test data accuracy. We can observe almost all of the same effects of candidate rule addition and lateral tuning as in the case of 0.0  Pareto EC ECW Pareto EC ECW  0.00 0.01 0.05 0.10 0 2 4 6 Average rank  Fig. 7.  Average rank for the training data accuracy over all the data sets A smaller rank is better Pareto EC ECW Pareto EC ECW  0.00 0.01 0.05 0.10 0 2 4 6 Average rank  Fig. 8.  Average rank for the test data accuracy over all the data sets A smaller rank is better  C  Comparison between Low and High Imbalanced Data We divided the data sets used in our experiments into two subsets: Low imbalance data sets \(IR < 3.0\gh imbalanced data sets \(IR > 3.0\gs. 9 and 10 show the average rank for the test data accuracy over the low imbalanced data sets and the high imbalance data sets respectively. We can see that the positive effect of genetic lateral tuning became weaker as became larger for the low imbalance data sets. On the other hand, the approaches with candidate rule addition and ge netic lateral tuning \(i.e., EC and ECW*\kept good ranking in almost all cases. This observation may indicate that the proposed extensions are effective especially for the high imbalanced data sets V   C ONCLUSION  In this paper, we proposed two extensions of our genetic fuzzy rule selection for designi ng more accurate classifiers One extension is to add compatible rules with misclassified patterns into candidate rules for genetic fuzzy rule selection The other extension is to tune membership functions by genetic lateral tuning after genetic fuzzy rule selection Experimental results showed that the candidate rule addition can improve the training data accuracy. This is because the possibility that the misclassified training patterns are classified by additional rules becomes high. Experimental results also showed that the genetic lateral tuning can improve the test data accuracy as well as the training data accuracy, since fuzzy membersh ip functions are properly adjusted according to the pattern distributions. The combination of the proposed two extensions would be the best choice As a future study, we will examine the effects of the proposed extensions using a large number of data sets TABLE  VI A DJUSTED P VALUES OBTAINED BY N EMENYI 222 S T EST   H OLM 222 S T EST   S HAFFER 222 S T EST  AND B ERGMANN H OMMEL P ROCEDURE FOR THE T EST D ATA A CCURACY OBTAINED IN OUR C OMPUTATIONAL E XPERIMENTS  i Hypothesis Unadjusted p  p Neme  p Holm  p Shaf  p Berg  1 Pareto vs. ECW 3.1 x 10 10 4.8 x 10 9 4.8 x 10 9 4.8 x 10 9 4.9 x 10 9  2 Pareto vs. EC 1.6 x 10 7 2.4 x 10 6 2.2 x 10 6 1.6 x 10 6 1.6 x 10 6  3 EC vs. ECW 1.6 x 10 4 0.0024 0.0021 0.0016 0.0016 4 Pareto vs. ECW 8.0 x 10 4 0.0119 0.0095 0.0080 0.0056 5 Pareto* vs. ECW 0.0012 0.0173 0 0127 0.0116 0.0081 6 Pareto vs. Pareto 0.0024 0.0355 0 0237 0.0237 0.0142 7 ECW vs. ECW 0 0033 0.0500 0.0300 0.0237 0.0142 8 EC vs. EC 0.0064 0.0963 0.0514 0.0449 0.0385 9 Pareto vs. EC 0 0119 0.1781 0.0831 0.0831 0.0475 10 Pareto* vs. EC 0 0277 0.4156 0.1662 0.1662 0.0831 11 ECW vs. EC 0.0592 0.8876 0.2959 0.2367 0.1183 12 EC* vs. ECW 0 2945 1.0000 1.0000 1.0000 1.0000 13 EC vs. ECW 0.4017 1.0000 1.0000 1.0000 1.0000 14 EC vs. Pareto 0 6002 1.0000 1.0000 1.0000 1.0000 15 ECW vs. Pareto 0 7532 1.0000 1.0000 1.0000 1.0000 


   including data sets with more than two classes. We also have to compare the proposed method with other learning algorithms. There are still a lot of things we have to improve in the proposed extensions, es pecially the search space and genetic operations in the lateral tuning. The discussion on the tradeoff relation among accuracy, complexity, and interpretability is also another important future research issue  Pareto EC ECW Pareto EC ECW  0.00 0.01 0.05 0.10 0 2 4 6 Average rank  Fig. 9.  Average rank for the test data accuracy over low imbalanced data sets A smaller rank is better Pareto EC ECW Pareto EC ECW  0.00 0.01 0.05 0.10 0 2 4 6 Average rank  Fig. 10.  Average rank for the test da ta accuracy over high imbalanced data sets. A smaller rank is better R EFERENCES  1  R. J. Bayardo Jr. and R. Agrawal 223Mining the most interesting rules,\224 Proc. of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ACM Press, New York, NY USA, pp. 145-154, 1999 2  B. Liu, W. Hsu, and Y. Ma, \223Integ rating classification and association rule mining,\224 Proc. of 4th International Conference on Knowledge Discovery and Data Mining New York, AAAI Press, August 27-31 1998, pp. 80-86 3  X. Yin and J. Han, \223CPAR: Cl assification based on predictive association rules,\224 Proc. of SIAM International Conference on Data Mining, San Francisco CA, May 2003, pp. 331-335 4  H. Ishibuchi, K. Nozaki, N. Yamamoto, and H. Tanaka, \223Selecting fuzzy if-then rules for classi fication problems using genetic algorithms,\224 IEEE Trans. on Fuzzy Systems vol. 3, no. 3, pp. 260-270 1995 5  H. Ishibuchi, T. Murata, and I B. Turksen, \223Single-objective and two-objective genetic algorithms for selecting linguistic rules for pattern classification problems,\224 Fuzzy Sets and Systems vol. 89, no. 2 pp. 135-150, 1997 6  H. Ishibuchi, T. Nakashima, and T Murata, \223Three-objective genetics based machine learning for linguistic rule extraction,\224 Information Sciences vol. 136, no. 1-4, pp. 109-133, 2001 7  H. Ishibuchi and T. Yamamoto, \223F uzzy rule selection by multi objective genetic local search algorith ms and rule evaluation measures in data mining,\224 Fuzzy Sets and Systems vol. 141, no. 1, pp. 59-88 2004 8  H. Ishibuchi, I. Kuwajima, and Y Nojima, \223Prescreening of candidate rules using association rule mining a nd Pareto-optimality in genetic rule selection,\224 Proc. of 11th International Conference on Knowledge Based and Intelligent Informa tion and Engineering Systems pp 509-516, 2007 9  I. Kuwajima, Y. Nojima, and H. Ishibuchi, \223Obtaining accurate classifiers with Pareto-optimal and near Pareto-optimal rules,\224 Proc. of 11th International Symposium on Artificial Life and Robotics pp 195-198, Beppu, Japan, January 31-February 2, 2008 10  H. Ishibuchi and T. Nakashima, \223E ffect of rule weights in fuzzy rule-based classification systems,\224 IEEE Trans. Fuzzy Systems vol. 9 no. 4, pp. 506-515, August 2001 11  H. Ishibuchi and T. Yamamoto, \223Rule weight specification in fuzzy rule-based classification systems,\224 IEEE Trans. on Fuzzy Systems vol 13, no. 4, pp 428-435, August 2005 12  J. M. Alonso and L. Magdalena, \223A n interpretability-guided modeling process for learning comprehensible fuzzy rule-based classifiers,\224 Proc of 9th International Conference on Intelligent Systems Design and Applications pp. 432-437, 2009 13  K. Deb, A. Pratap, S. Agarwal, a nd T. Meyarivan, \223A fast and elitist multiobjective genetic algorithm: NSGA-II,\224 IEEE Trans. on Evolutionary Computation vol. 6, no. 2, pp. 182-197, April 2002 14  R. Alcal\341, J. Alcal\341-Fdez, and F Herrera, \223A proposal for the genetic lateral tuning of linguistic fuzzy syst ems and its interaction with rule selection,\224 IEEE Trans. on Fuzzy Systems vol. 15, no. 4, pp. 616-635 2007 15  J. Alcal\341-Fdez, R. Alcal\341, M. J. Gacto, and F. Herrera, \223Learning the membership function contexts for mining fuzzy association rules by using genetic algorithms,\224 Fuzzy Sets and Systems vol. 160, pp 905-921, 2009 16  M. Kaya and R. Alhaji, \223Utilizi ng genetic algorithms to optimize membership functions for fuzzy weighted association rules mining,\224 Applied Intelligence vol. 24, no. 1, pp. 7-15, February 2006 17  W. Wang and S. M. Bridges, \223G enetic algorithm optimization of membership functions for mining fuzzy association rules,\224 Proc. of the 7th International Conference on Fuzzy Theory & Technology  pp.131-134, Atlantic City, NJ February 27-March 3, 2000 18  J. Dem\232ar, \223Statistical comparisons of classifiers over multiple data sets,\224 Journal of Machine Learning Research vol. 7, pp. 1-30, 2006 19  M. Friedman, \223The use of ranks to avoid the assumption of normality implicit in the analysis of variance,\224 Journal of the American Statistical Association vol. 32, pp. 675-701, 1937 20  P. B. Nemenyi Distribution-free Multiple Comparisons PhD thesis Princeton University, 1963 21  S. Holm, \223A simple sequentially re jective multiple test procedure,\224 Scandinavian Journal of Statistics vol. 6, pp. 65-70, 1979 22  J. P. Shaffer, \223Modified seque ntially rejective multiple test procedures,\224 Journal of the American Statistical Association vol. 81 pp. 826-831, 1986 23  G. Bergmann and G. Hommel, \223Impr ovements of general multiple test procedures for redundant systems of hypotheses,\224 In Bauer, Hommel and Sonnemann, eds  Multiple Hypotheses Testing Springer, Berlin, pp 100-115, 1988 24  S. Garc\355a and F. Herrera, \223An exte nsion on \221Statistical comparisons of classifiers over multiple data sets\222 for all pairwise comparisons,\224 Journal of Machine Learning Research vol. 9, pp. 2677-2694 December 2008 The source code is available at http sci2s.ugr.es/keel/multipleTest.zip   


Application of Chaotic Particle Swarm Optimization Algorithm in Chinese Documents Classification 763 Dekun Tan Qualitative Simulation Based on Ranked Hyperreals 767 Shusaku Tsumoto Association Action Rules and Action Paths Triggered by Meta-actions 772 Angelina A. Tzacheva and Zbigniew W. Ras Research and Prediction on Nonlinear Network Flow of Mobile Short Message Based on Neural Network 777 Nianhong Wan, Jiyi Wang, and Xuerong Wang Pattern Matching with Flexible Wildcards and Recurring Characters 782 Haiping Wang, Fei Xie, Xuegang Hu, Peipei Li, and Xindong Wu Supplier Selection Based on Rough Sets and Analytic Hierarchy Process 787 Lei Wang, Jun Ye, and Tianrui Li The Covering Upper Approximation by Subcovering 791 Shiping Wang, William Zhu, and Peiyong Zhu Stochastic Synchronization of Non-identical Genetic Networks with Time Delay 794 Zhengxia Wang and Guodong Liu An Extensible Workflow Modeling Model Based on Ontology 798 Zhenwu Wang Interval Type-2 Fuzzy PI Controllers: Why They are More Robust 802 Dongrui Wu and Woei Wan Tan Improved K-Modes Clustering Method Based on Chi-square Statistics 808 Runxiu Wu Decision Rule Acquisition Algorithm Based on Association-Characteristic Information Granular Computing 812 JianFeng Xu, Lan Liu, GuangZuo Zheng, and Yao Zhang Constructing a Fast Algorithm for Multi-label Classification with Support Vector Data Description 817 Jianhua Xu Knowledge Operations in Neighborhood System 822 Xibei Yang and Tsau Young Lin An Evaluation Method Based on Combinatorial Judgement Matrix 826 Jun Ye and Lei Wang Generating Algorithm of Approximate Decision Rules and its Applications 830 Wang Yun and Wu-Zhi Qiang Parameter Selection of Support Vector Regression Based on Particle Swarm Optimization 834 Hu Zhang, Min Wang, and Xin-han Huang T-type Pseudo-BCI Algebras and T-type Pseudo-BCI Filters 839 Xiaohong Zhang, Yinfeng Lu, and Xiaoyan Mao A Vehicle License Plate Recognition Method Based on Neural Network 845 Xing-Wang Zhang, Xian-gui Liu, and Jia Zhao Author Index 849 
xiii 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





