Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 45 November 2002 AN EFFECTIVE PROJECTION-REDUCTION ALGORITHM FOR MINING LONG FREQUENTS XUO-FENG WANG\221ab\222 TUN-RAN WANG\221\223\222 YUE ZHAO\221b\222 223\222Shenyang Institute of Automation Chinese Academy of Sciences China 221b\222Shenyang institute of Chemical Technology China E-MAIL: wrpjoy lf53.com Abstract An effective Projection-reduction Algorithm for mining long patterns frequent is pmented A new ideal of top-down mining frequent items is adopted and some of new conceptions 
such as transaction and items assofiation information tables key-items and reduction item projection DB etc are proposed The algorithm presented is very effective for mining long frequents the validity of proposed algorithms is proved througb analysis computing complexity Some examples of computation are given also The computing complexity of the algorithm has relation to the average lengtb of items reduction the complexity approximate8 to 0.5 X M X 0\(Zs X N\222 in worst of 
ease here S is the average lengtb of items reduction under min-support given by user N\222 is the number of tuples in the database N is numbers of the transaction in databases M is the average length of item sets in databases On the side using heuristic information for pruning useless candidate frequent itemsets the efficiency of algorithm is improved notably It is very effective for mining long frequent 
since S is very short for long pattern frequent the computing complexity approach to polynomial time Keywords Top-Down Data mining Frequent item Associate rules Reduction, Projection 1 Introduction Instead of beginning with I-itemsets as Apriori algorithm to generate the lager candidate itemsets we take the itemsets of transactions as candidates directly and find frequent If this itemset 221s support is not lager than min-sup we will generate the new candidates by with of erasing the key-item which works on the support from the itemset in 
order to increase their support Then the algorithm determines which of the new candidates is frequent itemset if not repeat the above procedure until finding frequent itemset From the above description we can say this algorithm mine from top to down We uses the fact that if a set of items S satisfies min-sup then any subset of S satisfies min-sup too That means we needn\222t go on finding the shorter frequent itemsets if we have got the frequent itemset. Hence we can speed up the efficiency of determining the frequent itemsets without the combination 51 5 The thought 
of generating key-item from itemset Cr is to find the subset of the itemset which impacts on the support most The thought of generating candidate itemset can be described a recycle as follow if the support of itemset doesn\222t pass the min-sup threshold, then the algorithm will cut off a key-item to increment the support as much as possible once the support satisfy the min-sup and now the itemset is the frequent itemset so that all subsets of a frequent itemset are also frequent Pay attention to that the key-item is maybe not sole and 
every itemset containing the key-item will possibly be frequent We will give some related concepts and definitions first and then introduce the key-item and candidate generation algorithm 2 Related concepts and definitions Definition 1 Information table of transaction-item mutuality Let I=[il i im be a itemset, D=\(U A he a transaction database where U={X,,XZ  xn A=T U C and C=[CI,C2  Cn C L I is the itemset T=\(tl.t2  t is the set of transaction identifiersvm 
So for every object X,EU there is a unique TID and a itemset In order to describe this relationship between transaction and itemset exactly we define S   U, A, V f  as information table of transaction-item mutuality \(information table for short and V={O,l TXC-V Example l Suppose we have a transaction database as depicted in Tablel then the above information table of transaction-item mutuality is shown as Table 2 Table 1 Transaction database 0-7803-7508-4/02/$17.00 02002 IEEE 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 Table 2 Information table of transaction-item mutuality TID IA IB IC ID IE TI T4 10 II 10 10 II Comparing infomation table with transaction database  It is clearly that the former is the extension of the latter, and the latter is the subset of the former So all of the information on the transaction and itemset can been showed in the table For example the database has the same TID with table Every record of the database form a row in the table The itemsets of the database are consistent with the attributes of the table and the itemsets of the database denoted by T accords with the attributes set of T row in the table But we should notice that there are two values for every attribute in the table I means there exists the item in the corresponding row of database  while 0 means not In additional to the same TID the total of 1 in every row of the table is equal to the length of the itemset in the record of database; and the total of 1 in each column is the support of the corresponding item in database In table it is significant whether or not the value of the attribute is 1 but as for counting support  only those items that have 1 are used Now if we have a transaction database, then there must be a corresponding information table, the reverse is true So we look the transaction database same as its information table except for statement especially On second thoughts we can regard the discemihility matrix of the information table as the discemihility of the database reduct of the table as database 221s reduct and table\222s core as database 221s core Definition 2 The projection of the database Given a itemset in the database its items and the corresponding transactions compose a subset of the origin database We regard this procedure as projecting of the database, and the subset of the database is called the projection database on the itemset Defiition 3 Core and reduct of the itemset There exists a projection database if given a itemset of origin database so the core and reduct of the project is that of the itemset The length of the itemset decides the number of information table\222s attributes Definition 4 Key-item sets If the itemset of the database is candidate frequent itemset so the core and reduct of this itemset is the key-item set of this candidate frequent itemset Detinition 5 Reduct database  Let X is the given itemset and Y is the reduct of X then the transaction set T of containing items of X and the elements of X compose the projection A of the origin database and the elements of reduct Y and corresponding T compose the projection of A called reduct database of the origin database There are the same transactions both in the reduct database and projection database A Every itemset of the reduct database is the subset of that of the projection database while every itemset of the projection database is superset of that of the reduct database The property nf the key-item set Assume that X is a itemset in the projection database of transaction database projected and its support of X is support\(X Y is reduct of X then the support of Y is support\(Y Proof There are two case as follow, after a transaction database is projected 1 Any itemset in projection database is not same each other X correspond to unique a transaction, Let Ti be its identifier, then the support of X is support X  1 Due to Y be a subset of X so that support CY support X  1 Assume that the support of Y is support Y support X  then there exists itemsets Xj correspond to Ti here Xi contain Y but does not contain X this means that Y can not discern Ti and Tj so that Y is not reduct, this is contrary to that Y is reduct known hence support X support Y 2 Itemset X repeat K times in the projection datahase. removing K-1 repeat from the database, Reduct of itemset is computed as above support  X  support CY  Then the K-l repeats is considered Because Y is subset of X Every transaction of containing X has Y support CY  support X K hence support CY  support XI m Deduction 1 If the support of given itemset X is m denoted as support\(X\and Y is the reduct of X EX-Y then suppolt\(X  support\(Z specially support\(X Z is hue when Z is also the reduct of X Deduction 2 If the support of given itemset X is m denoted as support\(X and Y is the reduct of X Y d[lCX XZG but Y is not the subset of X2 then support\(X\<support\(Z 3 The structure of candidate itemset and reducts of itemset 3.1 The structure and generating algorithm for candidate itemset According to the property of itemset reduct and deduction as above we have generating algorithm for candidate itemset as follow Suppose a itemset in database is X=\(xl  xm I maximum frequent itemset x\222\(initiating set is empty Input The itemset given X=\(xl  xm maximum frequent itemset x\222 Output The C set of containing s candida itemset with length m-l and maximum frequent itemset X  51 6 


Proceedings of the First lnternational Conference on Machine Lrarning and Cybernetics Beijing 45 November 2002 A I g n ri th m Gen-Candidate\(X X',C  Q 4 Z=X-Q Ql=gen-keyitems\(Z generating key itemset QI from itemset X here Q1=\(ql  while \(support Z  minsupport and Z 4 el Q Q U Q1 key-itemset length is s EX-Q l<s<mo endwhile C 4 S=IQI If Zisfrequent then x'=x'uz endif for i=l to s Yi=X-[qi generating the combination of items with length being m-1 If yi is not in X then generating candidate frequent items C=CU Yi 1 else Yi 4 avoiding repeat calculating  endif endfor EndGen-Canditae 3.2 Reducts of the infnnnation table As discussed above, the key-item itemsets are the core and reducts of the itemsets It is a NP problem to find all of the reducts of the given information system and it is insignificant for computing the frequent itemsets We only need to find one set of the reducts in order to determine the frequent itemset and it is feasible in fact There are two methods to find reducts and core of a given set of transactions one is Discemibility Mahix by which we can get the core of itemset easily and then compute all of the reducts by logic operating  the other method is to find a specifically reduct by with of heuristic information which uses the definition of the reduct The latter is efficient as well as simple and convenient For example, Xiaohua Hu's algorithm for finding a specifically reduct[8 Its complexiry about is M X O\(N X N where M is the number of item N is the number of tuples in the relation generalized by the database, the maximum of N is N X M N is the number of records in database It is more simple as for the information table system. We can get a more efficient algorithm if we improve the reduct algorithm by using the definition of reduct and the property of information table A reduct is the essential part of a information system which can discern all objects discernible by the original information system A core is the common parts of all the reducts The exact definition of reduct is as follows Let T=\(U,C,D he information system and U=\(xI.x 2...~m is a non-empty finite set of objects. D is set of decision attributes, while the elements of C are called the condition attributes then the C-positive region of D is defined as POSc\(D U CX  where CX  IND\(c and XE U/D That is to say POSc\(D U X  where XE U/lND\(C\and X CY  E U/D Let CEC amihute c be dispensable in T if POS c D POSc\(D otherwise attribute c is indispensable in T T U, C D is independent if all CEC are indispensable in T If the set of attributes R E C T  U R D is independent and POSR\(D  POSc\(D then the set of attributes R is called a reduct of C; The intersection of all reducts of C is called core of C In information system due to U/D=\([t tz   POSc\(D is the union of the X where XE U/TND\(C and X=\(ti here U/IND\(C\is the quotient set of U dived by IND\(C So set POSc\(D is the union of set contained only an element in the indiscriminate class of condition attribute C In other words Set POSc\(D constitute of transaction identifier according to items which is not repeat in transaction database In a other words Set POSc\(D is made up of the transaction identifier according to items with support 1  So POSc\(D\is calculated by computing the non-repeat items in the database Hence we present discrimination vector to compute the number of non-repeating itemsets in database in order to save memory Definition 6 Discrimination vector Given a transaction database D  U A and U  XI x2 __ xm A  T/C  C=[CI  CJ Ci is a set of transaction T Itl tz  tm is a set of transaction identifiers. Suppose the transaction itemset Cl iliz  i Yiconsisting of all itemsets in Ci that are different with the itemsets in C1 then Y=CIY  Y is the discrimination vector \(DV for short\of the database If we limit Yi G CI then the vector Y is the discrimination vector of C1 Property of DV  Discrimination Vector  1 We can compute Ci by using Cl,Yi and then get the discrimination itemsets between any two itemsets 2 If Yi or Y,=Yj then Ci=C1 or Ci=Ck Basing on the property of discrimination vector we can compute POSc\(D K\(C,D SGF\(a,R,D\easily K\(C,D  I POSc\(D represents the dependency between C and D SGF\(a,R,D  K\(RU a D  K\(R,D\is the significant value of the itemset I POSc\(D is the number of those nonempty and indifferent items in the discrimination vector Y comparing with C 51 7 


Proceedingi of the First International Conference on Machine Laming and Cybernetics Beijing 4-5 November 2002 I 51 8  Suppose Yic Y is the itemset in discrimination vector and Si=YinC where Si and SifSj then Y=IPOSc\(D is the number of Si in the vector K\(C,D POSc\(D for convenience give more details Because IUI is a const in this procedure we can set The method for reduct of itemset see 81 here do not 4 Miningalgorithm 4.1 Primary algorithm Assume that transaction database has N records each record has two.parts the transaction identifiers and the itemset, every the transaction identifiers is unique.Tk denote the k-th transaction L denote frequent itemset The top-down method for mining frequent from database op-Down-Miner L For k=l to N X=Tk.C  X=the candidate itemset in transaction identifiers Tr Del-minItems\(X remove the items with support less than min-support from X If L=Q then Gen-Candidatel\(X L C Generating //candidate frequent itemset C and frequent itemset L from itemset Else endif Endfor out L The algorithm for generating frequent itemset Gen-Candidate\(X L C Gen-frequent\(C L EndTop-Down-Miner Gen-frequent\(C L Nl=ICI For i=l to NI If support\(Ci then Gen-Candidate\(Ci,L,Y candidate frequent itemset from the key-itemset Gen-frequent\(Y,L call Gen-frequent\(Y L Else L=Luci endif Endfor EndGen-frequent This is a recursion algorithm for describing principle  when all the candidate frequent generated are frequent or without any candidate frequent generated the algorithm finish 5 Dseussion of the complexity and efficiency Top-down-Minfrequent algorithm provides an efficient way to generate maximum frequent itemsets which efficiency can be demonstrated by analyzing its worst case time complexity Suppose there are N records in the database the time complexity in the worst case is analyzed as follows For each itemset of record  it generates frequent itemsets once so the total times is N suppose the average length of itemset is M the average computing length of reduct is S and S candidate itemsets whose length is M-l generated each time it needs 1/2MZ for computing candidate itemsets whose length are M M-I  2 1 __  and the time spend on each is at most O\(2\222 So the sum is U2M2  O\(2\222 Since it takes M X O\(N X N\222 to generate reducts each time\(seen 121  the total computing time is about MXO\(N XN\221 1/2 2S O\(2\222 X N\222 here N is the number of tuples in the relation generalized by the database In fact M is dynamic and degressive and the average computing length of reduct\(S is also dynamic and degressive because it changes according to M in general 1  S  M S=M could be Vue only when all generated candidates are reducts no matter how long the length of candidate itemsets is And now the complexity is about 0.5 X M3NX O\(2\222X NZ  The frequent itemsets must be very short because M keeps down to 1 or 2 but it is impossible for mining long frequent itemsets since long frequent itemsets will pass the min-supp threshold when M is large That is to say, it will end far away O\(2\222 Additionally, the computing complexity can be depressed because we can prune the unnecessary branches during the mining procedure Hence the computing complexity for mining long frequent itemsets should be 0.5 X M3N X O 9 S,M X N2  where 2211 S M is a variable accounting to S and M The computing complexity approach to that of polynomial times In most cases the average computing length of reduct S will shorter after erasing the items whose support is smaller than the given min-supp The more the frequent itemsets are determined, the faster the speed will he since the algorithm can prune in time by comparing the known frequent itemsets with candidate itemsets when candidate itemsets already include the frequents Especially it will be a good idea to adopt Apriori approach if S is near to M and M is very large and there are scarcely long frequent itemsets At last the mined maximum frequent itemset is complete since we get frequent itemsets by determining each element of the candidate itemsets ,and each candidate itemsets scan the database using the discrimination vector 6 Conclusions As shown above, in this paper we proposed a top-down mining method for mining long frequent itemsets Our algorithm combined two different mining methods, one for mining association rules, the other for mining classification rules  we think it will be a good idea to combine two or more kinds of methods What\222s more it is still a primary 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 algorithm so there are many issues that should be studied in future to improve it The following are some topic for future research as examples using Apriori algorithm in proper time self-adjusting mining strategy accounting to the length of reduct, how to make the best use of the former result how to get the least number of reducts, developing incremental mining algorithm based on top-town algorithm and so on References Agrawal R Imielinski T Swami A CIMining Association Rules between Sets of Items in Large Databases In Proceedings of ACM SIGMOD Conference on Management of Data,1993,207-216 Agrawal R Srikant R Fast Algorithms for Mining Association Rules In Proceedings of 20 Int'l Conference on Very Large Databases, 1994.478 Lin D.-I and Kedem Z.M Pincer-Search A New Algorithm for Discovering the Maximum Freauent Set C]In Proc Of the Sixth European Conf on Extending database technology 1998 Roberto J Bayardo Jr Efficiently Mininr long patterns from In Proc of the 1998 ACM-SIGMOD Int Conf on Management of Data SIGMOD98 Qinghua Zou Henry Chiu Wesley W Chu Using patterns decomposition methods for finding all frequent Datterns in large datasets,[Rl UCLA CS-TR 200038 awlak Z Rough Sets Theoretical Aspects of Reasoning about Data MI Kluwer Academic Publishers,1991 J Han and J Pei Minine Freauent Patterns bv Pattern Growth Methodology and Implications JIACM SIGKDD Explorations Special Issue on Scaleble Data Mining Algorithms 2\(2 December 2000 Hu Xiaohua Knowledge discovery in database an attribute-oriented rough set approach Dissertation Regina. 1995 Skowron A and Rauszer C The discernibility matrices and functions in information systems Technical Report Research Report, ICSWUT, Warsaw 1991  _ IO Wang Xiaofeng,Yin Danna Shihchuan Cheng Mutuality Sets J Journal of Shenyang Institute of Chemical Technology,l999.3, 67-76 I 11 Wang Xiaofeng,Yin Danna Shihchuan Cheng Mutuality Sets And Its Applications in Reduct of Knowledge System Jl Journal of Tsinghua Unive&ty\(in Chinese 1998.7 No.S2,6-9 I21 Wang Xiaofeng Wang Tianran Mutuality Measure and Computation of Incremental Support and Confidence J Journal of Software in Chinese 2001 To appear  519 


because the use of the same system makes the comparisons more impartial Due to space restrictions we only include the results of the unordered algorithms The results of the CN2-SD algorithm were computed using both the multi plicative weights with y  0.5 0.7 0.9 and the additive weights Results withy  0.7 are not listed as they are al ways between those of 7  0.5 and y  0.9 as expected All other parameters of the CN2 algorithm were set to their de fault values bean-size  5 significance-threshold  99 Table 2 Area under the ROC curve with stan dard deviation AUC  sd for different vari ants of the unordered algorithm using 10-fold stratified cross-validation 3.5 and on CN2-WRAcc with a factor of 2 Note however that rules obtained with additive weights and multiplicative weights with high y are highly overlapping due to the rela tively modest decrease of example weights In addition there is also a substantial increase in the av erage likelihood ratio while the ratios achieved by CNZ standard are already significant at the 99 level this is fur ther pushed up by CNZ-SD with maximum values achieved by additive weights An interesting question to be verified with further experiments is whether the weighted versions of the CN2 algorithm improve the significance of the in duced subgroups also in the case when.CN2 rules are in duced without applying the significance test In summary CNZ-SD produces substantially smaller rule sets, where individual rules have higher coverage and sig nificance Table 3 Average size 9 coverage CVC and llkelihood ratio LHR of rules for different versions of the unordered algorithm induced from the entire data sets We also compared the sizes of the rule sets, average rule coverage, and the likelihd ratio of rules computed from the entire data sets not using cross-validation Table 3 compares CNZ-SD with CN2-standard and CN2-WRAcc in terms of the size of the rule set S is the number of rules in a rule set, including the default rule\average rule coverage CVG is computed as the averaged percentage of covered positive and negative examples per rule and likelihood ra tio\222 per rule The experimental results show that CN2-SD achieves im provements across the board In terms of AUC, the smallest improvement is achieved by additive weights and slightly better improvements of 34 are by multiplicative weights On the other hand additive weights result in ahout 2 times less rules on average than multiplicative weights and 6.5 times less rules than CNZ-standard Average rule coverage is also optimal for additive weights, improving on the av erage the coverage of CN2-standard rules with a factor of 222The likelihood ralio is used in CNZ for testing the significance of Ihs induced tule 141 For Iwo-class problems this stalislic is distributed ap prorimalely as xz wilh one degree of freedom Finally we illustrate our approach in the ROC space by means of the results on the Australian data set Fig ure 1 The solid lines in this graph indicate the ROC curves obtained by CN2-SD and CN2-standard evaluated with AUC-Method-2 i.e probabilistic classification with overlapping tules the top line \(squares for CN2-SD with additive weights, and the bottom line \(triangles for CNZ standard CN2-standard finds many more rules than CN2 SD which leads to overfitting as the ROC curve is mostly below the diagonal For illustrative purposes we also include positive and negative convex hulls constructed from individual sub groups using AUC-Method-l dotted lines The points on the X and Y-axes close to the origin are all small purely positive and negative subgroups found by CN2-standard that do not contribute to the convex hull \(presumably these are the rules that lead to poor performance using probabilis tic classification Using AUC-Method-l we can remove 271 


those overly specific subgroups leading to reasonable posi tive and negative convex hulls Notice, however that CNZ SD still improves on CN2-standard after removing redun dant subgroups Figure 1 Example ROC curves on the Aus tralian data set: solid curvesfor AUC-Method 2 and dotted positive and negative convex hulls for AUC-Method-1 squares for CNZ-SD with additive weights and triangles for CN2 standard 5 Related work Various rule evaluation measures and heuristics have been studied for subgroup discovery IO 241 aimed at bal ancing the size of a group referred to as factor g with its distributional unusualness \(referred to as factor p The properties of functions that combine these two factors have been extensively studied the so-called 221p-g-space\221 lo An alternative measure q   was proposed in 13 for expert-guided subgroup discovery in the TPIFP space aimed at minimizing the number of false positives FP and maximizing true positives TP guided by generalization pa rameter par Besides such 221objective\222 measures of interest ingness some 221subjective\222 measure of interestingness of a discovered pattern can be taken into the account such as ac tionability 221a pattern is interesting if the user can do some thing with it to his or her advantage\222 and unexpectedness 221a pattern is interesting to the user if it is surprising to the user\222 211 Instance weights play an important role in boosting I21 and alternating decision trees 20 Instance weights have been used also in variants of the covering algorithm imple mented in rule learning approaches such as SLIPPER 6 RL 1151 and DAIRY 191 A variant of the weighted cover ing algorithm has been used also in the context of subgroup discovery for rule subset selection 13 6 Conclusions We have presented a novel approach to adapting stan dard classification rule learning to subgroup discovery To this end we have appropriately adapted the covering algo rithm the search heuristics, the probabilistic classification and the performance measure Experimental results on 17 UCI data sets demonstrate that CNZ-SD produces substan tially smaller rule sets where individual rules have higher coverage and significance These three factors are important for subgroup discovery smaller size enables better under standing. higher coverage means larger support, and higher significance means that rules describe discovered subgroups that are significantly different from the entire population We have evaluated the results of CN2-SD also in terms of AUC-Method-2 and shown insignificant increase in terms of the area under the ROC curve In further work we will evaluate the results also by using AUC-Method-1 where each subgroup establishes a sepa rate point in the ROC space and compare the results with the MIDOS subgroup discovery algorithm We plan to in vestigate the behavior of CN2-SD also in multi-class prob lems An interesting question to he verified with further ex periments, is whether the weighted versions of the CN2 al gorithm improve the significance of the induced subgroups also in the case when CN2 rules are induced without ap plying the significance test Finally we plan to use the CNZ-SD subgroup discovery algorithm for solving practical problems in which expert evaluations of induced subgroup descriptions is of ultimate interest Acknowledgements Thanks to Dragan Gamberger for joint work on the weighted covering algorithm and JosC Hernbdez-Orallo and Cesar Ferr-Ramirez for joint work on AUC The work reported in this paper was supported by the Slovenian Min istry of Education Science and Sport the IST-1999-11495 project Data Mining and Decision Support for Business Competitiveness A European Virtual Enterprise and the British Council project Partnership in Science PSP-IS Refer en c e s l R Agrawal H Mannila R Srikant H Toivonen, and A.I Verkamo Fast discovery of association rules In U.M Fayyad G Piatetski-Shapiro P Smyth and R Uthurusamy editors, Advances in Knowledge Discov ery and Data Mining 307-328 AAA1 Press 1996 2 B Cestnik Estimating probabilities: A crucial task in machine learning In L Aiello editor Proc of rhe 9rh European Conference on Artificial Inrelligence 147 149. Pitman 1990 272 


3 P Clark and R. Boswell Rule induction with CNZ Some recent improvements. In Y Kodratoff editor Pmc of rhe 5rh European Working Session on Learn ing 151-163 Springer, 1991 141 P Clark and T Niblett The CNZ induction algorithm Machine Learning 3\(4 261-283 1989 5 W.W Cohen 1995 Fast effective rule induction. In Pmc of rhe 12th Intermrional Conference on Ma chine Learning 115-123 Morgan Kaufmann, 1995 161 W.W Cohen and Y Singer A simple fast and ef fective rule learner In Pmc of AAAI/lAAI 335 342. American Association for Artificial Intelligence 1999 171 S Dieroski B. Cestnik, and 1 Petrovski. \(1993 Us ing the m-estimate in rule induction Journal of Compuring andlnformarion Technology 1\(1 46 1993 8 U.M Fayyad and K.B Irani K.B Multi-interval dis cretisation of continuous-valued attributes for classi fication learning In R Bajcsy editor Pmc of the 13th Inrernarioml Joint Conference on Artificial In telligence 1022-1027 Morgan Kaufmann 1993 19 D Hsu 0 Etzioni and S Soderland A redundant cov ering algorithm applied to text classification In Pmc of the AAA1 Workshop on Learning from Tar Cate gorization American Association for Artificial Intel ligence, 1998 IO W Klosgen. Explora A multipattern and multistrat egy discovery assistant In U.M Fayyad G Piatetski Shapiro P Smyth and R Uthurusamy editors Ad vances in Knowledge Discovery and Data Mining 249-271 MITPress 1996 1111 C Fem-Ramirez P.A Flach and 1 Hemandez Orallo. Learning decision trees using the area under the ROC curve In Pmc of the 19th Internarional Conference on Machine Learning 139-146 Morgan Kaufmann 2002 I21 Y Freund and R.E Shapire. Experiments with a new boosting algorithm In Pmc of the 13th International Conference on Machine Learning 148-156 Morgan Kaufmann 1996 13 D Gamberger and N LavraE Descriptive induction through subgroup discovery A case study in a medi cal domain In Pmc of the 19th Internarional Confer ence on Machine Learning 163-170 Morgan Kauf mann 2002 I41 N Lavraf P Flach and B Zupan Rule evaluation measures A unifying view In Pmc of the 9th Inter national Workshop on Inducrive Logic Pmgramming 74-185 Springer 1999 I51 Y Lee B.G. Buchanan, and J.M. Aronis Knowledge based learning in exploratory science Learning rules to predict rodent carcinogenicity Machine Learning 30 217-240 1998 I61 R.S Michalski 1 Mozeti I Hong and N LavraE The multi-purpose incremental learning system AQl5 and its testing application on three medical domains In Pmc 5th National Conference on Artificial Inrelli gence 104-1045 Morgan Kaufmann, 1986 I71 P.M Murphy and D.W Aha UCI repos itory of chine learning databases http://www.ics.uci edurmleamiMLRepository.html Irvine CA University of California Department of Information and Computer Science 1994 I81 F Provost andT Fawcett Robust classification forim precise environments Machine Learning 42\(3 203 231,2001 I91 R.L Rivest Learning decision lists Machine Learn ing 2\(3 229-246 1987 201 R.E Schapire and Y Singer Improved boosting algo rithms using confidence-rated predictions In Pmc of the 11th Conference on Computational Learning The ory 80-91 ACM Press 1998  A Silbenchatz and A Tuehilin On subjective mea sures of interestingness in knowledge discovery In Pmc of the Isr Internarional Conference on Knowl edge Discovery and Data Mining 275-281 1995 1221 L Todorovski P Flach and N Lavraf Predictive performance of weighted relative accuracy In D.A Zighed 1 Komorowski, and J Zytkow, editors Pmc of the 4rh European Conference on Principles of Data Mining and Knowledge Discovery 255-264 Springer 2000 231 I.H. Witten and E Frank Data Mining: PracricalMa chine Learning Tools and Techniques wirh Java Imple mentations Morgan Kaufmann 1999 24 S Wrobel An algorithm for multi-relational discov ery of subgroups In Pmc of the 1st European Sym posium on Principles of Data Mining and Knowledge Discovery 78-87 Springer. 1997 273 


association-cube, base-cube and population-cube are derived from the volume cube; the confidence-cube is derived from the association cube and population cube and the support-cube is derived from the associationcube and base-cube. The slices of these cubes shown in Figure 2 correspond to the same list of values in dimension merchant, time, area and customer_group  Multidimensional and multilevel rules Representing association rules by cubes and underlying cubes by hierarchical dimensions, naturally supports multidimensional and multilevel rules. Also these rules are well organized and can be easily queried  First, the cells of an association cube with different dimension values are related to association rule instances in different scopes. In the association cube CrossSales cell CrossSales product \221A\222, product2 \221B\222  customer_group 221engineer\222, merchant \221Sears\222, area \221Los Angeles\222, time 221Jan98\222 represents the following multidimensional rule x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x,\221B\222  275 customer_group = \221engineer\222, merchant = \221Sears\222, area 221Los  Angeles\222, time =  \221Jan98\222 If this cell has value 4500, and the corresponding cell in the population cube has value 10000, then this rule has confidence 0.45 Next as the cubes representing rules can have hierarchical dimensions, they represent not only multidimensional but also multi-level association rules. For example, the following cells CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221Jan98\222 CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221 Year98 222 represent association rules at different area levels \(i.e the city level and the state level\d different time levels \(i.e., the month level, the year level x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221Jan98\222 x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221 Year98 222 The cell CrossSales\(product \221A\222, product2 \221B\222,  customer_group 221top\222, merchant \221top\222, area \221top\222,  time \221top\222 represents the customer-based cross-sale association rule for all customers, merchants, areas, and times in the given range of these dimensions, expressed as x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222 4.3  Generating Association Rule Related Cubes The basic task of our OLAP based association rule mining framework, either at the GDOS or at an LDOS is to convert a volume cube i.e. the cube representing the purchase volumes of customers dimensioned by product  area etc, into an association cube a base cube and a population cube These cubes are then used to derive the confidence cube and the support cube of multidimensional association rule instances. The following general steps are involved in cross-sale association rule mining 267  Roll up the volume cube SaleUnits by aggregating it along merchant, time, area dimensions 267  Derive cube NumOfBuyers from SaleUnits based on the antecedent condition SaleUnits 0 267  Populate cube NumOfShoppers by the counts of customers dimensioned by merchant, area  time not by product\at satisfy the antecedent conditions 267  Derive cube CrossSales from SaleUnits based on the association conditions SaleUnits  product  p 1  0 and SaleUnits  product2  p 2 0 267  Derive cube Confidence and cube Support using cell-wise operations 214  Confidence = CrossSales  NumOfBuyers 214  Support  CrossSales  NumOfShoppers  Cubes Confidence  Support  CrossSales are dimensioned by product  product2 customer_group,merchant  time, area NumOfBuyers is dimensioned by product  customer_group, merchant time, area  NumOfShoppers is dimensioned by customer_group, merchant  time, area Rules with confidence and support that exceed specified thresholds  may be considered interesting 4.4. Rules with Conjoint Items Cubes with conjoint dimensions can be used to represent refined multidimensional association rules For example, using OLAP, we can derive association rules across time  Time-variant or temporal association rules such as 


x 316 Customers buy_product\(x,\222 A\222, \221 Jan98\222  336 buy _product\(x, \221B\222, \221 Feb98\222   275 area = \221Los Angeles\222 can be used to answer such questions as \223 How are  the sales of B in Feb98  associated with the sales of A in Jan98 224 The items in this rule are value pairs of dimensions product and time In order to specify this kind of association rule we introduce a conjoint dimension product, time and mirror it with dimension product2, time2 This allows a cell in the association cube to cross two time values. Accordingly, the cubes related to association rule mining are defined as follows Association cube  CrossSales.2 \(<product, time>, <product2, time2 customer_group, merchant, area  Population cube  NumOfBuyers.2  \(<product, time>, customer_group merchant, area Base cube  NumOfShoppers.2  \( customer_group, merchant, area Confidence cube Confidence.2 \(<product, time>, <product2, time2 customer_group, merchant, area Support  cube  Support.2  product, time>, <product2, time2 customer_group, merchant, area  The steps for generating these cubes are similar to the ones described before. The major differences are that a cell is dimensioned by, besides others product, time and product2, time2 and the template of the association condition is  SaleUnit s  product p 1 time t 1  0 and  SaleUnits  product2 p 2 time2 t 2  0 where, in any instance of this condition, the time expressed by the value of time2 is not contained in the time expressed by the value of time The template of the antecedent condition is SaleUnits   product p 1 time t 1  0 In general, other dimensions such as area may be added to the conjoint dimensions to specify more refined rules 4.5. Functional Association Rules A multidimensional association rule is functional if its predicates include variables, and the variables in the consequent are functions of those in the antecedent.  For example, functional association rules can be used to answer the following questions, where a_month and a_year are variables q  What is the percentage of people in California who buy a printer in the next month after they bought a PC x 316 Customer buy_product\(x, \221PC\222, a_ month 336 buy_product\(x, \221printer\222, a_month+1  275 area = \221California\222 q  What is the percentage of people who buy a printer within the year when they bought a PC  x 316 Customer: buy_product\(x, \221PC\222, a_ year 336 buy_product\(x, \221printer\222, a_year 275 area = \221California\222 To be distinct, we call the association rules that are not functional as instance association rules; e.g x 316 Customer: buy_product\(x,\222 PC\222, \221Jan98\222 336 buy_product\(x,\222 printer\222, \221Feb98\222  275 area =  \221California\222 Time variant, functional association rules can be derived from time variant, instance association rules through cube restructuring. Let us introduce a new dimension time_delta that has values one_day, two_day 205, at the day level, and values one_month, two_month, \205, at the month level, etc. Then, let us consider the following functional association rule related cubes Association cube  CrossSales.3 \(product, product2, customer_group merchant, area, time_delta  Population cube  NumOfBuyers.3 \(product, customer_group, merchant area Base cube  NumOfShoppers.3 \( customer_group, merchant, area Confidence cube  Confidence.3 \(product, product2, customer_group merchant, area, time_delta Support cube  Support.3 \(product, product2, customer_group, merchant area, time_delta The association cube CrossSales.3  can be constructed from CrossSales.2   The cell values of CrossSales.2  in the selected time and time2 ranges are added to the corresponding cells of CrossSales.3 For example, the count value in cell  CrossSales.2\(<PC, Jan98>, <printer, Feb98>\205 is added to cell \(bin CrossSales.3\(PC, printer, one_month,\205 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


