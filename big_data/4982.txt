Programming ecological niche modeling workîows in the Cloud Daniele Lezzi 
Department of Computer Sciences Barcelona Supercomputing Center Barcelona Spain Email daniele.lezzi roger.rafanell rosa.m.badia 
 Roger Rafanell  Erik Torres  Renato De Giovanni  Ignacio Blanquer and Rosa M Badia  
 
002 002    002  002 
 
bsc.es Artiìcial Intelligence Research Institute IIIA Spanish National Research Council CSIC Instituto de Instrumentacion para Imagen Molecular I3M Centro mixto CSIC Universitat Politecnica de Valencia CIEMAT Valencia Spain Email ertorser@upv.es iblanque@i3m.upv.es Centro de Referencia em Informaao Ambiental Campinas SP Brasil Email renato@cria.org.br 
In the last decades biology scientists have relied on their own resources and tools to run the experiments and store the results of the analysis However the explosion of big data and the growing availability of computational methods nd 
   
Abstract 
an obstacle in the lack of computational and storage resources Cloud computing platforms are emerging as potential solution to overcome these limitations but adaptation of the applications to enable scientiìc users to beneìt from resources acquired on demand is a complex process requiring multidisciplinary expertise The EUBrazilOpenBio initiative is implementing an eInfrastructure that provides biodiversity community with a rich set of computational and data resources exploiting existing cloud technologies from EU and Brazil This paper presents the implementation of one of the two use cases selected the environmental niche modeling by means of implementing such 
workîow through the COMPSs framework and its deployment on the EUBrazil OpenBio platform The proposed approach has been evaluated on a Cloud testbed managed by the VENUS-C middleware 
Cloud niche modeling programming models workîows 
I I NTRODUCTION Cloud computing has emerged as suitable model for the provisioning of resources to those scientiìc communities that are typically excluded by the access to supercomputing infrastructures or whose experiments require a variable demand of resources thus enabling a reduction of maintenance costs Recently several initiatives have proposed frameworks and 
Index Terms 
services to foster the uptake of clouds for the execution of scientiìc applications resulting in the creation of service oriented components following different models IaaS PaaS SaaS The VENUS-C project implemented a user centric approach to the cloud putting the requirements of end-user communities at the forefront of development and providing scalable and interoperable cloud resources that combine both open source and commercial solutions to offer the best of both worlds VENUS-C provided the so called long tail of science with a generic cloud platform suitable for the execution of applications spanning diverse disciplines covering an existing user community of above 5.000 users 
The VENUS-C model has been adopted by the EUBrazilOpenBio initiative as one of the building blocks of the computational and data infrastructure designed to serve the needs and requirements of the biodiversity scientiìc community The EUBrazilOpenBio e-Infrastructure is built by leveraging primarily on resources textual publications and datasets maps taxonomies tools services computing and storage capabilities provided by Brazilian and European eInfrastructures sustained by existing projects and initiatives In particular the programming models layer is an important contribution of the VENUS-C project to the scientiìc community In conjunction with data access mechanisms the pro 
gramming frameworks developed in VENUS-C have proven to provide researchers with a suitable abstraction for scientiìc computing on top of virtualized resources One of these tools is COMP Superscalar le v eraged in VENUS-C to enable the interoperable execution of the use cases on the hybrid cloud platform The COMPSs programming framework allows the development of scientiìc applications and their seamless execution on a wide number of distributed infrastructures In cloud environments COMPSs provides scaling and elasticity features allowing to adapt the number of available resources to the actual need of the execution OpenModeller pro vides a e xible cross-platform en vi 
ronment to perform the main tasks related with ecological niche modelling The software includes facilities for reading species occurrence and environmental data in different formats as well as creating testing and projecting models into multiple scenarios Many algorithms are provided as plugins allowing models to be generated using different techniques A number of interfaces are also available including console command-line GUI and Web Services This paper describes the design and implementation of an openModeller workîow through the COMPSs framework and a performance evaluation on the EUBrazilOpenBio infrastruc 
2013 27th International Conference on Advanced Information Networking and Applications Workshops 978-0-7695-4952-1/13 $26.00 © 2013 IEEE DOI 10.1109/WAINA.2013.6 1223 


ture The rest of the paper is structured as follows section II brieîy describes the EuBrazil Architecture section III contains the description of the COMPSs framework and of the tools to enact the execution of applications section IV illustrates the implementation of the niche modeling workîow with COMPSs section V analyzes the performance of the ported application and section VI concludes the paper II E COLOGICAL N ICHE M ODELLING Ecological Niche Modelling ENM is a widely used approach to predict and to understand the distribution of species An ecological niche can be seen as the set of ecological requirements for a certain species to survive and maintain viable populations over the time In most cases ENMs are generated by relating locations where the species is known to occur with environmental variables that may inîuence its distribution and then applying an algorithm to create the model This method is known as correlative approach and the resulting model basically tries to nd a representation of the environmental conditions that are suitable for the species Such models can be projected into different geographical regions under different environmental scenarios making it possible to predict the impact of climate changes on biodiversity prevent the spread of invasive species identify geographical and ecological aspects of disease transmission help in conservation planning guide eld surveys among many other uses Practical problems in applying ENM are associated with intensive computational requirements when models need to be generated for a large number of species using complex modelling strategies involving several algorithms and high-resolution environmental data In the EUBrazilOpenBio project the second use case is related with the Brazilian Virtual Herbarium BVH of Flora and Fungi which has its own system capable of interacting with an openModeller Web Service OMWS instance to carry out a standard niche modelling procedure for many plant species that are native to Brazil Species that can be modelled by the system come from the ofìcial List of Species of the Brazilian Flora which currently contains 43.284 entries The corresponding occurrence points are retrieved from speciesLink a network that integrates data from distributed biological collections currently serving almost 4 million records considering only plant species For species with at least 20 occurrence points available the standard modelling procedure involves generating individual models with ve different techniques Ecological-Niche Factor Analysis GARP Best Subsets  Mahalanobis distance 12 Max ent 13 and One-class Support Vector Machines Besides generating the models a 10-fold cross-validation is performed to assess model quality and a nal step is needed to merge the individual models into a single model which is then projected into the present environmental conditions for Brazil The entire process is computing-intensive especially when dealing with multiple species Assuming that 30 of the Angiosperms which is just the group of owering plants will have enough data to generate models an initial estimate indicates that it would require more than 10 months to process all jobs using the computational resources currently available to the BVH a single server running with two Intel Xeon Six Core processors at 2.53GHz 32GB of memory and 1.7TB of storage Moreover the number of species occurrence points and modelling techniques is continuously changing requiring new models to be generated over the time III T HE E U B RAZIL O PEN B IO P LATFORM The main goal of the EUBrazilOpenBio Project is to aggregate disparate compute and data technologies into a coherent and integrated research environment for the biodiversity community Figure 1 depicts the prototype architecture for the implementation of the niche modeling scenario This architecture has been designed to simplify the access to the computing resources available in the EUBrazilOpenBio infrastructure The access to the computing resources is managed by the VENUS-C middleware through the Programming Model Enactment Service PMES able to schedule the jobs requests to virtual instances provided by VENUS-C and grid nodes provided by Condor the e x ecution of the openModeller workîows is orchestrated by the COMPSs runtime with the aim of optimizing the use of resources as explained in details in section IV-A The VENUS-C PMES receives the execution requests from a bridge component the ENM Service that has been designed as dispatcher of userês requests received from the Virtual Research Environment VRE portal The ENM Service exposes an extended OMWS interface OMWS in the picture to support multi-staging and multi-parametric experiments through COMPSs and openModeller These extensions are backwards compatible with the original OMWS speciìcation allowing legacy clients to be fully supported in the new implementation and therefore still able of submitting experiments to the execution resources without using the graphical user interface developed by the project The ENM Service provides a bridge between the VRE where the services of the infrastructure are provided and the OMWS-ext instances which can be outside of the VRE domain i.e in a different network or a service that is not part of the VRE Neither OMWS nor OMWS-ext provides user support For this reason the ENM Service provides additional operations for accounting the experiments submitted by users and also copies the results and the logs of the executions to the VRE in those cases that the execution resources are outside the VRE domain The VRE is implemented with gCube gCube a softw are framework designed to abstract over a variety of technologies belonging data process and resource management on top of Grid/Cloud enabled middleware Through gCube VREs groups of users have controlled access to distributed data services storage and computational resources integrated under a personalised interface 
consensus 
1224 


The openModeller XML Scheme deìnes the operations input parameters and output types of openModeller Each operation deìned with this scheme supports the execution of one simple action with openModeller for example create test or project a model When a user wants to perform several actions with the same dataset she has to submit each operation to openModeller separately For example to create ve models with the same species occurrence dataset using ve different modelling algorithms ve different requests are needed one per algorithm The same occurs for experiments that create models for different species occurrences using the same modelling algorithm In the case that the operations have dependences on one another for example creating a model and then use it to project a dataset of occurrences points the user is responsible for monitoring and retrieving the results of the operation that creates the model and also for including the serialized model as input parameter in the projection operation Another characteristic of the openModeller XML Scheme is that it doesnêt provide support for user sessions Instead every operation returns a ticket that the user has to store and to use with any other subsequent operation that depends on the results of the previous operation such as monitoring and retrieving the results of that operation Although biodiversity scientists often use openModeller to work with only one single model EUBrazilOpenBio is targeting the massive creation of models One of the representative use cases is to create models for an entire database of species occurrences with more than 10000 entries such as The Virtual Herbarium of Plants and Fungi of Brazil Additionally these models should be automatically tested in order to ensure a minimum level of model quality To this end this section presents an extension of the openModeller XML Scheme to deal with the execution of multi-staging and multi-parametric experiments through COMPSs The proposed extended openModeller format OM-ext provides four additional types to the basic types 1    and  The objective of these new types is to support the following experiment pipelines in addition to the legacy unique-stage experiments and a model and a model and a model and a model Any of these experiments will produce only one request and job id in the system reducing the information redundancy The second advantage over traditional OM is that these types support multi-parametric requests per experiment containing combinations of multiple species occurrences datasets and multiple algorithms deìnitions Along with the data types also an extension to the openModeller Web Service interface has been implemented to provide the new operations that support multi-staging multiparametric experiments through COMPSs The following methods are provided as extensions    and  OMWS-ext is backwards compatible with the original OMWS interface allowing the use of legacy methods such as  or  through legacy clients IV C OMPOSING A ENM WORKFLOW WITH COMPS S COMPSs is a programming framework that aims to ease the development of applications in distributed environments The unawareness of the execution environment is not the only interesting feature of COMPSs additionally the framework 1 The openModeller XML Scheme http://openmodeller.cria.org.br/xml/1.0/openModeller.xsd 
A Extension of the OMWS interface ExtendedExperimentRequest ExtendedExperimentStatus ExtendedExperimentResults ExtendedExperimentLogs submitExtendedExperiment getExtendedExperimentStatus extendedExperimentResults getExtendedExperimentLogs cancelExtendedExperiment createModel testModel projectModel 
Create test project Create test Create project Test project 
Fig 1 EUBrazilOpenBio Niche Modeling Architecture 
    
1225 


implements a task-based programming model and while applications are written following the sequential paradigm the runtime is able to detect data dependencies between tasks and exploit the inherent parallelism at task level The COMPSs framework has been recently extended in order to support the access to Cloud providers in order to provide scaling and elasticity features allowing to adapt the number of available resources to the actual need of the execution This is achieved through the use of connectors for IaaS offerings namely for Amazon EC2 OpenNeb ula 18 and for Open Cloud Computing Interface OCCI compliant middle w ares lik e OpenNebula and OpenStack A speciìc adaptor 21 allows COMPSs to beneìt from the Microsoft Azure Platformas-a-Service PaaS Interoperability is key in the development of the COMPSs framework In the context of the VENUS-C project COMPSs has been indeed enriched with the PMES service that eases the porting and execution of the applications on hybrid cloud infrastructures The PMES for COMPSs exposes a standard OGSA-BES compliant web service interf ace that pro vides interoperability with other execution services for Clusters and Grids In this w ay e-Science applications can be seamlessly executed on heterogeneous infrastructures without the need of developing speciìc adaptors The researcher uses a client to contact the PMES in order to submit the execution of a COMPSs application This request is expressed through a Job Submission Description Language OGF JSDL document containing the application name the input parameters and data references following the HPC-BP speciìcation The same client allows the user to interact with the cloud storage to upload a packaged version of his code such package contains the COMPSs binaries and conìguration les needed by the application In EUBrazilOpenBio this features enable the execution of the use cases integrating existing Brazilian grid compute and storage resources based on gLite Globus or Condor and European commercial and open source Cloud solutions provided by the VENUS-C As is described in section III-A the proposed extension of openModeller provides a way to automatically convert multistage  multi-parameter experiments into a set of single legacy operations supported by openModeller suite This is achieved by COMPSs which orchestrates the execution generating automatically the pipeline The ENM workîow is composed of the following operations converts a multi-stage and multi-parameter request into a set of single operation ones models the specie distribution from a chosen algorithm and a set of occurrence points coordinates tests and validates the model against a set of reference occurrence points projects the specie model into an environmental layer set layout generating the distribution map formats and colours the projected map into a viewable image When an document is received the method splits the experiment request into a set of single-operation requests species and algorithms available in the extended document having a single request per each operation algorithm and species The operations start computing each request thus blocking the following executions of the and operations which depend on the  The operation is also enqueued and started when the projected map is available coloring it with a provided palette and eventually converting it to a desired image format as depicted in the Fig 2 Fig 2 ENM COMPSs workîow of 1 specie and 2 algorithms V P ERFORMANCE EVALUATION In order to evaluate the performance of the implemented application a set of experiments has been conducted on a private cloud available at BSC and managed by Emotive Cloud  The cluster includes a total of 96 cores a v ailable in the following way 4 nodes with 12 Intel Xeon X5650 Six Core at 2.6GHz processors 24GB of memory and 2TB of storage each and 3 nodes with 16 AMD Opteron 6140 Eight Core at 2.6GHz processors 32GB of memory and 2TB of storage each The nodes are interconnected by a Gigabit Ethernet network and the storage is offered through a GlusterFS distributed le system running in a replica conìguration mode providing a total of 8TB of usable space On this testbed a total of 10 quad-core virtual instances with 2GB of memory and 1GB of disk space have been created running a Debian Squeeze Linux distribution Eight species of the genus  each one with more than 20 occurrence points were used to test the new architecture Models were generated using the following high resolution environmental layers from WorldClim mean diurnal temperature range maximum temperature of warmest month minimum temperature of coldest month precipitation of wettest quarter precipitation of driest quarter precipitation of warmest quarter precipitation of coldest quarter 
A Implementation of the ENM workîow ExtendedExperimentRequest Convert Model Test Project Model Translate Passiîora 
Convert Model Test Project Translate 
     
1226 


and altitude A simpliìed standard procedure consisting of model creation followed by an internal model test confusion matrix and ROC curve calculation with the same input points and a native model projection with the same environmental layers followed by a nal image transformation was used for each species with a set of three algorithms used by BVH executed with a different set of parameters The Brazilian territory served as a mask in all operations This scenario implied a total of 46 simultaneous single-operation requests The aim of these tests was to validate the ENM workîow COMPSs implementation evaluating the advantage of the elasticity features of the COMPSs runtime comparing the execution on a dynamically provided pool of virtual resources with a run on a pre-deployed and static virtual environment Grid like scenario Figure 3 depicts the evolution on number of virtual machines used along the execution highlighting how the runtime of COMPSs is sensible to the load produced by the tasks adapting the number of current resources to the tasks load Fig 3 ENM elasticity on Cloud resources After the initial scale-up phase the load remains constant until the end of the process is reached thus starting to free resources progressively This is not the case of the static execution scenario overlapped on the gure which despite being 18.4 faster than the dynamic approach is much more expensive from a cost point of view because of the continuous usage of idle resources Table V compiles the execution time and speedup of running the presented experiment on both conìgurations limiting the maximum number of virtual machines that the system could use As could be observed the speedup is moderate because the application does not offer a high degree of parallelism Figure 2 Despite this COMPSs reaches a good performance running on a on-demand provided environment with an average performance loss around the 9.6 with a mean serving time of the Cloud middleware of about 120 seconds per VM However the speedup is not dramatically penalized and the resources management is generally improved reducing the overall execution costs 
SVM ENVDIST and ENFA 
VMs Cores Cloud Grid Time Speedup Time Speedup 
1 4 02:00:21 1.00 01:46:9 1.00 2 8 01:00:47 1.98 00:53:23 1.97 4 16 00:33:52 3.55 00:31:06 3.38 8 32 00:25:16 4.76 00:18:03 5.82 10 40 00:23:57 5.02 00:19:32 5.38 TABLE I E XECUTION TIMES OF STATIC AND DYNAMIC APPROACH VI R ELATED WORK The niche modeling approach used here among other tools for biodiversity analysis belong to the wider eld of biodiversity informatics This eld is characterized by a big number of developments in several research projects including frameworks for the composition of workîows The Biodiversity World project de v eloped a W eb Services based Grid environment whose workîow capabilities are based on Triana  that pro vides a graphical interf ace to assemble tools resources and data Triana similarly to COMPSs is based on GAT to execute workîows on a variety of infrastructures An implementation of an openModeller workîow has been provided by the project The Kepler Workîow System is another framework supporting multiple models of computation suited to distinct types of analysis processing sensor data integrating differential equations etc and provides a graphical user interface and a runtime engine that can execute workîows either from within the graphical interface or from a command line Kepler workîows can leverage the computational power of grid technologies as well as take advantage of Keplerês native support for parallel processing The BioVel project  similarly to EuBrazilOpenBio is supporting research on biodiversity issues providing workîows services through the Taverna workîow management system a suite of tools used to design and execute scientiìc workîows and to aid in silico experimentation Taverna itself does not provide support for cloud execution of the workîows but it has been used as a service deployed on a cloud by several projects VII C ONCLUSIONS AND FUTURE WORK This paper analyzed the rst achievements of the EUBRazilOpenbio initiative on the support to the biodiversity community in order to provide the scientists with a computational and data infrastructure based on cloud technologies The project adopts different technologies developed in previous projects as the VENUS-C platform for the development and porting of applications on the cloud and the tools for the data infrastructure In the course of the project the EUBrazilOpenBio e-Infrastructure is being validated to serve relevant biodiversity scientiìc use cases coming by strategic Brazilian and European e-science projects and initiatives In this work the porting of one of the use case has been presented analyzing the implementation of a niche modeling workîow by means of the COMPSs programming framework using 
1227 


openModeller as modeling tool This implementation enables the automation of several operations used for producing testing and projecting the models Such composite applications are offered as a service through the availability of speciìc methods in an extended version of the OpenModeller Web Service Users are able on one hand to enhance the offered functionalities for model generation and on the other hand to outsource their execution to the VENUS-C Platform and its optimization thanks to the COMPSs runtime The COMPSs Programming Model Enactment Service acts as a bridge to different infrastructures like grids and clouds These advanced features are made available to the users through the deployment of a graphical interface in a customized Virtual Research Environment using the gCube technology Beside the provision of a resilient and adaptable environment for the ecological niche modeling community the evaluation of the workîow on a cloud testbed demonstrates that the proposed solution achieves good performance providing the typical beneìts of a cloud environment as elasticity and on demand provisioning of the resources Future work includes the development of a connector for Condor in order to extend the execution of the use cases to the brazilian grid and the use of services provided by the gCube framework in order to dynamically retrieve the locations of the layers querying the information service A CKNOWLEDGMENT This work has been supported by the Spanish Ministry of Science and Innovation contract no TIN2007-60625 and CSD2007-00050 and by the European Commission grant agreement no 288754 EUBrazilOpenbio project R EFERENCES  VENUS-C V irtual Multidisciplinary En vironments Using Clouds www.venus-c.eu 2102  D Lezzi R Raf anell A Carrion I Blanquer  V  Hernandez R.M Badia Enabling e-Science applications on the Cloud with COMPSs Euro-Par 2011 Parallel Processing Workshops vol 7155 2012 pp 25-34  M.E.S Muoz R Gio v anni M.F  Siqueira T  Sutton P  Bre wer  R.S Pereira D.A.L Canhos V.P Canhos openModeller a generic approach to species  potential distribution modelling Geoinformatica vol 15 2001 pp 111-135  J Grinnell Field tests of theories concerning distrib utional control American Naturalist 51 115-128 1917  J Sobern A.T  Peterson Interpretation of models of fundamental ecological niches and species distributional areas Biodiversity Informatics vol 2 2005 pp 1-10  A.T  Peterson J Sobern R.G Pearson R.P  Anderson E MartinezMeyer M Nakamura M.B Arajo Ecological niches and geographic distributions Princeton University Press ISBN 978-0-691-13688-2 2011  Brazilian V irtual Herbarium http://biogeo.inct.îorabrasil.net  Flora do Brasil http://îoradobrasil.jbrj.go v br/2010  speciesLink http://splink.cria.or g.br  A H Hirzel J Hausser  D Chessel N Perrin Ecological-niche f actor analysis How to compute habitat-suitability maps without absence data Ecology vol 83 no 7 2002 pp 2027-2036  R.P  Anderson D Le w  A.T  Peterson Ev aluating predicti v e models of species  distributions criteria for selecting optimal models Ecological Modelling vol 162 2003 pp 211-232  O F arber  R Kadmon Assessment of alternati v e approaches for bioclimatic modeling with special emphasis on the Mahalanobis distance Ecological Modelling vol 160 2003 pp.115-130  Phillips S.J Anderson R.P  and Schapire R.E 2006 
Ecological Modelling 190 231259  B Sch  olkopf J.C Platt J.C Shawe-Taylor A.J Smola R.C Williamson Estimating the support of a high-dimensional distribution Neural Computation vol 13 no 7 2001 pp 1443-1471 Available ACM Digital Library doi:10.1162/089976601750264965  Condor http://research.cs.wisc.edu/htcondor  L Candela G Kakaletris P  P agano G P apanik os F  Simeoni The gCube interoperability framework in Proc 2nd DL.org Workshop Making Digital Libraries Interoperable Glasgow Scotland 9-10 September 2010 pp 35 42 Donatella Castelli Yannis Ioannidis Seamus Ross eds DL.org 2010  Amazon Elastic Compute Cloud Amazon EC2 http://aws.amazon.com/en/ec2  Open Neb ula http://openneb ula.or g  Open Cloud Computing Interf ace W orking Group http://www occiwg.org  Open Stack http://www openstack.or g  F  Marozzo F  Lordan R Raf anell D Lezzi D T alia R M Badia Enabling Cloud Interoperability with COMPSs in Proc Euro-Par 2012 Parallel Processing vol 7484 2012 pp 16-27 Available SpringerLink doi:10.1007/978-3-642-32820-6 4  I F oster  A Grimsha w  P  Lane W  Lee M Mor gan S Ne whouse S Pickles D Pulsipher C Smith M Theimer OGSA Basic Execution Service Version 1.0 Grid Forum Document GFD-RP 108 8/8/2007  D Lezzi S Memon R Raf anell H Soncu M Riedel R.M Badia Interoperable execution of eScience applications on Grids  Clouds through open standards Unicore Summit 2012 Proceedings  A Anjomshoaa M Drescher  Job Submission Description Language JSDL Speciìcation Version 1.0 Grid Forum Document GFD-R.056 7 November 2005 A Savva Ed  The HPC Basic proìle speciìcation http://www.ogf.org/documents/GFD.114.pdf  I Goiri J Guitart J T orres Elastic Management of T asks in V irtualized Environments XX Jornadas de Paralelismo JP 2009 A Corua Spain September 16-18 2009 pp 671-676 ISBN 84-9749-346-8  W orldClim Global Climate Data http://www w orldclim.or g  J.S P ahw a R.J White A.C Jones M Bur gess W A Gray  N.J Fiddian T Sutton P Brewer C Yesson N Caithness A Culham F.A Bisby M Scoble P Williams S Bhagwat Accessing biodiversity resources in computational environments from workîow applications in Proc Workshop on Workîows in Support of Large-Scale Science in conjunction with the 15th IEEE International Symposium on High Performance Distributed Computing 2006 Paris France  T riana http://www trianacode.or g  D Pennington D Higgins A.T  Peterson M.B Jones B Ludaescher  S Bowers Ecological Niche Modeling Using the Kepler Workîow System in Proc Workîows for e-Science I Taylor D Gannon E Deelman and M Shields eds 2007 Springer-Verlag  Biodi v ersity V irtual e-Laboratory http://www bio v el.eu  T a v erna http://www ta v erna.or g.uk  Data Infrastructure Ecosystem for Science http://www d4science.eu 
Maximum entropy modelling of species geographic distributions 
1228 


Fig 4 Instantaneous network trafìc between the SCARF computational nodes and storage during a CEMS parallel data analysis job Colours indicate how close the trafìc is to saturating bandwidth red colours are closest The SCARF compute nodes are behind the three grey switches switches to a top switch itself connected to a local SCARF Panasas disk store and to the JASMIN storage Hence the cloud retrieval workîow currently involves copying data from JASMIN storage to SCARF storage and then the analysis involves reading and writing to the local storage within each task Products are eventually written back to JASMIN A snapshot of the network performance associated with these tasks appears in gure 4 At this time the cloud retrievals were running on about 1000 cores on approximately 100 nodes a mix of 10 cores/node and 12 cores/node jobs Each node has a 1 Gb/s connection to the storage Although the rest of SCARF was busy the I/O load was dominated by these jobs which could have been asking up to 100 Gb/s which is why we can see the three SCARF storage links are saturated at near 10 Gb/s We can also see that the SCARF Panasas link 50 Gb/s was not saturated Time to solution for these sort of jobs will be massively improved if enough compute is available within JASMIN there would be no need for the starting and nishing copying steps and more importantly some tasks would return to being compute bound If enough compute was available a similar job would then be better conìgured to use 50 nodes each connected at 10 Gb/s and utilising one JASMIN bladeset In such cases we would expect the bladeset I/O capability to be balanced with the I/O demand from the compute nodes IV JASMIN F UTURES It is clear from the rst year usage that further JASMIN expansion in both storage and adjacent compute is required by the pent-up demand from the community However that demand is effectively inìnite simulations can be run at higher resolution with more output and multiple versions of earth observation products can be compared and contrasted Execution times can be driven downwards but how much of that drive downwards could result from more hardware and how much from better software tools What is the right balance of computing to storage and between high performance storage and less performant including tape storage We already know that inefìcient but easy to use workîow can become prevalent in the community provided resources are available Sometimes this is the right thing to happen since inefìcient execution may be balanced by easy construction of the workîow Similarly experience elsewhere suggests that more than two-thirds of data stored in a simulation environment can remain unread Such data may be unread due to a variety of factors ranging from a non-existent user community maybe the wrong data was produced it is not of sufìcient quality or inadequate metadata means potential users are not even aware of the data through to an active user community who do not have the physical software or human resources to exploit the data In terms of the curated archive we have more than a decade of experience that suggests we can manage some of the information requirements but that many users have been previously hindered by not having suitable resources to manipulate the data Our one year of JASMIN experience suggests that many of these communities can be signiìcantly aided by an expansion of physical resources and our experience with SCARF and JASMIN shows that signiìcantly more compute will help those communities We have also identiìed new environmental science communities who will beneìt from a shared data storage and analysis platform including those providing hydrological services To that end we plan a signiìcant hardware expansion over the next two years with o\(3000 cores o\(5 PB of disk and o\(10 PB of tape to be added in two phases With the new investment we believe we will have the right balance of storage and compute for the target communities Regardless of the hardware investment we recognise that some applications will still be compute bound In particular we expect that both genomics and climate service applications may sporadically need more resources than we can deliver To that end we are also investing in the development of a cloud broker which will manage the allocation of resources between our private cloud and public commercial clouds This federation layer would be an abstraction over our the application programming interfaces to our VMware service and selected commercial clouds However while projects such as Helix Nebula http://helix-nebula.eu have demonstrated utility for science applications to use public clouds other projects report less success using commercial clouds e.g see the survey by F o r much of our w ork load we e xpect the cost in time and money of moving high volume data in and out of commercial clouds to be prohibitive hence we plan the provision of signiìcant internal cloud resources Some of our community have already built workîows suitable for clouds but many have not It is not yet clear whether the uptake of our batch compute reîects that balance or that we simply do not have enough cloud resource yet and so batch computing is more efìcient from a user perspective Currently we also have an issue with our internal cloud in that we are unable to export our high volume storage with full parallel access into our cloud since we cannot yet adequately constrain access While we are working on solutions our cloud can only make use of relatively poorly performant NFS access to the high volume storage That too will affect the balance of use Most of us believe that managed clusters like LOTUS or SCARF allow users to get the best performance and to get on with science rather than worrying about how to get a virtual cluster to work for them but they are not universal views 74 


particularly given that some of our workload will be portalbased user-services for whom different metrics of service and performance will be necessary There is a tension here between the constraints of working with a shared massively parallel le system and massively scalable compute We believe the scale of our upgrades will both allow us to serve the user communities and to explore both sides of this debate Whatever the workîow users settle on whether batch or cloud orientated most will have to make signiìcant improvements to their workîow to exploit massive parallelisation We have presented an example of how such workîows can be developed within a constrained virtual environment before exploiting massive batch computing but thus far these sort of interventions are very resource intensive We do not have the resources to intervene on such a scale for most users and we do not yet have suitable documentation in place to help users develop their own solutions To that end we are also procuring improvements in documentation and training materials V S UMMARY We have presented the JASMIN architecture rst year of usage and near term plans The physical architecture consists of 600 cores and 5 PB of fast disk connected by low latency networking The compute environment supports a range of virtualisation options from batch to cloud computing A diverse and growing user community is exploiting JASMIN examples include high resolution climate modelling and whole satellite mission analysis for cloud and land surface retrievals The use of Panasas for storage has been very successful with exibility reliability and low management overheads being key to that success However the existing JASMIN environment is underpowered in compute the storage is lling and difìculties exporting the high performance disk into the local VMware cloud computing environment remain JASMIN users are becoming accustomed to a new analysis environment and early adopters are getting signiìcant improvements in their workîow completely changing the nature of the science they can undertake However thus far the JASMIN support team has not yet been able to invest in comprehensive user documentation or training so not all the community has seen the beneìts of these investments To fully exploit JASMIN changes in software and workîow will be necessary for most users and these will take time to engender Recognising the limitations with the existing physical infrastructure and with new user communities anticipated hardware software and documentation will all be upgraded over the next two years R EFERENCES  B N La wrence V  Bennett J Churchill M Juck es P  K ersha w  P Oliver M Pritchard and A Stephens The JASMIN super-datacluster ArXiv e-prints  Apr 2012  K E T aylor  R  J  Stouf fer  and G A Meehl  A n o v ervie w o f CMIP5 and the experiment design Bulletin of the American Meteorological Society  vol 93 pp 485Ñ498 Oct 2011 A v ailable http://journals.ametsoc.org/doi/abs/10.1175/BAMS-D-11-00094.1  D N W illiams B N La wrence M Lautenschlager  D  Middleton and V Balaji The earth system grid federation Delivering globally accessible petascale data for CMIP5 in Proceedings of the 32nd Asia-Paciìc Advanced Network Meeting  New Delhi Dec 2011 pp 121Ö130 A v ailable http://usymposia.upm.my/inde x.php APAN  Proceedings/32nd APAN/paper/view/155  M S Mizielinski M J Roberts P  L V idale R Schiemann M E Demory J Strachan T Edwards A Stephens M Pritchard P Chiu A Iwi J Churchill C D C Novales J Kettleborough W Roseblade P Selwood M Foster M Glover and A Malcolm High resolution climate modelling the UPSCALE project a large simulation campaign Geoscientiìc Model Development  vol In preparation 2013  J.-P  Muller  P  L e wis J Fischer  P  North and U Framer  The ESA GlobAlbedo project for mapping the earths land surface albedo for 15 years from european sensors in Geophysical Research Abstracts  vol 13 Vienna 2011 p 10969 A v ailable http://www.globalbedo.org/docs/Muller-GlobAlbedo-abstractV4.pdf  D Ghent and J Remedios De v eloping rst time-series of land surface temperature from AATSR with uncertainty estimates in Geophysical Research Abstracts  vol 15 2013 p 5016 Available http://adsabs.harvard.edu/abs/2013EGUGA..15.5016G  C A Poulsen R Siddans G E Thomas A M Sayer  R  G  Grainger  E Campmany S M Dean C Arnold and P D Watts Cloud retrievals from satellite data using optimal estimation evaluation and application to ATSR Atmospheric Measurement Techniques  vol 5 no 8 pp 1889Ñ1910 Aug 2012 A v ailable http://www.atmos-meas-tech.net/5/1889/2012  J Cohen B Dolan M Dunlap J M Hellerstein and C W elton MAD skills new analysis practices for big data Proceedings of the VLDB Endowment  vol 2 no 2 pp 1481Ñ1492 2009 Available http://dl.acm.org/citation.cfm?id=1687576  K Shv achk o H K uang S Radia and R Chansler  The hadoop distributed le system in Mass Storage Systems and Technologies MSST 2010 IEEE 26th Symposium on  2010 pp 1Ñ10 A v ailable http://ieee xplore.ieee.or g/xpls/abs all.jsp arnumber=5496972  H Herodotou H Lim G Luo N Boriso v  L Dong F  B Cetin and S Babu Starìsh A self-tuning system for big data analytics in Proc of the Fifth CIDR Conf  2011 A v ailable http://x86.cs.duke.edu  gang/documents/CIDR11 Paper36.pdf  J Buck N W atkins J Lefe vre K Ioannidou C Maltzahn N Polyzotis and S Brandt Scihadoop Array-based query processing in hadoop Technical Report UCSC-SOE-11-04 UCSC Tech Rep 2011  G Sak ellari and G Loukas  A surv e y of mathematical models simulation approaches and testbeds used for research in cloud computing Simulation Modelling Practice and Theory  A v ailable http://www.sciencedirect.com/science/article/pii/S1569190X13000658 75 


Copyright © 2009 Boeing. All rights reserved  Architecture Server-1 Server-2 DB2 SURVDB XML Shredder WebSphere Message Broker Ext.4 H Ext.3 G Ext.2 F Ext.1 E C WebSphere MQ TCP/IP Live ASDI Stream IBM Cognos Server-3 IBM SPSS Modeler SPSS Collaboration Deployment Services 


Copyright © 2009 Boeing. All rights reserved  Database Modeling Schemas for correlated ASDI messages translated into equivalent relational schemas  Database tables generated based on classes created from schema definitions  Nine main, eleven supporting tables  Each main table contains FLIGHT_KEY 


Copyright © 2009 Boeing. All rights reserved  Database Modeling 


Copyright © 2009 Boeing. All rights reserved  Correlation Process To archive received ASDI data  Track messages must be correlated with flight plan messages FLIGHT_KEY assigned Uncorrelated data tagged Approx 30 minutes to correlate one day of data 


Copyright © 2009 Boeing. All rights reserved  Historical Data Processing To load correlated data  Uncompress, unmarshall  Create a list of files containing the correlated data  Write data to warehouse 


Copyright © 2009 Boeing. All rights reserved  Live Data Processing Processed using IBM MQ IBM Message Broker and a technique called XML Shredding Message Broker Compute Nodes  Uncompress Node  Extract correlated messages  Shred Node adds to DB Stored Procedure ìshreds XML docs and adds to tables 


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a ìkey, valueî list using an XSTL  Queries made against this list of ìkey, valueî pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


