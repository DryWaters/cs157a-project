Mining Mutually Dependent Patterns Sheng Ma and Joseph L Hellerstein IBM T.J Watson Research Center Hawthorne NY 10532 Abstract-In some domains, such as isolating problems in computer net works and discovering stock market irregularities there is more interest in patterns consisting of infrequent, but highly correlated items rather than patterns that occur frequently as defined by minsup the minimum sup port level Herein, we describe the m-pattern, a new pattern that is defined in terms of minp the minimum probability of mutual dependence of items in the pattern We show that all infrequent m-pattern can be discovered by an efficient algorithm that makes use of a a linear algorithm to qualify an m-pattern b an effective technique for candidate pruning based on a necessary condition for the presence 
of an m-pattern and e a level-wise search for m-pattern discovery which is possible because m-patterns are downward closed Further we consider frequent m-patterns, which are defined in terms of both minp and minsup Using synthetic data, we study the scalability of our algorithm. Then, we apply our algorithm to data from a production computer network both to show the m-patterns present and to contrast with frequent patterns We show that when minp 0 our al gorithm is equivalent to finding frequent patterns However with a larger minp our algorithm yields a modest number of highly correlated items which makes it possible to mine for infrequent but highly correlated item sets To date many actionable m-patterns have been discovered in produc tion systems I INTRODUCTION Data mining aims to discover useful patterns in large data 
sets To date much emphasis has been placed on finding frequent pat terns in the setting of the market basket  Market bas ket data consists of transactions each of which is a set of items purchased together An association rule relates two itemsets E1 and Ez Such a rule indicates that E1 is strongly dependent on E2 which is denoted by El  Ez Directly discovering all such rules is computationally intractable because the large number of possible itemsets To avoid this difficulty, Agrawal pro posed finding all frequent association rules Here 223frequent\224 places an additional requirement and enables the two-step al gorithm 2][4 for discovering all frequent itemsets followed by exhaustively computing association rules of the frequent item sets 
The efficiency gain is achieved by exploring the down ward closeness property of the frequent itemsets However, this diminishes quickly when searching for infrequent itemsets in which the minimum support minsup has to be set very low Some real world applications requires the discovery of infre quent but highly correlated patterns discussed this and analyzed pair-wise patterns for news articles and web logs Other such applications include problem detection in computer networks intrusion detection in computer systems and fraud detection in financial systems In these applications, normal be havior dominates abnormal behavior such as failures and in trusions is rare Thus it is interesting in discovering infrequent patterns that relate to problem situations Consider the exam ple in  101 in wliich the following events are generated from a router network interface card failure unreachable destination 
and \223cold start\224 trap The last event indicates that the router has failed and restarted If these events commonly occur together then the first two events may provide advance warning of when the third will occur One naive approach to mining for infrequent patterns is to discover all frequent patterns e.g using A-priori with very low minimum support minsup and then use post-processing to discover significant patterns However this is impractical Specifically to find infrequent patterns, the minimum support in A-priori must be set very low which in turn results in long processing time as well as many irrelevant patterns that must be examined Irrelevant patterns i.e pattern occurring sim ply by chances\are particularly problematic in real applications with a wide disparity in the frequency of items As an exam ple in event data some events have a very high probability of 
occurrence, such as the 223connection closed\224 events issued by network hubs that support the dynamic host configuration pro tocol \(DHCP\As a result there are a large number of patterns that have 223connection closed\224 in them even though this event has little correlation with the rest of events The foregoing motivates us to develop the m-pattern An m-pattern is an itemset for which any two subsets are strongly dependent with each other characterized by a minimum depen dence probability of at least minp a user-specified parameter where 0 5 minp 5 1 That is if some items in an m-pattern occurs then it is very likely that the other items in the item set occur as well Here minp plays a similar role to that of mincon f but requires 
for any two subsets An m-pattern differs from a frequent association   and the correlated  in three major aspects An m-pattern requires mutual dependence An association rule only requires one-way dependence For example, an as sociation rule might specify either diaper  beer or beer  diaper while an m-pattern is present only if diaper e beer A correlated pattern refers to an itemset whose items are not in dependent determined by a statistical test Thus it is a much 223weaker\224 correlation compared to both association rules and m patterns However, because of this it is not feasible to discover infrequent patterns We note that our m-pattern definition ex cludes the situation that some two subset pairs but not all of 
an itemset are strongly dependent. This strong dependence require ment provides many nice properties that enables us to efficiently discover all m-patterns, even if their supports are very low An m-pattern requires that any two subsets of the itemset be mutually dependent Neither an association rule nor a correlated pattern has such a property As we will show later, such a prop erty enables directly, efficiently discovering all m-patterns An m-pattern does not have a condition of minimum support although this can be added This makes it possible to find all infrequent m-patterns In contrast, only frequent associa tion rules and correlated rules can be efficiently discovered in practice To provide more insight into m-patterns and how they differ 0-7695-1 119-8/01 17.00 0 2001 IEEE 409 


TI a b c d e 1 9 R d r,9 T3 a b d 9 TI a d 9 TS f g 1 T6 e 1.9 V e t  Fig 1 Illustrative transaction data from frequent associations an example is presented Figure 1 displays data for transactions that are listed in the left side of the figure The i-th transaction is denoted by Ti and symbols a through g represent items Also, there is a table that displays support levels for patterns of length one and another table for patterns of length two For example, the support level for a b is 2 since a and b are contained in TI and T3 Frequent itemsets have a number of occurrences that exceeds minsup For exam ple let minsup  3 Then a d and a g are frequent item sets since both have a support level of 3 In contrast m-patterns are defined by mutual dependence using the threshold minp As we show later if minp  0.5 then a b is an m-pattern, but a,g is not Intuitively since g occurs in every transaction g  a although a g occurs frequently and a  g How can we use m-patterns As we will show in Section 5 m-patterns are common in event data as a result of the exten sive interdependencies introduced by logical and physical inter connections of network elements. These dependencies manifest themselves as a set of correlated events or alarms when a prob lem arises For example when a link of a local area network LAN\goes down all hosts connected to the link generates lost connection" events Clearly it is important to quickly pinpoint problem causes before there are wide spread service disruptions Discovering an m-pattern may provide a way to characterize or anticipate problem situations We refer to IO for more detailed explanations To efficiently discover all m-patterns for given minp we need to resolve two non-trivial issues First the definition of m-pattern requires any two subsets of an itemset to be mutually dependent On the surface, checking whether an itemset is a valid m-pattern requires exponential computational complexity in respect to the size of pattern This makes it very inefficient Second the search space is exponentially large in respect to the number of distinct items. Examining and qualifying each of the candidate is computationally intractable. This is the same issue as that of discovering all frequent itemset Herein we resolve these efficiency issues by exploiting spe cial properties of m-patterns We demonstrate analytically that examining a small number of two subset pairs is sufficient for qualifying an itemset as a m-pattern This results in a simple linear algorithm Further we show that an m-pattern has the downward closure property and thus it can be searched effi ciently through a level-wise search strategy Last we establish a necessary condition for qualifying m-patterns This necessary condition provides an upper bound that can be used to further prune candidates effectively To explore the relationship with frequent patterns we define frequent m-patterns in terms of both minp and minsup We show that when minp 0 frequent m-patterns reduce to fre quent patterns While with a larger minp our algorithm yields a modest number of highly correlated items which makes it possi ble to mine for relatively infrequent but highly correlated item sets Consequently we can explore the tradeoff between the strength of mutual dependence and the occurrence frequency of patterns A Related work Much prior work is relevant to this paper Agrawal    developed an elegant algorithm called Apriori that finds all frequent patterns in a level-wise manner Since then many techniques have been proposed to improve the algorithm's efficiency by minimizing the number of data scans needed memory required etc 1][6][18 In this paper, we propose mu tual dependence patterns We demonstrate that m-patterns can be discovered even if they occur infrequently Much work has been done for finding different types of pat terns e.g 16 ll 5  However most these efforts require that frequent itemsets be discovered first rules are de rived in a separate post-processing step As discussed before this approach cannot find infrequent patterns without examining a huge number of candidates Closely related to our work Cohen et a1.[8 discussed the needs for discovering infrequent but highly correlated item sets However, they defined a symetric similarity measure of two items and their algorithm can only discover itemsets containing only two items Herein we define the mutual depedency of an itemset with any length We develop an efficient level-wise algo rithm that discovers all infrequent but highly correlated patterns with any length Brin et.a1.[7 defined correlated patterns i.e itemsets that are not independent based on a chi-squared test The condition used for a correlated pattern is rather weak in that correlated patterns are upward closed That is any super set of a correlated pattern is a correlated pattern To avoid a potential explosion of patterns, the authors focus on frequent correlated patterns as specified by minsup In contrast m-patterns cap ture mutual dependency without using minsup by using minp As we show later m-patterns are downward closed which al lows for the efficient discovery of infrequent m-patterns B Our contributions Our main contributions can be summarized in the following 1 We define a new type of patterns called m-pattern that cap tures the mutual dependence of items We demonstrate that the major advantage of m-pattern is its ability to discover infre quent but mutually dependent patterns We demonstrate empir ically the significance \(e.g actionable interesting unexpected of m-patterns in system management applications 2 We develop an efficient algorithm that can check whether an itemset is a qualified m-pattern in a linear time This is not obvious because the definition of m-patterns requires that any 410 


two subset pairs are dependent with each other This leads to exponential computational complexity in respect to the size of a patterns 3 We derive an upper bound property of an m-pattern This can be used to effectively eliminate the search space 4 We derive an algorithm for efficiently discovering all m patterns Our algorithm integrates 2 and 3 above with the well-known level-wise search strategy 5 We show that the m-pattern condition defined by minp can be used together with the Occurrence frequence defined by minsup This allows to explore the tradeoff between the strength of mutual dependence and the occurrence frequency of patterns C Organization of this paper The remainder of this paper is organized as follows. Section 2 defines m-patterns and related concepts Section 3 presents our algorithm for finding m-patterns Section 4 discusses extensions to the basic algorithm to handle frequent m-patterns Section 5 provides empirical assessments of our algorithms Our conclu sions are contained in Section 6 11 DEFINITIONS OF M-PATTERNS AND RELATED CONCEPTS This section formalizes m-patterns and related concepts We begin by generalizing the notion of a qualified pattern a concept that is usually associated with support levels We then specify what is meant by mutual dependence, a concept that we formal ize in terms of the empirical distribution of the transaction data to be mined. With this background we then define m-patterns We begin by generalizing what is meant by a qualified pat tern Let I be a collection of distinct items, and let the itemset E be a non-empty subset of I As usual the transaction data D  Di}E1 is a collection of N transactions where the i-th transaction D c I We define a qualification function D\(E to be fD  E  true,false where fu\(E  true iff E is a qualified f-pattern in D For example let fu be the qualification func tion for a frequent pattern with minimum support level minsup Then, the qualification for frequent pattern fo\(E  true iff supportu\(E 2 minsup 1 where supportu\(E  I{DI E DIE Dt}I is the number of occurrences of E in data D Is1 represents the cardinality of a sets Now we formalize what is meant by dependence known as rule confidence in association rules and mutual dependence Let E1 and E2 be two itemsets El  E2 is the union of E1 and Ez The empirical probability of the occurrence of E is defined by pu\(E  suppmt~\(E 2 where N is a normalizationconstant For the market basket data N is the number of transactions in D Under a mild assumption the empirical probability PD  approaches the real probability P as the number of observations becomes large The dependence of itemset El on itemset E2 is quantified by the empirical conditional probability Denoted by PD\(E1 I Ez the empirical conditional probability is computed by Using Equation 2 we can further obtain The above equation provides another interpretation of the con ditional probability the ratio of the number of transactions in which E1 and E2 occur together to the number of transac tions in which E2 occurs Clearly a large PD\(ElIE2 indi cates that E1 depends more strongly on Ez In an extreme case PD\(ElIE2  1 implies that whenever E2 occurs E1 also occurs Note that PD\(E1IEz characterizes a one-way de pendence In order to have a mutual dependence we must also examine p~\(EzlE1 Definition I Two non-empty itemsets El E2 are signifi cantly mutually dependent for a given D and a minimum de pendence threshold 0  minp  l iff PD\(EIIE 2 minp and PD\(EzlE1 2 minp With this background we define an m-pattern to be an itemset for which any subset is significantly mutually dependent on all other subsets That is if any subset of an m-pattern is present the remaining items occur with high probability This is formal ized as follows Definition2 A nonempty itemset E is an m-pattern with minimum mutual dependence threshold minp iff PD\(ElIE2 2 minp 5 holds for any non-empty two subsets E1 and E2 of E where 0 5 minp  1 Note that this definition does not refer to a support level Thus unlike frequent associations m-patterns will be discov ered regardless of the frequency of their occurrences However as we note later it is quite reasonable to consider frequent m patterns that use both minp and minsup We now return to the example in Figure 1 to illustrate why a,d is an m-pattern but a,g is not assuming minp  0.5 M-patterns are defined by mutual dependence which re quires that all pairs of subsets be significantly mutually depen dent above the minp threshold For the itemset E  a b we compute two conditional probabilities Po a}l\(b  2/2 Therefore a b is a m-pattern with minp  0.5 How ever a,g is not an m-pattern since PD\({a}I{g  3/7  0.5 suppmtD\(\(a b a  2/3 and PD\({b  111 ALGORITHM FOR MINING M-PATTERNS This section develops an efficient algorithm for discovering all m-patterns Efficiency is obtained by addressing three is sues 1 how to test qualify\that an itemset is an m-pattern 2 how to exploit a level-wise search and 3 how to prune the search space using a necessary condition for the presence of an m-pattern 411 


A ESJiciently Qualifying an M-Pattern The definition of m-patterns \(Equation 5 can be used to test whether an itemset is an m-pattern However, doing so means computing the pairwise conditional probability of all subsets of a candidate m-pattern Assuming that El E2 are disjoint and observing that these sets are chosen so that an item is in at most one, then the number of conditional probabilities that must be computed is 0\(3 where k is the length of E If El E2 are not disjoint, the computational complexity is 0\(4 This means that the direct application of the definition to qualify an m-pattern is computationally intractable even for modest sized values of k To qualify an m-pattern efficiently we derive the follow ing equivalent definition of a m-pattern Roughly speaking it shows that we just need to compute a linear number of condi tional probabilities rather than to compute all pairwise condi tional probabilities of m-pattern subsets Property I Equivalent definition of m-patterns An itemset E is an m-pattern as in Definition 2 iff PD\(E  a}l{a L minp 6 for every item a in E Proof We first prove the necessary condition That is we must show that if E is an m-pattern, then PD\(E  a}~{u 2 minp for any item a E E But this follows by letting E1  E  a E2  U and using Definition 2 Now we prove that if PD\(E  a}l{a 2 minp for any a E E then E is m-pattern Let El and E2 represent any two non-empty subsets of E Let a E Ea Since support~\({a 2 support~\(E2 we obtain Po\({a 2 by using Equa tion 2 Similarly, since E1  E2 is a subset of E Po El  E2 2 Po E Therefore we obtain  E E Z PD\(E  7  PD\(E  u}l{u 2 minp Since E1 and E2 are any two subsets of E we have proven that E is an m-pattern by Definition 2 0 The above property allows us to qualify an m-pattern in linear time O\(k The specifics are described in the algorithm below Algorithm isMPattern Input itemset E the supports for E the support for each item in E and minp Output true false I For each item a in E 2 If SupportD\(E a  minp 3 Return false 4 End for 5 Return true B Level-Wise Searching Now we develop an efficient algorithm for discovering all m patterns Similar to frequent itemset discovery, the number of all potential m-patterns is huge on the order of n  where n is the number of distinct items and k is the maximum length of an itemset It is not uncommon that n can to be 1,000 or more Thus it is computationally intractable even for modest sized k We note that a downward closure \(See 7 for more discussion of downward closure can lead to an efficient search algorithm Now we demonstrate that m-patterns have such a property Definition 3 An itemset E is said to be downward closed un der a qualification function fo  if the following is true if the qualification condition f E holds then fo E holds for any E c E Put differently if D\(E does not hold for a subset E then fo\(E cannot be true With the downward closure we can greatly reduce the number of itemsets that must be exam ined For example frequent itemsets are downward closed since support~\(E 5 supportD\(E By taking advantage of this property Agrawal et.a1.[2][3 devised a level-wise algorithm called A-priori that discovers all frequent itemsets with a sup port of at least minsup In specific starting with k  1 A priori explores the candidate space iteratively in level wise by first finding all frequent itemsets of size k and then using this knowledge to construct the candidates of size k  1 based on the downward closure property Since then much work has been de veloped to improve the algorithm by exploring different search strategies \(see   and references therein All these work is built on the downward closure property of frequent itemsets Herein we first show the downward closure property of m patterns We then focus on the level-wise algorithm although all other algorithms can be applied equivalently Property 2 Downward closure property M-patterns are downward closed Proof let E be an itemset and E be a subset of E We need to prove that if E is an m-pattern, then E must be an m-pattern Let El and E2 be any two non-empty subsets of E Clearly E1 and E2 are also subsets of E because E C E Since E is an m-pattern, we obtain PD\(E1IE2 2 minp Equation 5 Therefore E is an m-pattern 0 Since m-patterns are downward closed we use a level-wise algorithm for discovering all m-patterns Algorithm DiscoverMPatterns Input minp and data D Output all qualified m-patterns Lk 1 L1  a}laE q;c2  a,b}la,bE I 2 Scan D to count the occurrences of each pattern in L 1 and c2 3 k  2 4 Compute the qualified candidate set Lk  U E ck lish/lPattern\(v  true 5 Compute the new candidate set Ck+l based on Lk 6 If Ck+l is empty, output Lk and terminate 7 Scan D and count the occurrence of each pattern Y E Ckfl 8 k  k  1 go back to 4 We note that all itemsets with size 1 are qualified m-patterns by our definition The set of candidate itemsets of size 2 are thus the set of all combinations of two items. Steps 1 and 2 treat this special situation Then, the algorithm iteratively searches the candidate space in a level-wise manner Step 4 finds all m patterns of length k by using isMPattern Step 5 constructs Ck+l the candidate itemsets for level k  1 based on Lk the qualified m-patterns found in level k This can be accomplished by the join operation followed by the pruning done in the A priori algorithm 2 If no more candidates can be generated 412 


step 6 terminates and outputs all m-patterns found Otherwise Step 7 scans the data to count the occurrences of each candidate in Ck+l Steps 4 through 8 are repeated until no more qualified candidates are found Implementing this algorithm requires keeping a counter for each candidate pattern in Ck As each market basket in D is examined the counter is increased by one if the candidate is a subset of the transaction We refer to 2 for further implemen tation details This algorithm needs k-1 data scans The complexity of this algorithm is linearly dependent on the length of data but is ex ponentially dependent on the size of the longest m-pattern In practice the algorithm converges quickly, especially when pat terns are not very long C Pruning Candidate M-Patterns The most time-consuming step in the above algorithm is counting the occurrences of candidates Clearly the smaller Ck the faster the algorithm Therefore we should prune candidates as much as possible before scanning the data and counting Be low we show how this can be done by making use of a necessary condition for the presence of an m-pattern Property 3 Upper bound property Let E\222 be a non-empty subset of E Let an item a E E\222 Then 2 PD\(E    5 PD\(E  E\222 E\222 Proof Recall that PD\(E  a}l{a  PD\(E  Equation3\Since PD\(E 5 PD\(E-{u the first conclusion holds. Further since a E E\222 PD\(E  U 5 PD\(E  E\222 and PD\({u 2 PD\(E\222 the second inequality holds as well 0 The above property provides an upper-bound for the empirical conditional probability PD\(E  a}l{a Moreover it also proves that PD\(E  a a is the tightest upper bound among possible upper bounds Using Property.3 we easily obtain a necessary condition for qualifying an m-pattern Property 4 Neeessary condition If E is an m-pattern with minp then 1 PD\(E  a}l{a 5 pD\(E  a a supportu\(E  a a 2 minp for any item a 8 Proof This follows by combining Equation 5 4 and Property 3 0 We note that the above necessary condition only depends on the support of patterns found at level 1 and level k  1 this is different from Property 1 which is dependent on support E computed by an additional data scan Thus this condition can be used to prune candidates in Step 5 thereby reducing the num ber of candidates for which counting is done in Step 7 We sum marize the pruning algorithm as follows Input a set of candidate patterns Ck Output a set of pruned candidate patterns 1 For each pattern E in Ck 2 For each item a in E 3 Algorithm Pruning If supportD\(E  a a  minp 4 5 Goto\(,v Ck  Ck  E 6 Endfor 7 End for 8 Return CI D Algorithm for Mining M-Patterns Putting the above described algorithms together we obtain Algorithm DiscoverMPatternsWithPruning Input minp and data D Output all qualified m-patterns Lk 1 Li a}l~E I};C2={{a,b}la,bEI 2 Scan D to count the occurrences of each pattern in L 1 and c2 3 k  2 4 Compute the qualified candidate set Lk  v E CklisMPattern\(v  true 5a Construct the new candidate set Ck+l based on Lk by the downward closure property 56 Prune Ck+l based on Property 4 using the Pruning algo rithm 6 if Ck+1 is empty output Lk and terminate 7 Scan D and count the occurrence of each pattern v E Ck+l 8 k  k  1 go back to 4 We note that the level-wise algorithm for m-patterns and that for the frequent association have two common steps construct ing the next level candidate patterns based on the previously qualified patterns \(Step 5a and counting occurrences of candi dates \(Step 7 However these algorithms differ in several ways In particular our algorithm for m-pattern discovery a requires a different treatment for the first and the second levels Steps 1 to 3 b takes advantage of an extra pruning step \(Step 5b and c\employs a different algorithm to qualify a candidate pattern Step 4 E Extensions Here we consider a couple of extensions to our algorithm for discovering m-patterns First, note that the definitions and results thus far presented make no assumption about how transactions in D are obtained Thus if items have a timestamp e.g temporal event data See 15 141 for the detailed definition transactions can be con structed using windowing schemes as in[15 Doing so allows us to discover temporal m-patterns I Second we show how further performance gain can be ob tained by partitioning items We note that Property 3 can be used to partition the search space To illustrate this consider E  a b By the property 3 E may be an m-pattern if both Pu\({a b 2 minp and Pu\({b a L minp By Equation 2 we obtain supportu b 5 support a 5 supportD b 9 Extending this to other items we can partition items so that the above equation will not hold for items in two different partitions In this way a potential m-pattern can only be a subset of items in one and only one partition Consequently, the original problem is divided into several sub-problems each of which relates to 221Some cares are needed to deal with the overlapped windows We refer to  151 for more implementation details 413 


one partition of items This reduces the search space as we only need to consider candidates within a partition Further, we can solve these sub-problems in parallel IV FREQUENT M-PATTERNS This section shows that the concepts of frequent itemsets and m-patterns can be combined to develop an algorithm that dis covers frequent m-patterns A frequent m-pattern is defined in terms of both minp and minsup That is, a frequent m-pattern is significantly mutually dependent with the dependence thresh old minp and has support threshold minsup How do frequent m-patterns compare with frequent associa tion rules A frequent association rule with the form E1  Ez requires two conditions I suppmt~\(E1  Ez 2 minsup and 2 PD\(EzlE1 2 mincon f We note that the second con dition does not have the closure property and thus it is com putationally intractable to consider 2 alone In contrast, m patterns can be discovered efficiently since downward closure holds However, mutual dependency, which is required by m patterns is a stronger condition than association as required by association rules We now formalize the notion of frequent m-patterns DeJinition4 E is said to be a frequent m-pattern with minsup and minp iff E is an m-pattern with minp and the number of occurrences of E is no less than minsup We note that the frequent m-pattern is a more general pattern than the frequent itemset and the m-pattern In that when minsup  0 the frequent m-pattern reduces to the m-pattern When minp  0 the frequent m-pattern becomes the frequent itemsets We can mine all frequent m-patterns efficiently The key insight is that frequent m-patterns are downward closed We demonstrate this by showing a much stronger result 1171 dis cusses this in a slightly different form Property 5 Conjunction and disjunction of downward closure properties Let boolean functions fl and f2  be two qualification functions of an item set such that fl and fa are both downward closed Then, the qualification func tion f,\(E A f2\(E is downward closed where 223A\224 represents the 223and\224 operation And fl\(E V fz\(E is also downward closed V is the 223or\224 operation Proof Let E be a nonempty itemset E\221 C E Assume that fl\(E A fz\(E holds Since fl\(E and fz\(E both hold we know that both fl\(E\222 and fz\(E\222 are true since fl and fa are downward closed Therefore we obtain fl E\222 A fa E\222 is true Similarly assume that fl E V f E holds. Then, at least one of fl\(E and fi\(E is true and so fl\(E\222 V fz\(E\222 is true 0 The foregoing allows us to construct a level-wise algorithms for frequent m-patterns by modifying Step 7 of the m-pattern mining algorithms For a frequent m-pattern the qualified pat terns at level k is Lk  U E CklisMPattern\(v  true and supportu\(v  minsup The remaining steps of the al gorithm are unchanged v EXPERIMENTAL RESULTS This section assesses our algorithms for discovering m patterns. Two kinds of assessments are presented The first eval uates the performance of our algorithm using synthetic trans 221 I I a 100 35 2 2 a0 350 1 1  ss0 rnfNmb..d*.\223.LDn.,\224 223..\223d Fig 2 Average nm time in second vs the number of transactions in 1,ooO action data The second studies our algorithm using real data collected from a production computer network A Synthetic data We begin by using synthetic data to study the scalability and efficiency of our algorithm for discovering m-patterns The syn thetic data are constructed by first generating items randomly and uniformly, and then adding instances of patterns into ran domly selected transactions Thus the synthetic data are spec ified by the following parameters the number of transactions the number of distinct items the average number of random items per transaction the number of patterns and their length and the noise to single ratio NSR\Here, the NSR for an item in a pattern is defined by the ratio between the number of ran dom instances to the number of the item instances in the pattern Throughout, the number of distinct items is 1000 the number of patterns is IO with length 5 the average number of random items in a transaction is 20 and the NSR is 5 We assess scalability by varying the number of transactions We compare the level-wise algorithm for mining frequent pat tern itemsets with our DiscoverMPatternsWithPruning algo rithm for mining m-patterns The values we choose for minsup for frequent patterns and minp for m-pattern are set2 so that there is no false positive above level 2 An experiment con sists of 5 runs done with different random seeds. Figure 2 plots the average CPU time against the total number of transactions in ten thousands The results for frequent itemsets are des ignated by the 222*\222 markers and those for m-patterns by the 222+\222 marker We see that the two curves are almost indistinguishable although the curve for frequent patterns is just below that for m-patterns It is somewhat surprising that m-pattern discovery is so efficient since qualifying an m-pattern requires k compar isons where as frequent itemset only requires one comparison This suggests that a linear algorithm for qualifying m-patterns is sufficiently fast Indeed we see that both algorithms scale linearly as the number of transactions increases Now we study the effect of minp and the benefits provided by the Pruning algorithm Here the number of transaction is fixed at 50,000 The results are plotted in Figure 3 The x-axis is minp and the y-axis is the CPU seconds required to discover m-patterns The line with the 222+\222 markers are the results for DiscoverMPatterns and the line with the 222*\222 markers is for DiscoverMPatternsWithPruning Note that for larger values 20ur results are not sensitive to the specific values used for rninsup and minp 414 


 t  Fig 3 Average run time vs minp of minp e.g minp  7 there is little impact on CPU con sumption This is because minp is sufficiently large compared to the fraction of \223noise\224 transactions However, when minp is small pruning provides significant benefits In fact, when minp is 6.5 a typical run generates about 3730 candidates at the third level With pruning the number of candidates reduces to 2550 B Production Data This section applies our algorithms for discovering m patterns in data from a production computer network. Here our evaluation criteria are more subjective than the last section in that we must rely on the operations staff to detect whether we have false positives or false negatives Two temporal data sets are considered The first was collected from an insurance company that has events from over two thou sand network elements e.g routers hubs and servers The second was obtained from an outsourcing center that supports multiple application servers across a large geographical region Events in the second data set are mostly server-oriented \(e.g the CPU utilization of a server is above a threshold and those in the first relate largely to network events e.g 223link down\224 events Each data set consists of a series of records describing events received by a network console An event has three attributes of interest here host name which is the source of the event alarm type which specifies what happened e.g a connection was lost port up and the time when the event message was received at the network console We preprocess these data to convert events into items, where an item is a distinct pair of host and alarm type. The first data set contains approximately 70,000 events for which there are over 2,000 distinct items during a two week period The second data set contains over 100,000 events for which there are over 3,000 distinct items across three weeks We apply our algorithm for m-pattern discovery to both data sets, and compare the results to those for mining frequent item sets We fix minsup to be 3 so as to eliminate a pattern with only one or two instances, and we vary minp Our results are reported in Figures 4 and 5 for data sets 1 and 2 respectively These figures plot the total number of m-patterns the solid line and the number of border m-patterns the dashed line against minp Here a border pattern refers to a pattern that is not a sub set of any other pattern The x-axis is minp and the y-axis is the number of m-patterns discovered on a log scale Clearly minp provides a very effective way to select the strongest patterns in that the number of m-patters discovered drops dramatically as 14 0.1 0 0 0 0 0 J Fig 4 M-patterns of the first data set 223-\224 the number of m-patterns in the log scale 223..\224 the number of border m-patterns in the log scale x-axis is minp Fig 5 M-patterns of the second data set 222-\224 the number of m-patterns in the log scale 223..\224 the number of border m-patterns in the log scale x-axis is minp minp increases Many of these patterns have very low support levels For example we found 59 border m-patterns with length from 2 to 5 in the first data set when minp  0.7 Half of these patterns have support levels below 10 To compare with frequent patterns it suffices to set minp  0 since the algorithm reduces to mining frequent patterns Figure 6 reports frequent patterns found in the first data Here the x-axis is minsup and the y-axis is the log of the number of patterns found Note that the number of frequent patterns is huge-in ex cess of 1 veri when when minsup is 20 Examining the frequent patterns closely we find that most are related to items that occur frequently, not necessarily items that are causally re lated This is not surprise since the marginal distribution of items in our data is highly skewed Indeed a small set of items account for over 50 of total events and consequently these items tend to appear in many frequent patterns Beyond the quality of the results produced by mining for fre t no U a man Fig 6 Frequent patterns of the first data set 223-\224 the number of frequent patterns in the log scale 223..\224 the number of border frequent patterns in the log scale; x-axis is minp 415 


quent itemsets, there is an issue with scalability as well In Fig ures 4 and 5 minp 2 0.1 and minsup  3 Suppose we have minp  0 and minsup  3 so that we are mining for frequent itemsets but with a very low support threshold When we at tempt to run this case more than 30k candidates are generated at the third level. Not only does this result in very large compu tation time, we ultimately run out of memory and so are unable to process the data We reviewed the m-pattern found with the operations staff Many patterns are related to installation errors \(e.g a wrong parameter setting of a monitoring agent and redundant events e.g 11 events are generated to signal a single problem In addition a couple of correlations were discovered that are being studied for incorporating into event correlation rules for the real time monitoring We emphasize that over half of the m-patterns discovered have very low support levels Why are m-patterns common in these data One reason is a result of physical dependence that manifests itself as a set of events when a problem arises For example when a local area network LAN fails ail hosts connected to the LAN gen erate 223lost connection\224 events. Further, the same hosts generate these events if the same failure occurs This results in the mu tual dependence of these events This observation suggests that m-patterns can be used to construct signatures for problematic situations A second cause of m-patterns is redundant information For example a device may generate an event to report a problem it detects However, there may also be several management agents that monitor the same device and report the same problem This results in an m-pattern consisting of redundant events Identify ing such m-patterns can aid in constructing filtering rules that re move redundant events More details and insights can be found in 13][10 VI CONCLUSION Motivated by the need to discover infrequent but strongly correlated patterns we propose a new pattern a mutual depen dence pattern or a m-pattern M-patterns are defined in terms of minp the minimum probability of mutual occurrence of items in the pattern In contrast to one-way dependence as in asso ciation rules an m-pattern is characterized by a strong mutual dependency between any two of its subsets. That is if any part of an itemset occurs, the other part is very likely to occur as well Our results suggest that such strong mutual dependencies are common in computer networks such as due to interrelated components that are impacted by the same failure We develop an efficient algorithm for discovering m-patterns This is accomplished in three steps First we develop a linear algorithm to qualify an m-pattern based on an equivalence we prove Second we show that a level-wise search can be used for m-pattern discovery a technique that is possible since we prove that m-patterns are downward closed Last, we develop an effec tive technique for candidate pruning by establishing a necessary condition for the presence of an m-pattern A significant impact of the resulting algorithm is that it discovers strongly correlated itemsets that may occur with low support levels something that is difficult to do with existing mining algorithms Using synthetic data we demonstrate that our algorithm scales well as the data set increases in size We also show that the pruning algorithm provides considerable benefit, especially for small values of minp We apply our algorithm to data collected from two produc tion computer networks The results show that there are many m-patterns, many of which of have very low support levels \(e.g fewer than 10 occurrences\Attempting to discover these pat terns using A-priori requires a very small value for support lev els, which results in an explosion of candidates that overruns the memory of the computer we used We further develop frequent m-patterns that are defined in terms of both minsup and minp We show that this is a more general pattern That is, when minp  0 this pattern is equiv alent to frequent itemsets and when minsup  0 frequent m patterns become m-patterns ACKNOWLEDGMENT The authors would like to thank Chang-shing Pemg for help ful discussions REFERENCES C Agganval C Agganval and V.V.V Parsad Depth first generation of long patterns In lnt\222l Conf on Knowledge Discover rind Drm Mining 2000 R Agrawal T Imielinski and A Swami Mining association rules be tween sets of items in large databases In Proc fj\222VLDB pages 207-216 1993 R Agrawal and R. Srikant Fast algorithms for mining association rules In Proc of VLDB 1994 R. Agrawal and R Srikant Mining sequential patterns In Proc of the I Ith Int 221I Conference on Datu Engineering Taipei Taiwan 1995 R Bayardo, R. Agrawal and D Gunopulos Constraint-based rule mining in large dense database In ICDE 1999 R.J Bayardo. Efficiently mining long patterns from database In SIGMOD pages 85-93 1998 S Brin, R. Motiwani and C Silverstein Beyond market baskets Gen eralizing association rules to correlations Datu Mining and Knowledge Discovery pages 39-68 1998 Edith Cohen Mayur Datar Shinji Fujiwara Aristides Gionis Piotr Indyk Rajeev Motwani, Jeffrey D Ullman and Cheng Yang Finding interesting associations without support pruning In ICDE pages 489-499 2000 J Han J Pei and Y Yin Mining frequent patterns without candidate generation \(pdf In Proc 2000 ACM-SIGMOD Int Cunf on Munugement of Data SIGMOD\222OO Dallas TX 2000 J.L Hellerstein and S Ma Mining event data for actionable patterns In lnternutional Conference for the resource manugement  perfiormance evaluation of enterprive computing systems 2000 B Liu and W Hsu Post-analysis of learned rules In AAA/-96 pages 828-834 1996 Bing Liu Wynne Hsu and Yiming Ma Pruning and summarizing the dis covered associations In Proceedings of the ACM SICKDD International Conjerence on Knowledge Discovery  Datu Mining pages 15 18 1999 S Ma and J.L Hellerstein Eventbrowser A flexible tool for scalable analysis of event data In DSOM\22299 1999 S Ma and J.L Hellerstein Mining partially periodic event patterns In ICDE pages 205-214,2001 H Mannila H Toivonen and A Verkamo. Discovery of frequent episodes in event sequences Data Mining mid Knowledge Discover 1\(3 1997 B Padmanabhan and A Tuzhilin A belief-driven method for discovering unexpected patterns In KDD-98 1998 J Pei and J Han Can we push more constraints into frequent pattern mining In CorS on Knowledge Discover rind Datu Mining KDD\222OO Boston MA 2000 H Toivonen Discovery of frequent patterns in large data collections 1996 Technical Report A-1996-5 Department of Computer Science Uni versity of Helsinki 416 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


