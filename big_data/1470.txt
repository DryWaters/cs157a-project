html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">A Novel  Method for Protecting Sensitive Knowledge in Association Rules Mining En Tzu Wang         Guanling Lee         Yu Tzu Lin Dept. of Computer Science &amp; Information Engineering National Dong Hwa University, Hualien, Taiwan, 974, R.O.C Email: m9221009@em92.ndhu.edu.tw Abstract Discovering frequent patterns from huge amounts of data is one of the most studied problems in data mining. However, some sensitive patterns with security policies may cause a threat to privacy. We investigate to find an appropriate balance between a need for privacy and information discovery on frequent patterns By multiplying the original database and a sanitization matrix together, a sanitized database with privacy concerns is obtained Additionally, a probability policy is proposed to against the recovery of sensitive patterns and reduces the modifications of the sanitized database. A set of experiments is also performed to show the benefit of our work 1. Introduction Rapid advances in network communications and software hardware technologies enable users to collect huge amounts of data. At the same time, high-speed computation has made it fea sible to analyze these data. This is called data mining. The infor mation discovered from data plays an important role in many applications such as business management, marketing analysis etc. However, it also brings some problems about privacy Data are valuable; however, they are also valuable to the op ponents if they are naked. Consider a scenario in [6] and extend it, suppose that there are a server and many clients; each client has a transactional database. The clients want the server to gath er statistical information about associations among items to pro vide recommendations to their customers. However, the clients do not want the server to know some sensitive patterns genera ted from their databases. A sensitive pattern is a frequent pattern with security policies, such as commercial consideration. There fore, before a client sends its database to server, sensitive patte rns should be hidden according to specific privacy policies. The server only can gather information from the modified databases In recent years, more and more researches in data mining emphasize the seriousness of the problems about privacy which can be classified into two categories: data privacy problems and information privacy problems. Data privacy problems with two broad approaches focus on the privacy of sensitive data. The randomization approach emphasizes individual privacy and sends randomized data to prevent original data revealing [3][5 6][15]. In the secure multi-party computation approach [7][9 14][16], each party owns a confidential database and wishes to build data mining models on the union of them without revea ling any unnecessary information like the individual records On the other hand, information privacy problems which are proved NP-Hard [1] focus on hiding association rules or freque nt patterns containing highly sensitive knowledge. There are many heuristic methods have been proposed to solve this prob lem [8][10][11][12][13][17]. In [10] and [11], a privacy presser vation framework is proposed to hide sensitive patterns. A trans action retrieval engine is used to speed up the process of finding the sensitive transactions which are identified according to the sensitive patterns. They also bring up a privacy threshold contro lled by users to decide the degree of alterations of these sensitive transactions. How to choose the sensitive transactions and how to choose the victim items from the sensitive transactions are the two most important issues in it. In [12], a heuristic approach, Sli ding Window Algorithm \(SWA in association rules mining. It hides association rules by decree sing their supports. Additionally, it has better performance than the framework proposed in [10]. However, the approaches prop osed in [10][11][12] all suffer from Forward-Inference Attack F-I Attack  is that if a pattern is hidden in a set of modified patterns which has to be published, but all sub-patterns of the pattern are still frequent in the set, then the attackers can infer that the pattern is 


frequent in the set, then the attackers can infer that the pattern is hidden painstakingly. To avoid F-I Attack problems, basing on the solution discussed in [13], at least one sup-pattern with leng th 2 of the pattern should be removed or the hiding pattern will be inferred recursively. However, they only modify the frequent patterns and don  t consider how to modify the original database In [8], the idea of using correlation matrix for hiding sensitive patterns is introduced. However, only the maximal patterns are considered in the work. In [17], the authors investigate confide ntiality issues of a broad category of association rules and prese nt several strategies for hiding the rules. Although the algorithms ensure privacy preserving, they may modify true data values and relationships by adding new items into the original transactions In this paper, we propose a novel method for modifying dat abase to hide sensitive patterns. By observing the relations betw een sensitive patterns and non-sensitive patterns, a sanitization matrix is defined. By setting the entries to appropriate values and multiplying the original transaction database to the sanitiza Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE tion matrix, a sanitized database which can resist F-I Attack is gotten. The sanitized database is the database that has been modified for hiding sensitive patterns with some privacy conc erns. Moreover, we use some probability policies with a level of confidence given by users to against the recovery of sensitive patterns and reduce the modifications of the sanitized database The rest of this paper is organized as follows. In Section 2 the basic concepts of our approach are introduced. The saniti zation process is discussed in Section 3. The simulation result is shown in Section 4. Finally, Section 5 concludes our work 2. Preliminary 2.1. Problem Definition The problem of discovering association patterns is defined as finding relations between the occurrences of items within transactions [2]. For example, an association pattern might be  bread, milk; support = 10  which means there are 10% of the transactions contain both items. In the association patterns each pattern should have a measure of certainty associated with it that accesses the validity of the pattern. It is called support The support of an association pattern refers to the percentage of task-relevant transaction for which the rule is true. Therefore minimum support is defined to be the minimum threshold for an association pattern to be meaningful. A frequent pattern is the association pattern that satisfies the minimum support In this approach, a transactional database is represented as a binary matrix D where the rows represent transactions and the columns represent items. Entry Dij is set to 1, if item j is purch ased in transaction i and 0, otherwise. The problem of hiding sensitive patterns can be formulated as follows, let P be the set of all frequent patterns mined from D except the patterns with length 1, PH be the set of sensitive patterns, ~PH be the set of remainder frequent patterns \(non-sensitive patterns PH = P. Our approach is to transform D into a sanitized D such that only the patterns belong to ~PHcan be mined from D Moreover, the patterns belong to PH will never suffer from F-I Attacks discussed in the previous section 2.2. Basic Concepts of Sanitization Matrix In our approach, D is multiplied by a sanitization matrix S to get D'. That is, D'n  m = Dn  m  Sm  m. If S is an identity matrix i.e., Sij = 1 if i = j, otherwise, Sij = 0 setting Sij where ji z to the appropriate value, D' will be gotten In the following, the basic concept of our approach is discussed 2.2.1 New Definition of Matrix Multiplication. In order to fit properties of transactional databases, some different definitions of matrix multiplication are given as follows 1. If Dij = 0, D'ij is set to 0 directly. Because our goal is to hide sensitive patterns by decreasing their supports, therefore, only need to take care of how and when an entry with its value equal to 1 in D should be converted to 0 in D'. Moreover, it also guarantees that there are no new artificial patterns created 


also guarantees that there are no new artificial patterns created by the sanitization process 2. If the value of    m k kjik SD 1 is not smaller than 1, set D'ij to 1 3. If the value of    m k kjik SD 1 is not larger than 0, set D'ij to 0 2.2.2. Setting of  1  To hide a pattern {i, j}, we have to decrease its support. For example, if Dki and Dkj are both equal to 1 for some k, we can re- place the value of Dki or Dkj by 0 to decrease the support of {i, j}. If a sufficient amount of such entries could be replaced by 0, {i, j} will no longer be a frequent pattern. Refer to Fig. 1, if S21 is set to  1, D'21 and D'41 will become 0, the support of item 1 is decr- eased; if S12 is set to  1 D'22 and D'42 will become 0, the support of item 2 is decreased Therefore, the support of {1, 2} can be decreased by setting S21 or S12 to  1. Moreover, if Sij is set to  1, for the row that Dti and Dtj both equal 1, D'tj will become 0  110 100 010 100 100 011 001 111 100 011 100 34 33 34 DSD u u u                       


   u              101 100 001 100 100 010 011 111 100 011 100 34 33 34 DSD u u u                          u            


  Fig.1: Effect of setting  1 in S 2.2.3. Setting of  1  Setting entries to  1 in S can reduce the support of sensitive patterns; however, it may also lead to conceal non-sensitive patt- erns. This kind of conditions can be solved by setting proper ent- ries to 1 in S. Consider the example in Fig2, let minimum supp- ort be 50%, {1, 2} and {1, 3} be the sensitive and non-sensitive pattern respectively. Refer to the equation on the left-hand side, {1, 2} and {1, 3} are both hidden in D'. However, if S31 is set to 1, those entries that Dt1 and Dt3 are both equal to 1, D't1 will keep the same value as Dt1. Therefore setting Sij to 1 can keep the relation between item i and item j by enhancing the strength of j  110 100 010 101 100 011 001 111 100 011 101 34 33 34 DSD u u u                         u             


  111 100 010 101 101 011 001 111 100 011 101 34 33 34 DSD u u u                         u             Fig.2: Effect of setting 1 in S 3. Sanitization Process PH Sanitization Matrix S Probabilities for each column of S SDD u PH large2 Marked-Set Setting Sanitization Matrix Setting Sanitization Algorithm 


Sanitization Algorithm Sanitized Database D Original Database D S Fig.3: The flowchart of the sanitization process Fig.3 shows the flowchart of the work. Each of the compon Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE ents will be discussed in this section 3.1. Sanitization Matrix Setting As discussed in section 1, to avoid F-I Attack, for each sensitive pattern P in PH, at least one pattern belong to the pair-subset of P should be hidden or the attacker can infer from the subset with length 2 to the sensitive pattern recursively. The pair-subset is defined as follows Definition 1: Let F be a frequent pattern. The pai-subpattern of F is a sub-pattern of F with length 2. The set which includes all pair-subpatterns of F is called the pair-subset of F For example: if {1, 2, 3} is a frequent pattern, then {1, 2} is a pair-subpattern of {1, 2, 3}. Moreover, {{1, 2}, {1, 3}, {2, 3}} is the pair-subset of {1, 2, 3}. Assume that {1, 2, 3} is sensitive, in order to avoid F-I Attack, at least one of the elements which belong to {{1, 2}, {1, 3}, {2, 3}} need to be hidden Fig.4: The algorithm of Marked-Set Generation In our work, a temporary set, Marked-Set, is used to store the victim pair-subpatterns. A victim pair-subpattern is the pair subpattern selected from the pair-subset of a sensitive pattern and used to be hidden. After setting up Marked-Set, all patterns in Marked-Set are used to set the related entries to  1 in S. The Marked-Set Generation is showed in Fig.4 In step 2, the remainder patterns whose pair-subpatterns are not stored in Marked-Set are used to generate groups for finding suitable pair-subpatterns. Because the patterns belong to ~PH sh ould not be affected, the selection of victim pair-subpatterns sho uld take the patterns in ~PH into account. In step4, the frequenc ies of the pair-subpatterns of the non-sensitive patterns are calcu lated. If the frequency of a pair-subpattern appearing in the pair subsets of the non-sensitive patterns is small, the number of patt erns in ~PH affected by hiding this pair-subpattern is also small Therefore, in step7, the pair-subpatterns with smaller frequenc ies are chosen to put in Marked-Set. In step 6, we sort the groups by frequency in increasing order to aggregate the patterns in the groups with small frequency; therefore fewer patterns in ~PH may be affected. In step3~7, the groups are merged according to the patterns stored in it, such that the sensitive patterns can be hi dden by hiding a common pair-subpattern at one time. Moreover the dissimilarity between D and D' can also be reduced After getting the Marked-Set, S can be set as Fig. 5 Fig.5: The algorithm of Sanitization Matrix Setting In step 2, because all patterns stored in Marked-Set are need ed to be hidden, thus, the item with smaller effect on the patterns in ~PH is chosen to be a victim item. A victim item is an item whose support is selected to be decreased to hide the pattern stored in Marked-Set. When the effect on the patterns in ~PH is the same, a victim item is chosen according to the number of times it appeared in Marked-Set. This is because choosing the more frequent one can decrease supports of many patterns which belong to Marked-Set at one time and reduce the Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE dissimilarity between D and D'. In step 3, the correlations of the patterns belong to ~PHare enhanced by setting the related entries in S to 1 3.2. Probability Policies 3.2.1. Distortion ProbabilityU . By setting  1 in sanitization matrix, the supports of the pair-subpatterns belong to PH can be decreased. However, refer to Fig.6, it brings the following 


decreased. However, refer to Fig.6, it brings the following problem  110 100 010 100 100 011 001 111 100 011 100 34 33 34 DSD u u u                         u              101 100 001 100 100 010 011 111 100 011 100 


100 34 33 34 DSD u u u                          u             Fig.6: Over Hiding problem of setting  1 in S No matter the left-hand or right-hand equation, the support of {1, 2} in D' is 0. That is, item 1 and item 2 never appear toge ther, and they are mutual exclusive! This situation almost never happens in the normal database. The attackers may interest in this situation and infer that {1, 2} is hidden deliberately. To hide the sensitive patterns, only need to make their supports smaller than minimum support and need not to decrease their support to 0. To solve the problem, we inject a probability ? which is called Distortion probability into this approach. Distortion probability is used only when the column j of the sanitization matrix S contains only one  1  i.e. Sjj = 1 0 1 d   m k k j i k  S D  m j n i j i  d d d d   1  1     D  i j  h a s   j probability to be set to 1 and 1  j probability to be set to 0 Lemma 1: Given a minimum support ?, and a level of confidence c. Let {i, j} be a pattern in Marked-Set, nij be the support count of {i, j}. ? is the Distortion probability of column j Without loss of generality, we assume that Sij  1. If ? satisfies    D n i j  u  u  V U   a n d    


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


