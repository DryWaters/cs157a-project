Prediction of Protein Folding Using the Shift-Learn Method with a Large Scale Neural Network Marius 0 Poliac George L Wilcox Yiyi Xin Tidhar Carmeli and Michael Liebman Minnesota Supercomputer Insititue 1200 Washington Ave S Minneapolis MN 55415 USA Abstract encoding the association between protein sequence and three dimensional structure for a small 
heterologous training set of small proteins In the present study we report the application of this approach to a selected homologous training set of 8 proteins using the Cray 2 supercompter at the Minnesota Supercomputer Center, Minneapolis USA The large memory of this machine allowed us to configure a network with more than 3 million connections and 30.000 
neural units a network of this size was necessary to accommodate a new training/testing set with 8 proteins of up to 140 amino acid residues This training set was constructed to investigate the performance of the neural network approach in prediction of structures within the protease class of proteins; proteases are enzymes which cleave the peptide bonds which join 
individual amino acid residues of other proteins The network learned the sequence-structure association for 4 of the proteins within 100 iterations selected in a random order and shifted by a random offset to the left or to the right When presented with novel sequences from related proteins, the network was able to predict three dimensional structures of the four proteins in the testing set The results of this study suggest 
that a neural network trained to recognize the entire sequence of a protein using the shift-learn method can retain some of the rules of protein folding in a form which allows prediction of three dimensional structures Our findings indicate that large scalar or vector supercomputer architectures are ideal for implementation of useful backpropagation neural networks Introduction the amino acid sequence of a protein apparently determines this 
structure completely The mechanism by which a protein\222s sequence determines its structure is not yet understood and numerous previous attempts to predict how a protein folds have failed Early mechanical or physical-chemical approaches Levitt and Warshel 1975 Chou and Fasman 1978 Tanaka and Scherraga 1978 could predict secondary structure with only 60 accuracy, and tertiary structure was inaccessible Later neural network approaches were similarly inaccurate \(Qian 
and Sejnowski 1988 Holley and Karplus 1989 One recent study attempted to apply a fairly large backpropagation neural network to learn sequence to secondary structure mapping for the serine protease family of proteins \(Bohr et al 1990 They encoded secondary structure as the distances from each amino acid on the polypeptide backbone to nearby amino acids the near-diagonal elements of their distance matrices \(see methods below Limitations in computer memory 
and time limited their attention to these near-diagonal elements of distance matrices relating mostly to secondary structure the local elements of protein folding Our approach by contrast involves We have demonstrated in previous studies the utility of large neural network simulations for The three dimensional structure of a protein is intimately related to its biological function and CH 3065-0,91/0000-1323 1.00 OIEEE 


applying a very large neural network to learn entire distance matrices which encode tertiary as well as secondary structure elements Wilcox et al 1991 We have applied a large neural network simulation called BigNet implemented on a four processor Cray 2 in an attempt to learn associations between sequences and structures of a group of proteins with known structure The structures are derived from the Bmkhaven Protein Data Bank PDB\protein structure database We have found that after several hundred learning iterations through a training set of 15 to 20 small proteins, training runs routinely converge to solutions where inputs of sequence coded by amino acid hydrophobicity yield output structure descriptions with less than 0.1  RMS deviation from actual structures Most recently we have seen evidence of generalization after presentation of four testing proteins with known structure but with sequences which are new to the network the trained network can correctly classify all of the novel sequences into one of four families and can accurately predict secondary and tertiary structures for the novel sequences \(Wilcox, Xin, Carmeli and Liebman unpublished This performance was obtained with a heterologous training set of 20 small proteins The present study is investigating the shift-learn approach to a homologous training set of only 8 proteins most of which belong to a family called the serine proteases Method Our simulation environment consists of 1 a neural network simulator called BigNet, which is capable of handling backpropagation networks of 100 million connections at speeds of 0.5 million 64-bit connections per second using a single Cray 2 processor and 2 a Network Description Language \(NDL which allows for the flexible design of backpropagation neural networks NDL allows easy definition of a generalized back propagation network, provides a convenient means for conducting complex simulation experiments and permits the user to construct various input and output data presentation formats The major NDL commands include the following functions layer defines a layer of nodes; connection defines a connection \(alias edge attaches a file to a layer or connection load loads a layer or a connection save saves a layer or a connection 10adConnections loads all connections alias 1oadEdges saveConnecrions saves all connections alias saveEdges learn initiates a simple learning session shifrLearn learns f\224 inputs or outputs which shift within the respective windows IearnFrom uses a list of files to build a training set which will be scanned sequentially or randomly though multiple itmtionspropagate propagates the input to the output through the existing weights shiftpopagate generates input arrays which have been shifted in the input window\array constructs a multidimensional array for input output or hidden layers Learning takes place during each learning iteration as each weight in the network is changed in such a way as to minimize the error at the output layer The change imposed on a weight at each learning iteration is determined on the basis of measured output errors difference between desired and actual outputs the values of nodes propagating through the connection and a parameter called e We have found that setting e to 0.1 minimizes inter-iteration jumps and gives best results for the complexity of our training set Each learning iteration consists of two distinct phases characteristic to the backpropagation algorithm In the first phase the state of the input layer given by its attached file is propagated to the output layers using the current values of the weights of each connection During the second phase the state of the output layers is compared to the desired output stored in the file attached to this layer Then the weights are adjusted after each item in the training set is presented in such a way that the mean squared output error is minimized if this process were conducted over the entire training set at once, the result would be very similar to multiple linear regression with several million coefficients In effect each learning iteration on each item in the training set changes the weights in such a way that the newly created network will better match the input pattems stored in the files attached to the input layer with the output pattems stored in the 1324 


files attached to the output layer Learning is the most potentially time-consuming aspect of backpropagation learning This paper focused on a training set of 4 proteins which were presented to the network in random order and a testing set of 4 related proteins The sequence and structure data were derived from the Protein Data Bank \(PDB files for each protein For brevity, each protein is identified here using its four letter PDB code Four proteases 2122 Scyt lpzp, lazu were submitted as a training set with inputs shifted by a variable number of positions and keeping the outputs fixed Four related proteases 41yz lccr 3bp2.2aza were included in the test set The input or bottom layer of the network consisted of 140 units whose value was supplied by an input array describing the sequence of the protein in question; the sequences, ranging from 120 130 residues in length were always registered in the 140 unit input layer We have used one 223alphabet\224 to describe the amino acids hydrophobicity values adapted from Liebman et al 1986 that ranged from 3.4 for the hydrophobic amino acid tyrosine to 3.3 for the hydrophilic amino acid lysine and were normalized into the range of fl described in Wilcox et al 1991 Since several amino acids have almost identical hydrophobicities this alphabet is degenerate We have chosen this alphabet for initial use because hydrophobicity may represent an important physico chemical interaction driving protein folding \(Eisenberg et al 1984\and because the neural network could be configured most economically if continuous values were used instead of name like categories The output or top layer of the network used in this study constitutes a window for distance matrices and is composed of 29,600 units \(a 140 by 140 unit two-dimensional array whose values are determined by forward propagation i.e they depend on the sums of all the weighted inputs from lower layers The three dimensional structure of each protein from the PDB was transformed into this two dimensional rotation-independent representation to facilitate presentation to the network Essentially, the distance from each amino acid in each protein to every other amino acid is calculated from the alpha carbon coordinates in the PDB and placed in each element of the distance matrix; the distance matrix represents a \223finger print\224 of each protein that preserves much of the three dimensional structure of the protein in an easily recognized, easily memorized form An accurately pred~cted distance matrix for a protein would describe much of the detail of how the protein folds locally in secondary structures such as alpha helices, and globally in supersecondary and temaq structural elements These distances were normalized into the range of 0 1 by dividing by the maximum distance in the training set 90 A The unit-by-unit differences between this output layer and the superimposed target pseudo-layer Le the actual distance matrix of the protein whose sequence is encoded in the input layer are calculated for each learning iteration these differences are used to determine the error propagated back to alter weights connecting the lower layers to the upper layers Rummelhart et al 1986 Results time on the Cray 2 The network of 29,600 output nodes and 60 to 90 hidden units contained more than 3 million connections Because the value of each connection was held in one 64 bit floating point word the problem required almost 3 million bytes of central memory One hundred learning iterations through the training set of 8 proteases required 1.2 billion cumulative updates of inter-unit connection weights Using the central processor in scalar, double precision mode with BigNet compiled using the standard C language compiler delivered a speed of 437,000 connection updates per second this learning speed is the most meaningful benchmark for backpropagation networks because the programs spend most time in the learning mode The speed of forward propagation the act of producing an output from an input through the existing weights is perhaps ten times this learning mode speed but the program spends relatively little time in this mode We report here the results of 12 learning and testing sessions which required 2 hours of CPU 1325 


We performed 4 experiments allowing an input shift of fl with a hidden layer ranging from 60 hidden units to 90 hidden units The best performance in learning and generalization was obtained using the network with 80 hidden units The results are plotted on a graph representing the performance of the network on the learning set in the forefront of each graph and the test set in the backpund We performed two more sets of 4 experiments allowing for an input shift of f2 and 3 respectively The best overall performance was obtained with an input shift of f2 and 80 hidden layers The graphs are presented at the end of this paper BigNet performed well in acquisition of this new, homologous training set of larger proteins using roughly the same number of hidden units required for our previous heterologous training set of smaller proteins \(Wilcox et al 1991 Associations between protein sequence and structure in the training set were learned to better than 99 precision within 100 iterations Our research indicates that for 8 families of small 440 amino acids proteins for which 4 members were included in the training set, testing with the sequence f\224 the remaining test set of 4 proteins redled the correct Acknowledgements King of the Minnesota Supercomputer Center Inc and of Joseph Habermann of the Minnesota Supercomputer Institute MSI for their help in development of graphic display programs References Bohr H Bohr J Brunak S Cotterill R.M Fredholm H Lautrup B Petersen S.B 223A structure to 93 accuracy The authors gratefully acknowledge the expert technical assistance of Ken Chin--el and Bill novel approach to prediction of 3-dimensional structures of protein backbones by neural networks\224 FEBS Lett 1990 261:43-46 Chou P and Fasman G.D 223Prediction of the secondary structure of proteins from their amino acid seqence\224 Ah Enz 1978.47 45-147 Eisenberg D Schwan E Komaromy M Wall R 223Analysis of membrane and surface protein sequences with the hydrophobicity moment plot\224 J Mol Biol 1984 179:125-142 Holley L.H Karplus M 223Protein structure prediction with a neural network\224 Proc Nut Acd Sei USA 1989 86 152-156 Levitt M Warshel A 223A computer simulation of protein folding\224 Nature London 1975,253 694-698 Liebman M.N 223Molecular modeling of protein structure and function a bioinfcnmatic approach\222\222 Liebman M.N Venanzi C.A Weinstein H 223Structural analysis of carboxypepti&se A and its J Comput Aided Mol Des 1987 1 323-341 complexes with inhibitors as a basis for modeling enzyme recognition and specificity\224 Biopolymers 1985 24:1721-58 Qian N Sejnowski T.J 223predicting the secondary structure of globular proteins using neural network models\224 J Mol Biol 1988.202 865-884 Rummelhart D.E Hinton G.E Williams R.J 223Learning representations by error propagation\224 In Parallel Distributed Processing vol 1 1986 pp 3 18-362 MIT Press Cambridge MA Tanaka S Scheraga H.A 1976 223Medium- and long-range interaction parameters between amino acids for predicting 3D structures of proteins\224 Macromolecules 1976,9 945-950 Wilcox GL Poliac MO and Liebman MN 1991 in press 221\221Neural Network Analysis of Protein Tertiary Structure\224, Tetrahedron Computer Methodology 1326 


STANDARD DEVIATION OF LEARNING PROGRESSION shifted by 1.60 hidden units STANDARD DEVIATION OF LEARNING PROGRESSION shifted by 1,70 hidden units STANDARD DEVIATION OF LEARNING PROGRESSION shifted by 1 80 hidden units 1327 STANDARD DEVIATION OF LEARNING PROGRESSION shifted by 1 90 hidden units 


STANDARD DNlATlON OF LEARNING PROGRESSION shifted by 2.60 hidden units STANDARO DEVIATION OF LEARNING PROGRESSION shifted by 2 70 hklden units STANDARD DEVIATION OF LEARNING PROGRESSION shifted by 2.80 hidden units STANDARD DEVlAllON OF LEARNING PROGRESSION shifted by 2.90 hidden units 1328 


STANDARD DEVIATION OF LEARNING PROGRESSION shined by 3,60 hidden units STANDARD OEVlATlON OF LEARNING PROGRESSION shined by 3.70 hidden units STANDARD DEVIATION OF LEARNING PROGRESSION shifted by 3 80 hidden units STANDARD DEVIATION OF LEARNING PROGRESSION shifted by 3 90 hidden units 1329 I 


ter performance for the SQL implementation through query optimization The SQL implementation provides a convenient method for using the optimization and parallelization capabilities in the database system for the bene\002t of the application It also provides a scalable implementation not memory bound for the algorithm Our work indicates that mining using the database engine is a feasible proposition However it requires careful attention to the data model and access pattern and suitable extensions to the relational database system to provide the new functionality objectrelational extensions such as UDFs and Table Functions required by data mining applications References 1 R  A g r a w a l  T  I m i e lin s k i a n d A  S w a m i  M in in g A s s o c i ation Rules between Sets of Items in Large Databases In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data  pages 207\226216 May 1993 2 R  A gr a w al and K  S hi m  D e v e l opi ng t i ght l y c oupl ed dat a mining applications on a relational database system In Proceedings of the 2nd International Conference on Knowledge Discovery in Databases and Data Mining  August 1996 3 R  A gr a w al and R  S r i kant  F ast A l gor i t h m s f o r M i n i n g A ssociation Rules In Proceedings of the 20th International Conference on Very Large Databases  September 1994 4 P  B oncz W  Q u ak a nd M  L K e r s t e n M onet a nd its Geographical Extensions a Novel Approach to HighPerformance GIS Processing In Proceedings of the 5th International Conference on Extending Database Technology  March 1996 5 D  C h a m b e r lin  Using the New DB2 IBM's ObjectRelational Database System  Morgan Kaufmann 1996 6 G  C o p e la n d a n d S  K h o s h a 002 a n  A D e c o m p o s itio n S to ra g e Model In Proceedings of the 1985 ACM SIGMOD International Conference on Management of Data  May 1985 7 D B 2 U n i v er sal D at abase  U D B   http//www.software.ibm.com/data/db2 Web Document 8 M  G yssens L  L akshm a nan and I  S ubr am ani a n T a bl es as a Paradigm for Querying and Restructuring In Proceedings of the 1996 ACM Symposium on Principles of Database Systems  June 1996 9 J  H an Y  F u K  K oper s ki  W  W ang and O  Z ai ane A Data Mining Query Language for Relational Databases In Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data  May 1996  M  H o l s hei m er and M  L  K er st en A r c hi t ect ur al Suppor t for Data Mining In International Workshop on Knowledge Discovery in Databases Sea ttle  1994  M  H out sm a a nd A  Sw am i  Set or i e nt ed M i ni ng of A ssociation Rules Technical Report RJ 9567 IBM Almaden Research Center October 1993  T  I m i e l i n ski  A  V i r m ani  and A  A bdul ghani  A ppl i cat i o n Programming Interface and Query Language for Database Mining In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining  August 1996  I n t e r n at i onal B usi n ess M achi n es IBM Intelligent Miner User's guide  Version 1 Release 1 SH12-6213-00 ed ition July 1996  B  R  I y er and D  W i l h i t e  D at a C om pr essi on Suppor t i n Databases In Proceedings of the 20th International Conference on Very Large Databases  September 1994  L Lakshm anan F  S adr i  a nd I  Subr am ani a n Schem a SQ L A Language for Interoperab ility in Relational Multidatabase Systems In Proceedings of the 22nd International Conference on Very Large Databases  1996  R  M e o G  Psai l a  a nd S C e r i  A N e w S Q L l i k e O per a t o r for Mining Association Rules In Proceedings of the 22nd International Conference on Very Large Databases  1996  K  R a j a m a ni  B  I yer  and A  C hadha U si ng D B 2 s O bj ect Relational Extensions for Mining Association Rules Technical Report TR 03,690 Santa Teresa Laboratory IBM Corporation September 1997  B  R e i n w a l d and H  P i r ahesh SQ L O pen H et er ogeneous Data Access In Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data  June 1998  S Sar a w a gi  S  T hom as a nd R  A g r a w a l  I n t e gr at i n g A ssociation Rule Mining with Relational Database Systems Alternatives and Implications In Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data  June 1998  J Shaf er  R  A gr a w al  a nd M  M e ht a SPR I N T  A S cal abl e Parallel Classi\002er for Data Mining In Proceedings of the 22nd International Conference on Very Large Databases  September 1996  D  Tsur  J  D  U l l m an S  A bi t e boul  C  C l i f t on R  M o t w ani  S Nestorov and A Rosenthal Query 003ocks a Generalization of Association-rule Mining In Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data  June 1998  M  Zaki  S  P ar t h asar at hy  W  L i  and M  O gi har a  N e w A l gorithms for Fast Discovery of Association Rules In Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining  August 1997 


a Computing view b query graph t f ivot method Figure 5 pivot method query graphs re\337ecting the produced columns of the select clause contributes with a value of pc 004 8 to the overall cost resulting in 14 004 pc 6 004 pt  5.2 Comparing different Methods The cost of computing the nearest neighbour using the t method may be split into the computation of the cost for the view de\336nition to perform the t of the prototype information and the core select statement using the t view As already mentioned the pt 212 1 ary join of the prototype table is easily optimized by the existence of an index of the prototype ID In this case each join partner has the size of 1 004 1 d  yielding pt 004 1 d  as the overall cost computing the view without any return operator The query itself joins the view with the PConformations table Reading the result of the view corresponds to one single tuple with pt 004 1  d  columns The access of the PConformations table has the size of pc 004 1  d   The outer query adds to the overall costs with reading the result of the inner query  pc 004 1  d  004  pt 1  and accessing the Prototypes table  pt 004 1  d   The return operator 336nally consumes a data stream of cardinality pc with d 3 attributes The cost of discretization using the self-join method can be computed in three steps The 336rst step considers the computation of the view P rototypeAssignment which requires the access to the two tables P roteinConf ormation  pc 004 1  d   and P rototypes  pt 004 1  d   The resulting data stream which has to be read for further processing yields due to the semantics of the Cartesian product to  pc 004 pt  004 1  d 2  The second step addresses the inner query consuming exactly the size of the view and producing a data stream of pc 004 2 for only two columns The outer query reads the result of the inner query and the result of the view   pc 004 pt  004 3  d   At last the return operator has costs the size of the join operator yielding pc 004  d 3  In a similar y the cost for the minpos  method can be computed Instead of the join with the P rototypeAssigment view in the outer query the P roteinConf ormation table with a cost of pc 004  d 1 a Computing view b query graph PrototypeAssignment of self join method Figure 6 self-join method query graphs Figure 7 cost reduction scenario is referenced Additionally the view must be executed only once further reducing the cution costs Table 1 summarizes the partial and total cost for all different methods computing the prototype for each amino acid residue Since this tabular and formular-based representation does not give any hint about the best strategy 336gure 7 gives a scenario with four different dimensions i.e d 1  4  7  10  and 5000 amino acid residues The scenario shows the resulting performance gain of the pivot method compared to minpos  and selfjoin method with 25 100 1000 and 2500 prototypes Within the proposed cost model the t method yields a slightly lower total cost than the minpos  method because the pt 212 1 ary selfjoin is considered extremely cheap r due to the dependency of the total cost from the number of prototypes the t method can not be considered a feasible solution for real applications with a reasonable high number of prototypes Compared to the self-join method the minpos  method yields a substantial cost reduction 6 Summary and Conclusion This paper introduces the problem of 336nding frequent substructures in protein data sets The analysis process is split into two parts The 336rst step consists in 336nding the Proceedings of the 15th International Conference on Sci entific and Statistical Database Management \(SSDBM\22203 1099-3371/03 $17.00 \251 2003 IEEE  


 t method view execution pt 001 1  d   f database operations inner query pt 001 1  d  pc 001 1  d   pt 1 joins outer query pc 001 1  d  001  pt 1 pt 001 1  d  pc 001  d 3 total cost pt 001 3  5 d  pc 001 5  3 d  pc 001 pt 001 1  d  self-join method view execution 2 001  pc 001 1  d  pt 001 1  d   f database operations inner query  pc 001 pt  001 3  d  1 join outer query pc 001 2 pc 001 pt  001 3  d  pc 001 3  d  2 cross product total cost pt 001 2  2 d  pc 001 7  3 d  pc 001 pt 001 6  2 d  1 group by minpos method view execution pc 001 1  d  pt 001 1  d   f database operations inner query  pc 001 pt  001 3  d  1 join outer query pc 001 2 pc 001 1  d  pc 001 3  d  1 cross product total cost pt 001 1  d  pc 001 7  3 d   pc 001 pt  001 3  d  1 group by Table 1 Cost comparison of the different methods nearest prototype in the multi-dimensional dihedral angle space To accomplish this task the data sets are brought into a relational schema and a method is proposed to compute the minimal distance considering the wrap-around effect in the angle space Three different methods to 336nd an associated prototype inside the database systems are compared A minimal SQL extension  minpos  maxpos  function results in much more ef\336cient query execution plans The second step of generating frequent item sets to detect frequent substructures within the amino acid sequences requires substantial SQL extension A ew operator as a new member of the OLAP grouping function operators is introduced This operator is a generic tool and may be ploited by a huge set of data mining applications To summarize a database system used to ef\336ciently analyse huge data volumes requires additional support from the technology  The required extension range from minimal UDFs like our proposed minpos  maxpos functions to more complex operators like our proposed grouping combinations operator f and only if the database community provides this kind of functionality the acceptance of database systems in the biotechnology community will increase in the near future References  R Agra w al T  Imielinski and A N Sw ami Mining association rules between sets of items in large databases In Proceedings of the International Conference on Management of Data  pages 207\320216 ACM Press 1993  S Bohl M Dink elack er  J  Griese and S Schrader  Highly adaptable amino acid side chain rotamer library in pdb coordinates In Workshop in Computational Biology at the Plant Biochemistry Department of the Albert-Ludwigs-Universitt Freiburg Germany  2002  M Bo wer  F  Cohen and R Dunbrack Homology modeling with a backbone-dependent rotamer library J Mol Biol  267:1268\3201282 1997  R Chandrasekaran and G Ramachandran Studies on the conformation of amino acids xi analysis of the observed side group conformations in proteins Int J Pept Prot Res  2:223\320233 1970  R Dunbrack and F  Cohen Bayesian statistical analysis of protein side-chain rotamer preferences Protein Sci  6:1661\320 1681 1997  J Gray  A  Bosw orth A Layman and H Pirahesh Data cube A relational aggregation operator generalizing groupby cross-tab and sub-total In Proceedings of the Twelfth International Conference on Data Engineering  pages 152\320 159 IEEE Computer Society 1996  A Hinneb ur g M Fischer  and F  Bahner  Finding frequent substructures in 3d-protein databases In Workshop on Bioinformatics at the 19th International Conference on Data Engineering  IEEE Computer Society 2003  M James and A Sielecki Structure and re\336nement of penicillo-pepsin at 1.8 a resolution J Mol Biol  125:299\320 361 1983  J K usze wski A Gronenborn and G Clore Impro ving the quality of nmr and crystallographic protein structures by means of conformational database potential derived from structure databases Protein Sci  5:1067\3201080 1996  S C Lo v ell J M W ord J S Richardson and D C Richardson The penultimate rotamer library Proteins Struct Funct Genet  40:389\320408 2000  M MCGre gor  S  Islam and M Sternber g Analysis of the relationship between side-chain conformation and secondary structure in globular proteins J Mol Biol  198:295\320310 1987  R Srikant and R Agra w al Mining generalized association rules In VLDB\32595 Proceedings of 21th International Conference on Very Large Data Bases Switzerland  pages 407\320 419 Morgan Kaufmann 1995  M J Zaki Ef 336cient enumeration of frequent sequences In Proceedings of the 1998 ACM CIKM International Conference on Information and Knowledge Management Bethesda Maryland USA November 3-7 1998  pages 68\32075 ACM 1998  M Zhang B Kao C L Y ip and D  W L Cheung Ffs an i/o-ef\336cient algorithm for mining frequent sequences In Knowledge Discovery and Data Mining PAKDD 2001 5th Paci\336c-Asia Conference Hong Kong China April 16-18 2001 Proceedings  volume 2035 of Lecture Notes in Computer Science  pages 294\320305 Springer 2001 Proceedings of the 15th International Conference on Sci entific and Statistical Database Management \(SSDBM\22203 1099-3371/03 $17.00 \251 2003 IEEE  


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


