Proceedings of the Fourth International Conference on Machine Learning and Cybernetics Guangzhou 18-21 August 2005 AN ALGORITHM FOR MINING STRONGLY CORRELATED PAIRS IN RELATIONAL TABLE JIAN-PEI ZHANG QIANG LI School of Computer Science and Technology Harbin Engineering University Harbin 150001,China E-MAIL zhangjianpeiO lsina.com wqq9930\(1 63.com Abstract Given a user-specified minimum correlation threshold and a relational table the problem of mining all-strong correlated pairs is to find all attribute value pairs with Pearson's correlation coefficients above the minimum correlation threshold However algorithms developed 
for transaction database will generate invalid candidate pairs due to fundamental property of the itemsets in relational table i.e 1NF they cannot contain more that one item per table column and hence encounter additional and unnecessary computation cost In this paper using this property the join step in the candidate generation phase is adapted to reflect this and to prune candidate set by not taking into itemsets which are not in 1NF Furthermore we propose other techniques to reduce the number of candidate pairs that are 
to be examined in the refinement step even when the upper bound based pruning technique is useless in case of very low correlation threshold Experimental results from real data sets exhibit that our algorithm can produce smaller candidate set and be faster than previous algorithms Keywords Correlation Association Rule Transactions Relational Table Data Mining 1 Introduction More recently the mining of statistic correlations is applied to transaction databases Ill which retrieves all pairs of items with high positive correlation in a 
transaction databases The problem can be formalized as follows 1 Given a user-specified minimum correlation threshold 0 and a transaction database with N items and T transactions all-strong-pairs correlation query finds all item pairs with correlations above the minimum correlation threshold 0 In this paper we will make special emphasis on the application of correlation mining in relational databases since a large number of relational systems are now on the market and most distributed database systems are also relational This makes relational databases a good 
target for Data Mining 0-7803-9091-1/05/$20.00 2512005 IEEE The notion of item must be redefined in a relational database Henceforth an item will be a pair a v where a is an attribute a column in a relational table and v is the value of a A tuple t contains an item a v if its column a has value v A tuple t contains an itemset if it contains all the items in itemset Apparently after redefining the notion of item in a relational database 
we can directly utilize those algorithms developed for transaction database to find correlations in relational database However a fundamental property of the itemsets derived from a relational table is that they cannot contain more that one item per table column if al v and a2 V2 belong to an itemset then a 1 a2 This is a consequence of the First Normal Form 1NF a relation is in 1NF if its attribute domains contain atomic values only This property will allow us to prune the candidate set during itemset 
generation and justifies our distinction between items in transactional databases and items in relational databases Unfortunately the Taper Algorithm developed in 1 will generate invalid candidate pairs and encounter unnecessary computation cost since it didn't consider such fundamental property of the itemsets in relational table Based on the above observations in this paper we propose a new algorithm called TaperR to incorporate this property That is in the join step in the candidate generation phase we prune candidate set by 
not taking into itemsets which are not in INF Experiments with real-life data show that TaperR outperforms Taper 2 Related Work Association rule mining is a widely studied topic in data mining research e.g 2-4 However it is well recognized that true correlation relationships among data objects may be missed in the support-based association-mining framework To overcome this difficulty correlation has been adopted as an interesting measure since most people are interested in not only association-like co-occurrences 1631 


Proceedings of the Fourth International Conference on Machine Learning and Cybernetics Guangzhou 18-21 August 2005 but also the possible strong correlations implied by such co-occurrences Related literature can be categorized by the correlation measures Brin et al 5 introduced lift and a x2 correlation measure and developed methods for mining such correlations Ma and Hellerstein 6 proposed a mutually dependent patterns and an Apriori-based mining algorithm Recently Omiecinski r7i introduced two interesting measures called all confidence and bond Both have the downward closure property Lee et al 8 proposed two algorithms by extending the pattern-growth methodology 4 for mining all confidence and bond correlation patterns As an extension to the concepts proposed in 7 a new notion of the confidence-closed correlated patterns is presented in 9 And an algorithm called CCMine for mining those patterns is also proposed in 9 Xiong et al l0 independently proposed the h-confidence measure which is equivalent to the all confidence measure of 7 In 1 efficiently computing Pearson's correlation coefficient for binary variables is considered In this paper we focus on developing a FP-Tree base method for efficiently identifying the complete set of all-strong correlated pairs between items with Pearson's correlation coefficient as correlation measure 3 Background-Pearson's Correlation Coefficient In statistics a measure of association is a numerical index that describes the strength or magnitude of a relationship among variables Relationships among nominal variables can be analyzed with nominal measures of association such as Pearson's Correlation Coefficient The q correlation coefficient El is the computation form of Pearson's Correlation Coefficient for binary variables As shown in 1 when adopting the support measure 2]fotw of association rule mining  for two items A and B in a transaction database we can derive the support form of the 0 correlation coefficient as shown below in Equation 1 0 sup A B sup\(A  sup\(B 1 Vsup\(A  sup\(B  1 sup\(A  1 sup\(B where sup\(A sup\(B and sup\(A,B are the support of item\(s A B and AB separately Furthermore as shown in 1 given an item pair A B the support value sup A for item A and the support value sup B for item B without loss of generality let sup A B The upper bound upper\(q\(A,B of an item pair A B is upper\(\(AB supB  sup\(A 2 PP iF A B sup\(A 1sup\(B 2 As can be seen in Equation 2 the upper bound of 0 correlation coefficient for an item pair A B only relies on the support value of item A and the support value of item B In other words there is no requirement to get the support value sup A B of an item pair A B for the calculation of this upper bound As a result in the original Taper algorithm 1 this upper bound is used to serve as a coarse filter to filter out item pairs that are of no interest thus saving I/O cost by reducing the computation of the support value of those pruned pairs The Taper algorithm 1 is a two-step filter-and-refine query processing strategy which consists of two steps filtering and refinement In the filtering step the Taper algorithm applies the upper bound of the 0b correlation coefficient as a coarse filter In other words if the upper bound of the 0S correlation coefficient for an item pair is less than the user-specified correlation threshold we can prune this item pair right way In the refinement step the Taper algorithm computes the exact correlation for each surviving pair from the filtering step and retrieves the pairs with correlations above the user-specified correlation threshold as the mining results 4 TaperR Algorithm TaperR which stands for Taper for relation table employs the fundamental property of the itemsets derived from a relational table to further optimize the mining process 4.1 TaperR Overview TaperR algorithm follows the same philosophy as Taper algorithm l filtering and refinement In the join step for the candidate pair generation phase we prune candidate set by not taking into item pairs that are not in 1NF is of prime importance 4.2 TaperR in Detail The input for TaperR algorithm is a relational table Let A1  Am be a set of categorical attributes with domains D1 Dm respectively Let the relational table D  X1 X2  XJ be a set of objects described by m categorical attributes A1  Am The value set Vi of Ai is set of values of Ai that are present in D Suppose that the 1632 


Proceedings of the Fourth International Conference on Machine Learning and Cybernetics Guangzhou 18-21 August 2005 number of distinct attribute values of Ai is Pi  e.g Pi  1 The notion of item is redefined in a relational database Henceforth an item will be a pair a v where a is an attribute a column Ai in a relational table and v is the value of a A tuple t contains an item a v if its column a has value v A tuple t contains an itemset if it contains all the items in itemset The TaperR algorithm also is a two-step filter-and-refine query processing strategy which consists of two steps filtering and refinement While in the join step for the candidate pair generation phase before filtering and refinement we prune candidate set by not taking into pairs that are not in 1NF Later we use the same procedure as that of Taper In the following we will theoretically present the advantages of TaperR against Taper Lemma 1 Taper algorithm will produce m in Pi Pi-1 nnirz 6h.fnri filti-xino 2 Proof Since in the relational table there are m E pi items totally Hence the Taper joins all items to get i=1 m m Ipi Ypi 1 i=l i=l 2 candidate pairs Lemma 2 TaperR algorithm will produce m m  i i  Pi  Pi 1 V AU nniro 2 2   before filtering Proof Since the number of pairs that are not in 1NF is P1  i-1 Pi Pi_ And TpaerR prune candidate set by not taking into pairs in the relational table that are not in 1NF hence TaperR algorithm will produce m m pi  I pi 1 m 1=1 Pi Pi 1 I X I  I n 1r i=l 2 Obviously form Lemma 1 and Lemma 2 we can derive the fact that Taper algorithm will produce additional Pi 2\(Pi1 unnecessary candidate pairs than i 2 TaperR And the number of those unnecessary candidate pairs depends on the number of distinct attribute values of each attribute Ai When the relational table contains many attributes with large amount of attribute values the additional computation cost will be very high More importantly some of those invalid candidate pairs generated by Taper can't be filtered out with the upper bound of the 0 correlation coefficient Hence they have to be pruned in the refinement step which will increase the computation cost That is in the refinement step Taper has to examine more pairs than that of TaperR This fact is presented in lemma 3 Lemma 3 Let RI 0 and R2 0 be the number of item pairs that are not pruned before computing their exact correlations when the correlation threshold is 0  for Taper and TaperR respectively and let R3 0 be the number of item pairs that are not pruned before computing their exact correlations among the additional E  Pi_ P1 unnecessary pairs considered in Taper i=l then R 0  R2 0  R3 0 Proof Let S1 and S2 denote the set of pairs produced by Taper and TaperR before filtering respectively From Lemma 1 and Lemma 2 we know that S2 c SI and S1 2 S3 is the set of invalid pairs Hence RI 0  R2 0  R3 0 Lemma 3 indicates that the Taper algorithm not only has to examine additional 2  P i  pairs in filtering step but also needs to examine additional R3 0 pairs in the in the refinement step So far we have presented the advantages of TaperR against Taper from a theoretical perspective 4.3 Further Enhancements for TaperR The power for both Taper algorithm and TaperR algorithm comes from the filtering step that is if the upper 1633 2 before filtering pallN UClVlC IIILC;1111g If I I I III N Fall 


i i=l Proceedings of the Fourth International Conference on Machine Learning and Cybernetics Guangzhou 18-21 August 2005 bound of the 0 correlation coefficient for an item pair is less than the user-specified correlation threshold we can prune this item pair to reduce the computation cost in refinement step However the upper bound value upper\(\(A,B  X B  1-sup\(A is larger PP~Y\(A,B sup\(A I1-sup\(B than zero If the user-specified correlation threshold is very low for example 0.001 the upper bound based pruning technique will lose its power and hence we have to examine a large number of pairs in the refinement step Fortunately we can use the special characteristics of relation table to reduce the number of candidate pairs that are to be examined in the refinement step even when the upper bound based pruning technique is useless in case of very low correlation threshold Without loss of generality considering two attribute Ai and A1 with their value set Vi u1 u2  up}.and Vj v1 V2  IVq}.respectively From these two attributes we can generate p*q item pairs However this number is not necessary and can be reduced since we know sup Uk  sup Uk VI SUp Uk V2  SUp Uk Vq That is sup Uk Vq SUp Uk Uk VI SUp Uk V2 SUp Uk nVq41 In other words we can derive the support value sup Uk VQ form sup Uk sup Uk vI sup Uk v2 5  sup Uk vq-1 Taking one step further this fact means that it is not necessary to count the support of pairs that contain Vq via scanning database Hence it is enough to generate only p-i q4 item pairs for examination in the refinement step Lemma 4 shows how our new technique reduces the number of item pairs Lemma 4 In case no item pairs are pruned in the filtering step of TaperR algorithm we only needs to pi pim 1 generate i=1 i=1 Pi 1 2 2i=l 2 i=l j=i+l item pairs for examination in the refinement step Proof For each attribute pair Ai and Aj using our technique Pi p j-*\(pi-l pj P*-I item pairs are not generated for examination in the refinement step Considering all attribute pairs we totally m m have E E pi  p1 1 pair savings From Lemma 2 TaperR algorithm will produce j=i+l item pairs for examination in the refinement step As Lemma 4 shows even when the upper bound based pruning technique is ineffective in case of very small correlation threshold we can also reduce the number of candidate pairs to save computation cost in TaperR 5 Experimental Results A performance study has been conducted to evaluate our method In this section we describe those experiments and their results Two real-life datasets from UCI 12 were used to evaluate the effectiveness of our TaperR algorithm again Taper algorithm Our algorithms were implemented in Java All experiments were conducted on a Pentium4-2.4G machine with 512 M of RAM and running Windows 2000 5.1 Real Life Datasets We experimented with two real-life dataset the Mushroom dataset and Soybean dataset which was obtained from the UCI Machine Learning Repository 4 The mushroom dataset has 22 attributes and 8124 records Each record represents physical characteristics of a single mushroom A classification label of poisonous or edible is provided with each record The numbers of edible and poisonous mushrooms in the dataset are 4208 and 3916 respectively The soybean data set has 35 attributes Each instance is labelled as one of the four diseases Diaporthe Stem Canker Charcoal Rot Rhizoctonia Root Rot and Phytophthora Rot 5.2 Results Fig.1 and Fig.2 show comparison between Taper and TaperR on the number of item pairs examined as the correlation thresholds are increased Just as we have pointed out in Section 3 Taper algorithm always needs to examine more pairs than TaperR algorithm this fact is verified in Fig 1 and Fig.2 empirically 1634 in m Y pi pi P m 1 mm  2 pairs before filtering Hence we only need to generate m P  p ar  i  Pi   Pi  Pi-1 1=1 2 v m mi ____p i _______p i=1 D _zP Pi 1 I l\(pi  pi l 1 i=1 2 


paper execution time fbr both algorithms decreases as Proceedings of the Fourth International Conference on Machine Learning and Cybernetics Guangzhou 18-21 August 2005 8000 4000 S 2000 0 0.1 0 2 0.3 0.4 0 5 0 6 0.7 0.8 0 9 The minimum correlation thelsholds Fig.1 Comparison between Taper anid TaperR oni the Number of Item Pairs Exined on Mushroom Dataset Fig.4 Execution Time Comparson betwen Taper atnd TaperR on Soybean Dataset 8000 6000 Taper UT 4000 2000 Iz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 The minimum correlation tesholds Fig.2 Comarison between Taper and TaperR on the Numhber of Item Pairs Examined oni Soybean Dataset Fig.3 and Fig.4 show the execution time of the two algonithms as the correlation thresholds are increased on mushroom dataset and soybean dataset As can be seen the for Association Mining IEEE Trans On Knowledge and Data Engineering 2000 12\(3 372-390 4 J Han J Pei J Yin Mining Frequent Patterns without Candidate Generation In Proc of SIGMOD'0O pp 1-12 2000 5 S Brin R Motwanii ad C Silverstein Beyond market basket Generalizing association rules to correlations In Proc SIGMOD'97 1997 6 S Ma and J L Hellerstein Mining mutually dependett patterns In Proc 1CDM'01,.2001 7 B Omiecinski Alternative interest measurs fbr we can see that TaperR algointhm always perfbrms better than Taper and hence is of practical use fbr mining correlations on relational databases 6 Conclusions In this we propose a new algoritm called TapeiR for mining statistical correlations on relational databases Exerimental results from Tan V Kumar Exploiting a Support-based Upper Bound of Pearson's Correlation Coefficient fbr Efficiently Idenitifing Strongly Correlated Pairs In Proc of ACM SIGKDD'04 2004 2 R Agrawal R Srikant Fast Algorithis fbr Miniyng Association Rules In Proc of VLDB'94 pp 478-499 1994 3 M J Zaki Scalable Algorithms the correlation thresholds are increased and the performance of the TapeR algonithm is always better than that of Taper It verified our theoretical analysis in Section 3 rig txeution ilme tompanson oetween iaper aU TaperR on Mushoom Dataset F3rom the above expenmental results mng associations IEEE Trans Knowlege and Data En;gineering 2003 1635 0  1 0.2 0.3 0.4 0.5 0.6 0.7t 0.8 0.9  The mlnimm Correlation thresholds 25 I 5 1 20 0.1 0.2 0.3 0 0.5 0.6 0.7 0.8 0.9 The minImum correlation thresholds real data sets exhibit that our algonrthm can produce smaller candidate set and be faster thlan previous algorithms Refrences 1 H Xiong S Shekhar P-N 


Proceedings of the Fourth International Conference on Machine Learning and Cybernetics Guangzhou 18-21 August 2005 8 Y.-K Lee W.-Y Klim Y D Cai J Han CoMine efficient mnitng of corelated patterns In Proc ICDM'03 2003 9 W.-Y Kim Y.-K Lee J Han CCMine Efficient Miniig of Confldence-Closed Correlated Patterns In Ptoc of PAKDD'04 2004 10 H Xiong P.-N Tan and V Kumar Mining hyper-cliqye paterns with confidence pruning In Tech Report Univ of Minnesota Minneapolis March 2003  1I H T Reynolds The Analysis of Cross-classifications The Free Press New Yoik 1977 12 C J Merz P Merphy UCI Repository of Machine Learning Databases 1996 Hu.w ICSuIMRRepos h wd 1636 


A S 2005#219_81_01616731.pdf.t xt t e xt  pl a i n 1 1629 xt a ppl i c a t i on oc t e t s t re a m 17972 xt t e xt  pl a i n 22442 A S 2005#219_84_01434871.pdf.t xt t e xt  pl a i n 24210 xt n 12139 xt t e xt  pl a i n 1577 A S 2005#219_87_01562776.pdf.t xt a ppl i c a t i on oc t e t s t re a m 23184 xt a ppl i c a t i on oc t e t s t re a m 17875 xt t e xt  pl a i n 22796 A S 2005#219_90_01547368.pdf.t xt t e xt  pl a i n 29777 xt a ppl i c a t i on oc t e t s t re a m 19235 xt n 9995 xt t e xt  pl a i n 8512 A S 2005#219_94_01547343.pdf.t xt t e xt  pl a i n 14 A S 2005#219_95_0152731 1.pdf.t xt t e xt  pl a i n 28623 xt a ppl i c a t i on oc t e t s t re a m 21584 A S 2005#219_97_01529474.pdf.t xt t e xt  pl a i n 27494 xt n 31529 xt a ppl i c a t i on oc t e t s t re a m 150 A S 2005#219_100_01545181.pdf.t xt a ppl i c a t i on oc t e t s t re a m 28617 A S 2005#219_101_01565673.pdf.t xt t e xt  pl a i n 43389 xt n 10040 xt t e xt  pl a i n 28430 A S 2005#219_104_01467710.pdf.t xt t e xt  pl a i n 22267 xt a ppl i c a t i on oc t e t s t re a m 17799 xt t e xt  pl a i n 21355 A S 2005#219_107_01416460.pdf.t xt a ppl i c a t i on oc t e t s t re a m 15752 xt n 1 1281 xt n 17635 xt n 15958 A S 2005#219_1 1 1_01572320.pdf.t xt t e xt  pl a i n 18685 xt n 33715 xt n 36162 xt n 28692 xt n 27476 xt a ppl i c a t i on oc t e t s t re a m 34155 xt a ppl i c a t i on oc t e t s t re a m 31425 xt n 39506 xt n 19383 xt t e xt  pl a i n 18545 A S 2005#219_121_01684322.pdf.t xt t e xt  pl a i n 7990 xt n 30289 xt a ppl i c a t i on oc t e t s t re a m 18715 A S 2005#219_124_01565744.pdf.t xt t e xt  pl a i n 14055 xt a ppl i c a t i on oc t e t s t re a m 29423 xt a ppl i c a t i on oc t e t s t re a m 28950 A S 2005#219_127_01591957.pdf.t xt t e xt  pl a i n 28826 xt n 20054 xt t e xt  pl a i n 18194 A S 2005#219_130_01595839.pdf.t xt t e xt  pl a i n 19792 


xt n 19792 xt t e xt  pl a i n 13977 A S 2005#219_132_01562645.pdf.t xt a ppl i c a t i on oc t e t s t re a m 32966 xt n 7519 xt n 3981 xt t e xt  pl a i n 19758 A S 2005#219_136_01565764.pdf.t xt a ppl i c a t i on oc t e t s t re a m 20034 xt a ppl i c a t i on oc t e t s t re a m 21069 xt t e xt  pl a i n 31044 A S 2005#219_139_01428463.pdf.t xt a ppl i c a t i on oc t e t s t re a m 34618 xt a ppl i c a t i on oc t e t s t re a m 38682 xt t e xt  pl a i n 20529 A S 2005#219_142_01647787.pdf.t xt a ppl i c a t i on oc t e t s t re a m 34775 xt a ppl i c a t i on oc t e t s t re a m 18993 xt a ppl i c a t i on oc t e t s t re a m 18379 A S 2005#219_145_04085288.pdf.t xt a ppl i c a t i on oc t e t s t re a m 17000 xt n 24341 xt t e xt  pl a i n 20984 A S 2005#219_148_01571 139.pdf.t xt t e xt  pl a i n 25796 xt n 18836 xt a ppl i c a t i on oc t e t s t re a m 19592 A S 2005#219_151_01565794.pdf.t xt a ppl i c a t i on oc t e t s t re a m 19937 xt n 17383 xt t e xt  pl a i n 26283 A S 2005#219_154_01571713.pdf.t xt t e xt  pl a i n 30495 xt n 21627 xt t e xt  pl a i n 24023 A S 2005#219_157_01410188.pdf.t xt a ppl i c a t i on oc t e t s t re a m 73548 xt n 25661 xt t e xt  pl a i n 30801 A S 2005#219_160_01565675.pdf.t xt a ppl i c a t i on oc t e t s t re a m 36582 xt n 19335 xt a ppl i c a t i on oc t e t s t re a m 35925 A S 2005#219_163_01524032.pdf.t xt t e xt  pl a i n 33370 xt n 19704 xt t e xt  pl a i n 37045 A S 2005#219_166_01522861.pdf.t xt a ppl i c a t i on oc t e t s t re a m 21236 xt n 26842 xt a ppl i c a t i on oc t e t s t re a m 39012 A S 2005#219_169_01598834.pdf.t xt t e xt  pl a i n 18689 xt n 12319 xt t e xt  pl a i n 16947 A S 2005#219_172_04133499.pdf.t xt t e xt  pl a i n 23083 xt n 17758 xt t e xt  pl a i n 29325 A S 2005#219_175_01425248.pdf.t xt t e xt  pl a i n 1285 A S 2005#219_176_01452149.pdf.t xt t e xt  pl a i n 15656 xt t e xt  pl a i n 39326 A S 2005#219_178_01527272.pdf.t xt t e xt  pl a i n 28030 xt n 15029 xt t e xt  pl a i n 33284 


xt n 33284 xt n 20304 xt n 25016 xt t e xt  pl a i n 39404 A S 2005#219_184_01647786.pdf.t xt t e xt  pl a i n 32135 xt n 7439 xt t e xt  pl a i n 20831 A S 2005#219_187_01578757.pdf.t xt a ppl i c a t i on oc t e t s t re a m 29401 xt n 22909 xt t e xt  pl a i n 7901 A S 2005#219_190_01565747.pdf.t xt t e xt  pl a i n 18145 xt n 19319 xt a ppl i c a t i on oc t e t s t re a m 158 A S 2005#219_193_01565708.pdf.t xt a ppl i c a t i on oc t e t s t re a m 37537 A S 2005#219_194_01587556.pdf.t xt t e xt  pl a i n 22203 xt a ppl i c a t i on oc t e t s t re a m 34180 xt t e xt  pl a i n 14418 A S 2005#219_197_01616237.pdf.t xt t e xt  pl a i n 14621 xt n 23075 xt t e xt  pl a i n 28629 A S 2005#219_200_01607471.pdf.t xt t e xt  pl a i n 27671 xt a ppl i c a t i on oc t e t s t re a m 25191 xt t e xt  pl a i n 34132 A S 2005#219_203_01587551.pdf.t xt t e xt  pl a i n 24699 xt a ppl i c a t i on oc t e t s t re a m 24690 xt t e xt  pl a i n 27266 A S 2005#219_206_01662306.pdf.t xt t e xt  pl a i n 16968 xt n 29446 xt t e xt  pl a i n 24323 A S 2005#219_209_01461779.pdf.t xt t e xt  pl a i n 19792 xt n 28753 xt n 15442 xt a ppl i c a t i on oc t e t s t re a m 20877 A S 2005#219_213_01631278.pdf.t xt a ppl i c a t i on oc t e t s t re a m 29361 xt n 22678 A S 2005#219_215_015001 1 1.pdf.t xt t e xt  pl a i n 14579 xt t e xt  pl a i n 36462 A S 2005#219_217_01553005.pdf.t xt t e xt  pl a i n 24200 xt  33655 A S 2005#219_219_01562644.pdf.t xt a ppl i c a t i on oc t e t s t re a m 20212 xt n 27490 xt t e xt  pl a i n 25676 A S 2006#237_3_04216805.pdf.t xt t e xt  pl a i n 26780 xt n 24003 xt t e xt  pl a i n 25270 A S 2006#237_6_04052773.pdf.t xt t e xt  pl a i n 20034 xt a ppl i c a t i on oc t e t s t re a m 31 129 xt  34205 A S 2006#237_9_01684993.pdf.t xt t e xt  pl a i n 26787 xt n 28994 xt a ppl i c a t i on oc t e t s t re a m 25060 


A S 2006#237_1 1_04021078.pdf.t xt a ppl i c a t i on oc t e t s t re a m 25060 xt t e xt  pl a i n 17067 A S 2006#237_13_04085462.pdf.t xt a ppl i c a t i on oc t e t s t re a m 24865 xt n 16444 xt t e xt  pl a i n 29459 A S 2006#237_16_04030898.pdf.t xt a ppl i c a t i on oc t e t s t re a m 31967 xt n 44817 xt t e xt  pl a i n 30604 A S 2006#237_19_01692192.pdf.t xt t e xt  pl a i n 14301 xt n 47081 xt a ppl i c a t i on oc t e t s t re a m 21216 A S 2006#237_22_01651995.pdf.t xt t e xt  pl a i n 19500 xt n 24747 xt t e xt  pl a i n 21056 A S 2006#237_25_04072208.pdf.t xt t e xt  pl a i n 20440 xt n 26159 xt t e xt  pl a i n 51484 A S 2006#237_28_0404161 1.pdf.t xt t e xt  pl a i n 34488 xt n 36884 xt t e xt  pl a i n 15297 A S 2006#237_31_04028223.pdf.t xt t e xt  pl a i n 19681 xt n 29194 xt a ppl i c a t i on oc t e t s t re a m 25157 A S 2006#237_34_04061370.pdf.t xt a ppl i c a t i on oc t e t s t re a m 30602 xt a ppl i c a t i on oc t e t s t re a m 31343 xt t e xt  pl a i n 26734 A S 2006#237_37_01647707.pdf.t xt a ppl i c a t i on oc t e t s t re a m 55637 xt a ppl i c a t i on oc t e t s t re a m 50522 xt t e xt  pl a i n 32709 A S 2006#237_40_04104869.pdf.t xt t e xt  pl a i n 26395 xt a ppl i c a t i on oc t e t s t re a m 23362 xt a ppl i c a t i on oc t e t s t re a m 23314 A S 2006#237_43_04028215.pdf.t xt t e xt  pl a i n 23042 xt n 27599 xt a ppl i c a t i on oc t e t s t re a m 19 A S 2006#237_46_04155477.pdf.t xt t e xt  pl a i n 27219 A S 2006#237_47_04018542.pdf.t xt t e xt  pl a i n 30149 xt n 27958 xt t e xt  pl a i n 29943 A S 2006#237_50_04072193.pdf.t xt a ppl i c a t i on oc t e t s t re a m 26669 xt n 31 120 xt n 17300 xt a ppl i c a t i on oc t e t s t re a m 21930 A S 2006#237_54_04077818.pdf.t xt t e xt  pl a i n 20656 xt n 34280 xt t e xt  pl a i n 22732 A S 2006#237_57_04028269.pdf.t xt t e xt  pl a i n 21082 xt n 24550 xt t e xt  pl a i n 30583 A S 2006#237_60_04053125.pdf.t xt a ppl i c a t i on oc t e t s t re a m 22832 


xt a ppl i c a t i on oc t e t s t re a m 22832 xt t e xt  pl a i n 17993 


the initial global candidate set would be similar to the set of global MFIs. As a result, during the global mining phase the communication and synchronization overhead is low  0 2 4 6 8 1 0 Number of Nodes Figure 5. Speedup of DMM 4.4.2 Sizeup For the sizeup test, we fixed the system to the 8-node con figuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local database sire at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remained the same as the local database size was increased. This is different from the speedup test, where the database repartitioning was per formed when the number of nodes was increased. The per formance of DMM is affected by the database repartitioning to some extent, although it is usually very small. During the sizeup test, the local mining result of DMM is not changed at all at each node The results shown in Figure 6 indicate that DMM has a very good sizeup property. Since increasing the size of local database did not affect the local mining result of DMM at each node, the total execution time increased just due to more disk U 0  and computation cost which scaled almost linearly with sizeup 5 Conclusions In this paper, we proposed a new parallel maximal fre quent itemset \(MFI Max-Miner \(DMM tems. DMM is a parallel version of Max-Miner, and it re quires low synchronization and communication overhead compared to other parallel algorithms. In DMM, Max Miner is applied on each database partition during the lo 0 45 90 135 180 225 270 Amwnt of Data per Node \(ME Figure 6. Sizeup of DMM cal mining phase. Only one synchronization is needed at thc end of this phase to construct thc initial global candi date set. In the global mining phase, a top-down search is performed on the candidate set, and a prefix tree is used to count the candidates with different length efficiently. Usu ally, just a few passes are needed to find all global maximal frequent itemsets. Thus, DMM largely reduces the number of synchronizations required between processing nodes Compared with Count Distribution, DMM shows a great improvement when some frequent itemscts are large \(i.e long patterns employed by DMM for efficient communication between nodes; and global support estimation, subset-infrequency based pruning, and superset-frequency based pruning are used to reduce the size of global candidate set. DMM has very good speedup and sizeup properties References I ]  R. Agrawal and R. Srikant  FdSt Algorithms for Mining As sociation Rules  Pmc. o f f h e  ZOrh VLDB Conf, 1994, pp 487499 2] R. Agrawal and I. C. Shafer  Parallel Mining of Association Rules  IEEE Trans. on Knowledge and Dura Engineering Vol. 8, No. 6, 1996, pp. 962-969 3] R. I. Bayardo  Efficient Mining Long Patlems from Databases  Proc. ofrhe ACM SIGMOD Inf  l Conf on Man ogemenr ofDara, 1998, pp. 85-91 4] S.  M. Chung and J. Yang  A Parallel Distributive Join Al gorithm for Cube-Connected Multiprocessors  IEEE Trans on Parallel and Disrribured Systems, Vol. 7, No. 2, 1996, pp 127-137 51 M. Snir, S. Otto. S. Huss-Lederman, D. Walker, and J. Don gana, MPI: The Complete Reference, The MIT Press, 1996 


gana, MPI: The Complete Reference, The MIT Press, 1996 6] R. Rymon  Search through Systematic Set Enumeralion   Pmc. of3rd Inr  l Con$ on Principles of Knowledge Repre sentation and Reasoning, 1992, pp. 539-550 507 pre></body></html 


sketch-index in answering aggregate queries. Then Section 5.2 studies the effect of approximating spatiotemporal data, while Section 5.3 presents preliminary results for mining association rules 5.1 Performance of sketch-indexes Due to the lack of real spatio-temporal datasets we generate synthetic data in a way similar to [SJLL00 TPS03] aiming at simulation of air traffic. We first adopt a real spatial dataset [Tiger] that contains 10k 2D points representing locations in the Long Beach county \(the data space is normalized to unit length on each dimension These points serve as the  airbases  At the initial timestamp 0, we generate 100k air planes, such that each plane \(i uniformly generated in [200,300], \(ii, iii destination that are two random different airbases, and iv  the velocity direction is determined by the orientation of the line segment connecting its source and destination airbases move continually according to their velocities. Once a plane reaches its destination, it flies towards another randomly selected also uniform in [0.02, 0.04 reports to its nearest airbase, or specifically, the database consists of tuples in the form &lt;time t, airbase b, plane p passenger # a&gt;, specifying that plane p with a passengers is closest to base b at time t A spatio-temporal count/sum query has two parameters the length qrlen of its query \(square number qtlen of timestamps covered by its interval. The actual extent of the window \(interval uniformly in the data space \(history, i.e., timestamps 0,100 air planes that report to airbases in qr during qt, while a sum query returns the sum of these planes  passengers. A workload consists of 100 queries with the same parameters qrlen and qtlen The disk page size is set to 1k in all cases \(the relatively small page size simulates situations where the database is much more voluminous specialized method for distinct spatio-temporal aggregation, we compare the sketch-index to the following relational approach that can be implemented in a DBMS. Specifically, we index the 4-tuple table lt;t,b,p,a&gt; using a B-tree on the time t column. Given a count query \(with window qr and interval qt SELECT distinct p FROM &lt;t,b,p,a&gt WHERE t?qt &amp; b contained in qr The performance of each method is measured as the average number of page accesses \(per query processing a workload. For the sketch-index, we also report the average \(relative Specifically, let acti and esti be the actual and estimated results of the i-th query in the workload; then the error equals \(1/100 set the number of bits in each sketch to 24, and vary the number of sketches The first experiment evaluates the space consumption Figure 5.1 shows the sketch index size as a function of the number of sketches used \(count- and sum-indexes have the same results more sketches are included, but is usually considerably smaller than the database size \(e.g., for 16 signatures, the size is only 40% the database size 0 20 40 60 80 


80 100 120 140 160 8 16 32 number of sketches size \(mega bytes database size Figure 5.1: Size comparison Next we demonstrate the superiority of the proposed sketch-pruning query algorithm, with respect to the na  ve one that applies only spatio-temporal predicates. Figure 5.2a illustrates the costs of both algorithms for countworkloads with qtlen=10 and various qrlen \(the index used in this case has 16 sketches also illustrate the performance of the relational method which, however, is clearly incomparable \(for qrlen?0.1, it is worse by an order of magnitude we omit this technique Sketch-pruning always outperforms na  ve \(e.g., eventually two times faster for qrlen=0.25 increases with qrlen, since queries returning larger results tend to set bits in the result sketch more quickly, thus enhancing the power of Heuristics 3.1 and 3.2. In Figure 5.2b, we compare the two methods by fixing qrlen to 0.15 and varying qtlen. Similar to the findings of [PTKZ02]4 both algorithms demonstrate  step-wise  growths in their costs, while sketch-pruning is again significantly faster The experiments with sum-workloads lead to the same observations, and therefore we evaluate sketch-indexes using sketch-pruning in the rest of the experiments 4 As explained in [PTKZ02], query processing accesses at most two paths from the root to the leaf level of each B-tree regardless the length of the query interval Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE sketch-pruning naive relational 0 100 200 300 400 500 600 700 800 900 0.05 0.1 0.15 0.2 0.25 number of disk accesses query rectangle length 300 0 100 200 400 500 600 1 5 10 15 20 number of disk accesses query interval length a qtlen=10 b qrlen=0.15 Figure 5.2: Superiority of sketch-pruning \(count As discussed in Section 2, a large number of sketches reduces the variance in the resulting estimate. To verify this, Figure 5.3a plots the count-workload error of indexes 


using 8-, 16-, and 32- sketches, as a function of qrlen qtlen=10 error \(below 10 it increases slowly with qrlen used, however, the error rate is much higher \(up to 30 and has serious fluctuation, indicating the prediction is not robust. The performance of 16-sketch is in between these two extremes, or specifically, its accuracy is reasonably high \(average error around 15 much less fluctuation than 8-sketch 32-sketch 16-sketch 8-sketch relative error 0 5 10 15 20 25 30 35 0.05 0.1 0.15 0.2 0.25 query rectangle length relative error 0 5 10 15 20 25 30 35 1 5 10 15 20 query interval length a qtlen=10, count b qrlen=0.15, count relative error query rectangle length 0 5 10 15 20 25 0.05 0.1 0.15 0.2 0.25 relative error query interval length 0 5 10 15 20 25 30 1 5 10 15 20 c qtlen=10, sum d qrlen=0.15, sum Figure 5.3: Accuracy of the approximate results The same phenomena are confirmed in Figures 5.3b where we fix qrlen to 0.15 and vary qtlen 5.3d \(results for sum-workloads number of sketches improves the estimation accuracy, it also leads to higher space requirements \(as shown in Figure 5.1 Figures 5.4a and 5.4b show the number of disk accesses for the settings of Figures 5.3a and 5.3b. All indexes have almost the same behavior, while the 32-sketch is clearly more expensive than the other two indexes. The interesting observation is that 8- and 16-sketches have 


interesting observation is that 8- and 16-sketches have almost the same overhead due to the similar heights of their B-trees. Since the diagrams for sum-workloads illustrate \(almost avoid redundancy 32-sketch 16-sketch 8-sketch number of disk accesses query rectangle length 0 50 100 150 200 250 300 350 400 0.05 0.1 0.15 0.2 0.25 number of disk accesses query interval length 0 50 100 150 200 250 300 350 1 5 10 15 20 a qtlen=10 b qrlen=0.15 Figure 5.4: Costs of indexes with various signatures Summary: The sketch index constitutes an effective method for approximate spatio-temporal \(distinct aggregate processing. Particularly, the best tradeoff between space, query time, and estimation accuracy obtained by 16 sketches, which leads to size around 40 the database, fast response time \(an order of magnitude faster than the relational method average relative error 5.2 Approximating spatio-temporal data We proceed to study the efficiency of using sketches to approximate spatio-temporal data \(proposed in Section 4.1 as in the last section, except that at each timestamp all airplanes report their locations to a central server \(instead of their respective nearest bases maintains a table in the form &lt;time t, plane p, x, y&gt;, where x,y with parameters qrlen and qtlen distinct planes satisfying the spatial and temporal conditions. For comparison, we index the table using a 3D R*-tree on the columns time, x, and y. Given a query, this tree facilitates the retrieval of all qualifying tuples, after which a post-processing step is performed to obtain the Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE number of distinct planes \(in the sequel, we refer to this method as 3DR method introduces a regular res  res grid of the data space, where the resolution res is a parameter. We adopt 16 sketches because, as mentioned earlier, this number gives the best overall performance Figure 5.5 compares the sizes of the resulting sketch indexes \(obtained with resolutions res=25, 50, 100 the database size. In all cases, we achieve high compression rate \(e.g., the rate is 25% for res=25 evaluate the query efficiency, we first set the resolution to the median value 50, and use the sketch index to answer workloads with various qrlen \(qtlen=10 


workloads with various qrlen \(qtlen=10 size \(mega bytes database size 0 20 40 60 80 100 120 140 160 25 50 100 resolution Figure 5.5: Size reduction Figure 5.6a shows the query costs \(together with the error in each case method. The sketch index is faster than 3DR by an order of magnitude \(note that the vertical axis is in logarithmic scale around 15% error observations using workloads with different qtlen Finally, we examine the effect of resolution res using a workload with qrlen=0.15 and qtlen=10. As shown in Figure 5.6c, larger res incurs higher query overhead, but improves the estimation accuracy Summary: The proposed sketch method can be used to efficiently approximate spatio-temporal data for aggregate processing. It consumes significantly smaller space, and answers a query almost in real-time with low error 3D Rsketch number of disk accesses query rectangle length 1 10 100 1k 10k 0.05 0.1 0.15 0.2 0.25 16 14% 15 15% 13 relative error number of disk accesses query interval length 1 10 100 1k 10k 1 5 10 15 20 16 15% 15% 12% 11 relative error a qtlen=10, res=25 b qrlen=0.15, res=25 0 500 1000 1500 2000 2500 25 50 100 number of disk accesses resolution 20% 15% 14 relative error c qrlen=0.15, qtlen=10 


c qrlen=0.15, qtlen=10 Figure 5.6: Query efficiency \(costs and error 5.3 Mining association rules To evaluate the proposed algorithm for mining spatiotemporal association rules, we first artificially formulate 1000 association rules in the form \(r1,T,90 with 90% confidence i randomly picked from 10k ones, \(ii in at most one rule, and \(iii Then, at each of the following 100 timestamps, we assign 100k objects to the 10k regions following these rules. We execute our algorithms \(using 16 sketches these rules, and measure \(i  correct  rules divided by the total number of discovered rules, and \(ii successfully mined Figures 5.7a and 5.7b illustrate the precision and recall as a function of T respectively. Our algorithm has good precision \(close to 90 majority of the rules discovered are correct. The recall however, is relatively low for short T, but gradually increases \(90% for T=25 evaluated in the previous sections, the estimation error decreases as the query result becomes larger \(i.e., the case for higher T 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 precision HT 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 recall HT a b Figure 5.7: Efficiency of the mining algorithm Summary: The preliminary results justify the usefulness of our mining algorithm, whose efficiency improves as T increases Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 6. Conclusions While efficient aggregation is the objective of most spatio-temporal applications in practice, the existing solutions either incur prohibitive space consumption and query time, or are not able to return useful aggregate results due to the distinct counting problem. In this paper we propose the sketch index that integrates traditional approximate counting techniques with spatio-temporal indexes. Sketch indexes use a highly optimized query algorithm resulting in both smaller database size and faster query time. Our experiments show that while a sketch index consumes only a fraction of the space required for a conventional database, it can process 


required for a conventional database, it can process queries an order of magnitude faster with average relative error less than 15 While we chose to use FM sketches, our methodology can leverage any sketches allowing union operations Comparing the efficiency of different sketches constitutes a direction for future work, as well as further investigation of more sophisticated algorithms for mining association rules. For example, heuristics similar to those used for searching sketch indexes may be applied to improve the brute-force implementation ACKNOWLEDGEMENTS Yufei Tao and Dimitris Papadias were supported by grant HKUST 6197/02E from Hong Kong RGC. George Kollios, Jeffrey Considine and were Feifei Li supported by NSF CAREER IIS-0133825 and NSF IIS-0308213 grants References BKSS90] Beckmann, N., Kriegel, H., Schneider, R Seeger, B. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD, 1990 CDD+01] Chaudhuri, S., Das, G., Datar, M., Motwani R., Narasayya, V. Overcoming Limitations of Sampling for Aggregation Queries. ICDE 2001 CLKB04] Jeffrey Considine, Feifei Li, George Kollios John Byers. Approximate aggregation techniques for sensor databases. ICDE, 2004 CR94] Chen, C., Roussopoulos, N. Adaptive Selectivity Estimation Using Query Feedback. SIGMOD, 1994 FM85] Flajolet, P., Martin, G. Probabilistic Counting Algorithms for Data Base Applications JCSS, 32\(2 G84] Guttman, A. R-Trees: A Dynamic Index Structure for Spatial Searching. SIGMOD 1984 GAA03] Govindarajan, S., Agarwal, P., Arge, L. CRBTree: An Efficient Indexing Scheme for Range Aggregate Queries. ICDT, 2003 GGR03] Ganguly, S., Garofalakis, M., Rastogi, R Processing Set Expressions Over Continuous Update Streams. SIGMOD, 2003 HHW97] Hellerstein, J., Haas, P., Wang, H. Online Aggregation. SIGMOD, 1997 JL99] Jurgens, M., Lenz, H. PISA: Performance Models for Index Structures with and without Aggregated Data. SSDBM, 1999 LM01] Lazaridis, I., Mehrotra, S. Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure. SIGMOD 2001 PGF02] Palmer, C., Gibbons, P., Faloutsos, C. ANF A Fast and Scalable Tool for Data Mining in Massive Graphs. SIGKDD, 2002 PKZT01] Papadias,  D., Kalnis, P.,  Zhang, J., Tao, Y Efficient OLAP Operations in Spatial Data Warehouses. SSTD, 2001 PTKZ02] Papadias, D., Tao, Y., Kalnis, P., Zhang, J Indexing Spatio-Temporal Data Warehouses ICDE, 2002 SJLL00] Saltenis, S., Jensen, C., Leutenegger, S Lopez, M.A. Indexing the Positions of Continuously Moving Objects. SIGMOD 2000 SRF87] Sellis, T., Roussopoulos, N., Faloutsos, C The R+-tree: A Dynamic Index for MultiDimensional Objects. VLDB, 1987 TGIK02] Thaper, N., Guha, S., Indyk, P., Koudas, N Dynamic Multidimensional Histograms 


SIGMOD, 2002 Tiger] www.census.gov/geo/www/tiger TPS03] Tao, Y., Papadias, D., Sun, J. The TPR*Tree: An Optimized Spatio-Temporal Access Method for Predictive Queries. VLDB, 2003 TPZ02] Tao, Y., Papadias, D., Zhang, J. Aggregate Processing of Planar Points. EDBT, 2002 TSP03] Tao, Y., Sun, J., Papadias, D. Analysis of Predictive Spatio-Temporal Queries. TODS 28\(4 ZMT+01] Zhang, D., Markowetz, A., Tsotras, V Gunopulos, D., Seeger, B. Efficient Computation of Temporal Aggregates with Range Predicates. PODS, 2001 ZTG02] Zhang, D., Tsotras, V., Gunopulos, D Efficient Aggregation over Objects with Extent PODS, 2002 Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE pre></body></html 


