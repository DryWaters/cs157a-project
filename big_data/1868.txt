SSDT A Scalable Subspace-Splitting Classifier for Biased Data Haixun Wang IBM T J Watson Research Center Yorktown Heights NY 10598 haixun us.ibm.com Abstract Decision trees are one of the most extensively used data mining models Recently a number of eficient scalable algorithms for constructing decision trees on large disk resident dataset have been introduced In this papel we study the problem of learning scalable decision trees from datasets with biased class distribution Our objective is 
to build decision trees that are more concise and more inter pretable while maintaining the scalability of the model To achieve this, our approach searches for subspace clusters of data cases of the biased class to enable multivariate split tings based on weighted distances to such clusters In order to build concise and interpretable models, other approaches including multivariate decision trees and association rules often introduce scalability and perjiormance issues The SSDT algorithm we present achieves the objective without loss in eficiency scalability, and accuracy 1 Introduction 
Decision trees are one of the most extensively used data mining models Decision tree induction is a greedy algo rithm that partitions the data in a top-down divide-and conquer manner Decision trees are especially attractive in mining large datasets because i the decision tree model is easier to interpret 5 and ii the induction process is more efficient compared to other methods  12,8 Recently a number of efficient scalable algorithms for constructing univariate decision trees from large disk-resident data have been introduced  
12 8,7 141 Our work focuses on the scenario where the training data has a highly imbalanced class distribution i.e., we assume there are only 2 class labels positive and negative and the positive target\class accounts for a small fraction say be tween 0.1 and 5 of the entire dataset This situation arises frequently in the data mining environment, such as in fraud detection, network intrusion detection, and etc In this paper we first discuss several limitations of the deci Philip S Yu IBM T J Watson Research Center Yorktown Heights 
NY 10598 psyu us.ibm.com sion tree induction process under this scenario Then we introduce a new technique, SSDT, which aims at overcom ing these problems while preserving the efficiency of the decision tree algorithms Representational limitations of univariate decision trees The decision tree induction process has several deficiencies Consider the training set in Figure l\(a where Play is the class label The C4.5 decision tree is shown in Figure l\(b which has only one node Obviously it misses the pattern of hot high that has a strong support of 
No The difficulty is largely due to univariate tests at each node and it is also compounded by the use of categorical attributes Applying multivariate decision tree algorithms, such as OC 1  111 and LMDT  to such datasets may reveal pat terns univariate decision tree can not discover However, it is computationally expensive to gauge the value of a linear combination of many variables per node and is practically infeasible to use on large disk-resident datasets Biased data distribution Training sets of certain data mining tasks have 
a very biased data distribution, inasmuch as the target class accounts for an extremely tiny fraction say 0.5 of the data Most learning algorithms such as decision trees will generate a trivial model that always predicts the majority class and reports an accuracy rate of 99.5  161 However data cases of the biased class often carries more significant meanings and are often the primary targets of mining Decision trees have been shown unsuitable for such tasks  101 In Figure l\(c we show the distribution of a syn thetic dataset in a 
two-dimensional space where the dark re gions represent the data cases of the biased class Data cases of the majority class are either clustered at different spots or simply distributed randomly and are not shown explicitly A representative univariate decision tree learned from such a dataset without pruning is shown in Figure l\(d The deci sion tree induction process keeps on partitioning the space either horizontally or vertically at each node and the re sulted decision tree often has a very large size. Furthermore 542 0-7695-1119-8/01 17.00 0 2001 IEEE 


Temp cool mild I mild I normal I yes I Humid Play high yes high yes I hot I drv I yes I I hot I normal I yes I hot high very hot high a\Training Set b C4.5 decision tree for \(a c\2-dimensional biased dataset d\decision tree for \(c Figure 1 Limitations of Decision Tree Classifiers it is easy to see that the size of the tree will grow with the dimensionality of the data, as in a higher dimensional space more tests are required to locate the subspace region where the data cases cluster A decision tree that assigns an unknown instance a def inite class label \(\222positive\222 or 222negative\222\or a probability based merely on the data distribution in the leaf nodes usu ally has a low predictive accuracy for instances of the biased target class Furthermore the model can hardly discrimi nate among data cases of the majority class on the basis of their closeness to the target class. For instance, the two 222X\222s in Figure l\(c are in the same leaf node Hence they are assigned a same probability by the model, although one of them is much closer to the biased class This causes prob lems in applications such as target marketing: the market ing department wants to send out promotions to 20 of the people in their database while only 1  of the people in the database are recorded buyers The rest 19% has to be se lected based on their closeness to the buyer class 2 Our Approach An Overview It is clear from the examples in Figure 1 that univariate splitting conditions often results in undetected patterns or decision trees of formidable sizes To build concise mod els it is essential that more than one variables are taken into consideration in splitting the data However scalability re quirements forbids us considering all possible combinations of these variables as OC1  1 I and LMDT 6 do We improve the decision tree induction process by pro viding an additional splitting criterion which results in more concise and more interpretable decision trees The mo tivation is that in datasets with biased class distribution while the negative instances are distributed \222everywhere\222 instances of the target class tend to form clusters in some subspaces These clusters are the foundation of accurate predictions of unknown instances. The proposed approach SSDT, uses an efficient multivariate search algorithm to lo cate subspace clusters so as to enable \(multivariate\splitting based on weighted distances to the centroid of the clusters While it is computationally prohibitive to search for all the clusters it is more feasible to search for clusters formed by points in the biased target class, since they only account for a small fraction of the data. Our algorithm detects candi date clusters from lower dimensions to higher dimensions and prunes away all the candidates as soon as we find that partitions based on these clusters can not offer a better pu rity than the univariate splits Related Works Much work has focused on how to tackle the deficiencies of the decision tree discussed in the pre vious section Among them multivariate decision trees overcome a representational limitation of univariate deci sion trees  13 41 However performing a multivariate par tition often leads to a much larger consumption of compu tation time and memory, which may be prohibitive for large datasets The target selection problem also attracts lots of atten tion 9 101 Closeness estimations of an unknown instance to a certain class can be solved by clustering and nearest neighbor algorithms A naive approach searches for clus ters of the biased data and scores an unknown sample by its distance to the closest cluster. Association rule mining 3 is also used to solve the target selection problem A po tential problem with association rules is the combinatorial explosion of 223frequent itemsets\224 [9 lo which can be pro hibitive for large datasets with biased data distribution even after sophisticated pruning Contributions of this Paper We use novel splitting cri teria in building decision trees We discover data clustering in correlated dimensions and partition the data by distance functions defined in the corresponding subspaces With our multivariate splitting condition, decision trees can be built 543 


smaller and more interpretable However unlike other mul tivariate decision tree algorithms that are usually prohibitive for large datasets or high dimensions our approach is effi cient and scalable 3 Definitions Let S denote the training set belonging to a node of a decision tree Each point in S has n attributes in addition to the special attribute class label Let C  21  zk be a cluster of points where each xi has the same class label The centroid of C is the algebraic mean of points in C To measure the closeness of a point to a clyster we use weighted Euclidean distance function Dist\(d 5  dw where zis a point the centroid of a cluster and w\222 the weight vector The Euclidean distance is indeed defined in a subspace that is specified by a set of dimensions whose weights are non-zero The Euclidean distance only works for numerical at tributes For ordinal attributes, we can map their values to the range of 0,1 For categorical attributes the hetero geneous Euclidean metric  151 defines the similarity of two values by their relative frequency of occurrences in the same class However for attributes with many values and a biased dataset certain values may never occur in the training set In our algorithm we use a distance matrix A4 supplied by the user such that the value M\(i,j E 0,1 denotes the distance between categorical value i and j We use the gini index j=1 wherepj is the relative frequency of class j in S to measure the \223goodness\224 of all the potential splits If S is partitioned into two subsets S1 and S2 the index of the partitioned data gini\(S can be obtained by where nl and 122 are the number of points of S1 and Sa respectively 4 Scalable Decision Tree Classifiers A decision tree classifier recursively partitions the train ing set until each partition consists entirely or almost en tirely of records from one class The SPRINT algorithm has been proposed to build de cision trees for large datasets  121 The splitting criterion used by SPRINT is based on the value of a single attribute univariate For a continuous attribute it has the form of A 5 C where A is an attribute and C is a value in the do main of attribute A SPRINT avoids costly sorting at each node by pre sorting continuous attributes only once at the beginning of the algorithm Values of each continuous attribute are main tained by a sorted list Each entry in the list contains i a value of the attribute ii\its record id rid and iii the class label of the record The node is split on the attribute which yields the least value of the gin2 index Gbest Based on the sorted list of the splitting attribute a hash table is con structed to map each record rid to one of the subnodes which the record belongs to after the split Entries in other attribute lists are moved to the attribute list of the subnodes after consulting the hash table as to which subnode this en try belongs to The sorted order is maintained as the entries are moved in pre-sorted order 5 The SSDT Algorithm The core of SSDT lies in detecting subspace clusters of positive points However finding all such clusters is both time consuming and unnecessary We are only interested in clusters that can offer a better split than univariate parti tions In Section 5.1 we prove an important property which enables us to narrow down our search to those clusters that have the potential The actual clustering algorithm is in troduced in Section 5.2 where we use an Apriori-like al gorithm to find subspace clusters from lower dimensions spaces to higher dimensional spaces In Section 5.3 we compute the exact gini index for cluster-based partitioning by scanning a small proportion of the data, thus keeping the overhead of the multivariate partitioning to a minimum SSDT is based on the framework of SPRINT, where pre sorted attribute lists are maintained at each node We con sider only two class labels positive and negative and the target class positive is biased usually account ing for only a small fraction of the data We normalize the values on each dimension to the range of 0 I The SSDT approach is outlined in Algorithm 1 To par tition a dataset we first compute the gini index on each of its attributes While we scan through the pre-sorted at tribute list we also derive I-dimensional clusters of the biased data for each dimension \(described in detail later Next we locate subspace clusters of the positive data cases The minimal gini index produced by the univariate splits Gbest is passed in as a parameter to ClusterDetect so that clusters can not possibly deliver a gini index smaller than Gbest are pruned as early as possible We then com pute the gini index of splits based on the distance to each subspace cluster The process DistanceEntropy de scribed in Algorithm 2 does not require globally reorder ing the data according to the distance If the minimal gini index is achieved by some subspace cluster we partition 544 


Algorithm 1 SSDT\(Dataset S 1 2 3 4 5 6 7 a 9 10 11 12 13 14 15 16 17 S t points of the biased class in S for each attribute k do scan the sorted attribute list of k and compute  Ik  the gini index on attribute k  Lk  clusters of points in S on attribute k end for Gbest t min Ik c t ClusterDetect\(Gb,,t s L for each cluster c E C do I t DistanceEntropy\(c S end for if mixi If  Gbest then CEC split S into two subsets SI S2 based on the distance else split S into subsets on the attribute with Gbest end if call SSDT\(Si on each subset Si if Si does not satisfy the termination condition k the dataset based on the diGance to such a cluster More spec$cally given a point d instead of using univariate test di 5 U we use a test in the form of Dist\(d,j w 5 U where is the centroid of the cluster and w is a weight vec tor of all the dimensions As in the SPRINT algorithm, the partition process also keeps the sorted order of the attribute lists so that no reordering is required The partition stops when a node is composed entirely of negative points 100 or almost entirely of positive points 5.1 Minimal Support of Subspace Clusters To find subspace clusters of points of the biased class we need to find i the centroid p'of the cluster and ii the weight w which defines the subspace Gi  0 means dimension i is irrelevant of the cluster Several subspace clustering algorithms have been introduced in the literature The CLIQUE algorithm 2 reports connected dense units in subspaces but the centroids of clusters are not detected Another method, called PROCLUS l uses a hill climb ing method to successively improve a set of centroids, and derives a set of dimensions for each cluster. This algorithm however requires that the number of clusters k to be found is pre-known Both methods are time consuming since they aim at discovering all the subspace clusters Given a found cluster, Algorithm 1 partitions a dataset S into 2 datasets S1 and S2 such that SI contains points close to the centroid of the cluster Instead of checking all I We assign a higher weight to each positive point to balance the biased distribution In our algorithm we stop if more than 90 of the points in the node is positive the subspace clusters we are only interested in those that can result in partitions with a gini index lower than Gbest the minimal gini index we get by partitioning the data on single attributes Proposition 1 tells us how to narrow our search on qual ified clusters We define the support of a cluster as P'lP where P is the number of positive\points in the cluster and P is the total number of positive points in S We prove the following proposition Proposition 1 If the gini index of a cluster-based parti tion of S is lower than Gbest then the cluster must have where q is the per a support greater than 2:-2 Gbest centage of the positive points in  Proot Let N be the total number of points in S Let P be the total number of positive points in S Thus q  PIN Assume S is partitioned into S1 and S2 and S1 contains the points in the cluster. According to Formula 2 we have 2 2 2-Gbest N-PI-N gini\(S    gini\(S1  N gini S2 where P and N are the number of positive points and negative points in SI respectively Given N  P it can be shown that the lowest gini\(S is achieved if SI contains only positive points that is N  0 and gini\(S1  0 That the partition produces a gini index lower than Gbest means Expanding gini\(S2 using Equation 1 we get 2NP  2P2  2PP'+ 2NP N\(N  PI Gbest Substituting PIN with q we get The minsup given by Formula 3 is a lower bound be cause the cluster we find usually does not contain only pos 0 itive points \(i.e N  0 5.2 Cluster Detection To find subspace clusters of points in the biased class we use an iterative approach that is very similar to the apri ori algorithm 3 for finding frequent itemsets A clus ter whose support is lower than minsup in k-dimensional space can not have support larger than minsup in k  1 dimensional space We first find clusters in 1 dimensional spaces then we combine them to form candidate clusters 545 


in 2-dimensional spaces We count the number of points in each cluster and eliminate those candidate clusters whose support does not satisfy the constraint in Proposition 1 Then we combine clusters in the 2-dimensional spaces to form candidates in the 3-dimensional spaces, and so on, un til no more qualified clusters can be found Figure 5.2\(a\shows an example where points of the bi ased class form two clusters in a 3-dimensional space We use a simple approach to detect 1-dimensional clusters In Figure 3 the range of each attribute is divided into 10 bins and we keep the counts of points that fall in each bin This is done when we scan through attribute lists to evaluate splits on single attributes so there is minimal extra cost intro duced. The horizontal line in Figure 3 indicates the average density and we regard each continuous region above the av erage density line as one cluster Thus we detect one cluster around 1 with radius 1 on attribute X two clusters around 3 and 7 respectively both with radius l on attribute Y and one cluster around 2 with radius l on attribute Z in Figure 5.2\(b Each centroid is represented by values and cj are centers of clusters on dimension i and j re spectively, and ri and rj are their radius Values on the other dimensions are unknown Next we make one scat ttrough all the points in the biased lass for all points d d E C  dri 2 14  ciI,rj 2 ldj  cjl we compute the mean ck  czEc lcl and the radius rk on each of the dimension IC on two dimensions only  ci/Ti  Cj/rj  where ci Algorithm 2 ClusterDetect\(Gini1ndex Gbest Biased Data S CenterRadius Lists L I ninsup t derived by Gbest Formula 3 Step 1 jnd clusters in 2-dimensional space 3 for each centerlradius c,/r in L do 4 for each centerlradius c,/rj in L j  i do 5 6 end for 2 1 t2;C t 0 add  cz/rz  c3/r3  to Cl I end for 8 while Cl  0 do 9 1 Step 2 jind clusters from lower to higher dimensions XlYlZ scan the points in S and increase the count of cluster c E Cl for each point that belongs to c c is less than minsup cl  combining clusters in dimension I 1 1/.1 7/.1 0 1  3/.1 2/.1 IO eliminate cluster c from Cl if the number of points in I 00 x  7/.1 2/.1 11 ltl+l 12 13 end while 14 return top-k leaf clusters a Points of biased class b Potential centroids form two clusters of clusters Step 3 return the clusters Figure 2 Cluster detection After clusters whose support is less than the value given by Formula 3 are pruned we explore clusters in higher di mensional spaces and repeat this process until no more clus ters can be found Finally ClusterDetect returns the found clusters We are only interested in leaf clusters which 0123456789 01 2 3 4 5 678 9 are clusters that do not contain other clusters Among all the a Density of Points on X b Density of Points on Y  leaf clusters we return the top K clusters in the higher di mensions, where K is a user-specified parameter In order to compute the weighted Euclidean distance be tween a point and a cluster we need to find out the weight on each dimension For a non-clustered dimension i we set Gi  0 otherwise we set Gi  l/r where ri is the radius of the points distribution on dimension i Thus the distance is normalized to reflect the span of the points on each dimension 5.3 Split by Distance                  average    _      density 0123456789 c Density of Points on Z Figure 3 Histograms of positive points on each attribute Assuming all the 1-dimensional clusters shown in Fig ure 3 has support larger than minsup we then form a list of potential centroids in the 2-dimensional subspace as shown For each cluster returned by CEusterDetect we de rived a distance function Dist G The next step is to 546 


find the value w so that the split by the test Did G 5 w offers the minimum gini index A straight-forward approach is to reorder all the points by their distances to the center 6 and compute the gini in dex by scanning the ordered points. This is costly for large datasets. Another approach is to discretize the distance into intervals and for each interval we keep the counts of posi tivehegative cases whose distance to are in that interval One shortcoming of this approach is the loss of accuracy due to discretization Our approach, outlined in Algorithm 3 avoids reorder ing all the data and any loss of accuracy. This is achieved by making the followin two observations: i if point d is close to centroid  then di the coordinate on the i-th dimension must also be close to i and ii\the best splitting position should be close to the boundary of the cluster Let N be the number of dimensions with non-zero weights \(clustered dimensions\Let D be the set of points t!at are within an initial radius T  6 to 5 For any point d E D the inequality 2iji\(di  6 5 T must hold for each clustered dimension i With the ordered attribute lists it is easy to find D points that satisfy the inequality on all the N clustered dimensions Obviously D 2 D for D can contain points whose distance to p'is up to fl After sorting D by distance we compute the gini index up to radius T and we keep the points in D  D and discard D We then increase the radius T by 6 and repeat the process However we do not have to consider all the points We are computing the gini index based on the distance to the cluster centroid we found Thus we expect a good gini index near the boundaries of the cluster. According to the weighting scheme discussed in the previous subsection Gi is set to for each clustered dimension i where i is the span of the points on that dimension Thus we have Gi\(di  5 1 for any point p'that is inside the cluster In additjon to these points we consider all points that satisfy G,\(di  6 5 2 on each dimension i Thus, the maximum 5.4 SSDT Examples Let us review the two problems in Section 1 Un like the C4.5 decision tree which fails to detect pattern hot high and builds a trivial decision tree in Fig ure l\(b  the SSDT algorithm accurately captures the pat tern and constructs a compact decision tree in Figure 4\(a The second problem is introduced by datasets with biased class distribution The decision tree model shown in Fig ure 1 d used 11 tests to classify a 2-dimensional dataset shown in Figure l\(c SSDT, shown in Figure 4\(b uses only 4 tests Apparently such differences tend to be more significant if the dataset has more than 2 dimensions Algorithm 3 distance-entropy\(Dataset S Centroid  Weight 5 1 Tt6 2 N t  of dimensions with non-zero weights 3 repeat 4 5 6 7 8 end for 9 10 for each relevant _dimension i do find instances d that satisfies r  6 5 t  6  T using the ordered attribute lists dlcount t dlcount  1 check ifd satisfies all the inequalities add dto the ordered set D if dlcount  N compute gini-index for splits by distance up to r remove in tree D the branch that represents data cases within distance T top 11 TtT+6 12 until T  W 13 return I and v n D\(hot,high a\SSDT of Figure l\(a b SSDT of Figure l\(c Figure 4 SSDT for datasets in Section 1 6 Evaluations We evaluate the SSDT algorithm in various aspects We study the size of the decision tree generated by the algo rithm, the influence of the biased class distribution the ac curacy of predictions, as well as the efficiency and scalabil ity issues. The tests were performed on a 700-MHz Pentium I11 machine with 256M of memory, running Linux Synthetic Data Generation We generate synthetic data in d-dimensional spaces with two class labels, positive and negative Points have coordinates in the range of 0,1 and positive points account for p  1  10 of the total data To generate clustered points in subspaces we use a method similar to  11 The difference is that the number of positive points is controlled by the biased class ratio Our method takes 4 parameters n the number of clusters IC a Poisson parameter that determines the number of relevant dimensions in each cluster p the percentage \(biased ratio of positive points and N the total number of points First we determine the subspace for each cluster. For a given cluster i the number of relevant dimensions Si is 541 


picked from a Poisson distribution with mean k However an additional restriction 2 5 Si 5 d must be observed We generate centroid 7i for each cluster i We simply generate a uniformly distributed point in the d-dimensional space We then decide the spread \(radius of the cluster on each dimension We set ij  0.5 for irrelevant dimension j For a clustered dimension we fix a spread parameter s and choose the radius Gj E 0 s uniformly at random. For our data generation we use 3 values for s  l 2 5 We generate positive points in each cluster i in two dif ferent ways i points are distributed uniformly in the re gion; ii\for each dimension j coordinates of points on the dimension follow a normal distribution with mean cj and variance ej We determine the size of each positive cluster by Ni  pN where wi is the volume of cluster i defined as vi  n,d=1\(2  rij Finally we generate 1  p negative points The negative points either i uniformly distribute at random in the entire space or ii\form clusters in subspaces and are generated by the method described above with parameters k  0.8d and s  0.5 If a negative point is generated inside one of the positive clusters it is discarded with a probability B For our data generation we choose 6  0.5 l 0.3 0.66 Experiments Tree Size and Scalability We generate 6 clusters Table 1 of positive points out of a training set of total lOOK records and 10 attributes The total number of positive cases account for 2 of the training set The av erage radius of the cluster on each clustered dimension is 0.05 and the negative points are uniformly distributed at random. The split at the root of the decision tree, for exam ple uses the distance function defined for Cluster 6 Total 5 clusters are used at different nodes for splitting, resulting in a tree of 37 leaf nodes before pruning, while the SPRINT algorithm uses 7 1 leaf nodes before pruning 121 1 L I Table 1.5 clusters are detected by SSDT Un clustered dimensions are denoted by 222-\222 We compare the size of the decision tree in terms of number of leaf nodes generated by SPRINT and SSDT The datasets we use have 10 attributes and the 5 clusters of biased points account for 1 2 and 5 of the total data. Figure 6\(a indicates that trees built by SSDT are sig nificantly smaller, and the sizes of the trees generally do not increase as the training sets become larger Next we vary the number of clusters in the training sets and show the results in Figure 6\(b The datasets are gen erated with the same class ratio 2 The size of the tree increases significantly as there are more positive clusters in the dataset. The trees generated by SSDT are much smaller Figure 6\(a shows the scalability of SSDT as the size of the dataset increases from 0.1 to 2.5 million The dataset has 10 attributes 8 clusters with an average dimensional ity of 4 and a biased class ration of 1 The execution time increases linearly with the size of the dataset since SSDT is able to detect the clusters and the resulted decision tree has similar heights which means the number of passes through the database does not change Figure 6\(b shows the scalability of SSDT when the average dimensionality of the positive clusters is increased from 2 to 12 The dataset used in the test has 1 million records 8 clusters 1 posi tive ratio and a total of 20 attributes. It indicates that cluster dimensionality has little impact on the performance We study the impact of the number of positive clusters on the scalability In Figure 6\(c we increase clusters from 4 to 20 The dataset has 1 million records, among which 1 are positive There are 10 attributes and the clusters have an average of 5 dimensions Since the number of positive data cases is kept unchanged during the test, each cluster contains fewer records as more clusters are used The curve is steeper than in the previous cases because more scans of the dataset have to be performed In Figure 6\(d using the dataset of the same size, dimensionality and 8 positive clusters we found the performance is stable We compare the performance of SSDT with SPRINT in Figure 6\(c The datasets have 10 attributes 8 positive clusters and a class ratio of 1 In this case SSDT has an advantage over SPRINT because SSDT trees are much smaller As we increase the class ratio and the number of clusters SPRINT becomes faster than SSDT Indeed SPRINT is 20 faster than SSDT when there are 20 clusters with a 15 class ratio which means SSDT works best with biased class distributions The association rule algorithm for mining datasets with biased class distribution does not scale well. Overall SSDT is an efficient and scalable algo rithm, despite the multivariate search it performs 7 Conclusion We presented a novel decision tree algorithm The key idea is to take advantage of the subspace clusters formed by 548 


20 2w I80 160 I40  120 5 1W 2 60  Bo 40 20 I 7W Bw 500 4w H 3w 2w 1W I 0 2MM WXa w 8woo lwwo 4 8 8 10 12 I4 16 0 05 I 15 2 25 lofnmdr  of FasdlY dYI1.R of rronlr NI rnllrnS a  of leaf nodes v training set size b  of leaf nodes v  of clusters c Execution Time Figure 5 Experiments and Comparisons mi      2  8 8 1011 Ddw"ta\(.i@l-pst lol&\(lombnr dnmmyld-a tdh.fnol Wmpns a  of records b dimensionality c  of clusters d class distribution Figure 6 Scalability the data in the biased class Once these subspace clusters are efficiently detected a compact and accurate decision tree can be constructed by splitting a node based on the distance to such clusters Our multivariate decision tree algorithm has proven to be scalable and efficient Indeed it has better performance over SPRINT for very skewed distributions References I C. C Aggarwal C Procopiuc J Wolf P S Yu and J S Park Fast algorithms for projected clustering In SIGMOD 1999 2 R. Agrawal J Gehrke D Gunopulos and P Raghavan Au thomatic subspace clustering of high dimensional data for data mining applications In SIGMOD 1998 3 R Agrawal and R Srikant. Fast algorithms for mining asso ciation rules In VLDB 1994 4 J Bioch 0 van der Meer and R. Potharst Bivariate de cision trees In Principles of Data Mining and Knowledge Discoveq 1997 5 L Breiman J Friedman R Olshen and C Stone Class cation and Regression Trees Wadsworth, 1984 6 C E Brodley and P E Utgoff Multivariate versus uni variate decision trees In Technical Report COINS-CR-92 8 Dept of Computer Science University of Massachusetts 1992 7 J Gehrke V Ganti R Ramakrishnan, and W Loh Boat optimistic decision tree construction In SIGMOD 1999 8 J Gehrke R Ramakrishnan and V Ganti Rainforest A framework for fast decision tree constructionof large datasets In VLDB 1998 9 G Guiffrida W W Chu and D M Hanssens Mining clas sification rules from datasets with large number of many valued attributes In EDBT pages 335-349,2000 IO Y Ma B Liu C K Wong P S Yu and S M Lee Tar geting the right students using data mining In SIGKDD Zurich, Switzerland, August 2000 ll S K Murthy S Kasif and S Salzberg A system for in duction of oblique decision trees In Journal of ArtiJicial Intelligence Research pages 1-32 1994 12 C Shafer R Agrawal and M Mehta Sprint A scalable parallel classifier for data mining In VLDB 1996 13 P E Utgoff and C E Brodley An incremental method for finding multivariate splits for decision trees In ICML pages 14 H Wang and C Zaniolo CMP A fast decision tree classi fier using multivariate predictions In ICDE pages 449-460 2000 I51 D Wilson and T Martinez Improved heterogeneous dis tance functions In Journal of ArtiJicial Intelligence Re search pages 1-34 1997 I61 B Zadrozny and C Elkan Learning and making decisions when costs and probabilities are both unknown In Technical Report CS2001-0664 Dept of Computer Sci UCSD 2001 58-65 1990 549 


Figure 7 Correlations between query con straints and new indexed constraint the index attribute may have different ranges We iden tify all possible constraint introduction solutions as follows Algorithm For each constraint on the index attribute CO.Roj Step 1 find the least expensive association from a query constraint to i.e find the incoming link C~.R  CO.Roj with the fewest exceptions For exam ple if the cardinalities of El     E5 are nl   n5 and n4 5 nz _ n3 then the least expensive association for CO.R is c6.b  CO.R~,\(E  E4 Step 2 filter out all the exceptions that do not satisfy at least one of the query constraints C1.Rl    C12 We do not need to test the exceptions for the antecedent of the corresponding rule since they satisfy it by definition The constraint introduction solutions identified in our example are the following  Introduced Constraints Exceptions ele t El Cz.n,\(e    C12.nl2\(e el  E4 C1.h e    C5.nS e C~.R e     C12.nll e ele  E5 C1.nl e     C5.nS e C~.R e    C12.n12 e CO.Rol CO.RO2 CO.RO3 5 Optimizing OQL queries In the previous sections a series of algorithms were given to find a collection of constraint elimination or con straint introduction solutions In this section we show how the original query is transformed to its optimized form using the optimal solution. Consider the OQL query select x fromExtentX as x where C~.R and   and Cn 5.1 Heuristic H1 Let  Cil   Ci,},Ei  be the maintained con straints and the exceptions of a solution i Only the main tained constraints of the optimization solution should be tested on the objects of the whole extent however all the constraints should be tested on the exception cases and the objects that satisfy the maintained constraints but not the ones omitted should be removed from the result Hence the original query should be converted to the following one select x from Extent2 as x where Cil and   and Ci except Ei If we assume that tests on the query constraints take roughly the same time, the optimal solution is the one with fewest exceptions Ei 5.2 Heuristic H2 Let  Co.Roi Ei  be the index constraint and the corre sponding exceptions of a constraint introduction solution Instead of testing the query constraints C1.~l    C,.R on the entire extent Extent X we apply them only on the re sults of the subquery select x from Extent2 as x 40 where CO.R Since Co.Roi is a constraint on a cluster index attribute, the select operation is expected to be quite fast The original query is transformed to its more efficient form select x from 90 as x where Cl and    and C union Ei The exceptions Ei are merged to the result because they satisfy the query constraints but not the new index con straint Co.Roi Since the union operation is relatively cheap the optimal solution is the one that introduces the index con straint with the highest selectivity 6 Discussion We now look at two different scenarios and estimate the extent to which heuristics H1 and H2 speed up query exe cution The Jirst scenario concerns frequently executed queries Assume that the association rules which are used by algo rithm 3 are not modified We may optimize a query once at compilation time then execute its optimized form It is worth optimizing provided that the execution time of the optimized query is less than the execution time of the orig inal query For heuristic H1 this happens only if the time 132 


saved by omitting some constraints is greater than the time needed to remove the exceptions from the result except operation The more the eliminated constraints and the fewer the exceptions the better the optimization As one of the referees pointed out, the time saved by the elimina tion of constraints is CPU-related Since query execution is dominated by data access time this optimization is not expected to alter performance significantly It would help only in contexts rich in associations with few exceptions in which users express many constraints in their queries Heuristic H2 is expected to bring more significant ben efits Firstly, this optimization involves a union operation which is much cheaper than the except operation used in H1 Secondly instead of retrieving all the objects of an extent from the database we need only look at the subset retrieved through an indexed constraint Hence we save a considerable amount of data access time, spending a negli gible amount of CPU time in evaluating the additional con straint The second scenario concerns queries which are exe cuted only once In this case the time required for opti mization is significant. This time depends on the algorithm that finds associations between relaxed constraints and tight constraints \(see section 3 since this is the most expensive step in the optimization process This algorithm finds paths in a directed graph, and combines the exceptions associated with each edge of the path to derive the total exceptions for the path. Therefore its complexity is a function of i the av erage number of exceptions in the existing association rules and ii the number of different constraints found in the an tecedents and the consequents of the rules We have already implemented the algorithms for apply ing H1 the next step is to implement the corresponding al gorithms for H2 This should not be difficult since the main algorithm  finding associations between rule constraints  is common to the two heuristics We intend to set up an experimental model in order to evaluate H1 and H2 in the scenarios discussed above 7 Conclusion The use of association rules for query optimization is rel evant to both relational and object-oriented database sys tems There has been a lot of research on generating asso ciation rules and maintaining them in the presence of up dates Research has also focused on finding heuristics that take advantage of rules in order to optimize a query Most of this work 2 31 has considered integrity rules rather than association rules with exceptions Semantic optimiza tion heuristics were also applied without considering indi rect associations In this paper we implement algorithms that apply two optimization heuristics presented by Siegel et al taking account of both exceptions and indirect asso ciations We show how to use these heuristics to optimize an OQL query The complexity of the optimization process is closely related to the complexity of the constraint graph which represents the set of association rules in the data It also depends on the number of exceptions associated with each rule We have designed an experimental framework to evaluate the two optimization techniques both in the con text of queries repeated frequently over a period of time, and in the context of ad-hoc queries executed once only The re sults of this experimental work will be presented in a later paper 8 Acknowledgements We are grateful to the anonymous referees who read the paper carefully and critically and made many helpful suggestions Agathoniki Trigoni is supported by a scholar ship from the Greek Scholarships Foundation and is deeply obliged to the National Bank of Greece References I R Agrawal T Imielinski and A Swami Mining asso ciation rules between sets of items in large databases In Proceedings of the 1993 ACM SIGMOD Intl Conference on Management oj data pages 207-2 16 1993 2 U Chakravarthy J Grant and J Minker Logic-based ap proach to semantic query optimization ACM Transactions on Database Systems 15\(2 162-207 1990 3 J Grant, J.Gryz J Minker and L Raschid. Semantic query optimization for object databases In ICDE pages 444-453 1997 4 R Miller and Y Yang Association rules over interval data In ACM SIGMOD 1997 5 J Park An effective hash-based algorithm for mining asso ciation rules In ACM SIGMOD pages 175-186 1995 6 M Siegel E Sciore and S Salveter A method for auto matic rule derivation to support semantic query optimiza tion ACM Transactions on Database Systems 17\(4 600 December 1992 7 R Srikant and R Agrawal Mining quantitative association rules in large relational tables In ACM SIGMOD Intl Con ference on Management ojdata pages 1-12 1996 8 D Tsur J Ullman S Abiteboul C Clifton R Motwani S Nestorov and A Rosenthal Query flocks a general ization of association-rule mining In ACM SIGMOD Intl Conference on Management of data pages 1-12 1998 Intelligent query answering in deductive and object-oriented databases In Fourth ACM Intl Conference on Information and Knowledge Management pages 244 251 1994 IO S Yoon 1 Song and E Park Semantic query processing in object-oriented databases using deductive approach In Intl Conference on information and knowledge management pages 150-157 1995 9 S Yoon 133 


