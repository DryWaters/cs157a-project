An Efficient Weighted Algorithm for Web Information Retrieval System   Rohit Agarwal Deptt of Computer Engineering Application  G.L. A. University Mathura-281406, India  rohit.agrwal@gla.ac.in  K. V. Arya ABV-Indian Institute of Information Technology Management Gwalior, India kvarya@iiitm.ac.in  Shashi Shekhar Deptt of Computer Engineering Application  G.L. A. University Mathura-281406, India  shashi.shekhar@gla.ac.in  
Rakesh Kumar Deptt of Computer Engineering Application  G.L. A. University Mathura-281406, India  rakesh.kumar@gla.ac.in   Abstract Web Usage mining, also known as Web Log mining, is an application of data mining algorithms to Web access logs to find trends and regularities in Web usersê traversal patterns. The results of Web Usage Mining have been used in improving Web site design, business and marketing decision support, user profiling, and Web server system performance. Web page 
prediction technique is a very important research area in web technologies. Mining is useful for web path traversal pattern from web logs. This paper presents an efficient algorithm for web page prediction from large web logs visited by a user. We assign a significant weight to each page based on time spent by user on each page, visiting frequency and click event done on each page Keywords- Weighted Association Rule, Web path traversal, Web usage mining I   I NTRODUCTION  Web Usage Mining th e au to m a tic dis cov er y o f  user access patterns from Web servers. Organizations collect 
large volumes of data in their daily operations, generated automatically by Web servers that are collected in Web access log files. Analysis of these access data can provide useful information for server performance enhancements restructuring a Web site, and direct marketing in e-commerce Data Mining and Knowledge Discovery in Databases KDD\efined as the process of automatic extraction of implicit, novel, useful, and understandable patterns in large databases. There are many steps involved in the Data Mining process, which include data cleaning and preprocessing, data integration, data selection, data transformation and reduction data-mining task and algorithm selection, and lastly post processing and interpretation of discovered knowledg 
This process tends to be highly interactive, incremental and iterative The use of World Wide Web as the means for marketing and selling has increased dramatically in the very recent past Almost every major company has their own web site that acts at least as a promotional tool to increase awareness of the company and its products. As the e commerce activities become more important, organizations must spend more time  to provide the right level of information to their customers How can you tell what content s are being read, whether a web site is effective, or how users read the information Web Usage Mining is the application of established data mining techniques to analyze Web site usage. For an ecommerce company this means detecting future customers 
likely to make a large number of purchases, or predicting which online visitors will click on what ads or banners based on observation of prior visitors who have behaved both positively and negatively to the advertisement banners Most server analysis packages lack the ability to provide any true business insights about visitorsê online behavior Current traffic analysis tool s, like Accrued, Andromedia HitList, NetIntellect, NetTracker, and WebTrends provide high-level predefined reports about domain names, IP addresses, browsers, cookies, and other server activities. These types of reports aim at providing information on the activity of the server rather than the user. Because of the time variant and multi-dimensional nature of the Web logs, we could leverage on the existing OLAP technology and provide a multi 
dimensional browsing capability  The previous two [5 o pos ed s c hemes refl ect t h e  importance of the pages by assigning different weights on that but both of the models assume a fixed weight for each time while in the context of web usage mining as well as  page recommendation systems a page might have different importance in different session In this paper, we propose an efficient weighted algorithm for Web Information Retrieval System. The proposed system will use frequency of a page, time spent on a page and click history of a page to assign a quantitative weight to each page for a user. The nature of this proposed approach is that the time spent on pages [3 iting freq uency a nd click his t o r y  are three factors to show the interest on a page. The Proposed system can get better results of web traversal system 
The organization of the paper is as follows. In Section 2 overview of related work is presented. Section 3 presents the problem statement. In section 4 the proposed weighted association rule based algor ithm is presented. Experimental results and performance evaluation of proposed method is given in Section 5. Section 6 summarizes the paper  II  R ELATED W ORK  In the past ten years, a lot of research work has been done to discover meaningful information from large scale of Web server access logs. A Web mining system, called WEBMINER is presen They offered transactions 
2011 International Conference on Computational Intelligence and Communication Systems 978-0-7695-4587-5/11 $26.00 © 2011 IEEE DOI 10.1109/CICN.2011.25 132 
2011 International Conference on Computational Intelligence and Communication Systems 978-0-7695-4587-5/11 $26.00 © 2011 IEEE DOI 10.1109/CICN.2011.25 126 
2011 International Conference on Computational Intelligence and Communication Systems 978-0-7695-4587-5/11 $26.00 © 2011 IEEE DOI 10.1109/CICN.2011.25 126 


model for various web mining such as discovery of association rules and sequential pattern. They discover the significant association from a large collection of web data  A Web Utilization Miner \(WUM prov ides a robu s t  mining language in order to specify characteristics of discovered frequent paths that are interesting to the analysts In this approach, individual navigation paths, called trails, are combined into an aggregated tree structure. Queries can be answered by mapping them into the intermediate nodes of the tree structure Chen j ected t w o e f f ect iv e al g o rithm s f o r full-scan\lved disagreement between traversal pattern and association rules selective-scan\ able to avoid database scans in some passes. But none of the models reflects the semantic impact of different sequences except the statistical correlation of the sequences An efficient web traversal pattern mining algorithm based on suffix array is given by T. Jing [12  P r o po s e d a l go r i t h m  has lower time and space complexity compared with traditional approaches such as Apriori In ere is an in cre m e n t al data m i n i ng alg orit hm  f o r discovering web traversal patterns when the user sequences are inserted into and deleted from original database. This algorithm uses lattice structure to keep the previous mining results such that just new candidate sequences need to be computed. Hence, the web traversal pattern can be obtained rapidly when the traversal sequence database is updated. But it is unsuccessful when web site structure is changed Zhou ed h i gh utilit y pat h trav ers a l pattern  mining, which introduces the concept of utility into path traversal pattern mining model. They explored a Two-Phase algorithm that can discover high utility traversal paths highly efficiently. They proposed algorithm was applied on a realworld Weblog database A utility-based algorithm for web path traversal was introduced by C. F. Ahmed T h ey u s ed a patter n g r ow t h  sequential mining to prune a huge number of candidates. It effectively divides the search space by small projected databases recursively using the divide and conquers technique Therefore, it saves several scanning of the whole database as done by the existing algorithm Roh pr opos ed t w o ef f ecti v e alg o rithm s F T W  and FTPW for Web Information Retrieval based on Userês Navigational Pattern based on frequency and time spent on a web page All existing models recommend Web Path Traversal, but the work does not hold any precise mechanism to identify the accurate web path traversal system based on different users who have visited previously III  PROBLEM  DEFINITION As the WWW explodes, the use of World Wide Web as the mean for marketing has increased spectacularly in recent years. Almost every organization or company has their own Web site that acts at least as a promotional tool to increase responsiveness of the organizations and its products. As the ecommerce activities become more important, organizations must spend more time to provide the right level of information to their customers as they realize that information can generate revenues, reduce cost and enlarge market shares. But, how do you know what Web contents are being read, whether a Web site is effective, or how users read information? The solution is using Web usage mining  Nowadays, most Web usage analysis tools lack the ability to provide true business insights about visitorsê online behavior. These tools providing information to the user based on previously visited different user. Also, they do not concern the new coming and up-to-date log data. As a result, the derived information will be out-dated soon Keen competition in rapidly changing business environments is expected, and  these conditions will generate increasing demand for reliable, up-to-date and easy to access decision-making information The mathematical background required to develop and evaluate the performance of proposed system is explained as follows A  Page Ranking PageRank [17 is a p r o b a b ility d i strib u tio n  u s ed to rep resen t  the likelihood that a person randomly clicking on links will arrive at any particular page. PageRank can be calculated for collections of documents of any size. It is assumed in several research papers that the distribution is evenly divided between all documents in the collection at the beginning of the computational process. The PageRank computations require several passes, called "iterations", through the collection to adjust approximate PageRank values to more closely reflect the theoretical true value When calculating PageRank, pages with no outbound links are assumed to link out to all other pages in the collection. Their PageRank scores are therefore divided evenly among all other pages. In other words, to be fair with pages that are not sinks, these random transitions are added to all nodes in the Web, with a residual probability of usually d 0.85, estimated from the frequency that an average surfer uses his or her browser's bookmark feature. So, the Equation \(1 as follows 002\003\004\005 006 007\010\011\010\004\012\013\014\007\015\016\010\017\010\014\010 020 010 021 022 023\024\004\021 025 007 026\027\004\021 022 007 030\004\021 022 007 010\010\010\010\010 1 010 where p 1 p 2 p N are the pages under consideration, M\(pi is the set of pages that link to pi, L\(pj\s the number of outbound links on page pj, and N is the total number of pages B  Association rule based on weight Greater weights are given to more important items facilitating the discovery of important but less frequent itemsets However, existing models assume a fixed weight for each item, while in Web usage mining; a page might have different importance in different sessions Item weight is a value attached to an item representing its significance. We denote it as W\(i\ample, in a supermarket setting, it could be the profit per unit sale of a 
133 
127 
127 


certain item. In the web log mining setting where each item is a page visited in a click-stream/transaction, the weight can be related to a userês average dwelling time on that page. In other words, the item weight is a function of selected weighting attributes therefore denoted as W\(i\ f\(a\ this proposed work weight of page pi is denoted as W\(pi Definition.1 Visiting Frequency of a Web page Frequency is the number of times that a page is accessed by a user. It seems natural to assume that Web pages with a higher frequency are of stronger interest to users. A parameter that must be considered in the calculating the frequency of a page is the in-degree of that page \(e.g. the number of incoming links to the page\ obvious that a page with large indegree has more probability to be visited by a user than a page with small one. Specially, in comparing two pages with same visiting rate, the page with small in degree is more interesting Definition.2 Time Spent on a Web page  Several reasons validate the idea of using pages visit duration as one of the weighting parameters. First, it reflects the relative importance of each page, because a user generally spend more time on a useful page, because if a user is not interested in a page, he/she do not spend much time on viewing the page and usually jumps to another page quickly However, a quick jump might also occur due to the short length of a Web page so the size of a page may affect the actual visiting time. Hence, it is more appropriate to accordingly normalize duration by the length of the Web page that is, the total bytes of the page. Second, the rates of most human beings getting information from Web pages should differ greatly. As page duration can be calculated from Web logs, it is a good choice for inferring user interest Definition.3 Click Event done on a Web page Website administrators are constantly looking for new ways to optimize their site in order to attract more visitors. They are also concerned with increasing acceptance and popularity Whether a Website goal is to entertain, or provide information and services, its design should support the intended goals Website tracking and usability testing can provide valuable insight as to what changes are necessary in order to reach those goals Website visitors often change their surfing habits overtime. As visitors gain familiarity with a site, they approach it differently. Surfing experts navigate in entirely different way from novices \(using shortcuts, skimming through Websites searching for matching words, etc\ tool that monitors Web users as they evolve is necessary  MouseTrack [16 is a W e b b a sed u s ab ilit y  s y ste m  w i t h an  online configuration tool and visualization tool that displays the mouse path followed by Website visitors IV  THE  P ROPOSED APPROACH   We have proposed an efficient algorithm, for path traversal patterns which provides an online and up-to-date browsing capacity of user behavior, detecting user navigation paths and analyzing them may result in a better understanding of how users visit a Website. The proposed system captures the online behavior of a visiting user on a Website because it can be used to recommend the Web Path Traversal Pattern for the Web site Let U = {u 1 u 2   u n denote the set of web pages accessed by a user and each of them is distinctively recognized by its related URL. The weight can be calculated in number of ways. The primary sources of data are server access logs 002  FREQUENY, TIME AND CLICK BASED PAGE WEIGHT ALGORITHM  The proposed algorithm Frequency, Time and Click based Page Weight \(FTCPW\gorithm based on Visiting Frequency, Time Spent and click based on a Web page The admittance frequency of a page u is the number of times the page is visited after page v The frequency weight FW\ given by w h ere PR repres en ts t h e pag e rank a n d calculated with the help of \(1\ a particular page  FW\(u 011 031\032\033\034\035\036 037 !"#"$\010\037%\010&\010\021&'\035\004\032\007 037 032\033\034\035\036 037 \010*"#"$\010\037%\010&\\\010\021&'\035#\010 010  010 PR \(u 2  Time spent on a page replicates the comparative significance of each page, because a user generally spend more time on a more useful page and does not waste more time on screening the page and rapidly skip to another page. There are two parameters: transfer rate and web page size should be considered to calculate the real time spent on a page. The formula to calculate the time spent on each page is given in \(3   004  007 011 012\01034256\01075\0108\0104892 004  007 010<7=5>78?\010/012\010\004:\007 18 ABC 004/012\01034256\01075\0108\0104892 004  007 010<7=5>78?\010/012 004  007 007 3 Where  DEFGHEIJ\010,KLM\004.\007 011 N0O2\004:\007 P853Q2P\010R862\010Q7P\010S892\004:\007  4  If a user just opens the web page but has no interest on that web page \(for example user is not physically available to do any operation on that page\is will show high but inaccurate time spent weight on that page but actually user is not interested on that page. So, to overcome this  problem we are taking a new parameter i.e. click event done on that page which shows the userês interest on that page. This suggests that a mouse cursor is an indicative to where the user focuses its attention and can be used to classify user navigation behavior into several categories, such as scrolling, interacting with menus etc. We have used MouseTrack a tool f o r monitoring and visualizing mouse movement activity on any website. It allows administrators to easily incorporate mouse tracking into any existing website. Therefore, the third parameter is Click Event Weight \(CEW  002 003 004  otherwise done is event an If CEW 0 5  0 5   
134 
128 
128 


Three parameters: visiting frequency, time spent and click event are used to evaluate the userês interest on the web page and hence, the weight to each page is assigned using \(6\The Pseudo code algorithm to estimate these parameters is given in Figure 1 W\(u\W\(u\TW\(u\EW\(u\         \(6             Figure 1: Pseudo code for proposed Algorithm V  P ERFORMANCE E VALUATIONS  This section deals with the implementation details to the proposed structure. We first discuss the simulator developed for simulating real world scenario then we get into the details of the component implementations. We evaluated proposed algorithm by running it on the developed simulator and then comparing its performances based on some specific attributes A  SIMULATOR We have developed a simulator in .net framework using c#. We implement a number of prominent factors in our simulator. The following activities are major part of the simulator and block diagram of simulator is given by Figure 2 The main blocks of the simulator are described below             Figure 2: Block diagram of Simulator 002  Selection of Behavior for registered user   When a user comes to access the Web site, if the user is new then system will not recommend the Web Path Traversal. If the user has already visited the Web site then system will select the behavior of that user in terms of the weight of each Web page based on previous visit history. Behavior selection chooses distinguishing features from a set of features to generate useful and novel features from the original ones  002  Weight Calculation When a user will visit the Web site first time and on log out from Web site behavior of user will be stored in the data base and corresponding weight for each Web page is calculated The weight of each Web page shows the importance of that page i.e.  higher weight shows the more importance of that Web page  002  Results Interpretation After calculation the weight of each Web page, the system will recommend the Web Path Traversal based on the weight of each Web page. When a user logs in to the Web site second time the system will provide a link for recommendation of preferences for Web pages B  DATA SET We have evaluated the performance of proposed algorithm by using the same synthetic data set that we have already taken for previous two algorithms, for different users to calculate the weight of each web page. The following factors are considered for performance evaluation   Outbound links   Page Rank \(PR   Visiting frequency of each web page by user   Page Size   Time spent   Transfer Rate   Click event The above parameters used in proposed experimental set up which applies for the Web Path Traversal C  Evaluation Method The attributes taking into account for the training data set for each user of a web site is represented in terms of time spent on each page, frequency and click event. From these data sets we are calculating the weight for each web page for respective web site. The proposed approach uses three parameters i.e. visiting Frequency, Time Spent on a Web page and Click event on a Web page to measure the weight of each web page. To estimate the performance of the proposed approach we have considered the two another proposed algorithms D  Experimental Results This section presents the experimental results on the performance of proposed algorithm The performance of the proposed approach can be evaluated by comparing the performance of FPW and FTPW algorithms w h i c h di ffer i n num ber of para m e t e rs  considered for experimentation. The experimental setup uses Input: Web traversal path database Output: Weight for each page 1 Calculate PageRank for each page \(PRi 2 Initially Wi \(Weight for each web page\0 3 First check the user is registered or not, if YES then 4 check the user is first time, if YES then 5 return Wi 6 else 7  calculate FWi= no_visit/ T_visit * PRi 8  calculate D\(i\ Size\(i\ransfer Rate 9 calculate TWi=\(TSi-D\(i\X\(\(TSi-D\(i 10  if \(Click event done\n 11   CEW=0.5 12 else 13   CEW=0 14  SET   Wi = FWi+TWi+CEWi 15 return Wi   Selection of Behavior For registered Data Weight Calculation Interpretation of Results Knowled g e 
135 
129 
129 


five users and weights are plotted against v a Figure 3 shows the plot b etween by co m Frequency, Time spent on Web page and C l page together with Weight of a Web page Figure 3: Plot between \(frequency+time spent  weight Finally by applying FTCPW \(Frequency b ased Page Weight\orithm the recomm e Path Traversal based on  weight calculate d b een made and corresponding path is dep i  Figure 4: Recommendation Web Traversal Path based o  Figure5 shows the relative Recommendatio n Path based on FW, FTPW and FTCPW Alg o  USER\002 FPW Algorithm FTPW Algorithm USER 1 A->C->D->B C->B->D->A USER 2 D->B->A->C A->B->C->D USER 3 A->B->C->D B->C->A->D USER 4 A->B->C->D D->A->C->B USER 5 B->A->D->C B->D->A->C  Figure5: Recommendation of Web Traversal Path bas e FTCPW Algorithm  Figure 6 shows the relative execution for 0 0.5 1 1.5 2 Weight FTCPW \(Frequecy, Time and Clic k 0 0.5 1 1.5 2 Weight Web Pages a rious parameters m bing the Visiting l ick event on Web   click event\nd Time and Click e ndations for Web d in Figure 3 have i cted in Figure 4  o n FTCPW Algorithm n of Web Traversal o rith m  FTCPW Algorithm D->C->B->A B->A->C->D C->B->A->D D->B->A->C D->C->B->A e d on FW,FTPW and FPW, FTPW and FTCPW on the synthetic data FTCPW is more efficient tha n the performance of proposed increase the complexity of al g and provide better Web Path T r  Figure 5: Relative Accessibility t A matrix depicted in the comparison among all the th r the parameters are performa n result shows that the perform a we increase the number of par a FPW algorithm to FTCPW alg o  Attributes\002 FPW Algorithm Recognition of User behavior Less Visiting Frequency Yes Page Rank Yes Time Spent on Web page No Click Event on Web Page No Page Size No Data Transfer Rate No Accessibility Time for required Information High Enhancing server performance Low Figure 6: Co m  The experimental results dra w b etter and provides a method o optimized Web path traversal p ast navigation behavior by c o page. In addition to the same Frequency, Time Spent, Click E most appropriate page are use d k  User1 User2 User3 User4 User5 User1\(D->C->B->A User2\(B->A->C->D User3\(C->B->A->D User4\(D->B->A->C User5\(D->C->B->A 0 2 4 6 12 Accessibility Time for More required Information U sets, in which we can see that n other two algorithms. Hence algorith m increases when we g orithm in terms of parameters r aversal in less time   t ime for FPW, FTPW and FTCPW  Figure 6 describes rigorous r ee proposed algorithms. Here n ce centric and a comparison a nce of the system improves as a meters i.e. when we move from o rithm FTPW Algorithm FTCPW Algorithm Medium High Yes Yes Yes Yes Yes Yes No Yes Yes Yes No Yes Average Low Average High m parison Matrix w n for FTCPW algorithms are o logy for effective, efficient and for various users based on their o mputing weight for each Web the other attributes viz Visiting E vent, Page Size and Display of d for better comparison 345 ser FTCPW FTPW FPW 
136 
130 
130 


VI  C ONCLUSION  FUTURE WORK  Web page prediction technique plays a very important role in Web technologies. The key contribution of this paper is to provide a very efficient algorithm for web path traversal Three factors frequency, time spent and click event were used to decide the Web path traversal. The experimental results show that in the proposed system  when we increase the number of parameters for finding the Web path the accuracy of the system is enhanced drastically and FTCPW produces more accurate results than those achieved by FTPW and FTPW produced better than FPW In the future, we shall improve the Web Path Traversal by introducing the concept of Web personalization for accurate Web Path traversal R EFERENCES  1   F a yy a d  U  Pia t e t s k y Sh a p i r o G  a n d Smy t h P F r om Da t a Mi ni n g  t o  Knowledge Discovery: An Overview. In Advances in Knowlede Discovery and Data Mining, G. Piatetsky-Shapiro and J. Frawley editors, AAAI Press, Menlo Park, CA, \(1996 2    Za  a n e O. R Xin    M Ha n, J  Di s c over i n g W e b A c c e s s P a tt er n s  a nd Trends by Applying OLAP and Data Mining Technology on Web Logs In Proceedings of Advances in Digital Libraries Conference \(ADLê98 pp. 19-29, Santa Barbara, CA, USA, April \(1998 3 Da vi d  V  Ma ls eed  M T h e G oog le St ory  pp   37 200 5  A v a i la b l e a t  URL: http://www.thegooglestory.com 4  P e i, J., H a n, J Mo r t az av iA s l  B Z hu, H   M i n i ng A cce ss P a tte r ns  Efficiently from Web Logs. In: The Pacific-Asia Conference on Knowledge Discovery and Data Mining, 396--407 \(2000 5  Cai C  H F u A  W  C., C h e n g  C. H K w o ng  W  W   Min ing  Association Rules with Weighted Items. In Database Engineering and Applications Symposium \( IDEAS'98\, 68--77 \(1998 6  T a o  F Mur tag h, F F a r i d, N   W e ig hte d A s s o ciat io n Rul e Mi ni ng  using Weighted Support and Significance Framework, In: The 9th SIGKDD conference, \(2003 7 Ye n  Sh ow Ja n e A n E ffi c i e n t A p p r oa c h for A n a l y z i n g Use r B e h a vi ors in a Web-Based Training Environment. International Journal of Distance Education Technologies, 1\(4\, 55--71 \(2003 8 Yen  S h ow J a n e  L e e Yu e-Sh i  Ch o ,C hun g-W en   E ffi c i ent A p p r oa ch for the Maintenance of Path Traversal Patterns, In: IEEE International Conference on e-Technology, e-Commerce and e-Service \(EEE\, 207-214 \(2004 9  Mo ba s h e r B J a i n N   H a n  E  e t  a l   W e b m i ni ng  P a tte r n  dis co v e ry  from World Wide Web transactions, Tech Rep: TR96-050, \(1996 10 S p il io po ul o u  M F a ul s t ic h, L  C   W u m  A  w e b ut il iz atio n m ine r   ED BT  Workshop WebDB98, Springer Verlag \(1996  en  M  S Pa r k J  S. Yu P  S  E f fi c i ent da ta m i nin g for p a th t r a v ers a l  patterns.  IEEE Transactions on Knowledge and Data Engineering, 209-221 \(1998 12 J i ng T Z o u, W a nL i., Z h ang Ba ng Z uo A n Ef f i cie nt W e b T r av e r s a l  Pattern Mining algorithm Based On Suffix Array, Proceedings of the 3rd International Conference on Machine Learning and Cybernetics, 15351539 \(2004  Yen S h ow J a n e  Lee Yu eS h i   Hs i e h   Mi n C h i  A n E f fic i en t Incremental Algorithm for Mining Web Traversal Patterns. In: The 2005 IEEE International Conference on e-Business Engineering \(ICEBEê05 274-281 \(2005 14 Z h o u  L  L i u, Y W a ng  J S h i, Y   U t il ity bas e d W e b P a th T r av e r s a l  Pattern Mining. In: Seventh IEEE International Conference on Data Mining Workshops, 373-378 \(2007 15 A h m e d C  F T a nbe e r S  K  Je o n g  By e o ng S o o  L e e  Y o ung K oo  Efficient mining of utility-based web path traversal patterns. In: 11th International Conference on Advanced Communication Technology ICACTê09\, 2215-2218 \(2009 16 A r r o y o E., S e l k ar T W e i W   U s abil ity T o ol f o r  A n aly s is o f W e b Designs Using Mouse Tracks. CHI '06 extended abstracts on Human factors in computing systems MontrÈal, QuÈbec, Canada, 484-489 2006 17  ht tp  e n w ik ipe d i a  o r g  w iki/P ag e R a n k  18   Ro hi t A g ar w a l   K  V  A r y a S h as hi S h e k har   A n A r chite c t ur al  Framework for Web Information  Retrieval based on Userês Navigational Pattern é, Proceedings of the IEEE International Conference on Industrial and Information System \(ICIIS-2010\, pp. 195200, 2010  
137 
131 
131 


simplicity we choose D1=12 and D2=8, thus the universe of discourse for the specification stage is defined as U=[10 , 170]. This means that based on available historical data the effort records of specification stage is delimited between 10 to 170 man- months. Let U be divided into four equal intervals with equal length as following L= [ ] 40 4      with: W1 =[10, 50 W4 =[130, 170  Step 2: define a corresponding linguistic variable Fuzzy set discourse U. This step has been used intensively in Fuzzy time series. The number of Fuzzy sets must be related to the number of intervals. Let A1, A2,A3,,An be Fuzzy sets represent linguistic terms where A1 is defined as depicted in eq. 3    3  where  jA Wi? is the membership degree of interval jW in Fuzzy set iA n corresponds to the number of intervals Therefore the linguistic terms A1, A2 , A3 ,,An will be defined as follows 0,/0,/0......,/0,/0,/5.0,/1{ 1243211 nnn WWWWWWWA 0,/0,/0......,/0,/5.0,/1,/5.0{ 1243212 nnn WWWWWWWA   1,/5.0,/0...../0.,/0,/.0,/0{ 124321 nnnn WWWWWWWA Based on the previous example in step 1, the possible Fuzzy sets for the four intervals W1 , W2 ,W3 ,W4 should be defined as follows 0,0,5.0,1 43211 WWWW A 0,5.0,1,5.0 


43212 WWWW A 5.0,1,5.0,0 43213 WWWW A 1,5.0,0,0 43214 WWWW A  Step 3: determine the target stage and discover association rules between prior stage\(s stage. In this step we used predictive APRIORI algorithm [22] that is implemented in WEKA data mining tool [26]. The minimum support is set by 0.01 and minimum confidence is set by 0.8. These values have been carefully chosen to avoid too few rules that would occur if the confidence was very high In this paper we will replace the name of all stages with the following abbreviations. The number preceding the abbreviation represents the order of stage in software development process. \(1 planning stage. \(2 3 4 building stage. \(5 6 Effort of implementation stage 251 Step 4: filtering extracted rules. All generated rules are filtered to obtain interesting rules that contain specified target as consequent and all rules should respect stage order integrity. This means that all stages in antecedent parts should not precede target stage in consequent part. For example, if the target stage is the design phase: ED then all rules that contain this phase only as consequent will be considered for further processing and others are neglected. The following rules are accepted for further processing, the number after abbreviation denotes corresponding Fuzzy set interval EP1=>ED4 ES2 and EP3=>ED2 Conversely, the following rules are neglected because there have problems in either antecedent or 


consequent part EP1 and ES2=>ED1 & ET3: because ED1 should appear alone in consequent part ES1 and EI=>ED1: because EI cannot precede ED  Step 5: calculate the predicted output. Firstly defuzzify all expected outputs with regards to target stage 2,1        1 1 ni W WmW Adefuzz n j jA n j jjA i i i        4  where jWm is the centre value of expected interval of target stage in historical dataset. Secondly, the estimated effort is calculated by computing the weight average of defuzzification values. The weight here is confidence ratio of extracted rules as shown in Eq. 5 For example suppose we want to predict specification stage of a project. Consider prior stage is software plan phase and its effort value is located in 


the first interval \(EP1 following rules have been extracted EP1=>ES4 \(confidence= 0.932 EP1=>ES3 \(confidence= 0.843 EP1=>ES1 \(confidence= 0.78 Then corresponding Fuzzy sets that represent expected target stage based on previous rules should be defuzzfied. From this example we can observe that the input interval has many relations with target intervals i.e. EP1 has three significant relations with ES4, ES3 and ES2 in the specification phase. Therefore we need to take their impacts on the final estimate. The effort for specification phase stage is calculated as following      k i i k i ii confidencerule confidenceruleAdefuzz E 1 1   5 where k is the number of rules  15.000  1 5.0 0 0  43214    WmWmWmWm Adefuzz ES  


5.015.00  5.0 1 5.0 0  43213    WmWmWmWm Adefuzz ES  005.01  0 0 5.0 1  43211    WmWmWmWm Adefuzz ES  Assume 70 55 35 20 4321 ==== WmWmWmWm then 65 4 =ESAdefuzz , 75.53 3 =ESAdefuzz 25 1 =ESAdefuzz  By using equation 5 the predicted effort is  78.0843.0932.0  78.0 843.0 932.0 234    ESESES AdefuzzAdefuzzAdefuzzE monthsmanE ?= 08.49    VI. EVALUATION CRITERIA  Many evaluation criteria are introduced in software engineering literature, among them we selected three evaluation criteria are Bias, Mean Magnitude of relative errors \(MMRE relative errors \(MdMRE 6 


to check whether the proposed prediction model is biased and tends to under or over estimation. MMRE in equation \(7 in an individual estimate and should be less than 25 to be acceptable. Since the MMRE is sensitive to the individual prediction with large MRE we adopt median MRE \(MdMRE value of MRE. The acceptable target for MMRE and MdMRE is less or equal to25  i ii actual estimatedactualiBias 6    n i iBias n MMRE 1  1  \(7 252 iiBiasmedianMdMRE  8 VII. RESULTS AND DISCUSSION  The dataset used in empirical validation came from ISBSG [24]. The obtained dataset contains effort records for six phases are: plan effort \(EP specification effort \(ES ED effort \(EB ET effort \(EI processing we attempted to select the most representative data, therefore we ignored the projects records that contain missing values Determining the possible number of intervals in each stage is carried out based on the distribution of effort data in each stage as shown in Figures 1 to 6 There is no clear mechanism for how to determine the 


perfect number of intervals therefore we attempted to study density of data for each stage separately. The performed analysis resulted in different number of intervals between stages. The obtained number of intervals reflects the density and range of data in each stage as shown in Table I. For Association rule mining the minimum support is set by 0.01 and minimum confidence is set by 0.8. These values have been carefully chosen to avoid too few rules that would occur if the confidence was very high The theme of this paper is to address the following arising issue: can project manager relay on prior effort records to predict next stage effort? To answer this question, the proposed model has been evaluated using jack-knifing method. We used 34 projects with complete effort records. The complete procedure of experiment is described below 1. Project number i is removed from data set as test project \(i.e. for which the stage estimate is required 2. The proposed procedure applied on the remaining projects 3. Making prediction for the test project, and MRE and residuals of test project is recorded 4. Project i, which had been removed from the data set is added back Table II and Table III depict the results obtained by our proposed approach compared to exponential regression where target stage is regarded as dependent variable and all pervious stages as independent variables Table II we can observe that all outputs tend to be under estimation. Three out of five stages producing good estimate are specification, building and testing while design stage produced better results compared to implementation stage \(which produced the worst stage effort estimation in terms of MMRE related to that the ISBSG is scattered as result of collection from different worldwide companies. The effort records have complex structure in which there is no consistent structure for all effort records. Based on MdMRE we can observe that our approach in most of stages produced comparable estimation accuracy with maximum 30.2% in implementation stage 


 Table I.  Number of intervals Stage Interval No. Stage Interval No EP 7 ES 8 ED 10 EB 9 ET 8 EI 11  Table II.  Results using the proposed approach Stage Bias MMRE MdMRE ES -8.5% 27.0% 17.0 ED -33.1% 40.5% 13.7 EB -2.8% 9.3% 7.5 ET -11.6% 16.7% 7.23 EI -20% 91.0% 30.2  Table III.  Results using exponential regression Stage Bias MMRE MdMRE ES -24.3% 81.3% 49.7 ED -72.3% 120.4% 54.224 EB 0.7% 44.35% 37.6 ET -45.4% 81.1% 39.0 EI -179% 184% 104.0  Results shown in Table III revealed that most of predictions are under estimation which supports our approach findings. The best estimation accuracy was obtained in building stage, which also corroborates our findings that best estimation accuracy was in building stage. The negative values in Bias criterion show underestimation. It is acknowledged that MMRE is unbalanced in many validation circumstances and leads to overestimation more than underestimation. In our case, we found that MMRE leads to underestimation in most stages. This is may be related to the absence of systematic scheme between all prior effort records   253   Figure 1. Effort distribution of Planning stage 


Figure 2. Effort distribution of Specification stage Figure 3. Effort distribution of Design effort stage  Figure 4. Effort distribution of Building stage Figure 5. Effort distribution of Testing stage Figure 6. Effort distribution of Imp stage   Table IV. Statistical significance Stage sum rank Z-value p-Value ES 769 -4.31 <0.01 ED 713 -5.03 <0.01 EB 685 -5.4 <0.01 ET 595 -6.54 <0.01 EI 799 -3.93 <0.01  The comparison between our approach and exponential regression technique showed that there are considerable improvements in estimation accuracy on all phases of software development lifecycle. MMREs of our approach have been reduced by at least 35.05 and at most 93%. Biases have been reduced by at least 3.5% and at most 159%.We have to bear in mind that the length of interval plays important role in estimation accuracy, thus, when the universe of discourse is partitioned into several equal intervals, the distribution of data should be taken into account. Moreover, we should remove the extreme values because they affect interval partitioning, thus, estimation accuracy Figures 7 to 11 show comparison between proposed approach and exponential regression in each stage by using Boxplot. The Boxplot [17] offers a way to compare between estimation models based on their absolute residuals. The Boxplot is non-parametric statistics used to show the median as central tendency of distribution, interquartile range and the outliers of individual models [17]. The length of Boxplot from 


lower tail to upper tail shows the spread of the distribution. The length of box represents the interquartile range that contains 50% of observations The position of median inside the box and length of Boxplot indicate the skewness of distribution. A Boxplot with a small box and long tails represents a very peaked distribution while a Boxplot with long box represents a flatter distribution. The prominent and common characteristic among these figures is the spread of absolute residuals for our approach is less than spread of exponential regression which presents more accurate results. The larger interquartile of exponential regression indicates a high dispersion of the absolute residuals. The Boxplot revealed that the box length for our models is smaller than exponential regression which also indicates reduced variability of absolute residuals. The median of our model is smaller than median of exponential regression which revealed that at least half of the predictions of our model are more accurate than exponential regression 254  Figure 7. Boxplot of absolute residuals for the specification stage  Figure 8. Boxplot of absolute residuals for the design stage   Figure 9. Boxplot of absolute residuals for the building stage Figure 10. Boxplot of absolute residuals for the testing stage   The lower tails of our model is much smaller than upper tail which means the absolute residuals are skewed towards the smaller value Figure 11 illustrates the reason of why prediction of implementation stage in our approach produced the worst accuracy. The reason related to the existing of outlier. Although one project is considered as an outlier the MMRE is easily influenced with that project Based on the obtained results, we can observe that 


exponential regression gave bad accuracy. The reason may relate to the structure complexity of prior effort records. There is no correlation between all prior stages and target stage To ensure that the results obtained are not by chance we investigated the statistical significance of the proposed approach using Wilcoxon sum rank test for absolute residuals as shown in Table IV. In this test if the resulting p-value is small \(p<0.05 statistically significant difference can be accepted between the two samples median. The residuals obtained using the proposed approach were significantly different from those obtained by exponential regression Suggesting that, there is difference if the predications generated using the proposed approach or exponential regression and based on the accuracy comparison in Tables II and III we can safely conclude that our proposed method outperformed exponential regression for stage effort estimation Figure 11. Boxplot of absolute residuals for the implementation stage  VIII. CONCLUSIONS Some of software projects are failed due to the absence of re-estimation during software development which results in huge gap between initial plan and final outcome. Even with good estimate at first stage the project manager must keep update with project progress and should be able to re-estimate the project at any particular point of project in order to re-allocate the proper number of resources. The objective of this paper was to check whether the prior effort records can 255 be used to predict stage effort with reasonable accuracy or not. The obtained results revealed that using association rule and Fuzzy set theory lead to significant improvement in stage-effort estimation and give project manager an evolving picture about project progress. Comparing our approach with exponential regression showed that there is a considerable potential in estimation accuracy. As part of future plan, we 


intend to expand this work to involve some interesting features in each stage prediction and evaluate it on many datasets   REFERENCES  1] F. Ricardo, N. Ana, M. Paula, B. Gleidson, R. Fabiano ODE: Ontology-based software Development Environment, Proceedings of the IX Argentine Congress on Computer Science, pp. 1124-1135, 2003 2] E. Mendes, B. A. Kitchenham. Further comparison of cross-company and within-company effort estimation models for Web applications. In: Proc. 10th IEEE International Software Metrics Symposium, Chicago USA, pp.348-357, 2004 3] B. Boehm, R. Valerdi. Achievements and Challenges in Software Resource Estimation, Proceedings of ICSE 06 Shanghai, China, pp. 74-83,  2006 4] K. Molokken, M. Jorgensen. A review of software surveys on software effort estimation, Proceedings of International Symposium on Empirical Software Engineering \(ISESE 2003 5] M. Jorgensen, K. Molokken-Ostvold. How large are software cost overruns? A review of the 1994 CHAOS report, Information and Software Technology, Vol. 48 issue 4. PP. 297-301, 2006 6] X. Huanga, D. Hob, J. Rena, L. F. Capretz. Improving the COCOMO model using a neuro-Fuzzy approach Applied Soft Computing, Vol.7, issue 1, pp. 29-40, 2007 7] L. Briand, T. Langley, I. Wieczorek. A replicated assessment and comparison of common software cost modeling techniques, Proceedings of the 22nd international conference on Software Engineering, 2000 8] S.-J Huang, N. H. Chiu. Optimization of analogy weights by genetic algorithm for software effort estimation Information and Software Technology, Vol. 48, issue 11 pp. 1034-1045, 2006 9] Z. Xu, T. M. Khoshgoftaar. Identification of Fuzzy models of software cost estimation, Fuzzy Sets and Systems, Vol. 145, issue 1, pp. 141-163, 2004 10] R. Pressman. Software Engineering: practitioner 


approaches, McGraw Hill, London, 2004 11] M. Boraso, C. Montangero, H. Sedhi. Software cost estimation: an experimental study of model performance Universita di Pisa, Italy, 1996 12] Y. Wang, Q. Song, J. Shen., 2007. Grey Learning Based Software Stage-Effort Estimation. International Conference on Machine Learning and Cybernetics, pp 1470-1475, 2007 13] S. G. MacDonell, M. J. Shepperd. Using prior-phase effort records for re-estimation during software projects Ninth International, Software Metrics Symposium, pp 73- 86, 2003 14] M .C Ohlsson, C. Wohlin. An Empirical Study of Effort Estimation during Project Execution, Sixth International Software Metrics Symposium \(METRICS'99 1999 15] N. H. Chiu,,S. J. Huang.  The adjusted analogy-based software effort estimation based on similarity distances Journal of Systems and Software, Vol. 80, issue 4, pp 628-640, 2007 16] P. Sentas, L. Angelis, I. Stamelos, G.  Bleris. Software productivity and effort prediction with ordinal regression Information and Software Technology, Vol. 47, issue 1 pp. 17-29, 2005 17] E. Mendes, N. Mosley. Comparing effort prediction models for Web design and authoring using boxplots Australian Computer Science Communications,  Vol. 23 Issue 1, pp. 125-133, 2001 18] E. Mendes, N. Mosley, I. Watson. A comparison of casebased reasoning approaches, Proceedings of the 11th international conference on World Wide Web, pp. 272280, 2002 19] Q. Zhao, S. S. Bhowmick. Association Rule Mining: A Survey  http://citeseer.ist.psu.edu/734613.html, 2003 20] S. Morisak, A. Monden, H. Tamada. An Extension of association rule mining for software engineering data repositories, Information Science Technical Report NAIST, 2006 21] Q. Song, M. Shepperd.  M. Cartwright, C. Mair. Software defect association mining and defect correction effort prediction, IEEE transaction on software engineering Vol. 32, No.2, pp. 69-82, 2006 


22] R. Agrawal, T. Amielinski, A. Swami. Mining association rule between sets of items in large databases Proceedings of the ACM SIGMOD International Conference on Management of Data, pp. 207-216, 1993 23] M-J. Huang, Y-L. Tsou, S-C. Lee.  Integrating Fuzzy data mining and Fuzzy artificial neural networks for discovering implicit knowledge, J. Knowledge-Based Systems, Vol.19 \(6 24] ISBSG International Software Benchmarking standards Group, Data repository release 10, Site http://www.isbsg.org, 2007 25] L. Zadeh. Toward a theory of Fuzzy information granulation and its centrality in human reasoning and Fuzzy logic. J. Fuzzy sets and Systems 90, pp. 111-127 1997 26] I. H. Witten, E. Frank. Data Mining: Practical machine learning tools and techniques, 2nd Edition, Morgan Kaufmann, San Francisco, 2005   256 


encountering a related term, i.e. IC\(c e intuition behind the use of the negative likelihood is that the more probable a term to appear, the less information it conveys. All these features show that Jiangs measure tends to be more general and more appropriate for evaluating nontaxonomically related terms. Indeed, a high score of the relatedness measures suggests a strong relationship between terms Nevertheless, all relatedness measures have limitations because they assume that all the semantic content of a particular term is modeled by semantic links in WordNet Consequently, in many situations, truly related terms obtain a low scores even though their belongings to a certain category of tags, e.g., jargon tags Additionally, when measuring the quality of an automatically knowledge acquisition results, the typical measures used in Information Retrieval are Recall, Precision and F-Measure. However, computing Recall and F-Measure requires the availability of a Gold Standard. Hence, we will only compute the Precision which speci?es to which extent the non-taxonomic relationships is extracted correctly. In this case, the ratio between the correctly extracted relations i.e., their relatedness measures is greater than or equal to a minimum threshold, and the whole number of extracted ones is computed. Thus, we have Precision Total correctly selected entities Total selected entities 12 http://search.cpan.org/dist/WordNet-Similarity 13 A term refers to a tag subject or a tag object C. Evaluation of non-taxonomic relationships Only a percentage of the full set of non-taxonomic relationships \(89 is caused by the presence of non standard terms which are not contained in WordNet and, in consequence, cannot be evaluated using WordNet-based relatedness measures. Fig. 5 depicts the evaluation results of the extracted non-taxonomic relationships against their relatedness measures High relatedness score \(88 17% of the extracted relationships, as most of terms are strongly related with respect WordNet Null Scores were obtained for 5% of the extracted 


relationships. Analyzing this case in more detail, we have observed that the poor score is caused in many situations by the way in which Jiangs distance metric works. This latter completely depends on the distance between two terms based on the number of edges found on the path between them in WordNet. In consequence this measure returns a value that does not fully represent reality. For example, on the one hand, Jiangs distance metric returns a null value for the relationship between insurance and car, even though the ?rst is a commonly related to the second, i.e., car involved insurance Finally with a minimal Jiangs distance metric threshold, set to 46%, the computed precision of correctly extracted relationships candidates is equal to 68.8 An example of extracted non-taxonomic relationships is depicted in Table V where each relation describes the subject tag, e.g., tool, the predicate, e.g is being developed within, and the object tag, e.g mesh. Fig. 4 represent a fragment output of the extracted ontological structure where each concept de?nes a set of similar and synonym tags and labels, i.e., mentions has been, revealed, caused and is created with describe the predicates of the non-taxonomic relationships between terms Due to the limitations observed by the automatic evaluation procedure and the lack of gold standards containing non-taxonomic relationships, we have examined the extracted non-taxonomic relationships from a linguistic point 377 Top space      distance     quad great     groovy nifty caused address      addresses extension      quotation   reference  references extensions        referenz     source      refrence sources    rfrences    quotations research    search     searching searchs open-source     open_source 


opensource linux aim     design     designer      designers patern    project     patterns     projekte projects web+design    web_design webdesign internet       internetbs net          web network      networking networks      web discussion     news       password word      words community      communities is_created_with mentions revealed has_been Figure 4. A fragment output of the extracted ontological structure of view. This qualitative evaluation can bring some interesting insights about the kind of results one can expect Invalid relations are extracted: Even though a relation such as music cities skill is considered as correct one since tag subject, tag object and predicate are correctly extracted. From a semantic point of view, this relation has no meaning. Hence, a higher precision is expected Figure 5. Summary of non-taxonomic evaluation measure Table V EXAMPLES OF EXTRACTED NON TAXONOMIC RELATIONSHIPS Subject Predicate Object search has been reference reference mentions search tool is being developed within mesh security added encoding search revealed reference java provides library by performing the sense analysis on complete relations An ambiguity in the extracted predicates between terms is observed: Hence, same relations are redundant since they use a synonym predicates between terms, e.g java provides library and java yields library. Thus we expect that the redundancy removal within extracted relations will be of bene?t for the improvement of the 


obtained results VI. CONCLUSION AND FUTURE WORK The extraction of non-taxonomic relationships from folksonomies is to the best of our knowledge is the least tackled task within ontology building from folksonomy. This is why there is a need of novel and general purpose approaches covering the full process of learning relationships. In this paper, we introduced a new approach called NONTAXFOLKS that starts by pre-processing tags aiming at getting a set of frequent tagsets corresponding to an agreed representation Then, they are used to retrieve related tags using external resources such as WordNet. Thanks to the particular structure of triadic concepts, it allows grouping semantically related tags by considering the semantic relatedness embodied in the different frequencies of co-occurences among users, resources and tags in the folksonomy. Thereafter we introduced an algorithm called NTREXTRACTION for extracting non-taxonomic relationships between pair of tags picked from the triadic concepts. In summary, our approach uses several well known techniques \(such as formal concept analysis or association rule discovering the social bookmaring environnement in order to propose a new way of extracting labeled non-taxonomic relationships between tags. Currently, we are investigating the following topic concerning the discovered predicates between two terms. Indeed, in order to avoid relationships redundancy and thus a redundancy in the builded ontology. One can try to classify them into prede?ned semantic classes, detect synonyms, inverses, etc. A standard classi?cation of verbs could be used for this purpose, adding additional information about the semantic content, e.g., senses, verb types, thematic roles, etc., of predicates relationships 378 REFERENCES 1] J. Pan, S. Taylor, and E. Thomas, Reducing ambiguity in tagging systems with folksonomy search expansion, in Proceedings of the 6th Annual European Semantic Web Conference \(ESWC2009 2] V. S. M. Kavalec, A. Maedche, Discovery of lexical entries for non-taxonomic relations in ontology learning, in Proceedings of the SOFSEM 2004, LNCS, vol. 2932, 2004, pp 249256 


3] L. Specia and E. Motta, Integrating folksonomies with the semantic web, in Proceedings of the 4th European Semantic Web Conference \(ESWC 2007 Innsbruck, Austria, vol. 4519, June 2007, pp. 624639 4] P. Mika, Ontologies are us: A uni?ed model of social networks and semantics, in Proceedings of the 4th International Semantic Web Conference \(ISWC2005 3729, Galway, Ireland, June 2005, pp. 522536 5] P. Schmitz, Inducing ontology from ?ickr tags, in Proceedings of the Workshop on Collaborative Tagging \(WWW 2006 Edinburgh, Scotland, May 2006 6] M. Zhou, S. Bao, X. Wu, and Y. Yu, An unsupervised model for exploring hierarchical semantics from social annotations, in Proceedings of the 6th International Semantic Web Conference and 2nd Asian Semantic Web Conference ISWC/ASWC2007 Korea, vol. 4825, November 2006, pp. 673686 7] C. Schmitz, A. Hotho, R. Jaschke, and G. Stumme, Mining association rules in folksonomies, in Proceedings of the 10th IFCS Conference \(IFCS 2006 2006, pp. 261270 8] A. Hotho, A. Maedche, S. Staab, and V. Zacharias, On knowledgeable unsupervised text mining, in Proceedings of Text Mining Workshop, Physica-Verlag, 2003, pp. 131152 9] A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme, Information retrieval in folksonomies: Search and ranking, in The Semantic Web: Research and Applications, vol. 4011 Springer, 2006, pp. 411426 10] F. Lehmann and R. Wille, A triadic approach to formal concept analysis, in Proceedings of the 3rd International Conference on Conceptual Structures: Applications, Implementation and Theory. Springer-Verlag, 1995, pp. 3243 11] R. Jaschke, A. Hotho, C. Schmitz, B. Ganter, and G.Stumme Discovering shared conceptualizations in folksonomies Web Semantics: Science, Services and Agents on the World Wide Web, vol. 6, pp. 3853, 2008 12] A. Mathes, Folksonomies - cooperative classi?cation and communication through shared metadata, Graduate School of Library and Information Science, University of Illinois Urbana-Champaign, Tech. Rep. LIS590CMC, December 2004 13] H. Lin, J. Davis, and Y. Zhou, An integrated approach 


to extracting ontological structures from folksonomies, in Proceedings of the 6th European Semantic Web Conference ESWC 2009 vol. 5554, 2009, pp. 654668 14] M. Szomszor, H. Alani, K. OHara, and N. Shadbolt, Semantic modelling of user interests based on cross-folksonomy, in Proceedings of the 7th International Semantic Web Conference \(ISWC 2008 15] G.Begelman, P. Keller, and F.Smadja, Automated tag clustering: Improving search and exploration in the tag space, in Proceedings of the the Collaborative Web Tagging Workshop WWW 2006 16] R. Jaschke, A. Hotho, C. Schmitz, B. Ganter, and G. Stumme TRIAS - an algorithm for mining iceberg tri-lattices, in Procedings of the 6th IEEE International Conference on Data Mining, \(ICDM 2006 2006, pp. 907911 17] C. Borgelt, Ef?cient implementation of APRIORI and ECLAT, in FIMI, COEUR Workshop Proceedings, COEURWS.org, vol. 126, 2003 18] J. Tang, H. Leung, Q. Luo, D. Chen, and J. Gong, Towards ontology learning from folksonomies, in Proceedings of the 21st international jont conference on Arti?cal intelligence IJCAI 2009 20892094 19] L. Ding, T. Finin, A. Joshi, R. Pan, R. Cost, Y. Peng P. Reddivari, V. Doshi, and J. Sachs, Swoogle: A search and metadata engine for the semantic web, in Proceedings of the 13th ACM Conference on Information and Knowledge Management, ACM Press, 2004, pp. 652659 20] A. Hliaoutakis, G. Varelas, E. Voutsakis, E. Petrakis, and E. E Milios, Information retrieval by semantic similarity, International Journal on Semantic Web and Information Systems IJSWIS 21] G. Pirro, M. Ruffolo, and D. Talia, Secco: On building semantic links in peer to peer networks, Journal on Data Semantics XII, LNCS 5480, pp. 136, 2009 22] C. Meilicke, H. Stuckenschmidt, and A. Tamilin, Repairing ontology mappings, in Proceedings of the International Conference AAAI 2007, Vancouver, British Columbia, Canada 2007, pp. 14081413 23] S. Ravi and M. Rada, Unsupervised graph-based word sense 


disambiguation using measures of word semantic similarity in Proceedings of the International Conference ICSC 2007 Irvine, California, USA, 2007 24] H. G. A. Budanitsky, Semantic distance in wordnet: an experimental application oriented evaluation of ?ve measures in Proceedings of the International Conference NACCL 2001 Pittsburgh, Pennsylvania, USA, 2007, pp. 2934 25] J. Jiang and D. Conrath, Semantic similarity based on corpus statistics and lexical taxonomy, in Proceedings of the International Conference ROCLING X, 1997 379 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


