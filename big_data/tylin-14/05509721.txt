Indoor SLAM Based on Composite Sensor Mixing Laser Scans and Omnidirectional Images Gabriela Gallegos and Patrick Rives INRIA Sophia Antipolis Mediterranee 2004 Route des Lucioles BP 93 Sophia Antipolis, France Gabriela.Gallegos,Patrick.Rives}@sophia.inria.fr Abstract Vision sensors give mobile robots a relatively cheap means of obtaining rich 3D information of their environment, but lack the depth information that a laser range finder can provide. This paper describes a novel composite sensor approach that combines the information given by an omnidirectional camera and a laser range finder to efficiently solve the indoor Simultaneous Localization and Mapping problem and reconstruct a 3D representation of the environment. We report the results of validating our methodology using a mobile robot equipped with a 2D laser range finder and an omnidirectional camera I. INTRODUCTION A key issue in mobile robotics is to give robots the ability to navigate in an autonomous way in unknown environments based only on their perception. Thus, a mobile robot must be equipped with a perception system capable of providing accurate information of its current location and its surroundings, so that the robot is able to reconstruct a reliable and consistent representation of the environment. There are two interdependent tasks that any mobile robot has to solve localization and mapping. When neither the location of the robot nor the map are known, both tasks must be performed concurrently. This problem, known as Simultaneous Localization and Mapping \(SLAM the seminal work of Smith and Cheeseman [18], [19], and is closely related to the development of sensor technology Nowadays, laser range finders have replaced sonars when possible because of its superior efficacy in estimating distances accurately and their better signal to noise ratio. Many techniques have been developed to make the most of this type of sensor for solving the SLAM problem. Since a laser scan directly provides metric information of the scene, the localization problem can be stated in terms of an odometrybased method where the incremental displacement is found by computing the best rigid transformation that matches two 


successive scans. To match two scans it is necessary to link the individual measurements in one scan with the corresponding measurements in the other scan. This association can be done either using an intermediate representation of the laser data \(e.g. a polygonal approximation [1 by exploiting the raw data [2 Gabriela Gallegos gratefully acnowledges her funding by a CONACYT Mexico Several methods can be found in the literature for 2D and 3D scan matching. These methods are often categorized based on their association rule such as feature to feature or point to point matching. In the feature-based approach [3 5], features such as line segments and corners are extracted from laser scans and then matched against each other. Such approach requires the identification of appropriate features in the environment. On the other hand, point to point matching does not require the environment to be structured or contain any predefined features The Iterative Closest Point [4] \(ICP the most widely used point to point scan matching method that works with range sensors. ICP uses a nearest neighbor association rule to match points, and least squares optimization to compute the best transformation between two scans Two enhanced methods based on ICP were proposed by Lu and Milios [6]: the Iterative Matching Range Point \(IMRP and the Iterative Dual Correspondence \(IDC though ICP and its extensions are fast and in general produce good results, they are only guaranteed to converge towards a local minimum and may not always find the correct transformation. Furthermore, these algorithms suffer from computational complexity problems when dealing with largescale environments because the point to point association rules they use result in a O\(n log\(n best case \(where n is the number of points in a scan overpass these constraints, Diosi and Kleeman proposed the Polar Scan Matching method [7] which avoids searching for point associations by simply matching points with the same bearing. We will discuss this approach in more detail in Section III Despite all the work that has been done to improve techniques to use lasers to solve the SLAM problem, the use of 2D lasers alone limits SLAM to planar motion estimation 


and does not provide sufficiently rich information to reliably identify previously explored regions. Vision sensors are a natural alternative to 2D laser range finders because they provide richer perceptual information. Many works have pursued research on vision-based SLAM [9], either relying on feature-based representations [11] or, more recently, on a direct approach [10]. However, standard cameras only have a small field of view \(typically between 30? and 40 2010 IEEE International Conference on Robotics and Automation Anchorage Convention District May 3-8, 2010, Anchorage, Alaska, USA 978-1-4244-5040-4/10/$26.00 2010 IEEE 3519 using a catadioptric camera [21], [22] one can obtain a full 360? view of the environment. Image acquisition with these omnidirectional cameras has many advantages: it can be done in real time, it is easier to recognize previously observed places whatever the orientation of the robot is and it is also less likely that the robot gets stuck when facing a wall or an obstacle. Thus, vision sensors provide dense and rich 3D information about the environment. Nevertheless, vision alone does not provide the depth information that a laser range finder does, which is crucial for solving the localization problem In this paper we describe a hybrid sensor combining the advantages of a laser range finder and an omnidirectional camera. In its formulation, our work is close to Bibers [20 The major difference is that the process we describe is fully automated and does not require manual postprocessing by an operator The rest of the paper is organized as follows: in Section II we describe the experimental testbed used to validate our methodology and discuss the data acquisition and synchronization process; Section III briefly overviews the Polar Scan Matching method, while our SLAM approach is presented in Section IV; in Section V we discuss the merging of omnidirectional images with laser range data to extract vertical lines and build a 3D representation of the environment; we end with some concluding remarks in Section VI II. EXPERIMENTAL TESTBED Hannibal \(Fig. 1 mobile platform \(MP-S500 


Sick LD-LRS1000 laser, capable of collecting full 360 data. The laser head revolves with variable frequency from 5Hz to 10Hz and the angular resolution can be adjusted up to 1.5? at multiples of 0.125?. The laser has a 30m range. To perform a 360? scan with a resolution of 0.25?, it was necessary to reduce the frequency of the rotor to 5Hz thus obtaining 1400 data points per scan. The perspective camera is a progressive-scan CCD camera \(Marlin F-131B equipped with a telecentric lens and a parabolic mirror \(S80 from Remote Reality the camera is required for merging image and laser data We used the Matlab Omnidirectional Calibration Toolbox developed by Mei [13] to estimate the intrinsic parameters of the camera and the parameters of the parabolic mirror. For the calibration between the camera and the laser we used the method described in [12]. Figure 2 shows the projection of the laser range measurements on the omnidirectional image after calibration Data acquisition and synchronization. Odometry data arrives at a frequency of 50Hz, omnidirectional images at 10Hz and laser measurements at 5Hz. Since data from the different sensors that we use arrive at different frequencies, we implemented a function to synchronize the data as it comes out from the robot Fig. 1. Hannibal robot experimental testbed Fig. 2. Laser data projected on an omnidirectional image after calibration III. POLAR SCAN MATCHING Polar Scan Matching \(PSM  scan matching method that exploits the natural representation of laser scans in a polar coordinate system to reduce the complexity of the matching process. As other scan matching approaches, like the Iterative Closest Point \(ICP PSM method finds the pose of a laser scan with respect to a reference scan by performing a gradient descent search for the transformation that minimizes the square error between corresponding points. In contrast to other matching methods PSM avoids an expensive search for corresponding points by matching points with the same bearing. The method assumes the reference and current scans are given as sequences of range and bearing measurements of the form {rri,?ri}ni=1 and {rci,?ci}ni=1, respectively, and requires an initial estimate xc,yc,?c position and orientation 


current scan in the reference scan coordinate frame. The method may be best described by describing each of its phases a robustness of the method, the filter developed by A.Victorino in [23] is first applied to both scans. The measurements in each resulting scan are then classified into segments according to simple criteria: two consecutive measurements not further than a threshold or three measurements lying ap3520 proximately on the same polar line are assigned to the same segment. Segments consisting of a single point are discarded most mixed pixels maximum range is limited so that two consecutive readings belonging to the same segment cannot be too far apart b mate of the current scan the method needs to know how the current scan would have been measured from the point of view of the reference scan. The projection of the current scan into the reference scan coordinate frame is a sequence of measurements \(r?ci,? ?ci r?ci  rci cos\(?c + ?ci rci sin\(?c + ?ci 1 ci = atan2\(rci sin\(?c + ?ci c + ?ci 2 The bearings of the above sequence do not necessarily coincide with bearings where the laser would have sampled a reading. A range measurement r??ci is computed for each sample bearing by linear interpolation among points belonging to a same segment. Points that would have been occluded are not taken into account, only the smallest range measurement for a bearing is kept c method alternates between translation and orientation estimation. After making a correction to the pose estimate the projection phase is repeated with the corrected estimate The process stops when the magnitude of the last position and orientation correction is smaller than a given threshold hopefully indicating that a minimum has been reached Translation is estimated using a standard weighted least squares method. A correction \(?xc,?yc estimate is found by minimizing the weighted sum of the 


square range residuals ?ni=1 wi\(rri ? r??ci orientation unchanged. The weights are computed as recommended by Dudek and Jenkin [8 wi c2 rri? r??ci 3 Orientation is estimated by computing the average range residual for 1? shifts of the current scan in a 20? window The new orientation estimate is found by fitting a parabola to the shift with the minimum average error and its left and right neighbors The implementation of the PSM method provided by Diosi is tailored to a laser with 1? angular resolution and 180 bearing range. These assumptions are used when transforming sample bearings from radians to indexes into arrays and back. We generalized Diosis implementation to lift these assumptions. Our implementation is parametrized so that it can deal with lasers with arbitrary angular resolution and bearing range. In addition, instead of just returning the pose estimate at the moment the algorithm stops, our implementation keeps record of the estimate with the minimum error and returns it as a result IV. LOCAL AND GLOBAL MAPS WITH SLAM AFFINE-TRANSFORMATION\(x,y return   cos? ?sin? 0 x sin? cos? 0 y 0 0 1 0 0 0 0 1   Fig. 3. Affine transformation for a translation \(x,y rotation around the origin by an angle GLOBAL-MAP\(scan[N 1 SR ? scan[1 2 T1 ? AFFINE-TRANSFORMATION\(SR.x,SR.y,SR 3 T3 ? T1TL 4 Map? APPLY-TRANSFORMATION\(T3,SR 5 for i? 2 to N 


6 SC ? scan[i 7 T2 ? AFFINE-TRANSFORMATION\(SC .x,SC.y,SC 8 T ? T?1L T?11 T2TL 9 \(x,y T\(1,4 2,4 T\(2,1 2,2 10 \(x,y SR,SC,x,y 11 T ?3 ? AFFINE-TRANSFORMATION\(x,y 12 T3 ? T3T ?3 13 Map?Map?APPLY-TRANSFORMATION\(T3,SC 14 SR ? SC 15 T1 ? T2 16 return Map Fig. 4. Pseudocode of the procedure used to incrementally build a global map from a sequence of laser range scans with odometry information We build 2D local maps of the environment using the enhanced PSM implementation described in the previous section. Local maps will be used both, in the localization process and for mapping the environment. Later, these maps will be used in SLAM to reconstruct a 2D global map from which it is possible to recover the pose of the robot at each instant Let TL be the rigid transformation between the laser coordinate frame and the robot coordinate frame. We fix as a global coordinate frame the coordinate frame of the odometry data. Let \(x,y coordinate frame. The affine transformation matrix from the laser coordinate frame to the global coordinate frame is given by the procedure in Fig. 3 We use the procedure in Fig. 4 to build a global map and reconstruct the path of the robot from a sequence of laser range scans with associated odometry data. We begin by taking the first scan in the sequence as the reference scan SR. Initially, the map consists only of the points in the scan SR represented in the global coordinate frame, but it will be incrementally enriched at each iteration of the loop. We keep at every moment a transformation matrix T3, from the coordinate frame of the laser in the reference scan frame to the global coordinate frame. At the beginning of each iteration we take the next scan in the sequence to update the current scan SC. We then use the odometry data to obtain an initial estimate for the pose of the laser in the current scan with respect to the reference scan coordinate frame. We feed this estimate to the PSM procedure described in the previous 


3521 section, and get as a result a new estimate of the pose. Using this new estimate, we update the T3 matrix, transform the points in the current scan to the global coordinate frame and add them to the global map. The current scan becomes then the reference scan and the whole process is repeated again Because the short-term odometry of the robot when traveling on a flat surface is relatively accurate, in practice we do not need to use scan matching to compute the pose of the robot in every scan. Instead, we only use scan matching to get a better estimate of the pose of the robot when it has traveled a certain distance or rotated a certain angle, or when a certain lapse of time has passed since the last time scan matching was used Using the results obtained using the PSM algorithm, the odometry data of the whole sequence can be recomputed It suffices to multiply after each iteration matrix T3 by the transformation matrix T?1L , which gives the transformation matrix from the robot \(not the laser the current scan to the global frame. The pose \(x,y can be readily extracted from this last matrix. Figure 5 shows the position of the robot at several instants in the sequence as given by the original odometry data \(in red and as computed by SLAM \(in green generated map. The sequence was obtained by manually commanding the robot to explore the ground floor of a building in a closed loop. Note that although we did not perform closed-loop detection or corrections of any kind the results are quite satisfactory. The recomputed odometry represents a big improvement over the original odometry that even drifts out of the building V. VERTICAL LINE EXTRACTION FROM OMNIDIRECTIONAL IMAGES AND LASER SCANS This section explains the procedure we developed to extract vertical lines from omnidirectional images and to estimate their 3D positions using information from the laser range finder. We first project the laser information on the omnidirectional image in order to get an approximation of the depth information missing in the image. To achieve that, the unified projection model defined in [15] is applied, which is an extension of Geyers [17] and Barretos [16] models. The 


generalized camera projection matrix K is computed from the generalized focal lengths \(?1,?2 u0,v0 K   1 0 u0 0 ?2 v0 0 0 1   Using K, we can compute the normalized coordinates of a point p in the image \(represented in the camera coordinate frame  Xs,Ys,Zs] as follows \(see Fig 6 X s   1+\(1?? 2 x2+y2 x2+y2+1 x 1+\(1?? 2 x2+y2 x2+y2+1 y 1+\(1?? 2 x2+y2 x2+y2+1   p X Xs zm xs zs ymRm Rp Cm ~xm ys Cp K m  1 m 


p Fig. 6. Unified projection model Fig. 7. Detection of vertical lines and the corresponding laser measurements where ? is the mirror parameter, which is equal to 1 for parabolic mirrors We then extract the quasi-radial lines in the scene, corresponding to approximately vertical features \(e.g. walls facades, doors, windows system perpendicular to the floor where the robot moves, we can guarantee that vertical lines are approximately mapped to radial lines on the camera image plane. To extract prominent vertical lines, we first apply the Canny edge detector to obtain a binary edge image and then apply the Hough transform to detect lines in the binary image. To extract vertical lines we compute the image center \(i.e, where all radial lines intersect in by the Hough transform that do not lie on radial directions As shown in Fig. 7, by overlapping in the omnidirectional image the laser scan data and the radial lines we can find the laser range measurements corresponding to vertical lines This gives us the depth information missing. We detect those laser measurements and save them in the original camera frame together with its corresponding point in the image plane \(which also corresponds to a point on a vertical line 3522 Fig. 5. Global map obtained by SLAM together with the original and recomputed position of the robot at several key instants Let Ms0 = [X s0 ,Y s0 ,0]T be a laser measurement lying on a vertical line expressed in the camera coordinate frame a 3D plane defined in the camera frame, and msi xsi ,ysi ,zsi ]T , i = 1,2 the endpoints of the vertical line where the laser measurement lies expressed in the sphere \(mirror coordinate frame. These last points are computed by inverting the projections of the unified model of Fig. 6 We reconstruct the 3D lines as follows. Let us be the director vector. For every Msi ? ?, the vector  Ms0Msi is colinear to us. Thus  Ms0Msi = ?ius 


  Xi?X s0 = ?iusx Yi?Y s0 = ?iusy Zi?Zs0 = ?iusz 4  OMsi = i  Osmi   Xi = ixsi Yi = iysi Zi = izsi 5 Substituting \(5 4 equations  ixsi ?X s0 = ?iusx iysi ?Y s0 = ?iusy izsi ?Zs0 = ?iusz 6 If ? is a vertical plane in the sphere frame Rs, i.e. us 0,0,1]T , then  ixsi ?X s0 = 0 iysi ?Y s0 = 0 izsi ?Zs0 = ?i 7 Because we know [xsi ,ysi ,zsi ]T and [X s0 ,Y s0 ,0]T , we can compute i for each i. We can then substitute in Equation \(5 obtain the extreme points of the lines in ?. Finally, we apply the homogeneous transformation to transform the coordinates of those points to the global coordinate system and trace the 3D lines. The result is shown in Fig. 8. Observe how the vertical lines are consistent with the 2D map Fig. 8. Environment with 3D lines VI. DISCUSSION AND PERSPECTIVES This paper describes an original composite sensor approach that takes advantage of the information given by an omnidirectional camera and a laser range finder to ef 


ficiently solve the Simultaneous Localization and Mapping problem for indoor environments, and to reconstruct a 3D representation of the environment. The accompanying video illustrates the incremental generation of a 2D map and the estimation of the robot trajectory alongside the laser range data projected on omnidirectional images. It also shows the vertical lines detected in the images and their mapping into a 3D reconstruction of the environment In order to show the robustness of the methodology, we tested the algorithm with a sequence taken in a different indoor environment with our old robot Anis which is equipped with the same catadioptric camera and an AccuRange 4000 2D laser range finder. This laser is composed of a laser telemeter with a rotating mirror that allows measurements of points on 360?, except for an occlusion cone of approximately 30? caused by the assembly of the mirror. The resulting 2D map is shown in Figure 9. The vertical line extraction and the reconstruction of the 3D environment were 3523 Fig. 9. Global Map obtained by SLAM in Borel Building consistent as well The SLAM problem has been solved using many different approaches, however some important problems need to be addressed that are often directly linked to the sensors used Laser range finders cannot help in evaluating the translation of a robot moving in a straight line in a corridor. Mapping in dynamic environments is also hard with only laser data. On the other hand, using visual sensors alone introduces issues such as propagating correctly the scale factor, initializing the range when using a monocular sensor, and merging data when using multiples cameras In our approach, the laser provides metric information of the environment that helps to fix a scale factor \(removing the difficulty of propagating the scale factor need to use multiple cameras. Throughout the paper we have identified several advantages of combining laser and visual sensors. Our experimental results are encouraging and give us valuable insight into the possibilities offered by this composite sensor approach We have considered several research directions that could be pursued to improve the results obtained so far. We have thought about extending our algorithm with loop closure de 


tection. This would allow the algorithm to detect previously visited locations and improve the accuracy of mapping and the precision in the estimation of the robot pose. Being able to detect previously visited places is of great importance to solve the problem of global localization and to recover the robot from kidnapping, a situation occurring when the robot is displaced by something out of its control \(e.g. taking an elevator, being transported from one location to another Therefore, solving the loop closure problem will not only improve SLAM performance, but will as well enable new capabilities Further work will concentrate on an extension of the PSM algorithm to exploit the information about vertical lines detected using omnidirectional images. Segmentation of the ground \(floor a dense \(textured onto the geometric model of the world. Finally, we believe the general approach can be extended to solve the full six degrees of freedom \(6DOF active field of research REFERENCES 1] L.Charbonnier and O.Strauss, A suitable polygonal approximation for laser range finder data, Proceedings of the Intelligent Vehicles 95 Symposium, Detroit, Mi, 1995, pp 118-123 2] J.Nieto, T.Bailey and E.Nebot, Recursive scan-matching SLAM Robotics and Autonomous Systems, vol. 55, 2007, pp 39-49 3] J.S. Gutmann,T.Weigel and B. Nebel, A fast, accurate and robust method for self-localization in polygonal environments using laser range finders, Advanced Robotics Journal, vol 14, 2001, pp 651-667 4] P.J. Besl and N.D. Mackay, A method for registration of 3D shapes IEEE Transactions on Pattern Analysis and Machine Intelligence, vol 14, 1992, pp.239-256 5] F.Ramos, J.Nieto and H.Durrant-Whyte, Recognising and modelling landmarks to close loops in outdoor SLAM, Proceedings of the IEEE International Conference on Robotics and Automation, Roma, It, 2007 pp 2036-2041 6] F.Lu and E.Milos, Robot pose estimation in unknown environments by matching 2D range scans, Journal of Intelligent and Robotic Systems vol. 20, 1997, pp 249-275 7] A.Diosi and L.Kleeman, Laser Scan Matching in polar coordintes with application to SLAM, Proceedings of the IEEE/RSJ International Conference on Robotics and Automation, Edmonton, Canada, 2005, pp 


3317-3322 8] G.Dudek and M.Jenkin, Computational Principles of Mobile Robotics Cambridge University Press, Cambridge, 2000 9] T.Lemaire, C.Berger, I.K.Jung and S.Lacroix, Vision-Based SLAM Stereo and Monocular Approaches, International Journal of Computer Vision, vol. 74, 2007, pp 343,364 10] G.Silveira, E.Malis and P.Rives, An efficient direct method for improving visual SLAM, IEEE International Conference on Robotics and Automation, Roma, It, 2007, pp 4090-4095 11] A.J.Davison, Real-time simultaneous localisation and mapping with a single camera. Proceedings of International Conference on Computer vision, vol. 2, 2003, pp 1403-1410 12] C.Mei and P.Rives, Calibration between a Central Catadioptric Camera and a Laser Range Finder for Robotic Aplications, IEEE International Conference on Robotics and Automation, Orlando, Florida, 2006 13] http://www.robots.ox.ac.uk  cmei/Toolbox.html 14] C.Mei and E.Malis, Fast central catadioptric line extraction, estimation, tracking and structure from motion. Proceedings of of the IEEE/RSJ International conference on Intelligent Robots and Systems Beijing, China, 2006, pp. 4774-4779 15] C.Mei, Laser-Augmented Omnidirectional Vision for 3D localisation and mapping.PhD thesis, Ecole des mines de Paris, Inria Sophia Antipolis, 2007 16] J.P. Barreto, General central projection systems, modeling, calibration and visual servoing, PhD thesis,University of Coimbra, Department of electrical and computer engineering, 2003 17] C.Geyer and K.Daniilidis, A Unifying Theory for Central Panoramic Systems and Practical Applications, in European Conference on Computer Vision, 2000, pp. 445-461 18] R.Smith and P.Cheeseman, On the representation of spatial uncertainty, International Journal of Robotic Research, vol 5, No.4, 1987 pp.56-68 19] R.Smith, M.Self and P.Cheeseman Estimating Uncertain Spatial Relationships in Robotis, Proceedings of the Second Annual Conference on Uncertainty in Artificial Intelligence, Philadelphia, PA, USA Elsevier, 1986, pp. 435-461 20] P.Biber, H.Andreasson, T.Duckett and A.Schilling,3D Modeling of Indoor Environments by a Mobile Robot with a Laser Scaner and Panoramic Camera, IEEE/RSJ International conference on Intelligent Robots and Systems, Sendai, Japan, October 2004 


21] S. Baker and S.K. Nayar, A Theory of Catadioptric Image Formation, IEEE International Conference on Computer Vision \(ICCV pp.35-42, Jan, 1998 22] S.K. Nayar, Catadioptric Omnidirectional Cameras, IEEE Conference on Computer Vision and Pattern Recognition \(CVPR 488, Jun, 1997 23] A.Victorino, La commande referencee capteur: une approche robuste au proble`me de navigation, localisation et cartographie simultanees pour un robot dinterieur. PhD thesis, LUniversite de Nice-Sophia Antipolis, Inria Sophia Antipolis, 2002 3524 


ec  d Fig. 5: Computation Performance Comparison Tab. 4: Computation Savings by TOP-MATA K Connect K Retail K Wap La12 50 58.35% 100 0.01% 200 0.83% 23.04 150 55.91% 400 2.65% 400 30.12% 45.38 250 53.61% 700 1.84% 800 20.03% 25.95 350 48.28% 1100 3.95% 1600 13.06% 27.89 450 43.12% 1400 1.48% 3200 6.14% 12.70 550 39.36% 1700 4.00% 6400 5.63% 7.11 Second, Fig. 5 shows the results of four data sets computed by TOP-MATA and TOP-DATA, respectively. As can be seen, in general, TOP-MATA shows a better performance than TOP-DATA. And as the increase of the ? value, the advantage tends to be even more impressive for these four data sets 4.3. The Computation Saving of TOP-MATA As can be seen in the Tab. 4, four data sets, enjoy signi?cant computation savings brought by TOP-MATA. We can conclude that the computation saving is a major factor for the performance of TOP-MATA. That is, compared with TOP-DATA, a higher computation saving implies a much better performance of TOP-MATA. Since this saving is more signi?cant as the increase of the items, TOP-MATA works better for large scale data sets with a large number of items 5. Conclusion In this paper, we studied the problem of searching for top? item pairs with the highest cosine values among all item pairs. Speci?cally, we provided a novel algorithm TOPMATA which employ a Max-First traversal strategy for ef?ciently performing top-? cosine similarity search. Extensive experimental results veri?ed the effectiveness of the algorithms, And TOP-MATA algorithm is superior to TOPDATA for large-scale data sets with multiple items Acknowledgment This research was partially supported by the National Natural Science Foundation of China \(NSFC No. 70901002 and the Ph.D. Programs Foundation of Ministry of Education of China \(No. 20091102120014 


REFERENCES 1] R. Agrawal, T. Imielinski, and A. Swami, Mining association rules between sets of items in large databases, in SIGMOD 1993 2] C. Alexander, Market Models: A Guide to Financial Data Analysis. John Wiley & Sons, 2001 3] W. Kuo, T.-K. Jensen, A. Butte, L. Ohno-Machado and I. Kohane, Analysis of matched mrna measurements from two different microarray technologies Bioinformatics, vol. 18, p. 405C412, 2002 4] H. Xiong, X. He, C. Ding, Y. Zhang, V. Kumar, and S. Holbrook, Identi?cation of functional modules in protein complexes via hyperclique pattern discovery in PSB, 2005 5] J. Han, H. Cheng, D. Xin, and X. Yan, Frequent pattern mining: Current status and future directions DMKD, vol. 15, no. 1, pp. 5586, 2007 6] P.-N. Tan, M. Steinbach, and V. Kumar, Introduction to Data Mining. Addison-Wesley, 2005 7] S. Brin, R. Motwani, and C. Silverstein, Beyond market basket: generalizing association rules to correlations, in SIGMOD 1997, Tucson, AZ, 1997, pp 265276 8] E. Omiecinski, Alternative interestmeasures formining associations, TKDE, vol. 15, pp. 5769, 2003 9] H. Xiong, S. Shekhar, P.-N. Tan, and V. Kumar Exploiting a support-based upper bound of pearsons correlation coef?cient for ef?ciently identifying strongly correlated pairs, in KDD 2004, 2004, pp 334343 10] I. Ilyas, V. Markl, P. Haas, P. Brown, and A. Aboulnaga, Cords: Automatic discovery of correlations and soft functional dependencies, in SIGMOD 2004 2004, pp. 647658 11] J. Zhang and J. Feigenbaum, Finding highly correlated pairs ef?ciently with powerful pruning, in CIKM 2006, 2006, pp. 152161 12] H. Xiong, W. Zhou, M. Brodie, and S. Ma, Top-k correlation computation, JOC, vol. 20, no. 4, pp 539552, 2008 13] S. Zhu, J. Wu, and G. Xia, Top-k cosine similarity interesting pairs search, in 


http://datamining.buaa.edu.cn/TopKCos.pdf 14] M. Zaki, Scalable algorithms for association mining, TKDE, vol. 12, pp. 372390, 2000 


enhance item-based collaborative filtering, in 2nd IASTED International Conference on Information and Knowledge Sharing, Scottsdale, Arizona, 2003 476 2010 10th International Conference on Intelligent Systems Design and Applications 


Basi Association Rles Basi c  Association R u les Association is basically connecting or tying up occurrences of Association is basically connecting or tying up occurrences of events Ol dib t f ilt t O n l y d escr ib e se t s o f s i mu lt aneous even t s Cannot describe patterns that iterate over time e g  itemset a  0  b  0  g    Eg If you sense higher data rates on the downlink than normal AND New Route generated Implies high chances of Intrusion AND New Route generated Implies high chances of Intrusion Associative IDS for NextGen Frameworks Dr S Dua LA Tech 20 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


