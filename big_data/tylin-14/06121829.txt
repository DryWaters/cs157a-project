Inferring Gene-Gene Associations from Quantitative Association Rules M Mart  nez-Ballesteros I Nepomuceno-Chamorro J.C Riquelme Department of Computer Science University of Seville Seville Spain Email mariamartinez,inepomuceno,riquelme@us.es Abstract The microarray technique is able to monitor the change in concentration of RNA in thousands of genes simultaneously The interest in this technique has grown exponentially in recent years and the dif“culties in analyzing data from such experiments which are characterized by the high number of genes to be analyzed in relation to the low number of experiments or samples available Microarray experiments are generating datasets that can help in reconstructing gene networks One of the most important problems in network reconstruction is nding for each gene in the network which genes can affect it and how Association Rules are an approach of unsupervised learning to relate attributes to each other In this work we use Quantitative Association Rules in order to de“ne interrelations between genes These rules work with intervals on the attributes without discretizing the data before and they are generated by a multi-objective evolutionary algorithm In most cases the extracted rules con“rm the existing knowledge about cell-cycle gene expression while hitherto unknown relationships can be treated as new hypotheses Keywords Data mining evolutionary algorithms;quantitative association rules gene networks I I NTRODUCTION Microarray technology has revolutionized the biological research due to its ability to monitor changes in RNA concentration in thousands of genes simultaneously Research in molecular biology has traditionally focused on the study gene to gene but nowadays we are in the genomic era and genes are studied in thousands or even whole genomes Standard approaches to microarray analysis biomarker discovery are based on the identication of differentially expressed genes and the assumption that genes act independently However it is known that powerful prognostic biomarkers may be encoded by genes that are not highly differentially expressed across control and disease patients Therefore a s ystems-le v e l approach can pro vide insights into the interplay of genes and their association with clinical phenotypes In this context we present the result of applying a data mining technique specically association rules to gene expression data from experiments using microarray technology The aim of mining association rules is to discover the sets of attributes which appear in a dataset with a certain frequency in order to obtain rules that show the existing relationships among the attributes specically this technique is applied to discover associations between genes from microarray datasets in which gene expression is linked to another gene expression A revision of the published literature reveals that exist many algorithms such as Apriori t o nd ARs Ho we v e r  many of these tools that work in continuous domains just discretize the attributes by using a specic strategy and deal with these attributes as if they were discrete Man y algorithms are based on evolutionary algorithms EAs which have been extensively used for the optimization and adjustment of models in data mining tasks EAs are used to discover ARs due to they offer a set of advantages for knowledge extraction and specically for rule induction processes In 7 the authors proposed an EA to obtain numeric ARs dividing the process in two phases Another EA was used in t o obtain ARs where the condence w a s optimized in the tness function The mining process of ARs can be considered as a multiobjective problem rather than a single objective one in which the measures used for evaluating a rule can be thought as different objectives In the last two decades an increasing interest has been developed in the use of EAs for multiobjective optimization There are multiple proposals such as the algorithms NSGA II or SPEA2 11 for instance In a m ulti-objecti v e pareto-based EA w a s p resented and another multi-objective GA to AR mining is proposed in 13 In preliminary works such as the proposed algorithms in and 15 henceforth called QARGA Quantitati v e Association Rules by Genetic Algorithm authors of this paper developed several single-objective EA that use a weighting scheme for the tness function which involved some evaluation measures However it is known that a scheme of this nature is not ideal compared to multiobjective schemes so that could reduce the features used in the tness function for applying a multi-objective technique Thus the main motivation of this paper is to extend these algorithms to a multi-objective approach based on the NSGA-II algorithm The non-dominated multi-objective evolutionary algorithm proposed in this work can nd quantitative association rules in databases with continuous attributes from microarray data avoiding the discretization as a step in the process The results will show that the rules obtained have been able to successfully characterize the data 12 4 1 978-145 77-1676-8/11/$26  00 c  2011 I EEE 


underlying and also to group relevant genes for the problem studied The rest of the paper is organized as follows Section II provides a brief preliminary on ARs Section III describes the methodology used in this work The results obtained by the developed algorithm are discussed in Section IV Finally Section V provides the achieved conclusions II A SSOCIATION RULES Data mining is one of the most used instrumental tools for discovering knowledge from transactions In the eld of data mining the learning of ARs is a popular and well-known research method for discovering interesting relations among variables in large databases The disco v e ry of ARs is unlike classication a non-supervised learning tool as ARs are descriptive Descriptive mining tasks identify patterns that explain or summarize the data that is they are used to explore the properties of the data instead of predicting the class of new data This form of knowledge extraction is based on statistical techniques such as correlation analysis and variance One of the most widely used algorithms is the Apriori algorithm Formally AR were rst dened by Agrawal et al in as follows Let             be a set of  items and             a set of  transactions where each   contains a subset of items Thus a rule can be dened as 012    where 012    and 012      Finally 012 and  are called antecedent or left side of the rule and consequent or right side of the rule respectively When the domain is continuous the ARs are known as Quantitative Association Rules QAR In this context let           be a set of features with values in   Let 015 and  be two disjoint subsets of   that is 015        and 015      A QAR is a rule 012   in which features in 015 belong to the antecedent 012  and features in  belong to the consequent   such that 012 and  are formed by a conjunction of multiple boolean expressions of the form           The consequent  is usually a single expression In this proposal QAR are used because the domain is a continuous domain It is important measure the quality of the rule in order to select the best rules and evaluate the results obtained by the proposed algorithm In the ARs mining process probabilitybased measures that evaluate the generality and reliability of ARs have been selected  In particular  s upport is used to represent the generality of the rule and condence lift and leverage are used to represent the reliability of the rule Others popular measures are conviction gain certainty factor and accuracy In most cases it is sufcient to focus on a combination of support condence and either lift or leverage to quantitatively measure the quality of the rule However the real value of a rule in terms of usefulness and actionability is subjective and depends heavily of the particular domain and business objectives III M ETHODOLOGY In this section we describes the main features of the proposed algorithm in order to discover ARs from datasets whose attribute are real data A Search of Rules In a continuous domain it is necessary to group certain sets of values that share same features and therefore it is required to express the membership of the values to each group Adaptive intervals instead of xed ranges have been chosen to represent the membership of such values in this work The search for the most appropriate intervals has been carried out by means of the proposed algorithm Thus the intervals are adjusted to nd QAR with high values for support and condence together with other measures used in order to quantify the quality of the rule Our proposal is based on the NSGA-II approach and its main purpose is to evolve the population based on the non-dominated sort of the solutions in fronts of dominance The rst front is composed of the non-dominated solutions of the population the Pareto front the second is composed of the solutions dominated by one solution the third of solutions dominated by two and so on The operating scheme of the algorithm proposed can be seen in Figure 1 The overall complexity of the algorithm NSGA-II is       which is governed by the nondominated sorting part of the algorithm In the population each individual constitutes a rule These rules are then subjected to an evolutionary process in which the mutation and crossover operators are applied and at the end of the process the best individual the Pareto front is designated as the best rule Our proposal performs an IRL process Iterative Rule Learning to penalize instances already covered by rules found by the algorithm in order to emphasize the covering of instances still not covered The IRL affects the generation of initial population in each evolutionary process which is described in Subsection III-C In order to optimize the mining of AR by the proposed algorithm thus rules with high quality and precision two interestingness measures are selected as objectives  Con“dence  012     Condence is dened as the probability that instances satisfying 012  also satisfy   In other words it is the support of the rule divided by the support of the antecedent   012       012 012     012        012  1 where   012  is the support of the antecedent that is dened as the ratio of instances in the dataset that satisfy the antecedent 012  and   012     is the 12 4 2 2011 11th International Conference on Inte lligent Systems Design and Applications 


 Inputs  Maximum number of rules    012	\015  Maximum number of generations    012\015  Output  Population of the last generation Multi-objective Algorithm MaxNumRules MaxNumGen Initialize the rule counter   Repeat 1 Initialize the generation counter   2 Initialize parent population    based on instances covered by fewer rules 3 Evaluate the individuals of    based on the measures selected as objectives 4    is ranked using the Fast non dominated Sort that consists in sorting the individuals of a population in different Pareto fronts    according to their non dominance Repeat a an offspring population   of same size as   is generated using crossover and mutation operators over the individuals of   selected using binary Tournament selection-based method b The individuals of   and   are merged into   and the Fast Non dominated Sort is carried out c The next population    consists of the  best individuals of    Initialize the front counter    Repeat If the current level of          Pareto front has less than or equal to  individuals the individuals of   are added to the population     In other case if the current level of          Pareto front has more than  individuals the best individuals are used to ll the population of next generation      and for that purpose the Crowding distance assignment used in order to sort the population of the current level and select the best individuals that represent the best rules Increment the front counterr       While the next population    is not complete d Increment the generation counter       While the maximum number of generations is not reached 5 Return best individual thus the rule in the rst Pareto front     which reach a higher crowding distance value 6 Penalize the instances covered by the best rule found 7 Increment the rule counter       While the number of desired rules is not reached Return the best rules found Figure 1 General scheme of the algorithm support of the rule thus the percentage of instances in the dataset that satisfy 012 and  simultaneously  Leverage  012     Le v erage measures the proportion of additional cases covered by both 012 and  above those expected if 012 and  were independent of each other Leverage takes values inside 1 V alues equal or under value 0 indicate a strong independence between antecedent and consequent On the other hand values near 1 are expected for an important association rule Values above 0 are desirable In addition leverage is a lower bound for support and therefore optimizing only the leverage guarantees a certain minimum support contrary to optimizing only the condence or only the lift   012       012        012      2 where     is the support of the consequent of the rule that is the ratio of instances in the dataset that satisfy the consequent   The proposed algorithm doesn’t use a threshold for minimum support and minimum condence The different parts of the algorithm are dened in the following subsections B Individuals Codi“cation The lower and upper limits of the intervals of each attribute will be represented by the different genes of an individual Because the attributes are continuous individuals are represented by a real coding An individual consists of a not xed number of attributes less than   which represents the number of attribute in the database The representation of an individual consists in two data structures as shown in Figure 2 The upper structure includes all the attributes of the database where   is the lower limit of the range and   is the upper limit The bottom structure indicates the membership of an attribute to the rule represented by an individual The type of each attribute    can have three values 0 when the attribute does not belong to the rule 1 if it belongs to the antecedent of the rule and 2 when it belongs to the consequent part If an attribute is wanted to be retrieved for a specic rule it can be done by modifying the value equal to 0 of the type by a value equal to 1 o or 2 depending on the antecedent or consequent l 2  u 2 l 1  u 1 t 2 t 1 l n  u n  t n  Figure 2 Representation of an individual of the population C Initial Population The generation of the initial population in the proposed algorithm was carried out at the beginning of each evolutionary process and is perform such at least one chosen sample or instance of the dataset was covered The samples of the dataset are selected based on their level of hierarchy The hierarchy is organized according to the number of rules which cover a sample Thus the records are sorted by the number of rules that are covered and the samples covered by a few rules have a higher priority A sample is selected according to the inverse of the number of rules which cover such sample Intuitively the process is similar to roulette selection method where the parents are selected depending on their tness Thus the samples covered by a few rules have a greater portion of 2011 11th International Conference on Inte lligent Systems Design and Applications 12 43 


roulette and therefore they will be more likely selected In the rst evolutionary process all samples have the same probability to be selected Constraints to generate individuals are given by the number of attributes that belong to rule represented by an individual the number of attributes in the antecedents and consequents and the structure of the rule attributes xed or not xed in consequent D Genetic Operators The genetic operators implemented in the genetic algorithm proposed are Crossover and Mutation described in  In addition a n e w Mutation operator has been added Concretely the Antecedent  Consequent Mutation that works as follow If the type   of the selected attribute is antecedent 1 changed to consequent 2 else if the type   of the selected attribute is consequent 2 changed to antecedent 1 IV R ESULTS We applied our methodology to the microarray datasets of Spellman and Cho for the budding yeast Saccharomyces cerevisiae cell-cycle and 23 These data were synchronized by three different methods cdc15 cdc28 and alpha-factors Therefore these three gene expression data sets may be dened as statistically independent The same training experiments with cdc15 dataset used by Soinov et al in were analyzed to achie v e a comparison between the two methods We considered a set of welldescribed genes which encode proteins important for cellcycle regulation We selected these genes for the performance analysis of the proposed method in order to establish comparisons with the previous study A Parameters con“guration As the proposed algorithm is non-deterministic it has been executed ve times for the dataset The main parameters are as follows 100 for the number of the rules to obtain 50 for the size of the population 50 for the number of generations 0.1 for the mutation probability   of the individuals 0.2 for the mutation probability   015 of each gene in the individual B Discussion of Results In order to choose the best individual rule of each generation the individual with the highest support value in the rst Pareto front has been selected in order to cover the maximum number of examples by the obtained rules We have extracted the relationships between attributes belonging to the antecedent and attributes belonging to the consequent for each AR found by the proposed algorithm in each run For example if we have the following rule 015  0.2     0.3 015   0.5 the relationships or associations between the attributes of the antecedent and consequent of the rule are 015    and 015    Then we have built a graph with associations derived from the rules where each attribute that belongs to the rule is a graph node and each association obtained between attributes is an edge of the graph For the resulting graph we performed the intersection between the graphs obtained in each of the ve executions carried out by the algorithm in order to nd the frequent interrelations between genes Table I shows some of the QAR obtained by the algorithm resulting after performing the intersection of the graphs constructed for each algorithm execution The Sup Rule column shows the support of the rule that is the percentage of samples covered by the rule The Conf column indicates the probability that instances satisfying the antecedent also satisfy the consequent The Lev column presents the leverage of the rule and measures the proportion of additional cases covered by both antecedent and consequent above those expected if they were independent of each other The Acc column describes the accuracy of the rule and means the percentage success of the rule The CF column presents the Certainty Factor of the rule The interest of the rule is shown in column Lift and the Amp column presents the average amplitude of the intervals of the attributes belonging to each rule It is important that the values of all interestingness measures of the AR are as high as possible For better understanding Table I shows rules containing 2 attributes one attribute in the antecedent and one in the consequent Rules formed by 3 attributes are shown only for the relationships of genes that are not obtained in any rule of 2 attributes Because the format of the rules obtained by the algorithm is not xed that is any attribute may belong to the antecedent or the consequent rules have been obtained with the same attributes but the sense of the implication of the association is different For example rules 0 and 1 rules 3 and 4 which are represented as directed edges in the graph in Figure 3 We can see that the support value of all rules between 25  and 50  is good enough for the problem at hand Equally remarkable the values of condence certainty factor and accuracy for most of the rules is equal to 1 or very close to 1 which means that these measures have their highest value and indicates that the rule is totally accurate and the implication of the rule is perfect The lift and leverage values are quite high and this means that the rules are interesting and provides valuable information about antecedent and consequent occurring together in the dataset In addition the proportion of instances covered by both antecedent and consequent is greater than ones covered by antecedent and consequent separately Leverage is a lower bound for support so optimizing leverage guarantees 12 44 2011 11th International Conference on Inte lligent Systems Design and Applications 


Table I Q UANTITATIVE A SSOCIATION R ULES AND G ENE G ENE ASSOCIATIONS INFERRED BY THE PROPOSED ALGORITHM  ID Rule Sup Rule Conf Lev Acc CF Lift Amp Gene-Gene associations Soinov inferred by our method 0    0.23      0.84 0.292 1 0.207 1 1 3.429 0.26 CLN1 CLN2 1    0.61      0.2 0.333 1 0.222 1 1 3 0.296 CLN2 CLN1  2    0.23      1.34 0.5 0.857 0.184 0.875 0.688 1.582 0.332 CDC20 CLN1  3    1.37      1.74 0.5 1 0.25 1 1 2 0.496 CLB1 CLB2  4    1.74      1.37 0.458 1 0.229 0.958 1 2 0.483 CLB2 CLB1  5    0.92      0.58 0.375 1 0.219 0.958 1 2.4 0.285 CLB6 CLB5  6    0.58      0.92 0.333 1 0.208 0.958 1 2.667 0.254 CLB5 CLB6  7    0.61      0.25 0.333 1 0.222 1 1 3 0.352 CLN2 CLB5 8    0.42      1.02 0.458 1 0.172 0.833 1 1.6 0.399 CLB2 CLB5 9    0.24      0.56 0.542 1 0.226 0.958 1 1.714 0.418 CLB2 SW15  10  012  1.17      0.28 0.458 1 0.248 1 1 2.182 0.45 CDC34 MBP1  11    0.52    012  1.17 0.417 1 0.243 1 1 2.4 0.352 MBP1 CDC34  12    0.52      1.47 0.375 1 0.203 0.917 1 2.182 0.358 MBP1 SKP1  13    0.83      0.52 0.33 1 0.194 0.917 1 2.4 0.241 SKP1 MBP1 14    0.3      1.88 0.375 1 0.18 0.875 1 2 0.321 SW15 CLN2  15    0.07      1.88 0.458 0.917 0.208 0.917 0.833 1.833 0.469 CLB1 CLN2 16    1.37      1.46 0.333 1 0.194 0.917 1 2.4 0.349 CLB1 SW15  17    1.74   0.458 1 0.248 1 1 2.182 0.481 CLB2 CLN2     1.37     0 18    1.12     0.62   0.458 1 0.21 0.917 1 1.846 0.387 CDC53 SKP1    0.21 19  012  0.14   012  0.09   0.417 1 0.243 1 1 2.4 0.387 SW14 CDC34  012  0.06     0.59 a certain minimum support contrary to optimizing only condence or only lift C Biological Relevance The associations inferred by our approach are summarized in the tenth column of Table I The eleventh column of Table I indicates gene-gene associations that were also inferred by the proposed methods by Soinov in  using the same dataset The Gene Regulatory Network corresponding to the rules inferred by our approach and Soinov is shown in Figure 3 and 4 respectively CLB1 SKP1 MBP1 SW15 CLB2 CDC53 CLN2 CLB5 CLN1 SW14 CDC34 CLB6 CDC20 Figure 3 Directed graph obtained by the proposed algorithm In summary all rules inferred by the decision-tree-based method 13 in total were also inferred by our approach with the addition of new seven rules inferred only by our proposal The biological relevance of the rules inferred by our approach was veried by analyzing whether such rules reect functional properties relating to the different CLN1 SW15 CLN2 CLB1 CLB5 MBP1 CDC20 CDC34 CLB2 SKP1 CLB6 Figure 4 Directed graph obtained by Soinov cell-cycle phase The rules which are supported by the literature are 3 4 5 6 9 10 11 12 14 16 The rules 1 and 2 are consistent with the prior knowledge and are detected by Soinov The rules which are not supported by the literature i.e 0 y and 7 are new hypothesis to analyze in the laboratory V C ONCLUSION A multi-objective evolutionary algorithm for mining quantitative association rules has been proposed in this work The approach is based on the well-known NSGA-II and has determined the intervals that form the rules without discretizing the attributes as a rst step of the process In order to evaluate its performance the approach has been applied in a dataset and compared to other published results The results report the relevance and signicance in the group of genes found in the rules obtained for the problem studied in terms of support condence accuracy interest and leverage As a conclusion an advantage of network reconstruction using our approach is that the method is able to construct 2011 11th International Conference on Inte lligent Systems Design and Applications 12 45 


a network correctly i.e reproducing the logic of a network consistent with the data as The netw ork reconstructed from cell cycle yeast dataset is consistent with the knowledge store in the literature Furthermore the method can be improve by adding prior knowledge and more gene expression proles Our method constitute an interactive expert system for gene association networks where the expert decides when to stop adding new gene expression proles and what biological meaning represent the network A CKNOWLEDGMENT The nancial support from the Spanish Ministry of Science and Technology project TIN2007-68084-C-00 and from the Junta de Andaluca project P07-TIC-02611 is acknowledged R EFERENCES  P  Bro w n and D Botstein Exploring the ne w w orld of the genome with dna microarrays Nature Genet  vol 21 no Suppl pp 33–37 1999  F  Azuaje and Y  D  adn DR W agner  Coordinated modular functionality and prognostic potential of a heart failure biomarker-driven interaction network BMC Syst Biol  vol 4 p 60 2010  R Agra w a l and R Srikant F ast algorithms for mining association rules in large databases in Proceedings of the International Conference on Very Large Databases  1994 pp 478–499  M V annucci and V  Colla Meaningful discretization of continuous features for association rules mining by means of a som in Proceedings of the European Symposium on Arti“cial Neural Networks  2004 pp 489–494  E D Goldber g Genetic Algorithms in Search Optimization and Machine Learning  Addison-Wesley Publishing Company 1989  J Alcal  a-Fdez N Flugy-Pape A Bonarini and F Herrera Analysis of the effectiveness of the genetic algorithms based on extraction of association rules Fundamenta Informaticae  vol 98 no 1 pp 1001–1014 2010  J Mata J L  Alvarez and J C Riquelme Discovering numeric association rules via evolutionary algorithm Lecture Notes in Arti“cial Intelligence  vol 2336 pp 40–51 2002  X Y an C Zhang and S Zhang Genetic algorithm-based strategy for identifying association rules without specifying actual minimum support Expert Systems with Applications An International Journal  vol 36 no 2 pp 3066–3076 2009  K Deb Multi-Objective Optimization Using Evolutionary Algorithms  John Wiley  Sons Inc 2001  K Deb A Pratap S Agarw al and T  Me yari v an  A f ast and elitist multiobjective genetic algorithm Nsga-ii Evolutionary Computation IEEE Transactions on  vol 6 no 2 pp 182 197 2002  E Zitzler  M  Laumanns and L  Thiele Spea2 Impro ving the strength pareto evolutionary algorithm EUROGEN vol 3242 no 103 pp 95  100 2001  B Alatas E Akin and A Karci MODEN AR Multiobjective differential evolution algorithm for mining numeric association rules Applied Soft Computing  vol 8 no 1 pp 646–656 2008  H Qodmanan M Nasiri and B Minaei-Bidgoli Multi objective association rule mining with genetic algorithm without specifying minimum support and minimum condence Expert Systems with Applications  vol 38 no 1 pp 288–298 2011  M Mart  nez-Ballesteros F Mart  nez Alvarez A Troncoso and J C Riquelme Mining quantitative association rules based on evoluationary computation and its application to atmospheric pollution Integrated Computer-Aided Engineering  vol 17 pp 227–242 2010  M Mart  nez-Ballesteros F Mart  nez Alvarez A Troncoso and J Riquelme An evolutionary algorithm to discover quantitative association rules in multidimensional time series Soft Computing  vol 15 no 10 pp 2065–2084 2011  J Han and M Kamber  Data Mining Concepts and Techniques  Morgan Kaufmann 2006  R Agra w a l T  Imielinski and A Sw ami Mining association rules between sets of items in large databases in Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data  1993 pp 207–216  L Geng and H  Hamilton Interestingness measures for data mining A survey ACM Comput Surv  vol 38 no 3 p 9 2006  G Piatetsk y-Shapiro Disco v e ry  a nalysis and presentation of strong rules in Knowledge Discovery in Databases  1991 pp 229–248  B Miller and D  Goldber g Genetic algorithms tournament selection and the effects of noise Complex Systems vol.9 pp 193–212 1995  G V e nturini SIA A Supervised Inducti v e Algorithm with genetic search for learning attribute based concepts in Proceedings of the European Conference on Machine Learning  1993 pp 280–296  P  Spellman G Sherlock M Zhang V  Iyer  K  A nders M Eisen P Brown D Botstein and B Futcher Comprehensive identication of cell cycle-regulated genes of the yeast saccharomyces cerevisiae by microarray hybridization Mol Biol Cell 1998  vol 9 pp 3273–3297 1998  R Cho M Campbell E W inzeler  L  Steinmetz A Conw ay  L Wodicka T Wolfsberg A Gabrielian D Landsman D Lockhart and R Davis A genome-wide transcriptional analysis of the mitotic cell cycle Mol Cell  vol 2 pp 65–73 1998  L Soino v  M Krestyanino v a  a nd A Brazma T o w ards reconstruction of gene networks from expression data by supervised learning Genome Biology  vol 4 p R6 2003 12 4 6 2011 11th International Conference on Inte lligent Systems Design and Applications 


minutes or through a continuous trickle feed Once the load interval \(e.g. five minutes\is up, the freshly loaded tables are simply swapped into production and the tables with the now stale data are released from production 10  This can be accomplished through the dynamic updating of views by simple renaming tables or by swapping partitions The downside of this type of n minute cycle-loading process is that the data in the EventBase is not truly real time For applications where true real-time data is required, the best approach is to continuously trickle-feed the changing data from the source system directly into the EventBase. Obviously, near real-time data integration cannot be as fast as bulk load integration Therefore it is necessary to carefully identify critical data which needs to be integrated continuously with minimized latency In the following, an approach is introduced for integrating trickle-feed event data with event trace capture and loading ETCL\. A key difference of ETCL to traditional bulk loading of data warehouse systems can be summarized as follows Trace Files instead of Flat Files  Traditional ETL tools use flat files where records of the same type are collected and periodically loaded into the database. ETCL uses event traces which collect in memory the records resulting from the processing of a single event. Thereby, various types of records e.g records for event data metrics scores etc are created Finally, the event trace is applied to the database Size of Data Chunks  The data loads for ETCL handle smaller data sets and is performed after the completing the processing of an event. The size of the data to be loaded into the database depends on the event processing results Loading Data from Memory  During the event processing the SARI system collects event trace data in memory. When loading the data into the EventBase, the event trace data is directly transferred to the database without using intermediate storage \(e.g. a flat-file Consistent Event Data A trace file keeps a record of the result from event processing tasks and thereby maintains the temporal order the created records When bulk-loading data the records are split into flat files which are individually loaded into the database During the data loading simultaneous user queries must be turned off in order to avoid locked database records and inconsistent query results ETCL solves the problem by continuously applying consistent fragments of the event data trace during the loading process thereby keeping the stored event data consistent When capturing data for the event trace the event data is process ed  in multiple stages Each stage produces its own set of records which are added to the trace On the higher level we distinguish the following types of records 1 the attribute data for events which contain the elementary information of event objects, 2\ information on relationships between events and 3\ metrics and scores which are calculated during the data staging. The records are continuously created during the event processing and finally loaded in chunks into the EventBase database. Figure 7 illustrates this process The SARI system allows the user to configure which artifacts, i.e., which parts of the event trace, shall be stored to the EventBase This configuration is called event trace outline  Note that there are several dependencies between the various trace levels For instance tracing correlations of events by activating the trace for correlation sets requires that the correlated events are also captured in the trace. Otherwise, th e event trace contains incomplete records when writing the trace to the database en-GB A  Methods for Storing Event Traces In the following we discuss multiple methods for loading the records of event traces into a database system Transactional Inserts  The simplest way of loading the data into the database is to generate and execute INSERT SQL statements By using normal INSERT SQL statements the processing is fully transactional, similar to OLTP applications Since data is only added to the EventBase, a low isolation level can be used for the transactions A major shortcoming of this approach is a high number of transactions and database round trips for storing the event trace records in the EventBase Transactional Batch Processing  The processing of the transactional INSERT statements can be optimized by processing them in a batch mode i.e by sending multiple INSERT commands to the database in a batch Another approach is to use stored procedures which can consume the event trace information as input and perform inserts on database level. Using batch processing for the data inserts can significantly improve the performance However one shortcoming of this approach is that the database system still has the overhead for processing each record with a separat e SQL statement Transactional In Memory Bulk Loading This approach combines the transactional batch processing with bulk loading capabilities. Instead of loading the data from flat files, records from the event trace are directly loaded from memory into the de-AT  Figure 7   EventBase Data Staging  


database system However many   database system do not support large in-memory data-loads directly. In order to circumvent this problem, mechanisms can be used to load data from in memory documents For instance Microsoft  SQL Server or Oracle database systems allow preparing XML documents in memory which can then be used to efficiently query data items for inserting them in one step in to  the database The data inserts for a single XML document is performed in a single transaction Using Transactional Logs for Event Traces  In this approach a database-specific event trace format is used for storing record information When applying the event trace to the database system the trace can be applied directly by the database system thereby minimizing the processing and time for the data integration. The key advantage for this approach is that data is inserted in a highly efficiently manner A major shortcoming of  this approach however is that transactional logs for database system are propri etary and vary from system to system  en-GB B  Performance Experiments with ETCL Using an event processing application from the fraud domain 23 we conducted an experiment for storing the event processing results with event tracing. We ran SARI, using the transactional batch mode  on two nodes with four-CPU Intel servers  We used a separate machine with the same technical specification as the database server   running Microsoft SQL Server 2008 The following table summarizes the perfor mance results  TABLE I  E VENT D ATA S TAGING WITH ETCL   en-GB Input  en-GB Number of source events  en-GB 16 8 232 events  en-GB  en-GB Average  number of attributes  per event  en-GB 18 event attributes  en-GB  en-GB Number of event types   en-GB 14 event types  en-GB  en-GB Event tracing mode   en-GB Transactional batch  en-GB Output  en-GB Processing t ime \(total  en-GB 4 minutes  40 seconds  en-GB  en-GB Events processed / second  en-GB 601 events   sec  en-GB  en-GB Stored events   en-GB 171.327 events  en-GB  en-GB Stored correlations   en-GB 12.301 correlations  en-GB  en-GB Metrics and score updates   en-GB 31.323 updates  en-GB  en-GB Total number of inserted or  updated database record s   en-GB 412.420 records  en-GB  en-GB Database r ecords inserted or updated / second  en-GB 1472 records   sec   We were able to store the events with the full event trace at a rate of about 600 events per second Please note that the processing of an event potentially generates new events e.g alert events\ which are also stored as part of the event trace We conducted another experiment using in memory bulk loading. By using this storage mode with SQL Server 2008, we created XML documents which containing the full event trace information and sent these XML documents to a stored procedure for inserting the data With this storage mode we were able to store 424 events per second. The lower throughput is caused by the significant amount of XML processing which increases the CPU utilization on the SARI nodes as well as on the database node Finally, we did an experiment with the transactional insert storage mode, which allowed us to store 127 events per second The significantly lower throughput was caused by the higher number of database activity due to more database roundtrips VII  E VENT D RIVEN B USINESS I NTELLIGENCE  As yet we presented the concept of EDWH and showed how events, correlations and other artifacts are persisted in the EventBase In this section we focus on the analysis part of SARI: Event-driven BI means the creation of knowledge about underlying business environments from historic event data. As such knowledge can be used to adapt and further improve realtime event-processing logic applied event-driven BI is essential for upto date and continuously refining event-based systems In the following, we demonstrate event-driven BI through a real-world use-case from the fraud detection domain Fraud detection and prevention is a major issue across technologydriven business domains relying on online payment solutions and customer interactions Suntinger et al 23  showed that fraud analysis fits particularly well with the event-driven approach Fraud analysis requires root-cause and cause-chain analysis on the level of single user-actions both in near real time for the intime prevention of ongoing fraud and a posteriori for the continuous improvement of the knowledge base\. Event-based systems use a rule-based approach to continuously evaluate the stream of customer interactions. For the expert-driven analysis of past customer data relevant events are stored in the EventBase As an exemplary analysis framework we utilize the EventAnalyzer 24 The EventAnalyzer is the first grownto maturity BI tool built upon the EventBase. It offers a range of visualization opportunities tailored to the characteristics of event data. The key visualization techniques are derived from the Event Tunnel metaphor of seeing past events flowing through a 3D cylinder and providing different viewing on it Figure 8 above shows a screenshot of the analysis framework A detailed description of the depicted panels will be given throughout the below example In the following, we assume that a well-established onlinebetting provider uses a SARI-based fraud-detection system to continuously monitor ongoing customer interactions We furthermore assume that the system automatically generates alerts for users that match known fraud patterns After receiving two temporally close fraud alerts the business analyst decides to further investigate the affected user accounts In the analysis framework the analyst formulates a query selecting the corresponding accou nt history correlationsessions from the Event Base 


In the EventAnalyzer, the definition of queries is facilitated by the query builder \(Figure 8a\: Here, analysts can select those event types that are considered relevant from the overall list of event types defined in the given event-model the same is provided for correlation sets If required for their investigations analysts can define more sophisticated clauses as, for instance, on event attribute values The event-tunnel top-view in Figure 8c shows the results of the above query: Single events are plotted as glyphs, with userdefined mappings Figure 8b from event attributes to color shape and size Correlations between events are plotted as colored bands, connecting the correlated events by their times of occurrence As depicting similar-looking glyphs at similar points in time, the plot in Figure 8c unveils that the suspicious   sers  officials by the system From visualizations of the EventBase business analysts gain deep insight into the various business processes persisted therein Query-driven and tailored for a quick and step-wise navigation through the EventBase, analytical applications such as the Event Analyzer push it one step further: From an initial clue such as the above fraud alarms they allow the analyst detecting previously unconsidered facts and relationships Consider the business analyst formulating a new query on the event-data warehouse now selecting all events from the BetPlaced-event table that are a   occurred recently. The corresponding event-tunnel side-view in Fig ure 8d scatter of occurrence \(x axis\. It is easy to see that soon after the detected fraud attempts \(mapped to blue based on their account IDs an outlier showing a significantly higher bet amount occurs mapped to red as exceeding a threshold  For an experienced analyst this data point represents a valuable link to related and possibly conspicuous data. One possible interpretation could be that the outlier was placed by a so-called putteron  a fraudster that places bets for people who are prohibited to bet. The suspicion  available in the EventBase and depicted over time in Figure 8e The step chart shows that  inactive with sequences of cash-ins placing a bet winning a bet and cash-outs occurring straightly every few days The example shows that with tailored analysis tools such as the EventAnalyzer the EventBase serves as a valuable and easy to access source for knowledge discovery Thus far the visual analysis of event data was proven useful adoptions of well-known data-mining techniques such as similarity searching are however in current development For more information on the EventAnalyzer the Event Tunnel and its application in fraud detection, readers may refer to Suntinger et al 24  VIII  C ONCLUSION AND O UTLOOK  In this paper we have addressed several open issues regarding event persistence and analysis of existing CEP solutions In particular we have presented a solution for efficient event data repository management as an extension for CEP solutions in order to enable post analysis of events and break up current event processing limitations The presented solution is based on a formal and efficient data schema maintained by a common RDBMS It solves common problems to efficiently persist events their relationships the preservation of calculated event metrics and the representation of non-native database types including de-AT  de-AT   Figure 8  Event Driven Business Intelligence with the EventAnalyzer    


advanced concepts such as event inheritance The implementation and the evaluation have been conducted upon the existing event processing solution SARI The work presented in this paper is part of a larger, longterm research effort aiming at developing a comprehensive set of technologies and tools for event analysis Furthermore we plan to evaluate the integration of the Event Data Warehouse EDWH respectively the linking of EventBase entities to external DWH dimensions to bundle dynamic real-time event information with large historical information repositories for better situation detection and verification of  decision making processes A CKNOWLEDGEMENT  We want to thank the Senactive development team for their valuable discussions and for implementing this research work R EFERENCES  1  Aalst W Weijters A J M M and Maruster L 2004  mining: Discovering process models from event logs. IEEE Transactions on Knowledge and Data Engineering, 16, 9, 1128 1142  2  Abadi, D.J., Ahmad, Y., Balazinska, M., \307etintemel, U., Cherniack, M Hwang, J.H., Lindner, W., Maskey, A.S., Rasin, A., Ryvkina, E., Tatbul N Xing Y and Zdonik S 2005 The Design of the Borealis Stream Processing Engine In Proc of the Conf on Innovative Data Systems Research, Asilomar, CA, USA, 277 289 3  Abadi D J Carney D Cetintemel U Cherniack M Convey C Lee, S., Stonebraker, M., Tatbul, N. and Zdonik, S. 2003. Aurora: A new model and architecture for data stream management. VLDB Journal, 12 120 139 4  Adi A and Etzion O 2004  AMIT  the situation manager The VLDB Journal, 13, 2, 177-203 5  Brobst S and Ballinger C 2000 Active Data Warehousing Whitepaper EB 1327, NCR Corporation 6  Chen, S.-K., Jeng, J.-J. and Chang, H. 2006. Complex Event Processing using Simple Rule-based Event Correlation Engines for Business Performance Management. CEC/EEE 7  Esper, http://esper.codehaus.org/, 2009-0201  8  Hoppe A. and Gryz J. 2007. Stream Processing in a Relational Database a Case Study Database Engineering and Applications Symposium 2007. IDEAS 2007. 11th International Volume, 216 224 9  Inmon B., Imhoff C. and Sousa R. 2001. Corporate Information Factory Wiley, New York 10  Kimball R 1996 The Data Warehouse Toolkit Practical Techniques for Building Dimensional Data Warehouses. John Willey, 1996 11  Luckham, D. 2005. The Power Of Events. Addison Wesley 12  Mannila H and Moen P 1999 Similarity between event types in sequences, Proc. First Intl. Conf. on Data Warehousing and Knowledge Discovery  271 28 13  Moen P 2000 Attribute Event Sequence and Event Type Similarity Notions for Data Mining Ph.D thesis Department of Computer Science, University of Helsinki, Finland 14  Rozsnyai S 2006 Efficient indexing and searching in correlated business events. PhD thesis, Vienna University of Technology 15  Rozsn yai S., Schiefer J and Sch atten A  2007 Concepts and Models for Typing Events for Event Based Systems  International Conference on Distributed Event Based Systems   Toronto  Canada  DEBS  0 7   16  Rozsnyai, S., Vecera, R., Schiefer, J and Schatten, A. 2007 Event cloud  searching fo r correlated business events. In CEC/EEE, IEEE Computer Society  409 420  17  Schiefer, J. and Seufert, A. 2005. Management and controlling of timesensitive business processes with sense & respond. In CIMCA/IAWTIC IEEE Computer Society, 77 82  18  Schiefer J Rozsnyai S Saurer G and Rauscher C 2007 Event Driven Rules for Sensing and Responding to Business Situations International Conference on Distribut ed Event Based Systems, Toronto   19  Schiefer, J. and Seufert, A. 2005. Management and Controlling of TimeSensitive Business Processes with Sense  Respond International Conference on Computational Intelligence for Modelling Control and Automation \(CIMCA\, Vienna 20  Schrefl M and Thalhammer T 2000 On Making Data Warehouses Active In Proc of the 2nd Intl Conf on Data Warehousing and Knowledge Discovery DaWaK Springer LNCS 1874 London UK 34 46 21  Seirio M. and Berndtsson M. 2005 Design and Implementation of an ECA Rule Markup Language. RuleML, Springer Verlag, 98 112 22  Stonebraker M  and 307etintemel U 2005  One Size Fits All An Idea Whose Time Has Come and Gone, ICDE 2005, 2-11 23  Suntinger M Schiefer J Roth H and Obweger H 2008 Data Warehousing versus Event-Driven BI Data Management and Knowledge Discovery in Fraud Analysis  International Conference on Software Knowledge Information Management and Applications  Kathmandu, Nepal  08  24  Suntinger, M., Obweger, H., Schiefer, J and Groeller M. E  2007 The E vent T unnel  Interactive visualization of complex event streams for busin ess process pattern analysis Technical report Institute of Computer Graphics and Algorithms  Vienna University of Technology  25  Vecera R 2007 Efficient indexing Searching and Analysis of Event Streams. PhD thesis, Vienna University of Technology 26  Vecera R Rozsnyai S and Roth H 2007 Indexing and search of correlated business events The Second International Conference on Availability, Reliability and Security, Ares 2007, 1124 1134 27  Widom J  Ceri S and Dayal U 1994 Active Database Systems Triggers and Rules for Advanced Database Processing Morgan Kaufmann Publishers Inc., San Francisco, CA 28  Wu P Bhatnagar R Epshtein L Bhandaru M and Shi Z 1998 Alarm correlation engine \(ACE\, In Proceedings of the IEEE/IFIP 1998 Network Opera tions and Management Symposium NOMS New Orleans  29  Zdonik S Stonebraker M., Cherniack M. Cetintemel U Balazinska M. and Balakrishnan, H. 2003. The Aurora and Medusa Projects. IEEE Data Engineering Bulletin, 26  1  


4 Heart 270 13 2 5 Diabetes 768 8 2 6 Pima 768 8 2  For a classifier, classification accuracy is a basic performance measurement, which is the ratio of the number of cases truly predicted by the classifier over the total number of cases in the whole test dataset, e.g Number of cases truly predictedClassification accuracy= 100 Total number of cases  2 We compared BitTableAC with some associative classification algorithms on accuracy, such as CBA and CMAR. We also include the C4.5 and LIBSVM results on the same datasets as a reference. C4.5 is a well-known traditional classifier based on decision tree induction technique, and LIBSVM is an accurate support vector machine classifier For the fair of the comparison, we do not implement these algorithms, but their classification accuracies are obtained from their research literatures. For BitTableAC, the MinSup MinConf and NFP \(num of fuzzy partitions and 3, respectively The comparison results are shown in Table 4. As shown in this table, BitTableAC has the satisfactory classification accuracy. It achieves the highest accuracy in four of six datasets used in experiments and also outperforms other algorithms on average Table 4 Experiment Results Dataset C4.5 LIBSVM CBA CMAR BitTableAC 1 Glass 68.70 77.57 72.60 70.10 75.23 2 Iris 93.60 94.00 92.90 94.00 96.25 3 Breast 95.70 96.14 95.80 96.40 98.53 4 Heart 82.50 88.45 81.50 82.20 86.67 5 Diabetes 72.10 73.83 75.30 75.80 80.00 6 Pima 75.50 79.69 73.10 75.10 81.82 Average 81.35 84.95 81.87 82.27 86.42  IV. CONCLUSION In this paper, an accurate associative classifier BitTableAC is proposed. It employs BitTable to mine association rules 


efficiently, and fuzzy c-means \(FCM attributes. To evaluate the performance of the proposed algorithm, we compare BitTableAC with other well-known classifiers on accuracy including previous associative classifiers, C4.5 and LIBSVM on 6 test datasets from UCI Machine Learning Repository. The results show that, in terms of accuracy, BitTableAC outperforms others REFERENCES 1] G. Goulbourne, F. Coenen, and P. Leng, "Algorithms for computing association rules using a partial-support tree," Knowledge-Based Systems vol. 13, pp. 141-149, Apr 2000 2] M. J. Zaki, "Scalable algorithms for association mining," Ieee Transactions on Knowledge and Data Engineering, vol. 12, pp. 372-390 May-Jun 2000 3] F. Bonchi, F. Giannotti, A. Mazzanti, and D. Pedreschi, "Efficient breadth-first mining of frequent pattern with monotone constraints Knowledge and Information Systems, vol. 8, pp. 131-153, Aug 2005 4] G. Grahne and J. F. Zhu, "Fast algorithms for frequent itemset mining using FP-trees," Ieee Transactions on Knowledge and Data Engineering, vol 17, pp. 1347-1362, Oct 2005 5] I. I. Artamonova, G. Frishman, and D. Frishman, "Applying negative rule mining to improve genome annotation," Bmc Bioinformatics, vol. 8, pp. -, Jul 21 2007 6] J. W. Han, J. Pei, Y. W. Yin, and R. Y. Mao, "Mining frequent patterns without candidate generation: A frequent-pattern tree approach," Data Mining and Knowledge Discovery, vol. 8, pp. 53-87, Jan 2004 7] Y. C. Hu and G. H. Tzeng, "Elicitation of classification rules by fuzzy data mining," Engineering Applications of Artificial Intelligence, vol. 16, pp 709-716, Oct-Dec 2003 8] J. D. Holt and S. M. Chung, "Mining of association rules in text databases using Inverted Hashing and Pruning," Data Warehousing and Knowledge Discovery, Proceedings, vol. 1874, pp. 290-300, 2000 9] Y. J. Li, P. Ning, X. S. Wang, and S. Jajodia, "Discovering calendar-based temporal association rules," Data & Knowledge Engineering, vol. 44, pp 193-218, Feb 2003 10] Y. J. Tsay and J. Y. Chiang, "CBAR: an efficient method for mining association rules," Knowledge-Based Systems, vol. 18, pp. 99-105, Apr 2005 11] B. Liu, W. Hsu, and Y. Ma, "Integrating Classification and Association Rule Mining," in Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining, KDD'98, AAAI, New York, 1998, pp. 80-86 12] W. Li, J. Han, and J. Pei, "CMAR: accurate and efficient classification 


based on multiple class association rule," in Proceedings of the 2001 IEEE International Conference on Data Mining, ICDM'01, San Jose, CA, 2001 pp. 369-376 13] D. Janssens, G. Wets, T. Brijs, and K. Vanhoof, "Adapting the CBA algorithm by means of intensity of implication," Information Sciences, vol 173, pp. 305-318, Jun 23 2005 14] W. Song, B. R. Yang, and Z. Y. Xu, "Index-BitTableFI: An improved algorithm for mining frequent itemsets," Knowledge-Based Systems, vol. 21 pp. 507-513, Aug 2008 15] J. Dong and M. Han, "BitTableFI: An efficient mining frequent itemsets algorithm," Knowledge-Based Systems, vol. 20, pp. 329-335, May 2007  532 


 Table I.  Number of intervals Stage Interval No. Stage Interval No EP 7 ES 8 ED 10 EB 9 ET 8 EI 11  Table II.  Results using the proposed approach Stage Bias MMRE MdMRE ES -8.5% 27.0% 17.0 ED -33.1% 40.5% 13.7 EB -2.8% 9.3% 7.5 ET -11.6% 16.7% 7.23 EI -20% 91.0% 30.2  Table III.  Results using exponential regression Stage Bias MMRE MdMRE ES -24.3% 81.3% 49.7 ED -72.3% 120.4% 54.224 EB 0.7% 44.35% 37.6 ET -45.4% 81.1% 39.0 EI -179% 184% 104.0  Results shown in Table III revealed that most of predictions are under estimation which supports our approach findings. The best estimation accuracy was obtained in building stage, which also corroborates our findings that best estimation accuracy was in building stage. The negative values in Bias criterion show underestimation. It is acknowledged that MMRE is unbalanced in many validation circumstances and leads to overestimation more than underestimation. In our case, we found that MMRE leads to underestimation in most stages. This is may be related to the absence of systematic scheme between all prior effort records   253   Figure 1. Effort distribution of Planning stage 


Figure 2. Effort distribution of Specification stage Figure 3. Effort distribution of Design effort stage  Figure 4. Effort distribution of Building stage Figure 5. Effort distribution of Testing stage Figure 6. Effort distribution of Imp stage   Table IV. Statistical significance Stage sum rank Z-value p-Value ES 769 -4.31 <0.01 ED 713 -5.03 <0.01 EB 685 -5.4 <0.01 ET 595 -6.54 <0.01 EI 799 -3.93 <0.01  The comparison between our approach and exponential regression technique showed that there are considerable improvements in estimation accuracy on all phases of software development lifecycle. MMREs of our approach have been reduced by at least 35.05 and at most 93%. Biases have been reduced by at least 3.5% and at most 159%.We have to bear in mind that the length of interval plays important role in estimation accuracy, thus, when the universe of discourse is partitioned into several equal intervals, the distribution of data should be taken into account. Moreover, we should remove the extreme values because they affect interval partitioning, thus, estimation accuracy Figures 7 to 11 show comparison between proposed approach and exponential regression in each stage by using Boxplot. The Boxplot [17] offers a way to compare between estimation models based on their absolute residuals. The Boxplot is non-parametric statistics used to show the median as central tendency of distribution, interquartile range and the outliers of individual models [17]. The length of Boxplot from 


lower tail to upper tail shows the spread of the distribution. The length of box represents the interquartile range that contains 50% of observations The position of median inside the box and length of Boxplot indicate the skewness of distribution. A Boxplot with a small box and long tails represents a very peaked distribution while a Boxplot with long box represents a flatter distribution. The prominent and common characteristic among these figures is the spread of absolute residuals for our approach is less than spread of exponential regression which presents more accurate results. The larger interquartile of exponential regression indicates a high dispersion of the absolute residuals. The Boxplot revealed that the box length for our models is smaller than exponential regression which also indicates reduced variability of absolute residuals. The median of our model is smaller than median of exponential regression which revealed that at least half of the predictions of our model are more accurate than exponential regression 254  Figure 7. Boxplot of absolute residuals for the specification stage  Figure 8. Boxplot of absolute residuals for the design stage   Figure 9. Boxplot of absolute residuals for the building stage Figure 10. Boxplot of absolute residuals for the testing stage   The lower tails of our model is much smaller than upper tail which means the absolute residuals are skewed towards the smaller value Figure 11 illustrates the reason of why prediction of implementation stage in our approach produced the worst accuracy. The reason related to the existing of outlier. Although one project is considered as an outlier the MMRE is easily influenced with that project Based on the obtained results, we can observe that 


exponential regression gave bad accuracy. The reason may relate to the structure complexity of prior effort records. There is no correlation between all prior stages and target stage To ensure that the results obtained are not by chance we investigated the statistical significance of the proposed approach using Wilcoxon sum rank test for absolute residuals as shown in Table IV. In this test if the resulting p-value is small \(p<0.05 statistically significant difference can be accepted between the two samples median. The residuals obtained using the proposed approach were significantly different from those obtained by exponential regression Suggesting that, there is difference if the predications generated using the proposed approach or exponential regression and based on the accuracy comparison in Tables II and III we can safely conclude that our proposed method outperformed exponential regression for stage effort estimation Figure 11. Boxplot of absolute residuals for the implementation stage  VIII. CONCLUSIONS Some of software projects are failed due to the absence of re-estimation during software development which results in huge gap between initial plan and final outcome. Even with good estimate at first stage the project manager must keep update with project progress and should be able to re-estimate the project at any particular point of project in order to re-allocate the proper number of resources. The objective of this paper was to check whether the prior effort records can 255 be used to predict stage effort with reasonable accuracy or not. The obtained results revealed that using association rule and Fuzzy set theory lead to significant improvement in stage-effort estimation and give project manager an evolving picture about project progress. Comparing our approach with exponential regression showed that there is a considerable potential in estimation accuracy. As part of future plan, we 


intend to expand this work to involve some interesting features in each stage prediction and evaluate it on many datasets   REFERENCES  1] F. Ricardo, N. Ana, M. Paula, B. Gleidson, R. Fabiano ODE: Ontology-based software Development Environment, Proceedings of the IX Argentine Congress on Computer Science, pp. 1124-1135, 2003 2] E. Mendes, B. A. Kitchenham. Further comparison of cross-company and within-company effort estimation models for Web applications. In: Proc. 10th IEEE International Software Metrics Symposium, Chicago USA, pp.348-357, 2004 3] B. Boehm, R. Valerdi. Achievements and Challenges in Software Resource Estimation, Proceedings of ICSE 06 Shanghai, China, pp. 74-83,  2006 4] K. Molokken, M. Jorgensen. A review of software surveys on software effort estimation, Proceedings of International Symposium on Empirical Software Engineering \(ISESE 2003 5] M. Jorgensen, K. Molokken-Ostvold. How large are software cost overruns? A review of the 1994 CHAOS report, Information and Software Technology, Vol. 48 issue 4. PP. 297-301, 2006 6] X. Huanga, D. Hob, J. Rena, L. F. Capretz. Improving the COCOMO model using a neuro-Fuzzy approach Applied Soft Computing, Vol.7, issue 1, pp. 29-40, 2007 7] L. Briand, T. Langley, I. Wieczorek. A replicated assessment and comparison of common software cost modeling techniques, Proceedings of the 22nd international conference on Software Engineering, 2000 8] S.-J Huang, N. H. Chiu. Optimization of analogy weights by genetic algorithm for software effort estimation Information and Software Technology, Vol. 48, issue 11 pp. 1034-1045, 2006 9] Z. Xu, T. M. Khoshgoftaar. Identification of Fuzzy models of software cost estimation, Fuzzy Sets and Systems, Vol. 145, issue 1, pp. 141-163, 2004 10] R. Pressman. Software Engineering: practitioner 


approaches, McGraw Hill, London, 2004 11] M. Boraso, C. Montangero, H. Sedhi. Software cost estimation: an experimental study of model performance Universita di Pisa, Italy, 1996 12] Y. Wang, Q. Song, J. Shen., 2007. Grey Learning Based Software Stage-Effort Estimation. International Conference on Machine Learning and Cybernetics, pp 1470-1475, 2007 13] S. G. MacDonell, M. J. Shepperd. Using prior-phase effort records for re-estimation during software projects Ninth International, Software Metrics Symposium, pp 73- 86, 2003 14] M .C Ohlsson, C. Wohlin. An Empirical Study of Effort Estimation during Project Execution, Sixth International Software Metrics Symposium \(METRICS'99 1999 15] N. H. Chiu,,S. J. Huang.  The adjusted analogy-based software effort estimation based on similarity distances Journal of Systems and Software, Vol. 80, issue 4, pp 628-640, 2007 16] P. Sentas, L. Angelis, I. Stamelos, G.  Bleris. Software productivity and effort prediction with ordinal regression Information and Software Technology, Vol. 47, issue 1 pp. 17-29, 2005 17] E. Mendes, N. Mosley. Comparing effort prediction models for Web design and authoring using boxplots Australian Computer Science Communications,  Vol. 23 Issue 1, pp. 125-133, 2001 18] E. Mendes, N. Mosley, I. Watson. A comparison of casebased reasoning approaches, Proceedings of the 11th international conference on World Wide Web, pp. 272280, 2002 19] Q. Zhao, S. S. Bhowmick. Association Rule Mining: A Survey  http://citeseer.ist.psu.edu/734613.html, 2003 20] S. Morisak, A. Monden, H. Tamada. An Extension of association rule mining for software engineering data repositories, Information Science Technical Report NAIST, 2006 21] Q. Song, M. Shepperd.  M. Cartwright, C. Mair. Software defect association mining and defect correction effort prediction, IEEE transaction on software engineering Vol. 32, No.2, pp. 69-82, 2006 


22] R. Agrawal, T. Amielinski, A. Swami. Mining association rule between sets of items in large databases Proceedings of the ACM SIGMOD International Conference on Management of Data, pp. 207-216, 1993 23] M-J. Huang, Y-L. Tsou, S-C. Lee.  Integrating Fuzzy data mining and Fuzzy artificial neural networks for discovering implicit knowledge, J. Knowledge-Based Systems, Vol.19 \(6 24] ISBSG International Software Benchmarking standards Group, Data repository release 10, Site http://www.isbsg.org, 2007 25] L. Zadeh. Toward a theory of Fuzzy information granulation and its centrality in human reasoning and Fuzzy logic. J. Fuzzy sets and Systems 90, pp. 111-127 1997 26] I. H. Witten, E. Frank. Data Mining: Practical machine learning tools and techniques, 2nd Edition, Morgan Kaufmann, San Francisco, 2005   256 


encountering a related term, i.e. IC\(c e intuition behind the use of the negative likelihood is that the more probable a term to appear, the less information it conveys. All these features show that Jiangs measure tends to be more general and more appropriate for evaluating nontaxonomically related terms. Indeed, a high score of the relatedness measures suggests a strong relationship between terms Nevertheless, all relatedness measures have limitations because they assume that all the semantic content of a particular term is modeled by semantic links in WordNet Consequently, in many situations, truly related terms obtain a low scores even though their belongings to a certain category of tags, e.g., jargon tags Additionally, when measuring the quality of an automatically knowledge acquisition results, the typical measures used in Information Retrieval are Recall, Precision and F-Measure. However, computing Recall and F-Measure requires the availability of a Gold Standard. Hence, we will only compute the Precision which speci?es to which extent the non-taxonomic relationships is extracted correctly. In this case, the ratio between the correctly extracted relations i.e., their relatedness measures is greater than or equal to a minimum threshold, and the whole number of extracted ones is computed. Thus, we have Precision Total correctly selected entities Total selected entities 12 http://search.cpan.org/dist/WordNet-Similarity 13 A term refers to a tag subject or a tag object C. Evaluation of non-taxonomic relationships Only a percentage of the full set of non-taxonomic relationships \(89 is caused by the presence of non standard terms which are not contained in WordNet and, in consequence, cannot be evaluated using WordNet-based relatedness measures. Fig. 5 depicts the evaluation results of the extracted non-taxonomic relationships against their relatedness measures High relatedness score \(88 17% of the extracted relationships, as most of terms are strongly related with respect WordNet Null Scores were obtained for 5% of the extracted 


relationships. Analyzing this case in more detail, we have observed that the poor score is caused in many situations by the way in which Jiangs distance metric works. This latter completely depends on the distance between two terms based on the number of edges found on the path between them in WordNet. In consequence this measure returns a value that does not fully represent reality. For example, on the one hand, Jiangs distance metric returns a null value for the relationship between insurance and car, even though the ?rst is a commonly related to the second, i.e., car involved insurance Finally with a minimal Jiangs distance metric threshold, set to 46%, the computed precision of correctly extracted relationships candidates is equal to 68.8 An example of extracted non-taxonomic relationships is depicted in Table V where each relation describes the subject tag, e.g., tool, the predicate, e.g is being developed within, and the object tag, e.g mesh. Fig. 4 represent a fragment output of the extracted ontological structure where each concept de?nes a set of similar and synonym tags and labels, i.e., mentions has been, revealed, caused and is created with describe the predicates of the non-taxonomic relationships between terms Due to the limitations observed by the automatic evaluation procedure and the lack of gold standards containing non-taxonomic relationships, we have examined the extracted non-taxonomic relationships from a linguistic point 377 Top space      distance     quad great     groovy nifty caused address      addresses extension      quotation   reference  references extensions        referenz     source      refrence sources    rfrences    quotations research    search     searching searchs open-source     open_source 


opensource linux aim     design     designer      designers patern    project     patterns     projekte projects web+design    web_design webdesign internet       internetbs net          web network      networking networks      web discussion     news       password word      words community      communities is_created_with mentions revealed has_been Figure 4. A fragment output of the extracted ontological structure of view. This qualitative evaluation can bring some interesting insights about the kind of results one can expect Invalid relations are extracted: Even though a relation such as music cities skill is considered as correct one since tag subject, tag object and predicate are correctly extracted. From a semantic point of view, this relation has no meaning. Hence, a higher precision is expected Figure 5. Summary of non-taxonomic evaluation measure Table V EXAMPLES OF EXTRACTED NON TAXONOMIC RELATIONSHIPS Subject Predicate Object search has been reference reference mentions search tool is being developed within mesh security added encoding search revealed reference java provides library by performing the sense analysis on complete relations An ambiguity in the extracted predicates between terms is observed: Hence, same relations are redundant since they use a synonym predicates between terms, e.g java provides library and java yields library. Thus we expect that the redundancy removal within extracted relations will be of bene?t for the improvement of the 


obtained results VI. CONCLUSION AND FUTURE WORK The extraction of non-taxonomic relationships from folksonomies is to the best of our knowledge is the least tackled task within ontology building from folksonomy. This is why there is a need of novel and general purpose approaches covering the full process of learning relationships. In this paper, we introduced a new approach called NONTAXFOLKS that starts by pre-processing tags aiming at getting a set of frequent tagsets corresponding to an agreed representation Then, they are used to retrieve related tags using external resources such as WordNet. Thanks to the particular structure of triadic concepts, it allows grouping semantically related tags by considering the semantic relatedness embodied in the different frequencies of co-occurences among users, resources and tags in the folksonomy. Thereafter we introduced an algorithm called NTREXTRACTION for extracting non-taxonomic relationships between pair of tags picked from the triadic concepts. In summary, our approach uses several well known techniques \(such as formal concept analysis or association rule discovering the social bookmaring environnement in order to propose a new way of extracting labeled non-taxonomic relationships between tags. Currently, we are investigating the following topic concerning the discovered predicates between two terms. Indeed, in order to avoid relationships redundancy and thus a redundancy in the builded ontology. One can try to classify them into prede?ned semantic classes, detect synonyms, inverses, etc. A standard classi?cation of verbs could be used for this purpose, adding additional information about the semantic content, e.g., senses, verb types, thematic roles, etc., of predicates relationships 378 REFERENCES 1] J. Pan, S. Taylor, and E. Thomas, Reducing ambiguity in tagging systems with folksonomy search expansion, in Proceedings of the 6th Annual European Semantic Web Conference \(ESWC2009 2] V. S. M. Kavalec, A. Maedche, Discovery of lexical entries for non-taxonomic relations in ontology learning, in Proceedings of the SOFSEM 2004, LNCS, vol. 2932, 2004, pp 249256 


3] L. Specia and E. Motta, Integrating folksonomies with the semantic web, in Proceedings of the 4th European Semantic Web Conference \(ESWC 2007 Innsbruck, Austria, vol. 4519, June 2007, pp. 624639 4] P. Mika, Ontologies are us: A uni?ed model of social networks and semantics, in Proceedings of the 4th International Semantic Web Conference \(ISWC2005 3729, Galway, Ireland, June 2005, pp. 522536 5] P. Schmitz, Inducing ontology from ?ickr tags, in Proceedings of the Workshop on Collaborative Tagging \(WWW 2006 Edinburgh, Scotland, May 2006 6] M. Zhou, S. Bao, X. Wu, and Y. Yu, An unsupervised model for exploring hierarchical semantics from social annotations, in Proceedings of the 6th International Semantic Web Conference and 2nd Asian Semantic Web Conference ISWC/ASWC2007 Korea, vol. 4825, November 2006, pp. 673686 7] C. Schmitz, A. Hotho, R. Jaschke, and G. Stumme, Mining association rules in folksonomies, in Proceedings of the 10th IFCS Conference \(IFCS 2006 2006, pp. 261270 8] A. Hotho, A. Maedche, S. Staab, and V. Zacharias, On knowledgeable unsupervised text mining, in Proceedings of Text Mining Workshop, Physica-Verlag, 2003, pp. 131152 9] A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme, Information retrieval in folksonomies: Search and ranking, in The Semantic Web: Research and Applications, vol. 4011 Springer, 2006, pp. 411426 10] F. Lehmann and R. Wille, A triadic approach to formal concept analysis, in Proceedings of the 3rd International Conference on Conceptual Structures: Applications, Implementation and Theory. Springer-Verlag, 1995, pp. 3243 11] R. Jaschke, A. Hotho, C. Schmitz, B. Ganter, and G.Stumme Discovering shared conceptualizations in folksonomies Web Semantics: Science, Services and Agents on the World Wide Web, vol. 6, pp. 3853, 2008 12] A. Mathes, Folksonomies - cooperative classi?cation and communication through shared metadata, Graduate School of Library and Information Science, University of Illinois Urbana-Champaign, Tech. Rep. LIS590CMC, December 2004 13] H. Lin, J. Davis, and Y. Zhou, An integrated approach 


to extracting ontological structures from folksonomies, in Proceedings of the 6th European Semantic Web Conference ESWC 2009 vol. 5554, 2009, pp. 654668 14] M. Szomszor, H. Alani, K. OHara, and N. Shadbolt, Semantic modelling of user interests based on cross-folksonomy, in Proceedings of the 7th International Semantic Web Conference \(ISWC 2008 15] G.Begelman, P. Keller, and F.Smadja, Automated tag clustering: Improving search and exploration in the tag space, in Proceedings of the the Collaborative Web Tagging Workshop WWW 2006 16] R. Jaschke, A. Hotho, C. Schmitz, B. Ganter, and G. Stumme TRIAS - an algorithm for mining iceberg tri-lattices, in Procedings of the 6th IEEE International Conference on Data Mining, \(ICDM 2006 2006, pp. 907911 17] C. Borgelt, Ef?cient implementation of APRIORI and ECLAT, in FIMI, COEUR Workshop Proceedings, COEURWS.org, vol. 126, 2003 18] J. Tang, H. Leung, Q. Luo, D. Chen, and J. Gong, Towards ontology learning from folksonomies, in Proceedings of the 21st international jont conference on Arti?cal intelligence IJCAI 2009 20892094 19] L. Ding, T. Finin, A. Joshi, R. Pan, R. Cost, Y. Peng P. Reddivari, V. Doshi, and J. Sachs, Swoogle: A search and metadata engine for the semantic web, in Proceedings of the 13th ACM Conference on Information and Knowledge Management, ACM Press, 2004, pp. 652659 20] A. Hliaoutakis, G. Varelas, E. Voutsakis, E. Petrakis, and E. E Milios, Information retrieval by semantic similarity, International Journal on Semantic Web and Information Systems IJSWIS 21] G. Pirro, M. Ruffolo, and D. Talia, Secco: On building semantic links in peer to peer networks, Journal on Data Semantics XII, LNCS 5480, pp. 136, 2009 22] C. Meilicke, H. Stuckenschmidt, and A. Tamilin, Repairing ontology mappings, in Proceedings of the International Conference AAAI 2007, Vancouver, British Columbia, Canada 2007, pp. 14081413 23] S. Ravi and M. Rada, Unsupervised graph-based word sense 


disambiguation using measures of word semantic similarity in Proceedings of the International Conference ICSC 2007 Irvine, California, USA, 2007 24] H. G. A. Budanitsky, Semantic distance in wordnet: an experimental application oriented evaluation of ?ve measures in Proceedings of the International Conference NACCL 2001 Pittsburgh, Pennsylvania, USA, 2007, pp. 2934 25] J. Jiang and D. Conrath, Semantic similarity based on corpus statistics and lexical taxonomy, in Proceedings of the International Conference ROCLING X, 1997 379 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


