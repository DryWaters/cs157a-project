Mining Non-Derivable Hypercliques Anna Koufakou U.A Whitaker College of Engineering Florida Gulf Coast University Fort Myers FL USA akoufakou@fgcu.edu Pradeep Ragothaman Quest Diagnostics Madison NJ USA pradeep.ragothaman@gmail.com Abstract Hypercliques have been successfully applied in a number of applications e.g clustering and noise r emo v a l  A hyperclique  is an itemset containing items that ar e strongly correlated with each other Even though hypercliques have been shown to handle datasets with skewed support distribution and low support threshold well they might still 
face problems for dense datasets and lower h-con“dence threshold In this paper we propose a new pruning method based on combining hypercliques and Non-Derivable Itemsets NDIs in order to substantially r e duce the amount of generated hyperclique sets Speci“cally we propose a new collection of hypercliques called Non-Derivable Hypercliques NDHC and present an ef“cient algorithm to mine these sets called NDHCMiner The proposed NDHC collection is a lossless representation of hypercliques i.e given the itemsets in NDHC we can generate the complete collection of hypercliques and their support without additional scanning of the dataset We experimentally compare NDHC with Hypercliques HC 
as well as another condensed representation of hypercliques Maximal Hyperclique Patterns MHP Our experiments show that the NDHC collection offers substantial advantages over HC and even MHP especially for dense datasets and lower h-con“dence threshold values Keywords hyperclique non-derivable itemset frequent itemset mining dense data skewed support distribution I I NTRODUCTION Frequent itemset mining FIM has attracted substantial attention since the seminal paper by Agrawal and Srikant  introducing the Apriori recognized today as one of the ten most in”uential data mining algorithms FIM aims to extract patterns or itemsets  sets of items that co-occur 
frequently in the dataset based on a user-entered threshold of support 002  The frequent itemsets generated by an FIM algorithm may contain items that are weakly related to each other Furthermore there might be many spurious combinations of items if a low 002 threshold is used and the frequency distribution of items is skewed Finally with a high 002 threshold value strong af“nity itemsets involving items with low support levels are missed To address these issues a hyperclique pattern w as proposed as a new type of association pattern that contains items that are highly af“liated with each other Speci“cally 
the presence of an item in one transaction strongly implies the presence of every other item that belongs to the same hyperclique The h-con“dence measure captures the strength of this association and is de“ned as the minimum con“dence of all association rules containing an itemset An itemset is a hyperclique if the h-con“dence of this itemset is greater than 003 002  a user-speci“ed minimum h-con“dence threshold A hyperclique pattern is a maximal hyperclique pattern if no superset of this pattern is a hyperclique pattern Hypercliques can be generated at extremely low levels of support in dense datasets and have been shown to be useful in a number of applications For example used 
hypercliques to lter out noisy objects in order to improve clustering performance Hypercliques have been shown to work well for large datasets and low support threshold however they might still face problems for large dense datasets e.g census data especially as the threshold 003 002 decreases Dense datasets contain many strong correlations and are typically characterized by items with high frequency and many frequent patterns For example for the Pumsb dataset 002 002\003 and 003 002 002\003 004 004 
 over 3 million hypercliques are generated see Section V This is a known problem for FIM algorithms which perform well with sparse data sets such as market-basket data but face problems for dense data T o solv e this issue much work has been done toward obtaining Condensed representations of FIs CFIs The goal of these methods is to generate a smaller collection of representative sets from which all FIs can be deduced Many CFIs have been proposed e.g maximal  closed  005 free  non-derivable see 
 for a CFI surv e y  In this paper we propose Non-Derivable Hypercliques NDHC a condensed representation of Hypercliques based on Non-Derivable Itemsets NDIs a w ell-kno wn condensed representation of FIs NDIs have been shown to lead to large performance gains over FIs and ha v e recently been applied successfully to Outlier Detection Similar to NDIs our proposed collection is a lossless representation of hypercliques that is all hypercliques and their individual support can be generated given the NDHC collection of sets and their support without additional scans of the dataset which leads to signi“cant runtime advantages for certain datasets We also present an algorithm 
2011 23rd IEEE International Conference on Tools with Artificial Intelligence 1082-3409/11 $26.00 © 2011 IEEE DOI 10.1109/ICTAI.2011.80 489 


NDHCMiner to ef“ciently generate the NDHC itemsets and discuss how to generate all hypecliques and their support from NDHC We compare our proposed representation NDHC with Hypercliques HC as well as with Maximal Hyperclique Patterns MHP for se v eral datasets and for dif ferent parameter values Our experimental results show that the NDHC collection is overall signi“cantly smaller than the HC collection and grows much slower than HC as the hcon“dence threshold 003 002 decreases Moreover NDHC offers signi“cant advantages compared to HC and MHP for dense datasets such as census data and lower 003 002 values Finally we present runtime results for deriving all hypercliques and their support from NDHC The organization of this paper is as follows Section II summarizes previous related work In Section III we describe Hypercliques HC and Non-Derivable Itemsets NDI and in Section IV we present our proposed collection of itemsets Non-Derivable Hypercliques NDHC an ef“cient algorithm to mine NDHC sets NDHCMiner and discuss how to generate all hypercliques from NDHC Section V contains our experiments and results Finally in Section VI we summarize our work and provide concluding remarks II R ELATED W ORK Hyperclique patterns were introduced in and their properties and applications were discussed in The authors in presented a h ybrid approach for mining Maximal Hyperclique Patterns MHP An algorithm for mining Maximal Hyperclique Patterns in data sets containing quantitative attributes was presented in An algorithm for hierarchical clustering using hypercliques Hierarchical Clustering with Pattern Preservation HICAP was proposed in A data cleaning method based on hypercliques HCleaner was proposed in in order to remove noise with the goal to improve clustering or association rule performance noise was de“ned as irrelevant or weakly relevant objects to be removed before data analysis An unsupervised clustering algorithm that selects constraints automatically based on Hyperclique patterns called HP-KMEANS was presented in The authors in applied h ypercliques to the identi“cation of functional modules in protein complexes Hyperclique patterns were used in to detect unauthorized access to off-topic documents Finally the HICAP algorithm from w as compared with the bisecting K-means Cluster ing with pAttern Preservation K-CAP algorithm in III B ACKGROUND We consider a dataset 002 which contains 006 data points or transactions rows Let 003 002 004 007 002  007 003   007 003 005 be a set of 010 items in 002  Each transaction row 011 in 002 is a set of items such that 011 006\003 Given 012  a set of some items in Table I E XAMPLE DATASET  002 002\003 003\004 002 002\004 005 005  tid Items 1 006\003 007 2 007 3 010\003 011 4 006\003 007 5 006\003 010\003 011 6 006\003 007\003 010 7 006 8 006\003 010\003 011 9 007 10 006\003 007\003 010\003 011 003  we say that 011 contains 012 if 012 006 011  The support of 012  supp  012  is the percentage of transactions in 002 that contain 012  We say that 012 is frequent if it appears at least 002 times in 002  where 002 is a user-de“ned threshold The collection of frequent itemsets is denoted by 007\003  007\003 002 004 012 006\003\010 013\014\015\015 005 012 006 011 002 005 A Hypercliques HC In this section we present background information on hypercliques from The h-con“dence of an itemset 016 002 004 007 002  007 003   007 003 005 is de“ned as 003\017\020\006\021 005 016 006\002 022\007\006 004 017\020\006\021 005 007 002 012 007 003 023\004\004\004\023\007 003 006 023 017\020\006\021 005 007 003 012 007 002 023\007 004 023\004\004\004\023\007 003 006 023 004\004\004\023 017\020\006\021 005 007 003 012 007 002 023\004\004\004\023\007 003 002 002 006 005 where 017\020\006\021 is the association rule con“dence As shown in the 003\017\020\006\021 above is equivalent to 003\017\020\006\021 005 016 006\002 013\014\015\015 005 004 007 002 023\007 003 023\004\004\004\023\007 003 006 005 006 022\024\025 002 003 004 003 003 004 013\014\015\015 005 007 004 006 005 Given a user-de“ned support threshold 002 and a userde“ned h-con“dence threshold 003 002  itemset 012 is a hyperclique if it is frequent i.e 013\014\015\015 005 012 006 011 002  and its hcon“dence is at least 003 002 i.e 003\017\020\006\021 005 012 006 011 003 002  An itemset 016 is a Maximal Hyperclique Pattern MHP if none of its supersets is a hyperclique pattern The collection of hypercliques is shown below 026\027 002 004 012 006\003\010 013\014\015\015 005 012 006 011 002 013 003\017\020\006\021 005 012 006 011 003 002 005 H-con“dence has three important properties speci“cally the anti-monotone property the cross-support property and the strong af“nity property see for details The antimonotone property is used in this paper for mining hypercliques it states that if itemset 016 has h-con“dence below the 003 002 threshold so does every superset of 016  Example 1  Given the example dataset in Table I with 10 transactions and items 024  030  017  031  let support 002 equal to 2 and h-con“dence threshold 003 002 equal to 0.3 Itemset 016 002 004 024\017\031 005 is a hyperclique itemset with 013\014\015\015 005 024\017\031 006\002\007 032\002  and 003\017\020\006\021 005 024\017\031 006\002 013\014\015\015 005 024\017\031 006 014 010\011\012 004 013\014\015\015 005 024 006 023\013\014\015\015 005 017 006 023\013\014\015\015 005 031 006 005 002\007 033 004\002\003 004 013\007 032\003 002  
490 


Mining all Hypercliques HC can easily be implemented using an Apriori-like algorithm for mining 007\003 all frequent itemsets When 003 002 is 0 026\027 is the same as 007\003 During the candidate generation phase based on the anti-monotone property of the h-con“dence measure we can prune a candidate itemset of length 022 if any of its 005 022 015 014\006 length subsets is not a hyperclique pattern Finally after computing exact support and h-con“dence for all candidate itemsets we then prune candidates using the user speci“ed support threshold 002 and the h-con“dence threshold 003 002  B Non-Derivable Itemsets NDI In the following we present background on the NDI representation from Calders et al 10 sho wed that itemsets whose support can be deduced or derived from their subsets i.e derivable sets  can be pruned this can dramatically reduce the total amount of sets generated Let a general itemset 034  be a set of items and negations of items e.g 034 002 004 024\030 017 005  The support of 034 in this case is the number of transactions where items 024 and 030 are present while item 017 is not present We say that a general itemset 034  012 016 035 is based on itemset 016 if 016  012 016 035 The deduction rules in are based on the inclusion-e xclusion IE principle F o r e xample using the IE principle we write the following for the support of another general itemset 004 024 030\017 005  013\014\015\015 005 024 030\017 006\002 013\014\015\015 005 024 006 015 013\014\015\015 005 024\030 006 015 013\014\015\015 005 024\017 006\015 013\014\015\015 005 024\030\017 006  Based on 013\014\015\015 005 024 030\017 006 011 003  we can write the following 013\014\015\015 005 024\030\017 006 011 013\014\015\015 005 024\030 006\015 013\014\015\015 005 024\017 006 015 013\014\015\015 005 024 006 004 The above is an upper bound on the support of set 024\030\017   extended this concept to compute rules in order to derive the upper and lower bounds on the support of itemset 016 based on its subsets Let 036\037 005 016 006 and 037 005 016 006 be the lower and upper bounds on the support of itemset 016  036\037 005 016 006\002 022\024\025 004 005 005 005 016 006 023 010 016 017 012 010 020\031\031 005 037 005 016 006\002 022\007\006 004 005 005 005 016 006 023 010 016 017 012 010 006 005 where 005 005 005 016 006 denotes the summation shown below 005 005 005 016 006\002 002 005 004 006 005 007 005 015 014\006 006 007 007 006 006 005\002 013\014\015\015 005  006 004 Given a database 002 and threshold 002  the NDI algorithm produces the condensed representation 016 below which contains only the non-derivable frequent itemsets 016 002 004 012 006\003\010 013\014\015\015 005 012 006 011 002 013 036\037 005 012 006 020 002 037 005 012 006 005 Example 2 Given the dataset and 002 valueinTableI Itemset 016 002 004 024\017\031 005 is derivable because its lower and upper support bounds are equal to 3 as shown below 002\003\004\005\006\007\010 002\003\004\006\007\011 002\003\005\006\007\010  002\004\005\006\007\012 003\004\005\006\007\010 002\003\006\007\013 002\004\006\007\013 002\005\006\007\012 003\004\006\007\011 004\005\006\007\013 003\005\006\007\010 002\006\007\014 003\006\007\015 004\006\007\016 005\006\007\013 017\020\021 020\021  017\020\022\023 Figure 1 Generated itemsets for the example dataset with 002 002\003 and 004 002 002\004 005 005  Non-Derivable Itemsets NDI Hypercliques HC and Nonderivable Hypercliques NDHC shading denotes a frequent itemset 013\014\015\015 005 024\017\031 006 011 0 021 013\014\015\015 005 024\017 006\002\013 023\013\014\015\015 005 024\031 006\002\007 023 013\014\015\015 005 017\031 006\002\007 011 013\014\015\015 005 024\031 006\015 013\014\015\015 005 024\017 006 015 013\014\015\015 005 024 006\002\003 011 013\014\015\015 005 024\017 006\015 013\014\015\015 005 017\031 006 015 013\014\015\015 005 017 006\002\007 011 013\014\015\015 005 024\031 006\015 013\014\015\015 005 017\031 006 015 013\014\015\015 005 031 006\002\007 021 013\014\015\015 005 024\017 006\015 013\014\015\015 005 024\031 006\015 013\014\015\015 005 017\031 006 015 015 013\014\015\015 005 024 006 015 013\014\015\015 005 017 006 015 013\014\015\015 005 031 006\015 015 013\014\015\015 005 022 006\002\016 004 In order to nd the frequent itemsets the NDI algorithm uses Apriori-Gen the candidate generation step of Apriori to generate candidate sets and then prune infrequent candidates If the lower and upper bounds are equal then the itemset is derivable If a set 012 is NDI i.e 036\037 005 012 006 020 002 037 005 012 006  the algorithm needs to count the support of 012  if it is found that 013\014\015\015 005 012 006\002 036\037 005 012 006 or 013\014\015\015 005 012 006\002 037 005 012 006 allstrict supersets of 012 are derivable and they can be pruned This process is repeated until candidate sets cannot be generated further Monotonicity If itemset  is derivable then all its supersets 016   006 016  are derivable Finally the 016 collection is a condensed representation of 007\003  i.e all sets in 007\003 and their individual support can be generated given the 016 collection without scanning the dataset additional times IV M INING N ON D ERIVABLE H YPERCLIQUES NDHC We propose to use the ideas behind Non-Derivable Itemsets NDI to considerably prune the Hyperclique HC collection Speci“cally we propose to compute bounds on the support of a hyperclique as in the NDI context and then prune derivable hypercliques In the following we de“ne and discuss our proposed itemset collection Non-Derivable Hypercliques NDHC and present algorithms to rst mine NDHC sets then to generate all hypercliques and their support given the NDHC collection 
491 


A Non-Derivable Hypercliques NDHC De“nition 1 An itemset 012 is a non-derivable hyperclique if 012 is frequent  013\014\015\015 005 012 006 011 002  and 012 is a hyperclique  003\017\020\006\021 005 012 006 011 003 002  and 012 is non-derivable  036\037 005 012 006 020 002 037 005 012 006  Given a dataset 002  support threshold 002  and h-con“dence threshold 003 002  the collection of Non-Derivable Hypercliques 026\027 is de“ned below 026\027 002 010\011\012 004 012 006\003\010 013\014\015\015 005 012 006 011 002 013 003\017\020\006\021 005 012 006 011 003 002 013 036\037 005 012 006 020 002 037 005 012 006 005 Based on this de“nition and the de“nitions of NDIs and hypercliques the NDHC collection preserves the monotonicity principle In other words if itemset 016 is a non-derivable hyperclique then every subset 012  012 006 016  is also nonderivable and a hyperclique If itemset 016 is a derivable itemset with h-con“dence less than 003 002  then all its supersets are also derivable and not hypercliques The NDHC collection will always be smaller than the NDI collection and equal to NDI for 003 002 threshold equal to 0 Also the NDHC collection will always be smaller than or equal to HC as some of the hypercliques might be derivable Example 3  Given the dataset 002  and 003 002 values in Table I Itemset 016 002 004 024\017\031 005 is a derivable hyperclique as shown in Examples 1 and 2 On the other hand itemset  002 004 024\030\017 005 is non-derivable with support bounds 017\020 023 007\021  but it is not a hyperclique with 003\017\020\006\021 005  006\002\020 033 004\002\003 004 020\022 003 002  Figure 1 depicts the Non-Derivable Itemsets NDI Hypercliques HC and Non-derivable Hypercliques NDHC for the example dataset in Examples 1-3 with 002 002\020  and 003 002 002\003 004 007  In summary the collections shown in Figure 1 for the example dataset are 007\003 002 004 024\023\030\023\017\023\031\023\024\030\023\024\017\023\030\017\023\024\031\023\017\031\023\024\030\017\023\024\017\031 005  016 002 004 024\023 030\023 017\023 031\023 024\030\023 024\017\023 030\017\023 024\031\023 017\031\023 024\030\017 005  026\027 002 004 024\023 030\023 017\023 031\023 024\030\023 024\017\023 030\017\023 024\031\023 017\031\023 024\017\031 005  026\027 002 004 024\023 030\023 017\023 031\023 024\030\023 024\017\023 030\017\023 024\031\023 017\031 005  B NDHCMiner Algorithm Ana  ve method to mine Non-Derivable Hypercliques is to rst mine all Non-Derivable Itemsets  016  and in a second step prune those with h-con“dence less than 003 002  However there are some datasets for which a high number of NDIs is generated as shown in As both h ypercliques and NDIs make use of the monotonicity principle we are able to prune itemsets 016 if any of their subsets  either a are derivable i.e 036\037 005  006\002 037 005  006  or b have 003\017\020\006\021 005  006 less than 003 002  The algorithm for mining the NDHC sets NDHCMiner is shown in Algorithm 1 NDHCMiner generates candidates and computes support bounds as the NDI algorithm in steps 3-7 Instead of mining all NDIs and then calculating 003\017\020\006\021 005 016 006 for each set 016 in 016  we calculate 003\017\020\006\021 005 016 006 Algorithm 1 NDHCMiner Algorithm Input Database 002  support threshold 002  h-con“dence threshold 003 002 Output 026\027 collection 1 Count support of single items 2 026\027 023 single items with support 011 002 3 for itemset length  023 020 004\004 do 4 027 004  Generate non-derivable candidates of length  with Apriori-Gen 5 for all candidates 016 024 027 004 do 6 Compute support bounds for 016  036\037 005 016 006 and 037 005 016 006  based on NDI deduction rules 7 if 036\037 005 016 006 020 002 037 005 016 006 and 037 005 016 006 011 002 then 8  016 is a non-derivable itemset 9 Count 013\014\015\015 005 016 006 10 Compute 003\017\020\006\021 005 016 006 11 026\027 023 016 such that 013\014\015\015 005 016 006 011 002 and 003\017\020\006\021 005 016 006 011 003 002 12 else 13 Prune candidate 016 14 end if 15 end for 16 end for step 10 per Section III-A right after we count 013\014\015\015 005 016 006 step 9 Then non-derivable itemsets with 003\017\020\006\021 less than 003 002 can be excluded from the NDHC collection at the same time as non-derivable itemsets that were found to be infrequent step 11 C Deriving all hypercliques from NDHC The complete set of hypercliques and the support of each hyperclique can be generated given the NDHC collection The process for deriving all hypercliques from NDHC is similar to generating all frequent itemsets 007\003  and their support from the NDI collection presented in In our case the itemsets that were pruned in Algorithm 1 were either infrequent derivable or had h-con“dence less than 003 002  The itemsets that we wish to generate to obtain the complete HC collection are those sets which are not contained in NDHC but are contained in HC which are the derivable hypercliques Given non-derivable hyperclique 012 in the NDHC collection we generate a superset itemset 016 by adding an item 024 to itemset 012  016 002 004 012 016\004 024 005\005  Then we calculate support bounds 036\037 005 016 006 and 037 005 016 006  using the same deduction rules as in generating NDIs As in we prune itemset 016 if 036\037 005 016 006 002  i.e 016 is infrequent On the other hand if 016 is frequent it is either a derivable itemset or a non-derivable itemset that is not a hyperclique otherwise it would have been included in the NDHC collection If it is derivable i.e 013\014\015\015 005 016 006\002 036\037 005 016 006\002 037 005 016 006  we compute its h-con“dence 003\017\020\006\021 005 016 006  and prune 016 if 
492 


Table II D ATA S E T D ETAILS N UMBER OF TRANSACTIONS  ROWS  AV E R AG E TRANSACTION LENGTH  COLUMNS  SINGLE DISTINCT ITEMS Dataset Transactions Avg Transaction Items length Mushroom 006\007\003\010 003\005 007\007\011 Pumsb 010\011\004\010\012 013\010 003\007\007\005 Pumsb 002 010\011\004\010\012 014\007 005 010\006 003\004\006\011 Connect 012\013\014\014\013 010\005 007\003\011 T40I10D100K 007\004\004\004\004\004 010\004 005 012\007 011\010\003 003\017\020\006\021 005 016 006 003 002  The itemsets 016 that are not pruned are the derivable hypercliques and their support 013\014\015\015 005 016 006 is by de“nition equal to the support bounds Finally the complete HC collection is the union of NDHC and the derivable hypercliques V E XPERIMENTS A Experimental Setup We conducted all experiments on a Pentium 020 004 023\014 GHz processor with 020 GB RAM We used the NDI code available online 1  and implemented the rest in C For the comparison with Hypercliques we use the NDI code and set 003 002 to 0 We note that our implementation does not include the crosssupport pruning shown in F or the Maximal Hyperclique Patterns MHP we used the MHP code provided from the authors of Our experiments were conducted using four real datasets Mushroom Pumsb Pumsb and Connect and one arti“cially generated dataset T40I10D100K All datasets were obtained from the FIMI repository 2 see Table II for dataset details The Mushroom dataset contains characteristics from different species of mushrooms The Pumsb dataset is based on census data and is a benchmark set for evaluating the performance of FIM algorithms on dense data sets One of its characteristics is the skewed support distribution e.g 81.5 of the items in this set have support less than 1 while 0.95 of them have support greater than 90 Pumsb 010 is the same dataset as Pumsb except all items of 80 support or more have been removed making it less dense Finally the Connect dataset contains different game positions B Results We ran several experiments with various values for support threshold 002  and the h-con“dence threshold 003 002  1 Comparison with Hypercliques HC Table III contains the number of generated sets for the Hyperclique collection HC and the Non-Derivable Hyperclique collection NDHC for all datasets and various 002 and 003 002 combinations Table IV contains runtime in seconds for the mining of 1 http://www.adrem.ua.ac.be/goethals/software 2 http://“mi.ua.ac.be/data Table III C OMPARISON OF G ENERATED S ETS H YPERCLIQUES HC VS  N ON D ERIVABLE H YPERCLIQUES NDHC a T40I10D100K 002 002\004 005 004\004\003\014 002 002\004 005 004\004\014 002 002\004 005 004\007 004 002 HC NDHC HC NDHC HC NDHC 0.30 1084 1084 1008 1008 860 860 0.20 2186 2144 2019 1978 2254 2212 0.15 6551 6183 6140 5773 6659 6291 0.10 55186 34353 52784 32048 17254 14524 b Mushroom 002 002\004 002 002\004 005 007 002 002\004 005 003 004 002 HC NDHC HC NDHC HC NDHC 0.7 197 160 103 83 88 68 0.5 485 327 375 236 335 208 0.3 5506 1316 5264 1122 4610 818 0.2 59274 3228 58302 2663 53624 1143 0.1 619693 9781 574473 4347 53664 1143 c Pumsb 002 002\004 002 002\004 005 003 002 002\004 005 014 004 002 HC NDHC HC NDHC HC NDHC 0.95 2617 2399 553 337 413 244 0.90 6500 3112 4430 1044 3709 910 0.80 179905 6821 177819 4737 174753 4484 0.70 11948 9831 9378 0.50 62340 60029 47764 d Pumsb 002 002 002\004 005 004\003 002 002\004 005 004\010 002 002\004 005 004\012 004 002 HC NDHC HC NDHC HC NDHC 0.90 1021 377 927 283 902 258 0.70 17339 878 17238 777 17213 752 0.60 36530 1806 36530 1704 36525 1675 0.50 436825 4820 436825 4718 436820 4687 0.30 42251 42129 42086 e Connect 002 002\004 005 003 002 002\004 005 014 002 002\004 005 006 004 002 HC NDHC HC NDHC HC NDHC 0.95 2579 197 2558 176 2548 166 0.90 28839 274 28818 253 28808 243 0.80 553386 452 553386 431 533884 348 0.70 689 668 348 0.20 7574 1397 348 hypercliques versus the NDHCMiner A  015  symbol indicates that we had to stop execution of the corresponding algorithm due to the very large number of itemsets generated and extremely high execution time of the corresponding algorithm As this table shows the NDHC collection is much smaller than the HC collection especially for lower 003 002 values The collections are close in number of sets for the arti“cial dataset T40I10D100K except for the lowest 003 002 value of 0.1 see Table III\(a Differences in number of sets are larger for the real datasets as 003 002 decreases For example for the Mushroom dataset see Tables III\(b and IV\(a 002 002\003  and 003 002 002\003 004 014  there are more than 600 thousands hypercliques generated in about 4 minutes versus less than 10 thousand non-derivable hypercliques generated in 2 seconds 
493 


Table IV R UNTIME C OMPARISON  IN SECONDS  YPERCLIQUE HC MINING VS N ON D ERIVABLE H YPERCLIQUES NDHC a Mushroom 002 002\004 002 002\004 005 007 002 002\004 005 003 004 002 HC NDHC HC NDHC HC NDHC 0.71 11 11 1 0.51 11 11 1 0.33 13 13 1 0.2 20 1 20 1 19 1 0.1 260 2 244 2 19 1 b Pumsb 002 002\004 002 002\004 005 003 002 002\004 005 014 004 002 HC NDHC HC NDHC HC NDHC 0.95 37 29 28 20 23 15 0.90 67 32 56 24 46 19 0.80 2775 50 2591 40 2653 33 0.70 70 60 53 0.50 362 329 212 c Connect 002 002\004 005 003 002 002\004 005 014 002 002\004 005 006 004 002 HC NDHC HC NDHC HC NDHC 0.95 39 8 37 6 34 5 0.90 324 9 323 8 318 5 0.80 10 8 3375 5 0.70 11 9 5 0.20 22 10 5 The gains become more pronounced for dense datasets and for relatively high 003 002 values For example for the Pumsb dataset see Table III\(c and Table IV\(b 002 002\003 and 003 002 002\003 004 024  HC contains almost 180 thousand sets generated in 46 minutes versus less than 7 thousand NDHC sets generated in 50 seconds As 003 002 becomes 0.7 there are more than 3 million hypercliques versus less than 12 thousand nonderivable hypercliques generated in 71 seconds We make similar observations for the Pumsb 010 and Connect datasets see Table d For example for the Connect dataset and 002 002 003 002 002\003 004 024  HC mining takes 56 minutes versus 5 seconds for the NDHCMiner see Table IV\(c The NDHC collection grows in size faster i.e for higher 003 002 values for the dense datasets Pumsb and Connect Table III\(c and e than the other two real datasets Figures 2 and 3 contain a pictorial representation of generated sets in NDHC versus HC for the Mushroom and Pumsb dataset respectively as 003 002 decreases for two 002 values As these gures clearly show the HC collection grows much faster than the NDHC collection as 003 002 decreases 2 Comparison with Maximal Hyperclique Patterns MHP for dense datasets Figures 4 and 5 depict the number of Non-Derivable Hypercliques NDHC versus the number of Maximal Hyperclique Patterns MHP for two dense datasets Pumsb and Connect as the h-con“dence threshold 003 002 decreases while the support threshold 002 is 0 As can be seen in these gures the MHP collection increases in size much faster than NDHC for these dense 0.1 0.2 0.3 0.5 0.7 5000 25000 50000  h c Generated Sets  NDHC 0 NDHC 0.2 HC 0 HC 0.2   Figure 2 Non-Derivable Hypercliques NDHC versus Hypercliques HC for the Mushroom dataset as 004 002 decreases  002 002\004 or 0.2 0.50 0.70 0.80 0.90 0.95 10000 100000 150000  Generated Sets  h c NDHC 0 NDHC 0.2 HC 0 HC 0.2   Figure 3 Non-Derivable Hypercliques NDHC versus Hypercliques HC for the Pumsb dataset as 004 002 decreases  002 002\004 or 0.2 datasets For example for the Pumsb dataset see Figure 4 002 002\003  and 003 002 002\003 004 016  there are more than 290 thousand MHPs versus a little over 60 thousand NDHC The collections grow slower for the Connect dataset but the difference between NDHC and MHP becomes larger as 003 002 decreases see Figure 5 for this dataset 002 002\003 and 003 002 002\003 004 014  there are a little over 36 thousand NDHC and more than 240 thousand MHP It is also important to note that it is possible to derive all hypercliques and their individual support from the NDHC collection while only hypercliques not their support can be generated from the MHP collection 3 Deriving all Hypercliques and their support from the NDHC collection Figures 6 and 7 show a runtime comparison between mining all hypercliques HC versus mining 
494 


0.5 0.7 0.8 0.9 5000 50000 100000 200000 Generated Sets h c NDHC MHP Figure 4 Non-Derivable Hypercliques NDHC versus Maximal Hypercliques MHP for the Pumsb dataset  002 002\004  0.1 0.2 0.3 0.5 0.7 0.9 50000 100000 200000 h c Generated Sets NDHC MHP Figure 5 Non-Derivable Hypercliques NDHC versus Maximal Hypercliques MHP for the Connect dataset  002 002\004  the NDHC collection and then deriving the total HC collection NDHC-DeriveAllHC including hypercliques and their supports for the Pumsb dataset Figure 6 and for the Connect dataset Figure 7 with 002 002\003  The time to mine non-derivable hypercliques is included in the times shown in these gures In Figure 6 both algorithms that produce all hypercliques have similar runtime for 003 002 values higher than 0.85 As 003 002 decreases to 0.8 the runtime difference between the two algorithms becomes larger 46 minutes for HC mining versus 1 minute for deriving all hypercliques from NDHC NDHCDeriveAllHC We had to stop execution of the HC mining algorithm for 003 002 002\003 004 004  while the NDHC-DeriveAllHC algorithm nishes in under 95 minutes It is noteworthy that the NDHCMiner algorithm nishes in 71 seconds for the same dataset and parameter values As noted in the previous section there are over 3 million hypercliques versus under 12 thousand non-derivable hypercliques for the Pumsb dataset 002 002\003  and 003 002 002\003 004 004 see Table II The large gains in runtime is because we avoid additional scans of the dataset that would be needed to count the support of over 3 million derivable hypercliques instead we calculate the support of the derivable itemsets from their subsets Similar observations can be made for the Connect dataset in Figure 7 where the only runtime shown for HC is 324 seconds with 003 002 002\003 004 022  For example for 003 002 002\003 004 004\016  NDHCMiner nishes in 12 seconds NDHC-DeriveAllHC generates 1636534 hypercliques and their support in under 16 minutes while execution of HC mining had to be terminated For 003 002 002\003 004 004  execution of the NDHCDeriveAllHC algorithm had to be terminated as well more than 4 million hypercliques were generated before execution was terminated versus a total of 759 NDHC sets VI C ONCLUSION Hypercliques HC ha v e been sho wn to nd strong af“nity itemsets at low support levels and have been successful in applications such as clustering In this paper  we propose a new method to substantially reduce the size of the hyperclique collection based on the Non-Derivable Itemsets NDIs a condensed representation of Frequent Itemsets Our proposed representation of HC called Non-Derivable Hypercliques NDHC is a lossless representation of HC that is all hypercliques and their individual support counts can be generated given the NDHC collection without additional scans of the dataset We present an algorithm to ef“ciently mine all NDHC sets NDHCMiner and discuss how to generate all hypercliques and their support from the NDHC sets Our experiments show that the NDHC collection presents signi“cant advantages compared to HC especially for dense datasets and lower h-con“dence threshold values Moreover for dense datasets such as census data the NDHC collection grows much slower than another hyperclique representation Maximal Hyperclique Patterns MHP as the h-con“dence threshold 003 002 decreases Future directions include utilizing non-derivable hypercliques in applications such as clustering and further reducing the size of the hyperclique collection R EFERENCES  H Xiong M Steinbach P  T an and V  K umar  Hicap Hier archical clustering with pattern preservation in Proceedings of the 4th SIAM International Conference on Data Mining  2004 pp 279…290 
495 


0.7 0.75 0.8 0.85 0.9 0.95 1000 3000 5000 h c Time \(sec NDHCŠDeriveAllHC HC NDHCMiner Figure 6 Runtime in seconds to derive all hypercliques and their support from NDHC NDHC-DeriveAllHC versus Hyperclique Mining HC and NDHCMiner for the Pumsb dataset  002 002\004  0.7 0.75 0.8 0.85 0.9 100 500 1000 h c Time \(sec NDHCDeriveAllHC HC NDHCMiner Figure 7 Runtime in seconds to derive all hypercliques and their support from NDHC NDHC-DeriveAllHC versus Hyperclique Mining HC and NDHCMiner for the Connect dataset  002 002\004   H Xiong G P a nde y  M Steinbach and V  K umar  Enhancing Data Analysis with Noise Removal IEEE Transactions Knowledge and Data Engineering  vol 18 no 3 pp 304 319 2006  H Xiong P  T a n and V  K umar  Mining strong af nity association patterns in data sets with skewed support distribution in Third IEEE International Conference on Data Mining  2003 pp 387…394  T  Calders and B Goethals Mining all non-deri v a ble frequent itemsets Proc PKDD Int Conf Principles of Data Mining and Knowledge Discovery  pp 74…85 2002  R Agra w al and R Srikant F ast algorithms for mining association rules in large databases Proc International Conference on Very Large Data Bases VLDB  pp 487…499 1994  X W u V  K umar  J Ross Quinlan J Ghosh Q Y a ng H Motoda G McLachlan A Ng B Liu P Yu Z Zhou M Steinbach D Hand and D Steinberg Top 10 algorithms in data mining Knowledge and Information Systems  vol 14 no 1 pp 1…37 2008  Y  Huang H Xiong W  W u  a nd Z Zhang  A hybrid approach for mining maximal hyperclique patterns in IEEE International Conference on Tools with Arti“cial Intelligence ICTAI  2004 pp 354…361  M Zaki and C Hsiao Ef cient Algorithms for Mining Closed Itemsets and Their Lattice Structure IEEE Transactions Knowledge and Data Engineering  vol 17 no 4 pp 462…478 2005  T  Calders C Rigotti and J Boulicaut A surv e y on condensed representations for frequent sets LNCS Constraintbased mining and Inductive Databases  vol 3848 pp 64…80 2004  T  Calders and B Goethals Non-deri v a ble itemset mining  Data Mining and Knowledge Discovery  vol 14 no 1 pp 171…206 February 2007  A K ouf ak ou J Secretan M F ox G Gramajo G C Anagnostopoulos and M Georgiopoulos Outlier detection for large high-dimensional categorical data using non-derivable and non-almost-derivable sets in International Conference on Data Mining DMIN  2009 pp 505…511  H Xiong P  T a n and V  K umar  Hyperclique pattern disco very Data Mining and Knowledge Discovery  vol 13 no 2 pp 219…242 2006  Y  Huang H Xiong W  W u a nd S Sung Mining quantitative maximal hyperclique patterns A summary of results Advances in Knowledge Discovery and Data Mining  pp 552 556 2006  Y  Chang D Lee J Archibald and Y  Hong Unsupervised clustering using hyperclique pattern constraints in IEEE 19th International Conference on Pattern Recognition ICPR  2008 pp 1…4  H Xiong X He C Ding Y  Zhang V  K umar  and S Holbrook Identi“cation of functional modules in protein complexes via hyperclique pattern discovery in Paci“c Symposium on Biocomputing  2005 p 221  T  Hu Q Xu H Y uan J Hou and C Qu Hyperclique Pattern Based Off-Topic Detection in Lecture Notes in Computer Science APWeb/WAIM  vol 4505 2007 pp 374 381  H Xiong M Steinbach A Ruslim and V  K umar  Characterizing pattern preserving clustering Knowledge and Information Systems  vol 19 pp 311…336 2009  B Ganter and R W ille Formal concept analysis  SpringerVerlag 1999  A K ouf ak ou J Secretan and M Geor giopoulos Nonderivable itemsets for fast outlier detection in large highdimensional categorical data Knowledge and Information Systems published online  2010 
496 


In the EventAnalyzer, the definition of queries is facilitated by the query builder \(Figure 8a\: Here, analysts can select those event types that are considered relevant from the overall list of event types defined in the given event-model the same is provided for correlation sets If required for their investigations analysts can define more sophisticated clauses as, for instance, on event attribute values The event-tunnel top-view in Figure 8c shows the results of the above query: Single events are plotted as glyphs, with userdefined mappings Figure 8b from event attributes to color shape and size Correlations between events are plotted as colored bands, connecting the correlated events by their times of occurrence As depicting similar-looking glyphs at similar points in time, the plot in Figure 8c unveils that the suspicious   sers  officials by the system From visualizations of the EventBase business analysts gain deep insight into the various business processes persisted therein Query-driven and tailored for a quick and step-wise navigation through the EventBase, analytical applications such as the Event Analyzer push it one step further: From an initial clue such as the above fraud alarms they allow the analyst detecting previously unconsidered facts and relationships Consider the business analyst formulating a new query on the event-data warehouse now selecting all events from the BetPlaced-event table that are a   occurred recently. The corresponding event-tunnel side-view in Fig ure 8d scatter of occurrence \(x axis\. It is easy to see that soon after the detected fraud attempts \(mapped to blue based on their account IDs an outlier showing a significantly higher bet amount occurs mapped to red as exceeding a threshold  For an experienced analyst this data point represents a valuable link to related and possibly conspicuous data. One possible interpretation could be that the outlier was placed by a so-called putteron  a fraudster that places bets for people who are prohibited to bet. The suspicion  available in the EventBase and depicted over time in Figure 8e The step chart shows that  inactive with sequences of cash-ins placing a bet winning a bet and cash-outs occurring straightly every few days The example shows that with tailored analysis tools such as the EventAnalyzer the EventBase serves as a valuable and easy to access source for knowledge discovery Thus far the visual analysis of event data was proven useful adoptions of well-known data-mining techniques such as similarity searching are however in current development For more information on the EventAnalyzer the Event Tunnel and its application in fraud detection, readers may refer to Suntinger et al 24  VIII  C ONCLUSION AND O UTLOOK  In this paper we have addressed several open issues regarding event persistence and analysis of existing CEP solutions In particular we have presented a solution for efficient event data repository management as an extension for CEP solutions in order to enable post analysis of events and break up current event processing limitations The presented solution is based on a formal and efficient data schema maintained by a common RDBMS It solves common problems to efficiently persist events their relationships the preservation of calculated event metrics and the representation of non-native database types including de-AT  de-AT   Figure 8  Event Driven Business Intelligence with the EventAnalyzer    


advanced concepts such as event inheritance The implementation and the evaluation have been conducted upon the existing event processing solution SARI The work presented in this paper is part of a larger, longterm research effort aiming at developing a comprehensive set of technologies and tools for event analysis Furthermore we plan to evaluate the integration of the Event Data Warehouse EDWH respectively the linking of EventBase entities to external DWH dimensions to bundle dynamic real-time event information with large historical information repositories for better situation detection and verification of  decision making processes A CKNOWLEDGEMENT  We want to thank the Senactive development team for their valuable discussions and for implementing this research work R EFERENCES  1  Aalst W Weijters A J M M and Maruster L 2004  mining: Discovering process models from event logs. IEEE Transactions on Knowledge and Data Engineering, 16, 9, 1128 1142  2  Abadi, D.J., Ahmad, Y., Balazinska, M., \307etintemel, U., Cherniack, M Hwang, J.H., Lindner, W., Maskey, A.S., Rasin, A., Ryvkina, E., Tatbul N Xing Y and Zdonik S 2005 The Design of the Borealis Stream Processing Engine In Proc of the Conf on Innovative Data Systems Research, Asilomar, CA, USA, 277 289 3  Abadi D J Carney D Cetintemel U Cherniack M Convey C Lee, S., Stonebraker, M., Tatbul, N. and Zdonik, S. 2003. Aurora: A new model and architecture for data stream management. VLDB Journal, 12 120 139 4  Adi A and Etzion O 2004  AMIT  the situation manager The VLDB Journal, 13, 2, 177-203 5  Brobst S and Ballinger C 2000 Active Data Warehousing Whitepaper EB 1327, NCR Corporation 6  Chen, S.-K., Jeng, J.-J. and Chang, H. 2006. Complex Event Processing using Simple Rule-based Event Correlation Engines for Business Performance Management. CEC/EEE 7  Esper, http://esper.codehaus.org/, 2009-0201  8  Hoppe A. and Gryz J. 2007. Stream Processing in a Relational Database a Case Study Database Engineering and Applications Symposium 2007. IDEAS 2007. 11th International Volume, 216 224 9  Inmon B., Imhoff C. and Sousa R. 2001. Corporate Information Factory Wiley, New York 10  Kimball R 1996 The Data Warehouse Toolkit Practical Techniques for Building Dimensional Data Warehouses. John Willey, 1996 11  Luckham, D. 2005. The Power Of Events. Addison Wesley 12  Mannila H and Moen P 1999 Similarity between event types in sequences, Proc. First Intl. Conf. on Data Warehousing and Knowledge Discovery  271 28 13  Moen P 2000 Attribute Event Sequence and Event Type Similarity Notions for Data Mining Ph.D thesis Department of Computer Science, University of Helsinki, Finland 14  Rozsnyai S 2006 Efficient indexing and searching in correlated business events. PhD thesis, Vienna University of Technology 15  Rozsn yai S., Schiefer J and Sch atten A  2007 Concepts and Models for Typing Events for Event Based Systems  International Conference on Distributed Event Based Systems   Toronto  Canada  DEBS  0 7   16  Rozsnyai, S., Vecera, R., Schiefer, J and Schatten, A. 2007 Event cloud  searching fo r correlated business events. In CEC/EEE, IEEE Computer Society  409 420  17  Schiefer, J. and Seufert, A. 2005. Management and controlling of timesensitive business processes with sense & respond. In CIMCA/IAWTIC IEEE Computer Society, 77 82  18  Schiefer J Rozsnyai S Saurer G and Rauscher C 2007 Event Driven Rules for Sensing and Responding to Business Situations International Conference on Distribut ed Event Based Systems, Toronto   19  Schiefer, J. and Seufert, A. 2005. Management and Controlling of TimeSensitive Business Processes with Sense  Respond International Conference on Computational Intelligence for Modelling Control and Automation \(CIMCA\, Vienna 20  Schrefl M and Thalhammer T 2000 On Making Data Warehouses Active In Proc of the 2nd Intl Conf on Data Warehousing and Knowledge Discovery DaWaK Springer LNCS 1874 London UK 34 46 21  Seirio M. and Berndtsson M. 2005 Design and Implementation of an ECA Rule Markup Language. RuleML, Springer Verlag, 98 112 22  Stonebraker M  and 307etintemel U 2005  One Size Fits All An Idea Whose Time Has Come and Gone, ICDE 2005, 2-11 23  Suntinger M Schiefer J Roth H and Obweger H 2008 Data Warehousing versus Event-Driven BI Data Management and Knowledge Discovery in Fraud Analysis  International Conference on Software Knowledge Information Management and Applications  Kathmandu, Nepal  08  24  Suntinger, M., Obweger, H., Schiefer, J and Groeller M. E  2007 The E vent T unnel  Interactive visualization of complex event streams for busin ess process pattern analysis Technical report Institute of Computer Graphics and Algorithms  Vienna University of Technology  25  Vecera R 2007 Efficient indexing Searching and Analysis of Event Streams. PhD thesis, Vienna University of Technology 26  Vecera R Rozsnyai S and Roth H 2007 Indexing and search of correlated business events The Second International Conference on Availability, Reliability and Security, Ares 2007, 1124 1134 27  Widom J  Ceri S and Dayal U 1994 Active Database Systems Triggers and Rules for Advanced Database Processing Morgan Kaufmann Publishers Inc., San Francisco, CA 28  Wu P Bhatnagar R Epshtein L Bhandaru M and Shi Z 1998 Alarm correlation engine \(ACE\, In Proceedings of the IEEE/IFIP 1998 Network Opera tions and Management Symposium NOMS New Orleans  29  Zdonik S Stonebraker M., Cherniack M. Cetintemel U Balazinska M. and Balakrishnan, H. 2003. The Aurora and Medusa Projects. IEEE Data Engineering Bulletin, 26  1  


4 Heart 270 13 2 5 Diabetes 768 8 2 6 Pima 768 8 2  For a classifier, classification accuracy is a basic performance measurement, which is the ratio of the number of cases truly predicted by the classifier over the total number of cases in the whole test dataset, e.g Number of cases truly predictedClassification accuracy= 100 Total number of cases  2 We compared BitTableAC with some associative classification algorithms on accuracy, such as CBA and CMAR. We also include the C4.5 and LIBSVM results on the same datasets as a reference. C4.5 is a well-known traditional classifier based on decision tree induction technique, and LIBSVM is an accurate support vector machine classifier For the fair of the comparison, we do not implement these algorithms, but their classification accuracies are obtained from their research literatures. For BitTableAC, the MinSup MinConf and NFP \(num of fuzzy partitions and 3, respectively The comparison results are shown in Table 4. As shown in this table, BitTableAC has the satisfactory classification accuracy. It achieves the highest accuracy in four of six datasets used in experiments and also outperforms other algorithms on average Table 4 Experiment Results Dataset C4.5 LIBSVM CBA CMAR BitTableAC 1 Glass 68.70 77.57 72.60 70.10 75.23 2 Iris 93.60 94.00 92.90 94.00 96.25 3 Breast 95.70 96.14 95.80 96.40 98.53 4 Heart 82.50 88.45 81.50 82.20 86.67 5 Diabetes 72.10 73.83 75.30 75.80 80.00 6 Pima 75.50 79.69 73.10 75.10 81.82 Average 81.35 84.95 81.87 82.27 86.42  IV. CONCLUSION In this paper, an accurate associative classifier BitTableAC is proposed. It employs BitTable to mine association rules 


efficiently, and fuzzy c-means \(FCM attributes. To evaluate the performance of the proposed algorithm, we compare BitTableAC with other well-known classifiers on accuracy including previous associative classifiers, C4.5 and LIBSVM on 6 test datasets from UCI Machine Learning Repository. The results show that, in terms of accuracy, BitTableAC outperforms others REFERENCES 1] G. Goulbourne, F. Coenen, and P. Leng, "Algorithms for computing association rules using a partial-support tree," Knowledge-Based Systems vol. 13, pp. 141-149, Apr 2000 2] M. J. Zaki, "Scalable algorithms for association mining," Ieee Transactions on Knowledge and Data Engineering, vol. 12, pp. 372-390 May-Jun 2000 3] F. Bonchi, F. Giannotti, A. Mazzanti, and D. Pedreschi, "Efficient breadth-first mining of frequent pattern with monotone constraints Knowledge and Information Systems, vol. 8, pp. 131-153, Aug 2005 4] G. Grahne and J. F. Zhu, "Fast algorithms for frequent itemset mining using FP-trees," Ieee Transactions on Knowledge and Data Engineering, vol 17, pp. 1347-1362, Oct 2005 5] I. I. Artamonova, G. Frishman, and D. Frishman, "Applying negative rule mining to improve genome annotation," Bmc Bioinformatics, vol. 8, pp. -, Jul 21 2007 6] J. W. Han, J. Pei, Y. W. Yin, and R. Y. Mao, "Mining frequent patterns without candidate generation: A frequent-pattern tree approach," Data Mining and Knowledge Discovery, vol. 8, pp. 53-87, Jan 2004 7] Y. C. Hu and G. H. Tzeng, "Elicitation of classification rules by fuzzy data mining," Engineering Applications of Artificial Intelligence, vol. 16, pp 709-716, Oct-Dec 2003 8] J. D. Holt and S. M. Chung, "Mining of association rules in text databases using Inverted Hashing and Pruning," Data Warehousing and Knowledge Discovery, Proceedings, vol. 1874, pp. 290-300, 2000 9] Y. J. Li, P. Ning, X. S. Wang, and S. Jajodia, "Discovering calendar-based temporal association rules," Data & Knowledge Engineering, vol. 44, pp 193-218, Feb 2003 10] Y. J. Tsay and J. Y. Chiang, "CBAR: an efficient method for mining association rules," Knowledge-Based Systems, vol. 18, pp. 99-105, Apr 2005 11] B. Liu, W. Hsu, and Y. Ma, "Integrating Classification and Association Rule Mining," in Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining, KDD'98, AAAI, New York, 1998, pp. 80-86 12] W. Li, J. Han, and J. Pei, "CMAR: accurate and efficient classification 


based on multiple class association rule," in Proceedings of the 2001 IEEE International Conference on Data Mining, ICDM'01, San Jose, CA, 2001 pp. 369-376 13] D. Janssens, G. Wets, T. Brijs, and K. Vanhoof, "Adapting the CBA algorithm by means of intensity of implication," Information Sciences, vol 173, pp. 305-318, Jun 23 2005 14] W. Song, B. R. Yang, and Z. Y. Xu, "Index-BitTableFI: An improved algorithm for mining frequent itemsets," Knowledge-Based Systems, vol. 21 pp. 507-513, Aug 2008 15] J. Dong and M. Han, "BitTableFI: An efficient mining frequent itemsets algorithm," Knowledge-Based Systems, vol. 20, pp. 329-335, May 2007  532 


 Table I.  Number of intervals Stage Interval No. Stage Interval No EP 7 ES 8 ED 10 EB 9 ET 8 EI 11  Table II.  Results using the proposed approach Stage Bias MMRE MdMRE ES -8.5% 27.0% 17.0 ED -33.1% 40.5% 13.7 EB -2.8% 9.3% 7.5 ET -11.6% 16.7% 7.23 EI -20% 91.0% 30.2  Table III.  Results using exponential regression Stage Bias MMRE MdMRE ES -24.3% 81.3% 49.7 ED -72.3% 120.4% 54.224 EB 0.7% 44.35% 37.6 ET -45.4% 81.1% 39.0 EI -179% 184% 104.0  Results shown in Table III revealed that most of predictions are under estimation which supports our approach findings. The best estimation accuracy was obtained in building stage, which also corroborates our findings that best estimation accuracy was in building stage. The negative values in Bias criterion show underestimation. It is acknowledged that MMRE is unbalanced in many validation circumstances and leads to overestimation more than underestimation. In our case, we found that MMRE leads to underestimation in most stages. This is may be related to the absence of systematic scheme between all prior effort records   253   Figure 1. Effort distribution of Planning stage 


Figure 2. Effort distribution of Specification stage Figure 3. Effort distribution of Design effort stage  Figure 4. Effort distribution of Building stage Figure 5. Effort distribution of Testing stage Figure 6. Effort distribution of Imp stage   Table IV. Statistical significance Stage sum rank Z-value p-Value ES 769 -4.31 <0.01 ED 713 -5.03 <0.01 EB 685 -5.4 <0.01 ET 595 -6.54 <0.01 EI 799 -3.93 <0.01  The comparison between our approach and exponential regression technique showed that there are considerable improvements in estimation accuracy on all phases of software development lifecycle. MMREs of our approach have been reduced by at least 35.05 and at most 93%. Biases have been reduced by at least 3.5% and at most 159%.We have to bear in mind that the length of interval plays important role in estimation accuracy, thus, when the universe of discourse is partitioned into several equal intervals, the distribution of data should be taken into account. Moreover, we should remove the extreme values because they affect interval partitioning, thus, estimation accuracy Figures 7 to 11 show comparison between proposed approach and exponential regression in each stage by using Boxplot. The Boxplot [17] offers a way to compare between estimation models based on their absolute residuals. The Boxplot is non-parametric statistics used to show the median as central tendency of distribution, interquartile range and the outliers of individual models [17]. The length of Boxplot from 


lower tail to upper tail shows the spread of the distribution. The length of box represents the interquartile range that contains 50% of observations The position of median inside the box and length of Boxplot indicate the skewness of distribution. A Boxplot with a small box and long tails represents a very peaked distribution while a Boxplot with long box represents a flatter distribution. The prominent and common characteristic among these figures is the spread of absolute residuals for our approach is less than spread of exponential regression which presents more accurate results. The larger interquartile of exponential regression indicates a high dispersion of the absolute residuals. The Boxplot revealed that the box length for our models is smaller than exponential regression which also indicates reduced variability of absolute residuals. The median of our model is smaller than median of exponential regression which revealed that at least half of the predictions of our model are more accurate than exponential regression 254  Figure 7. Boxplot of absolute residuals for the specification stage  Figure 8. Boxplot of absolute residuals for the design stage   Figure 9. Boxplot of absolute residuals for the building stage Figure 10. Boxplot of absolute residuals for the testing stage   The lower tails of our model is much smaller than upper tail which means the absolute residuals are skewed towards the smaller value Figure 11 illustrates the reason of why prediction of implementation stage in our approach produced the worst accuracy. The reason related to the existing of outlier. Although one project is considered as an outlier the MMRE is easily influenced with that project Based on the obtained results, we can observe that 


exponential regression gave bad accuracy. The reason may relate to the structure complexity of prior effort records. There is no correlation between all prior stages and target stage To ensure that the results obtained are not by chance we investigated the statistical significance of the proposed approach using Wilcoxon sum rank test for absolute residuals as shown in Table IV. In this test if the resulting p-value is small \(p<0.05 statistically significant difference can be accepted between the two samples median. The residuals obtained using the proposed approach were significantly different from those obtained by exponential regression Suggesting that, there is difference if the predications generated using the proposed approach or exponential regression and based on the accuracy comparison in Tables II and III we can safely conclude that our proposed method outperformed exponential regression for stage effort estimation Figure 11. Boxplot of absolute residuals for the implementation stage  VIII. CONCLUSIONS Some of software projects are failed due to the absence of re-estimation during software development which results in huge gap between initial plan and final outcome. Even with good estimate at first stage the project manager must keep update with project progress and should be able to re-estimate the project at any particular point of project in order to re-allocate the proper number of resources. The objective of this paper was to check whether the prior effort records can 255 be used to predict stage effort with reasonable accuracy or not. The obtained results revealed that using association rule and Fuzzy set theory lead to significant improvement in stage-effort estimation and give project manager an evolving picture about project progress. Comparing our approach with exponential regression showed that there is a considerable potential in estimation accuracy. As part of future plan, we 


intend to expand this work to involve some interesting features in each stage prediction and evaluate it on many datasets   REFERENCES  1] F. Ricardo, N. Ana, M. Paula, B. Gleidson, R. Fabiano ODE: Ontology-based software Development Environment, Proceedings of the IX Argentine Congress on Computer Science, pp. 1124-1135, 2003 2] E. Mendes, B. A. Kitchenham. Further comparison of cross-company and within-company effort estimation models for Web applications. In: Proc. 10th IEEE International Software Metrics Symposium, Chicago USA, pp.348-357, 2004 3] B. Boehm, R. Valerdi. Achievements and Challenges in Software Resource Estimation, Proceedings of ICSE 06 Shanghai, China, pp. 74-83,  2006 4] K. Molokken, M. Jorgensen. A review of software surveys on software effort estimation, Proceedings of International Symposium on Empirical Software Engineering \(ISESE 2003 5] M. Jorgensen, K. Molokken-Ostvold. How large are software cost overruns? A review of the 1994 CHAOS report, Information and Software Technology, Vol. 48 issue 4. PP. 297-301, 2006 6] X. Huanga, D. Hob, J. Rena, L. F. Capretz. Improving the COCOMO model using a neuro-Fuzzy approach Applied Soft Computing, Vol.7, issue 1, pp. 29-40, 2007 7] L. Briand, T. Langley, I. Wieczorek. A replicated assessment and comparison of common software cost modeling techniques, Proceedings of the 22nd international conference on Software Engineering, 2000 8] S.-J Huang, N. H. Chiu. Optimization of analogy weights by genetic algorithm for software effort estimation Information and Software Technology, Vol. 48, issue 11 pp. 1034-1045, 2006 9] Z. Xu, T. M. Khoshgoftaar. Identification of Fuzzy models of software cost estimation, Fuzzy Sets and Systems, Vol. 145, issue 1, pp. 141-163, 2004 10] R. Pressman. Software Engineering: practitioner 


approaches, McGraw Hill, London, 2004 11] M. Boraso, C. Montangero, H. Sedhi. Software cost estimation: an experimental study of model performance Universita di Pisa, Italy, 1996 12] Y. Wang, Q. Song, J. Shen., 2007. Grey Learning Based Software Stage-Effort Estimation. International Conference on Machine Learning and Cybernetics, pp 1470-1475, 2007 13] S. G. MacDonell, M. J. Shepperd. Using prior-phase effort records for re-estimation during software projects Ninth International, Software Metrics Symposium, pp 73- 86, 2003 14] M .C Ohlsson, C. Wohlin. An Empirical Study of Effort Estimation during Project Execution, Sixth International Software Metrics Symposium \(METRICS'99 1999 15] N. H. Chiu,,S. J. Huang.  The adjusted analogy-based software effort estimation based on similarity distances Journal of Systems and Software, Vol. 80, issue 4, pp 628-640, 2007 16] P. Sentas, L. Angelis, I. Stamelos, G.  Bleris. Software productivity and effort prediction with ordinal regression Information and Software Technology, Vol. 47, issue 1 pp. 17-29, 2005 17] E. Mendes, N. Mosley. Comparing effort prediction models for Web design and authoring using boxplots Australian Computer Science Communications,  Vol. 23 Issue 1, pp. 125-133, 2001 18] E. Mendes, N. Mosley, I. Watson. A comparison of casebased reasoning approaches, Proceedings of the 11th international conference on World Wide Web, pp. 272280, 2002 19] Q. Zhao, S. S. Bhowmick. Association Rule Mining: A Survey  http://citeseer.ist.psu.edu/734613.html, 2003 20] S. Morisak, A. Monden, H. Tamada. An Extension of association rule mining for software engineering data repositories, Information Science Technical Report NAIST, 2006 21] Q. Song, M. Shepperd.  M. Cartwright, C. Mair. Software defect association mining and defect correction effort prediction, IEEE transaction on software engineering Vol. 32, No.2, pp. 69-82, 2006 


22] R. Agrawal, T. Amielinski, A. Swami. Mining association rule between sets of items in large databases Proceedings of the ACM SIGMOD International Conference on Management of Data, pp. 207-216, 1993 23] M-J. Huang, Y-L. Tsou, S-C. Lee.  Integrating Fuzzy data mining and Fuzzy artificial neural networks for discovering implicit knowledge, J. Knowledge-Based Systems, Vol.19 \(6 24] ISBSG International Software Benchmarking standards Group, Data repository release 10, Site http://www.isbsg.org, 2007 25] L. Zadeh. Toward a theory of Fuzzy information granulation and its centrality in human reasoning and Fuzzy logic. J. Fuzzy sets and Systems 90, pp. 111-127 1997 26] I. H. Witten, E. Frank. Data Mining: Practical machine learning tools and techniques, 2nd Edition, Morgan Kaufmann, San Francisco, 2005   256 


encountering a related term, i.e. IC\(c e intuition behind the use of the negative likelihood is that the more probable a term to appear, the less information it conveys. All these features show that Jiangs measure tends to be more general and more appropriate for evaluating nontaxonomically related terms. Indeed, a high score of the relatedness measures suggests a strong relationship between terms Nevertheless, all relatedness measures have limitations because they assume that all the semantic content of a particular term is modeled by semantic links in WordNet Consequently, in many situations, truly related terms obtain a low scores even though their belongings to a certain category of tags, e.g., jargon tags Additionally, when measuring the quality of an automatically knowledge acquisition results, the typical measures used in Information Retrieval are Recall, Precision and F-Measure. However, computing Recall and F-Measure requires the availability of a Gold Standard. Hence, we will only compute the Precision which speci?es to which extent the non-taxonomic relationships is extracted correctly. In this case, the ratio between the correctly extracted relations i.e., their relatedness measures is greater than or equal to a minimum threshold, and the whole number of extracted ones is computed. Thus, we have Precision Total correctly selected entities Total selected entities 12 http://search.cpan.org/dist/WordNet-Similarity 13 A term refers to a tag subject or a tag object C. Evaluation of non-taxonomic relationships Only a percentage of the full set of non-taxonomic relationships \(89 is caused by the presence of non standard terms which are not contained in WordNet and, in consequence, cannot be evaluated using WordNet-based relatedness measures. Fig. 5 depicts the evaluation results of the extracted non-taxonomic relationships against their relatedness measures High relatedness score \(88 17% of the extracted relationships, as most of terms are strongly related with respect WordNet Null Scores were obtained for 5% of the extracted 


relationships. Analyzing this case in more detail, we have observed that the poor score is caused in many situations by the way in which Jiangs distance metric works. This latter completely depends on the distance between two terms based on the number of edges found on the path between them in WordNet. In consequence this measure returns a value that does not fully represent reality. For example, on the one hand, Jiangs distance metric returns a null value for the relationship between insurance and car, even though the ?rst is a commonly related to the second, i.e., car involved insurance Finally with a minimal Jiangs distance metric threshold, set to 46%, the computed precision of correctly extracted relationships candidates is equal to 68.8 An example of extracted non-taxonomic relationships is depicted in Table V where each relation describes the subject tag, e.g., tool, the predicate, e.g is being developed within, and the object tag, e.g mesh. Fig. 4 represent a fragment output of the extracted ontological structure where each concept de?nes a set of similar and synonym tags and labels, i.e., mentions has been, revealed, caused and is created with describe the predicates of the non-taxonomic relationships between terms Due to the limitations observed by the automatic evaluation procedure and the lack of gold standards containing non-taxonomic relationships, we have examined the extracted non-taxonomic relationships from a linguistic point 377 Top space      distance     quad great     groovy nifty caused address      addresses extension      quotation   reference  references extensions        referenz     source      refrence sources    rfrences    quotations research    search     searching searchs open-source     open_source 


opensource linux aim     design     designer      designers patern    project     patterns     projekte projects web+design    web_design webdesign internet       internetbs net          web network      networking networks      web discussion     news       password word      words community      communities is_created_with mentions revealed has_been Figure 4. A fragment output of the extracted ontological structure of view. This qualitative evaluation can bring some interesting insights about the kind of results one can expect Invalid relations are extracted: Even though a relation such as music cities skill is considered as correct one since tag subject, tag object and predicate are correctly extracted. From a semantic point of view, this relation has no meaning. Hence, a higher precision is expected Figure 5. Summary of non-taxonomic evaluation measure Table V EXAMPLES OF EXTRACTED NON TAXONOMIC RELATIONSHIPS Subject Predicate Object search has been reference reference mentions search tool is being developed within mesh security added encoding search revealed reference java provides library by performing the sense analysis on complete relations An ambiguity in the extracted predicates between terms is observed: Hence, same relations are redundant since they use a synonym predicates between terms, e.g java provides library and java yields library. Thus we expect that the redundancy removal within extracted relations will be of bene?t for the improvement of the 


obtained results VI. CONCLUSION AND FUTURE WORK The extraction of non-taxonomic relationships from folksonomies is to the best of our knowledge is the least tackled task within ontology building from folksonomy. This is why there is a need of novel and general purpose approaches covering the full process of learning relationships. In this paper, we introduced a new approach called NONTAXFOLKS that starts by pre-processing tags aiming at getting a set of frequent tagsets corresponding to an agreed representation Then, they are used to retrieve related tags using external resources such as WordNet. Thanks to the particular structure of triadic concepts, it allows grouping semantically related tags by considering the semantic relatedness embodied in the different frequencies of co-occurences among users, resources and tags in the folksonomy. Thereafter we introduced an algorithm called NTREXTRACTION for extracting non-taxonomic relationships between pair of tags picked from the triadic concepts. In summary, our approach uses several well known techniques \(such as formal concept analysis or association rule discovering the social bookmaring environnement in order to propose a new way of extracting labeled non-taxonomic relationships between tags. Currently, we are investigating the following topic concerning the discovered predicates between two terms. Indeed, in order to avoid relationships redundancy and thus a redundancy in the builded ontology. One can try to classify them into prede?ned semantic classes, detect synonyms, inverses, etc. A standard classi?cation of verbs could be used for this purpose, adding additional information about the semantic content, e.g., senses, verb types, thematic roles, etc., of predicates relationships 378 REFERENCES 1] J. Pan, S. Taylor, and E. Thomas, Reducing ambiguity in tagging systems with folksonomy search expansion, in Proceedings of the 6th Annual European Semantic Web Conference \(ESWC2009 2] V. S. M. Kavalec, A. Maedche, Discovery of lexical entries for non-taxonomic relations in ontology learning, in Proceedings of the SOFSEM 2004, LNCS, vol. 2932, 2004, pp 249256 


3] L. Specia and E. Motta, Integrating folksonomies with the semantic web, in Proceedings of the 4th European Semantic Web Conference \(ESWC 2007 Innsbruck, Austria, vol. 4519, June 2007, pp. 624639 4] P. Mika, Ontologies are us: A uni?ed model of social networks and semantics, in Proceedings of the 4th International Semantic Web Conference \(ISWC2005 3729, Galway, Ireland, June 2005, pp. 522536 5] P. Schmitz, Inducing ontology from ?ickr tags, in Proceedings of the Workshop on Collaborative Tagging \(WWW 2006 Edinburgh, Scotland, May 2006 6] M. Zhou, S. Bao, X. Wu, and Y. Yu, An unsupervised model for exploring hierarchical semantics from social annotations, in Proceedings of the 6th International Semantic Web Conference and 2nd Asian Semantic Web Conference ISWC/ASWC2007 Korea, vol. 4825, November 2006, pp. 673686 7] C. Schmitz, A. Hotho, R. Jaschke, and G. Stumme, Mining association rules in folksonomies, in Proceedings of the 10th IFCS Conference \(IFCS 2006 2006, pp. 261270 8] A. Hotho, A. Maedche, S. Staab, and V. Zacharias, On knowledgeable unsupervised text mining, in Proceedings of Text Mining Workshop, Physica-Verlag, 2003, pp. 131152 9] A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme, Information retrieval in folksonomies: Search and ranking, in The Semantic Web: Research and Applications, vol. 4011 Springer, 2006, pp. 411426 10] F. Lehmann and R. Wille, A triadic approach to formal concept analysis, in Proceedings of the 3rd International Conference on Conceptual Structures: Applications, Implementation and Theory. Springer-Verlag, 1995, pp. 3243 11] R. Jaschke, A. Hotho, C. Schmitz, B. Ganter, and G.Stumme Discovering shared conceptualizations in folksonomies Web Semantics: Science, Services and Agents on the World Wide Web, vol. 6, pp. 3853, 2008 12] A. Mathes, Folksonomies - cooperative classi?cation and communication through shared metadata, Graduate School of Library and Information Science, University of Illinois Urbana-Champaign, Tech. Rep. LIS590CMC, December 2004 13] H. Lin, J. Davis, and Y. Zhou, An integrated approach 


to extracting ontological structures from folksonomies, in Proceedings of the 6th European Semantic Web Conference ESWC 2009 vol. 5554, 2009, pp. 654668 14] M. Szomszor, H. Alani, K. OHara, and N. Shadbolt, Semantic modelling of user interests based on cross-folksonomy, in Proceedings of the 7th International Semantic Web Conference \(ISWC 2008 15] G.Begelman, P. Keller, and F.Smadja, Automated tag clustering: Improving search and exploration in the tag space, in Proceedings of the the Collaborative Web Tagging Workshop WWW 2006 16] R. Jaschke, A. Hotho, C. Schmitz, B. Ganter, and G. Stumme TRIAS - an algorithm for mining iceberg tri-lattices, in Procedings of the 6th IEEE International Conference on Data Mining, \(ICDM 2006 2006, pp. 907911 17] C. Borgelt, Ef?cient implementation of APRIORI and ECLAT, in FIMI, COEUR Workshop Proceedings, COEURWS.org, vol. 126, 2003 18] J. Tang, H. Leung, Q. Luo, D. Chen, and J. Gong, Towards ontology learning from folksonomies, in Proceedings of the 21st international jont conference on Arti?cal intelligence IJCAI 2009 20892094 19] L. Ding, T. Finin, A. Joshi, R. Pan, R. Cost, Y. Peng P. Reddivari, V. Doshi, and J. Sachs, Swoogle: A search and metadata engine for the semantic web, in Proceedings of the 13th ACM Conference on Information and Knowledge Management, ACM Press, 2004, pp. 652659 20] A. Hliaoutakis, G. Varelas, E. Voutsakis, E. Petrakis, and E. E Milios, Information retrieval by semantic similarity, International Journal on Semantic Web and Information Systems IJSWIS 21] G. Pirro, M. Ruffolo, and D. Talia, Secco: On building semantic links in peer to peer networks, Journal on Data Semantics XII, LNCS 5480, pp. 136, 2009 22] C. Meilicke, H. Stuckenschmidt, and A. Tamilin, Repairing ontology mappings, in Proceedings of the International Conference AAAI 2007, Vancouver, British Columbia, Canada 2007, pp. 14081413 23] S. Ravi and M. Rada, Unsupervised graph-based word sense 


disambiguation using measures of word semantic similarity in Proceedings of the International Conference ICSC 2007 Irvine, California, USA, 2007 24] H. G. A. Budanitsky, Semantic distance in wordnet: an experimental application oriented evaluation of ?ve measures in Proceedings of the International Conference NACCL 2001 Pittsburgh, Pennsylvania, USA, 2007, pp. 2934 25] J. Jiang and D. Conrath, Semantic similarity based on corpus statistics and lexical taxonomy, in Proceedings of the International Conference ROCLING X, 1997 379 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


