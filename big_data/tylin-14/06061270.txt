A Hybrid Framework for Building a Web-Page Recommender System  Vasileios Anastopoulos   Panagiotis Karampelas  Panagiotis Kalagiakos  Reda Alhajj     Hellenic American University NH, USA   Dept. of Computer Science University of Calgary Canada  Abstract  Recommender systems aim to facilitate World Wide Web users against information and product overloading. They are usually intermediate programs that try to predict users preferences and items of their interest. In this paper, we present 
a hybrid framework that uses open source information such as web logs in combination with social network analysis and data mining, to extract useful information about users browsing patterns and construct a recommendation engine. A case study based on real data from an organization of 250 employees is presented and a system prototype is constructed based on the results  Keywords-component; recommender system; social network; data mining; association rules; system prototype; hybrid framework  I   I NTRODUCTION  It is widely accepted that World Wide Web has become the 
main source of information for practically anything someone would like to know. This abundance of information, together with the advertisements and in combination with the various hyperlinks inherent to the webpages, has increased the information that is irrelevant to a user's interest on a topic Thus, finding information has changed into a time consuming and sometimes disturbing task for both novice and experienced users. To facilitate users in this quest, recommendation systems are implemented as an intermediate service between the user and the provider of the information, aiming to predict the user’s preferences and recommend items of his/her interest Recommendation systems usually draw data from the 
existing users of a website or service. These data are usually recorded into the log files of web and proxy servers, resulting into data sources that contain record entries that store the time and the URL of the web pages accessed by each user of the network. From these data, valuable information can be extracted about each user’s preferences, increasing the accuracy and quality of the recommendations presented This paper  proposes a hybrid framework that uses web logs as data source and combines social network analysis with data mining, to increase the accuracy and the quality of web page recommender systems. The work presented is divided into the following four main phases, a\ data collection, b\ social 
network analysis, c\ data mining, and d\ implementation of the recommender system. Data is collected from various sources which may be open or proprietary and can provide information about the time and the URL of the web pages a user has requested. These web logs are pre-processed to remove unnecessary or private data and integrate them into one data set that contains only the necessary information in the appropriate format for the social network analysis. The data set is then represented as social networks of users and web pages, and various measurements are performed and analyzed to conclude on the networks stability and the importance of certain actors 
The data set is then expanded and the same process is repeated until a sufficient amount of data has been analyzed. The framework continues with the data mining process that again pre-processes the web logs to transform them to the appropriate format for the application of data mining algorithms. The frequent item-sets mining results into frequent patterns of web pages access and are used as input for the association rules mining; this consequently results in a set of associations between web pages, based on which recommendations will be made by the recommender system. The construction of the recommender system starts with the preparation of the grouping of rules and the selection of the more accurate ones 
Finally, the recommender system is implemented focusing on the recommendation engine which is a system prototype that runs completely on the client's browser, differentiating from the usual approach that places the recommendation engine at a web or proxy server The remainder of the paper is organized as follows. Section 2 provides a brief overview of related work on web log preprocess approaches of mining web page navigational patterns and the types of recommender systems that are common in literature. Section 3 describes in detail the proposed framework, the problems and methods of the data preprocess the measurements and algorithms that are applied, as well as the process of constructing the recommendation engine 
Following, in Section 4 the framework is applied on real usage data from an organization of 250 employees and results in a system prototype that is implemented as an extension of the Google Chrome web browser. The paper concludes in Section 5 with the future work that could improve the performance and accuracy of the recommender system II  RELATED  WORK The most important problem in collecting reliable usage data is caching, either from the users’ browsers or proxy servers. This process is necessary when the objective is to 
2011 European Intelligence and Security Informatics Conference 978-0-7695-4406-9/11 $26.00 © 2011 IEEE DOI 10.1109/EISIC.2011.40 385 


minimize the traffic over the network and increase performance. As a result, web server logs do not include requests that were satisfied by locally cached web pages, or in the case of proxy server intermediation, all requests have the same identifier even though they correspond to various users Cooley, Mobasher and Srivastava f ron t ed t h i s probl e m  with   preprocessing of the logs, user identification and session identification Preprocessing of the logged data is necessary to remove records that are not actually re levant to the user browsing behavior. These records are HTTP requests that are implicitly made from the browser in order to complete the display of the web page. As the HTTP protocol requires each request to be a separate session, a record log is created for each one. Common solution to this problem which was also employed in the proposed framework is to remove requests based on suffixes i.e jpg, jpeg, gif, cgi etc. Depending on the information of interest, the list of suffixes to be removed can vary User identification is another important task since requests of different users are logged in the proxy server as being made from the same IP address. This task is more complicated and the proposed methods in the literature rely on the cooperation of the user or on heuristics h e u s er s  cooperation is usually achieved by requiring login to a web site first which tracks the usage. Accepting cookies from a server is another form of user cooperation, as the user's browser will send the cookie with each new request, and thus by identifying the cookie the web server actually can identify the user. There are however serious drawbacks in this approach since the user may delete stored cookies or be negative to registrations, as privacy is in most cases of primary concern. Heuristics are mostly based on the assumption that different operating systems or web browsers at the same IP address indicate different users, but  two users with the same IP address that use the same browser on the same operating system can be easily be regarded as a single user. Another heuristic method is to combine the web log with the site topology. A web page that is not reachable through the links of the web pages already accessed by the user can be assumed that was requested by another user having the same IP Session identification is usually applied in usage logs that cover long periods of time, since a user may visit the same web page more than once during this period. Each time the user accesses the web site it is considered a new session and the aim of the method, is to divide the web pages the user has accessed to separate sessions. A common approach is to define a timeout, which in literature varies from ten minutes to two hours, after which it is assumed that the user starts a new session Coming to the construction of collaborative recommender systems, the major approaches in literature are the use of memory-based and model-based algorithms m o ry based algorithms use the entire data set that corresponds to the items each user accessed and is represented as a user-item matrix. In order to generate recommendations the k-nearestneighborhood or association rules algorithms can be applied In k-nearest-neighborhood, the similarity between users in item rating or accessed web pages is calculated in the useritem matrix and similar users form a proximity-based neighborhood. It is assumed that the items accessed by the user's neighbors will probably interest him/her and thus they are recommended Association rules are usually applied to “market basket data, meaning that each user transaction has an ID and the items accessed. Usually they are mined using the Apriori or FP-growth algorithms. These algorithms initially generate frequent item-sets, which are patterns that appear often in the transactions and then associations between these sets of items are derive s s o ciatio n ru les are i n terpreted as  if a user accessed items A, then he will probably access items B A=>B The strength of these rules is evaluated by their support and confidence Association rules are used in the proposed framework to create recommendation, so they are presented in detail to the following sections The model-based collaborative filtering approach, aims to derive a model from the rating data that will be used in continuance to generate recommendations. This is achieved applying machine learning algorithms, such as neural networks, Bayesian networks, clustering and latent semantic analysis [4 o f th e s e is  co v e red in d e tail i n d a ta m i n i n g  literature and they are not discussed in this work since they are not related to the proposed framework In recent literature, social network analysis algorithms are also combined with data mining, representing the collaborative relationships as social networks. These networks are analyzed in order to understand the relationships, between users and items of their interest, the collaboration among users, how they change in time and their reflections on their preferences a s e d on s o cial n e t w ork a n al y s i s n e w  f r ie nds  or new professional contacts can be recommended. Merging social network analysis and data mining has proved to increase the quality of the data and as a result the efficiency of the recommender systems The proposed framework benefits from the available web log analysis methods to prepare the data for social network analysis and data mining. Social network analysis facilitates the identification of important web pages and users representing the user usage as a network of interaction between the users and the web pages. For the generation of recommendations the memory-based approach of association rules mining is employed, as it can be applied efficiently on large data sets and the quality of the recommendations can be easily evaluated. The combination of different methods or approaches increases the complexity of the construction process, but significantly benefits from the advantages of each one which is the main objective of the proposed framework Social network analysis is usually performed on networks where people are the actors and by finding friends communities or similarities among them, enhances the quality of the recommendations. The proposed framework differentiates from this approach, choosing web pages as actors of the social network. The users are also part of the approach as through their usage behavior the web pages are implicitly connected forming social networks. The users influence web pages networks adding the dynamic and evolutionary features 
386 


to these networks. The information extracted from the analysis of both users and web pages social networks leads to useful conclusions that are used to increase the quality of the recommendation engine III  PROPOSED  FRAMEWORK A  Data Collection and Preprocess The required data to work with the proposed framework are the web pages that have been accessed by each user. These can be collected using the SNMP protocol, applications that monitor the traffic on a network such as tcpdump, argus, mrtg ethereal and other packages or logged information from web and proxy servers. Initially, a data set is collected covering a small period of time, e.g., a few hours, and consequently this data set is extended to several days of network traffic Data preprocess is a necessary process to improve the quality of the data and as a consequence the results of link analysis and data mining [3 p u t is th e p r ev io u s l y  mentioned data and the output is data sets containing only the necessary data for the link and data analysis processes. The forms of data preprocess can be summarized to data cleaning data integration, data transformation and data reduction. Data cleaning attempts to correct incomplete, noisy and inconsistent data. Data integration merges data from various sources, since network traffic can be recorded into flat files and database tables, while transformation brings them to the appropriate format for analysis and mining to be performed by the software tools. The data is then reduced, replacing each web page with a numeric value, making it is easier to handle and less demanding in storage The main problem with web logs is that both proxy servers and browsers cache web pages and that web browsers automatically request only new content in order to complete the display of a web page. This results in difficulties to identify the user and its behavior, in addition to the HTTP protocol that requires a separate connection for every requested file. There are processes available to overcome these problems at m i ng to us er an d s e s s ion  iden ti f i catio n  an d including user path completion and formatting. The preprocessing to be performed depends on the actual data that are considered adeq uate in order to identify the user's behavior. If, for example, a user is accessing paintings on a web site, the jpg or gif requests should not be removed but in all other cases they would be removed as automatic requests of the browser to complete the display of the web page  B Social Network Construction and Analysis The social networks are composed of nodes and links These nodes relate with other nodes through their links. The links can have a direction; link from node A to B is different from node B to A. A 2-mode network is represented with an incidence matrix, where a value indicates the presence of a link, a number if weighted or 1 if binary. This 2-mode network can then be folded to create two 1-mode networks, one for each dimension. To fold a network, it is first transposed to the desired dimension and then multiplied with the initial incidence matrix, resulting to the adjacency matrix In our framework, the construction of the 2-mode network begins with a small data set, a few hours of network traffic These relational data are used to create the incidence matrix which is a |V| by |E| array. The |V| array is the host IP addresses and the |E| array the web pages requested, when a host i requests a web page j the weight of the link is added to cell ij The two-dimension incidence matrices will be folded into both dimensions to form two 1-mode networks, with the respective adjacency matrices. The result of the |V| x |E  folding  will be the V x V and E x E arrays, where each cell contains the weight between v i  and v j   e i  and e j respectively The analysis of the two 1-mode networks aims to the identification of important nodes in each network, meaning important users and web sites. To measure importance the degree, closeness, eigenvector and betweenness centrality will be measured The degree centrality is the number of links that a node has and is distinguished into in an out degree, when the links are directed to or from the node, respectively. In our case, the constructed network is undirected, so there is no need to distinguish the in from the out degree; the total degree centrality of the nodes will be measured. Let G=\(V,E\e the graph representation of a square network and a node v The Total Degree Centrality of node v deg / 2  V|-1\here deg  card  u 002 V  v  u  002 E 001  u  v  002 E  n ode w i t h h i gh degree centrality is well connected node and can potentially directly influence many other nodes  Closeness centrality is the average geodesic distance of a node from all other nodes in the network, where geodesic distance is the length of the shortest path between two nodes Let G=\(V,E\ be the graph representation of a square network then the closeness centrality of a node v 002 V is v V|-1\st where dist 002\010 d G   v  i  i 002 V if every node is reachable from v  and v V| if some node is not reachable  from v  cited in   T h e clos es t a n ode is to oth e rs th e f a s t e s t it s acces s  to information and influence to others [8 Another measurement that is used to identify important nodes is betweenness centrality which is defined for a node v, as the percentage of shortest paths, between node pairs, that pass through v. Let G=\(V,E\he graph representation of a symmetric network. Let n=|V| and a node v 002 V. For \(u,w  002 VxV, let n G   u  w   be the number of geodesics in G from u to w. If \(u,w 002 E, then set n G   u  w Now, let S  u  w  VxV  d G   u  w  d  u  v  d G   v  w d let between 002\010 n G u,v  n G v ,w G  u ,w 002 S then n-2\/2  as ci t e d i n 7 A node w i t h  h i gh bet w eenn e s s i s  important because it connects many nodes and a possible removal would affect the network The last node level measurement that is applied is the eigenvector centrality It is a measure of the node's connections with other highly connected nodes. It calculates the eigenvector of the largest positive eigenvalue of the adjacency matrix representation of the square network. To 
387 


compute the eigenvalues and vectors a Jacobi method is used 9 in 7   T h e n o d e s w ith  h i g h eig e n v ec to r  centrality can mobilize other important nodes Apart from the node level measurements it is important to analyze the networks, from a network level perspective. The measurements that are applied in the proposed framework are density, fragmentation, component count, isolate count. These measurements are very useful as they describe the network as a whol h i c h i n  combination with the node level measurements provide us comprehensive information about the networks' cohesion Fragmentation measures the proportion of the network's nodes that are disconnected. Let an undirected network G=\(V,E\ith n = |V| and s k be the number of nodes in the k th  component of G, 1 000 k 000 n , then Fragmentation = 1 000  000 s k s k  000 1\\(n 000  cited in 7  Density is the ration of the number of links, existing on a network, versus the maximum possible ones. For a network with adjacency matrix M and dimensions m x n, the density is m  m-1\ the network is unimodal and Density = sum\(M\/\(m  n\, if the network is bimodal Density = sum\(M\\(m  n ci t e d i n   Following the identification of important nodes in the networks, the proposed framework continues with the removal of the top valued ones. The nodes are sorted with descending order and we start removing the top nodes one-by-one repeating the measurements after each removal. This process is repeated for each measurement and it aims to observe how the network is affected from the removal of each node. The removal of a node is an exogenous impact, a shock for the network, whose results to the network's dynamics and cohesion are observed. A network may remain stable the links between the nodes do not change, or mutate initiate an evolutionary process or i t s coh e s i on  m a y c h a n g e  increasing the network's fragmentation and components  The next phase in the proposed framework is to extend the data collection to the period of one day. This data set is divided to characteristic time periods and is further analyzed repeating the measurements previously presented The data set is divided to twenty-four subsets, one for each hour of the day. The node and network level measurements are calculated for each network and the results are compared in the dimension of time to identify its dynamics, whether it is stable or not, whether it remains coherent or not. Drastic changes are obvious in social networks, whilst small are difficult to detect. This makes CUSUM charts suitable for social networks, as they perform well in detecting small changes over time and also provide detection of the point the change occurred [10  T h e C U SUM control chart compares sequentially the statistic C t  against a decision interval until Ct  A Since one is not interested in concluding that the network process is unchanged, the cumulative statistic is Ct  max{0,Z t k + C t-1  EWMA control chart and moving window analysis are also applicable but in the proposed framework only CUSUM control charts are generated The analysis of a network on the time domain, may lead to erroneous conclusions because of periodicity. For example a company meeting scheduled to take place at a specific day and time, every week, could mislead the analysis to identify a shock at that network. In the proposed framework the process to identify and handle periodicity analyzed in u s e d   applying spectral analysis to the network's data on the dimension of time In continuance, the same data set is divided into two subsets, working hours and not-working hours, and the same analysis is applied. Finally, the same process is followed after dividing the data into four subsets, the working hours split into two and not-working hours split into two subsets. To conclude, the data collected are extended to six days and the same procedure is repeated. The output of this analysis helps us determine the cohesion and stability of the data. Patterns or specific time periods might need to be taken into account while mining for the association rules  C. Data Mining  In this section data mining techniques are applied to the dataset. Depending on the results of the social network analysis, on the network's stability or patterns of usage, the logs can be divided by day or by specific time periods and the data mining process is repeated to each one of these subsets The process starts with the preprocessing of the data continues with frequent item sets and association rules mining algorithms The transactional data, as previously prepared and used so far are in a multi-instance format as shown in Table I \(a\In order to apply the data mining algorithms they need to be transformed to the single-instance format of Table I \(b  TABLE I: Multi - Instance \(a\ vs Single - Instance \(b\ Transactional Data  IP Webpages  IP Webpages IP B 40  IP B 40 IP C 138  IP C 138, 139, 140 IP C 139  IP D 138, 139, 140 IP C 140  IP E 1 IP  D 138  b IP D 139   IP D 140   IP E 1   a   An itemset in our case is a set of web pages. An itemset containing k items is a k itemset, which has a support count that equals to the number of its occurrences in the transactions data set. When an itemset satisfies a minimum support count threshold, it is a frequent itemset, denoted by L k The mining of the frequent itemsets can be performed applying Apriori or FP-Growth algorithms. Apriori is a seminal algorithm that scans the data set to find frequent 1-itemsets and then joins them to generate candidate 2-itemsets. These candidate 
388 


itemesets are evaluated scanning again the data set and the iterations continue finding \(k+1\-itemsets from previously known k-itemsets. Its drawbacks are that it may generate a huge amount of candidate sets and the repeated scan of the database, which is a costly transaction [3      In the proposed framework, the FP-Growth algorithm is preferred, as it is faster than Apriori and is suitable for large data sets. The algorithm applies a divide-and-conquer approach and consists of two steps, the FP-tree construction and the mining of the frequent itemset. To construct the FPtree, a “null” root node is created and then the data set is scanned to obtain a list of frequent items. These are ordered in descending order based on their support. Using this order, the items in each transaction of the data set are reordered, while each node n in the FP-tree represents a unique itemset X. All the nodes have a counter indicating the transactions that share the node, except for the root node. The algorithm scans the items in each transaction, it searches the already existing nodes in FP-tree and if a representative node exists the counter of the node is incremented by 1, else, a new node is created The support of each item is stored in a header table, while the same table is used for each item to point to its occurrences in the tree. This way the problem of mining large data sets for frequent patterns, has transformed to the mining of the FPtree. The FP-tree mining starts from each frequent pattern of length-1 \(initial suffix\nstructing its conditional pattern base \(the set of prefix paths in the FP-tree co-occurring with the suffix pattern\hen constructs its conditional FP-tree and mines recursively this tree. The pattern growth is achieved concatenating the suffix pattern with the frequent patterns generated from the conditional FP-tree [3  The above algorithms provide us with a set of frequent itemsets, which will be used as input for the association rules mining algorithm. An association rule is an expression A 000 B  where A and B are itemsets, of web pages in our case, and it is translated in if a host requested the web pages of A, then he will also request the web pages of B itemset The support of rule is support\(A 000 B\=support\(A 001 B and  it is the percentage of the transactions that this association rule appears, while the confidence of the association rule confidence \(A 000 B support\(A 001 B\/support\(A is again a percentage that indicates the conditional probability that a transaction containing A will also contain B The higher the support count and confidence is, the stronger the rule is. Both have to satisfy the thresholds that are set previously from the analys The association rules are then subject to correlation analysis, since rules with high support and confidence may sometimes be misleading. In the proposed framework the lift correlation measure is measured for each of the resulted association rules. Let an association rule A 000 B the lift corresponds to lift\(A 000 B\ = P\(B|A\/P\(B or lift\(A 000 B confidence\(A 000 B\/sup\(B The numerator is the likelihood of a host requesting both, while the denominator is what the likelihood would have been if the two visits were completely independent. Values greater than one indicate positive correlation between the two itemsets, values less than one indicate negative correlation and values equal to one indicate independence of A and B 3 Co rrelatio n an al y s i s  w ill o u t p u t A 000 B  support, confidence. Lift  strong association rules to be used for the recommendation engine  D Recommender System Construction  In this section the association rules which were produced from the data mining process, are used for the construction of the recommender system. The association rules whose Lift is negative or 1, are discarded as they indicate negative correlation or independence of the itemsets, respectively From those with positive Lift value, some will be selected based on their support or confidence High support indicates that the rule appears often in the transactions data set, thus high confidence indicates increased probability for the consequence of the rule to appear together with the antecedent. All association rules satisfy the support and confidence thresholds that were set during the mining process so they can all be used for the recommendation system, but depending on their values and their total number, the rules may be further filtered to reduce their multitude and keep the stronger ones. For example, if all rules have high confidence  then they can be sorted based on their support to choose the highest valued ones and discard the others. In this framework we selected the rules with high confidence since the aim is to provide accurate recommendations to the user The set of association rules that will be finally used for the recommendation engine is then grouped based on the number of items in their antecedent. The result is a group of rules with one item in the antecedent, a group with two items and so on. The recommendation system captures the user's request, searches the one-item antecedent for a match and, if found, recommends the items in the consequence. When two web pages accessed by the user are known, it searches the one-item group for the second web page and in continuance searches the two-items group for both consequent requests When no match is found the system erases the user's browsing history and starts tracking it again when a match is found Following, the recommender system can be implemented either to run on a web or proxy server, or on the client. In the first approach, a database server is also needed to store the user IP and the respective web pages accessed as well as the association rules. In the second approach which was chosen for the system prototype, there is no need to track the different users, but the recommendation engine and the association rules need to be installed on each host IV  SYSTEM  PROTOTYPE The proposed framework was applied to real data collected through the log of a Microsoft ISA server used for Internet access from an organization of 250 employees The recommender system was implemented as an extension for the Google Chrome web browser, using JavaScript and the Chrome API. The extension's architecture is depicted in Figure 1 and has four main components 
389 


  Figure 1: Extension’s Architecture 1  manifest.json: contains the necessary information for the installation of the extension 2  contentscript.js: gets lo aded when a new browser window or tab opens and gets executed each time a request is performed 3  backgroundpage.html: runs in the background and implements the recommendation engine 4  popup.html: pops up when the user clicks on the extension's icon and displays the recommendations The above components can be installed in a single directory and then a browser action icon is displayed. When a user opens the web browser the contentscript.js is loaded and executed each time a URL is requested. It sends a message to the background.html page calling a Chrome API function. The background.html page has a listener that waits for messages from the content script. When the message is received the listener calls a JavaScript function that implements the recommendation algorithm. It then captures the requested URL and searches in antecedent of the rule, and if the URL is found then it recommends the consequent of the association rule. This process runs in the background and when the user wants to present the recommendations, he/she just clicks on the browser action icon causing the popup.html to pop up and present the recommended URLs, Figure 2. Choosing one of the recommended URLs opens it in a new tab Figure 2: Snapshot of the Pop-up Page The application runs only in the user's browser and monitors only the user’s usage data, so there is no need to distinguish between different users as in most recommender systems. The association rules are stored in a JavaScript file rules.js which can be easily replaced with an updated one. In this file, two JavaScript arrays are used to store the antecedence l d consequence right the association rules. The recommendation engine searches the left array for a match and if found, recommends the corresponding value of the right array. The whole application is of small size, consumes the minimum resources and can be easily distributed and managed V  CONCLUSION A hybrid method for the construction of a web page recommender system was presented that combines social network analysis and data mining to open source web usage data and results in the construction of a system prototype. The process started with the collection of the data which were then preprocessed and represented as social networks linking users and web pages. The analysis of the networks led to the identification of the critical users and web pages. This was achieved by separating the data to specific time periods and by analyzing and comparing various combinations of these data sets in the dimension of time. Data mining algorithms were then applied in continuance to mine association rules that were used for the recommendation engine; additionally, correlation analysis was performed to verify the strength of the rules Future work may include the extension of the framework to include content-based filtering, explicit ratings from the users and classification of the users according to their usage behavior and preferences. The Chrome extension could also be distributed and tested from a larger audience of users in order for the system to be further evaluated. The social networks constructed by the users and by the web sites could also be further analyzed to identify how they influence and affect each other. A step further at the association rules preparation could be taken, clustering the rules and ranking them based on the confidence of the recommendation, correlating the groups of association rules. The rules’ clustering increases the search performance, which could be further examined in combination with the systems scalability R EFERENCES  1  R. Cooley, B. Mobasher, and J. Srivastava, “Data preparation for mining World Wide Web browsing patterns Journal of Knowledge and Information Systems 1\, 1999 2  E. Vozalis and K. Margaritis, “Analysis of Recommender Systems Algorithms The 6th Hellenic European Conference on Computer Mathematics & its Applications \(HERCMA Athens, Greece, 2003 3  J. Han and M. Kamber, “Data Mining: Concepts and Techniques”, 2 nd  Edition, pages 227-266, 2007 4  G. Xu, Y. Zhang and L. Li, “Web Mining and Social Networking Techniques and Applications”,  Springer, 1st Edition, 2010 5  S. P. Borgatti, “The Key Player Problem”, In Dynamic Social Network Modeling and Analysis, R. Breiger, K. Carley, & P. Pattison, \(Eds National Academy of Sciences Press, pp. 241-252, 2003 6  L.C. Freeman, Centrality in Social Networks I: Conceptual Clarification Social Networks, 1, 215-239, 1979 7  K. M. Carley, J. Reminga, J. Storrick, and D. Columbus, “ORA User’s Guide”, June 2010, 2010 CMU-ISR-10-120 8  T. L. Frantz, “Annual Tools/Computational Approaches/Methods Conference”, March 19, 2008, Carnegie Mellon University 9  P. Bonacich, “Power and centrality: A family of measures”, American Journal of Sociology 92: 1170-1182, 1987   I. McCulloh, “Detecting Changes in a Dynamic Social Network”, March 31, 2009 CMU-ISR-09-104   S. P. Borgatti, “Identifying sets of key players in a network Computational, Math. & Org. Theory, 12\(1\-34, 2006   S. Wasserman and K. Faust. Social Network Analysis: Methods and Applications. Cambridge: Cambridge University Press, 1994 
390 


V F UTURE W ORK In the future two main issues should be addressed First we would like to investigate approaches to improve the quality of the output in particular in the case of incomplete or noisy data Second we want to improve on memory and space requirements A Improving the Quality of the Output One should not forget that the algorithms that are used here are mathematically proven to yield a sound and complete base of minimal cardinality for the dataset provided That means provided that the data is complete and error-free there is no room for improvement in terms of quality In practice we are unlikely to ever encounter perfect data Hence the question is how can the effects of poor data be mitigated In Section IV-C1 we have observed the two different types of in\003uence of incomplete data and faulty data In association rule mining these two problems are usually dealt with by introducing the measures of support and con\002dence  A nai v e translation of the support and con\002dence to the terminology of EL GCIs could look like this supp A v B   j  C u D  I j 001 I conf C v D   j  C u D  I j j C I j 10 The idea is that by searching only for GCIs with a certain minimal support one can mitigate the effects of incomplete data At the same time imposing a minimum on the con\002dence can limit the effects of noise in the data Several obstacles need to be overcome to make this approach work First we have argued that the large number of possible GCIs requires a compact representation e.g a base Now if A v B and B v C both have a con\002dence greater than 0.9 the GCI A v C  even though it follows from the 002rst two may have a con\002dence as low as 0.81 Hence imposing a lower limit on the con\002dence results in a set of GCIs that is no longer closed with respect to semantic deductions In such a situation computing a base is meaningless An obvious way to mend this is to consider both GCIs with suf\002cient con\002dence and support as well as all GCIs that follow semantically from them to be trustworthy This set of trustworthy GCIs has the necessary closure property but has not been researched theoretically An alternative approach is to look for other compact representations of the GCIs with suf\002cient support One might think of an analogous notion to frequent closed itemsets e.g frequent closed EL concept descriptions and Iceberg lattices We also need to address whether the naive translation of the classical notions of support and con\002dence really does what one intuitively expects it to As a toy example assume that a dataset about art is very incomplete containing only one Museum  the Louvre  but thousands of pieces of art that belong to the Louvre cf Figure 3 Clearly the GCI Museum v 9 hasLocation  FrenchCity  Mona Lisa Venus de Milo Louvre Paris Museum FrenchCity hasLocation ownedBy ownedBy    Fig 3 Classical Support is Counterintuitive which states that all museums are in a French city is no more trustworthy than the GCI 9 ownedBy  Museum v 9 ownedBy   9 hasLocation  FrenchCity   which states that something that is owned by a museum is owned by a museum in a French city While the 002rst GCI has very small support there is only one museum in the dataset the latter has very large support in the dataset there are thousands of pieces of art belonging to a museum Whether other notions of support are less counterintuitive needs to be investigated B Speeding Up the Mining Process In its current state the runtime and the memory requirements of our implementation leave plenty of room for improvement While it is unlikely that the bottleneck with respect to runtime that is presented by Next-Closure can be avoided completely there are some improvements that can be made There is hope that the introduction of a concept constructor  describing the empty concept will signi\002cantly improve memory requirements by making the All description redundant All is a very large concept description and since it occurs quite frequently a large amount of memory is quickly 002lled with various instances of All  Another rather technical improvement would be to avoid computing large intermediate results This is mostly due to computing large description graphs which then turn out to be highly redundant It would be more desirable to directly compute a smaller part of this description graph which is suf\002cient for our purposes Improvements in this direction would not only decrease the memory requirements for the implementation but also the overall runtime because extra computation for reducing description graphs could be avoided As a rather drastic measure one could impose an upper bound on the role depth of the EL descriptions This would both remove the need for cyclic concept descriptions and eliminate concept descriptions that consume large amounts of memory Our experiments provide anecdotal evidence that GCIs with larger role depths are usually among the least interesting C Quality Assessment The computed GCIs are sound and complete for the given data set However that neither means that they are correct 
1089 


in the domain nor that they are complete therein Therefore the results computed should merely be regarded as a starting point for constructing of a knowledge base Whether the quality of the GCIs that are produced is suf\002cient in practical applications should be examined in a case study in cooperation with domain experts VI C ONCLUSION We have prototypically implemented an algorithm that is able to mine a small set of EL GCIs from a dataset We have tested this algorithm on two real world datasets The results at least for the relatively error-free dataset of the DrugBank appear promising However for a 002nal verdict a quality assessment with domain experts will be needed We have also observed and discussed the implications of incomplete and faulty data as they appeared in the DBpedia dataset To the purpose of improving the quality of the GCIs when mining from imperfect data we have suggested to look into measures similar to support and con\002dence which are known from association rule mining R EFERENCES   F Baader D Calvanese D McGuinness D Nardi and P F PatelSchneider Eds The Description Logic Handbook Theory Implementation and Applications  Cambridge University Press 2003   R Agrawal T Imielinski and A Swami Mining association rules between sets of items in large databases in Proceedings of the ACM SIGMOD International Conference on Management of Data  May 1993 pp 207–216   F Distel Learning description logic knowledge bases from data using methods from formal concept analysis Ph.D dissertation TU Dresden Dresden Germany June 2011   F Baader and F Distel Exploring 002nite models in the Description Logic EL gfp  in Proc of the 7th Int Conf on Formal Concept Analysis ICFCA 2009  S Ferr  e and S Rudolph Eds Springer 2009   S Auer C Bizer G Kobilarov J Lehmann and Z Ives Dbpedia A nucleus for a web of open data in In 6th Intl Semantic Web Conference Busan Korea  Springer 2007 pp 11–15   D2R server publishing the drugbank database FU Berlin Available http://www4.wiwiss.fu-berlin.de/drugbank   the data hub Comprehensive Knowledge Archive Network Available http://ckan.net   K Spackman K Campbell and R Cote SNOMED RT A reference terminology for health care pp 640–644 1997 fall Symposium Supplement   M Ashburner C A Ball J A Blake D Botstein H Butler J M Cherry A P Davis K Dolinski S S Dwight J T Eppig M A Harris D P Hill L Issel-Tarver A Kasarskis S Lewis J C Matese J E Richardson M Ringwald G M Rubin and G Sherlock Gene Ontology Tool for the uni\002cation of biology Nature Genetics  vol 25 no 1 pp 25–29 2000   B Motik B C Grau I Horrocks Z Wu A Fokoue and C Lutz Owl 2 web ontology language Pro\002les W3C Recommendation October 2009   F Baader B Ganter U Sattler and B Sertkaya Completing Description Logic knowledge bases using Formal Concept Analysis in IJCAI07  2007   S Rudolph Relational exploration  combining Description Logics and formal concept analysis for knowledge speci\002cation Ph.D dissertation Technische Universit  at Dresden 2006   B Ganter and R Wille Formal Concept Analysis Mathematical Foundations  New York Springer 1997   F Distel Hardness of enumerating pseudo-intents in the lectic order in Proc of the 8th Int Conf on Formal Concept Analysis ICFCA 2010  ser Lecture Notes in Arti\002cial Intelligence B Sertkaya and L Kwuida Eds vol 5986 Springer 2010 pp 124–137   D Borchmann Implementing the exploration algorithm for ELgfp TU Dresden Tech Rep 2011   D2R server publishing the diseasome dataset FU Berlin Available http://www4.wiwiss.fu-berlin.de/diseasome   G Stumme R Taouil Y Bastide N Pasquier and L Lakhal Computing iceberg concept lattices with TITANIC Data Knowl Eng  vol 42 no 2 pp 189–222 2002 
1090 


incremental option for a data mining algorithm is of course preferable in an inductive database system, since it allows the exploitation of all the available information in the system in order to speed up the response time The better performance of incremental algorithm depicted in the result section  worked on problems with item and context dependent constraint present a solution for item extraction from frequently updated database. This is done by running first the  Proceedings of the International Conference on Communication and Computational Intelligence 2010  523  mining algorithm of our choice \(on the problem defined by the query but without the context dependent constraints then applying the incremental algorithm on top of it \(with the addition of context dependent constraints whenever the mining constraints select a very small part of the original dataset, proposed incremental update strategy is likely to be very fast and efficient REFERENCES 1]  G. Grahne and J. Zhu, Mining Frequent Itemsets from Secondary Memory, IEEE Intl Conf. Data Mining \(ICDM 04 2]   J. Han, J. Pei, and Y. Yin, Mining Frequent Patterns without Candidate Generation, ACM SIGMOD, 2000 3]  Y.-L. Cheung, Mining Frequent Itemsets without Support Threshold With and without Item Constraints, IEEE Trans. Knowledge and Data Eng., vol. 16, no. 9, pp. 1052-1069, Sept. 2004 4]   G. Cong and B. Liu, Speed-Up Iterative Frequent Itemset Mining with Constraint Changes, IEEE Intl Conf. Data Mining \(ICDM 02 107-114, 2002 5] C.K.-S. Leung, L.V.S. Lakshmanan, and R.T. Ng, Exploiting Succinct Constraints Using FP-Trees, SIGKDD Explorations Newsletter, vol. 4 no. 1, pp. 40-49, 2002 6]  T. Uno, M. Kiyomi, and H. Arimura, LCM ver. 2: Efficient Mining Algorithms for Frequent/Closed/Maximal Itemsets, IEEE ICDM Workshop Frequent Itemset Mining Implementations \(FIMI 7]   J. Pei, J. Han, and L.V.S. Lakshmanan, Pushing Convertible Constraints in Frequent Itemset Mining, Data Mining and Knowledge Discovery, vol. 8, no. 3, pp. 227-252, 2004 8]   M. Botta, J.-F. Boulicaut, C. Masson, and R. Meo, A Comparison 


between Query Languages for the Extraction of Association Rules 9]  E. Baralis, T. Cerquitelli, and S. Chiusano, Index Support for  Frequent Itemset Mining in a Relational DBMS, 21st Intl Conf. Data Eng ICDE 10]   G. Liu, H. Lu, W. Lou, and J.X. Yu, On Computing, Storing and Querying Frequent Patterns, Ninth ACM SIGKDD Intl Conf Knowledge Discovery and Data Mining \(SIGKDD 


frequent, and there is no equal-support pruning in its subsets. So the Bitwise And Operation on binary strings of 0001100011 and 0001100011 is employed. As the result is 0001100011, the support of the itemset {G C, E} is 4, which is equal to {G, C}. E is put into the equal-support set of {G, C}, and does not need to add a new son node. When itemset {C, A, E} is checked, all of its subsets are frequent, and none of them is in the equal-support set. The corresponding two bit strings 0001101111 and 0011101011, are operated by &. The resultant string is 0001101011. The number of 1 in string 0001101011 is 5. So the support of itemset {C, A E} is 5, which is not equal to the support of {C, A}. So a new son node is added to {C, A}. The Trie after generation\(3  5 8 8 G C A E A 4 C 4 E 6 A 6 E 6 E 7 5 E 0001101011 E  Fig. 2. Trie after generation\(3 In generation\(4 therefore, the height of the Trie does not increase. All frequent itemsets are in Fig. 2. In the last step, all frequent itemsets are written out according to the Trie  5. EXPERIMENTAL RESULTS  


The proposed algorithm is tested on all the five datasets prepared by Roberto Bayardo, from UCI and PUMSB datasets. The datasets are available at http://fimi.cs.helsinki.fi. The characteristics of the datasets are shown in Table 6. The first column contains the names of the datasets. The second column shows the number of items contained in each dataset. The third column shows the average length of each transaction and the last column indicates the total number of transactions in each dataset TABLE 6 DATABASE CHARACTERISTICS Datasets Items Avg. length Records mushroom 119 23 8,124 chess 75 37 3,196 pusmsb* 2,088 50.4 49,046 connect 129 43 65,557 pusmsb 2,113 74.0 49,046 In order to illustrate the performance of the proposed algorithm, BitApriori is compared to the fast Apriori implemented in Ferenc [20], and another similar recently published, algorithm Index-BitTableFI proposed by Song [16]. In order to show the efficiency of the pruning technology employed in BitApriori another algorithm BitAprioriNE, which is the same as BitApriori, except that it does not use the special equal-support pruning, is designed. All of the above four algorithms are implemented in C++ and compiled with Microsoft Visual C++ 6.0. The experiments are performed on a Windows XP PC equipped with a Pentium 2.0 GHz CPU and 1.5 GB of RAM memory  For each dataset, a mass of different support thresholds are tested, and the five most important of them are chosen for reporting in this paper. The experimental results are shown in Fig. 3-7. In the figures, the y-coordinate denotes the execution time \(in seconds while x-coordinate denotes the support threshold 0 100 200 300 400 


500 600 0.05 0.06 0.07 0.08 0.09 0.1 0.11 Apriori BitApriori BitAprioriNE Index-BitTableFI  Fig.3 Execution time comparison on mushroom  0 200 400 600 800 1000 0.45 0.5 0.55 0.6 0.65 0.7 0.75 Apriori BitApriori BitAprioriNE Index-BitTableFI  Fig.4 Execution time comparison on chess  0 500 1000 1500 2000 2500 3000 3500 4000 0.25 0.3 0.35 0.4 0.45 0.5 0.55 Apriori BitApriori BitAprioriNE Index-BitTableFI  Fig.5 Execution time comparison on pusmsb 0 1000 


2000 3000 4000 5000 6000 0.65 0.7 0.75 0.8 0.85 0.9 0.95 Apriori BitApriori BitAprioriNE Index-BitTableFI  Fig.6 Execution time comparison on connect  0 500 1000 1500 2000 2500 0.675 0.725 0.775 0.825 Apriori BitApriori BitAprioriNE Index-BitTableFI  Fig.7 Execution time comparison on pusmsb  As shown in Fig. 3, BitApriori outperforms all other algorithms, and the dominance is apparent. In this dataset, the special equal-support pruning works efficiently. In Fig. 4, BitAprioriNE beats Apriori and Index-BitTableFI. In Fig. 5, the effectiveness of the proposed algorithm is verified, especially when the minimum support is low. In Fig. 6, BitAprioriWE exhausts the memory when the support threshold is lower than 0.8, but BitApriori does not. That means the special equal-support pruning contributes to save the memory. In Fig. 7, when the threshold is larger than 0.725, Apriori beats the BitApriori. But BitApriori outperforms the Apriori, when the threshold is lower than 0.725  


Apriori does not use the binary string and the special equal-support pruning. The BitTable is employed in Index-BitTableFI, but there is no special equal-support pruning, except for the frequent 2-itemsets BitAprioriNE outperforms Apriori in Fig. 3-5, but not in Fig. 6-7, because of the limitation of memory BitAprioriNE does better than Index-BitTableFI in Fig 4 and Fig. 5. In the mushroom, there is a vast number of equal-support itemsets for frequent 2-itemset. So Index-BitTableFI outperforms BitAprioriNE  On one hand, the special equal-support pruning is a useful technique for improving efficiency. The performance is improved significantly, especially when the databases contain many equal-support itemsets, such as the mushroom. It also reduces memory requirement On the other hand, the technique of binary string in BitApriori improves the efficiency of Apriori. These two techniques combine perfectly in BitApriori. So BitApriori has very good performance. It is the best for all of the five datasets  6. CONCLUSIONS  In this paper, two effective techniques are employed to improve the performance of Apriori, by reducing the cost of candidate generation, and by support counting The two effective techniques are integrated perfectly in BitApriori, and improve the computational efficiency significantly. Experimental results have shown that BitApriori outperforms the fast Apriori and Index-BitTableFI, especially when the minimum support threshold is low  When the database is large, the BitApriori may suffer the problem of memory scarcity. So how to solve the memory problem will be the question addressed in one of our future works. And another work is to improve Bitwise And Operation on the binary string, or replace it by some more effective techniques  REFERENCES 


 1] Agrawal R., T. Imielinski, A. Swami, Mining association rules between sets of items in large databases, in Proceedings of the ACM SIGMOD Conference on Management of Data. pp. 207-216 1993 2] Agrawal R., R. Srikant, Fast algorithms for mining association rules, The International Conference on Very Large Databases, pp. 487-499, 1994 3] Zaki M.J., S. Parthasarathy, M. Ogihara, W. Li New algorithms for fast discovery of association rules, in Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining, pp. 283-296, 1997 4] Han J., J. Pei, Y. Yin, Mining frequent patterns without candidate generation," in Proceedings of the 2000 ACM SIGMOD international conference on Management of data, ACM Press, pp. 1-12, 2000 5] Pork J.S., M.S. Chen, P.S. Yu, An effective hash based algorithm for mining association rules, ACM SIGMOD, pp. 175-186, 1995 6] Brin S., R. Motwani, J.D. Ullman, S. Tsur Dynamic itemset counting and implication rules for market basket data, in Proceedings of the ACM SIGMOD International Conference on Management of Data, pp. 255264, 1997 7] Brin S., R. Motwani, C. Silverstein, Beyond market baskets: generalizing association rules to correlations, in Proceedings of the ACM SIGMOD International Conference on Management of Data Tuscon, Arizona, pp. 265-276, 1997 8] Toivonen H., Sampling large databases for association rules, in Proceedings of 22nd VLDB Conference, Mumbai, India, pp. 134-145, 1996 9] Savasere A., E. Omiecinski, S.B. Navathe, An efficient algorithm for mining association rules in large databases, in Proceedings of 21th International Conference on Very Large Data Bases VLDB'95 10] Tsay Y.J., J.Y. Chiang, CBAR: an efficient method for mining association rules, Knowledge Based Systems, 18 \(2-3 


11] Liu G., H. Lu, W. Lou, Y. Xu, J.X. Yu, Efficient mining of frequent patterns using ascending frequency ordered prefix-tree, Data Mining Knowledge Discovery, 9 \(3 12] Grahne G., J. Zhu, Fast algorithms for frequent itemset mining using FP-Trees, IEEE Transaction on Knowledge and Data Engineering, 17 \(10 1347-1362, 2005 13] Zaki M.J., Scalable algorithms for association mining, IEEE Transactions on Knowledge and Data Engineering, 12 \(3 14] Zaki M.J., K. Gouda, Fast Vertical Mining Using Diffsets, in Proceedings of the ACM SIGMOD International Conference on Knowledge Discovery and Data Mining, pp. 326-335, 2003 15] Dong J., M. Han, BitTableFI: an efficient mining frequent itemsets algorithm, Knowledge Based Systems, 20 \(4 16] Song W., B.R. Yang, Z.Y. Xu, Index-BitTableFI An improved algorithm for mining frequent itemsets, Knowledge Based Systems, 21 \(6 507-513, 2008 17] Ferenc B., Surprising results of trie-based FIM algorithms, IEEE ICDM Workshop on Frequent Itemset Mining Implementations \(FIMI'04 CEUR Workshop Proceedings, vol. 90, G. Bart, J.Z Mohammed, and B. Roberto, Eds, Brighton, UK 2004 18] Ferenc B., A Survey on Frequent Itemset Mining Technical Report, Budapest University of Technology and Economic, 2006 19] Bart G., Survey on Frequent Pattern Mining Manuskript, 2002 20] Ferenc B., A fast APRIORI implementation IEEE ICDM Workshop on Frequent Itemset Mining Implementations \(FIMI'03 USA, 2003  


15] R. Agrawal and R. Srikant, "Fast algorithms for mining association rules in large Databases," presented at the Proceedings of the 20th International Conference on Very Large Data Bases, 1994 16] NKUDIC. \(June, 2006, National Kidney and Urologic Diseases Information Clearinghouse:Prostate Enlargement. Available http://kidney.niddk.nih.gov/kudiseases/pubs/prostateenlargement accessed on 10/06/2010 17] M. J. ZAKI, "Mining Non-Redundant Association Rules," Data Mining and Knowledge Discovery, 2004 18] Y. Xu and Y. Li, "Mining for Useful Association Rules Using the ATMS," presented at the International Conference on Computational Intelligence for Modelling, Control and Automation, and International Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC05 544 2010 10th International Conference on Intelligent Systems Design and Applications 


8]  Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual Web search engine. Proceedings of the seventh international conference on World Wide Web 7: pp. 107-117, 1998 9]  J. Pei, J. Han, B. Mortazavi-Asl and H.Zhu, Mining Access Patterns Efficiently from Web Logs, Proceedings of the Pacific-Asia Conference on Knowledge Discovery and Data Mining, pp. 396-407 2000 10]  C. H. Cai, A. W. C. Fu, C.H. Cheng and W. W. Kwong, Mining Association Rules with Weighted Items, In Database Engineering and Applications Symposium, Proceedings IDEAS'98, pp. 68  77, 1998 11]  F. Tao, F. Murtagh and M. Farid, Weighted Association Rule Mining using Weighted Support and Significance Framework, In Proceedings of the 9th SIGKDD conference, 2003 12]  Show-Jane Yen, An Efficient Approach for Analyzing User Behaviors in a Web-Based Training Environment, International Journal of Distance Education Technologies, Vol. 1, No. 4, pp.55-71, 2003 13]  Show-Jane Yen, Yue-Shi Lee and Chung-Wen Cho, Efficient Approach for the Maintenance of Path Traversal Patterns, In Proceedings of IEEE International Conference on e-Technology, eCommerce and e-Service \(EEE 14]  M. Spiliopoulou and L. C. Faulstich, Wum: A web utilization miner EDBT Workshop WebDB98, Springer Verlag, 1996 15]  M. S. Chen, J. S. Park and P. S. Yu, Efficient data mining for path traversal patterns,  IEEE Transactions on Knowledge and Data Engineering, pp. 209-221, 1998 16]  H. Yao,H. J. Hamilton, and C. J. Butz, A Foundational Approach to Mining Itemset Utilities from Databases, Proceedings of the 4th SIAM International Conference on Data Mining, Florida, USA, 2004 17]  Z. Chen, R. H. Fowler and A. Wai-Chee Fu, Linear Time Algorithm for Finding Maximal Forward References, Proceedings of International Conference on Information Technology. Computers and Communications  \(ITCC'2003 18]  T. Jing, Wan-Li Zou and Bang-Zuo Zhang, An Efficient Web Traversal Pattern Mining algorithm Based On Suffix Array, Proceedings of the 3rd International Conference on Machine Learning and Cybernetics , pp 1535-1539, 2004 19]  Show-Jane Yen, Yue-Shi Lee and Min-Chi Hsieh, An efficient incremental algorithm for mining Web traversal patterns, Proceedings of the 2005 IEEE International Conference on e-Business Engineering ICEBE05 20]  L. Zhou, Y. Liu, J. Wang and Y. Shi, Utility-based Web Path  Traversal Pattern Mining, Seventh  IEEE International Conference on Data 


Mining Workshops, pp. 373-378, 2007 21]  C. F. Ahmed, S. K. Tanbeer, Byeong-Soo Jeong and Young-Koo Lee Efficient mining of utility-based web path traversal patterns, 11th International Conference on Advanced Communication Technology ICACT09 22]   http://en.wikipedia.org/wiki/PageRank 23] en.wikipedia.org/wiki/Association_rule_mining  Attributes? FPW Algorithm FTPW Algorithm Recognition of User behavior Visiting Frequency Page Rank Time Spent on Web page Page Size Accessibility of required information in less time Improving Web navigation and system design of Web applications  Enhancing server performance 2010 5th International Conference on Industrial and Information Systems, ICIIS 2010, Jul 29 - Aug 01, 2010, India 200 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


