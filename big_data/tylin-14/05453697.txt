  Study of Privacy Pres erving Data Mining Haisheng Li East China Jiaotong University, Nachang330013,China lhscqdx@163.com   Abstract  There has been an important research area that how to protect private information or sensitive knowledge from leaking in the mining process, meanwhile obtain more accurate results of data mining. This paper describes data distortion, data encryption and reconstruction techniques in detail. Following a comprehensive comparison and analysis of existing technologies, the future work is showed Keywords Data Dining; Privacy Preserving; Data Distortion Data Encryption; Data Reconstruction I  I NTRODUCTION  With the database technology and network technology 
development, people generate and collect data has increased dramatically. Data mining as a powerful data analysis tools can find the potential models and rules in data, and is applied more and more in-depth such as business decisions scientific and medical research areas. At the same time, data mining is directly on the original data set, which also produced the inevitable leakage of privacy. So the main research direction of privacy preserving data mining is that how to protect private information or sensitive knowledge from leaking in the mining process, meanwhile obtain accurate results of data mining Privacy preserving data mining can be divided into two levels  T h e  fi r s t  l e v e l o f pr i v ac y  p r e s er vi ng da t a  m i ni ng  
is the protection of sensitive data, such as name, id number address and other sensitive data. The second level of privacy preserving data mining, called knowledge hiding in database is the protection of sensitive knowledge that is showed by data mining. It is problem to be solved is how to effectively hide sensitive rules of the data set, with minimal impact on non-sensitive rules and the usefulness of data sets Privacy protection technology is what is used to hide sensitive data or sensitive knowledge, which is mainly focused on data distortion, data encryption and data reconstruction technology to the study at present II  THE  DISTORTION-BASED  PRIVACY  
PRESERVING Data distortion perturbs the original data to achieve privacy preserving. The perturbed data would meet the two conditions. First, an attacker cannot discover the real original data. In other words, the attacker cannot reconstruct the real original data from the issuance of the distortion data Second, the distorted data is still to maintain some properties of the original data, namely some of the information derived from the distorted data are equivalent to data obtained from the original information. So it ensures that some applications based on the distorted data are feasible. At present, the techniques of privacy preserving based on data distortion include randomization [2  d a t a bl oc ki ng [3 a n d s o  o n D a t a  
randomization is the technique that adds random noise to the original data, and then distributes the disturbed data Randomization techniques include two types. The first type of randomization is called random perturbation. The other type of randomization is called randomized response A  random disruption based on association rule hiding Random perturbation modifies sensitive data in random process, thus achieving data privacy preserving. The basic idea of data transformation method is: to find the sensitive transactions to support the sensitive rule in the original database, to delete them or add items, so support or confidence of the sensitive rule reduced to below the 
threshold specified in order to achieve the sensitive rule hidden The problem of mining association rules was introduced in  Le t I i 1 i 2  001\002 i n be a set of literals, called items. Let D be a set of transactions, which is the database that is going to be disclosed. Each transaction t 001\031  D is an item set such that t 002 I, an association rule is an expression X 000 Y where X 002 I, Y 002  I, and X 001 Y 000 The X and Y are called respectively the left hand side and right hand side of the rule. The confidence of the rule is 
calculated as | X 001 Y |/|X|, where |X| is the number of transactions containing X and | X 001  Y | is the number of transactions containing both X and Y. The support of the rule is calculated as | X 001  Y |/|N|, where |N| is the number of transactions in D. The problem of mining association rules is to find all rules that are greater than the user-specified minimum support and minimum confidence B  data blocking Data blocking is different from data transformation modifying the data and provide non-real data, it don’t distribute some specific data to achieving data privacy preserving because some applications hope more to conduct 
studies based on real data. Blocking specific responses to the data table that the certain value of data table is replaced with an uncertain symbol. For example, an uncertain symbol with the exception of {0,1} is introduced to realize the Boolean association rule hiding. Because some values are instead of  ”?”, then the count of some itemsets is an uncertain value that is located between a minimum estimate and the maximum estimated value range. So achieving sensitive association rule hiding is to design an algorithm, in case of the data values as little as possible blocking, the 
Third International Symposium on Intelligent Information Technology and Security Informatics 978-0-7695-4020-7/10 $26.00 © 2010 IEEE DOI 10.1109/IITSI.2010.14 700 


  support and confidence of sensitive association rule is controlled at below a predetermined threshold C  random disruption based on classification rule Classification is a process, which can identify the model or function to describe and distinguish data classes or concepts, in order to be able to use predictive models to mark the unknown object classes. Classification goal is to construct a classification model to predict future data trends At present classification methods are mainly used in classification rules, decision trees, neural networks and so on[5  Parsimonious downgrading is a combination of rules and decision tree classification of privacy preserving method [6   Parsimonious downgrading is such an algorithm that the information in the data to be downgraded will be removed as a formal description. The so-called downgrading is to make the sensitivity level or the privacy level reduced to level can be announced. In other words, downgrade is right to make public release of information privacy preserving handling process. The parsimonious downgrading uses 002\031 instead of the data to be blocked, while the other blocking methods are often used “?” instead of the data to be blocked 002\031 values are between 0 and 1,which represents the blocked property to take the probability of a certain value D  randomized response[7  The basic idea of randomized response is: the data owner distributes the original perturbed data, so that an attacker can not be higher than the predetermined threshold probability to obtain the original data whether to include some real or false information. Randomized response technique and random perturbation techniques difference is that sensitive data is through a kind of indirect way of answering specific questions provided to the outside world. Randomized response model has two types: related-question model and unrelated-question model. Related-question model designs the two opposing issues of sensitive data. For example   I contain sensitive values A    I don’t contain sensitive value A  Data owners according to own data select at random a question to be answered. But questioner isn’t aware of the specific problems to be answered by data owners. After large amounts of data owners answered questions, the proportion of respondents containing sensitive values and the proportion of respondents not containing sensitive values can be obtained by calculating. It is supposed that he probability of respondents randomly selecting the first question is 002\031 and then the probability of the second question to be answered is 1\002\031 In various proportions stated as follows   P \(A=yes\, The proportion of the data owners containing sensitive values A    P \(A=no\, The proportion of the data owners not containing sensitive values A    P*\(A=yes\he proportion of respondents answered yes    P*\(A=no\, The proportion of respondents answered no The following equation set   P* \(A=yes\P \(A=yes 000h 002\031 P \(A=no 000h 1\002\031    P*\(A=no\=P \(A=no 000h 002\031 P \(A=yes 000h 1\002\031  Based on the above two equations, and combined estimate of all respondents drawn from p * \(a = yes\ and p a = no\, p \(a = yes\, p \(a = no\an be obtained Throughout this process, the inability to identify issues related to the respondents to answer and therefore cannot determine whether they contain sensitive data values Randomized response technique used to provide information with response model, so are used for processing categorical data III  THE  ENCRYPTION  BASED  PRIVACY  PRESERVING In a distributed environment the primary issue to achieve privacy preserving is the security of communications, and encryption technology just to meet this demand. Therefore privacy preserving based on data encryption technology commonly applies to distributed applications. Distributed applications store data using two models: vertically partitioned data model and horizontally partitioned data model. Vertically partitioned data refers to data by property located in different sites, all sites stored data does not overlap. Horizontally partitioned data refers to data distributed in each site according to records in this condition the various sites without having to know the specific record information to other sites; we can calculate the overall association rules [10   There are a lot of data mining algorithms on cryptogr aphy technology to solve real privacy issues, for example secure multi-party computation, SMC. SMC is defined as in a distrust of the multi-user networks; each user can be coordinated through the network to complete the reliable computing tasks, while maintaining the security of their data 8   B.P i nka s p u t for w a r d a t h eor e t i c a l s t ud y o f  cryptography used in data mining privacy preserving, and demonstrated different kinds of data mining problems can be transformed into SMC [8 For the vertically partitioned data mining association rules, the difficulty is how to calculate the support of itemset while using secure scalar product or secure size of set intersection to the problem can be resolved [12  L ite ra tu r e    d e s c ri be d a n al gor i t h m a p p l i e d t o E x p e c t at i o n  Maximization clustering without disclosure of information on each site because the algorithm used secure sum computing. In the multi- classification mining areas Wenliang Du proposed security classification algorithm on vertically partitioned data using secure scalar product   While Liddell made the use of encryption methods to establish the horizontally partitioned data decision tree, and it translate the search for the best classification property into secure multi-party computation [14   IV  THE  RECONSTRUCTION  BASED  PRIVACY  PRESERVING Data reconstruction includes numerical data reconstruc tion and binary data and classification data reconstruction 
701 


  For the numerical data of the reconstruction techniques: the original data is modified by discrimination methods and the value of the deformation method, then use reconstruction algorithm to construct the distribution of original data [11   Literature [1  p r op os e d  an i m pr ov e d m e t h od  b a s e d o n  Bayesian reconstruction with Expectation Maximization algorithm. In the case of a large enough data set, Expectation Maximization algorithm can get the original data distribution of the maximum likelihood estimate. In addition, numerical data reconstruction can achieve the original data protection, but sensitive knowledge is not protected. For the binary data and classification data reconstruction: literature [1 r a n d o m i z a t i on technology to modify the binary data and classification data of the association rules as dimensions V  ASSESSMENT  OF  THE  PRIVACY  PRESERVING  DATA  MINING  ALGORITHMS So far, there is no a privacy preserving data mining algorithm to effectively hide the various data sets. The current algorithms are mostly designed for specific data sets therefore, there is no a specific criterion to obtain an accurate assessment of the performance of each algorithm But generally speaking, privacy preserving algorithms can be evaluated and compared from the following areas [17     Efficiency of the algorithm: The main is the algorithm running time to hide sensitive data or sensitive information. It is necessary to evaluate an important indicator of various algorithms   Availability of the algorithm: Availability of the algorithm refers to data sets processed by privacy preserving technology, it contains information that should be as far as possible to meet the needs of data mining. If the global association rules derived from data sets processed by privacy preserving algorithms is wrong, or does not reflect the true situation, so that the algorithms lost availability   Level of privacy preserving: Level of privacy preserving refers to the extent to which the success of sensitive information hidden and in the data set to be distributed on the use of varieties of data mining algorithms excavated the success rate of private information   Scalability of the algorithm: Scalability of the algorithm refers to the ability to handle massive data sets or the variation trend in processing efficiency when the amount of data increases. The efficiency of change is relatively slow for a good scalability of the algorithm when the amount of data increases VI  CONCLUSION  Privacy preserving is applied widely in many fields and is the research subject of the emerging academic in recent years. This paper describes the distortion-based privacy preserving, the encryption -based privacy preserving and the reconstruction -based privacy preserving. At present a variety of privacy preserving data mining algorithms are still some shortcomings, and are targeted at specific applications and data sets, rather than to be extended to the general. The premise of ensuring the privacy of how to reduce the loss of accuracy, how to further improve the algorithm efficiency and privacy preserving generality in different types distribution characteristics of different data sets are the direction of the future worthy of further study A CKNOWLEDGMENT  The author wishes to thank the peers for supporting and advising this research R EFERENCES  1  V. Verykios, E. Bertino, I.G. Fovino, L.P. Provenza, Y. Saygin, and Y. Theodoridis, State-of-the-art in Privacy Preserving Data Mining SIGMOD Record, Vol. 33, No. 1, 50-57, March 2004 2  Kargupta H, Datta S, Wang Q, Sivakumar K. On the privacy preserving properties of random data perturbation techniques Proceedings of the IEEE International Conference on Data Mining ICDM\. Melbourne, Florida, 2003: 99-106 3  Moskowitz I S, Chang L W. A decision theoretical based system for information downgrading//Proceedings of the 5th Joint Conference on Information Sciences \(JCIS\ AtlanticCity, NJ USA, 2000: 82-89 4  VERYKIOS V S, ELMAGARMID A, BERTINO E, et al Association rule hiding[J  I EEE T r a n s o n K n o w le d g e an d D a t a  Engineering,2004,16\(4\: 434-447 5  Pinkas B. Cryptographic techniques for privacy-preserving datamining· SIGKDD Explorations,2002,4\(2 6  DU Wen-liang, ZHAN Zhi-jun. Building decision tree classifier on private data[C  Pr o c o f  I EEE I n te r n a tio n a l Co f e r e n c e o n Pr iv acy  Security and DataMining. Darlinghurst: Australian Computer Society, 2002: 1-8 7  ZHOU Shui-Geng, LI Feng1, TAO Yu-Fei, XIAO Xiao-Kui. Privacy Preservation in Database Applications: A Surve. CHINESE JOURNAL OF COMPUTER,2009,32\(5 8  Pinkas B. Cryptographic techniques for privacy-preservingdata mining [J A C M S I G K D D E x pl o r ati o ns N e w s l e tte r 200 2 4 2  1 21 9   9  Du Wenliang, Attalah M J. Secure multi problem computation problems and their applications: A review and open problems R CE RI A S T e ch Re po r t  20 0 1 5 1 Ce nte r f o r E d uc at io n a n d  Research in Information Assurance and Security and Department of Computer Sciences, Purdue University, West Lafayette,IN,2001   Kantarcioglu M, Clifton C· Privacy preserving Distributed Mining of Association Rules on Horizontally Partitioned Data In: ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, 2002   Agrawal R, Srikant R. Privacy-preserving data mining [A   Proceedings of the 2000 ACM SIGMOD international conference on Management of data[C Da lla s  T e xa s  Un it ed   S t a t es A C M  200 0 439-450   Clifton C 9 Kantarcioglou M 9 Zhu Y M. Tools for privacy preserving distributed data mining[J SI G K D D E x pl o r atio ns 4 9 2002 9 2   Du Wenliang 9 Zhan Zhijun. Building decision tree classifier on private data[C  I n Pr o c e e d in g s o f th e  I EEE I C D M W o r k sh o p o n  Privacy 9 Security and DataMining 9 2002   Lindell Y 9 Pinkas B. Privacy preserving data mining[C I n  Advances in Cryptology-CRYPTO 2000 9 2000 9 36-54   Rizvi S J, Haritsa J R. Maintaining data privacy in association rule mining [A I n P r o cee ding s o f  the  2 8 t h I n te r n atio nal  Co nf e r e n ce o n  Very Large Databases\(VLD\ [C H o ng K o ng  C hin a   s  n     2 0 0 2   682-693   Agrawal D,Aggarwal C C.On the design and quantification ofprivacy preserving data mining algorithms[A r o cee ding s o f the tw e n tie t h  ACM SIGMOD-SIGACT-SIGART symposium on Principles of 
702 


  database systems[C S an t a B a rb ar a C a l i fo rn i a U n i t e d S t a t e s  A C M  Press, 2001.247-255   LIXue-ming, LIU Zhi-jun, QIN Dong-xia. Privacy preserving data mining. Application Research of Computers, 2008, 25\(12 
703 


536 


537 


preserved in the last basic window, then can get large itemsets ?L  in window SW 3. Forward to slide a basic window 7 basic window in sliding window  \(1,2,, N 4. Mine association rules in new sliding window 8 the ?? LX  support. Let sup.|{ SWsXLXO SW those itemsets who appear in DN but not belong to L , get the local large itemsets NewL  in new basic window. For the local large itemsets NewL  in new basic window ND scan database ?D  of ?SW  and get the large itemsets W which meet the sliding window minimum support Merge the above large itemsets WOL get the large itemsets L  after sliding a basic window. Call large items time vector generating function 9 vector generating function 10 5 The above algorithm is efficient, because we use a similar method of incremental association rules, improve it and just scan a small number of the data in database B. Algorithm complexity analysis Let DS| be the number of the records in the whole database, N be the number of the basic windows in the whole database, k be the size of the basic window, and then the average number of the records in each basic window be DS N When the algorithm under the worst situation, each basic window because of sliding window function will be repeat scanned many times. The times of the basic windows in the first sliding window and sliding window may respectively scan to be 1, 2, ..., k. The middle of the basic window may scan k times repeated, which is equivalent to a basic window repeat scan k\(k+1 N-2k condition is to find out n-large itemsets, and the average number of records of each basic window is | / |DS N . So, the whole time complexity is approximately 1 2 


the new basic window has no new large itemsets, so the time complexity is approximately   complexity greatly reduced. The average time complexity of the algorithm is \( \( \( 1 2  IV. NUMERICAL REUSLTS By using the adventure works database: SQL SERVER2005, we test the sales data to mine association rules. After pretreatment, the same order number in a shopping basket, we take one of the August 1, 2003 to July 31, 2004 analysis of data throughout the year, a total of 23,434 records The algorithm is implemented using PHP language experimental platform: CPU is a Pentium \(R 543 E5300 2.60GHz, memory is 2G, the operating system is Windows XP Under the support of 0.04, confidence level of 0.6 conditions, the results are obtained with the time vector of association rules as follows 871 870 872 870 880 870 930 921 929 921 2003 8 1,2004 7 31 2003 8 1,2004 7 22 2003 8 1,2003 9 25 2003 8 1,2004 7 31 2003 8 7,2003 11 8 2003 11 14  CV CV CV CV CV T T T T 


T           934 923 923 934 2003 12 30 2004 6 18,2004 7 31 2003 10 21,2004 7 31 2003 12 24,2004 2 19 2004 3 12,2004 5 21 CV CV T T        From the above formula, we can see that each association rule in which time is effective, but some time is invalid. For example, the associations rule 871? 870 throughout the period included on the database is valid. The associations rule 923? 934 in the time period December 24, 2003 to February 19, 2004 and the time period March 12, 2004 to May 21, 2004 as these two time periods is effective. The above association rules are graphically represented in Figure 1 as follows  20038-1 20038-31 


20039-30 200310-30 200311-29 200312-29 20041-28 20042-27 20043-28 20044-27 20045-27 20046-26 20047-26 20048-25 871->870 872->870 880->870 930->921 929->921 934->923 923->934  Figure 1.  Time periods of association rule To compare the algorithm execution time under the different support for the association rule with time, the result is shown in Figure 2. From the Figure 2, when the algorithm is in a high support, efficiency is very high with the increase of support. Particularly, when support is small the algorithm is much less efficient. This is because the result of uneven distribution of the data. The adventure works sales data in the database are distributed unevenly so that some of the basic window data is very dense under the 


low support. Hence, the algorithm will produce a large number of large itemsets, resulting in reduced efficiency of the algorithm. Therefore, the algorithm under the even data distribution can achieve better results 0 1000 2000 3000 4000 5000 6000 0.2 0.1 0.08 0.04 t s  Figure 2.  The algorithm execution time under the different support V. CONCLUSION This paper studies the dynamic association rules with time, based on the time vector to represent the dynamic association rules. We propose an algorithm that can mine the dynamic association rules. These dynamic association rules include some practical significance in what time period will appear on the association rules. The numerical experiments show that the algorithm can efficiently reduce the number of the scanning database within the less execution time. In addition, there are some limitations of the algorithm, such as some large itemsets may be lost and sensitive to the change of support. These problems are to study further REFERENCES 1] Rakesh Agrawal,Tomasz Imielinski,Arun Swami, Mining association rules between sets of items in large database, ACM SIGMOD?Washington,D?C., 1993, pp.207-216 2] David Wai-Lok Cheung,Jiawei Han, Vincent Ng, C.Y. Wong Maintenance of discovered association rules in large databases:An incremental updating technique, proceeding of ICDE, 1996, pp.106114 3] Liu Jinfeng, Rong Gang, Mining dynamic association rules in databases, Proc. of Int Conf. on Computational Intelligence and Security, Xian, 2005, pp.688-695 4] Rong G, Liu J F, Gu H J. Mining dynamic association rules in databases, Control Theory & Applications, 2007, Vol.24, No.1 


pp.129-133. \(in Chinese 5] Liu  Jun, Xie Yanfeng, Zhang Zhonglin, Jia Limin, Research of min ing meta association rules for dynamic association rule based on model of grey Markov,  Computer Applications, 2008, Vol.28 , No.9 pp.2353-2356.\(in Chinese 6] Liu Xuejun?Xu Hongbing?Dong Yisheng?Qian jiangbo?Wang Yongli, Mining frequent closed patterns from a sliding window over data streams, Journal of Computer Research and Development Vol.43, No.10, 2006, pp.1738-1743. \(in Chinese  554 


While focusing on the sample cost and using the strata that are more likely to contain tuples where A = a is clearly important, variance of estimation is another important issue, since it corresponds to the error of estimation. Thus, we de?ne the integrated cost by taking into account the variance of estimation 2s and the sampling cost SampCost Cost = ?s  SampCost + ?v  ?2s \(5 where ?s and ?v are weights for sampling cost and estimate variance, respectively, such that their sum is 1. Users could set the two weights based on the importance they attach to these two factors. For example, if response to a query is needed quickly sampling cost should have a higher weightage The relation between input attributes and output attributes, as well as data distribution, are modeled by a tree built on the query space recursively. Each node in the tree represents a query subspace, and is associated with a query that comprises speci?c values for a subset of input attributes, the sample size, as well as the potential splitting attributes. The sample size shows how many data records would be drawn from the subpopulation of the node Algorithm 1 shows the process of splitting a node N , which as we stated above, is associated with the query Q, the sample size s, and a list of potential splitting attributes, PS. The inputs of the algorithm also include set Lf , which represents leaf nodes of the tree. At the beginning, the entire query space is represented by the root node. The corresponding query of the root node is null the sample size n is the size of the sample that is to be drawn from the entire population, and the potential splitting attributes list is the complete set of input attributes of this data source. The initial set of leaf nodes is empty The main goal of the algorithm is to split the query tree in a greedy way. For each potential splitting attribute, the integrated cost after splitting is computed according to the Expression 5. The input attribute which brings the most reduction in the integrated cost is selected. More speci?cally, ?rst, the integrated cost for the subpopulation of the node N is calculated. If this value is smaller than a prede?ned threshold ?, node N is set to be a leaf node \(Lines 2-5 considering the set of potential splitting attributes PS \(Lines 621 would be |DIA| strata under the space of node N . The integrated cost on |DIA| strata is computed. Let M ? PS with domain DM 


denote the input attribute with the minimum integrated cost after the split, and let the set of allocated sample sizes, computed by the sample allocation method explained later, be ASM . Then DM | children are generated for the node M . For each child CHi, i = 1, . . . , |DM |, the associated query is QN ?{M = mi sample size is asi ? ASM , and potential splitting attributes is PSN ? M . The process of splitting is then recursively applied to children of node N In the process of calculating costs during the strati?cation process, we need to perform sample allocation, i.e., divide the parent nodes sample size among the potential children nodes This is required for calculating the integrated cost for the potential split. This is based on our sample allocation method, which we 328 describe next in Section IV-B. Furthermore, for calculating the integrated cost, the variance of target value ?2i and probability of output attribute A = a, ?i, for each stratum is computed based on the pilot sample Initially, the strati?cation process on the query space begins by calling Stratification\(R, null, F IA, n, null root node. The process of strati?cation would stop if there is no leaf node with integrated cost larger than a prede?ned threshold Each leaf node in the tree is a ?nal stratum for sampling, and the associated sample size denotes the number of data records drawn from the subpopulation of the stratum B. An Optimized Sample Allocation Method Now, we introduce our optimized algorithm for sample allocation which integrates variance reduction and sampling cost As introduced in section IV-A, integrated cost is de?ned by taking into account of variance of estimation and sampling cost The goal of our sample allocation algorithm is to minimize the integrated cost by choosing the sample size, ni, for each stratum In our algorithm, we adjust the value of SampCost and ?2s so that their in?uences on the integrated cost are in the same unit SampCost SampCost SmpCost\(r where SmpCost\(r entire population, and ?r denotes the probability of A = a being true for the entire population 2 2s 


2r where ?2r 2 n denotes the variance of estimation of the target value on the entire population The key constraint on the values of the sample sizes for each strata is that their sum should be equal to the total sample size A vector n = {n1, n2, . . . , nH} is used to represent the sample sizes, where the ith element, ni, is the sample size for the ith stratum By including sampling cost and variance of estimation into the integrated cost, our sample size determination task leads to the following optimization problem Minimize Cost\(n  i\(?s ni i v N2i niN2 2 i subject to  i ni = n 6 where Ni denotes the population size of data records under the space of A = a in ith stratum. Note that this value may not be known if A is an output attribute. However, the total number of records in the ith stratum is typically known, and can be denoted as DNi. Then Ni can be estimated by ?iDNi, and the population size of A = a on the entire population is estimated by N  i ?i DNi For ?nding the minimum of integrated cost, we utilize Lagrange multipliers, a well know optimization method. Lagrange multipliers aims at ?nding the extrema of a function object to constraints. Using this approach, a new variable ? called a Lagrange multiplier is introduced and de?ned by n n  


 i ni ? n If n is a minimum solution for the original constrained problem then there exists a ? such that \(n Lagrange function. Stationary points are those points where the partial derivatives of ?\(n n,??\(n 7 In our problem, by conducting partial derivatives on Formula 7 a group of equations are yielded as follows s i v N 2 i n2iN2 2 i + ? = 0 i = 1, ..H i ni = n 8 where the solution n leads to the minimum value of integrated cost in Formula 5 However, it is dif?cult to solve the group of equations directly Thus, we use numerical analysis to approximate the real solution. Newtons method is utilized for ?nding successively better approximations to the zeroes \(or roots Given an equation f\(x x tive of function f\(x method iteratively provides, xt+1, a better approximation of the root, based on xt, the previous approximation of root according to the following formula xt+1 = xt ? f\(xt f ?\(xt The iteration is repeated until a suf?ciently accurate value is reached, i.g. |f\(xt In our problem of Formula 8, there are H + 1 equations F \(xt xt xt n1, .., nH , ?}, where the equations in F \(xt fi\(xt s i v N 


2 i n2iN2 2 i + ? i = 1, .., H fH+1\(xt  i ni ? n The Newtons method is also applied iteratively via the system of linear equations JF \(xt xt+1 ? xt xt 9 where JF \(xt H + 1 H + 1 equation system F \(xt vector xt. The entry JF \(i, j d\(fi\(x dxj where fi\(x xt x From the Expression 9, a better approximation xt+1 is obtained based on previous approximation xt. The iterative procedure would be stopped if ?i|fi\(xt threshold, and then the sample size ni is allocated for each stratum so that the integrated cost is minimized. In reality, we are required to pick an integral number of records from each stratum during the sampling step. Thus, we round down each ni to its nearest integer, ni + 0.5 In the example shown in Table I, suppose we set both weights v and ?s to be 0.5. Further, assume the variance of the entire population, ?2r , to be 80000. The probability of A = a over the entire population ?r is 0.242. By using the proposed optimized sample allocation method, the sample sizes for the three strata 329 are 162, 299, and 139, respectively. In this case, the variance of estimation according to the Expression 2 is 93.66, and the estimated cost is 1943.7. We can see that, compared with Neyman allocation, the sampling cost is decreased by 42.1%, but results some increase in variance. Overall, this example shows that we can achieve lower sampling cost by trading off some accuracy C. Overall Sampling Process Algorithm 2 DiffRuleSampling\(DW1, DW2, F IA, t, St 1: PS ? a pilot sample from DW1, DW2 


2: DR ? identi?ed rules from PS 3: OA ? output attributes of DW1, DW2 4: for all R : X ? DW1\(t t 5: if X ?OA = null then 6: Acquire St data records from the space of X 7: else 8: R ? root node 9: Lf ? null 10: Strati?cation\(R,null, F IA, St, Lf 11: for all N ? Lf do 12: s ? sample size of N 13: Draw s data records from the subpopulation of N 14: end for 15: end if 16: Update the mean value of DW1\(t t 17: end for Algorithm 2 shows the overall sampling process for differential rule mining on two deep web data sources, DW1 and DW2 and with differential attribute t. The inputs of the algorithm also contain the full set of input attributes FIA as well as the sample size St. The algorithm starts with a pilot sample PS, from which the differential rules are identi?ed. For the rule R : X ? DW1\(t t St data records are directly drawn from the space of X \(Lines 5-6 t t containing output attributes, query spaces of DW1 and DW2 are strati?ed and sample is recursively allocated to each stratum with corresponding query subspace \(Line 10 of the tree built by strati?cation, a sample is drawn according to its sample size \(Line 13 t t is updated by the further sample \(Line 16 for association rule mining is very similar and not shown here V. EVALUATION STUDY We evaluate our sampling methods for association mining and differential rule mining on the deep web using two datasets described below US Census data set: This is a 9-attribute real-life data set obtained from the 2008 US Census on the income of US households. This data set contains 40,000 data records with 7 categorical attributes about the race, age, and education level of the husband and wife of each household and 2 numerical attributes about the incomes of husband and wife 


Yahoo! data set: The Yahoo! data set, which consists of the data crawled from a subset of a real-world hidden database at http://autos.yahoo.com/. Particularly, we download the data on used cars located within 50 miles of a zipcode address. This yields 30,000 data records. The data consists of 7-attribute with 6 categorical attributes about the age, mileage, brand, etc, of the cars and one numerical attribute, which is the price of the car Variance of Estimation is estimated for the target value \(i.e mean value in differential rule mining, and con?dence in association rule mining is calculated according to the Expression 2. Since variance of estimation reveals the variation of the estimated value from the true value, smaller variance suggests better estimation Sampling Cost is estimated by the number of queries submitted to data sources in order to acquire a certain number of data records containing target output attributes, i.e. A = a. Larger sample size implies higher sampling costs in a deep web setting where the queries are executed over the internet Estimate accuracy is estimated by Absolute Error Rate \(AER Small AER value indicates higher accuracy. For an estimator on variable Y with true value y and estimated value y, the AER of the estimator is calculated by AER\(y A. Association Rule Mining In this section, we present the results of our method for association rule mining. Using our overall approach, we have created four different versions, which correspond to four different sets of weights assigned to variance of estimation and sampling costs. 1 the weight ?v = 1.0 and ?s = 0.0, 2 the weight ?v = 0.7 and ?s = 0.3, 3 v = 0.5 and ?s = 0.5, and 4 weights ?v = 0.3 and ?s = 0.7. In addition, we also compare these approaches with a simple random sampling method, which is denoted by Random We focus on the queries in the form of A = a ? B = b where A and B are output categorical attributes. Other categorical attributes in the data set are considered as input attributes Our goal is to estimate Supp\(A=a,B=b A=a association rules are randomly selected from the datasets. Each of the 50 association rules are re-processed 100 times using 100 different \(pilot sample, sample iterations result is the average result for 5000 executions 


In all charts reported in this section, the X-axis is k, which denotes the size of sample under the space of a target rule drawn from deep web. The sample size for each point on X-axis is k x, where x is a ?xed value for our experiment, and depends upon the dataset. At each time, queries are issued to obtain kx data records under the space of a target rule. Overall, all our experiments show the variance of estimation, sampling costs and sampling accuracy with varying sample size Figure 1 shows the result from our strati?ed sample methods on the US census data set. The size of pilot sample is 2000, from which all of the 50 initial rules are derived. In this experiment the ?xed value x is set to be 300, which means the smallest sample size at k = 1 is 300, and the largest sample size at k 10 is 3000. Figure 1 a the ?ve sampling procedures. Figure 1 b cost for the sampling procedures. In order to better illustrate the experiment result, in each execution of sampling, the variance of 330 6DPSOLQJ9DULDQFH            9D UL DQ FH R I V WL PD WL RQ  


9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW           6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF            5 


 9DU 9DU 9DU 5DQG c Fig. 1. Evaluation of Sampling Methods for Association Rule Mining on US Census Dataset estimation and sampling cost for the sampling procedures var7 var5, var3, and rand are normalized by the corresponding values of Full Var. Thus, in our experiment, the values of sampling cost and variance of estimation for sampling procedure Full Var are all 1. Furthermore, Figure 1 c sampling procedures From Figure 1 a pared with sampling procedures Var7, Var5 and Var3, Full Var has the lowest estimation variance and the highest sampling cost. From sampling procedures Var7, Var5, and Var3, we can see a pattern that the variance of estimation increases, and the sampling cost decreases consistently with the decrease of the weight for variance of estimation. At the largest sample size of k = 10, the estimation variance of sampling procedure Var3 is increased by 27% and the sampling cost is decreased by 40 compared with sampling procedure Full Var. The experiment shows that our method decreases the sampling cost ef?ciently by trading off a percent of variance of estimation. Similar to variance of estimation, the sampling accuracy of these procedures also decreases with the decrease of the weight on variance of estimation. For the largest sample size at k = 10, we can see that the AER of sampling procedure Var3 is increased by 20 compared with sampling procedure Full Var. However, for many users, increase of the AER will be acceptable, since the sampling cost is decreased by 40%. By setting the weights for sampling variance and sampling cost, users would be able to control the trade-off between the variance of estimation, sampling cost, and estimation accuracy In addition, compared with sampling procedure of Full Var Var7, Var5, and Var3, sampling procedure Random, has higher estimation of variance, sampling cost and lower estimation accuracy. Thus, our approach clearly results in more effective methods than using simple random sampling for data mining on the deep web Figure 2 shows the experiment result of our proposed strati?ed 


sampling methods on the Yahoo! data set. The size of pilot sample on this data set is 2,000, and the ?xed value x for sample size is 200. The results are similar to those from the US census dataset. We can still see the pattern of the variance of estimation increasing with the decrease of its weight. Besides, the sampling accuracy is also similar to the variance of estimation. However although the variance estimation of sampling procedure Random is 60% larger than sampling procedure Full Var, the sampling cost of Random is 2% smaller than Full Var. This is because Full Var does not consider sampling cost. It is possible that Full Var assigns a large sample to a stratum with low ?, which denotes the probability of containing data records under the space of A = a, resulting the larger sampling cost than that of simple random sampling. Sampling procedures Var7, Var5, Var3 consider sampling cost as well, and have smaller variance estimation and sampling cost, compared with Random. Furthermore, Random has smaller sampling accuracy than Full Var, Var7 and Var5, but has larger sampling accuracy than Var3. This is because Var3 assigns much more importance to the sampling cost, and loses accuracy to a large extent To summarize, our results shows that our proposed strati?ed sampling are clearly more effective than simple random sampling on the deep web. Moreover, our approach allows users to tradeoff variance of estimation and sampling accuracy to some extent while achieving a large reduction in sampling costs B. Differential Rule Mining In this section, we present results from experiments based on differential rule mining. Particularly, we look at the rules of the form A = a ? D1\(t t categorical attribute and t is an output numerical attribute, while other categorical attributes in the data set are considered as input attributes In this experiment, we also evaluate our proposed method with different weights assigned to variance of estimation and sampling cost. Five sampling procedures, Full Var, Var7, Var5,Var3 and Random, have same meanings with those in the experiments of association rule mining. Similarly, 50 rules are randomly selected from the datasets, and each of the 50 differential rules are reprocessed 100 times using 100 different \(pilot sample, sample iterations 5000 runs First, we evaluated the performance of these procedures on 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


