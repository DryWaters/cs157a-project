Study on the Application of Multi-level Association Rules Based on Granular Computing  Yanguang Shen School of Information & Electronic Engineering Hebei University of Engineering Handan, China shenyanguang@yahoo.com.cn Jing Shen School of Information & Electronic Engineering Hebei University of Engineering Handan, China jinghuayuan007@sina.com Yongjian Fan School of Information & Electronic Engineering Hebei University of Engineering Handan, China fanyongjian@126.com   Abstract For the issue that classical association rules can not mine multi-level association rules, we proposed a multi-level association rule mining method based on binary information granules in granular computing and multiple minimum supports and gave the definition of the support and confidence based on binary information granules. In this new association rules method, we can reduce the generation search space of frequent itemsets, extract multi-level association information\(including cross-level information Keywords-data mining; granular computing; multi-level association rule; multiple minimum supports I.  INTRODUCTION Association rules proposed by R. Agrawal,etc[1] for the first time is one of the most important research areas in data mining Its target is to find out all frequent itemsets in the database that meet the minimum support, and to generate all association rules used by all frequent itemsets to meet the minimum degree of confidence. Granular computing [2-4]is a kind of knowledge processing method which can express the semantic information of the data stored in the system. A large amount of complex 


information can be divided into a number of relatively simple pieces in accordance with their respective characteristics and performance, each of which is called an information granule The information processing process is called Information granulation. Using Granular computing in mining association rules, this method enables people to easily observe, analyze and solve problems from different granularity Traditional data mining methods are mostly single-level association rule mining. In practice, because of the sparsity of the data in multi-dimensional data space, it is very difficult to find out strong association rules among the low-level data items. For instance, the apparent hierarchy, fruit -- fresh fruit apples, is usually recorded by the particular item "Apple" in the database. The traditional association rule mining algorithm can only get such association rules as "Apple - Chestnut ", while other higher level association rules, such as "fresh fruit  nuts can not be derived. So we require to spot strong association rules at a higher level to obtain meaningful knowledge Minimum support and minimum confidence are two important factors in the process of mining association rules, while minimum confidence is obtained by minimum support, so the minimum support becomes a key factor that has an impact on mining association rules. Usually, there is only one minimum support in the whole process in the traditional association rules method.As the following two rules Buys?X?fresh fruit? ?buys?X?chestnut sup=8%?confidence=70%]       ?Formula 1 Buys?X?apple? ?buys?X?chestnut sup=2%?confidence=70%]       ?Formula 2 If the minimum support is set to 8%, we can not find the underlying rules \(Formula 2 association between frequent itemsets because of the too low minimum support. Therefore it may arise a lot of meaningless rules, leading to combinatorial explosion. So, we make use of multiple minimum support of the data items to generate all the frequent itemsets, and use minimum confidence to generate all the association rules. In this way, we can find different rules relied on different data items by different minimum support Using this new model can find rules contained in new scarce items, and avoid to generate large amounts of useless rules because of high frequency itemsets In this paper, through the hierarchical coding of attributes 


we give granularity, support, multi-minimum supports and several other basic definitions of terms based on binary information granules from the perspective of information granulation. Then we proposed a multi-level association rules mining method based on binary information granules in granular computing and multi-minimum support constraint. By the application of association rules in the agricultural products Information, we prove that the proposed method can reduce the generated search space of frequent itemsets effectively, and can achieve multi-layer association rules \(including cross-level association rules II. RELATED CONCEPTS Let the item set I = {a1,a2,,am} be a set of m different items .For a set A, if  A? I  and k =|A|, then it is called k item sets For a given transaction database D, transaction number in the D is called support of A, which is denoted by sup \(A For the minimum support ,which defined by users is less than |D|, and denoted by s Based on the above assumptions, we have the following definition 2010 International Conference on Intelligent Computation Technology and Automation 978-0-7695-4077-1/10 $26.00  2010 IEEE DOI 10.1109/ICICTA.2010.656 564 Definition 1: For a given transaction database D and support s, for the item-set A? I, if sup \(A the frequent itemsets of D Definition 2: For the item set A = {a1,a2,,am} \(1 ?  k m ak MIS \(ai III. HIERARCHICAL ASSOCIATION RULES BASED ON GRANULAR COMPUTING A. Hierarchical structure coding The relationship of nodes among the tree structure can constitute a hierarchical classification structure. If the child nodes of the tree structure have sequential relationship, it is called ordered tree. Using different symbols mark at each node of the ordered tree, so the tree is called addressing trees. Both the general tree structure and the ordered tree structure can express the hierarchical structure easily, express the multi-level concept or use the concept to achieve multi-level algorithm 


 Figure 1.  Hierarchical classification structure We select the interesting properties, and use the hierarchical structure to encode data from the multi-level structure of the projects to the projects of various single-level structures. Then we obtain a multi-level association rules through the single-level structure of the various projects. Such as the user's purchase information shown in Table 1, is corresponding to Figure 1 of the fruit-level as the layer of 0 and then can be divided into first level and second level and so on. Take the transaction identifier TID "121" as an example the first "1" in TID corresponds to the first layer of the "fresh fruit." the second "2" in TID corresponds to the second layer of melons." Finally, the "Watermelon" corresponds to the last 1" in TID, belonging to the third layer. So 121 in TID represents the watermelon of the melons in the fruit, you can encode values to correspond to the hierarchical structure quickly TABLE I.  TRANSACTION DATABASE TID Items Hierarchical coding Level 2 Level 1 T1 {chestnut?apple} {22*?111} {22*?11*} 2**?1 T2 { wild grape?watermelon} {3**?121} {12*} {3**?1 T3 { watermelon?pear} {121?112} {12*?11*} {1 T4 {chestnut?watermelon?apple 22*?121 111 22*?12 11 2 1 T5 { wild grape } {3**}  {3 T6 { chestnut } {21*} {21*} {2 B. The definition of support and confidence based on Granular Computing When we settle and process a large number of complex information issues, the information in accordance with their respective characteristics and performance can be divided into a number of relatively simple pieces, each of these is called a information granule. Some definitions of granules are as follows[2-4 Definition 3, for a given information system S = \(U, A = C D 


composed of condition attribute set C and decision attribute set D. Let B ? C, if the range of B is VB = \(b1, b2, b3, ..., bK the number of range B is | VB | = k, and | U / IND {B}| = k then B can be decomposed into k binary information granules by quotient set|U/IND {B}|The decision attribute set D can be decomposed into | U / IND \(D by quotient set |U/IND {D For example: The values in corresponding to the objects of the item of TID in Table 1 were {chestnut, apple, wild grape watermelon, pears, walnuts}, can be decomposed into six binary information granules as follows:a1={100100 a2={100100}?a3={010010}?a4={011100}?a5={001000 a6={000001 Definition 4 The number of 1 in the binary information granules P is defined as granularity of the binary information granules, which is denoted by | P For example: The granularity of the binary information a1 100100 000001 Definition 5 For a given information system S = \(U, A = C D in Ci, then we have the granularity associated with computing is ci?cj??ck \(where ? on behalf of binary Boolean "and operator For example: Two binary information granules are a1 101000011001 100101001101 operation is as follows: 101000011001 ? 100101001101 100000001001 Based on the above definition, we give the definitions of support and confidence based on binary information granules as follows Definition 6 For any item set X ? I, Y ? I, X ? Y =?, the support of X is sup \(X  of collection of objects; [X] = [x1] ? [x2] ? ... ? [xn], xi is one of the project in X; Y is the same like X The support of the rule X?Y sup\(X ? Y   The confidence of the rule X?Y confidence\(X?Y sup\(X?Y sup\(X X Y X For example ? The support between the two binary information granules a1 and b2 is sup\(a1 565 


b2 confidence\(a1?b2 a1?b2 a1 C. The computing of multiple minimum support based on granular computing When we use multiple minimum support in the multi-level association rules, we can get the rules of high frequency of data entry rules by higher minimum support, as well as low frequency data entry rules by lower minimum support. In this paper, for setting multiple minimum support of each item, we adopt the multiple minimum support proposed by Lin[5] to remove some redundant rules as much as possible, and to reduce the generation search space of frequent itemsets Definition 7 Let I={a1?a2??an},and sup?ai??sup ai+1?,1?i?n-1, Then minimum support can be specified as follows       niasup 1ni1asupasup  a i 1ii i  Definition 8 Let A ? B be a frequent itemset, A ?  B and without loss of generality, let a? A , and a be the one with the smallest minimum support, i.e. MIS\(a ai the minimum support of a is set as MIS \(a a minconf, then the association rule A?B is strong Summarize the above two different definitions of the minimum support, so that the overwhelming majority of rules generated are effective. To prune the spurious frequent itemsets so as to make most of the generated rules become interesting we combine these two specifications as shown below, which we call the definition 9 Definition 9 Let I={a1?a2??an},and sup?ai?? sup ai+1?,1 ? i? n-1, Then minimum support can be specified as follows 


      niasup 1ni1}asupminconf{maxasup  a i 1ii i  For example: we can see from Table 2, the support of [1 is 4/6, and the support of [1 **] is the greatest in the all items, so its MIS value is its own support 2/3. From this calculation, we can draw the support of various items and items among layers in Table 3 IV. APPLICATION EXAMPLE We use the form of a hierarchical structure to encode the user purchase information in table 1, express the value in the corresponding with the various items hierarchical encoded of the TID by information granulation, and calculate the support corresponding to each item encoded by the hierarchical structure, which are shown in table 2. Through the definition of multiple minimum supports based on Granular Computing and the minimum support derived form the various items calculated in Table 2,we can calculate minimum supports corresponding to each item and level-items and show  in Table 3 The entire mining process of multi-level association rules is start at the search of is high-level frequent itemsets. Each item has a minimum support by definition 2. Search the transaction data in table 2 to find the frequent itemsets which meet the minimum support in same layer and multiple layers, while record them in table 4. For example, we calculate the multiple minimum support of [1 **] and [2 **] is1 / 3, sup \([1 **] [2  111100 ? 100101 which meet the minimum support 1 / 3. therefore, [1 **] and [2 are combined as frequent itemset. To extract the frequent itemsets repeatedly, and filter the candidate itemsets which does not meet the minimum support. Finally all the frequent 2 


itemsets are shown in table 4. While no further increase in levels, the entire search process will end Then we find frequent 3 - itemsets from all the frequent 2 itemsets in table 4 which meet the multiple minimum support of definition 2, and filter out the candidate itemsets which does not meet. Repeatly extract the frequent 3  itemsets, until the entire search process is completed, the results is shown in table 5 The generation process of frequent itemsets of multi-level mining is as follows TABLE II.  THE EXPRESSION ANDSUPPORT OF BINARY INFORMATION GRANULE The Name of Information Granule The Expression of Binary Information Granule Support 21*] 000001 1/6 112] 001000 1/6 3**] 010010 2/6 22*] 100100 2/6 111] 100100 2/6 11*] 101100 3/6 12*] 011100 3/6 121] 011100 3/6 2**] 100101 3/6 1**] 111100 4/6 TABLE III.  MULTIPLE MINIMUM SUPPORTS Items MIS Items MIS 1**] 2/3 [21*] 1/12 2**] 1/3 [22*] 1/6 3**] 1/6 [111] 1/6 11*] 1/4 [112] 1/12 12*] 1/4 [121] 1/4 TABLE IV.  TABLE TYPE STYLES The Combination of Items The Expression of Binary Information Granules Support 1**]and[2**] 100100 2/6 1**]and[22*] 100100 2/6 1**]and[3**] 010000 1/6 11*]and[12*] 001100 2/6 


11*]and[22*] 100100 2/6 11*]and[2**] 100100 2/6 11*]and[121] 001100 2/6 12*]and[111] 000100 1/6 12*]and[112] 001000 1/6 12*]and[22*] 000100 1/6 12*]and[3**]  010000 1/6 111]and[2**]  100100 2/6 566 111]and[22*] 100100 2/6 111]and[121] 000100 1/6 112]and[121] 001000 1/6 121]and[22*] 000100 1/6 121]and[3**] 010000 1/6 TABLE V.  TABLE TYPE STYLES The Combination of Items The Expression of Binary Information Granules Support 11*]and[12*]and[22*] 000100 1/6 11*]and[121]and[22*] 000100 1/6 12*]and[111]and[22*]  000100 1/6 12*]and[111]and[2**] 000100 1/6 121]and[111]and[22*] 000100 1/6 121]and[111]and[2**] 000100 1/6 The multiple minimum support of each item is compared with the support assembled by various items shown in the table 4, then we can get the sets of the table 5. From the results, we can get the association rules of the same layer, such as [11 and [12 *] and [22 *], and they all satisfy the minimum support MIS \([11 *] and [12 *] and [ 22  12 22 rules of different layers, such as [12 *] and [111] and [2 they also satisfy the minimum support MIS \([12 *] and [111 and [2  111 2 6, so that we achieve the multi-level association rules Different rules support of different data items produced need to meet different multiple minimum support in order to find these rules, such as the support of [12 *] and [2 **] is sup 12 *] and [2 011100 ? 100101 multiple minimum supports MIS \([12 *] and [2 


MIS \([MIS \([12 2 12 *] and 2 frequency down-generating search space of frequent itemsets V. CONCLUSIONS This paper presents a multi-level association rule mining method based on binary information granules operations and multiple minimum support constraint, with hierarchical encoding and binary granular computing of information granules operation to acquire frequent itemsets at intra level as well as inter level. We give a new definition of support and confidence based on binary information granules. And combined with the definition of multiple minimum supports we effectively restrained the generation search space of frequent itemsets, and found new rules implied in the scarce data items, achieved association rule mining of multi-layer including cross-layer. We got more meaningful rules, and avoided the generated useless rules from the high frequency data items. At last, the method is applied to mining agricultural information association rules, which has been proven to be effective and practical REFERENCES  1] Agrawal R, Imielinski T, Swami A. Mining association rules between sets of items in large databases[C] // Proceedings of the 1993 ACM SIGMOD. Washington:ACM SIGMOD, 1993: 207-216 2] Lin Q.Granular Language and Its Deductive Reasoning.[J Communications of ACM,2002,5\(2 3] Xu Jianfeng, Liu Lan, Qiu Taorong, Hu Ran. On Data Ming Algorithms Based on Binary Numeral Granular Computing. [J].Compter Science 2008,35\(3 4] Liu Qing,Jiang S L.Reasoning about Information Granules Based on Rough Logic.In:RSCTC 2002,L NA I 2475,2002.139?143 5] Ming-Cheng Tseng,Wen-Yang Lin.Efficient mining of generalized association rules with non-uniform minimum support.[J].Data knowledge engineering,62\(2007  567 


of the proposed approach are described. A simulation dataset with 64 items and 10000 transactions were used in the experiments. The dataset followed the exponential distribution. The initial population size P is set at 50, the archive size is set at 30, the crossover rate pc is set at 0.8, and the mutation rate pm is set at 0.001. The parameter d of the crossover operator is set at 0.35 according to Herrera et al.s paper [14] and the set of minimum support values is {3 4%, , 13%}. The experiments were first made for demonstrating the evolution of the Pareto fronts by the proposed approach. The evolution of the Pareto fronts of chromosomes in the archive along with different generations by the proposed approach is shown in Fig. 1 From Fig. 1, we can observe that the solutions were distributed on the Pareto fronts and the final solutions after 500 generations were better than those in different generations. Besides, we can also found that the derived solutions on a Pareto front are trade-offs between the two objectives. It thus depends on the user preference to decide which solutions on a Pareto front are desired. The experiment was then made for comparing the final Pareto front of chromosomes in the archive of the proposed approach with the previous approach [2], and is shown in Fig. 2  250 300 350 400 450 500 550 600 60 70 80 90 100 110 120 130 140 Suitability To tal N um be r o f L 1 


Generation = 0 Generation = 100 Generation = 200 Generation = 300 Generation = 400 Generation = 500  Fig. 1. The Pareto fronts derived by the proposed approach with different generations 450 500 550 600 65 70 75 80 85 90 95 Suitability To tal N um be r o f L 1 The Proposed Approach The Previous Approach  Fig. 2. Comparison results of final Pareto fronts between the proposed approach and the previous approach  From Fig. 2, it is easily to know that the Pareto front derived by using the proposed approach is better than the previous one.  From the experimental results, we thus can conclude that the proposed approach is not only effective in finding an appropriate set of solutions, but also can provide different options to users for further analysis VI. CONCLUSIONS AND FUTURE WORKS The SPEA2 adopted a fine-grained fitness assignment strategy, a density estimation technique, and an enhanced archive truncation method to derive better Pareto solutions 25]. In this paper, we have utilized it to propose a more sophisticated multi-objective approach to find the appropriate sets of membership functions for fuzzy data mining. Two objective functions are used to find the Pareto front. They are minimizing the suitability of membership functions and maximizing the total number of large 1-itemsets respectively Experiments on a simulation dataset were also made to 


show the effectiveness of the proposed approach. The results show that the proposed approach is effective in finding an appropriate set of solutions. Further, the experiments also show that the proposed approach can derive better Pareto front than the previous one [2]. In the future, we will continuously enhance the multi-objective genetic-fuzzy approach for more complex problems REFERENCES 1] C. C. Chan and W. H. Au, Mining fuzzy association rules, The Conference on Information and Knowledge Management, Las Vegas pp. 209-215, 1997 2] C. H. Chen, T. P. Hong, Vincent S. Tseng and L. C. Chen, A multi-objective genetic-fuzzy mining algorithm, The 2008 IEEE International Conference on Granular Computing, 2008 3] C. H. Chen, T. P. Hong, Vincent S. Tseng and C. S. Lee, A genetic-fuzzy mining approach for items with multiple minimum supports, Soft Computing, Vol. 13, No. 5, pp. 521-533, 2009 4] C. H. Chen, Vincent S. Tseng and T. P. Hong, Cluster-based evaluation in fuzzy-genetic data mining, IEEE Transactions on Fuzzy Systems, Vol. 16, No. 1, pp. 249-262, 2008 5] O. Cordn, F. Herrera, and P. Villar, Generating the knowledge base of a fuzzy rule-based system by the genetic learning of the data base IEEE Transactions on Fuzzy Systems, Vol. 9, No. 4, pp. 667674, 2001 6] C. A. Coello, D. A. Van Veldhuizen and G. B. Lamont, Evolutionary Algorithms for Solving Multi-objective Problems, Kluwer Academic Publishers, 2002    7] K. Deb, Multi-objective Optimization Using Evolutionary Algorithms John Wiley & Sons, 2001 8] K. Deb, S. Agrawal, A. Pratab and T. Meyarivan, A fast and elitist multiobjective genetic algorithm: NSGA-II, IEEE Transactions on Evolutionary Computation, Vol. 6, No. 2, pp. 681-695 9] C. M. Fonseca and P. J. Fleming, "Genetic algorithms for multiobjective optimization: Formulation, discussion and generalization," The International Confidence on Genetic Algorithms pp. 416-423, 1993 10] T. P. Hong, C. H. Chen, Y. L. Wu and Y. C. Lee, " Genetic-Fuzzy Data Mining with Divide-and-Conquer Strategy", IEEE Transactions on Evolutionary Computation, Vol. 12, No. 2, pp. 252-265, 2008 11] T. P. Hong, C. H. Chen, Y. L. Wu and Y. C. Lee, "A GA-based fuzzy 


mining approach to achieve a trade-off between number of rules and suitability of membership functions", Soft Computing, Vol. 10, No. 11 pp. 1091-1101. 2006 12] T. P. Hong, C. S. Kuo and S. C. Chi, "Mining association rules from quantitative data," Intelligent Data Analysis, Vol. 3, No. 5, pp 363-376, 1999 13] T. P. Hong, C. S. Kuo and S. C. Chi, "Trade-off between time complexity and number of rules for fuzzy mining from quantitative data," International Journal of Uncertainty, Fuzziness and Knowledge-based Systems, Vol. 9, No. 5, pp. 587-604, 2001 14] F. Herrera, M. Lozano and J. L. Verdegay, Fuzzy connectives based crossover operators to model genetic algorithms population diversity Fuzzy Sets and Systems, Vol. 92, No. 1, pp. 2130, 1997 15] M. Kaya and R. Alhajj, A clustering algorithm with genetically optimized membership functions for fuzzy association rules mining The IEEE International Conference on Fuzzy Systems, pp. 881-886 2003 16] M. Kaya and R. Alhaji, Utilizing genetic algorithms to optimize membership functions for fuzzy weighted association rules mining Applied Intelligence, Vol. 24 ,  No 1, pp. 7-15, 2006 17] M. Kaya and R. Alhajj, Integrating multi-objective genetic algorithms into clustering for fuzzy association rules mining, The IEEE International Conference on Data Mining, pp. 431-434, 2004 18] M. Kaya, Multi-objective genetic algorithm based approaches for mining optimized fuzzy association rules, Soft computing, Vol. 10 pp. 578-586, 2006 19] C. Kuok, A. Fu and M. Wong, Mining fuzzy association rules in databases, SIGMOD Record, Vol. 27, No. 1, pp. 41-46, 1998 20] Y. C. Lee, T. P. Hong and W. Y. Lin, Mining fuzzy association rules with multiple minimum supports using maximum constraints, Lecture Notes in Computer Science, Vol. 3214, pp. 1283-1290, 2004 21] H. Roubos and M. Setnes, Compact and transparent fuzzy models and classifiers through iterative complexity reduction, IEEE Transactions on Fuzzy Systems, Vol. 9, No. 4, pp. 516-524, 2001 22] J. D. Schaffer, Multiple objective optimization with vector evaluated genetic algorithms, The International Conference on Genetic Algorithms, pp. 93-100, 1985 23] C. H. Wang, T. P. Hong and S. S. Tseng, Integrating membership functions and fuzzy rule sets from multiple knowledge sources, Fuzzy Sets and Systems, Vol. 112, pp. 141-154, 2000 24] S. Yue, E. Tsang, D. Yeung and D. Shi, Mining fuzzy association rules with weighted items, The IEEE International Conference on Systems 


Man and Cybernetics, pp. 1906-1911, 2000 25] E. Zitzler, M. Laumanns and L. Thiele, "SPEA2: Improving the strength Pareto evolutionary algorithm for multiobjective optimization," Proc. Evolutionary Methods for Design, Optimization and Control with App. to Industrial Problems \(Barcelona, Spain, 2001 pp. 95-100 


9] Y. Gong, S. Mabu, C. Chen, Y. Wang, and K. Hirasawa, "Intrusion detection system combining misuse detection and anomaly detection using genetic network programming," in Proc. of the SICE-ICASE International Joint Conference, 2009, pp. 3463-3467 10] "Kddcupl999 data. " [Online]. Available: kdd.ics.uci.eduldatabases kddcup99/kddcup99.htrn1 11] R. P. Lippmann, D. J. Fried, I. Graf, J. Haines, K. P. Kendall, D. Mc Clung, D. Weber, S. Webster, D. Wyschogrod, R. K. Cunningham and M. A. Zissman, "Evaluating intrusion detection systems: The 1998 darpa offline intrusion detection evaluation," in Proc. of DARPA Information Survivability Conference and Exposition 2000, vol. 2 IEEE Computer Society Press, 2000 12] K. Shimada, K. Hirasawa, and J. Hu, "Class association rule mining with chi-squared test using genetic network programming," in Proc. of the IEEE International Conference on Systems, Man and Cybernetics 2006, pp. 5338-5344 


n-dimension data cube\( I1  I k Support=sup_count/total_count 2 3 4. Performance Analysis Example 2 Lets talk about a practical problem just like the status of sales. Assume that we will mine the association rules involved 4 dimension attributes of sales, the minsup=25%. First of all, using OLAP technology to build a 4-D data cube and the 4 dimension attributes are: time location, item, and supplier. For location dimension which contains area, country and so on, we choose province level We use brand level for item dimension, company level for supplier dimension. Time dimension can be divided as Q1 1-3 4-6 7-9 10-12 location\(P1,P2,L1,L2 York, item\(B1,B2,B3,B4 C1,C2,C3 sales data cube can be generalized like this Graphic 2: The 4-Dimension Data Cube of Sales The details of this sales data cube are in the table follow: The amount of cells is 100 Location Time Item Supplier Count Cell-1 P1 Q1 B1 C2 5 Cell-2 P1 Q3 B1 C1 2 Cell-99 L1 Q3 B1 C1 3 Cell-100 L2 Q4 B1 C3 11 Table 2: The details data table of sales data cube We use original Apriori_Cube Algorithm to find frequent predicate set with minsup_num= 25%*100=25 According the data table we calculate that sup_count of every member of dimension L is \(P1:8, P2:5, L1:1, L2:24 and also T, I, S. So there is the process Graphic 3: The processes of old algorithm As we know, through comparing with the minsup_num dimension location has no one frequent 1-predicate set, so that there have no frequent 4-predicate set in the output by the original algorithm. But users are interested in the Candidate 1-Predicate set L T I S P1 P2 L1 


L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate set T I S Q1 Q3 B2 B3 C1 C2 I1 I 2 C1              C2                 C3 2 12 1 3 5 1 6 2 12 8 11 Q 1  Q 2  Q 3  Q 4 P 1 P 2 L 1 L 2 Candidate 2-Predicate set Q1,B2},{Q1,B3 Q1,C1},{Q1,C2 Q3,B2},{Q3,B3 


Q3,C1},{Q3,C2 Candidate 3-Predicate set Q1,B2,B3}{Q 1,Q3,B2},{Q3 C1,B2 Frequent 2-Predicate set Q1,B2}{Q1,B 3},{Q3,B2 Q3,C1 Output: 1L U 2L U 3L Frequent 3-Predicate set Q1,B2,C1},{Q1,B3,B2 Q1,B2,B3  6 level from the same dimension and SP to pruning Candidate 3 Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special requirement on location B by users 7 1 max_support of every dimension 1 frequent 4-predicate set which contains location dimension And the candidate 3-predicate set {Q1,Q3,B2} is useless because when we mine multi-dimension association rules we should follow that one predicate set contains no more than one different level from the same dimension even the sup_count >minsup_num when there are no requirements from users. But this time, users require that different members from dimension item can exist at the same time So we redo this mining process by the improved Algorithm Graphic 4: The processes of improved algorithm At last, we got the frequent 4-predicate set which the users are interested in, and then can format the rules 5. Conclusion In this paper, we have studied issues and methods on efficient mining of multi-dimension association rules based on data cube. With the development of technology, the size 


of database has become much huge than ever before unimaginable. So the multi-dimension model of database based on data cube has become useful and popular relatively how to mine useful multi-dimension association rules based on this model has been important too. Among the algorithms for the problems we choose the most classify one to improve, to make it more efficient and useful An efficient algorithm, Apriori_Cube_Improved, has been developed which explores the multi-dimension association rules. First, we use max_support and min_support of every frequent 1-predicate set to check the levels of dimensions, and then adjust the data cube by roll-up and drill-down operation immediately. This step is embed in the process of mining association rules, so it makes the rules much more useful and flexible; second applying one predicate set contains no more than one different level from the same dimension and SP pruning can help reduce the amount of predicate set, especially with the great growing of the number. So the speed of algorithm improved is faster than ever At last, there are also many other interesting issues and flaws of the algorithm which call for further study including efficient mining multi-dimension association rules of complex measures and so on. Finally, I should thank all the people who have give me help for this paper Reference 1] Jiawei Han, Micheline Kamber. Data Mining Concepts and Techniques. China Machine Press, 2007 2] Agrawal R, Imielinski T and Swami A. Mining association rules between sets of items in large database. Proc. of the ACM SIGMOD Conf., Washington DC, 1993 3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional Association Rule Mining Method Based on Data Cube Computer Engineering, China,2003 4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research Multi-dimensional Association Rule Ming Based on Apriori Science Technology and Engineering, China, 2009 5] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proc. of the 20th Int.Conf. on VLDB Santiago, Chile, 1994 6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang Mining Multi-Dimension Constrained Gradients in Data 


Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy 2001 7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE Transactions on Knowledge and Data Engineering, 2000 Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Candidate 1-Predicate set L T I S P L Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate L T I S P L 


Q1 Q3 B2 B3 C1 C2 Frequent 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3 Output L1 U L2 U L3 U L4 The Dimension need to be adjust Dimension: Location Operation: Roll-up to increase the level of this dimension Result P\(P1,P2 L1,L2 2 3 2 3 The Features Of Every 1-Predicate Set Location: min_support=13 max_support=25 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 4 5 4 max_support of every dimension 5 minsup_num=25, and no one has to be adjusted Frequent 3-Predicate set Q3,C1,B2},{Q1,B2,L Q1,B3,L},{Q3,P,B2 Candidate 4-Predicate set 


Q3,C1,B2,P},{Q1,B2,B3,L}\(7 The Features of Every 1-Predicate Set Location: min_support=1 max_support=24 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 Candidate 2-Predicate set Q1,B2},{Q1,B3},{Q1,C1 Q1,C2},{Q3,B2},{Q3,B3 Q3,C1},{Q3,C2},{P,Q1 P,Q3},{P,B2},{P,B3},{P,C1 P,C2},{L,Q1},{L,Q3 L,B2},{L,B3},{L,C1},{L,C2 Frequent 2-Predicate set Q1,B2}{Q1,B3},{Q3,B2 Q3,C1},{P,Q3},{L,Q1 6 Candidate 3 -Predicate set Q3,C1,B2},{Q3,C1,P},{Q1,B 2,L},{Q1,B3,L},{Q3,P,B2},{Q 1,B2,B3  6 level from the same dimension and SP to pruning Candidate 3 Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special requirement on location B by users 7 1 max_support of every dimension 1 frequent 4-predicate set which contains location dimension And the candidate 3-predicate set {Q1,Q3,B2} is useless because when we mine multi-dimension association rules we should follow that one predicate set contains no more than one different level from the same dimension even the sup_count >minsup_num when there are no requirements from users. But this time, users require that different 


members from dimension item can exist at the same time So we redo this mining process by the improved Algorithm Graphic 4: The processes of improved algorithm At last, we got the frequent 4-predicate set which the users are interested in, and then can format the rules 5. Conclusion In this paper, we have studied issues and methods on efficient mining of multi-dimension association rules based on data cube. With the development of technology, the size of database has become much huge than ever before unimaginable. So the multi-dimension model of database based on data cube has become useful and popular relatively how to mine useful multi-dimension association rules based on this model has been important too. Among the algorithms for the problems we choose the most classify one to improve, to make it more efficient and useful An efficient algorithm, Apriori_Cube_Improved, has been developed which explores the multi-dimension association rules. First, we use max_support and min_support of every frequent 1-predicate set to check the levels of dimensions, and then adjust the data cube by roll-up and drill-down operation immediately. This step is embed in the process of mining association rules, so it makes the rules much more useful and flexible; second applying one predicate set contains no more than one different level from the same dimension and SP pruning can help reduce the amount of predicate set, especially with the great growing of the number. So the speed of algorithm improved is faster than ever At last, there are also many other interesting issues and flaws of the algorithm which call for further study including efficient mining multi-dimension association rules of complex measures and so on. Finally, I should thank all the people who have give me help for this paper Reference 1] Jiawei Han, Micheline Kamber. Data Mining Concepts and Techniques. China Machine Press, 2007 2] Agrawal R, Imielinski T and Swami A. Mining association rules between sets of items in large database. Proc. of the ACM SIGMOD Conf., Washington DC, 1993 3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional Association Rule Mining Method Based on Data Cube 


Computer Engineering, China,2003 4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research Multi-dimensional Association Rule Ming Based on Apriori Science Technology and Engineering, China, 2009 5] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proc. of the 20th Int.Conf. on VLDB Santiago, Chile, 1994 6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang Mining Multi-Dimension Constrained Gradients in Data Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy 2001 7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE Transactions on Knowledge and Data Engineering, 2000 Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Candidate 1-Predicate set L T I S P L Q1 Q2 Q3 Q4 B1 B2 


B3 B4 C1 C2 C3 Frequent 1-Predicate L T I S P L Q1 Q3 B2 B3 C1 C2 Frequent 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3 Output L1 U L2 U L3 U L4 The Dimension need to be adjust Dimension: Location Operation: Roll-up to increase the level of this dimension Result P\(P1,P2 L1,L2 2 3 2 3 The Features Of Every 1-Predicate Set Location: min_support=13 max_support=25 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 4 5 


4 max_support of every dimension 5 minsup_num=25, and no one has to be adjusted Frequent 3-Predicate set Q3,C1,B2},{Q1,B2,L Q1,B3,L},{Q3,P,B2 Candidate 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3,L}\(7 The Features of Every 1-Predicate Set Location: min_support=1 max_support=24 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 Candidate 2-Predicate set Q1,B2},{Q1,B3},{Q1,C1 Q1,C2},{Q3,B2},{Q3,B3 Q3,C1},{Q3,C2},{P,Q1 P,Q3},{P,B2},{P,B3},{P,C1 P,C2},{L,Q1},{L,Q3 L,B2},{L,B3},{L,C1},{L,C2 Frequent 2-Predicate set Q1,B2}{Q1,B3},{Q3,B2 Q3,C1},{P,Q3},{L,Q1 6 Candidate 3 -Predicate set Q3,C1,B2},{Q3,C1,P},{Q1,B 2,L},{Q1,B3,L},{Q3,P,B2},{Q 1,B2,B3  


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


