Recommending APIs for mashup completion using association rules mined from real usage data Boris Tapia Romina Torres Hernan Astudillo Pablo Ortega Departamento de Inform 264 atica Universidad T 264 ecnica Federico Santa Mar 264 021a Valpara 264 021so Chile Email f btapia romina hernan portega g inf.utfsm.cl Abstract 227Mashups are becoming the de facto approach to build customer-oriented Web applications by combining several Web APIs into a single lightweight rich customized Web front-end To help mashup builders to choose among a plethora of available APIs to assemble in their mashups some existing recommendation techniques rank candidate APIs using popularity 050a social measure\051 or keyword-based measures 050whether semantic or unveri\002ed tags\051 This article proposes to use information on co-usage of APIs in previous mashups to suggest likely candidate APIs and introduces a global measure which improves on earlier local co-API measures The gCAR 050global Co-utilization API Ranking\051 is calculated using association rules inferred from historical API usage data The MashupRECO tool combines gCAR and a keywordbased measure to avoid the 224cold-start\224 problem for new or unused APIs Evaluation of MashupRECO versus the keyword search of the well-known ProgrammableWeb catalog show that the tool reduces the search time for comparable degree of completeness Keywords Web mashup Web API recommender system association rules frequent itemsets I I NTRODUCTION Mashups are becoming the de facto approach to build customer-oriented Web applications by combining operations on several Web APIs 050Application Programming Interfaces\051 into a single lightweight rich customized Web front-end They allow to develop complete applications by searching composing and executing functionality provided by external sources Typically mashups are built with more than one API 050according to ProgrammableWeb 1 45 of the registered mashups are built with more than one API\051 In order to take advantage of the previous compositions made by mashup developers we expect the API discovery process to have a memory on the iterative items selection API Web catalogs provide besides APIs documentation the valuable information about which APIs are used on the registered mashups In our previous work we ar gued the need to combine descriptions with social information where description-based techniques can be leveraged by social indicators This combination allows the discovery of candidates that would have passed unnoticed because 1 http://www.programmableweb.com of their poor quality descriptions or their low popularity We discussed in ho w our combined approach enriches keyword-based search with the social information provided by the mashup community We also argued that using this balanced approach could reduce the cold start problem that new APIs exhibit on a market with a preferential attachment trend on their usage We proposed two indicators the Web API Rank 050WAR\051 which measures API utilization over time and the Co-utilization API Rank 050CAR\051 which measures the joint usage of a group of APIs In this work we calculate the CAR indicator using symbolic methods for knowledge discovery in databases speci\002cally extracting association rules to discover complementary APIs to use in the construction of a mashup The remainder of the paper is organized as follows Section II discusses the motivation of our research Section III discusses related work Section IV identify the speci\002c problem addressed in this paper Section V presents the background needed to introduce the approach Section VI and section VII present the complete approach to obtain candidate components to build a mashup in an iterative fashion Section VIII presents the implementation details of the MashupRECO tool Section IX shows the experiments performed to test the effectiveness and ef\002ciency of our approach and 002nally section X draws conclusions II M OTIVATION In this section we describe a case in which a workshop organizer wants to create a mashup for the venue section of the workshop's website The 002rst step is to identify the speci\002c functionalities needed 050see Figure 1\051 Typically in a venue section we can 002nd information regarding the celebration's place the city and entertainment nearby hotels photos of the city attractions and in some cases videos Then the organizer needs APIs to display a map of the place locate points of interest 002nd hotels show photos and videos and as the aim is to keep assistants updated on the latest news send updates in an easy manner Using the ProgrammableWeb search site the organizer tries to search an API that provides given a location photos ProgrammableWeb uses a search engine based on tags then 
2011 30th International Conference of the Chilean Computer Science Society 1522-4902/12 $26.00 © 2012 IEEE DOI 10.1109/SCCC.2011.12 83 
2011 30th International Conference of the Chilean Computer Science Society 1522-4902/12 $26.00 © 2012 IEEE DOI 10.1109/SCCC.2011.12 83 
2011 30th International Conference of the Chilean Computer Science Society 1522-4902/12 $26.00 © 2012 IEEE DOI 10.1109/SCCC.2011.12 83 


Figure 1 Mashup Brainstorm the search could be composed of the words photo and location If providers did not describe their APIs with representative tags then their probabilities to be discovered even when they could be good options are low There are some problems with the ProgrammableWeb site we can notice differences between the simple and advanced search For instance when the organizer searches for photo and location the number of results between using the simple and advanced searching interface increases from 0 to 6 Nevertheless ProgrammableWeb has a remarkable value for the community because is an open live mashup catalog with an API that provides access to all the registry waiting to be properly mined Following the example the organizer obtains six results for the 002rst search trying to 002nd APIs that return photos given a location Assuming the organizer selects the 002rst API 050the results of the 002rst search are not ranked because it is a binary search the tag is present or not\051 now he needs to 002nd the other APIs Ideally the organizer should be able to save the current selection and then perform the next search constrained by that selection This option is not available in ProgrammableWeb Although the user can see a list of the other APIs used in mashups of a given API this information is not retained for later use and it is not possible to know this for a group of APIs Thus the richness of the compositions is not being exploited to help the selection process III R ELATED W ORK Given the increasing trend of major 002rms providing APIs for public use mashup community is rapidly expanding There are studies characterizing the mashup ecosystem as a API-Mashup network which intended to e xploit this information In the authors proposed the serviut score to rank APIs based on their utilization and popularity To calculate the serviut score they also considered the number of mashups that use the given API but also other aspects that we believe are too ambiguous to be considered such as classifying mashups in the same category as the API Even according to ProgrammableWeb mashups are not classi\002ed in categories because by de\002nition a mashup is a mix of different Web APIs therefore is quite dif\002cult to classify them in functional categories According to our experiments the taxonomy of APIs and mashups are quite different In authors proposed a social technique to mine annotated tags of mashups and APIs in order to recommend mashup candidates managing the cold start problem for new competitors The problem of using tags is that they are not reusable between different catalogs Then by using tags we obtain speci\002c taxonomies that are not generic enough to be used with APIs obtained from different catalogs Web API authors do not necessarily use the same tags to describe them they typically adapt them according to the tags that are used on each catalog In authors proposed MashupAdvisor that also assist developers to build mashups Similar to our approach MashupAdvisor suggests APIs that could be part of the mashup under construction using a probabilistic approach based on their popularity in the mashup repository But because MashupAdvisor assists the mashup building process instead of only the selection this approach is based on speci\002c inputs and outputs Typically only Web service APIs have this data Mostly because of their complexity and lack of standards general APIs do not have interface information of each operation Then this approach performs well over Web services but not over general Web APIs Even when the results are encouraging they actually simulate the data of ProgrammableWeb to conduct the experiments In authors proposed ServiceRank to dif ferentiate ser vices from a set of functional-equivalent services based on their quality and social aspects The problem is that it needs to access data that providers may not be willing to give such as the response time and availability measurements Also because providers publish their own measurements this process could be not completely reliable In authors proposed MatchUp a tool that supports mashup creators to locate components to mash up based on the current component selection and a complete database that describes which components have been used in the different mashups 050at the input/output level\051 The algorithm performs well but is only feasible at level of intra organization because in general this information is not shared or public IV T HE PROBLEM Typically mashups are built with more than one API Even more these APIs are iteratively selected and previous selections in\003uence current ones Then the problem is how can developers restrict and guide their search given the current selection of APIs 
84 
84 
84 


V B ACKGROUND A Functional-equivalent APIs and Formal Concept Analysis Due to the increasing proliferation of APIs we assume that for each API there is a set of functional-equivalent APIs that are natural candidates to substitute it Service discovery can be improved by maintaining an API classi\002cation that helps users to drive their search for a given functionality However these classi\002cations are typically handled manually by API providers or catalog owners which could lead to rigid and poor classi\002cation systems with low or erroneous retrieval capabilities To represent functional-equivalent APIs we use a latticebased classi\002cation approach which is a symbolic data mining technique used for extracting a set of concepts organized within a hierarchy This structure allows us to arrange nodes of functionality de\002ned by a set of features and a set of APIs that share those features We decided to use lattices because as has ar gued a canonical approach does not deal with objects belonging to multiple categories and lattices do Lattices also allow category overlap Formal Concept Analysis 050FCA\051 is a mathematical theory of data analysis using formal contexts and concept lattices The approach takes as input a matrix specifying a set of objects and their properties called attributes and 002nds both the clusters of attributes and the clusters of objects in the input data where an object cluster is the set of all objects that share a common subset of attributes and a property cluster is the set of all attributes shared by one of the natural object clusters A concept is a pair containing both a property cluster and its corresponding object cluster Formally given a formal context 050 A K I 051  where A is in this case a set of APIs K is a set of keywords 050attributes\051 which describes functionality and a binary relation I 022 A 002 K which speci\002es which APIs have which attributes A formal concept of the context A K I is a pair 050 X Y 051 where X 022 A  Y 022 K  where the set X is called the extent 050the set of APIs that belong to this concept\051 and Y is called the intent of the concept 050which terms better describe this concept\051 A concept 050 X Y 051 is a subconcept of 050 U V 051 if X 022 U  then 050 U V 051 is called a superconcept of 050 X Y 051  Then we can extract the complete lattice using this partial order relation between concepts 050  051 where a lattice at least has a bottom and a top concept in which the relation bottom  top is true In our previous work we provided a concise example of building a lattice for APIs The advantage of building a lattice representing the functionalities provided by APIs is we can given a required functionality navigate the lattice to 002nd a set of functionalequivalent APIs for the requirement When a composer is searching for an API he is actually searching for a set of functionalities To represent this search he uses a set of keywords This query is transformed into a virtual concept Table I F ORMAL CONTEXT   APIs vs functionality  T1  T2  T3    A1  x  x  x   A2  x  x    A3   x  x   A4    x   which intent is the set of keywords Then we na vig ate the lattice in order to 002nd the concept which has exactly the same intent in order to retrieve its extent If there is no concept that matches the intent the virtual concept must be arranged within the lattice From the potential set of parents of this virtual node 050nodes which intent is a subset of the virtual concept's intent\051 we select the concept\050s\051 whose intent is maximum and then we suggest the extent of that concept\050s\051 Similar to we w alk the semantic graph from the virtual concept to their ancestors The distance from a child node and their direct parents is one Then the nodes with minimal distance to the virtual node are the best candidates to be recommended In this work we are only considering nodes with distance of one but it is possible to enrich the searching process by walking the graph at longer distances Algorithm 1 describes this procedure  Algorithm 1 Searching functional-equivalent APIs  Require The set A of all the available APIs Require The set K of all the selected keywords representing the functionalities extracted from descriptions Require The set C of all the concepts in the lattice 1 Let C I i 022 A be the intent of the concept i  2 Let C E i 022 K be the extent of the concept i  3 Let T be the set of terms of the query Q  4 Remove stop words from T 5 for all t 2 T do 6 Stem t  7 end for 8 if 9 i 2 C j C I i  T then 9 return C E i 10 else 11 Let v be a virtual concept with intent C I v  T and extent C E i    12 Let P v  f p 2 C j C I p 032 T g be the set of potential parent concepts of v  13 return C E p of the concept p 2 P v with maximum j C I p j 14 end if  Consider the following example We have a set of APIs A1 TwitterVision A2 Twitter A3 Rummble and A4 Google Maps which expose different functionalities represented by the terms T1 microblogging T2 social and T3 mapping Table I shows which functionalities provide each API We call this table the context that is represented by the set of relations R between APIs and functionalities where R i;j is blank if the functionality j is not provided by the API i  In Figure 4 we can appreciate the lattice in a visual representation where we can see at the top concept the whole dataset Because we are considering 
85 
85 
85 


Figure 2 API taxonomy APIs of any kind of functionality we 002nd the top concept empty On the medium-level we 002nd formal concepts as the set f TwitterVision Twitter Rummble g  that provides 223social\224 functionality or the set f TwitterVision Rummble GoogleMaps g  that provides 223mapping\224 functionality If we browse deeper we can see concepts as the set f TwitterVision Twitter g that provides two functionalities at the same time 223microblogging\224 and 223social\224 or the set f TwitterVision Rummble g  that provides 223social\224 and 223mapping\224 functionalities At the bottom level and for this particular small dataset we 002nd one Web API that provides all functionalities at the same time 223social\224 223mapping\224 and 223microblogging\224 B Mining Rules From the context 050 A K I 051 described in the previous section the frequent itemsets can be extracted Basically this process consists of extracting from a formal boolean context the sets of APIs sharing a set of common properties From these frequent itemsets it is possible to generate association rules of the form L 000 R relating a subset of properties on the left side with a subset of properties on the right side To extract the association rules we build a different context 050 M A I 051  where M represents the set of mashups A the set of APIs and I the binary relation which speci\002es which APIs conform a mashup An item corresponds to an API which is part of a mashup and an itemset to a set of APIs An itemset is said to be frequent if its support is greater than a given frequency threshold An association rule is de\002ned as the support of the itemset L u R 050where u denotes the union of the itemsets\051 The con\002dence of the rule is de\002ned as the quotient support 050 L u R 051  support 050 L 051  i.e the probability of R knowing L  A rule is said to be e xact Figure 3 API Collaboration network if its con\002dence is 1  i.e support 050 L u R 051  support 050 L 051  otherwise the rule is partial VI P ROPOSAL  G LOBAL C O UTILIZATION API R ANKING In we introduced a social indicator of API usage to support the discovery process In we g athered this information and proposed the Co-utilization API Rank 050CAR\051 Using the ProgrammableWeb catalog which provides APIs and mashups information we built an af\002liation network between both as shown in Figure 3 where each API is represented as a node in the graph and each edge represents the joint usage of the connected APIs within a mashup Then each time a user selects an API a group of 223coAPIs\224 is suggested and ranked according to the number of mashups where this assertion is valid In authors mined association rules of an open scienti\002c work\003ow community in order to answer questions as 223given services which I plan to use which other services are usually used together with them?\224 Same question answers our CAR Our aim is to support composers to build mashups using a co-API-driven search according they select APIs This way the effort to discover all the needed APIs is reduced In this work we implement the CAR by extracting and exploiting association rules from the mashups and the APIs which conform them First we build the context 050 M A I 051  From this context we select the frequent itemsets with support greater than 1 Second we extract the association rules and build our knowledge base only considering the rules with con\002dence greater than 50  For instance the association rule f LinkedIn del:icio:us g  f T witter g 050supp=10 
86 
86 
86 


conf=1.000 suppL=10 0.17 suppR=589  pro vides rel e v ant information to the composer by saying that in the 100 of the cases 050in the dataset\051 where the LinkedIn and del.icio.us APIs were used the Twitter API was also used From this rule we can also know that there are exactly 10 mashups that have mashed the three named APIs there are 10 mashups which have mashed the two 002rst ones and there are 589 mashups which use the Twitter API 050which represents the 10 of the entire dataset\051 Third we calculate the CAR and the list of recommended APIs using the knowledge base Algorithm 2 describes this calculation  Algorithm 2 API recommendation and CAR calculation  Require The set A of all APIs Require The set H of association rules Require The set S of selected APIs 1 Let C be the set of recommended APIs 2 Obtain association rules H S  f h 2 H j S 022 L h g  where h  f L h 000 R h g  3 if H S 6   then 4 for all h 2 H S do 5 Rank r 2 R h according to the con\002dence c h of the rule h  6 Add r to C  7 end for 8 else 9 Let J  A be the set of the intersection of the right side of the association rules of S  10 for all s 2 S do 11 Let T   be the set of the right side of the association rules of s  12 Obtain association rules H s  f h 2 H j s 022 L h g  where h  f L h 000 R h g  13 for all h 2 H s do 14 T  T  R h 15 end for 16 J  J 134 T 17 Rank j 2 J 134 T as the maximum con\002dence between the intersected rules 18 end for 19 C  J 20 end if 21 return C  VII O UR APPROACH Given a set S of APIs already selected and a query Q which describes in terms of keywords the required functionality our proposal is to recommend other APIs which in one hand provide the required functionality and on the other are popular enough inside this subspace and there is evidence of its co-use with the current selection of APIs In other words our approach consists of 002rst exploring the searching space and then exploit this subspace using the social information Similar to composers can set up the importance of the social indicators to each particular problem Algorithm 3 summarizes the complete approach VIII I MPLEMENTATION Our dataset was extracted from the well-known ProgrammableWeb's catalog where the number of published  Algorithm 3 API discovery and recommendation  Require The set M of all mashups 1 Let K A i  f t 1   t m g be the set of keywords de\002ning the type of API the composer searches at step i  2 Let I be the number of APIs that will comprise the mashup 3 Let S be the initial empty set of selected APIs 4 for i  1 to I do 5 Remove stop words from K A i 6 Stem K A i 7 Using K A i obtain the API category C A which intent is closest to K A i as explained in section V 8 Get the APIs 2 C A  f a 1   a K g 9 for k  1 to K do 10 Calculate semantic rank R k given the frequency matrix of the API terms 11 Let n k the number of mashups m 2 M in which AP I k is used 12 Let n max  max 1 024 k 024 K n k 13 Calculate global WAR of AP I k as W AR G k  n k  n max 14 Calculate the 002nal rank F R k of AP I k as F R k  013 001 W AR G k  0501 000 013 051 001 R k 15 end for 16 The user selects one API adding it to S  probably the one with highest 002nal rank F R k 17 Obtain the set C of recommended APIs as described in Algorithm 2 18 end for  Figure 4 MashupRECO architecture APIs has increased on around 30 from December 2010 to mid-July of 2011 period in which the number of mashups has also increased in about 20 We reuse the MashupRECO prototype tool 2 in order to evaluate our results with users In this case the MashupRECO socially in\003uence using the mining CAR the discovery results obtained by a keyword-based search We are working with an association rules set with con\002dence 025 50 and support 024 6 050empirically tuned\051 The MashupRECO tool contains three main components that give support to the discovery and recommendation process The Data collector  which extracts descriptions and usage data of APIs and mashups the Taxonomy builder that takes those descriptions to create functional categories of 2 http://bunker.toeska.cl/mashup-reco 
87 
87 
87 


APIs and the Rules extractor that obtains association rules for APIs based on the joint usage of APIs within mashups The architecture of MashupRECO is presented in Figure 4 A Data collector This module obtains data from external catalogs It gathers information about APIs and mashups A snapshot was extracted from ProgrammableWeb's catalog in May of 2011 collecting 3318 APIs and 5848 mashups The data includes which APIs has been used to create each mashup their descriptions tags protocols among others B Taxonomy builder In order to 002nd functional equivalent APIs we construct a taxonomy using the textual descriptions of APIs First we extract a set of tokens from the descriptions and using the TreeTagger tool 3  we 002lter uncommon nouns and stop words From the resulting set of terms we must choose those relevant enough to represent the different objects For this task we used Term Frequency/Inverse Document Frequency 050TF/IDF\051 TF/IDF is a common mechanism in Information Retrieval for generating a robust set of representative keywords from a corpus of documents Then using Coron System 4  we generate a concept lattice that allows us to hierarchically arrange nodes 050or concepts\051 each one containing a set of APIs and a set of terms The number of concepts generated was 754  2 050inner nodes plus top and bottom nodes\051 and the concepts contained no more than 5 terms C Rules extractor For the recommendation system we take the information about usage of APIs within mashups and construct a formal context not using APIs and keywords but using mashups and APIs which leads us to obtain association rules between APIs based on their past usage within mashups We used Coron to obtain a total of 118046 rules with different values of con\002dence Using these rules is possible to recommend APIs by calculating the CAR for an individual or a set of APIs Table II E XCERPT OF THE ASSOCIATION RULES WITH 100 CONFIDENCE   Antecedent  Consequent  Support    TwitPic  Twitter  25/.43   YouTube Panoramio  Google Maps  12/.21   Twitter YouTube Yelp  Flickr  7/.12   US Postal Service UPS  FedEx  6/.10   Table II shows an excerpt of the association rules with 100 of con\002dence For instance if the user already selected the TwitPic API we could suggest to complete the 3 http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger 4 http://coron.loria.fr mashup with Twitter because all the mashups that have used TwitPic before 050100 con\002dence\051 have been mashed with Twitter We can also support this suggestion arguing that this information is valid for 25 mashups that represents the 0.21 of the total number of mashups IX E XPERIMENTS An experiment was carried to evaluate the tool versus searching directly on ProgrammableWeb's catalog In order to compare both approaches the number of steps or different searches required to 002nd suitable APIs for a mashup were measured Ten target mashups with different number of APIs were evaluated The results are listed in Table III where for each case we have the number of APIs of the mashup the number of steps required using ProgrammableWeb's catalog 050PW\051 and MashupRECO tool 050MR\051 and the relative variation using the tool versus the catalog The results show that we can reduce in average 37 of the steps required to obtain relevant APIs for a mashup When the number of APIs increases the tool can make recommendations based on the current selection and gives the possibility to reduce steps in the process One of the dif\002culties found using the catalog is the need to supply keywords that match exactly the existing tags This becomes more problematic when two or more tags are used together if one of them does not match a tag the whole query is affected and no results are listed Another drawback is that all the items listed in the results have the same importance they will appear on the list if all the keywords in the query are present in the tags of the item By default the list is sorted alphabetically but can be sorted also by date popularity and category MashupRECO solves these problems by applying a preprocessing stage where the words are stemmed and then used to create the categories of APIs This allows to 002nd results not only using exact keywords but also variations of them Also the results are ranked according to two measures one is a rank based on the closeness of the query to a category in the taxonomy and the other is a social rank 050the Web API Rank\051 based on the past usage of the API These two ranks are combined using a trade-off factor to sort the results Table III E XPERIMENT RESULTS   Case  APIs  Steps PW  Steps MR  Variation    1  2  2  2  0   2  2  2  2  0   3  2  3  3  0   4  3  7  5  29   5  4  8  5  38   6  4  4  1  75   7  4  4  2  50   8  5  6  3  50   9  6  6  2  67   10  6  7  3  57   
88 
88 
88 


X C ONCLUSIONS In this work we presented a semiautomatic approach to support developers in the iterative and explorative process of selecting APIs for a mashup The decisions made by the community can be exploited to make recommendations at each stage of the process based on the current selection of APIs For this task the MashupRECO tool extracts association rules from historical data to correlate APIs Results show that steps can be reduced from the selection process using these recommendations A CKNOWLEDGEMENTS This work was partially funded by projects ContentCompass 050FONDEF D08i1155\051 DGIP-24.08.58 050UTFSM\051 CCTVal 050CONICYT Basal\051 and CAMPUS 050STIC-Amsud\051 Torres work was funded by Conicyt Basal FB0821 Project FB.02PG.11 and UTFSM PIIC R EFERENCES  R T orres B T apia and H Astudillo 223Impro ving web API discovery by leveraging social information.\224 in ICWS  IEEE Computer Society 2011 pp 744\226745  B T apia R T or res and H Astudillo 223Simplifying mashup component selection with a combined similarityand socialbased technique,\224 in Proceedings of the 5th International Workshop on Web API s and Service Mashups  ser Mashups 11 New York NY USA ACM 2011 pp 8:1\2268:8  S Y u and C J W oodard 223Service-oriented computing 227 ICSOC 2008 workshops,\224 G Feuerlicht and W Lamersdorf Eds Berlin Heidelberg Springer-Verlag 2009 ch Innovation in the Programmable Web Characterizing the Mashup Ecosystem pp 136\226147  K Gomadam A Ranabahu M Nag arajan A P  Sheth and K Verma 223A faceted classi\002cation based approach to search and rank web APIs,\224 in Proceedings of the 2008 IEEE International Conference on Web Services  ser ICWS 08 Washington DC USA IEEE Computer Society 2008 pp 177\226184  K Goaran y  G K ulc zycki and M B Blak e 223Mining social tags to predict mashup patterns,\224 in Proceedings of the 2nd international workshop on Search and mining user-generated contents  ser SMUC 10 New York NY USA ACM 2010 pp 71\22678  H Elme lee gy  A Iv an R Akkiraju and R Goodwin 223Mashup Advisor A recommendation tool for mashup development,\224 in Proceedings of the 2008 IEEE International Conference on Web Services  ser ICWS 08 Washington DC USA IEEE Computer Society 2008 pp 337\226344  Q W u A Iyeng ar  R Subramanian I Rouv ellou I Silv aLepe and T Mikalsen 223Combining quality of service and social information for ranking services,\224 in Proceedings of the 7th International Joint Conference on Service-Oriented Computing  ser ICSOC-ServiceWave 09 Berlin Heidelberg Springer-Verlag 2009 pp 561\226575  O Greenshpan T  Milo and N Polyzotis 223 Autocompletion for mashups.\224 PVLDB  vol 2 no 1 pp 538\226549 2009  C Roth and P  Bour gine 223Lattice-based dynamic and o v er lapping taxonomies The case of epistemic communities,\224 Scientometrics  vol 69 no 2 pp 429\226447 2006  R W ille 223F ormal concept analysis as mathematical theory of concepts and concept hierarchies,\224 in Formal Concept Analysis  2005 pp 1\22633  C Carpineto and G Romano 223Exploiting the potenti al of concept lattices for information retrieval with CREDO.\224 J UCS  vol 10 no 8 pp 985\2261013 2004  A Napol i 223 A smooth introduction to symbolic methods for knowledge discovery,\224 in Categorization in Cognitive Science  H Cohen and C Lefebvre Eds Elsevier 2006  W  T an J Zhang R K Madduri I T  F oster  D D Roure and C A Goble 223Servicemap Providing map and gps assistance to service composition in bioinformatics.\224 in IEEE SCC  H.-A Jacobsen Y Wang and P Hung Eds IEEE 2011 pp 632\226639 
89 
89 
89 


5  According to the user feedback and history of user behavior, we can provide more precise personal recommendation B  Disadvantages 1  Difficult to judge the precision of prediction results 2  When the system does not have enough data of user profile and web access log, the precision of prediction result will decrease 3  Need domain experts to add more knowledge data and maintain the knowledge rules periodically to reflect the changes C  Extensions Because this recommendation model is a prototype there are still many improvements to be made in the future Some directions of extension research are described below 1  Source data transformation At this time our model can only transform the source data of historical monuments and cultural heritage. We need to adopt an integrated transformation approach that can be used in other domains easily 2  Metadata definition Any domain has their own metadata; it is hard to integrate with each other. The development of a unified metadata definition for any domain is a challenge 3  Ontology Using semantic expression can make users easily understand the meaning of contents in the proper context 4  Agent mechanism Agents can be used to search content, prune uninterested ones and make inference automatically 5  Active recommendation In our recommendation model, the system make recommendations passively. We need triggering mechanisms to provide recommendation in an automatic way 6  Intelligent assistant To reduce user load, an intelligent assistant can record all the user behavior using some automatic processes based on users actions 7  Location awareness service The system can provide in time services to travelers or bikers based on where they are and their user behavior history VI  C ON C LUSION AND F UTURE W ORK  In this paper we propose an intelligent recommendation model with a case study on the historical monuments and cultural heritage to show the feasibility of our approach Different from the past experience of using only one technique to make recommendations to users, we adopt a hybrid approach to recommend content according to the type of user. We also use association rules of data mining technique to find the potential patterns in web access logs while classification is used to assign users into different groups suitable for them. The incremental algorithm of our method can calculate the ranking value of content to be more precise. Finally, a new technique is used to present the recommendation results with interactive graphic user interfaces In the future research we will continue to enhance the recommendation model and deploy this model in the cloud computing environment. Besides, we will also keep working in some extension direction, such as intelligent assistant source data transformation, and location awareness service Finally, another future work is to make recommendations for a group of people instead of an individual A CKNOWLEDGMENT  This research is supported in part by Ministry of Education and National Science Council in Taiwan, under grant numbers NSC 98-2221-E-035-059-MY2 and NSC 982218-E-007-005  R EFERENCES  1   Abolghasem Sadeghi Niaraki, Kyehyun Kim. Ontology based personalized route planning system using a multi-criteria decision making approach. Expert Systems with Applications 36 \(2009 pp.2250-2259 2   Angel Garcia-crespo, Javier Chamizo, Ismael Rivera, Myriam Mencke, Ricardo Colomo-Palacios, Juan Miguel Gomez-Berbis SPETA: Social pervasive e-Tourism advisor. Telematics and Informatics, 26 \(2009\ pp. 306-315 3   Bo Shao, Dingding Wang, Tao Li, and Mitsunori Ogihara. Music Recommendation Based on Acoustic Features and User Access Patterns. IEEE Transactions on Audio, Speech, Language Processing, vol. 17, no. 8, November 2009. pp.1602-1611 4   Dirksen Liu and Maiga Chang. Recommend Touring Routes to Travelers According to Their Sequential Wandering Behaviors IEEE 10 th International Symposium on Pervasive Systems Algorithm, and Networks, 2009. pp. 350-355 5   Hyoseop Shin, Minsoo Lee, and Eun Yi Kim. Personalized Digital TV Content Recommendation with Integration of User Behavior Profiling and Multimodal Content Rating. IEEE Transactions on Consumer and Electronics, vol. 55, issue. 3, August 2009. pp. 14171423 6   Katerina Kabassi. Review Personalizing recommendations for tourists. Telematics and Informatics 27 \(2010\, pp. 51-66 7   Loc Nguyen, Phung Do. Learning Concept Recommendation based on Sequential Pattern Mining. IEEE 3 rd International Conference on Digital Ecosystems and Technologies, 2009. pp. 66-71 8   Mei-Ling Shyu, Choochart Haruechaiyasak, Shu-Ching Chen and Na Zhao. 2005. Collaborative Filtering by Mining Association Rules from User Access Sequences. IEEE International Workshop on Challenge in Web Information Retrieval and Integration, 2005 9   Rana Forsati, Mohammad Reza Meybodi, Afsaneh Rahbar. An Efficient Algorithm for Web Recommendation Systems. IEEE/ACS International Conference on Computer Systems and Applications 2009. pp. 579-586    Silvia Schiaffino, Analia Amandi. Building an expert travel agent as a software agent. Expert Systems with Applications 36 \(2009\ pp 1291-1299    Tonderia Maswera, Ray Dawson, Janet Edwards. E-commerce adoption of travel and tourism organisations in South Africa Kenya, Zimbabwe and Ugande. Telematics and Informatics 25 2008\. pp. 187-200    Yan Zheng Wei, Luc Moreau, and Nicholas R. Jennings. Learning Users Interests by Quality Classification in Market-Based Recommender Systems. IEEE Transactions on Knowledge and Data Engineering, vol. 17, no. 12, December 2005. pp. 1678-1688    Yuxia Huang, Ling Bian. A Bayesian network and analytic hierarchy process bases personalized recommendations for tourist attractions over the internet. Expert Systems with Application 36 2009\, pp. 933-943  
79 


020\021 n 020\021 n r\021\027 r\030\031\032 r\021\027 r\030\031\032 r\021\027 r\030\031\032 002\003\004 002\003\004 C  002\005\006\007 006\f\002 Proj. DBs of <A,B  Proj. DBs of <A,B,C   its supports and con\223dence can be computed from the projected databases w.r.t in is is neither in Seq ID  Statistics PHT Fig 1  line 6 in Algorithm 2 for any with 022\005 A  C        Proj. DBs of <A,C                                             i.e i.e i.e r 021 002 003\004\005 003\004\005 D Algorithm Comparative Analysis 016\017\017 r 016\017\017 021 016\017\017 016\017\017 007\b\002 005 005 016\017\017 007\b\002 004 004 004 003 004 003 004 004 004 005 005 005 005 005 005 005 005 005 005 005 005 005 005 005 005 005 007 007 007 002 003 003 002 006 007 007 033 033 033 033 033 033 033 033 033 B  034 034 034 034 034 034 034 034 034 034 034 034 034 034 034 034 034 n 034 034 034 034  013 032 032 013  t\r\016 t\r\016 007\b\b  b\004 b\004 b\004 b\004 b\004 b\f\002 b\004 b b b\004 b\004 b\004 b 002\003\004\005\006 006 006 006 b b 006 006 006   and       yet the pre-condition  The worst case complexity of mining frequent patterns of length at most is O  The complexity of our approach is at most O       The 223rst term of the 223rst formula i.e 007\b\002 r 007\b\002 007\b\002 007\b\002 007\b\002 007\b\002 007\b\002 007\b\002 021 007\b\002 007\b\002 021 007\b\002 007\b\002 021 007\b\002 007\b\002 007\b\002 007\t\n\013 007\t\n\013 007\t\n\013 007\t\n\013 007\t\n\013 007\t\n\013 007\t\n\013 007\t\n\013 007\t\n\013 intoa bucket using 034 021 021 021 021 034 034 034 034 034 034 034 034 020 017 020 017 n 034 021 034 021 034  032     034   034    034  e.g 007\007 017\020\021\022 017\020\021\022 017\020\021\022 003\006\007\b 003\006\007\b 003\006\007\b Pre\223x Hash Tree 034 b  034 b  034 b  We store Eliminating Redundant Rules 004\002\003 004\002\003 t t t t 002 003 003 002 002 004 003 003 003 003 004 005 005 005 005 005 005 005 005 005 1051 032\034 032#\b 032\034 032#\b 032#\b 032#\b 032#\b 032\034 032\034 032#\b 032 032 032 032\034 032#\b 032 032 032 032 weuse the formula de\223ned in Proposition 1 and use the projected database Pre\223x Hash-Tree PHT Data Structure Embedding the Anti-Monotonicity Property Sequence     002 002  002 r 003 016 003 016 003 004 003    002 003  002 r 003 016 003 016 003 004 003   002\003  iii how to compute the supports and con\223dence of a rule iv how to test whether a rule is redundant  For  all patterns are stored in the PHT in reverse order Each node in this tree represents a pattern obtained by following the path from root to this node and is associated with its corresponding projected databases We store both stored in the PHTs To compute   the same triple  and 3 Composition of premises and consequents to form non-redundant rules Let us also assume that both the premises and consequents have a maximum length of 003 002 003 005 002 005 and  if we 223nd has a low support or con\223dence we can skip scanning  and continue to and and and nor and and and 002\003\004\005\006 002\003\004\005\006 with each  For example in Figure 1 suppose it is the PHT of post-condition candidates for some pre-condition candidate we compare to all occurrences of  is the size of  For these cases we need to re-scan the database to 223nd the instance support of has a sequence support instance support and con\223dence of 2 2 and 100 These are the same as those of rule  Although subsumes is not considered redundant due to by Theorem 2 Similarly is not considered redundant due to 220s with the same supports and con\223dence  a hash table Then for each bucket we want to remove the redundant rules   the ones with their super-pattern in the same bucket Consider a database containing a set of sequences with events coming from an alphabet  database scan operations Consider a set of patterns and works in two steps 1 Mine a pruned set of pre-conditions obeying minimum sequence support threshold and Theorem 2 from  2 For each pre-condition mine a set of post-conditions obeying the minimum con\223dence and minimum instance support thresholds At the end of the 223rst step for each pre-condition  LKL08 constructs a projected database  Another mining operation is then performed on this projected database The complexity is thus O  once a rule has a low support or low con\223dence we can skip scanning the whole subtree below and look for common sequences where the premise  For computing by Theorem 3 We map rules  and the last is due to the composition of premises and consequents to form rules Remember many rules can be constructed without requiring any additional database scan operation Some however require the re-scanning of the database to compute their instance support values see Section VI-C The algorithm in LKL08 tak es as input a s equence database Computing Supports and Con\223dence to facilitate the pairing procedure ii in what order to pair each We use a pre\223x hash-tree PHT data structure to organize the set of candidates  Each projected database is stored implicitly discussed in Section VI-B Each node has a hash table to quickly locate one of its child in a constant lookup operation given an event Figure 1 shows an example of a PHT  in a PHT in reverse order and scan in DFS order By following the DFS order to visit nodes in the PHT a post-condition is always scanned earlier than its backward extensions This feature enables us to embed the anti-monotonicity properties of con\223dence Theorem 1 and sequence support into the pairing algorithm  in PHTs For a rule occurs 223rst before  The ratio of the occurrences of which is usually in the PHTs However there are cases where is a signi\223cant rule while In line 8 of Algorithm 2 we want to remove redundant rules Some redundant rules have been pruned early in stage I But to eliminate all redundant rules this step is needed To illustrate the need for this stepafter applying Theorem 2 and 3 consider the following database    The rule from such a database is O  the worst case complexity of constructing from patterns in  database scan operations In our analysis we use database scan as the unit of operation We ignore the time needed to eliminate redundant rules since no database scan operation is involved In BOB we perform 1 two mining operations on the original sequence database to obtain the set of  2 construction of  is due to the mining of the premises the second is due to the mining of the consequents the third and fourth are due to the construction of  Notice that if 


006 006 016\017\017 007\b\002 007\b\002 r conf=50 In Figure 4 and 5 we plot the runtime needed and the number of rules mined from the D10C10N10R0.5 dataset when varying the minimum sequence support threshold and the minimum con\223dence threshold respectively Again BOB is up to 10 times faster than LKL08 and the gap between Datasets 022\005 1052 034   034 b  Environment and Pattern Miners 005 005 033 033   we store summary information e.g Rules right for D5C20N10R0.5 dataset when varying min  Results 005 005 005 all pruning properties do not work In the worst case all possible rules up to a particular length are signi\223cant and none of them is redundant The following points summarize reasons behind the superiority of our approach as compared to 1 We employ two new pruning strategies described by Theorem 1 and 3 These strategies are embedded into our new mining algorithm to remove search space not pruned before by the approach in 2 The projected database created in could be v ery lar ge especially if patterns 002\003\004 002\003\004 b b\004 b\004 b\004 b\004 b\004  032 016\017\017 r especially in cases where the number of repetitions of Fig 2 Runtime left  i.e and worst case is not large the complexity of our approach is smaller by an exponential factor than that of LKL08 In the worst case in a sequence many times For a premise  the projected database could be larger than the original  for each pattern In Figure 2 and 3 we plot the runtime needed and the number of rules mined from D5C20N10R0.5 when varying the minimum sequence support threshold and the minimum con\223dence threshold respectively For simplicity sake we set the minimum instance support threshold to be equal to the minimum sequence support threshold Each of the runtime graphs has two lines corresponding to the two algorithms\220 runtime at various thresholds We note that BOB is up to one order of magnitude i.e 10 times faster than LKL08 BOB pruning strategy is also effective in reducing required runtime when the minimum con\223dence threshold is raised from 50 to 90 On the other hand no signi\223cant performance change could be noticed in LKL08 case though However in frequent pattern mining worst case analysis is often not interesting This is so as in the recur within each sequence containing it is high Also projecting w.r.t a premise can produce a very localized dataset i.e a single sequence is split into its many suf\223xes resulting in hard-to-mine dense dataset with a large number of frequent patterns even at a high support threshold 3 During the mining of premises and consequents from  This information is used to immediately prune insigni\223cant rules not satisfying minimum sequence support and con\223dence thresholds see Section VI-C For this pruning we do not need to re-scan the database rather only the summary information needs to be analyzed The algorithm in can only perform database scan operations to prune candidate rules To compare effectiveness of various pruning strategies experiments on various datasets are needed We perform this empirical evaluation in Section VII VII E MPIRICAL E VA L UAT I O N Experiments have been performed to evaluate the scalability of our approach A case study on analyzing traces from an instant messaging application has also been conducted All experiments are performed on a Pentium Core 2 Duo 3.17GHz PC with 3GB main memory running Windows XP Professional Algorithms are written in Visual C#.Net We compare the approach presented in with our approach W e refer to the tw o approaches as LKL08 and BOB respectively To reduce the threat of external validity i.e the generalizability of our result we investigate a variety of datasets Four datasets two synthetic and two real are studied IBM synthetic data generator is used It is modi\223ed to produce sequences of events rather than sequences of sets of events The generator accepts a set of parameters We focus on four parameters D C N and R They correspond to the number of sequences in 1000s the average number of events per sequence the number of different events in 1000s and the repetition level range 0 to 1 respectively All other parameters of the synthetic data generator are set to their default values We experiment with two synthetic datasets D5C20N10R0.5 and D10C10N10R0.5 Dataset D5C20N10R0.5 contains sequences with an average length of 64.4 and a maximum length of 275 Dataset D10C10N10R0.5 contains sequences with an average length of 31.2 and a maximum length of 133 D5C20N10R0.5 has less sequences of longer lengths On the other hand D10C10N10R0.5 has more sequences of shorter lengths We also experiment on a click stream dataset   Gazelle dataset from KDDCup 2000 which has also been used to evaluate frequent sequential pattern miners i.e CloSpan and BIDE The dataset contains 29,369 sequences with an average length of 3 and a maximum length of 651 Compared to the two synthetic datasets this real data has a lower average length but contains sequences of longer lengths The gap in the lengths of long and short sequences is also wider To evaluate our algorithm performance on mining from program traces we generate traces from TotInfo program in the Siemens Test Suite The test suite comes with 893 correct test cases We run these test cases to obtain 893 traces Each trace is a sequence of events where every event is a method invocation We refer this dataset as the TotInfo dataset The TotInfo dataset contains sequences with an average length of 12.1 and a maximum length of 136 s-sup at min  


Rules right for D5C20N10R0.5 dataset when varying min  conf at min  s-sup 2.4 the performance of BOB and LKL08 is increased when the minimum con\223dence threshold is raised from 50 to 90 We do not experiment mining at lower con\223dence thresholds as low con\223dence rules have little use and are likely to only capture noises Rules right for D10C10N10R0.5 dataset when varying min  Rules right for D10C10N10R0.5 dataset when varying min  conf at min  s-sup 0.5 In Figure 6 and 7 we plot the runtime needed and the number of rules mined from the Gazelle dataset when varying the minimum sequence support threshold and the minimum con\223dence threshold respectively We notice that BOB improves the performance of LKL08 by two orders of magnitude i.e more than 100x faster At support level 0.020 LKL08 is not able to 223nish within 8 hours Thus BOB can successfully mine rules at a lower support threshold that is not minable by LKL08 in a reasonable amount of time Rules s-sup at min  Rules right for for Gazelle dataset when varying min  Rules right for TotInfo dataset when varying min  s-sup at min  Rules right for TotInfo dataset when varying min  s-sup 5.6 LKL08 is not able to run 006 006 006 006 006 006 006 006 006 006 006 006 006 006 conf=50 conf=50 sup=0.034 In Figure 8 and 9 we plot the runtime needed and the number of rules mined from the TotInfo dataset when varying the minimum sequence support threshold and the minimum con\223dence threshold respectively At all support levels shown in Figure 8 and 50 minimum con\223dence threshold LKL08 is not able to run due to an out of memory exception while BOB is able to 223nish in less than 2 minutes When we decrease the minimum con\223dence threshold we see an exponential increase in the runtime of LKL08 BOB runtime on the other hand remains the same LKL08 performs a pattern mining operation for every projected database of each mined pre-conditions this causes the high runtime values  conf=50 Fig 3 Runtime left  Fig 4 Runtime left  Fig 5 Runtime left  Fig 6 Runtime left  Fig 7 Runtime left  conf\(at min  Fig 8 Runtime left  Fig 9 Runtime left  conf at min  In past studies recurrent rules either in full or restricted form have been mined from various software datasets 27 Mined rules correspond to interesting temporal properties extracted from execution traces of programs In Lo et al mined rules from e x ecution traces generated by running test cases of JBoss Application Server In in a study within Microsoft Lo et al mined a restricted form of two-event recurrent rules with quanti\223cation from Windows device drivers and other Windows applications In this study we consider another trace data obtained from user interactions with a drawing utility of an instant messaging application Many systems like the one that we analyze do not 1053 Case Study s-sup at min  right for Gazelle dataset when varying min  


Handbook of Statistics Int Conf on Data Eng Int Conf on Tools and Algo for the Constr and Ana of Sys Knowledge and Info Sys Int Conf on Data Eng Int Conf on Data Eng Int Conf on Data Eng Int Conf on Database Sys for Adv App Int Conf on Soft Eng Model Checking Soft Product Line Conf Euro Conf on Soft Maintenance and Re-Eng Data Mining and Knowledge Disco Data Mining and Knowledge Disco SIAM Int Conf on Data Mining Int Conf on Soft Eng  getMyJID  Cambridge 2004  H K ugler  D Harel A Pnueli Y  Lu and Y  Bontemps 215T emporal logic for scenario-based speci\223cations,\216 in  2006  D Lo S.-C Khoo and C Liu 215Ef 223cient mining of iterati v e patterns for software speci\223cation discovery.\216 in  2007  204\204 215Ef 223cient mining of recurrent rules from a sequence database 216 in  2008  M Dwyer  G A vrunin and J Corbett 215P atterns in property speci\223cations for 223nite-state veri\223cation.\216 in  1999  E Clark e O Grumber g and D Peled  2004  R Capilla and J Duenas 215Light-weight product-lines for e v olution and maintenance of web sites,\216 in  2003  R Agra w al and R Srikant 215F ast algorithms for mining association rules,\216 in  1994  204\204 215Mining sequential patterns 216 in  1995  H Mannila H T oi v onen and A V erkamo 215Disco v ery of frequent episodes in event sequences,\216  1999  M Huth and M Ryan  2005  W  Jamroga 215 A temporal logic for stochastic multi-agent systems 216 in  2008  P  Monteiro D Ropers R Mateescu A Freitas and H de Jong 215Temporal logic patterns for querying dynamic models of cellular interaction networks,\216  2008  M Arenas P  Barcelo and L Libkin 215Combining temporal logics for querying xml documents,\216 in  2007  S Zhang 215 A temporal logic for supporting historical databases 216  2000  N P asquier  Y  Bastide R T aouil and L Lakhal 215Disco v ering frequent closed itemsets for association rules,\216 in  1999  M Zaki 215Mining non-redundant association rules 216  2004  J Pei J Han B Mortaza vi-Asl H Pinto Q Chen U Dayal and M.C Hsu 215Pre\223xspan Mining sequential patterns ef\223ciently by pre\223xprojected pattern growth.\216 in  2001  X Y an J Han and R Afhar  215CloSpan Mining closed sequential patterns in large datasets.\216 in  2003  J W ang and J Han 215BIDE Ef 223cient mining of frequent closed sequences.\216 in  2004  S Harms J Deogun and T  T adesse 215Disco v ering sequential association rules with constraints and time lags in multiple sequences.\216 in  2002  G Garriga 215Disco v ering unbounded episodes in sequential data 216 in  2003  B Ding D Lo J Han and S.-C Khoo 215Ef 223cient mining of closed repetitive gapped subsequences from a sequence database.\216 in  2009  M Hutchins H F oster  T  Goradia and T  Ostrand 215Experiments on the effectiveness of data\224owand control-\224ow-based test adequacy criteria,\216 in  1994  D Lo G Ramalingam V P  R anganath and K V asw ani 215Mining quanti\223ed temporal rules Formalism algorithms and evaluation.\216 in  2009  215Jeti V ersion 0.7.6 Oct 2006 http://jeti.sourcefor ge.net 1054 PictureChat showWindow PictureChat PictureHistory      vol 1 pp 259\205289 1997  M Spiliopoulou 215Managing interesting rules in sequence mining 216 in method This identi\223er is later af\223xed to the object drawn Finally the canvas records the operation in a object VIII C ONCLUSION This work proposes a new approach to mine recurrent rules in the form of 215Whenever a series of events occurs another series of events also occurs\216 The proposed approach is more scalable than the previous approach in Rather than performing a mining operation for each non-redundant pre-condition as proposed in our ne w approach emplo ys a number of new pruning strategies embedded in a new mining algorithm that requires only two mining and some additional database scanning operations Under a condition which holds in many cases the complexity of the proposed algorithm is smaller by an exponential factor than the complexity of the one proposed in W e ha v e e xperimented on v arious datasets synthetic and real Experiments have shown that the new algorithm improves the runtime of the previous algorithm by up to two orders of magnitude In the future we are looking into more applications of the mining algorithm and opportunities to further speed up the mining process R EFERENCES  M Gupta and S Ray  215Sequence pattern disco v ery with applications to understanding gene regulation and vaccine design.\216 in PC-  nu.fw.jeti.plugins.drawing.shapes.PictureChat PH -  nu.fw.jeti.plugins.drawing.shapes.PictureHistory Fig 10 Jeti Instant Messaging Application Drawing Scenario We use Jeti a popular instant messaging application which supports many features We record 30 interactions with the drawing tool of Jeti application and collect 30 traces The traces have an average length of 1,430 and a maximum length of 11,838 events Each event is a method call The purpose of this case study is to show the usefulness of the mined rules by discovering frequent and signi\223cant rules describing behaviors of the drawing sub-component of Jeti Using minimum sequence support and instance support of 25 traces/instances and a minimum con\223dence threshold of 90 BOB could complete in 57 seconds while LKL08 is only able to complete in 2844 seconds A total of 19 rules are collected after applying the following post-processing steps 1 Density Only report a mined rule iff the number of its unique events is more than 80 of its length 2 Ranking Order mined rules according to their lengths and support values A sample mined rule is shown in Figure 10 The rule captures the scenario when a user draws an object e.g a rectangle a line etc to a canvas First a resource i.e a Map object is created by a KDD Int Conf on Very Large Data Bases Paci\223c Rim Int Conf on Multi-Agents Int Symp on Intel Sys Euro Conf on Prin and Prac of Knowledge Disco in Databases come with suf\223cient test cases We ask a student who is not involved in this study to interact with the system  MIT Press 1999  S Deelstra M Sinnema and J Bosch 215Experiences in softw are product families Problems and issues during product derivation,\216 in Euro Conf on Prin and Prac of Knowledge Disco in Databases Bioinformatics ICDT Legend Consequent Map PC.createWritersMap void PC.showWindow void PC.unselect void PC.showWindow JID PC.getMyJID void PC.draw\(Shape  PH.addShapeDrawnByMe\(\203 Premise Logic in Computer Science ICDT Working Conf on Reverse Eng method are made by different callers When the application starts an empty window is 223rst shown or displayed After an object is drawn the drawn object i.e rectangle line etc would request the canvas i.e object Next multiple invocations of object to 215unselect\216 and redraw itself The system next retrieves the identi\223er of the user that draws the object by the invocation of 


TABLE II.     SUMMARY OF METRICS COLLECTED FOR DS2 Load Generator Processor Time Orders/minute Network Bytes Sent/sec Network Bytes Received/Sec Tomcat Processor Time Threads Virtual Bytes Private Bytes MySQL Processor Time Private Bytes Bytes written to disk/sec Context Switches/sec Page Reads/sec Page Writes/sec Committed Bytes In Use Disk Reads/sec Disk Writes/sec I/O Reads Bytes/sec I/O Writes Bytes/sec TABLE I.   AVERAGE PRECISION AND RECALL of Test Scenarios Duration per Test hours Size of Data per Test Avg Precision Avg Recall DS2 4 1 360 KB 100% 52 JPetStore 2 0.5 92 KB 75% 67 Enterprise System 13 8 4.5 MB 93% N/A  


37  Analysis of Test B: The goal of this experiment is to show that the rules generated by our approach are stable under normal system operation. Since Test B shares the same configuration and same load as Test A, ideally our approach should not flag any metric Our prototype did not report any problematic metric in Test B. The output is as expected, since Test B uses the same configuration as Test A and no performance bug was injected Analysis of Test C: In test C, we injected a databaserelated bug to simulate the effect of an implementation error This bug affects the product browsing logic in DS2. Every time a customer performs a search on the website, the same query will be repeated numerous times, causing extra workload for the backend database and Tomcat server Our approach flagged a database related metric \(# Disk Reads/sec Threads and # private bytes signaling that the metrics are violated during the whole test The result agrees with the nature of the injected fault: each browsing action generates additional queries to the database As a result, an increase in database transaction leads to an increase of # Disk Reads/sec. When the result of the query returns, the application server uses additional memory to extract the results. Furthermore, since each request would take longer to complete due to the extra queries, more threads are created in the Tomcat server to handle the otherwise normal workload. Since 3 out of 6 expected problematic metrics are detected, the precision and recall of our approach in Test C are 100% and 50% respectively Analysis of Test D: We injected a configuration bug into the load driver to simulate that a wrongly configured workload is delivered to the system. This type of fault can either be caused by a malfunctioning load generator or by a performance analyst when preparing for a performance regression test [14 In the case where a faulty load is used to test a new version of the system, the assessment derived by the performance analyst may not depict the actual performance of the system under test In Test D, we double the visitor arrival rate in the load driver. Furthermore, each visitor is set to perform additional browsing for each purchase. Figure 7 below shows the violated 


metrics reported by our prototype. The result is consistent with the nature of the fault. Additional threads and memory are required in the Tomcat server to handle the increased demand Furthermore, the additional browsing and purchases lead to an increase in the number of database reads and writes. The extra demand on the database leads to additional CPU utilization Because of the extra connections made to the database caused by the increased number of visitors, we would expect the # context switch metric in the database to be high throughout the test. To investigate the reason for the low severity of a databases context switch rate \(0.03 examined the rules flagged the # context switch metric. We found that the premises of most rules that flagged the context switch metric also contain other metrics that were flagged with high severity. Consequently, the premises of the rules that flagged # context switch are seldom satisfied resulting in the low detection rates of the # context switch metrics. Since 7 out of 13 expected metrics are detected, the precision and recall of our approach in this test are 100% and 54% respectively B. Studied System: JPetStore System description: JPetStore [1] is a larger and more complex e-commerce application than DS2. JPetStore is a reimplementation of Sun's original J2EE Pet Store and shares the same functionality as DS2. Since JPetStore does not ship with a load generator, we use a web testing tool to record and replay a scenario of a user logging in and browsing items on the site Data collection: In this case study, we have conducted two one-hour performance regression tests \(A and B performance signatures are extracted from Test A during which caches are enabled. Test B is injected with a configuration bug in MySQL. Unlike the DS2 case study where the configuration bug is injected in the load generator, the bug used in Test B simulates a performance analysts mistake to accidentally disable all caching features in the MySQL database. Because of the nature of the fault, we expect the following metrics of the database machine to be affected: CPU utilization, # threads context switches, # private bytes, and # I/O read and write bytes/sec Analysis of Test B: Our approach detected a decrease in memory footprint \(# private bytes sec in the database, and increase in # disk reads/sec and 


threads in the database. The I/O metrics include reading and writing data to network, file, and device. These observations align with the injected fault: Since the caching feature is turned off in the database, less memory is used during the execution of the test. In exchange, the database needs to read from the disk for every query submitted. The extra workload in the database  Figure 7. Performance Regression Report for DS2 Test 4 \(Increased Load TABLE III.     SUMMARY OF INJECTED FAULTS FOR DS2 Test Fault Injected Expected Problematic metric A No fault N/A B No fault No problem should be observed C Busy loop injected in the code responsible for displaying  item search results Increase in # I/O reads bytes /sec, and disk read/sec in database Increase in # threads, # private and virtual bytes, and CPU utilization in the Tomcat server D Heavier load applied to simulate error in load test configuration Increase in CPU utilization, # threads private and virtual bytes in the Tomcat server Increase in database CPU utilization disk reads, writes and I/O read bytes per second, and # context switches Increase in # orders/minute and network activities in the load generator 38 translates to a delay between when a query is received and the result is sent back, leading to a decrease in # IO write bytes/sec to the network Instead of an increase, an unexpected drop of the # threads was detected in the database. Upon verifying with the raw data for both tests, we found that the thread count in Test A \(with cache without cache 


and 21 respectively. Upon inspecting the data manually, we do not find that the decrease of one in thread count constitutes a performance problem and this is therefore a false positive Finally, throughout the test, there is no significant degradation in the average response time. Since 4 out of 6 expected problems are detected, our performance regression report has a precision of 75% and recall of 67 C. Studied System: A Large Enterprise System System description: Our third case study is conducted on a large distributed enterprise system. This system is designed to support thousands of concurrent requests. Thus, performance of this system is a top priority for the organization. For each build of the software, performance analysts must conduct a series of performance regression tests to uncover performance regressions and to file bug reports accordingly. Each test is run with the same workload, and usually spans from a few hours to a few days. After the test, a performance analyst will upload the metric data to an internal website to generate a time series plot for each metric. This internal site also serves the purpose of storing the test data for future reference. Performance analysts then manually evaluate each plot to uncover performance issues. To ensure correctness, a reviewer must sign off the performance analysts analysis before the test can be concluded. Unfortunately, we are bounded by a NonDisclosure Agreement and cannot give more details about the commercial system Data collection: In this case study, we selected thirteen 8hour performance regression tests from the organizations performance regression testing repository. These tests were conducted for a minor maintenance release of the software. The same workload was applied to all tests. In each test, over 2000 metrics were collected Out of the pool of 13 tests, 10 tests have received a pass status from the performance analysts and are used to derive performance signatures. We evaluated the performance of the 3 remaining tests \(A, B and C the performance analysts assessment \(summarized in table 4 In the following sections, we will discuss our analysis on each target test \(A, B and C Analysis of Test A: Using the history of 10 tests, our approach flagged all throughput and arrival rate metrics in the system. The rules produced in the report imply that throughputs 


and arrival rates should fall under the same range. For example component A and B should have similar request rate and throughput. However, our report indicates that half of the arrival rates and throughput metrics are high, while the other half is low. Our approach has successfully uncovered problems associated with the arrival rate and throughput in Test A that were not mentioned in the performance analysts report. We have verified our finding with a performance analyst. Our performance regression report has a precision of 100 Analysis of Test B: Our approach flagged two arrival rate metrics, two job queue metrics \(each represents one subprocess consulting with the time-series plots for each flagged metric as well as the historic range, we found that the # database scans/sec metric has three spikes during the test. These spikes are likely the cause of the rule violations. Upon discussing with a performance analyst, we find that the spikes are caused by the systems periodic maintenance and do not constitute a performance problem. Therefore, the # database scans/sec metric is a false positive. Our performance analysis report has a precision of 80 Analysis of Test C: Our approach did not flag any rule violation for this test. Upon inspection of the historical value for the metrics noted by the performance analyst, we notice that the increase of # database transactions/sec observed in Test C actually falls within the metric historical value range. Upon discussing with the Performance Engineering team, we conclude that the increase does not represent a performance problem. In this test, we show that our approach of using a historical dataset of prior tests is more resistant to fluctuations of metric values. Our approach achieves a precision of 100 The case studies show that our approach is able to detect problems in metrics when the faults are present in the systems Our approach detects problematic metrics with high precisions in all three case studies. In our case studies with the two open source systems, our approach is able to cover 50% and 67% of the expected problematic metrics VI. DISCUSSION AND FUTURE WORK A. Quantitive Techniques Although there are existing techniques [10, 11] to correlate anomalies with performance metrics by mining the raw performance data without discretization, these techniques 


usually assume the presence of Service Level Objectives \(SLO that can be used to determine precisely when an anomaly occurs. As a result, classifiers that predict the state of SLO can be induced from the raw performance data augmented with the SLO state information. Unfortunately, SLOs rarely exist during development. Furthermore, automated assignment of SLO states by analyzing metric deviations is also challenging as there could be phase shifts in the performance tests, e.g., the spikes do not align. These limitations prevent us from using classifier based techniques to detect performance regression TABLE IV.      SUMMARY OF ANALYSIS FOR THE ENTERPRISE SYSTEM Test Performance Analysts Report Our Findings A No performance problem found Our approach identified abnormal behaviors in system arrival rate and throughput metrics B Arrival rates from two load generators differ significantly Abnormally high database transaction rate High spikes in job queue Our approach flagged the same metrics as the performance analysts analysis with one false positive C Slight elevation of database transactions/sec. No metric flagged  39 B. Sampling period and Metric Discretization We choose the size of time interval for metric discretization based on how often the original data is sampled. For example an interval of 200 seconds is used to discretize data of the enterprise system, which was originally sampled approximately every 3 minutes. The extra 20 second gap is used because there was a mismatch in sampling frequencies for some metrics. We also experimented with different interval lengths. We found that less metrics are flagged as the length of the interval increases, while precision is not affected 


In our case studies, we found that the false negatives metrics that were expected to show performance regressions but were not detected by our approach no rule containing the problematic metrics was extracted by the Apriori algorithm. This was caused by our discretization technique sometimes putting all values of a metric that had large standard deviation into a single level. Candidate rules containing those metrics would exhibit low confidence and were thus pruned. In the future, we will experiment on other discretization techniques, such as Equal Width Interval Binning C. Performance Regression Testing Our approach is limited to detecting performance regressions. Functional failures that do not have noticeable effect on the performance of the system will not be detected Furthermore, problems that span across the historical dataset and the new test will not be detected by our approach. For example, no problem will be detected if both the historical dataset and the new test show the same memory leak. Our approach will only register when the memory leak worsens or improves D. Passed Tests The historical dataset from which the association rules are generated should contain tests that have the same workload configuration, preferably same hardware, and exhibit correct behavior. Using tests that contain performance problems will decrease the number of frequent item sets extracted, making our approach less effective in detecting problems in the new test. In our case study with the enterprise system, we applied the following measure to avoid adding problematic tests to our historical dataset We selected a list of tests from the repository that have received a pass status from the performance analyst We manually examined the performance metrics that are normally used by a performance analyst in each test from the list of past test to ensure no abnormal behavior was found E. System Evolution and Size of Training Data The system is often updated to support new environments or requirements. These updates may lead to changes in performance. A large variability in metric values will negatively affect the confidence of association rules generated 


in our approach. Therefore, it is necessary to update the set of tests included in the historical dataset. We are currently studying the effect of using a sliding window to select prior tests to include in the historical dataset. A sliding window allows us to automatically discard outdated tests that no longer reflect the current systems performance. However, the optimal size of the sliding window will likely be project-dependent since each project has different release frequency Alternatively, the historical dataset can also be derived from within the run. For example, the first hour of the current test can be used to derive performance signatures. Assuming that the system runs correctly during the first hour, the performance signature generated from this historical dataset will be useful to assess the stability of the system F. Hardware Differences In practice, performance regression tests of a system can be carried out on different hardware. Furthermore, third party components may change in between tests. In the future, we plan to improve our learning algorithm so that, given a new test, our tool will automatically select the tests from the repository with similar configurations G. Automated Diagnosis Our approach automatically flags metrics by using association rules that show high deviations in confidence between the new tests and the historical dataset. These deviations represent possible performance regressions or improvements and are valuable to performance analysts in assessing the system under test. Performance analysts can adjust the deviation threshold to restrict the number of rules used and, thus, limit the number of metrics flagged. Alongside with the flagged metrics, our tool also displays the list of rules that the metric violated. Performance analysts can inspect these rules to understand the relations among metrics. From our case study, we notice that some of the rules produced are highly similar. In the future, we will research for ways to merge similar rules to further condense information for performance analysts to analyze The association rules presented in our performance regression report represent metric correlations rather than causality. Performance analysts can make use of these correlations to manually derive the cause of a given problem VII. RELATED WORK 


Our goal in this work is to detect performance problems in a new test using historical data. Existing approaches monitor or analyze a system through one of two sources of historical data execution logs and performance metrics A. Analyzing Execution Logs Reynolds el al. [18] and Aguilera et al. [22] developed various algorithms for performance debugging on distributed systems. Their approach analyzes message trace of system components to infer the dominant causal paths and identify the components that account for a significant fraction of the systems latency. Unfortunately, the accuracy of the inferred paths decreases as the degree of parallelism increases, leading to low precision in identifying problematic components. Our approach is different from Reynoldss and Aguileras in that we pinpoint performance issues on the metric level rather than locating the system components that contribute significantly to system latency. Jiang et al. introduce a technique [14] to 40 identify functional problems in a load test from execution logs The authors extended this approach to analyze performance in scenarios as well as in the steps of each scenario [13]. Chen et al. proposed Pinpoint [21] to locate the subset of system components that are likely to be the cause of failures. Our work is different from Pinpoint in that Pinpoint focuses on identifying system fault rather than performance regression which can occur even when the system functions correctly In contrast to the above studies, which analyze execution logs, our approach analyzes performance metrics to identify performance problems B. Analyzing Performance Metrics Bondi [9] presented a technique to automatically identify warm-up and cool-down transients from measurements of a load test. While Bondis technique can be used to determine if a system ever reaches a stable state in the test, our approach can detect performance problems at the metric level Cohen et al. [11, 12] applied supervised machine learning techniques to induce models on performance metrics that are likely to correlate with observed faults. Bodik et al. improved Cohens work [8] by using logistic regression. Our approach is different from the above work as we do not require knowledge of violations of Service Level Objectives Jiang et al. proposed an approach [16] for fault detection 


using correlations of two system metrics. A fault is suspected when the portion of all derived models that report outliers exceeds a predefined threshold. Our approach is based on frequent item sets that can output correlations of more than two metrics. Performance analysts can leverage these metric correlations to better understand the cause of a fault. Jiang et al 15] proposed an approach to identify clusters of correlated metrics with Normalized Mutual Information as similarity measure. The authors were able to detect 77% of the injected faults and the faulty subsystems, without any false positives While the approach in [15] can output only the faulty subsystems, our approach can detect and report details about performance problems, including metrics that deviate from the expected behaviors VIII. CONCLUSIONS It is difficult for performance analysts to manually analyze performance regression testing results due to time pressure large volumes of data, and undocumented baselines Furthermore, subjectivity of individual analysts may lead to incorrect performance regressions being filed. In this paper, we explored the use of performance regression testing repositories to support performance regression analysis. Our approach automatically compares new performance regression tests to a set of association rules extracted from past tests. Potential performance regressions of system metrics are presented in a performance regression report ordered by severity. Our case studies shows that our approach is easy to adopt and can scale well to large enterprise system high precision ACKNOWLEDGMENT We are grateful to Research In Motion \(RIM access to the enterprise application used in our case study. The findings and opinions expressed in this paper are those of the authors and do not necessarily represent or reflect those of RIM and/or its subsidiaries and affiliates. Moreover, our results do not in any way reflect the quality of RIMs products REFERENCES 1] iBATIS JPetStore, http://sourceforge.net/projects/ibatisjpetstore 2] MMB3, http://technet.microsoft.com/enus/library/cc164328%28EXCHG.65%29.aspx 3] The Dell DVD Store, http://linux.dell.com/dvdstore 4] The R Project for Statistical Computing. http://www.r-project.org 5] R. Agrawal, R.Srikant, Fast Algorithms for Mining Association Rules 


in Large Databases, Proc. of 20th Intl Conf. Very Large Data Bases 1994 6] A. Avritzer and B. Larson, Load testing software using deterministic state testing, Proc. of Intl Symp. on Software Testing and Analysis 1993 7] A. Avritzer, E. J. Weyuker, The automatic generation of load test suites and the assessment of the resulting software, IEEE Trans. Softw. Eng 21\(9 8] P. Bodik, M. Goldszmidt, A. Fox, HiLighter: Automatically Building Robust Signatures of Performance Behavior for Small- and Large-Scale Systems, Proc. of the  3rd SysML, Dec 2007 9] A. B. Bondi, Automating the Analysis of Load Test Results to Assess the Scalability and Stability of a Component, Proc. of 33rd Intl CMG Conf., San Diego, CA, USA, Dec. 2-7, 2007 10] L.  Bulej, T.  Kalibera, P. Tuma, Regression Benchmarking with Simple Middleware Benchmarks,  Proc. of the 2004 IPCCC, 2004 11] I. Cohen, M. Goldszmidt, T. Kelly, J. Symons, J. S. Chase, Correlating instrumentation data to system states: A building block for automated diagnosis and control, Proc. of 6th OSDI, Dec. 2004 12] I. Cohen, S. Zhang, M. Goldszmidt, J. Symons, T. Kelly, A. Fox Capturing, indexing, clustering, and retrieving system history Proc. of the 20th ACM Symp. on Operating Systems principles, 2005 13] Z. M. Jiang, A. E. Hassan, G. Hamann, P. Flora, Automated Performance Analysis of Load Tests, Proc. of the 25th ICSM, Sept 09 14] Z. M. Jiang, A. E. Hassan, P. Flora, G. Hamann, Automatic Identification of Load Testing Problems, Proc. of the 24th Intl Conf on Softw. Maintenance, Sept 2008 15] M. Jiang, M. A. Munawar, T.  Reidemeister, P A.S. Ward, Automatic Fault Detection and Diagnosis in Complex Software Systems by Information-Theoretic Monitoring, Proc. DSN, Jun 2009 16] M. Jiang, M. A. Munawar, T. Reidemeister, P. A. S. Ward System Monitoring with Metric-Correlation Models: Problems and Solutions Proc. of the 6th Intl Conf. on Autonomic Computing, 2009 17] T. Kalibera, L. Bulej, P. Tuma, Automated Detection of Performance Regressions: The Mono Experience, 13th MASCOTS, 2005 18]  P. Reynolds, J. L. Wiener, J.C. Mogul, M. K. Aguilera, A. Vahdat WAP5: Black-box Performance Debugging for Wide-Area Systems Proc. of the 15th Intl World Wide Web Conf.s, 2006 19] E. J. Weyuker, F. I. Vokolos, Experience with performance testing of software systems: Issues, an approach, andcase study, IEEE Trans Softw. Eng., 26\(12 20] I. H. Witten, E. Frank, Data Mining: Practical Machine Learning Tools 


and Techniques, Morgan Kaufmann, June 2005 21] M. Y. Chen , E. Kiciman , E. Fratkin , A. Fox , E. Brewer, Pinpoint Problem Determination in Large, Dynamic Internet Services, Proc. of the 2002 Intl Conf. on Dependable Systems and Networks, June  2002 22] M. K. Aguilera , J. C. Mogul , J. L. Wiener , P. Reynolds , A Muthitacharoen, Performance debugging for distributed systems of black boxes, Proc. of the 19th ACM Symp. on Operating systems principles, Oct 2003 41 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


