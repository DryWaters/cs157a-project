978-1-4244-5998-8/10/$26.00 c 2010 IEEE Combining BOW representation and Appriori algorithm for Text mining A. El Oirrak 1 D. Aboutajdine 2 1 Faculty of sciences Semlalia, Laboratory LISI, Marrakech, Morocco oirrak@yahoo.fr 2 Faculty of sciences, GSCM Rabat, Morocco The field of text mining seeks to extract useful information from unstructured textual data through the identification and exploration of interesting patterns. The techniques employed usually do not involve deep linguistic analysis or parsing, but re ly on simple “Bag-Of-Words”  \(BPW\t representations based on vector space In this paper we combine the BOW representation and Appriori algorithm to detect clusters of similar documents and associated rules Index Terms Text Mining \(TM\, Clustering, dissimilarity, Appriori algorithms, associated rules I I NTRODUCTION he purpose of Text Mining is to process unstructured textual\ information, extract meaningful numeric indices from the text, and, thus, make the information contained in the text accessible to the various data mining \(statistical and machine learning\lgorithms [1 n f o rma tio n can b e  extracted to derive summaries for the words contained in the documents or to compute summaries for the documents based on the words contained in them. Hence, you can analyze words, clusters of words used in documents, etc., or you could analyze documents and determine similarities between them or how they are related to other variables of interest in the data mining project. In the most general terms, text mining will turn text into numbers" \(meaningful indices\, which can then be incorporated in other analyses such as predictive data mining projects, the application of unsupervised learning methods \(clustering\, etc Three basic types of approaches to text mining have been prevalent. Co-occurrence–based methods do no more than look for concepts that occur in the same unit of text—typically a sentence, but sometimes as large as an abstract—and posit a relationship between them. \(See [2 r an ear l y  co occurrence–based system Two more common \(and more sophisticated\ approaches to text mining exist: rule-based or knowledge-based approaches and statistical or machine-learning-based approaches. The variety of types of rule-based systems is quite wide. In general, rule-based systems make use of some sort of knowledge. See [3 o r an ea r l y ru leb ased  s y stem   an d   4  fo r a discussion of rule-based approaches to various text mining tasks In contrast, statistical or machine-learning–based systems operate by building classifiers that may operate on any level from labelling part of speech to choosing syntactic parse trees to classifying full sentences or documents. \(See [5  f o r an  early learning-based system, and [4 o r a d i s c u ssi o n  o f  learning-based approaches to various text mining tasks Rule-based and statistical systems each have their advantages and disadvantages. For example, rule systems are often assumed \(not necessarily correctly\o take a significant amount of time to develop. Statistical systems typically require large amounts of expensive-to-get labelled training data. In practice, statistical and rule-based systems can be fruitfully combined. For example, a statistical system that classifies documents as to whether or not they are relevant to the subject of genetic variation in mouse genes might use the output of a rule-based mutation recognizer as one of its feature extractors. Many systems also employ an initial statistical processing step, followed by rule-based post-processing The following figure \(figure 1\ gives the principle text mining process Text Preprocessing  Syntactic/Semantic Text Analysis  Features Generation Bag of Words Feature Selection Simple Counting Statistics Text/Data Mining Classification- Supervised Learning Clustering- Unsupervised Learning Analyzing Results Fig 1 - text mining process T 


Combining BOW representation and Appriori algorithm for Text mining    ISIVC 2010 2 The paper is organized as follow Section 1 present the dissimilarity algorithm, section 2 present the Appriori algorithm and its application to BOW Sections 3 and 4 present text processing and tools II D ISSIMILARITY ALGORITHM A document d i is defined by the variables w 1 w N for N words\ . The value of variable w k indicate the presence or no of word k in document i  its BOW representation The dissimilarity between documents is defined by \( Jaccard Index    0 0          jk ik jk ik j i w or w k w w k d d d Where U denote the cardinal of set U In this distance the number of words in a considered document is missed. To solve this problem the value w ik is replaced by m ik the number of word k along the document  i \( Cosinus distance        N k jk N k ik N k jk ik j i m m m m d d d 1 2 1 2 1        The table of distance can be as input for clustering algorithm as K_means for example\.  In this work we prefer to use Appriori algorithm III A PPRIORI ALGORITHM Association rule mining is to find out association rules that satisfy the predefined minimum support and confidence from a given database. The problem is usually decomposed into two subproblems Find those itemsets whose occurrences exceed a predefined threshold in the database; those itemsets are called frequent or large itemsets Generate association rules from those large itemsets with the constraints of minimal confidence Suppose one of the large itemsets is L k I 1 I 2 I k  association rules with this itemsets are generated in the following way: the first rule is {I 1 I 2 I k  1 I k By checking the confidence this rule can be determined as interesting or not. Then, other rules are generated by deleting the last items in the antecedent and inserting it to the consequent, further the confidences of the new rules are checked to determine the interestingness of them. This process iterates until the antecedent becomes empty Since the second subproblem is quite straight forward, most of the research focuses on the first subproblem. The Apriori algorithm finds the frequent sets L in Database D Find frequent set L k  1  Join Step C k is generated by joining L k  1 with itself Prune Step Any \(k  1\ -itemset that is not frequent cannot be a subset of a frequent k -itemset, hence should be removed Where C k Candidate itemset of size k L k frequent itemset of size k Example:  In this experimentation let us consider the simple case of 5 documents constituted with 9 words. Let us define nine visits \(Table 1 TAB. 1- BOW representation  Documents Words  A B C D E w 1 1 1 0 0 1 w 2 0 1 0 1 0 w 3 0 1 1 0 0 w 4 1 1 0 0 1 w 5 1 0 1 0 0 w 6 0 1 1 0 0 w 7 1 0 1 0 0 w 8 1 1 1 0 1 w 9 1 1 1 0 0 Using the Appriori algorithm, an associated rule is generated  If you read A and E you can read B  If you read B and E you can  read A  If you read E you can read A and B This rule is deduced using a minimal support count =2, and minimum confidence of 70%. C 3 ABC\ \(ABE\and L 3 ABC\\(ABE IV T EXT PROCESSING Table 1 shows the BOW representation, now we will see how to extract this representation 


Combining BOW representation and Appriori algorithm for Text mining    ISIVC 2010 3 The initial step in automatic text processing is tokenisation 6 wh ich iden tif i e s the b a s i c t e x t ua l u n it s wh ich nee d no t  be further decomposed. Even this basic problem cannot be resolved straightforwardly by relying on white spaces and punctuation marks as explicit delimiters. Tokenisation is typically followed by some form of lexical processing, which may include part-of-speech tagging \(mapping of individual words to their lexical classes, eg noun, verb, adjective\, [7  word stemming \(reducing a word to its stem or root form, eg both inhibitor and inhibited are reduced to inhibit\ [8 o r  lemmatisation \(mapping a word to its lemma or the base form eg bind is the lemma for binds, bound, binding\ Syntactic processing usually involves parsing as the process of determining the syntactic structure of a whole sentence \(full or deep parsing\ or some of its parts \(partial or shallow parsing\[9   S y nt a c t i c st r u c t u r e  o f t e n i m pli e s t h e s e m a n t i c  relations between the concepts described Ontologies and semantic similarity Ontologies \( figure 2\  are typically organised in a hierarchy using the is-a relation between concepts. This property can be used to quantify the similarity between the concepts and implicitly, the semantic similarity between the terms used to designate these concepts [10   Such numerical information that can be inferred from an ontology, on top of the symbolic information it explicitly stores, is of particular value for TM applications For example, semantic similarity measure can be used as a vehicle of Machine learning  approaches \(instance-based approaches such as k-nearest neighbour and case-based reasoning\ [11  t o a va r i e t y  o f T M t a s k s  e g c l ust e r i n g 1 2   and classification [13 o f  bo th in div i du a l  t e r m s a nd t h e  documents containing them Fig 2 – Ontologies and semantic similarity V T EXT MINING S OFTWARE In this section we will give some TM computer programs which are available from a large number of commercial and open source companies Commercial text mining software can be expensive and difficult to learn. Several free open source languages can perform text mining Commercial software and applications AeroText, Attensity ,  SAS , SPSS Open-source software and applications GATE, UIMA, YALE/RapidMiner, TROPES We present her an example performed using TROPES [14  The document used for analyse is “Abraham Lincoln”. Figure 3 present Terminology extraction using TROPES   Fig 3 – Terminology extraction using TROPES We plan to extend our text mining system to use the concept features to represent text and to extract the more useful association rules that have more meaning 


Combining BOW representation and Appriori algorithm for Text mining    ISIVC 2010 4 In addition, we intend to conduct experiments on four real data sets \(Reuters-21578, OHSUMED, 20 Newsgroups, and Movies\o evaluate the performance of our approach for document classification [15, 16, 17, and 18   I n  t h e  f o ll o w ing  a description of each data set is provided 1. Reuters-21578 . This collection of documents is one of the most widely used for text categorization research The documents in the Reuters-21578 collection appeared on the Reuters newswire in 1987. The documents were assembled and indexed with categories by personnel from Reuters Ltd. and Carnegie Group, Inc 2. OHSUMED. The OHSUMED test collection is a set of 348,566 references from MEDLINE, the online medical information database, consisting of titles and/or abstracts from 270 medical journals over a fiveyear period \(1987-1991\. The available fields are title abstract, MeSH indexing terms, author, source, and publication type. About two-thirds \(233,445\ of the references contain an abstract. Each document is labeled with a subset of 500 of the MeSH terms 3. 20 Newsgroups \(20NG\. The 20 Newsgroups data set is a popular collection of approximately 20,000 newsgroup documents, partitioned nearly evenly across 20 different newsgroups \(about 1,000 documents per class 4. Movie Reviews \(Movies\. This collection contains 2,000 reviews of movies from the Internet Movie Database archive. Half of the reviews express a positive sentiment \(opinion\out the movie, and half express a negative opinion Moreover, we intend to visualize the extracted association rules in graphical representation in two or three-dimension association networks VI CONCLUSION This paper advocate the use of BOW representation as input for Appriori algorithm Association rules, for instance, can tell what is the next document you would like to read, and help you finding the information you are looking for, even if you hadn’t realized it yet. This ability for “guessing” the user’s wishes can be provided to the site by analysing the usage of documents by other users, and finding out about their own preferences R EFERENCES 1 S e e K i on g N g  2 0 06 In t e gr a t i n g t e xt  mi n i n g w i t h d a t a m i n i n g   I n   Ananiadou S, McNaught J, editors. Text mining for biology and biomedicine. Boston/London: Artech House 2 J e n sse n T K   Lægr e i d  A  K o mor o w s ki J, H o vi g E  2 001 A  l i t e r a t u re network of high-throughput analysis of gene expression. Nat Genet 28 21–28 3 B l a s c h k e C  A ndr ade MA  Ouz o un i s  C  Va l e ncia A 1 99 9  A u t o m a t i c extraction of biological information from scientific text: Protein–protein interactions. Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology. Menlo Park \(California AAAI Press. pp. 60–67 4 C o h e n K B  H u nt er L 200 4   N a t u ral  l a nguag e  p r o c e s s i ng a n d sy st e m s biology. In: Dubitzky W, Pereira F, editors. Artificial intelligence and systems biology. Berlin: Springer Verlag 5 C r a v e n M K u m l e i n J 1 9 99 C o nst r uct i ng bi ol og i c a l  k n o w l e dg e b a s e s  by extracting information from text sources. Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology Menlo Park \(California\ AAAI Press. pp. 77–86 6  We b s t e r  J. J  a n d K i t  C    1 992   T oke n i za t i on a s t h e i n i t i a l  ph a s e  i n  NLP’, in ‘Proceedings of the 15th International Conference on Computational Linguistics’, Nantes, France, pp. 1106–1110 7 B ri l l  E   1 9 9 2    A  s i m p l e r u l e b as e d p a rt o f  s p e e c h  t a g g e r   i n  Proceedings of the 3 rd Conference on Applied Natural Language Processing’, Trento, Italy, pp. 152–155 8 H ul l  D   A  1996  S t e m m i ng a l gor i t hm s   A c a s e  s t ud y fo r det a i l ed evaluation’, J. Amer. Soc. Information Sci., Vol. 47\(1\, pp. 70–84 9 J ur a f sk y   D  a n d M a r t i n   J. H   2 0 00  S pee c h  a nd L a n g u a ge Processing: An introduction to natural language processing computational linguistics and speech recognition’, Prentice Hall, Upper Saddle River, NJ 10 L o r d  P St e v e n s R   Br a ss A  a n d G o b l e   C   2003\ ‘Semantic similarity measures as tools for exploring the gene ontology,’ in Proceedings of the 8th Pacific Symposium on Biocomputing’, 3rd–7th January, Hawaii, Altman, R. et al., Eds, World Scientific Publishing Company, Singapore, pp. 601–612 1 M i t c hel l  T 199 7   Mac hi ne L earni ng    McGraw Hill, New York 12 N e n a d i c  G  Sp a s i c   I  a n d A n a n i a d o u  S  2 004  M i n i n g t e r m  similarities from corpora’, Terminology, Vol. 10\(1\, pp. 55–80 1 S p as i c   I    A n a n i a dou  S   an d T s uj i i J  20 0 5    M aS T e rC l a s s   A ca s e based reasoning system for the classification of biomedical terms Bioinformatics, Vol. 21\(11\, pp. 2748–2758 1 TR OPES  Hi gh Perfo rm ance  T e x t A n a l y s is fo r Pro f es s i o n al Us ers   1994 http://www.semantic-knowledge.com/tropes.htm 1 C a r n eg i e G r o up I n c  and R e u t ers L t d R e u t er s 2 1 5 7 8 t e x t  categorization test collection, 1997 16 W  Her s h  C  B u ck l e y  T   L e o n e an d D Hi c k a m  Oh su m e d   A n  interactive retrieval evaluation and new large test collection for research In Conference on Research and Development in Information Retrieval Dublin, Ireland, 1994. ACM/Springer 1 K L a ng  N e ws wee d er  L earn i ng to f i l t er ne t n ews   I n I n tern ati onal  Conference on Machine Learning, Tahoe City, California, 1995. Morgan Kaufmann 1 B  P a ng   L   L e e  and S  V a i t h y anat ha n Thum b s up? sent i m ent  classification using machine learning techniques. In Conference on Empirical Methods in Natural Language Processing, Pennsylvania Philadelphia, 2002 


is proposed to integrate ILP and statistical modeling for document classification and retrieval Given this conceptual framework, we can describe the most recent approaches to the maximal frequent itemset problem. As a baseline, Apriori traverses the lattice in a pure breadth-first manner, discovering all frequent nodes at level k before moving to level \(k+1 support information by explicitly generating and counting each node. Max Miner performs a breadth-first traversal of the search space as well, but also performs look aheads to prune out branches of the tree. The look aheads involve superset pruning, using apriori in reverse \(all subsets of a frequent itemset are also frequent work better with a depth-first approach, but Max Miner uses a breadth-first approach to limit the number of passes over the database. Depth Project performs a mixed depth-first traversal of the tree, along with variations of superset pruning. Instead of a pure depth-first traversal Depth Project uses dynamic reordering of children nodes With dynamic reordering, the size of the search space can be greatly reduced by trimming infrequent items out of each nodes tail. Also proposed in Depth Project is an improved counting method and a projection mechanism to reduce the size of the database. The other notable maximal pattern methods are based on graph-theoretic approaches MaxClique and MaxEclat both attempt to divide the subset lattice into smaller pieces \(cliques these in a bottom-up Apriori-fashion with a vertical data representation. The VIPER algorithm has shown a method based on a vertical layout can sometimes outperform even the optimal method using a horizontal layout. Other vertical mining methods for finding FI are presented by Holsheimer and Savasere et al. The benefits of using the vertical tid-list were also explored by Ganti et al. GenMax is a backtrack search based algorithm which was proposed by Karam Gouda and Mohammed J. Zaki[32] for mining maximal frequent itemsets. GenMax uses a number of optimizations to prune the search space. It uses a novel technique called progressive focusing to perform maximality checking, and diffset propagation to perform fast frequency computation III. PROPOSED APPROACH 


The proposed approach focuses on Mining Maximal Frequent Itemset Generation. In this paper, Array based approach and Effective Pruning Mechanism is used for generating Maximal frequent patterns There are two main ingredients to develop an efficient MFI algorithm. The first is the set of techniques used to reduce the size of search space, and the second is the representation used to perform fast frequency computations. This paper describes how proposed algorithm achieves the same  In general the structure of the transactional database may be in two different ways -Horizontal data format and Vertical data format. Here, we are using vertical data format for storing the transactions in the database. In vertical data format, the data is represented as item-tidset format, where item is the name of the item and tidset is the set of transaction identifiers containing the item Consider our example database which includes six different items, I = {A, B, C, D, E, F} and six transactions T= {1, 2, 3, 4, 5, 6}. The vertical data format of the database DB is given below Table 1 : Vertical Data format of the transactional database DB All Frequent items are extracted first. The support is directly given by the number of transactions in the tidset of each item. Let us consider the minimum support to be 3 From the above structure, all items except F are frequent The items A, B, C, D and E are frequent items and will be considered to next level In the next level a dynamic array \(N intersecting the tidsets of every two frequent items. The constructed array \(N two frequent items. The size of the array will be n\(n+1 where n is the number of frequent items. Value of each and every cell in the array is initialized to zero. The value of cell N[i,j] is 1 if number of transaction occurred in the intersection of tidset of frequent item i and j satisfies the user specified minimum support. Otherwise the cell value is zero. Once the array is constructed successfully, all possible Maximal frequent itemsets\(PMFI from the array. The constructed array is given below N ABCD 


E D C B  Table 2: Dynamic array \(N frequent items The possible maximal frequent itemsets \(PMFI obtained from this array are considered to the next level All the other itemsets are pruned. The number of possible large itemsets is less than or equal to number of frequent itemsets. From this array, PMFIs are obtained in two ways. We can take all columns and first row. \(Columns A B, C, D and row E Rows E D, C, B and Column A One column entry or one row entry is a single PMFI. For example the itemset \(x,y,z itemset if and only if there is an entry 1 in both cells N[x,y] and N[x,z]. For example ADC is a PMFI because of the values of both cells N [A, D] and N [A, C] is 1 Once all PMFIs are retrieved from the array, they are arranged in descending order with respect to their size First column entries produce the PMFI that includes the frequent item A \(ADC the PMFI includes the frequent item B \(B entries produce the PMFI that includes the frequent item C CDE the frequent item D \(D PMFI that includes the frequent item E \(EC are arranged in descending order with respect to their size So the Itemsets in PMFI are ADC, CDE, EC, B, D The First itemset ADC has no superset in MFI and it is frequent. So ADC is a MFI\(with support count 3 added to MFI The Second itemset CDE has no superset in MFI and it is a infrequent item set. so all \(n-1 includes C CDE are generated. 2-itemsets of CDE are CD and CE.\(DE is not taken for test. It does not include the item C ignored. CE is frequent and it has no supersets in MFI. So it is added to MFI. CE is a MFI\(with support count 3 obtained from CDE 


The Third itemset EC is already included in MFI and it is ignored The Forth item B has no superset in MFI and it is a frequent item and it is added to MFI The Fifth item D has a superset in MFI and it is ignored From above example, MFIs with support count 3 are ADC, CE, B The process will be continued till testing all possible maximal frequent itemsets. The pruning can be done while finding the MFI itself, but not after finding FI completely The pseudo code for proposed algorithm is given below in figure 1  Item  Tidset A  T1, T2, T3, T4 B  T1, T4, T5 C  T1, T2, T3, T5, T6 D  T1, T2, T3 E  T2, T4, T5, T6 F  T6, T5    Pseudo code Find All MFI\(PMFIs,min_sup all Maximal Frequent Itemsets Inputs i Possible Maximal Frequent itemsets ii for mining process Interfacing Functions Output i Find All MFI\(PMFIs, min_sup For each x  PMFIs if x has a superset in MFI continue else if x is frequent MFI=MFI U x else Find MFI by obtaining Permutations\(x,min_sup For End return MFI 


Find MFI by obtaining Permutations \(PMFI, min_sup Function to find Maximum Frequent Itemsets form Kitemsets of PMFI Inputs i ii defined for mining process Interfacing Functions Figure 1. Pseudo code for ABMFI Algorithm Output i Find MFI by obtaining Permutations \(PMFI, min_sup n=number of items in PMFI k=n-1; // k-itemsets Freq_item={}; S={}; do In_Freq=0; //to check infrequent itemsets C=generate k-itemsets that includes first item of PMFI; Foreach x C if x has a superset in MFI S=S U x; continue; else if x is frequent Freq_item=Freq_item U x; else In_Freq=1; For End; If PMFI==unique\(Freq_item U S MFI=MFI U Freq_item; return; End if; K--; while\(In_freq!=0 && k!=0 The proposed algorithm performs better because MFI is being calculated directly before computing FI completely The Pruning mechanism works effectively and counting is not performed for the subset of MFIs. So, the time taken to compute MFI is negligible. As we are following vertical data format, support also need not be calculated separately In this case, support is directly given by the number of transactions in the tidlist of each FI. The vertical representation has the following major advantages over the horizontal layout: Firstly, computing the support of itemsets is simpler and faster with the vertical layout since it involves only the intersections of tidsets. Secondly, with the vertical layout, there is an automatic reduction of the database before each scan in that only those itemsets that are relevant to the following scan of the mining process are accessed from disk Pruning The Possible Maximal Frequent itemsets\(Maximal 


Candidate Itemsets array. All Maximal frequent itemsets are obtained from only these PMFIs. Other itemsets are pruned automatically. The pruning can be done while finding the MFI itself, but not after finding FI completely. The proposed approach applies superset checking to eliminate the non maximal frequent itemsets. Once all PMFIs are generated, each PMFI is checked whether it is a subset of any maximal pattern. If so the itemset is eliminated entirely. Counting is not performed for this itemset and next PMFI is taken for test IV. RESULTS The testing of the proposed algorithm has been carried out on the real dataset \(containing long itemsets the number of candidate itemsets taken by the proposed algorithm to find MFIs and it is compared to Genmax algorithm for various values of minimum support. The support is varied from 75 to 95. The proposed algorithm had been compared with GenMax algorithm and results show the proposed approach generates less number of candidate itemsets to find all MFIs Figure 2 illustrates that, the proposed approach generates less number of candidate itemsets and produces all MFIs very quickly than GenMax algorithm. Support is taken as x axis and the number of candidate itemsets taken to find all MFI is taken as y axis For Mushroom, the improvement is best explained by how the MFI is computed at each level and found directly without waiting for FI completely. This leads to a much greater reduction in the overall search space, since the reductions is so great at highest levels  Figure 2. Number of Candidate itemsets taken by ABMFI and GenMax Algorithm from Mushroom dataset This approach will be working very efficiently for any sparse and dense dataset, when the size of maximal frequent itemsets is close to the number of frequent itemsets V. CONCLUSION In this paper we have investigated an array based approach and algorithm \(ABMFI itemsets. The algorithm is straight forward  basic steps are finding frequent items from the database, Dynamic 


array construction and Pruning infrequent itemset obtaining Possible Maximal Frequent itemsets from array and finding MFIs from PMFIs. Our algorithm had been compared with GenMax algorithm and obtained that the proposed algorithm generates less number of candidate itemsets to find all MFIs. The vertical data format representation of the database, Dynamic array construction and directly computing MFIs from PMFIs are the added advantages of this algorithm REFERENCES 1]. Agrawal, R., T. Imielinski and A. Swami, 1998. Mining association rules between sets of items in very large databases In the Proceedings of the ACM SIGMOD International Conference on Management of Data, May 25-28, Washington D.C., US, pp: 207-216. http://doi.acm.org 10.1145/170035.170072. [2]. Jiawei Han and Micheline Kamber, 2001. Data Mining: Concepts and Techniques. 1st Edn., Morgan Kaufmann pp: 500. ISBN-10: 1558604898. [3 Ganti, V., J. Gehrke and R. Ramakrishnan, 2000. DEMON mining and monitoring evolving data. ICDE 2000, San Diego CA., pp: 439-448. http://wwwdb.cs.wisc.edu/dbseminar/spring00 / talks/demon_paper.pdf [4 Holsheimer M., M. Kersten, H. Mannila and H. Toivonen, 1995 A perspective on databases and data mining. Proceeding of the 1st International Conference on Knowledge Discivery and Data Mining, Aug. 1995, AAAI Press, Montreal, Canada, pp 150-155, http://www.cs.helsinki.fi/ research fdk/datamining/pubs/kdd95.ps.gz [5]. Savasere, A., E Omiecinski and S. Navathe, 1995.An efficient algorithm for mining association rules in large databases. Proceedings of 21 st International VLDB Conference on Very Large Data Bases, Sep. 11-15, Morgan Kaufmann Publishers Inc. San Francisco, CA, USA ., pp:432-444 http://portal/acm.org/citation.cfm?id=673300 [6]. Ramesh C Agarwal, Charu C. Aggarwal and V.V.V. Prasad, 2001. A tree projection algorithm for generation of frequent itemsets. J Parallel Distribut. Comput., 61: 350-371. DOI: 10.1006 jpdc.2000.1693 [7]. Agrawal, R. H. Mannila, R. Srikant, H Toivonen and A.I. Verkamo, 1996. Fast Discovery of Association Rules. In: Advances in Knowledge Discovery and Data Mining, Usama Fayyad, M.G.P. Shapiro, P. Smyth, and R 


Uthurusamy,\(Eds pp: 307 -28 I. SBN:0-262-56097-6 [8]. Aggarwal, C.C. and P.S Yu, 1998. Mining large itemsets for association rules. Bull IEEE Comput. Soc. Technical Committee Data Eng.,: 23-31 http://citeseerx.ist.psu.edu/viewdoc/summary?doi 10.1.1.48.306 [9]. Aggarwal, C.C. and P.S. Yu, 1998. Online generation of association rules. In Proceedings of the 14th International Conference on Data Engineering, Feb. 23-27,IEEE Xplore, Orlando, FL, USA., pp: 402-411. DOI 10.1109/ICDE.1998.655803 [10]. Mohammed J. Zaki, 2000 Scalable algorithms for association mining. IEEE Trans. Knowl Data Eng., 12: 372 390. DOI: 10.1109/69.846291. [11]. Shenoy, P., J. Haritsa, S. Sudarshan, G Bhalotia, M. Bawa and D. Shah, 2000. Turbo-charging vertical mining of large databases. Proceeding of ACM SIGMOD International Conference on Management of Data, June 2000, Dallas, Texas USA,pp:22-33.http://doi.acm.org/10.1145/ 335191.335376 [12 Gunopulos, D., H. Mannila and S. Saluja, 1997. Discovering all most specific sentences by randomized algorithms. In Proceedings of the 6 th International Conference on Database Theory, Jan. 08-10, Springer-Verlag London, UK pp:215-229. http://portal.acm.org/citation.cfm? id=65 6097 [13]. Agrawal, R and R. Srikant, 1994. Fast algorithms for mining association rules Proceedings of the 20th International Conference on Very Large Databases Sep. 12-15, Santiago de chile, Chile, pp: 487-499. DOI: 10.1.1.40.7506. [14 Lin, D.I. and Z.M. Kedem, 1998. Pincer search: A new algorithm for discovering the maximum frequent sets. In Proceedings of the 6 th International Conference on Extending Database Technology, Mar. 23-27, Springer-Verlag London, UK.,pp:105-119. http://portal.acm.org/citation.cfm id=645338.6503 96. [15]. Park, J.S., M.S. Chen, P.S. Yu, 1995. An effective hash based algorithm for mining association rules. ACM SIGMOD Record 24: 175-186. http://doi.acm.org/10.1145/ 68271.223813 [16]. Rin Popescul and Lyle H. Ungar and Steve Lawrence and David M. Pennock 2002. Towards structural logistic regression: Combining relational and statistical learning. In Proceedings of KDD2002 Workshop on Multi-Relational Data Mining 02, ACM, Alberta, Canada, pp: 130-141 http://citeseerx.ist.psu.edu/ viewdoc/summary?doi=10.1.1.19.6235 [17 Taskar, B., E. Segal and D. Koller, 2001. Probabilistic classification and clustering in relational data. In Proceedings of the 17 


th International Joint Conference on Artificial Intelligence 01, Lawrence Erlbaum Associates Ltd USA.,pp:870-876 http://direct.bl.uk/bld/PlaceOrder.do?UIN=107907 71&ETOC=RN&from=searchengine 18]. Dunkel, B. and N. Soparkar, 1999. Dataorganization and access for effcient data mining. In the Proceedings of the 15th International Conference on Data Engineering, Mar. 23-26, IEEE Xplore, Sydney, NSW, Australia, pp 522-529. DOI: 10.1109/ICDE.1999.754968 [19]. Mohammed Zaki, J. and C.J Hsiao, 2002. CHARM: An efficient algorithm for closed itemset mining. In Proceedings of SDM02Conference http://citeseerx.ist.psu.edu/viewdoc /summary?doi=10.1.1.111.520 20]. Bastide, Y., R. Taouil, N. Pasquier, G. Stumme and L. Lakhal, 2000 Mining frequent patterns with counting inference. ACM SIGKDD Explorations Newsletter,2:66-75. http://doi.acm.org/10.1145 /380995 381017 [21]. Pasquier, N., Y. Bastide, R. Taouil and L. Lakhal, 1999 Discovering frequent closed itemsets for association rules. In Proceedings of the 7 th International Conference on Database Theory,Jan. 10-12 Springer-Verlag London, UK., pp:398-416. http://portal acm.org/citation.cfm?id=645503.656256 [22]. Getoor, L., N. Friedman, D Koller and B. Taskar, 2001. Learning probabilistic models of relational structure. In Proceedings of International Conference on Machine Learning ICML'01 177.http://direct.bl.uk/bld/PlaceOrder.do?UIN=100556 121&ETOC=RN&from=searchengine [23]. Zaki, M.J., S Parthasarathy, M. Ogihara and W. Li, 1997. New algorithms for fast discovery of association rules. In Proceeding of the 3 rd International Conference on Knowledge Discovery and Data Mining 97, AAAI Press, pp: 283-286 http://citeseerx.ist.psu.edu/viewdoc/summary?doi 10.1.1.42.5143 [24]. Zaki, M.J., 2000. Generating non-redundant association rules. In Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Aug. 20-23, Boston, Massachusetts, US., pp: 34-43 http://doi.acm.org/10.1145/347090.347101 [25]. Ganter B. and R. Wille, 1999. Formal Concept Analysis: Mathematical Foundations. 1st Edn., Springer-Verlag, USA., pp 284. ISBN-10: 3540627715 


26]. Gouda, K. and M.J. Zaki, 2001. Efficiently mining maximal frequent itemsets. In the Proceedings of International Conference on Data Mining, Nov 29-Dec. 02 2001, IEEE Computer Society Washington, DC, USA., pp 163-170. http://portal.acm.org/citation. cfm?id = 645496.6580 47&coll=GUIDE&dl=GUIDE [27]. Bayardo, R.J., 1998. Efficiently mining longpatterns from databases. In the Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data, Seattle, June 001-04 Washington, United States, pp: 85-93 http://doi.acm.org/10.1145/276304.27631 [28]. Gunopulos, D., H. Mannila and S. Saluja, 1997. Discovering all the most specific sentences by randomized algorithms. In Intenational Conference on Database Theory, Jan 08-10, Springer-Verlag London,UK.,pp:215-229 http://portal.acm.org/citation.cfm?id=645502.6560 97 [29]. Burdick D., M. Calimlim and J. Gehrke, 2001. MAFIA: A maximal frequent itemset algorithm for transactional databases. In International Conference on Data Engineering, Apr. 02-06, IEEE Computer Society Washington, DC, USA pp:443-452 http://portal.acm.org/citation.cfm?id=645484.6563 86&coll=GUIDE&dl=GUIDE. [30]. Agrawal, R., C. Aggarwal and V Prasad, 2000. Depth first generation of long patterns. In the Proceedings of the 6th ACM SIGKDD international Conference on Knowledge Discovery and Data Mining, Aug. 20-23, Boston, Massachusetts, United States, pp: 108-118. http://doi.acm.org/10.1145/347090.347114 31]. A.M.J. Md. Zubair Rahman and P. Balasubramanie,2008 Kongu Engineering College, Perundurai, Tamilnadu, India An Efficient Algorithm for Mining Maximal Frequent Item Sets [32]. Karam Gouda and Mohammed J. Zaki, 2005. GenMax: An Efficient Algorithm for Mining Maximal Frequent Itemsets. In the Proceedings of the Data Mining and Knowledge Discovery, 11, 120, 2005. [33]. Han, J., J. Pei and Y. Yin, 2000. Mining frequent patterns without candidate generation In the Proceedings of the 2000 ACM SIGMOD International Conference on Management o f Data, May 15-18, Dallas, Texas, United States, pp 1-12 http://doi.acm.org/10.1145/342009.335372 


experts can start their ore deposits estimations with much clearer data which is easier to deal with XII. CONCLUSION We have presented how data mining can be applied to the borehole data coming from active mine area. We are certain that data mining has a huge potential for other types of borehole data. One of the important steps in data mining is the preparation of data into useful form for various algorithms. Together with domain experts we have identified a way for transforming data to a form acceptable to k-NN classification and association rules mining algorithms Although we have shown how the k-NN classification and association rules mining techniques can be applied here this framework will open new possibilities to perform other data mining tasks to this type of data. Our experimental results are very promising in this regard as they show that we are not only able to match the accuracy of the results with IDW method used in the mining industry, but also exceed it \(93.1% accuracy was obtained by 3-NN method while IDW gave 88.5% accuracy rules as a separate analysis tool can improve not only k-NN classification results but also IDW interpolation results. This is particularly important for convincing mining companies 119 of the benefits of data mining, as with a little effort they can improve the method they are already using As we have shown in this paper, not only choosing right classification techniques can help us to improve interpolation, but more general analysis on data like association rules can contribute a lot. Thus, the more knowledge we have on the hidden relationships and patterns the more accurately you can construct an interpolation. Moreover, discovering hidden correlations not only important for this task, but also can contribute to understanding about complex geological processes that this specific area undergone. Therefore data mining, which can discover useful knowledge purely from data is of great importance and will be area of research for next generation of exploration and mining specialists Finally, we have proposed to use mathematical morphology for filtering the results of rock type interpolation. In the example given here, we have seen that it performed well in removing relatively small objects and filtering out large 


areas of interest from rock types XIII. FUTURE WORK This paper provides a framework for using data mining techniques on the borehole data. Using this framework we would like to investigate possibilities of using other classification techniques on this type of data. We mentioned that borehole data can contain more information about the area besides spatial coordinates, rock types and metal grades so classification that utilizes this extra information will be on our immediate research agenda Application of mathematical morphology provides possibilities for further research on domaining \(filtering out large areas of interest be given also to this ACKNOWLEDGEMENT I want to thank the Director of the WH Bryan Mining and Geology Research Centre, at the University of Queensland Professor Alan Bye and his PhD student Mr Younes Fadakar Alghalandis for contributing their expertise to this research This work is supported by the AuScope National Collaborative Research Infrastructure Strategy by the Australian Commonwealth, the Queensland State Government and The University of Queensland REFERENCES 1] A. G. Journel and C. J. Huijbregts, Mining geostatistics Academic Press, London, 1978 2] D. Shepard, A two-dimensional interpolation function for irregularly-spaced data, in Proc. ACM Annual Conference 1968, pp. 517524 3] C. Caruso and F. Quarta, Interpolation methods comparison Computers Math. Applications., vol. 35, pp. 109126, 1998 4] K. Gibert, M. S?nchez-Marre`, and I. Rodr?guez-Roda, Short communication: Gesconda: An intelligent data analysis system for knowledge discovery and management in environmental databases, Environ. Model. Softwares, vol. 21, no. 1 pp. 115120, 2006 5] A. R. Solow, Mapping by simple indicator kriging, Mathematical Geology, vol. 18, no. 3, pp. 335352, 1986 6] M. Armstrong, Problems with universal kriging, Mathematical Geology, vol. 16, no. 1, pp. 101108, 1984 7] N. Roussopoulos, S. Kelley, and F. Vincent, Nearest neighbor queries, in Proc. SIGMOD Conference, 1995, pp. 7179 


8] M. Ankerst, H.-P. Kriegel, and T. Seidl, A multistep approach for shape similarity search in image databases, IEEE Trans Knowl. Data Eng., vol. 10, no. 6, pp. 9961004, 1998 9] K. S. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft When is nearest neighbor meaningful? in Proc. of Database Theory - ICDT 99, 7th International Conference 1999, pp. 217235 10] P. Ciaccia and M. Patella, Pac nearest neighbor queries Approximate and controlled search in high-dimensional and metric spaces, in Proc. of ICDE, 2000, pp. 244255 11] X. Wu, V. Kumar, J. Quinlan, J. Ghosh, Q. Yang, H. Motoda G. McLachlan, A. Ng, B. Liu, P. Yu, Z.-H. Zhou, M. Steinbach, D. Hand, and D. Steinberg, Top 10 algorithms in data mining, Knowledge and Information Systems, vol. 14, no. 1 pp. 137, 2008 12] X. Cheng, R. Dolin, M. O. Neary, S. Prabhakar, K. Ravikanth D. Wu, D. Agrawal, A. El Abbadi, M. Freeston, A. K. Singh T. Smith, and J. Su, Scalable access within the context of digital libraries, in Proc of ADL, 1997, pp. 7081 13] C. Faloutsos, M. Ranganathan, and Y. Manolopoulos, Fast subsequence matching in time-series databases, in Proc. of SIGMOD Conference, 1994, pp. 419429 14] F. Korn, N. Sidiropoulos, N. Faloutsos, E. Siegel, and Z. Protopapas, Fast nearest neighbor search in medical image databases, in Proc. of VLDB, 1996, pp. 215226 15] T. Kahveci and A. K. Singh, Efficient index structures for string databases, in Proc. of VLDB, 2001, pp. 351360 16] M. Ankerst, G. Kastenmuller, H.-P. Kriegel, and T. Seidl Nearest neighbor classification in 3d protein databases, in Proc of International Conference on Intelligent Systems for Molecular Biology, ISMB, 1999, pp. 3443 17] R. Agrawal and R. Srikant, Fast algorithms for mining association rules in large databases, in Proc of VLDB, 1994 pp. 487499 18] B. Liu, W. Hsu, and Y. Ma, Integrating classification and association rule mining, in Proc of KDD, 1998, pp. 8086 19] J. Serra, Image Analysis and Mathematical Morphology Orlando, FL, USA: Academic Press, Inc., 1983 120 


http://datamining.buaa.edu.cn/TopKCos.pdf 14] M. Zaki, Scalable algorithms for association mining, TKDE, vol. 12, pp. 372390, 2000 


enhance item-based collaborative filtering, in 2nd IASTED International Conference on Information and Knowledge Sharing, Scottsdale, Arizona, 2003 476 2010 10th International Conference on Intelligent Systems Design and Applications 


Basi Association Rles Basi c  Association R u les Association is basically connecting or tying up occurrences of Association is basically connecting or tying up occurrences of events Ol dib t f ilt t O n l y d escr ib e se t s o f s i mu lt aneous even t s Cannot describe patterns that iterate over time e g  itemset a  0  b  0  g    Eg If you sense higher data rates on the downlink than normal AND New Route generated Implies high chances of Intrusion AND New Route generated Implies high chances of Intrusion Associative IDS for NextGen Frameworks Dr S Dua LA Tech 20 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


