Event Data Warehousing for Complex Event Processing  Heinz Roth Secure Business Austria Favoritenstrasse 16 1040 Vienna, Austria roth@securityresearch.at de-AT Josef Schiefer, Hannes Obweger, Szabolcs Rozsnyai de-AT Senactive GmbH de-AT Inkustra\337e 1-7 de-AT 3400 Klosterneuburg, Austria de-AT jschiefer, hobweger, srozsnyai}@senactive.at de-AT  de-AT  de-AT  Abstract   In the last few years Complex Event Processing CEP has emerged as a new paradigm for event-driven applications The research focus in this area has so far bee n primarily on operational issues and not on the 
ex post analysis of event data. On the other side, approaches like data warehousing have proven useful in the past to extract further valuable information from the data available within an organization In this paper, we elaborate the concept of a fully-implemented event data warehouse as an add-on for CEP that allows to efficiently archive and query valuable event data for later analysis We outline the overall architecture and describe the relevant metamodels for an integrated data management approach The data management itself is implemented using a RDBMS and its schema is automatically synchronized with the CEP models Finally we present a real-world use case to illustrate the 
application of the event data warehouse in practice Complex Event Processing, Data Warehousing, Event Streams I   I NTRODUCTION  Complex Event Processing \(CEP\ provides a new paradigm to collect and process data flowing through an organization in near real time in order to automate and accelerate decision cycles while facilitating an agile approach to keep the underlying business logic adaptable to changing business requirements Until today, the main research focus in the field of CEP has been devoted towards operational issues e.g performance query and rule implementations event type models or event correlation 11 
The persistence of event data, even for pure ly  operational tasks like recoverability or scalability has not yet been studied in greater detail in any scientific materials that we are aware of Nevertheless, the usage of business data for expost analytical processing has been studied in great detail in the context of data mining and data warehousing \(DWH 9  In this paper, we propose a DWH approach to be used for data processed by CEP applications i.e event data The advantages of such an approach are not limited to those of traditional data warehousing Knowledge gained from eventdata analysis can also help to adapt and improve the underlying 
CEP applications. The seamless integration between CEP and an event data warehouse system allows reusing event-type- and event-correlation information available in CEP solutions wh ich facilitates the time-consuming and expensive task to setup and change the data integration process es  In this paper we describe an approach to store event data processed by CEP applications efficiently in an event data repository for ex-post analysis. We refer to the repository as an Event Data Warehouse \(EDWH\ in the remainder of this paper We extend the DWH definition of Bill Inmon 9 and define an EDWH as follows An Event Data Warehouse EDWH is a subject-oriented 
integrated time-variant and non-volatile collection of event data in support of operational or management's decisionmaking process. The EDWH stores data items which originate from a continuous stream of events and stores the following types of data items 1 data items representing the original events in event streams 2 data items capturing relationship information for event sequences resulting from event correlations and 3 derived or calculated data items from events or event streams. An EDWH supports a query language for accessing all three types of data items In order to analytically process event data ex post, it has to be permanently archived somewhere. The common solution for 
this problem is to make use of a data warehouse DWH approach where data is added periodically in a batch using an extract, transform and loading \(ETL\ process. We identified a list of requirements which existing CEP systems do not sufficiently address Analytical Processing  Existing CEP systems support flexible pattern-matching mechanisms most of them SQLbased or rule-based Although pattern matching can be quite complex there is no sophisticated analytical data processing such as any kind of forecasting classification clustering or association rule learning Data Aggregations CEP systems allow to aggregate data of event streams within a certain time window Nevertheless 
this data aggregation is in most cases limited to  pattern detection There is no simple way of accessing generated aggregates or the current value of aggregates. Furthermore, it is difficult to have aggregate levels for data items which is a prerequisite for many types of data analysis Processing Historical Data  CEP systems only have a temporary memory and capture historical event data only for a certain period of time time windows If a CEP system goes 978-1-4244-4840-1/10/$25.00 ©2010 IEEE 


offline all event data is gone Many business applications require at least some persistent memory in order to maintain state for event stream processing Traceability  Existing CEP systems are not able to adequately maintain and track event processing steps event correlations calculated aggregates etc Many CEP system vendors offer debugging tools for finding out what is going on during the event processing however there is no way to ask the system for all generated downstream or upstream artifacts for a single event II  R ELATED W ORK  Related work in this field can be split up into CEP and DWH approaches Several different academic and commercial CEP implementations exist today varying to a great extent in expressiveness of rules and queries, ease of use, performance and the way they are integrated into the overall IT landscape of an organization. Many solutions use an SQL-based approach to query a stream of events e.g the open-source CEP engine Esper 7  or Aurora 3  as well as its successors Borealis 2  and Medusa 29  The queried data itself is nevertheless transient and kept in memory only. Another important concept in CEP is the definition of rules, which is a focus of RuleCore 21  an eventdr iven rule processing engine based on EventCondition-Action ECA rules Correlation of events i.e to create an ordered set of related events from an incoming event stream is a second essential concept in CEP studied by Wu et al 28 and Chen et al 6  AMIT 4  is another event stream engine that provides a mature user interface to model business situations with four types of entities events situations lifespans and keys The definition of an event is the base entity which can be related to other events and is comparable to the event type definition used in this paper Lifespans are used to define time intervals between related events to describe certain situations A situation is the main instance for specifying queries The importance of a solid event model and event typing is also crucial for event mining applications Moen 12  13  describes algorithms which use typed events to determine similarities between event objects in order to discover similar event patterns An EDWH complements this approach by providing the data needed to feed such event-mining applications The second field related to the proposed solution is the domain of DBMS and traditional data warehouses. CEP can be seen as advancement from Active Databases 27 While CEP   model The difference as pointed out by Stonebraker 22  is  processing requires all data to be stored  processing allows the data to be processed first and only optionally stored afterwards resulting in a significant rise in performance Another shortcoming of Active Databases is the lack of full-fledged programming support to execute the actions derived from evaluating a set of rules usually in the form of database triggers Stonebraker 22  further argues t   centric applications and predicts that the commercial world will fracture into a collection of independent database engines We tend to agree with Stonebreaker that event stream processing within a single database system is no longer applicable However we also believe that separating database engines opens a completely new set of technical problems which have to be solved DWH and especially Active DWH 20  pursue a similar approach as we do in this paper Active Data Warehouses combine active mechanisms based on ECA \(Event-ConditionAction rules with analysis capabilities of data warehouse solutions to extend \(passive\ systems with reactive capabilities They are event-driven since data is continuously updated  providing a near real-time perspective on an organization Aside from sharing the same shortcomings as Active  biggest difference to an EDWH is their loosely coupled integration into an IT landscape involving time-consuming activities to implement them from scratch or adapt them according to changing business needs III  A RICHTECTURAL O VERVIEW  The EventBase architecture provides in contrast to other traditional DWH approaches, the ability to store and maintain events in real time in a central repository together with historical and analytical data This type of persistent organization of events allows for seamless access on real-time and historical event data. Furthermore, the typical ETL steps of a DWH and analytical solutions are managed by the event processing model Figure 1 illustrates the SARI architecture The bottom of the figure shows different source systems i.e the event producing components continuously generate event notifications A sense layer represents the adapters of SARI that can be docked to event-producing systems or the communication infrastructure The adapters gather events in either a push or pull process and propagate them into the event processing realms 


The internal communication infrastructure uses an event bus for publishing the received events to the event processing models SARI uses sockets as a generic interface for sending and receiving events to and from event processing models. The processing of event streams is performed in event processing maps where the flow of events is modeled with various components according to the business requirements For the event  event data to the database In other words users can define for any type of socket whether received events shall be stored in the database The EventBase   model with an efficient upto date operational storage together with retrieval mechanisms for business events for analytical as well as operational purposes. The query language for retrieving near real-time events and creating conjunctions with historical events metrics and scores is SARI-SQL  which in contrast to Event  12 16 26  is a structured query approach SARI-SQL is tailored to satisfy the special requirements of analytical business users and meet the characteristics of events and their relationships The core access component of the EventBase is a query engine supporting SARI-SQL  Set on top of the EventBase data repository it exposes its services through programming interfaces and a graphical user interface SARI-SQL can be used by a wide range of frontend tools for accessing the data within the EventBase; this includes reporting, incident tracking as well as data mining tools which are used by analysts to examine the business environment based upon events processed by the event-based system. A detailed discussion of SARI-SQL is outside the scope of this paper The interested reader may refer to Rozsnyai 14   IV  E VENT S TREAM M ETA M ODEL  Before we discuss the data model and further data management issues we present the relevant meta-models In the following we outline meta-models of the event-based system SARI. We will use the meta-models as a foundation for mapping and preparing event data to a data repository We distinguish two types of meta-models 1 meta-models for deriving the data model and 2 meta-models for storing and preparing event-data in runtime en-GB A  Meta-Models for Deriving the Data Model For generating the data model from a stream of events, the following information is required en-GB  Structural information of various types of events including data types of attributes, nested event types constraints for attributes en-GB  Information on how structural information is reused inheritance, exheritance en-GB  Relationship information on how events are correlated with each other  de-AT    Figure 1   Architectural Overview  of the SARI Event Processing System   


Figure 2 shows the event and correlation meta-model 18  of the SARI system The event meta-model includes information on various event types which capture the meta-data carrying the structural information of events Please note that the event meta-model is simplified and includes only relevant information for the data mapping. For a detailed discussion of the event meta-model, please refer to Rozsnyai et al 15 An event type in SARI can have a set of attributes complying with an attribute type e.g String Integer etc SARI supports special event types such as virtual dynamic and duck types Virtual event types define views on existing event types similar as views for tables in the relational database world Dynamic event types automatically derive the structural information from a set of existing event types. Duck types can be used for temporary deactivating the typing system \(e.g. for event data staging purposes and at a later point in time the appropriate event type for the unclassified event data can be inferred For further details on the various event types, please refer to 15  On the right side of Figure 2 you can see the correlation meta-model of the SARI system SARI uses so-called correlation sets for modeling these event relationships between events In other words correlations between events are declaratively defined in a model which is used by the correlation engine during runtime Correlation sets are able to define relationships based on matching key-value pairs of attributes among event types. Correlation sets can have a set of correlation bands which define a sequence of events that use the same matching approach for the event correlation For instance if we want to correlate order events with transportation events of the same order we might have one band defining the correlation between order events e.g OrderCreated OrderShipped OrderFulfilled and a second band defining the correlation between transport events e.g TransportStart TransportEnd The correlation set is able to link multiple bands thereby correlating the events of all its bands SARI supports various types of correlation bands such as simple correlation elementary matching semantic matching correlation self-referencing correlation and bridges for correlation sets \(for chaining multiple correlation sets en-GB B  Meta-Models for Event Data Staging When storing event stream data to a database, we need an infrastructure for 1\ mapping the event data to database tables and 2\ generating downstream artifacts from event data such as consolidated event data or performance indicators. SARI solely uses the meta-models for event types for the mapping of event data to database tables and correlation Nevertheless when integrating the event data during runtime, SARI uses an event processing model and a rule model for generating downstream artifacts based on discovered event patterns Figure 3 gives an overview of the rule model of SARI Rules have one or more rule triggers which can have other rule triggers as a precondition Based on a set of preconditions event actions can be triggered. In SARI, an event action can be the generation of a new event object, a calculation of a score or metric or the data collection for an entity All these actions generate data items \(event objects, scores, metrics, entity data which are stored in the database and linked to the triggering event\(s     Figure 2   Event Meta Model and Correlation Meta Model     Figure 3 Rule Meta Model  


The rule model of Figure 3 needs to be interpreted by a rule engine SARI uses so-called Event Processing Maps EPMs for defining event processing flows Similar to a construction kit, an EPM offers various adapters and services for the event processing Dependent on the business requirements these components can be flexibly conjoined or disconnected For instance, EPMs can be used to clean or enrich event data before it is processed by a rule service. The processing of the rules is performed by a rule service, which is one of many components within an EPM Figure 4 shows the elements of the EPM which are relevant for storing event data in a database An EPM consists of customizable components which can be combined in an event processing flow. A processing flow always starts with a sense socket and ends with a response socket. The sense and respond socket correspond to adapters for receiving or sending the event data Sockets can also be used for connecting multiple EPMs or as a central distribution hub for events For the event data storage SARI uses sockets for  can define for any type of socket, whether the received events of the socket should be stored in the database In summary the EPMs form the data staging area of the CEP system Rules are used to match event patterns and subsequently, to generate downstream artifacts from the event pattern such as scores metrics and entity information  Figure 4 only shows the relevant elements of EPMs for the data staging SARI supports many other elements such as filters modifiers and synchronization blocks 19  whose discussion is out of the scope of this paper   Figure 4. SARI Event Processing Map  V  R ELATIONAL M APPING OF E VENT D ATA  In this section we describe the database schema used to map events correlations between those events as well as associated metrics scores and entities to a relational database which serves as an EDWH repository Some parts of the database schema namely the tables for typed events metrics and scores are changed dynamically based on the definitions of the underlying CEP application  Figure 5  shows an exemplary ER diagram for a simple CEP application with three di   metric a score and an entity have been defined in the CEP application  In the following, we describe the purpose of each table and discuss how they are used to fulfill the requirements of an EventBase Events which are at the core of this schema, are stored in the main Events table. It contains the generic information about all the events stored in the EventBase and is used to link events part of the same correlation to metrics scores and entities on which they had an influence Apart from the event type  serialized version of the event is stored, which can be used to reconstruct the event object For each event type, a separate table exists which splits up the event attributes of this event type into database columns as   Figure 5 These columns allow for efficient querying of event attributes by analytical applications. Each event type attribute which has a runtime type supported by the underlying database is directly mapped to a database column of a corresponding database type Event attributes having unsupported runtime types are available only after deserializing the whole event from the serialized version in the Events table    Figure 5. Entity Relation \(ER\ Model  Events of an inheriting event type are only stored to the  included in this table; see  in Figure 5. Inheritance is then implemented through the use of database views, as is shown in Figure 6. A view is created for 


each event type but, unlike the event type tables before, a view may contain events of different event types i.e event types inheriting from the same base event type. For example, Events A View contains events from the base event type A as well as from the event type inheriting from A i.e  query against the Events A View returns all the events sufficing event type A, hence also those events of the inheriting   Indexes are individually created for those event attributes which might often be queried for analysis of the CEP application A full-text search as known from Web search service providers is provided on-demand if supported by the underlying database. The serialized XML version of the event contains all the event attributes in textual format to serve as the full-text searched column For a trade-off in performance, the same feature is available to search in a set of correlated events by appending the serialized XML versions of these events in the database 16  During the lifetime of a CEP, applications may change; this includes the addition removal or change of event type definitions. While adding and removing event type definitions as a whole can be handled by simply dropping the associated tables and views, changing an event type definition with related events already stored in the EventBase may be difficult Dropping an event attribute can be treated the same way as dropping an event type, i.e., by either dropping the column or keeping the column and inserting NULL values for new events from now on. Which approach is correct solely depends on the CEP application at hand Correlations  are persisted in the Correlation table with their ids temporal information about the correlation and a foreign key to their associated correlation set Correlations have an nto m  relationship to events which is established through the Events2Correlation table Consolidated serialized XML versions of all events part of the same correlation can furthermore be included to provide the possibility for full-text searching over correlations Metrics and Scores  values calculated from various correlated events are stored in tables created specifically for each metric definition similar to events The difference between metrics and scores is that metrics are always re\calculated as a whole, whereas for scores, the current value is either increased or decreased. Nevertheless, on a database level they can be treated analogously and therefore only metrics will be described in greater detail in this section As described in previous sections metric values are updated upon the arrival or absence of events after certain preconditions have been met within a rule.  Metrics consist of an identifier, a value and a set of properties usually populated with certain attributes of an associated event and are stored in two different tables. The metric table, e.g. Metric A depicted in  Figure 5 contains the current metric value for a certain set of properties The metric history table e.g MetricHistory A contains the history of all metric values and is associated with the event from the events table which triggered the calculation of this metric, resulting in a specific metric value. Each time a metric calculation is triggered by an event the new record is added to the metric history table and an update is executed against a metric table containing the current metric values for the given metric properties The event-driven calculation of metrics is a common task executed by CEP applications to detect certain situations of interest and almost all decisions made at the rule-processing end of these applications rely on this information. Most of the time, it wi ll therefore be of great interest for ex-post analytical processing and the preservation of this information proves to be a useful add-on for an EDWH and spares us to recalculate this information again later on The archiving of this information may have an impact on performance; nevertheless an ex-post calculation may require storing many other generally irrelevant events just to come up with correct numbers afterwards Entities  As practice showed metric properties usually refer to the real-world entities like customers regions or products Entity tables are used to manage and enrich the information about an entity This information can later on be used to find related metrics via common properties, or to find related entities using social network analysis methods Furthermore, entities are linked to those events which helped to enrich the entity in any way VI  S CALABLE E VENT D ATA S TAGING  The data staging process for an EDWH includes multiple phases of preparing the event data for analytical purposes. One major challenge for continuously integrating event data is that the integration process cannot take for granted that it has free reign to drop tables re-load tables and conduct other major database operations without disturbing simultaneous end-user qu eries In order to work around this challenge data that changes throughout the day can be loaded into a parallel set of tables either through a batch process that runs every few   Figure 6 Views on Events  


minutes or through a continuous trickle feed Once the load interval \(e.g. five minutes\is up, the freshly loaded tables are simply swapped into production and the tables with the now stale data are released from production 10  This can be accomplished through the dynamic updating of views by simple renaming tables or by swapping partitions The downside of this type of n minute cycle-loading process is that the data in the EventBase is not truly real time For applications where true real-time data is required, the best approach is to continuously trickle-feed the changing data from the source system directly into the EventBase. Obviously, near real-time data integration cannot be as fast as bulk load integration Therefore it is necessary to carefully identify critical data which needs to be integrated continuously with minimized latency In the following, an approach is introduced for integrating trickle-feed event data with event trace capture and loading ETCL\. A key difference of ETCL to traditional bulk loading of data warehouse systems can be summarized as follows Trace Files instead of Flat Files  Traditional ETL tools use flat files where records of the same type are collected and periodically loaded into the database. ETCL uses event traces which collect in memory the records resulting from the processing of a single event. Thereby, various types of records e.g records for event data metrics scores etc are created Finally, the event trace is applied to the database Size of Data Chunks  The data loads for ETCL handle smaller data sets and is performed after the completing the processing of an event. The size of the data to be loaded into the database depends on the event processing results Loading Data from Memory  During the event processing the SARI system collects event trace data in memory. When loading the data into the EventBase, the event trace data is directly transferred to the database without using intermediate storage \(e.g. a flat-file Consistent Event Data A trace file keeps a record of the result from event processing tasks and thereby maintains the temporal order the created records When bulk-loading data the records are split into flat files which are individually loaded into the database During the data loading simultaneous user queries must be turned off in order to avoid locked database records and inconsistent query results ETCL solves the problem by continuously applying consistent fragments of the event data trace during the loading process thereby keeping the stored event data consistent When capturing data for the event trace the event data is process ed  in multiple stages Each stage produces its own set of records which are added to the trace On the higher level we distinguish the following types of records 1 the attribute data for events which contain the elementary information of event objects, 2\ information on relationships between events and 3\ metrics and scores which are calculated during the data staging. The records are continuously created during the event processing and finally loaded in chunks into the EventBase database. Figure 7 illustrates this process The SARI system allows the user to configure which artifacts, i.e., which parts of the event trace, shall be stored to the EventBase This configuration is called event trace outline  Note that there are several dependencies between the various trace levels For instance tracing correlations of events by activating the trace for correlation sets requires that the correlated events are also captured in the trace. Otherwise, th e event trace contains incomplete records when writing the trace to the database en-GB A  Methods for Storing Event Traces In the following we discuss multiple methods for loading the records of event traces into a database system Transactional Inserts  The simplest way of loading the data into the database is to generate and execute INSERT SQL statements By using normal INSERT SQL statements the processing is fully transactional, similar to OLTP applications Since data is only added to the EventBase, a low isolation level can be used for the transactions A major shortcoming of this approach is a high number of transactions and database round trips for storing the event trace records in the EventBase Transactional Batch Processing  The processing of the transactional INSERT statements can be optimized by processing them in a batch mode i.e by sending multiple INSERT commands to the database in a batch Another approach is to use stored procedures which can consume the event trace information as input and perform inserts on database level. Using batch processing for the data inserts can significantly improve the performance However one shortcoming of this approach is that the database system still has the overhead for processing each record with a separat e SQL statement Transactional In Memory Bulk Loading This approach combines the transactional batch processing with bulk loading capabilities. Instead of loading the data from flat files, records from the event trace are directly loaded from memory into the de-AT  Figure 7   EventBase Data Staging  


database system However many   database system do not support large in-memory data-loads directly. In order to circumvent this problem, mechanisms can be used to load data from in memory documents For instance Microsoft  SQL Server or Oracle database systems allow preparing XML documents in memory which can then be used to efficiently query data items for inserting them in one step in to  the database The data inserts for a single XML document is performed in a single transaction Using Transactional Logs for Event Traces  In this approach a database-specific event trace format is used for storing record information When applying the event trace to the database system the trace can be applied directly by the database system thereby minimizing the processing and time for the data integration. The key advantage for this approach is that data is inserted in a highly efficiently manner A major shortcoming of  this approach however is that transactional logs for database system are propri etary and vary from system to system  en-GB B  Performance Experiments with ETCL Using an event processing application from the fraud domain 23 we conducted an experiment for storing the event processing results with event tracing. We ran SARI, using the transactional batch mode  on two nodes with four-CPU Intel servers  We used a separate machine with the same technical specification as the database server   running Microsoft SQL Server 2008 The following table summarizes the perfor mance results  TABLE I  E VENT D ATA S TAGING WITH ETCL   en-GB Input  en-GB Number of source events  en-GB 16 8 232 events  en-GB  en-GB Average  number of attributes  per event  en-GB 18 event attributes  en-GB  en-GB Number of event types   en-GB 14 event types  en-GB  en-GB Event tracing mode   en-GB Transactional batch  en-GB Output  en-GB Processing t ime \(total  en-GB 4 minutes  40 seconds  en-GB  en-GB Events processed / second  en-GB 601 events   sec  en-GB  en-GB Stored events   en-GB 171.327 events  en-GB  en-GB Stored correlations   en-GB 12.301 correlations  en-GB  en-GB Metrics and score updates   en-GB 31.323 updates  en-GB  en-GB Total number of inserted or  updated database record s   en-GB 412.420 records  en-GB  en-GB Database r ecords inserted or updated / second  en-GB 1472 records   sec   We were able to store the events with the full event trace at a rate of about 600 events per second Please note that the processing of an event potentially generates new events e.g alert events\ which are also stored as part of the event trace We conducted another experiment using in memory bulk loading. By using this storage mode with SQL Server 2008, we created XML documents which containing the full event trace information and sent these XML documents to a stored procedure for inserting the data With this storage mode we were able to store 424 events per second. The lower throughput is caused by the significant amount of XML processing which increases the CPU utilization on the SARI nodes as well as on the database node Finally, we did an experiment with the transactional insert storage mode, which allowed us to store 127 events per second The significantly lower throughput was caused by the higher number of database activity due to more database roundtrips VII  E VENT D RIVEN B USINESS I NTELLIGENCE  As yet we presented the concept of EDWH and showed how events, correlations and other artifacts are persisted in the EventBase In this section we focus on the analysis part of SARI: Event-driven BI means the creation of knowledge about underlying business environments from historic event data. As such knowledge can be used to adapt and further improve realtime event-processing logic applied event-driven BI is essential for upto date and continuously refining event-based systems In the following, we demonstrate event-driven BI through a real-world use-case from the fraud detection domain Fraud detection and prevention is a major issue across technologydriven business domains relying on online payment solutions and customer interactions Suntinger et al 23  showed that fraud analysis fits particularly well with the event-driven approach Fraud analysis requires root-cause and cause-chain analysis on the level of single user-actions both in near real time for the intime prevention of ongoing fraud and a posteriori for the continuous improvement of the knowledge base\. Event-based systems use a rule-based approach to continuously evaluate the stream of customer interactions. For the expert-driven analysis of past customer data relevant events are stored in the EventBase As an exemplary analysis framework we utilize the EventAnalyzer 24 The EventAnalyzer is the first grownto maturity BI tool built upon the EventBase. It offers a range of visualization opportunities tailored to the characteristics of event data. The key visualization techniques are derived from the Event Tunnel metaphor of seeing past events flowing through a 3D cylinder and providing different viewing on it Figure 8 above shows a screenshot of the analysis framework A detailed description of the depicted panels will be given throughout the below example In the following, we assume that a well-established onlinebetting provider uses a SARI-based fraud-detection system to continuously monitor ongoing customer interactions We furthermore assume that the system automatically generates alerts for users that match known fraud patterns After receiving two temporally close fraud alerts the business analyst decides to further investigate the affected user accounts In the analysis framework the analyst formulates a query selecting the corresponding accou nt history correlationsessions from the Event Base 


In the EventAnalyzer, the definition of queries is facilitated by the query builder \(Figure 8a\: Here, analysts can select those event types that are considered relevant from the overall list of event types defined in the given event-model the same is provided for correlation sets If required for their investigations analysts can define more sophisticated clauses as, for instance, on event attribute values The event-tunnel top-view in Figure 8c shows the results of the above query: Single events are plotted as glyphs, with userdefined mappings Figure 8b from event attributes to color shape and size Correlations between events are plotted as colored bands, connecting the correlated events by their times of occurrence As depicting similar-looking glyphs at similar points in time, the plot in Figure 8c unveils that the suspicious   sers  officials by the system From visualizations of the EventBase business analysts gain deep insight into the various business processes persisted therein Query-driven and tailored for a quick and step-wise navigation through the EventBase, analytical applications such as the Event Analyzer push it one step further: From an initial clue such as the above fraud alarms they allow the analyst detecting previously unconsidered facts and relationships Consider the business analyst formulating a new query on the event-data warehouse now selecting all events from the BetPlaced-event table that are a   occurred recently. The corresponding event-tunnel side-view in Fig ure 8d scatter of occurrence \(x axis\. It is easy to see that soon after the detected fraud attempts \(mapped to blue based on their account IDs an outlier showing a significantly higher bet amount occurs mapped to red as exceeding a threshold  For an experienced analyst this data point represents a valuable link to related and possibly conspicuous data. One possible interpretation could be that the outlier was placed by a so-called putteron  a fraudster that places bets for people who are prohibited to bet. The suspicion  available in the EventBase and depicted over time in Figure 8e The step chart shows that  inactive with sequences of cash-ins placing a bet winning a bet and cash-outs occurring straightly every few days The example shows that with tailored analysis tools such as the EventAnalyzer the EventBase serves as a valuable and easy to access source for knowledge discovery Thus far the visual analysis of event data was proven useful adoptions of well-known data-mining techniques such as similarity searching are however in current development For more information on the EventAnalyzer the Event Tunnel and its application in fraud detection, readers may refer to Suntinger et al 24  VIII  C ONCLUSION AND O UTLOOK  In this paper we have addressed several open issues regarding event persistence and analysis of existing CEP solutions In particular we have presented a solution for efficient event data repository management as an extension for CEP solutions in order to enable post analysis of events and break up current event processing limitations The presented solution is based on a formal and efficient data schema maintained by a common RDBMS It solves common problems to efficiently persist events their relationships the preservation of calculated event metrics and the representation of non-native database types including de-AT  de-AT   Figure 8  Event Driven Business Intelligence with the EventAnalyzer    


advanced concepts such as event inheritance The implementation and the evaluation have been conducted upon the existing event processing solution SARI The work presented in this paper is part of a larger, longterm research effort aiming at developing a comprehensive set of technologies and tools for event analysis Furthermore we plan to evaluate the integration of the Event Data Warehouse EDWH respectively the linking of EventBase entities to external DWH dimensions to bundle dynamic real-time event information with large historical information repositories for better situation detection and verification of  decision making processes A CKNOWLEDGEMENT  We want to thank the Senactive development team for their valuable discussions and for implementing this research work R EFERENCES  1  Aalst W Weijters A J M M and Maruster L 2004  mining: Discovering process models from event logs. IEEE Transactions on Knowledge and Data Engineering, 16, 9, 1128 1142  2  Abadi, D.J., Ahmad, Y., Balazinska, M., \307etintemel, U., Cherniack, M Hwang, J.H., Lindner, W., Maskey, A.S., Rasin, A., Ryvkina, E., Tatbul N Xing Y and Zdonik S 2005 The Design of the Borealis Stream Processing Engine In Proc of the Conf on Innovative Data Systems Research, Asilomar, CA, USA, 277 289 3  Abadi D J Carney D Cetintemel U Cherniack M Convey C Lee, S., Stonebraker, M., Tatbul, N. and Zdonik, S. 2003. Aurora: A new model and architecture for data stream management. VLDB Journal, 12 120 139 4  Adi A and Etzion O 2004  AMIT  the situation manager The VLDB Journal, 13, 2, 177-203 5  Brobst S and Ballinger C 2000 Active Data Warehousing Whitepaper EB 1327, NCR Corporation 6  Chen, S.-K., Jeng, J.-J. and Chang, H. 2006. Complex Event Processing using Simple Rule-based Event Correlation Engines for Business Performance Management. CEC/EEE 7  Esper, http://esper.codehaus.org/, 2009-0201  8  Hoppe A. and Gryz J. 2007. Stream Processing in a Relational Database a Case Study Database Engineering and Applications Symposium 2007. IDEAS 2007. 11th International Volume, 216 224 9  Inmon B., Imhoff C. and Sousa R. 2001. Corporate Information Factory Wiley, New York 10  Kimball R 1996 The Data Warehouse Toolkit Practical Techniques for Building Dimensional Data Warehouses. John Willey, 1996 11  Luckham, D. 2005. The Power Of Events. Addison Wesley 12  Mannila H and Moen P 1999 Similarity between event types in sequences, Proc. First Intl. Conf. on Data Warehousing and Knowledge Discovery  271 28 13  Moen P 2000 Attribute Event Sequence and Event Type Similarity Notions for Data Mining Ph.D thesis Department of Computer Science, University of Helsinki, Finland 14  Rozsnyai S 2006 Efficient indexing and searching in correlated business events. PhD thesis, Vienna University of Technology 15  Rozsn yai S., Schiefer J and Sch atten A  2007 Concepts and Models for Typing Events for Event Based Systems  International Conference on Distributed Event Based Systems   Toronto  Canada  DEBS  0 7   16  Rozsnyai, S., Vecera, R., Schiefer, J and Schatten, A. 2007 Event cloud  searching fo r correlated business events. In CEC/EEE, IEEE Computer Society  409 420  17  Schiefer, J. and Seufert, A. 2005. Management and controlling of timesensitive business processes with sense & respond. In CIMCA/IAWTIC IEEE Computer Society, 77 82  18  Schiefer J Rozsnyai S Saurer G and Rauscher C 2007 Event Driven Rules for Sensing and Responding to Business Situations International Conference on Distribut ed Event Based Systems, Toronto   19  Schiefer, J. and Seufert, A. 2005. Management and Controlling of TimeSensitive Business Processes with Sense  Respond International Conference on Computational Intelligence for Modelling Control and Automation \(CIMCA\, Vienna 20  Schrefl M and Thalhammer T 2000 On Making Data Warehouses Active In Proc of the 2nd Intl Conf on Data Warehousing and Knowledge Discovery DaWaK Springer LNCS 1874 London UK 34 46 21  Seirio M. and Berndtsson M. 2005 Design and Implementation of an ECA Rule Markup Language. RuleML, Springer Verlag, 98 112 22  Stonebraker M  and 307etintemel U 2005  One Size Fits All An Idea Whose Time Has Come and Gone, ICDE 2005, 2-11 23  Suntinger M Schiefer J Roth H and Obweger H 2008 Data Warehousing versus Event-Driven BI Data Management and Knowledge Discovery in Fraud Analysis  International Conference on Software Knowledge Information Management and Applications  Kathmandu, Nepal  08  24  Suntinger, M., Obweger, H., Schiefer, J and Groeller M. E  2007 The E vent T unnel  Interactive visualization of complex event streams for busin ess process pattern analysis Technical report Institute of Computer Graphics and Algorithms  Vienna University of Technology  25  Vecera R 2007 Efficient indexing Searching and Analysis of Event Streams. PhD thesis, Vienna University of Technology 26  Vecera R Rozsnyai S and Roth H 2007 Indexing and search of correlated business events The Second International Conference on Availability, Reliability and Security, Ares 2007, 1124 1134 27  Widom J  Ceri S and Dayal U 1994 Active Database Systems Triggers and Rules for Advanced Database Processing Morgan Kaufmann Publishers Inc., San Francisco, CA 28  Wu P Bhatnagar R Epshtein L Bhandaru M and Shi Z 1998 Alarm correlation engine \(ACE\, In Proceedings of the IEEE/IFIP 1998 Network Opera tions and Management Symposium NOMS New Orleans  29  Zdonik S Stonebraker M., Cherniack M. Cetintemel U Balazinska M. and Balakrishnan, H. 2003. The Aurora and Medusa Projects. IEEE Data Engineering Bulletin, 26  1  


4 Heart 270 13 2 5 Diabetes 768 8 2 6 Pima 768 8 2  For a classifier, classification accuracy is a basic performance measurement, which is the ratio of the number of cases truly predicted by the classifier over the total number of cases in the whole test dataset, e.g Number of cases truly predictedClassification accuracy= 100 Total number of cases  2 We compared BitTableAC with some associative classification algorithms on accuracy, such as CBA and CMAR. We also include the C4.5 and LIBSVM results on the same datasets as a reference. C4.5 is a well-known traditional classifier based on decision tree induction technique, and LIBSVM is an accurate support vector machine classifier For the fair of the comparison, we do not implement these algorithms, but their classification accuracies are obtained from their research literatures. For BitTableAC, the MinSup MinConf and NFP \(num of fuzzy partitions and 3, respectively The comparison results are shown in Table 4. As shown in this table, BitTableAC has the satisfactory classification accuracy. It achieves the highest accuracy in four of six datasets used in experiments and also outperforms other algorithms on average Table 4 Experiment Results Dataset C4.5 LIBSVM CBA CMAR BitTableAC 1 Glass 68.70 77.57 72.60 70.10 75.23 2 Iris 93.60 94.00 92.90 94.00 96.25 3 Breast 95.70 96.14 95.80 96.40 98.53 4 Heart 82.50 88.45 81.50 82.20 86.67 5 Diabetes 72.10 73.83 75.30 75.80 80.00 6 Pima 75.50 79.69 73.10 75.10 81.82 Average 81.35 84.95 81.87 82.27 86.42  IV. CONCLUSION In this paper, an accurate associative classifier BitTableAC is proposed. It employs BitTable to mine association rules 


efficiently, and fuzzy c-means \(FCM attributes. To evaluate the performance of the proposed algorithm, we compare BitTableAC with other well-known classifiers on accuracy including previous associative classifiers, C4.5 and LIBSVM on 6 test datasets from UCI Machine Learning Repository. The results show that, in terms of accuracy, BitTableAC outperforms others REFERENCES 1] G. Goulbourne, F. Coenen, and P. Leng, "Algorithms for computing association rules using a partial-support tree," Knowledge-Based Systems vol. 13, pp. 141-149, Apr 2000 2] M. J. Zaki, "Scalable algorithms for association mining," Ieee Transactions on Knowledge and Data Engineering, vol. 12, pp. 372-390 May-Jun 2000 3] F. Bonchi, F. Giannotti, A. Mazzanti, and D. Pedreschi, "Efficient breadth-first mining of frequent pattern with monotone constraints Knowledge and Information Systems, vol. 8, pp. 131-153, Aug 2005 4] G. Grahne and J. F. Zhu, "Fast algorithms for frequent itemset mining using FP-trees," Ieee Transactions on Knowledge and Data Engineering, vol 17, pp. 1347-1362, Oct 2005 5] I. I. Artamonova, G. Frishman, and D. Frishman, "Applying negative rule mining to improve genome annotation," Bmc Bioinformatics, vol. 8, pp. -, Jul 21 2007 6] J. W. Han, J. Pei, Y. W. Yin, and R. Y. Mao, "Mining frequent patterns without candidate generation: A frequent-pattern tree approach," Data Mining and Knowledge Discovery, vol. 8, pp. 53-87, Jan 2004 7] Y. C. Hu and G. H. Tzeng, "Elicitation of classification rules by fuzzy data mining," Engineering Applications of Artificial Intelligence, vol. 16, pp 709-716, Oct-Dec 2003 8] J. D. Holt and S. M. Chung, "Mining of association rules in text databases using Inverted Hashing and Pruning," Data Warehousing and Knowledge Discovery, Proceedings, vol. 1874, pp. 290-300, 2000 9] Y. J. Li, P. Ning, X. S. Wang, and S. Jajodia, "Discovering calendar-based temporal association rules," Data & Knowledge Engineering, vol. 44, pp 193-218, Feb 2003 10] Y. J. Tsay and J. Y. Chiang, "CBAR: an efficient method for mining association rules," Knowledge-Based Systems, vol. 18, pp. 99-105, Apr 2005 11] B. Liu, W. Hsu, and Y. Ma, "Integrating Classification and Association Rule Mining," in Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining, KDD'98, AAAI, New York, 1998, pp. 80-86 12] W. Li, J. Han, and J. Pei, "CMAR: accurate and efficient classification 


based on multiple class association rule," in Proceedings of the 2001 IEEE International Conference on Data Mining, ICDM'01, San Jose, CA, 2001 pp. 369-376 13] D. Janssens, G. Wets, T. Brijs, and K. Vanhoof, "Adapting the CBA algorithm by means of intensity of implication," Information Sciences, vol 173, pp. 305-318, Jun 23 2005 14] W. Song, B. R. Yang, and Z. Y. Xu, "Index-BitTableFI: An improved algorithm for mining frequent itemsets," Knowledge-Based Systems, vol. 21 pp. 507-513, Aug 2008 15] J. Dong and M. Han, "BitTableFI: An efficient mining frequent itemsets algorithm," Knowledge-Based Systems, vol. 20, pp. 329-335, May 2007  532 


 Table I.  Number of intervals Stage Interval No. Stage Interval No EP 7 ES 8 ED 10 EB 9 ET 8 EI 11  Table II.  Results using the proposed approach Stage Bias MMRE MdMRE ES -8.5% 27.0% 17.0 ED -33.1% 40.5% 13.7 EB -2.8% 9.3% 7.5 ET -11.6% 16.7% 7.23 EI -20% 91.0% 30.2  Table III.  Results using exponential regression Stage Bias MMRE MdMRE ES -24.3% 81.3% 49.7 ED -72.3% 120.4% 54.224 EB 0.7% 44.35% 37.6 ET -45.4% 81.1% 39.0 EI -179% 184% 104.0  Results shown in Table III revealed that most of predictions are under estimation which supports our approach findings. The best estimation accuracy was obtained in building stage, which also corroborates our findings that best estimation accuracy was in building stage. The negative values in Bias criterion show underestimation. It is acknowledged that MMRE is unbalanced in many validation circumstances and leads to overestimation more than underestimation. In our case, we found that MMRE leads to underestimation in most stages. This is may be related to the absence of systematic scheme between all prior effort records   253   Figure 1. Effort distribution of Planning stage 


Figure 2. Effort distribution of Specification stage Figure 3. Effort distribution of Design effort stage  Figure 4. Effort distribution of Building stage Figure 5. Effort distribution of Testing stage Figure 6. Effort distribution of Imp stage   Table IV. Statistical significance Stage sum rank Z-value p-Value ES 769 -4.31 <0.01 ED 713 -5.03 <0.01 EB 685 -5.4 <0.01 ET 595 -6.54 <0.01 EI 799 -3.93 <0.01  The comparison between our approach and exponential regression technique showed that there are considerable improvements in estimation accuracy on all phases of software development lifecycle. MMREs of our approach have been reduced by at least 35.05 and at most 93%. Biases have been reduced by at least 3.5% and at most 159%.We have to bear in mind that the length of interval plays important role in estimation accuracy, thus, when the universe of discourse is partitioned into several equal intervals, the distribution of data should be taken into account. Moreover, we should remove the extreme values because they affect interval partitioning, thus, estimation accuracy Figures 7 to 11 show comparison between proposed approach and exponential regression in each stage by using Boxplot. The Boxplot [17] offers a way to compare between estimation models based on their absolute residuals. The Boxplot is non-parametric statistics used to show the median as central tendency of distribution, interquartile range and the outliers of individual models [17]. The length of Boxplot from 


lower tail to upper tail shows the spread of the distribution. The length of box represents the interquartile range that contains 50% of observations The position of median inside the box and length of Boxplot indicate the skewness of distribution. A Boxplot with a small box and long tails represents a very peaked distribution while a Boxplot with long box represents a flatter distribution. The prominent and common characteristic among these figures is the spread of absolute residuals for our approach is less than spread of exponential regression which presents more accurate results. The larger interquartile of exponential regression indicates a high dispersion of the absolute residuals. The Boxplot revealed that the box length for our models is smaller than exponential regression which also indicates reduced variability of absolute residuals. The median of our model is smaller than median of exponential regression which revealed that at least half of the predictions of our model are more accurate than exponential regression 254  Figure 7. Boxplot of absolute residuals for the specification stage  Figure 8. Boxplot of absolute residuals for the design stage   Figure 9. Boxplot of absolute residuals for the building stage Figure 10. Boxplot of absolute residuals for the testing stage   The lower tails of our model is much smaller than upper tail which means the absolute residuals are skewed towards the smaller value Figure 11 illustrates the reason of why prediction of implementation stage in our approach produced the worst accuracy. The reason related to the existing of outlier. Although one project is considered as an outlier the MMRE is easily influenced with that project Based on the obtained results, we can observe that 


exponential regression gave bad accuracy. The reason may relate to the structure complexity of prior effort records. There is no correlation between all prior stages and target stage To ensure that the results obtained are not by chance we investigated the statistical significance of the proposed approach using Wilcoxon sum rank test for absolute residuals as shown in Table IV. In this test if the resulting p-value is small \(p<0.05 statistically significant difference can be accepted between the two samples median. The residuals obtained using the proposed approach were significantly different from those obtained by exponential regression Suggesting that, there is difference if the predications generated using the proposed approach or exponential regression and based on the accuracy comparison in Tables II and III we can safely conclude that our proposed method outperformed exponential regression for stage effort estimation Figure 11. Boxplot of absolute residuals for the implementation stage  VIII. CONCLUSIONS Some of software projects are failed due to the absence of re-estimation during software development which results in huge gap between initial plan and final outcome. Even with good estimate at first stage the project manager must keep update with project progress and should be able to re-estimate the project at any particular point of project in order to re-allocate the proper number of resources. The objective of this paper was to check whether the prior effort records can 255 be used to predict stage effort with reasonable accuracy or not. The obtained results revealed that using association rule and Fuzzy set theory lead to significant improvement in stage-effort estimation and give project manager an evolving picture about project progress. Comparing our approach with exponential regression showed that there is a considerable potential in estimation accuracy. As part of future plan, we 


intend to expand this work to involve some interesting features in each stage prediction and evaluate it on many datasets   REFERENCES  1] F. Ricardo, N. Ana, M. Paula, B. Gleidson, R. Fabiano ODE: Ontology-based software Development Environment, Proceedings of the IX Argentine Congress on Computer Science, pp. 1124-1135, 2003 2] E. Mendes, B. A. Kitchenham. Further comparison of cross-company and within-company effort estimation models for Web applications. In: Proc. 10th IEEE International Software Metrics Symposium, Chicago USA, pp.348-357, 2004 3] B. Boehm, R. Valerdi. Achievements and Challenges in Software Resource Estimation, Proceedings of ICSE 06 Shanghai, China, pp. 74-83,  2006 4] K. Molokken, M. Jorgensen. A review of software surveys on software effort estimation, Proceedings of International Symposium on Empirical Software Engineering \(ISESE 2003 5] M. Jorgensen, K. Molokken-Ostvold. How large are software cost overruns? A review of the 1994 CHAOS report, Information and Software Technology, Vol. 48 issue 4. PP. 297-301, 2006 6] X. Huanga, D. Hob, J. Rena, L. F. Capretz. Improving the COCOMO model using a neuro-Fuzzy approach Applied Soft Computing, Vol.7, issue 1, pp. 29-40, 2007 7] L. Briand, T. Langley, I. Wieczorek. A replicated assessment and comparison of common software cost modeling techniques, Proceedings of the 22nd international conference on Software Engineering, 2000 8] S.-J Huang, N. H. Chiu. Optimization of analogy weights by genetic algorithm for software effort estimation Information and Software Technology, Vol. 48, issue 11 pp. 1034-1045, 2006 9] Z. Xu, T. M. Khoshgoftaar. Identification of Fuzzy models of software cost estimation, Fuzzy Sets and Systems, Vol. 145, issue 1, pp. 141-163, 2004 10] R. Pressman. Software Engineering: practitioner 


approaches, McGraw Hill, London, 2004 11] M. Boraso, C. Montangero, H. Sedhi. Software cost estimation: an experimental study of model performance Universita di Pisa, Italy, 1996 12] Y. Wang, Q. Song, J. Shen., 2007. Grey Learning Based Software Stage-Effort Estimation. International Conference on Machine Learning and Cybernetics, pp 1470-1475, 2007 13] S. G. MacDonell, M. J. Shepperd. Using prior-phase effort records for re-estimation during software projects Ninth International, Software Metrics Symposium, pp 73- 86, 2003 14] M .C Ohlsson, C. Wohlin. An Empirical Study of Effort Estimation during Project Execution, Sixth International Software Metrics Symposium \(METRICS'99 1999 15] N. H. Chiu,,S. J. Huang.  The adjusted analogy-based software effort estimation based on similarity distances Journal of Systems and Software, Vol. 80, issue 4, pp 628-640, 2007 16] P. Sentas, L. Angelis, I. Stamelos, G.  Bleris. Software productivity and effort prediction with ordinal regression Information and Software Technology, Vol. 47, issue 1 pp. 17-29, 2005 17] E. Mendes, N. Mosley. Comparing effort prediction models for Web design and authoring using boxplots Australian Computer Science Communications,  Vol. 23 Issue 1, pp. 125-133, 2001 18] E. Mendes, N. Mosley, I. Watson. A comparison of casebased reasoning approaches, Proceedings of the 11th international conference on World Wide Web, pp. 272280, 2002 19] Q. Zhao, S. S. Bhowmick. Association Rule Mining: A Survey  http://citeseer.ist.psu.edu/734613.html, 2003 20] S. Morisak, A. Monden, H. Tamada. An Extension of association rule mining for software engineering data repositories, Information Science Technical Report NAIST, 2006 21] Q. Song, M. Shepperd.  M. Cartwright, C. Mair. Software defect association mining and defect correction effort prediction, IEEE transaction on software engineering Vol. 32, No.2, pp. 69-82, 2006 


22] R. Agrawal, T. Amielinski, A. Swami. Mining association rule between sets of items in large databases Proceedings of the ACM SIGMOD International Conference on Management of Data, pp. 207-216, 1993 23] M-J. Huang, Y-L. Tsou, S-C. Lee.  Integrating Fuzzy data mining and Fuzzy artificial neural networks for discovering implicit knowledge, J. Knowledge-Based Systems, Vol.19 \(6 24] ISBSG International Software Benchmarking standards Group, Data repository release 10, Site http://www.isbsg.org, 2007 25] L. Zadeh. Toward a theory of Fuzzy information granulation and its centrality in human reasoning and Fuzzy logic. J. Fuzzy sets and Systems 90, pp. 111-127 1997 26] I. H. Witten, E. Frank. Data Mining: Practical machine learning tools and techniques, 2nd Edition, Morgan Kaufmann, San Francisco, 2005   256 


encountering a related term, i.e. IC\(c e intuition behind the use of the negative likelihood is that the more probable a term to appear, the less information it conveys. All these features show that Jiangs measure tends to be more general and more appropriate for evaluating nontaxonomically related terms. Indeed, a high score of the relatedness measures suggests a strong relationship between terms Nevertheless, all relatedness measures have limitations because they assume that all the semantic content of a particular term is modeled by semantic links in WordNet Consequently, in many situations, truly related terms obtain a low scores even though their belongings to a certain category of tags, e.g., jargon tags Additionally, when measuring the quality of an automatically knowledge acquisition results, the typical measures used in Information Retrieval are Recall, Precision and F-Measure. However, computing Recall and F-Measure requires the availability of a Gold Standard. Hence, we will only compute the Precision which speci?es to which extent the non-taxonomic relationships is extracted correctly. In this case, the ratio between the correctly extracted relations i.e., their relatedness measures is greater than or equal to a minimum threshold, and the whole number of extracted ones is computed. Thus, we have Precision Total correctly selected entities Total selected entities 12 http://search.cpan.org/dist/WordNet-Similarity 13 A term refers to a tag subject or a tag object C. Evaluation of non-taxonomic relationships Only a percentage of the full set of non-taxonomic relationships \(89 is caused by the presence of non standard terms which are not contained in WordNet and, in consequence, cannot be evaluated using WordNet-based relatedness measures. Fig. 5 depicts the evaluation results of the extracted non-taxonomic relationships against their relatedness measures High relatedness score \(88 17% of the extracted relationships, as most of terms are strongly related with respect WordNet Null Scores were obtained for 5% of the extracted 


relationships. Analyzing this case in more detail, we have observed that the poor score is caused in many situations by the way in which Jiangs distance metric works. This latter completely depends on the distance between two terms based on the number of edges found on the path between them in WordNet. In consequence this measure returns a value that does not fully represent reality. For example, on the one hand, Jiangs distance metric returns a null value for the relationship between insurance and car, even though the ?rst is a commonly related to the second, i.e., car involved insurance Finally with a minimal Jiangs distance metric threshold, set to 46%, the computed precision of correctly extracted relationships candidates is equal to 68.8 An example of extracted non-taxonomic relationships is depicted in Table V where each relation describes the subject tag, e.g., tool, the predicate, e.g is being developed within, and the object tag, e.g mesh. Fig. 4 represent a fragment output of the extracted ontological structure where each concept de?nes a set of similar and synonym tags and labels, i.e., mentions has been, revealed, caused and is created with describe the predicates of the non-taxonomic relationships between terms Due to the limitations observed by the automatic evaluation procedure and the lack of gold standards containing non-taxonomic relationships, we have examined the extracted non-taxonomic relationships from a linguistic point 377 Top space      distance     quad great     groovy nifty caused address      addresses extension      quotation   reference  references extensions        referenz     source      refrence sources    rfrences    quotations research    search     searching searchs open-source     open_source 


opensource linux aim     design     designer      designers patern    project     patterns     projekte projects web+design    web_design webdesign internet       internetbs net          web network      networking networks      web discussion     news       password word      words community      communities is_created_with mentions revealed has_been Figure 4. A fragment output of the extracted ontological structure of view. This qualitative evaluation can bring some interesting insights about the kind of results one can expect Invalid relations are extracted: Even though a relation such as music cities skill is considered as correct one since tag subject, tag object and predicate are correctly extracted. From a semantic point of view, this relation has no meaning. Hence, a higher precision is expected Figure 5. Summary of non-taxonomic evaluation measure Table V EXAMPLES OF EXTRACTED NON TAXONOMIC RELATIONSHIPS Subject Predicate Object search has been reference reference mentions search tool is being developed within mesh security added encoding search revealed reference java provides library by performing the sense analysis on complete relations An ambiguity in the extracted predicates between terms is observed: Hence, same relations are redundant since they use a synonym predicates between terms, e.g java provides library and java yields library. Thus we expect that the redundancy removal within extracted relations will be of bene?t for the improvement of the 


obtained results VI. CONCLUSION AND FUTURE WORK The extraction of non-taxonomic relationships from folksonomies is to the best of our knowledge is the least tackled task within ontology building from folksonomy. This is why there is a need of novel and general purpose approaches covering the full process of learning relationships. In this paper, we introduced a new approach called NONTAXFOLKS that starts by pre-processing tags aiming at getting a set of frequent tagsets corresponding to an agreed representation Then, they are used to retrieve related tags using external resources such as WordNet. Thanks to the particular structure of triadic concepts, it allows grouping semantically related tags by considering the semantic relatedness embodied in the different frequencies of co-occurences among users, resources and tags in the folksonomy. Thereafter we introduced an algorithm called NTREXTRACTION for extracting non-taxonomic relationships between pair of tags picked from the triadic concepts. In summary, our approach uses several well known techniques \(such as formal concept analysis or association rule discovering the social bookmaring environnement in order to propose a new way of extracting labeled non-taxonomic relationships between tags. Currently, we are investigating the following topic concerning the discovered predicates between two terms. Indeed, in order to avoid relationships redundancy and thus a redundancy in the builded ontology. One can try to classify them into prede?ned semantic classes, detect synonyms, inverses, etc. A standard classi?cation of verbs could be used for this purpose, adding additional information about the semantic content, e.g., senses, verb types, thematic roles, etc., of predicates relationships 378 REFERENCES 1] J. Pan, S. Taylor, and E. Thomas, Reducing ambiguity in tagging systems with folksonomy search expansion, in Proceedings of the 6th Annual European Semantic Web Conference \(ESWC2009 2] V. S. M. Kavalec, A. Maedche, Discovery of lexical entries for non-taxonomic relations in ontology learning, in Proceedings of the SOFSEM 2004, LNCS, vol. 2932, 2004, pp 249256 


3] L. Specia and E. Motta, Integrating folksonomies with the semantic web, in Proceedings of the 4th European Semantic Web Conference \(ESWC 2007 Innsbruck, Austria, vol. 4519, June 2007, pp. 624639 4] P. Mika, Ontologies are us: A uni?ed model of social networks and semantics, in Proceedings of the 4th International Semantic Web Conference \(ISWC2005 3729, Galway, Ireland, June 2005, pp. 522536 5] P. Schmitz, Inducing ontology from ?ickr tags, in Proceedings of the Workshop on Collaborative Tagging \(WWW 2006 Edinburgh, Scotland, May 2006 6] M. Zhou, S. Bao, X. Wu, and Y. Yu, An unsupervised model for exploring hierarchical semantics from social annotations, in Proceedings of the 6th International Semantic Web Conference and 2nd Asian Semantic Web Conference ISWC/ASWC2007 Korea, vol. 4825, November 2006, pp. 673686 7] C. Schmitz, A. Hotho, R. Jaschke, and G. Stumme, Mining association rules in folksonomies, in Proceedings of the 10th IFCS Conference \(IFCS 2006 2006, pp. 261270 8] A. Hotho, A. Maedche, S. Staab, and V. Zacharias, On knowledgeable unsupervised text mining, in Proceedings of Text Mining Workshop, Physica-Verlag, 2003, pp. 131152 9] A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme, Information retrieval in folksonomies: Search and ranking, in The Semantic Web: Research and Applications, vol. 4011 Springer, 2006, pp. 411426 10] F. Lehmann and R. Wille, A triadic approach to formal concept analysis, in Proceedings of the 3rd International Conference on Conceptual Structures: Applications, Implementation and Theory. Springer-Verlag, 1995, pp. 3243 11] R. Jaschke, A. Hotho, C. Schmitz, B. Ganter, and G.Stumme Discovering shared conceptualizations in folksonomies Web Semantics: Science, Services and Agents on the World Wide Web, vol. 6, pp. 3853, 2008 12] A. Mathes, Folksonomies - cooperative classi?cation and communication through shared metadata, Graduate School of Library and Information Science, University of Illinois Urbana-Champaign, Tech. Rep. LIS590CMC, December 2004 13] H. Lin, J. Davis, and Y. Zhou, An integrated approach 


to extracting ontological structures from folksonomies, in Proceedings of the 6th European Semantic Web Conference ESWC 2009 vol. 5554, 2009, pp. 654668 14] M. Szomszor, H. Alani, K. OHara, and N. Shadbolt, Semantic modelling of user interests based on cross-folksonomy, in Proceedings of the 7th International Semantic Web Conference \(ISWC 2008 15] G.Begelman, P. Keller, and F.Smadja, Automated tag clustering: Improving search and exploration in the tag space, in Proceedings of the the Collaborative Web Tagging Workshop WWW 2006 16] R. Jaschke, A. Hotho, C. Schmitz, B. Ganter, and G. Stumme TRIAS - an algorithm for mining iceberg tri-lattices, in Procedings of the 6th IEEE International Conference on Data Mining, \(ICDM 2006 2006, pp. 907911 17] C. Borgelt, Ef?cient implementation of APRIORI and ECLAT, in FIMI, COEUR Workshop Proceedings, COEURWS.org, vol. 126, 2003 18] J. Tang, H. Leung, Q. Luo, D. Chen, and J. Gong, Towards ontology learning from folksonomies, in Proceedings of the 21st international jont conference on Arti?cal intelligence IJCAI 2009 20892094 19] L. Ding, T. Finin, A. Joshi, R. Pan, R. Cost, Y. Peng P. Reddivari, V. Doshi, and J. Sachs, Swoogle: A search and metadata engine for the semantic web, in Proceedings of the 13th ACM Conference on Information and Knowledge Management, ACM Press, 2004, pp. 652659 20] A. Hliaoutakis, G. Varelas, E. Voutsakis, E. Petrakis, and E. E Milios, Information retrieval by semantic similarity, International Journal on Semantic Web and Information Systems IJSWIS 21] G. Pirro, M. Ruffolo, and D. Talia, Secco: On building semantic links in peer to peer networks, Journal on Data Semantics XII, LNCS 5480, pp. 136, 2009 22] C. Meilicke, H. Stuckenschmidt, and A. Tamilin, Repairing ontology mappings, in Proceedings of the International Conference AAAI 2007, Vancouver, British Columbia, Canada 2007, pp. 14081413 23] S. Ravi and M. Rada, Unsupervised graph-based word sense 


disambiguation using measures of word semantic similarity in Proceedings of the International Conference ICSC 2007 Irvine, California, USA, 2007 24] H. G. A. Budanitsky, Semantic distance in wordnet: an experimental application oriented evaluation of ?ve measures in Proceedings of the International Conference NACCL 2001 Pittsburgh, Pennsylvania, USA, 2007, pp. 2934 25] J. Jiang and D. Conrath, Semantic similarity based on corpus statistics and lexical taxonomy, in Proceedings of the International Conference ROCLING X, 1997 379 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


