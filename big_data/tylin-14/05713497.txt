 Abstract  is an implication of the form described by equation 1 and Only one of these classes is of the concern which in turn again classified into broad categories of Simon Fong Department of Computer and Info. Sci Faculty of Science and Technology University of Macau Macau SAR   1.b 2.a 2.c were related to hiding large itemsets. Metrics used in all of these five algorithms were I Exact Approaches 1.b 2.c AR AR where  020=\n\002\b\002\b  naeem@email.com 007 204 Privacy Preserving in Data Mining \(PPDM\ is a process by which certain sensitive information is hidden during data mining without precise access to original dataset Majority of the techniques proposed in the literature for hiding sensitive information are based on using Support and Confidence measures in the association rules, which suffer from limitations. In this paper we propose a novel architecture which acquired other standard statistical measures instead of conventional framework of Support and Confidence to generate association rules. Specifically a weighing mechanism based on central tendency is introduced. The proposed architecture is tested with UCI datasets to hide the sensitive association rules as experimental evaluation. A performance comparison is made between the new technique and the existing one. The new architecture generates no ghost rules with complete avoidance of failure in hiding sensitive association rules. We demonstrate that Support and Confidence are not the only measures in hiding sensitive association rules. This research is aimed to contribute to data mining areas where privacy preservation is a concern Border-Based Approaches  I NTRODUCTION  Association rules is one of the most popular data mining techniques [1 in us e, b u t it is  of ten f o llow e d w i th a pri v ac y  preservation concern. Personal information could be technically inferred in the process of data mining, though data mining intends to generalize across populations. The problem of privacy-preserving in data mining has many applications in domains like homeland security, customer transaction analysis, and medical database mining Therefore, an effective way to release the database with sensitive rules hidden is important. This leads to the research topic of sensitive rule hiding. Numerous techniques have been studied in the literature to cope with the privacy preservation problems [2, 3, 4, 5, 6   A c o m m o n o b j e c tiv e  of all these techniques is to hide restricted association rules However, hiding these restricted association rules usually suffers from side effects. These side effects include generation of unwanted association rules, failure in hiding all of the association rules and hiding of non-sensitive genuine rules. A broader solution to these problems is to achieve the prime objective of hiding restricted association rules with ideally no side effects In this paper, we proposed an architecture which hides the restricted association rules with complete removal of the known side effects which are generation of unwanted, non genuine association rules while yielding no hiding failure Although the architecture proposed in this research is generalized to be applicable on any domain, we validated the new architecture over medical datasets which are usually characterized to be complex and have high dimensions The structure of the remaining paper is as follow: Section 2 presents a literature review of the existing techniques Section 3 describes a problem statement. Section 4 presents our proposed architecture. Section 5 elaborates results followed by a discussion. The last sections consist of comparison, conclusion and future work II data modification data restriction    Solutions for the problem of privacy preservation have been classified into three broad classes Hiding Sensitive Association Rules Using Central Tendency and and     Keywords- Data mining, Sensitive Association Rules, Privacy preservation, Central Tendency 1 data partition CAICIACA AR side effects 1.a sohail.asghar@Jinnah.edu.pk ccfong@umac.mo 2.b 1.a 2.a 2.b None of these five 478 . The other two classes namely \215BorderBased Approaches\216 and \215Exact Approaches\216 are out of the problem domain Clifton [8 p r opos ed th e i d e a  of li mit in g a c c e s s t o th e  database, fuzz the data, eliminate unnecessary grouping augmenting data and audit. The authors in [9  pr e s ente d f i ve  algorithms namely denotes set of Itemset in dataset were aimed towards hiding association rules. Algorithms is Rule Consequent or Head Antecedent is always placed on the Left Hand Side \(L.H.S of AR while Consequent is always preceded by efficiency and written on the Right Hand Side \(R.H.S\ of All of these algorithms fall in the category of distortion based technique Algorithms denotes Rule Antecedent or Tail L ITERATURE R EVIEW  Hiding sensitive association rules plays a pivoted role in PPDM. Prime objective of the PPDM is to discover the nonobvious information. To achieve this objective, we reviewed literature on PPDM in order to identify the strengths and limitations of the existing techniques. Association Rule  I D A C A and Dr. Sohail Asghar Center of Research in Data Engineering \(CORDE Mohammad Ali Jinnah University Islamabad, Pakistan Heuristic Approaches  Muhammad Naeem Center of Research in Data Engineering \(CORDE Mohammad Ali Jinnah University Islamabad, Pakistan      


are generated from plays a pivoted role in both of algorithms. Different sanitized databases are produced under the implementation of this effect of ordering the sensitive items for hiding sensitive association rules Moreover, the DSR algorithm seems to be more effective when the sensitive items have high confidence These framework. Privacy breach risk was minimized by keeping a high threshold while a small threshold yields high side effects. They argued that the major drawback of a blocking algorithm is the fact that the dataset, apart from the blocked values \(also known as unknowns\, is not distorted. Thus, an adversary can disclose the hidden rules by identifying those generated itemsets which contain question marks and lead to rules with a maximum  of a rule by increasing the 007  rules\ while their corresponding itemsets contain unknowns. Thus, the identification of the sensitive rules becomes harder since the adversary is unable to tell which of the rules with maximum 003 with MST and MCT. In above the minimum threshold the sensitive are and which ones are the ghost ones. However, the introduction of ghost rules leads to a decrement in the data quality of the sanitized outcome. To balance the trade-off between privacy and data loss the proposed algorithm incorporates a safety margin which corresponds to extend the sanitization being performed on the dataset. The higher the safety margin the better the protection of the sensitive rules and eventually the data quality of the resulting dataset becomes worse A FP-tree based method is presented in [11  fo r i n v e rs e  frequent set mining which is based on reconstruction technique. The method defined two thresholds Minimum Support Threshold \(MST\ and Minimum Confidence Threshold \(MCT\. It makes use of a transactional database 212 yx rule of counter hiding N YUX  Sensitive rules   2 confidence R h all of the rule by decreasing the denotes Minimum Confidence and Minimum Support respectively. The hiding counter is increased until the First strategy increases the count of support \(A\ without affecting the count of support \(AUB Second strategy incorporates count of support\(A\ as unchanged while decreasing the count of support\(AUB\. The authors in [17  pr opose d a s e t of e va l u a t i on p a r a m e t e r s including the completeness and consistency evaluation Unlike other techniques, their approach takes into account two more important aspects: relevance of data and structure of database. They provide a formal description that can be used to magnify the aggregate information of interest for a target database. Framework of 007  gets below the minimum threshold values of y y When all the sensitive association rules are hidden, clusters are converted into a modified database. This technique shows a high level of side effects both in terms of ghost rules and loss of non sensitive rules Two modification schemes can be found in [13, 14   B oth incorporate unknowns and aimed at hiding predictive association rules which are the rules containing the sensitive items on their LHS. Both algorithms rely on the distortion of a portion of the database transactions to lower the confidence In this technique, two strategies were employed to be used to decrease the D R D R R h R h D R confidence M confidence\(x M support\(x M confidence M support support A framework is proposed in   Va r i a b le num b e r s  of fuzzy membership functions were applied on quantitative data. Association rules were formed out of non boolean dataset. This fact makes this technique more realistic to real world application scenarios. This technique works on decreasing only Item ordering effect and or has 479 confidence confidence confidence confidence and and is changed to the released database that lies above the minimum the rules containing the sensitive items on their LHS, while the algorithm by [7  is able to hide any specific rule. The first strategy, called ISL decreases the confidence yx rule ofcounter hiding X YUX Rules D\220 D\220 support support ghost support support support support support support support support represent the sensitive association rules disclosing private information to public and need to be secured. In such situation, the original database framework. It can be particularly mentioned that these algorithms were primitive and first of their kind in hiding association rules techniques Side effects of these algorithms were also high The authors in [10 i n tr od uc ed a b l o c k in g  t e c h nique heuristic in In this way it was claimed that any association rule can be preserved. However, this technique did not mention about the released database. Among the objective outlined by [9   only the first of the three objectives was achieved. By increasing the denominator value in calculation of confidence  From  confidence 004 004 confidence confidence algorithms was best for all metrics. There were tradeoffs between efficiency and side effects. All of these algorithms were based on threshold. If the number of all of the association rules is small then the probability of identifying the sensitive rules among them also increases. To avoid this issue, the authors proposed a blocking algorithm that purposely creates rules that were nonexistent in the original dataset \(also known as R h and and 006 R  In  a  ne w t e ch nique i s e xt e nd e d f r o m the or i g i n a l  ones characterized by ISL and DSR [1  T h e  a u thor s in  mo dif i e d  both e q u a ti ons b y int r o duc in g a  ne w c o u n t e r  variable as shown in equation 2 and 3   3 where confidence B  of the association rules. Compared to the work of [7 th e  algorithms presented in [13, 14  r e qui r e d a r e duc e d  n u mb er  of database scans and exhibit an efficient pruning strategy However, by construction of this technique, they are assigned the task of hiding  of the itemset in its LHS. The second approach, called DSR reduces the  of the itemset in its RHS    and will not alter the original database Consequently, the technique did not release a database from which no sensitive rules can be generated at specified threshed values. It also did not mention the side effects including ghost rules and hiding of valid non-sensitive association rules A technique based on fuzzification of rules are mined under the same MST and MCT. Therefore, the sensitive information through rules is prevented from exposure. A similar algorithm called SRH where sensitive rules with single antecedent and consequent were clustered is proposed in  Ea c h r ule  is m odif i e d  in such a way that reduces its exists such that and of an association rule confidence 


 P ROBLEM S TATEMENT  In this section, first we describe some metrics related to hiding association rule mining problem. Figure 1 shows these metrics diagrammatically  D confidence Coverage\(A Conf\(A  11 We can formulate first research question as \215 and In the technique proposed by [19   f ir s tl y  si g n a l an d t e x t  f e a t ur e s  from images were extracted. Secondly, calculate their frequencies. Thirdly, apply well-known dimensionality reduction techniques such as stemming, stop word elimination, and Zipf\220s law to prune non-interesting features At the last stage, the remaining features were used to generate association rules. This technique shows that Correlation Coefficient, Laplace, Kappa and J-Measure result in better clustering compared to traditional E E Coverage  Confidence All-Confidence\(E\ = Supp\(E\ / Max\(Supp\(e element of E\\=P\(E\/Max\(P\(element of E C\=\(1-Supp\(C\\/\(1Conf\(A  A is primary pivoted measure in defining frequent Itemset. Frequent Itemset are su bsequently used to generate association rules. Ref a l so intr o d uc e d the  c on c e pt of  C In original formulation of association rules, only interesting measures defined were with 10 A characteristic of lift is that it does not suffer from the rare item problem and also does not exhibit down-ward closed closure property III been widely used in majority of the formulation of the association rules. However there may suffer from problems of Too Many Rules, Good Value for Threshold, Asymmetric Property of Confidence and Misleading Association Rules An overview of a number of symmetric objective interestingness measures, five of which are Lift \(or Interest Chi-Square, Correlation Coefficient, Log linear analysis and Empirical Bayes correction is provided in [12   The authors in  p r op os e d th r e e m e a sur e s f or  capturing relatedness between item pairs. Some of these measures offer properties that can be used to distinguish significant rules from non-significant rules There are other measures described in  wh i c h a r e  better in result as compared to traditional association rules interesting measures as  6 7 Ref.[2 de v e l o pe d  t he  c o nc e p t o f  Hiding Failure quantifies the percentage of the sensitive patterns that remain exposed in the sanitized dataset. Formally, it can be computed as in equation 11 denotes total number of sensitive association rules discovered in the released \(sanitized\ dataset Supp P  P  It is defined in equation 5 is also known as 4 Ref.[1  i n tro d u c ed  th e co n c ep t o f  confidence confidence confidence Confidence defined in equation 8 and Conviction\(A Lift\(A Ideally, the hiding failure is required to be null\216 confidence A\=Conf\(A A D support support support support support support C\=Supp\(A\=P\(C Support C\=Supp\(A C\/Supp\(A\=P\(A and C\/P\(A P\(C | A A\/Supp\(A\=P\(A and C\/\(P\(A\P\(C out-weights them. Likewise, the authors in   c r it ici z e d th e bo ole a n a s so cia ti on r u l e s g i v i n g  a n  example in which a user may stay at a web page for significant time, but s/he can visit other insignificant pages many times with negligible time Th i s m a y  l e a d t o the  visit count of insignificant web pages more than that of the significant web page. In the literature numerous interesting measures have been defined. We experimented using various statistically-inspired interestingness measures as functions to assign weights to itemsets of association rules. We described those measures which we have used in the proposed architecture mentioned in next section. Computational details of these measures can be found in [21, 22, 23, 24, 25   In order to compare them it is required to define all of them on the same units. For this purpose all of them have been defined in terms of probability P r o ba bil i t y  o f an event can be mapped to database by equation 4 where 5 defined as P  F r e q  E    D   Leverage Leverage  Ref.[22 rep l a c e d  t h e  SAR\220 SAR\220 HF Conviction   and confidence In literature SAR SAR HF C\Supp\(C Conf\(C  correspond to sensitive association rules appearing in the original dataset  Hence, all of the techniques investigated so far are based on the common framework of  However, there are situations where rules are valuable but low notC\/P\(A and not C 8 A and C\-\(P\(A\\(C 9 Ref.[23 intr o d uc ed c onc ept of I n t e r e st w h i c h got  popularity under the name of Lift. It is defined below 004 004 004 004 004 004 004 004 004 004 PPDM Data Sharing-based Sanitization Problem  Figure 1  480  which exhibit downward-closed closure property. It is defined by equation 7    was introduced by I t w a s ba s e d on the  concept of difference between two measures. Co-occurrence of Antecedent and Consequent is measured followed by calculation of their individual appearance as depicted by equation 9 C\Lift\(C Hiding Failure is defined in equation 6 and All-Confidence 


 006 b   Ni qi Median MD W  021    200 N i qi SU W Misses Cost 002 002 002 1 1 _ 11    1,1,1  NSAR NSAR NSAR MC D\220 GR D SAR\220  Recovery Factor and are determined by the total number of SAR and the total number of items in a SAR. Once the set of counts is determined, the weights are calculated according to equation 10 to 13 Equation 10 and 11 both calculate the Sum and Mean of the counts which are calculated in the equation 9. Equation 12 and 13 determine the values of the Median and Mode over the set of counts is also known as artifactual rules that is a quantifier of the percentage of the discovered association rules that are non genuine and artifact. This measure is computed as shown in equation 13 Recovery Factor NSAR NSAR\220 Secondly B D\220 RF SAR SARSAR SAR GR 12 13 P ROPOSED ARCHITECTURE  From our literature review, all the techniques so far are based on conventional framework comprising of whose values are set by number of transaction, number of SAR, number of Items     The flow of the proposed architecture as shown in figure 2 starts from data preparation of comma separated text data The values of this dataset are converted into a boolean dataset from CSV file format. Single value frequent items are generated from this database. These single valued frequent items are filtered out based on the provided frequency of count. Next step is to apply the sub-set algorithm on this set which generates all of the frequent items within provided threshold of frequency of count. Another component known as duplicate removal continuously remove the duplicate frequent items. The generation of frequent item is done through recursive programs of three functions. We know 481  and and This measure quantifies the percentage of the nonrestrictive association rules which are hidden as a side-effect of the sanitization process. It can be calculated by the equation 12 Third research question can be defined as describes the possibility by which an adversary can recover a sensitive rule based on the nonsensitive rules. If all  will be counted on. The upper limits of 215Achieving minimum number of Misses Cost \(MC\ during PPDM\216 C 215Ghost/Artifactual Rules should be ideally zero\216 D support Flow of the Proposed Architecture It is noticeable that there exists a compromise between the misses cost and the hiding failure since the more sensitive association rules one needs to hide the more legitimate association rules are expected to miss The second research question can be described as value is 0. Fourth research question can be formulated as Ghost Rules 200 confidence 9 and Firstly \215 IV   200  215How to minimize the side effects incurred during the hiding of association rules?\216   They suffer from side effects of miss cost or ghost rules or both whereas; some failed in hiding all the sensitive association rules. They are the stimulus of an innovative architecture by which other statistical measures are exploited for hiding sensitive association rules. Hence we devised a weighing mechanism based on central tendency. In the proposed architecture, we considered the five well-established measures to devise the association rules. The measures used in the architecture are discussed in mathematical notation in previous section. Second part of the proposed heuristic is comprised of different weights. These weights set, comprises of four kinds of statistical measures of central tendency. These include Sum, Mode, Mean and Median. Weights of each transaction in the dataset are generated by the measurement of how much a transaction defines a set of SAR. It counts how many times a single item appears in those SAR which are sub-sets of dataset transactions. These four statistical values comprise a set of weights out of this set we take mean, median, mode and sum Whereas mean, median, mode and sum each represents the weight assigned to a transaction. Weight of each transaction represents the degree of its correlation to the set of SAR Equation 9 represents the total count of single item in SAR while this single must be a sub-set of a Transaction. Counters are represented by is the maximum number of transaction in dataset up to which Thirdly A     1 _ 10 021  212  n\212   the sub-sets of a sensitive rule are recoverable from the sanitized dataset, then the recovery of the rule is also compromised. In this case, it is assigned an 215Is there any other measure which yields better results in privacy preservation?\216 is the number of association rules discovered in the original database RF RF  N i qi N ME W          0 _ 0 _ I SAR n zyx TIDx SARy Iz q 215Recovery Factor of each of the non-sensitive association rules after process of PPDM must be zero\216 What is the problem in association rule mining in reference to security when conventional framework of support and confidence is used?\216 Ghost Rules measure value of 1, otherwise and D  x y z n x y z is the number of all non-sensitive association rules in the original dataset is the number of those non-sensitive association rules revealed in released sanitized\ dataset SAR  In the proposed architecture, we have not incorporated this research question. On the basis of above four research questions we can devise our problem statements as below 12 where 13 where is the number of association rules discovered in released \(sanitized\ dataset Ni qi Mode MO W 


Weight Generation Algorithm Heuristic 1: Weights Generation B support Leverage     Figure 2  Figure 3 Flowchart of the Proposed Model for PPDM It takes SAR one by one. First it checks whether it is a sub-set of the first transaction with highest weight in the sorted database. The database is sorted on any of the four weights. The selected SAR is always composed of two or more items. At this stage the component provides user with two options. In the first option the item with highest frequency from selected SAR is modified. Its value of 1 is flipped to zero. In the second option, the item with lowest frequency from chosen SAR is selected and then transaction is modified. In this way only a single transaction is modified and the weight of the transaction needs to be recalculated This component continues the process of pruning until any of the two conditions is met. The first condition is that the measure value of every SAR falls below the minimum threshold measure value of all of the set of association rules The second condition is that the frequency of count of the sensitive association rule gets below the provided value on the basis of which all of the association rules were generated At the end we have a modified database which does not generate any sensitive association rule on the same provided frequency of count and certain minimum threshold of measure. The developed framework consists of two prime heuristic which are shown in Figures 3 and 4  Conviction Confidence and The term weight refers to the degree by which a transaction in a dataset supports all of the sensitive association rules. If all of the items involved in sensitive association rules are found in a specific transaction, then the transaction would be assigned a maximum value. Hence a modification in this transaction will mostly change the minimum measure values of the association rules. This component first calculates the number of occurrences of each item found in all of the sensitive association rules. After this there are four ways to find the weight of each transaction 1 Calculate Mean-Weights All of the occurrences are added and then an average value is calculated 2 Calculate Median-Weights Median value among all of the occurrences are found and assigned as weight 3 Calculate Mode-Weights Mode value for all of the occurrences of item in all of the sensitive association rules is calculated 4 Calculate Sum-Weights All of the occurrences are sum up to a single value After calculating the weights of each transaction, we have a dataset which we can sort for any of the weights in ascending or descending order 482 Lift All-Confidence is below certain threshold of frequency of count. The program now marks the association rule\(s\ as sensitive association rules. The next steps reduce any measure of these SAR so that they are eliminated out of the set of association rules. The measures refer to the set of from the Apriori algorithm [1 that f r e que nt  i t e m s c a n determine association rules. The next step generates all of the association rules through recursive functions based on subset problem. At this stage we have all of the association rules whose    Based on the set of SAR, we determine the weights of each transaction in the database. Pruning technique component plays a pivoted role as this component is responsible to modify the actual transactions of the database  


Comparison results of side effects 483 Transactions D ATASETS USED IN OUR EXPERIMENTS  R ESULTS AND DISCUSSION  We validated the proposed novel architecture for hiding sensitive association rules using the datasets from UCI Machine Learning Repository dataset http://mlearn.ics.uci.edu/databases\. These dataset are often used for benchmarking data mining algorithms. For our experiments we chose those items which were convertible to boolean values. The datasets used are shown in Table 1 TABLE I Lymphography Results Figure 6 depicts the comparison results to that of others techniques. In comparison to the technique proposed by  the proposed technique suffers less from miss cost. As far as other side effects of ghost rules and failure in hiding sensitive association rules are concerned, we observe that the technique in our proposed architecture produces zero ghost rules and can hide all of the sensitive association rules with zero hidden failure. It gives better result to all of those techniques with side effects of ghost rules and failure in hiding sensitive association rules. The works in [2,3,4,13  have their techniques implemented on single valued frequent items and single valued association rules; however the proposed technique is applicable to both single valued as well as multi valued frequent items and association rules. As shown in Figure 5, the number of new rules is minimized in a specific measure which is Leverage. This case outperforms the other peer measures as well as in comparison to the other techniques discussed in the literature. Nonetheless one notable result is that in all of the works performed, the technique in the proposed architecture has zero ghost rules whereas other techniques in literature have ghost rules  Instances Instances 10 149 10 115 1332 0 Total Total Missing Heuristic 2: Pruning Technique Rules Dataset Attributes DB DB  Figure 4  Figure 5  Figure 6 Hypothyroid 4822 1658 26 18       Pruning Algorithm FOC L YMPHOGRAPHY DATASET  Lymphography 148 None 19 9 Thyroid0387 9172 None 29 20 C Zoo Dataset 101 None 18 15  This technique first sorts all of the dataset items for any chosen weight in ascending or descending order. A sensitive association rules is chosen and checked for its certain threshold value against any selected measure. If this value is above the threshold value then it indicates this association rule is not hidden. It analyzes each item involved in this association rules. Two options are on either selecting the Item with highest frequency or lowest frequency Experiments have shown that they yield different results Sometimes, highest frequency item from SAR is a better choice and vice versa. After selection of a single item out of this association rule, we modify that item\220s value in transaction dataset. Now this will definitely lower down the certain threshold value of respective association rules. This will also alter the weight Table of that modified transition and its weight will be recalculated. This process is continued until all of the sensitive association rules have their certain threshold values below the specified values. At the end we can obtain the released database V Ghost Items Items Freq  Figure 5 and Table 2 shows respectively the results taken on the Lymphography dataset. In these results Leverage gives better results followed by Lift. This fact indicates that other than confidence, other measures also exist which can give better results TABLE II  We in the proposed architecture have selected association rules with all of these characteristics including single valued multi valued frequent items and association rules  Ordinal Attributes Assoc Rules 


Y. Saygin, S. Vassilios, V.S. Verykios, and A. K. Elmagarmid 215Privacy preserving association rule mining,\216 In Proceedings of the 12th International Workshop on Research Issues in Data Engineering 2002\, pp.151\205158 7 M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim, and Verykios V S., \215Disclosure limitation of sensitive rules,\216 in Proc. 1999 IEEE Knowledge and Data Engineering Exchange Workshop \(KDEX\22099 pp 45\20552, 1999 3 D. L. Seong,  and H.C. Park, \215Mining Frequent Patterns from Weighted Traversals on Graph using Confidence Interval and Pattern Priority\216, IJCSNS International Journal of Computer Science and Network Security, Vol.6 No.5A, May 2006 21 E. Bertino, and I.N. Fovino, \215Information driven evaluation of data hiding algorithms,\216 in Proc. 7th Internationa Conference on Data Warehousing and Knowledge Discovery, pp. 418\205427 \(2005 18 S. Brin, R. Motwani, J. D. Ullman, and T. Shalom, \215Dynamic itemset counting and implication rules for market basket data,\216 In SIGMOD 1997, Proceedings ACM SIGMOD Int. Conference on Management of Data, pp.255-264, USA, May 1997 24 H. H. Malik, and J. R. Kender, \215Clustering Web Images using Association Rules, Interestingness Measures, and Hypergraph Partitions\216, ICWE\22006, July 11\20514, 2006, Palo Alto, California, USA ACM 1-59593-352-2/06/0007 20 S. Vassilios, V.S. Verykios, A.K. Elmagarmid, E. Bertino, Y. Saygin and E. Dasseni, \215Association Rule Hiding, IEEE  Transactions on Knowledge and Data Engineering\216, vol. 16, no. 4, pp. 434-447, 2004 10 P. Shapiro, \215Discovery analysis, and presentation of strong rules,\216 Knowledge Discovery in Databases, 1991, p. 229-24 25 2\long with these measures we devised a statistical weighing mechanism which is based on central tendency VI T. Pang-Ning, K. Vipin, and S. Jaideep, \215Selecting the right objective m eas ure for association analysis\216. Information Systems, 29\(4 pp.293-313, 2004 22 M. Gupta, et al., \215Privacy Preserving Fuzzy Association Rules Hiding in Quantitative Data\216, International Journal of Cmputer Theory Engineering, Vol. No.4 Oct 2009 17 support Leverage  We observed that computationally there was no difference found among these four chosen statistical weights It correlates that for each transaction, higher the sum of the relationship of association rules, the greater would be the other three weights and vice versa. This implies that all of these are equivalent to each other with respect to the side effects of the technique. \(3\his architecture generates no artifactual or ghost rules. \(4\ Failure in hiding sensitive association rules is also completely eliminated. They are prime achievement in the proposed architecture. \(5\ Side effect of Lost Rules do exists. The inherent reason for failure in elimination of Misses Cost is characterized by marking the association rules as sensitive or non-sensitive. If sensitive rules are multi valued involving high value of measures then in order to reduce their measure value to prevent them from being discovered, Misses Cost factor would be high As a future work we intend to investigate other measures than those listed in   S e c o n d l y   w e s h a l l i n tr odu ce a  n e w  supplementary technique to this architecture. It calculates the probability of each item involved in the SAR. Based upon these frequencies the database transaction would be modified in a way to minimize the side effects of lost results. The software should dynamically select the sort-order, ascending order or descending, on each transaction basis. Also we shall pay attention to the calculation of Recovery Factor - the ability by which one can infer about the modified dataset transaction. We first calculate the recovery factors through all known available techniques and then minimize them R EFERENCES  1 S.L. Wang, and A. Jafari, \215Using unknowns for hiding sensitive predictive association rules,\216 In Proceedings of the 2005 IEEE International Conference on Information Reuse and Integration \(IRI 2005\, pp.223\205228, 2005 14 All-Confidence confidence L.Sweeney, \215k-anonymity: a model for protecting privacy,\216 International Journal on Uncertainty, Fuzziness and Knowledgebased Systems, 2002, pp.557\205570 26 and E. Dasseni., V.S. Verykios, A.K. Elmagarmid, and E. Bertino 215Hiding association rules by using confidence and support,\216 in proc 4th International Workshop on Information Hiding, pp 369\205383, 2001 4 W. Chih-Chia, C. Shan-Tai, and L. Hung-Che, \215A Novel Algorithm for Completely Hiding Sensitive Association Rules,\216 in Proc. Eighth International Conference on Intelligent Systems Design and Applications Taiwan, 2008 484 K. Duraiswamy, D. Manjula, and N. Maheswari,\215A New approach to Sensitive Rule Hiding\216, Journal of Computer and Information science, 2008 13 Conviction Lift R. Agarwal, T. Imielinski, and A. Swami, \215Mining associations between sets of items in large databases\216. SIGMOD93, pages 207216, Washington, D.C, USA, May 1993 2 C. Clifton, and D. Marks, \215Security and Privacy Implications of Data Mining,\216 in Proc. ACM Workshop Research Issues in Data Mining and Knowledge Discovery, 1996 9 S.R.M. Oliveira, and O.R. Zaiane, \215Privacy preserving frequent itemset mining,\216 in Proc. IEEE icdm Workshop on Privacy, Security and Data Mining, vol. 14, pp. 43\20554 \(2002 6 are normally used by various researchers; but we claimed that instead of this framework there exist other measures as well. Our proposed architecture hence adopted these measures such as G. Yuhong, \215Reconstruction-Based Association Rule Hiding\216, in Proc. SIGMOD2007 Ph.D. Workshop on Innovative Database Research \(IDAR 2007\, June 10, 2007, Beijing, China 12 R. Edward, et al., \215Alternative interest measures for mining associations in databases\216, IEEE TKDE 15 \(2003\ no.1, pp.57\20569 23 C.B. Ramesh et al., \215Hiding Sensitive Association Rules Efficiently By Introducing New Variable Hiding Counter,\216 IEEE, 2008 16 S. Oliveira, O. Zaiane, and Y. Saygin, \215Secure Association-Rule Sharing,\216 PAKDD Conference, 2004 15   V.S Verykios, E. Bertino, I. Nai Fovino, L. Parasiliti, Y. Saygin, and Y. Theodoridis, \215State-of-the-art in privacy preserving data mining,\216 SIGMOD Record 33\(1\ 50\20557 \(2004 5 C ONCLUSION AND F UTURE W ORK  PPDM is important in data mining. In this research paper we introduced a novel method for concealing sensitive association rules and producing minimum side effects. The innovative elements of our proposed model are summarized 1\ For concealing sensitive data, conventional measures of Median, Mode and Sum Y. Saygin, V.S. Verykios, and C.Clifton, \215Using unknowns to prevent discovery of association rules\216. ACM SIGMOD Record, 30\(4\45\20554 December 2001 8 Mean L. Chang, and I. Moskowitz, \215An integrated framwork for database inference and privacy protection,\216 Data and Applications Security Kluwer, 2000 19 and E.D. Pontikakis, A.A. Tsitsonis, and V.S. Verykios, \215An experimental study of distortion-based techniques for association rule hiding,\216 in proc. 18th Conference on Database Security \(DBSEC 2004\, pp. 325\205339, 2004 11                            


708 


709 


710 


calculating the weight for each web page for respective web site. The proposed approach uses Visiting Frequency and Time Spent on a Web page as two parameters to measure the weight of each web page To estimate the performance of the proposed two algorithms i.e. FPW and FTPW, discussed in section IV based on the above parameters involved in estimation C. Experimental Results The performance of the proposed approach can be evaluated by comparing the performance of FPW and FTPW algorithms which differ in number of parameters considered for experimentation. The comparison is made by taking the attribute like Visiting Frequency in FPW, and further Visiting Frequency and Time spent on a web page are clubbed together in FTPW as another attribute. The experimental setup uses five users and weights are plotted against various parameters Figure 2 shows the plot of Visiting Frequency v/s Weight of a web page              Figure 2: Plot between frequency and weight Algorithm: FTPW Input: Web traversal path database Output: Weight for each page 1 Calculate PageRank for each page \(PRi 3 2 Initially Set W\(Pi 3 Check the user is registered or not, if YES then 4       whether the user is first time visitor, if YES then 5  return W\(Pi 6    else 7      calculate FW\(Pi 4 


8            calculate TW\(Pi 6 9   SET   W\(Pi Pi Pi 10   return W\(Pi A C B    D 0 0.05 0.1 0.15 0.2 0 5 10 15 20 w ei gh t Visiting Frequency User1 User2 User3 user4 User5 2010 5th International Conference on Industrial and Information Systems, ICIIS 2010, Jul 29 - Aug 01, 2010, India 198  Using the weights calculated in Figure the higher weight for more frequently visited information in FPW algorithm the different the scenario described in Figure 1 for all th plotted in Figure 3            


   Figure 3: Recommendation for Web Path Trave Algorithm  The weight assigned to various pages Visiting Frequency and Time spent on web Figure 4                  Figure 4: Plot between \(frequency + time sp  The Figure 5 gives the details of recomm Traversal for different users by FTPW a weights calculated by considering two para and time spent in Figure 4 which indicates for more frequently visited pages and in term on web pages  0 0.02 0.04 0.06 0.08 0.1 0.12 


0.14 0.16 0.18 W ei gh t Web Pages Us Us Us Us Us 0 0.2 0.4 0.6 0.8 1 1.2 1.4 W ei gh t FTPW \(Frequecy  and 2 which indicate pages. Using this traversal paths in e users have been rsal based on FPW by combining the page is plotted in ent ended Web Path lgorithm and use meters frequency the higher weight s more time spent    


         Figure 5: Recommendation W Algo Figure 6 shows the relative on the synthetic data sets, in w more efficient than FPW algor performance of proposed a increase the complexity of alg and provide better Web Path Tr    Figure 6: Relative Access  The proposed FTPW algorithm parameters which otherwise ar FPW algorithm. A matrix dep comparison between above two parameters are performance ce show that the performance o increase the number of param FPW algorithm to FTPW algori The experimental results d better and provides a methodo optimized Web path traversal past navigation behavior by c page 0 1 2 3 4 5 1 2A 


cc es si bi lit y Ti m e fo r M or e re qu ir ed In fo rm at io n er1\(A->C->D->B er2\(D->B->A->C er3\(D->B->C->A er4\(C->B->A->D er5\(B->A->D->C Time User1 User2 User3 User4 User5 0 0.2 0.4 0.6 0.8 1 1.2 


W ei gh t Web Pages eb Path Traversal based on FTPW rithm  execution for FPW and FTPW hich we can see that FTPW is ithm. Hence, it is clear that the lgorithm increases when we orithm in terms of parameters aversal in less time  ibility time for FPW and FTPW consists of clubbing of various e not available in first proposed icted in the Figure 7 describes proposed algorithms. Here the ntric and a comparison results f the system improves as we eters i.e. when we move from thm rawn for FTPW algorithms are logy for effective, efficient and for various users based on their omputing weight for each web 3 4 5 User FTPW FPW User1\(C->B->A->D User2\(A->B->C->D User3\(B->C->A->D User4\(D->A->C->B User5\(B->D->A->C 2010 5th International Conference on Industrial and Information Systems, ICIIS 2010, Jul 29 - Aug 01, 2010, India 199 Figure 6: Comparison Matrix VI.  CONCLUSION & FUTURE WORK 


 Web Usage Mining have been used in improving Web site design and marketing decision support, user profiling, and Web server system performance. Web page prediction technique is a very important role in web technologies. This paper proposes efficient algorithms for web path recommendation based on Weighted Association Rule. Two factors frequency and time spent were used to decide the web path traversal. The experimental results show that in the proposed approach when we increase the number of parameters for finding the web path the accuracy of the system is enhanced drastically and FTPW produces more accurate results than those achieved by FPW In the future, we shall improve the Web Path Traversal by considering the parameter Data Transfer Rate to provide the accurate Web Path traversal REFERENCES 1] M. S. Chen, X. M. Huang and I. Y. Lin, Capturing User Access Patterns in the Web for Data Mining, Proceedings of the IEEE International Conference on Tools with Artificial Intelligence, pp. 345348, 1999 2]  R. Cooley, B. Mobasher, and J. Srivastava, Web Mining: Information and Pattern Discovery on the World Wide Web, Proceedings of the 9th IEEE International Conference on Tools with Artificial Intelligence, pp 558-567, 1997 3]  B. Mobasher,N. Jain,E. Han et al, Web mining: pattern discovery from World Wide Web transactions, Tech Rep: TR96-050, 1996 4]  C. Shahabi, A. Zarkesh, J. Abidi, V. Shah, Knowledge discovery from user's Web-page navigation,  in Proceedings of the 7th IEEE International Workshop on Research Issues in Data Engineering, 1997 5]  Yue-Shi Lee, Show-Jane Yen, Ghi-Hua Tu and Min-Chi Hsieh, Web Usage Mining: Integrating Path Traversal Patterns and Association Rules, Proceedings of International Conference on Informatics Cybernetics, and Systems \(ICICS'2003 6]  Yue-Shi Lee, Show-Jane Yen, Ghi-Hua Tu and Min-Chi Hsieh, Mining Traveling and Purchasing Behaviors of Customers in Electronic Commerce Environment, Proceedings of IEEE International Conference on e-Technology, e-Commerce and e-Service \(EEE'2004 pp. 227-230, 2004 7]  J. Srivastava, et al. Web Usage Mining: Discovery and Applications of Usage Patterns from Web Data. SIGKDD Explorations, pp. 12-23 2000 


8]  Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual Web search engine. Proceedings of the seventh international conference on World Wide Web 7: pp. 107-117, 1998 9]  J. Pei, J. Han, B. Mortazavi-Asl and H.Zhu, Mining Access Patterns Efficiently from Web Logs, Proceedings of the Pacific-Asia Conference on Knowledge Discovery and Data Mining, pp. 396-407 2000 10]  C. H. Cai, A. W. C. Fu, C.H. Cheng and W. W. Kwong, Mining Association Rules with Weighted Items, In Database Engineering and Applications Symposium, Proceedings IDEAS'98, pp. 68  77, 1998 11]  F. Tao, F. Murtagh and M. Farid, Weighted Association Rule Mining using Weighted Support and Significance Framework, In Proceedings of the 9th SIGKDD conference, 2003 12]  Show-Jane Yen, An Efficient Approach for Analyzing User Behaviors in a Web-Based Training Environment, International Journal of Distance Education Technologies, Vol. 1, No. 4, pp.55-71, 2003 13]  Show-Jane Yen, Yue-Shi Lee and Chung-Wen Cho, Efficient Approach for the Maintenance of Path Traversal Patterns, In Proceedings of IEEE International Conference on e-Technology, eCommerce and e-Service \(EEE 14]  M. Spiliopoulou and L. C. Faulstich, Wum: A web utilization miner EDBT Workshop WebDB98, Springer Verlag, 1996 15]  M. S. Chen, J. S. Park and P. S. Yu, Efficient data mining for path traversal patterns,  IEEE Transactions on Knowledge and Data Engineering, pp. 209-221, 1998 16]  H. Yao,H. J. Hamilton, and C. J. Butz, A Foundational Approach to Mining Itemset Utilities from Databases, Proceedings of the 4th SIAM International Conference on Data Mining, Florida, USA, 2004 17]  Z. Chen, R. H. Fowler and A. Wai-Chee Fu, Linear Time Algorithm for Finding Maximal Forward References, Proceedings of International Conference on Information Technology. Computers and Communications  \(ITCC'2003 18]  T. Jing, Wan-Li Zou and Bang-Zuo Zhang, An Efficient Web Traversal Pattern Mining algorithm Based On Suffix Array, Proceedings of the 3rd International Conference on Machine Learning and Cybernetics , pp 1535-1539, 2004 19]  Show-Jane Yen, Yue-Shi Lee and Min-Chi Hsieh, An efficient incremental algorithm for mining Web traversal patterns, Proceedings of the 2005 IEEE International Conference on e-Business Engineering ICEBE05 20]  L. Zhou, Y. Liu, J. Wang and Y. Shi, Utility-based Web Path  Traversal Pattern Mining, Seventh  IEEE International Conference on Data 


Mining Workshops, pp. 373-378, 2007 21]  C. F. Ahmed, S. K. Tanbeer, Byeong-Soo Jeong and Young-Koo Lee Efficient mining of utility-based web path traversal patterns, 11th International Conference on Advanced Communication Technology ICACT09 22]   http://en.wikipedia.org/wiki/PageRank 23] en.wikipedia.org/wiki/Association_rule_mining  Attributes? FPW Algorithm FTPW Algorithm Recognition of User behavior Visiting Frequency Page Rank Time Spent on Web page Page Size Accessibility of required information in less time Improving Web navigation and system design of Web applications  Enhancing server performance 2010 5th International Conference on Industrial and Information Systems, ICIIS 2010, Jul 29 - Aug 01, 2010, India 200 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


