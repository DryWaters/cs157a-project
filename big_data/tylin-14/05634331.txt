Distributed Association Mining on Message Passing Systems  Chia-Chu Chiang and Shen Lu Department of Computer Science University of Arkansas at Little Rock 2801 South University Avenue Little Rock, AR 72204-1099, USA e-mail: {cxchiang|sxlu}@ualr.edu   Abstract Association mining in finding relationships between items in a dataset has been demonstrated to be practical in business applications. Many companies are applying association mining on market data for analyzing consumers purchase behavior. The Apriori algorithm is the most established algorithm for association mining in finding frequent itemsets. However, the time complexity of the Apriori algorithm is dominated by the size of candidate itemsets Research to date has focused on the efficient discovery of itemsets in a large dataset. Those improvements include the optimizations of data structures, the partitioning of datasets and the parallelism of data mining. In this paper, we propose a distributed association mining algorithm in finding frequent itemsets. The work is different from many existing distributed algorithms where most of existing algorithms center on the reduction of the size of the dataset. Our distributed algorithm focuses on the reduction of the size of candidate itemsets. The work of candidate k-itemsets generation is evenly distributed to the nodes for workload balancing among processors. The complexity analysis of the distributed algorithm is also presented Keywords- Apriori, Association Rules, Data Mining Frequent Itemsets, Distributed Computing I.  INTRODUCTION Association mining in finding correlations between items has been shown to be useful in business applications. Retail stores routinely use association tools to learn about purchasing habits of its customers [1]. Healthcare organizations use association tools to improve the health care to the patients and reduce the costs also [2]. Many implementations of association mining have been proposed to find frequent itemsets in a very large dataset. They are 


broadly classified into two categories: Apriori [3] and FPgrowth [4]. The Apriori algorithm discovers large frequent itemsets by iteratively performing the two steps. The first step consists of finding large itemsets. The second step consists of finding associations with a user specified confidence among the large itemsets discovered in the first step. The Apriori algorithm discovers the correlations between items by generating maximal frequent itemsets iteratively. One major bottleneck of the Apriori algorithm is its time complexity of finding all large itemsets. The FPgrowth algorithm discovers frequent itemsets in a dataset without candidate generation process which is a very time consuming one in the Apriori algorithm. Although the FPgrowth algorithm outperforms the Apriori algorithm in performance, several variants of the Apriori algorithm have been made to speed up the performance of the Apriori algorithm In this paper, we present a distributed Apriori algorithm on message passing systems to improve the performance of finding frequent itemsets. We study the problems of the Apriori algorithm causing the bottleneck of the performance in finding large frequent itemsets. We then study the degree of parallelism and synchronization issues in parallelizing association rule mining. We also present a set of optimizations for the Apriori algorithm for parallelism The paper is organized as follows. Related work is surveyed and presented in Section II. Section III introduces the Apriori algorithm and our improved Apriori algorithm for distributed processing is presented in Section IV. The complexity of our presented algorithm is discussed in Section V. An optimized tree data structure is suggested in Section VI to reduce the sizes of the trees for candidate itemsets generation. Section VII presents the performance of our parallel algorithm. Finally, we summarize the paper in Section VIII II. RELATED WORK The Apriori algorithm was proposed by R. Agrawal and R. Srikant in 1994 [3]. Basically, the Apriori algorithm first finds all frequent itemsets and then generates association rules from the frequent itemsets. The process is iterative until no more frequent itemsets can be found. However, the performance of the Apriori algorithm is a major concern in 


finding frequent itemsets. Much of the time is spent in dealing with the creations of candidate itemsets. The algorithm has linear dependence on the size of the dataset but the exponential growth on the size of the itemset. Thus, the time spent for the algorithm is considerable Many variants of the Apriori algorithm have been proposed that focus on improving the efficiency of the original algorithm. Several of these improvements are hashbased technique, transaction reduction, partitioning, and sampling. The algorithms using hash table or hash tree to reduce the number of the candidate itemsets examined When scanning the transactions in the dataset to generate the International Symposium on Parallel and Distributed Processing with Applications 978-0-7695-4190-7/10 $26.00  2010 IEEE DOI 10.1109/ISPA.2010.35 208 candidate itemsets, the itemsets are distributed into the different buckets of a hash tree or tree. Examining the bucket count of the corresponding itemsets against the support for the removals of infrequent itemsets improves the efficiency of the algorithms. However, the algorithms are not primarily designed to reduce the size of a candidate itemset, thus, the algorithms might still need to spend considerable amount of time in generating them The partitioning algorithms divide the transactional dataset D into n non-overlapping partitions, D1, D2, , and Dn. The algorithms reduce the number of dataset scans to two. During the first scan, the algorithm finds all itemsets in each partition. Those local frequent itemsets are collected into the global candidate itemsets. During the second scan these global itemsets are counted to determine if they are large across the entire dataset. The partitioning algorithms improve the performance of finding frequent itemsets and also provide several advantages. Small partitions might be fit into main memory than large one. Because the size of each partition is small, the algorithms might reduce the size of candidate itemsets. In addition, the algorithms require only two scans on the dataset. However, the partition algorithms reduce the size of the dataset, they might still need to deal with the large size of itemsets Many algorithms apply the partitioning technique for association rule mining in parallel. The algorithms mainly 


partition datasets or candidate itemsets for parallelism Partitioning datasets for parallel association mining \(count distribution algorithms partitions. Partitions are distributed to processors where each processor creates its local candidate itemsets against its own dataset partition. The processors are then exchanging their local dataset partitions and candidate itemsets for the global candidate itemsets. Each processor removes its infrequent itemsets from its local candidate itemsets against the global one. The resulting candidate itemsets become the frequent itemsets for the  next candidate itemsets [5]. The extraction of the frequent itemsets is done on all the processors in parallel. However, the extraction can also be done on one master processor. By doing so, the master processor need to broadcast the result to the other processors. Partitioning datasets for parallelism has a weakness that it requires many synchronizations among processors Partitioning candidate itemsets for parallel association rule mining divides datasets and itemsets into partitions. The partitions are distributed to processors. Each processor creates its local candidate itemsets against its own local dataset partition. To determine the global candidate itemsets processors are exchanging their local candidate itemsets and dataset partitions. Each processor then determines the global itemsets and generates the next local candidate itemsets Unlike partitioning datasets, it also partition itemsets to reduce the size of the candidate itemsets. The algorithm suffers from heavy for data exchanges between processors Parallel association rules play an important role in mining very large datasets for efficiency. However, parallel processing requires the availability of parallel processors Sampling [6] for association rule mining does not require parallel processors. A sample is drawn from the dataset which it can fit into the computer core. The Apriori algorithm is applied to find the potentially frequent itemsets from the sample with a support threshold lowered so we are unlikely to miss any truly frequent itemsets. During the first scan, sampling obtains the candidate itemsets from the sample. It then adds the itemsets to the candidate using the negative boarder. These itemsets are not found as frequent in the sample, but they might be frequent in the dataset. The resulting itemsets become a candidate itemsets. The 


algorithm makes a scan over the dataset to determine the next frequent itemsets. If no itemsets in the negative border is frequent in the entire dataset, then the frequent itemsets are exactly those candidate itemsets that are above the threshold However, if there is an itemset of the negative border is found frequent, then the sampling algorithm repeatedly applies the negative border until the candidate itemsets do not grow further There are several papers for the tutorial of the Apriori algorithm. Dunham et al. [5] present their finding in comparing various association rules algorithms. Bodon conducted a similar examination on the efficiency of frequent itemsets mining algorithms [7]. Hegland [1] gives a tutorial on the Apriori algorithm. The tutorial reviews basic concepts of association rule mining and the corresponding algorithms III. THE APRIORI ALGORITHM In this section, we briefly introduce the concepts of association rule mining for frequent itemsets. Along with the definitions of association rule discovery, the Apriori algorithm is presented. The time complexity of finding frequent itemsets in the Apriori algorithm is addressed. The algorithm is also illustrated with examples A. Notation Definition 1. Let T = {t1, t2, t3, , tn} be a transactional dataset consisting of a finite set of business transactions where ti is a business transaction and 1 ? i ? n. Let I = {i1, i2 i3, ., im} be a finite set of items \(itemset and 1 ? i ? m. A business transaction ti = <TID, X> typically includes a transaction identity \(TID Definition 2. An itemset in Ti is called a k-itemset where k = |X|. The support of an itemset X in D denoted as support_count\(X containing X. An itemset X is frequent if its support count is greater than a support count threshold called minimum support count Definition 3. Let U = <D, T, I> be the association mining context, where T and E are finite sets of transactions and items, respectively in a dataset, D. D ? T  I, is a binary relation where D = {<ti, X> | ti ? T and X ? I}. In D, an association rule is of the form X ? Y where X ? I, Y ? I, X Y = ?, X ? ? and Y ? ?. The rule holds in D with support s 


where s is the percentage of transactions in D that contain X 209 and Y. It means that the probability of finding X and Y denoted as support\(X ? Y X ? Y support\(X ? Y X ? Y only interested in a high probability of finding Y in high support. This level of high support is called the minimum support threshold Definition 4. The rule X ? Y also has confidence c where c is the percentage of transactions in D containing X that also contain Y. It means that if we find an itemset X then we have a chance of finding an itemset Y denoted as confidence\(X ? Y Y|X X ? Y support\(X X ? Y X The following example reiterates the terms and definitions of association mining for frequent itemsets Example 1. Suppose we have T = {t1, t2, t3, t4, t5} and I = {a b, c, d, e} in the following dataset D where D = {<t1, {a, b c}>, <t2, {d, e}>, <t3, {a, c, e}>, <t4, {b, c, d, e}>, <t5, {a Figure 1 presents an example to illustrate frequent itemsets mining  Figure 1.  An example of the transactional dataset  Figure 2.  An example to illustrate frequent itemsets The association rules are illustrated below confidence\({a, c} ? {e a, c} ? {e support_count\({a, c confidence\({e} ? {a, c e} ? {a, c support_count\({e Definition 5. Large Itemsets Property. All nonempty subsets of a frequent itemset must also be frequent. In other word, if an infrequent itemset is added to an itemset I, then the resulting itemset is not frequent Proof: Given an itemset A such that support\(A min_support is added to an itemset I, show that support\(I A support\(I ? A I  ? A min{frequency\(I A frequency\(I I frequency\(A I A min_support. Therefore, support\(I ? A 


which is also not a frequent itemset With Definition 5, the Apriori algorithm [3] was proposed to find all significant association rules. The Apriori algorithm first determines the large frequent 1-itemsets. This frequent 1-itemsets is then used to generate the candidate 2itemsets. The process is repeated to find k-itemsets until the candidate k-itemsets is empty. The following we present the Apriori-Join algorithm used in the Apriori algorithm. The Apriori-Join algorithm takes a set of large k-itemsets, Lk-1 as an input argument and returns a superset of the set of all large k-itemsets an output argument Algorithm 1. Apriori-Join Algorithm\(Set of Large \(k-1 Candidate k-Itemsets: Ck 1. Ck 2. for each X ? Lk-1 do  for each Y ? Lk-1 and Y ? X do  if \(X ? Y Ck = Ck ? \(X ? Y    Using the same example 1, the Apriori-Join algorithm is illustrated below  Figure 3.  Illustration of the apriori-join algorithm Algorithm 2. The Apriori Algorithm\(Itemsets: I, Transactional Dataset: D Support: s, Large Itemsets: L 1. k = 0; // scan number 2. L 3. C1 = I; // Initial set of candidate itemsets 4. repeat 210  4.1 k = k + 1 4.2 Lk 4.3 for each X ? Ck do  frequency\(X  4.4 for each t=<ID, I> ? D do 


 4.4.1 for each X ? Ck and X ? I do  frequency\(X X   4.5 for each X ? Ck do  if \(\(frequency\(X Lk = Lk ? X  4.6 Ck+1 = Apriori-Join\(Lk 4.7 L = L ? Lk until Ck+1 = ?; // repeat until no more set of candidate k-itemsets   Figure 4.  Illustration of the apriori algorithm The Apriori algorithm spends time in dealing with large itemsets. Let |I| be the size of the initial itemsets, d, |Ck| be the size of generating candidate k-itemsets Ck, n be the number of transactions in D, and k be number of scans in D We also assume k? is the k time units spent in searching k items in a transaction. The timing complexity is approximately C  d k 1 k? ? |Ck| ? n 1 C1| + 2 ? |C2| + 3 ? |C3|  +  + k ? |Ck n ? d ? 2d-1 ? ? where    d k k d k 1 d ? 2d-1. As we know that timing complexity\(d2 2d-1 thus C = O\(2d weakness in performance with the exponential growth. The 


size of the itemsets is a key factor in determining the performance of finding significant association rules in knowledge discovery. A faster algorithm is proposed in this paper to speed up the performance of the Apriori algorithm IV. THE IMPROVED APRIORI ALGORITHM The algorithm presented in this section focuses on the size of candidate itemsets, d. The algorithm distributes frequent itemsets to distributed processors. For example, in Figure 5, we have an initial itemsets I for finding association rules. The itemsets I is divided into four sub-itemsets I1, I2 I3, and I4 and assigned to four processors P1, P2, P3, and P4 dispersed in different locations. All the sub-itemsets are disjoint. Each processor is generating its own candidate kitemsets locally. Since each processor has only the local view of the complete candidate k-itemsets, it needs to broadcast its local k-itemsets to the leading processor. A processor finishing its k-itemsets earlier needs to wait until the leading processor collects all the k-itemsets from the other participating processors. The leading processor whose rank is 0 then generates a complete itemsets, finding candidate k+1 k+1 assigning each sub-\(k+1 synchronization point, each processor continues. The algorithm terminates as the candidate itemsets are empty The leading processor should have the frequent k-itemsets in it  Figure 5.  Broadcast itemsets partitions Algorithm 3. The Apriori Broadcast Itemsets Partition Algorithm\(Itemsets Ii, Transactional Dataset D, Support: s, Processors: P1, P2, P3, , Pn, Large Itemsets: L Perform in parallel at each processor Pi 1. k = 0; // scan number 2. L 3. Ci = Ii; // Initial set of candidate itemsets 4. repeat  Leading Processor 4.1 if \(process rank = = 0  4.1.1 if \(iteration = = 0 k=k+1 


4.1.2 Lk 4.1.3 for each Xi ? Ci,k+1  do  frequency\(Xi  4.1.4 for each t=<ID, Ii> ? D do  for each Xi ? Ci,k+1 and Xi ? Ii do  frequency\(Xi Xi   4.1.5 Ck+1 = Apriori-Join\(Lk 4.1.6 Lk+1  =Apriori-Prune\(Ck+1, Lk 4.1.7 divide Lk+1  into n partitions, Lk+1,1  Lk+1,2    Lk+1,n 4.1.8 send Lk+1,i  to Processi  else  4.1.9 receive Ci,k+1 from Processi 4.1.10 Lk+1  =Apriori-Prune\(Ci,k+1, Li,k 4.1.11 divide Lk+1  into n partitions, Lk+1,1  Lk+1,2    Lk+1,n 211 4.1.12 send Lk+1,i  to Processi   Non-Leading Processor 4.2 if \(process rank != 0  4.2.1 receive new Lk from Process0 4.2.2 Ci,k+1 = Apriori-Join\(Lk 4.2.3 send Ci,k+1 to Process0  until Ci,k = ?; // repeat until no more set of candidate k-itemsets  The Apriori-Prune algorithm is shown below. Based on the Apriori property that all subsets of a frequent itemset must also be frequent, we can determine a frequent kitemsets by checking if its \(k-1 Apriori-Prune algorithm employs the Apriori Broadcast Itemsets Partition algorithm to remove candidates that have a subset that is not frequent 


  Algorithm 4. Apriori-Prune Algorithm\(Set of Candidate k-Itemsets: Ck, Set of Large \(k-1 1. Lk 2. for each X ? Ck do  for each subset Y ? X and Y ? Lk-1 do  Lk = Lk ? X    The Apriori algorithm suffers the performance of finding frequent itemsets as the size of the itemsets increases. Our algorithm improves the performance by reducing the size of the itemsets. However, a fundamental issue that highly influences the performance of our algorithm is the interprocessor communications between the local processors and the leading processor. Since the global knowledge about the support of any itemset is not available at local sites, such communication overhead is needed for the leading processor to compute the global support of an itemset. Therefore, the communication overhead increases as the number of frequent itemsets and processors increase V. PARTITIONING ITEMSETS FOR WORKLOAD BALANCING AMONG PROCESSORS Recall that the Apriori algorithm spends much time on the generations of candidate itemsets. The computation time grows exponentially. Our algorithm takes the workload issue on each processor into the consideration. Suppose we have 3 processors P1, P2, and P3 and L1 = {a, b, c, d , e , f, g}. From L1, the 1-itemset {a} contributes six 2-itemsets {ab, ac, ad ae, af, ag}. The 1-itemset {f} contributes one 2-itemset {fg only. Zaki et al. [8] point out that there are several ways of partitioning itemsets among processors. A simple block partitioning and an interleaved partitioning suffer from a load imbalance problem. They propose a new partitioning scheme called bitonic partitioning for load balancing. The scheme computes the work load Wi due to the itemset i. All the Wi are sorted. The itemset with the maximum value of Wi is assigned to the least loaded processor. This scheme is better 


than the simple block partitioning and the interleaved partitioning scheme and results in almost no imbalance among processors VI. OPTIMIZED TREE DATA STRUCTURE The Apriori algorithm and its variations including parallel association mining generally use tree-based data structures for candidate itemsets generation. Three common data structures are generally adopted in them including has trees, enumeration set trees and tries [9]. All the tree structures suffer a same problem. For a very long frequent itemsets, the trees can be very large. The trees corresponding to a long itemset with the size of n will create O\(n2 the trees. We improve the tree structure from the trie [10] to the suffix tree where all internal nodes of outdegree 1 are removed, and the label of an edge is now allowed to be an itemset. Several applications of suffix trees include on-line string matching, longest repeated substring, and substring identifiers [11]. Ye and Chiang implemented a fast Apriori algorithm using the trie structure. All itemset information is accrued during traversal to the node. The application of suffix tree substantially reduces the number of internal nodes in the tree which leads to the efficient use of the memory For example, in [10], the same structure using trie now becomes more condensed using the suffix tree. Although all itemset information is not explicitly represented in the tree nevertheless the tree structure does not loose all itemset information. The following definition proves this Apriori property Definition 6.  If Y id s frequent k-itemset and if X is a subset of Y then X is a frequent |X|-itemset The following example adapted from [10] demonstrates the improvement of the data structure used in the algorithm In Figure 6, a trie used in Ye and Chiangs candidate generation algorithm and a fundamentally equivalent suffix tree used in our algorithm are presented to show the improvements  Figure 6.  Two different data structures These two tree structures are structurally equivalent. The difference is that, while within the trie, all itemset information is accrued during traversal to the itemset, in the suffix tree, all itemset information is contained at the node 


For example, {AB} is shown a frequent 2-itemset in the suffix tree. Based on Definition 6, it derives {A} and {B} are both frequent 1-itemset also. Another example, {BCD} is a frequent 3-itemset, then we can derive that {BC, BD, CD should also be a frequent 2-itemsets. This Apriori property 212 shows that the suffix tree wont loose any itemset information corresponding to its trie VII. EXPERIMENTAL EVALUATION We have discussed the performance of the Apriori algorithm in Section III. The size of the itemsets, d, plays a key role in determining the performance of the algorithm This is the main reason we present the Broadcast Itemsets Participation algorithm in this paper to improve the performance. When long itemsets are likely, the algorithm is a suitable one for discovering frequent itemsets. Of course the other algorithms could be better for another dataset Table I shows the performance of our parallel algorithm relative to the Apriori algorithm for a dataset on the number of iterations from 1 to 5. The results are plotted on the graph in Figure 7 TABLE I.  EXECUTION TIME OF THE APRIORI ALGORITHM AND OUR ALGORITHM ON THE NUMBER OF ITERATIONS FROM 1 TO 5 Time on Each Iteration Apriori Parallel Apriori gen time 1 0.516666667 0.412 prune 1 2.633333333 1.287 gen time 2 147.2666667 0.18 prune 2  N/A 5.02 gen time 3  N/A 0.27 prune 3  N/A 2.39 gen time 4  N/A 0.15 prune 4  N/A 0.23 gen time 5  N/A 0.12 prune 5  N/A 0.25  Figure 7 shows the performance of the Apriori algorithm and our parallel algorithm. Apparently, as the number of iterations increases, the performance of our algorithm performs better than the Apriori algorithm. The Apriori algorithm even cannot handle the dataset as the number of iterations is greater than or equal to 3 


 Figure 7.  Performance comparison based on the number of iterations Table II shows the actual running time of our parallel algorithm for mining association rules from T25I10D10k and T25I20D100k with respect to the datasets for the number of processes. T25I10D10K and T25I20D100K are synthetic data resembling market basket data with and were downloaded from http://miles.cnuce.cnr.it/~palmeri/datam/DCI/datasets.php. T indicates the number of items, I indicates the average length of items, and D means the size of dataset TABLE II.  EXECUTION TIME OF FROM THE TWO DATASETS Parallel Apriori Cost of Time \(? secs Number of Processes T25I10D10k.data T25I20D100k.data 1 21 513 2 18 357 3 14 271 4 12 245 5 12 218 6 11 204 7 11 189 8 10 153  The execution time for T25I10D10k and T25I20D100k in terms of the number of processes is plotted on the graph in Figure 8. As the size of a dataset increases, the execution time of the algorithm increases. The algorithm performs better if the number of processes increases from 1 to 8. Thus since the nature of data mining is to operate on a large amount of data, with the datasets become larger and larger parallel computing seems cost effective   Figure 8.  Performance comparison of T25I10D10k and T25I20D100k VIII. SUMMARY Data mining deals with huge volume data for knowledge discovery. Association rule discovery is one of the areas in data mining. The Apriori algorithm was developed in the early years of data mining. However, the Apriori algorithm has a weakness in performance.  Various variations are proposed to improve the performance of the Apriori 


algorithm. Among the existing algorithms, two major algorithms including data distribution and task distribution are using the power of distributed computing. In this paper we present an improved algorithm, Broadcast Itemsets 213 Partition, for association rule discovery. The algorithm focuses on the distribution of frequent k-itemsets, d, not on the partitions of the transactional dataset, D. Therefore, the transactional dataset may not be able to fit into main memory of each processor. The algorithm leads to a straight-forward distributed for parallel processing algorithm. The weakness of the algorithm is that it requires many synchronization points. The communication overhead may outweigh the advantage of the parallel processing. The algorithm can be extended to have the transactional dataset partitioned and distributed to the processors also. The partition of the transactional dataset allows all the partitions to be able to fit into main memory REFERENCES 1] M. Hegland, The Apriori Algorithm  a Tutorial, WSPC, Lecture Notes, Vol. 9, No. 7, pp. 1-54, March 2005 2] P. B. Cerrito, Mining the Electronic Medical Record to Examine Physician Decisions, in Advanced Computational Intelligence Paradigms in Healthcare  1, Vol. 48, pp. 113-126, Springer-Verlag 2007 3] R. Agrawal and R. Srikant, Fast algorithms for mining association rules, Proceedings of the 20th International Conference on Very Large Data Bases, pp. 487-499, 1994 4] J. Han, J. Pei, and Y. Yin, Mining frequent patterns without candidate generation, Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, pp. 1-12, 2000 5] M. H. Dunham, Y. Xiao, L. Gruenwald, and Z. Hossain, A Survey of Association Rules, Technical Report, Southern Methodist University, Department of Computer Science, Technical Report TR 00-CSE-8, 2000 6] H. Toivonen, Sampling large databases for association rules Proceedings of the 22nd International Conference on Very Large Data Bases, pp. 134-145, 1996 7] F. Bodon, A Survey on Frequent Itemset Mining, Technical Report Budapest University of Technology and Economics, 2006 8] M. J. Zaki, M. Ogihara, S. Parthasarathy, and W. Li, Parallel data mining for association rules on shared-memory multi-processors 


Proceedings of Supercomputing 96, Pittsburg, PA, pp. 17-22 November 1996 9] Ceglar and J. Roddick, Association Mining, ACM Computing Surveys, Vol. 38, No. 2, pp. 1-42, July 2006 10] Y. Ye and C.-C. Chiang, A parallel apriori algorithm for frequent itemsets mining, Proceedings of the Fourth International Conference on Software Engineering Research, Management, and Applications SERA 06 11] J. JaJa, An Introduction to Parallel Algorithms, Upper Saddle River NJ: Addison Wesley, 1996  214 


12] Dhond, Gupta, A., Vadhavkar, S. Data mining techniques for optimizing inventories for electronic commerce[C]. In the Proceeding of the ACM-SIGKDD International Conference on Knowledge Discovery and Data Mining, 2005:480-486 13]Chui-Yu Chiu , Yi-Feng Chen. An intelligent market segmentation system using k-means and particle swarm optimization[J]. Expert Systems with Applications, 2009, 36: 45584565 14]Tzung-Shi Chen , Shih-Chun Hsu. Mining frequent tree-like patterns in large datasets[J]. Data & Knowledge Engineering, 2007,62:6583 15]H. Tsukimoto, Extracting rules from trained neural networks[J]. IEEE Trans.Neural Networks, 2000, 11 \(2 156 


http://datamining.buaa.edu.cn/TopKCos.pdf 14] M. Zaki, Scalable algorithms for association mining, TKDE, vol. 12, pp. 372390, 2000 


enhance item-based collaborative filtering, in 2nd IASTED International Conference on Information and Knowledge Sharing, Scottsdale, Arizona, 2003 476 2010 10th International Conference on Intelligent Systems Design and Applications 


Basi Association Rles Basi c  Association R u les Association is basically connecting or tying up occurrences of Association is basically connecting or tying up occurrences of events Ol dib t f ilt t O n l y d escr ib e se t s o f s i mu lt aneous even t s Cannot describe patterns that iterate over time e g  itemset a  0  b  0  g    Eg If you sense higher data rates on the downlink than normal AND New Route generated Implies high chances of Intrusion AND New Route generated Implies high chances of Intrusion Associative IDS for NextGen Frameworks Dr S Dua LA Tech 20 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


