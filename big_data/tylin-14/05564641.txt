Notice of Retraction      After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles  We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper  The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org Algorithm for Classification Based on Positive and Negative Class Association Rules LuoJunwei College of Computer Science and Technology Henan Polytechnic University JiaoZuo, China ljwonly@yahoo.com.cn Abstract-The negative class association rules are important to build accurate and efficient classifiers. Despite a great deal of research, a number of challenges still exist. In order to solve the problem of "difficult to build precise classifier", the paper presents a new algorithm for classification which integrates positive class association rules and negative class association rules. The algorithm applies Apriori method and correlation between itemsets and class labels to compute all positive and negative class association rules from training dataset Moreover, a classifier will be built to predict the label of a new data object. The performance study shows that the method is highly efficient and accurate in comparison with other reported associative classification methods Keywords-Classification; Positive Class Association Rule Nositive Class Association Rule; Associative Classification I. INTRODUCTION Building accurate and efficient classifiers for large databases is one of the important data mining techniques Given a set of cases with class labels as a training dataset classification aims to build a model \(called classifier predict future data objects for which the class label is 


unknown[l Previous studies propose that associative classification has high classification accuracy and strong flexibility at handling unstructured data[2]. Until now, there are two general approaches which use association rules for classification 1 for classification directly. In this way, all association rules in the training dataset will be mined firstly, secondly these methods use the association rules to build classifiers, and lastly the classifiers use the strength of association rules or select a subset of association rules which match the data object to judge the class label of data object 2 as classification attributes. In this way, association rules will be used as classification attributes to enhance the accuracy efficiency and scalability of training dataset, and then the methods use the traditional classifier for classification However, these approaches may also suffer some weakness 978-1-4244-5540-9/10/$26.00 2010 IEEE 536 LuoHuimin School of Computer and Information Engineering Henan University Kaifeng, China hmluo henu@yahoo.com.cn On one hand, it is not easy to find interesting or useful rules. Traditional techniques always select some simple criterions such as confidence, distance functions, rough set and so on. The simple pick may affect the classification accuracy On the other hand, a training dataset often generates a huge set of rules. It is challenging to store, retrieve, prune and sort a large number of rules efficiently for classification[3 To solve these problems, in this paper, we develop a new algorithm for classification based on positive and negative association rules. Because the traditional approaches building classifiers often are based on the positive association rules, while ignoring the value of negative association rules for classification. Actually some useful 


discovery can be dug out through negative association rules from training dataset; we can find the class labels which test case does not belong to and some contradictory judgments For example, there is a data object which has some attributes, and it is consistent with the relevant association rules: X ? c, X ? - c, then the data object can be determined that the possibility 0 f it belongs to the class label c is very small Therefore, in this paper we will fmd the positive and negative association rules from training dataset and prune contradictory positive and negative association rules According to the collection of positive and negative association rules, this paper presents a new classifier to classify the data object. The final comparative experiment shows that the algorithm for classification based on positive and negative association rules has a higher recall rate and precision rate, is feasible and effective This work makes the following contributions 1 positive association rules for classification. This classifier uses a small set of high confidence positive and negative rules to determine the class label of test case, and experimental results show that this way is, in general, more accurate than other techniques 2 associative rules which is small, useful and reasonable for data object The remaining of the paper is arranged as follows. Section II revisits the general idea of associative classification Section III presents the algorithm for generating positive and negative class association and building a new classifier. The experimental results on classification accuracy and the performance study on efficiency and scalability are reported in Section IV. The paper is concluded in Section V II. ASSOCIATIVE CLASSIFICATIONS This paper assumes that the training dataset is a normal relational table, which consists of N data objects described by L distinct attributes. These N data objects have been classified into q known classes. Attributes can be categorical or continuous. For a categorical attribute, we assume that all the possible values are mapped to a set of consecutive positive integers. For a continuous attribute, we assume that 


its value range is discredited into intervals, and the intervals are also mapped to consecutive positive integers. So, we treat all the attributes uniformly in this study[4 Let D be the training dataset. In the training dataset, Let I be the set of all items in D , that means every data object has some attributes following the form I={h, i2, ... , in} and there exists a class label associated with it. Let C={cJ, C2, . . . , cn be a set of class labels. We say that a data object d ED contains X ? I , a subset of items\( called itemset association rule \(CAR where X ? I , and c; ? C . The number of data objects in D matching X and having class label c is called the support of the rule X ---+c;, denoted as sup\(X ---+cJ.The ratio of the number of objects matching X and having class label c versus the total number of objects matching X is called the confidence of the rule X ---+c;, denoted as conf\(X ---+cJ. In general, given a training dataset, the task of classification is to build a classifier from the training dataset such that it can be used to predict class labels of unknown objects with high accuracy[5 For example, if 80% of customers who have bought apples also buy oranges, i.e. , the confidence of rule apple---+aranges is 80%, then we can use the rule to classify future data objects. To avoid noise, a rule is used for classification only if it has enough support. Given a support threshold and a confidence threshold, the method finds the complete set of class-association rules passing the thresholds. When a new object comes, the classifier selects the rule which matches the data object and has the highest confidence and uses it to predict the class label of the new object In the training dataset, there also exists other class association rules: X ---+-c;, -X ---+c; and -X ---+-C;. The rule X ---+-c; means the data objects which have itemset X do not have the label C;. The rule -X ---+C; means the data objects which do not have itemset X have the label C;. The rule -X ---+-c; means the data objects which do not have itemset X do not have the label C;. These rules can be called 537 negative class association rules. The rule X ---+C; can be called positive class association rule. The support and confidence of negative class association rules can be defined 


as the support and confidence of positive class association rule[6 III. ALGORITHM BASED ON POSITIVE AND NEGATIVE CLASS ASSOCIATION RULES The algorithm in this paper consists of two phases: rule generating and classification In the first phase: the algorithm computes the complete set of positive and negative class association rules such that sup\(R R thresholds, respectively. Furthermore, the algorithm prunes some contradictory rules and only selects a subset of high quality rules for classification In the second phase: classification, for a given data object the algorithm extracts a subset of rules fund in the first phase matching the data object and predicts the class label of the data object by analyzing this subset of rules A. Generating Rules To find rules for classification, the algorithm first mines the training dataset to find the complete set of rules passing certain support and confidence thresholds. This is a typical frequent pattern or association rule mining task. The algorithm adopts Apriori method to fmd frequent itemset Apriori method is a frequent itemset mining algorithm which is fast[7]. The algorithm also uses the correlation between itemsets to find positive and negative class association rules[8]. The correlation between itemsets can be defined as corr\(X,y XuY 1 sup\(X Y X and Yare itemsets When corr Y 1, X and Yhave positive correlation When corr Y When corr\(X, Y Also when corr\(X, }j> 1, we can deduce that corr\(X, }j<1 and corr j<1 So, we can use the correlation between itemset X and class label C; to judge the class association rules When corr cJ> 1, we can deduce that there exists the positive class association rule X ---+ C When corr\(X, cJ> 1, we can deduce that there exists the negative class association rule X ---+ -c;[9 So, the first step is to generate all the frequent itemsets by 


making multiple passes over the data. In the first pass, it counts the support of individual itemsets and determines whether it is frequent. In each subsequent pass, it starts with the seed set of itemsets found to be frequent in the previous pass. It uses this seed set to generate new possibly frequent itemsets, called candidate itemsets. The actual supports for these candidate itemsets are calculated during the pass over the data. At the end of the pass, it determines which of the candidate itemsets are actually frequent[lO The algorithm of generating frequent itemsets is shown as follow Algorithm 3.1 Input: tranining dataset T, min_sup Output:frequent itemsets F I T 2 f=minsup 3 k=2;Fk_1!=NULL  Pk=CandidateGen\(Fk_1 for \(each t ET  for \(each candidate p EPJ   if \(p is contained in t the number of p Fk={p EPk I sup\(p=minsup  4 In this algorithm, there is an important function CandidateGenO which generates k-itemsets based on Fk-1 The code of it is shown as follow Algorithm 3.2 Input: Fk-1 Output: Pk J 2 allfj,h EFk_j,fj={ij, i2, .. . ik-20 ik-d,f2={h i2, . . .  ik-2 jk-d, and ik-1<A-l   p= {h i2, .. . ik-2, h-j,jk-d Pk=PkU{P 


For\(each \(k-I  if \(s!EPk-d delete pfrom Pk  3 Then, the next step is to generate positive and negative class association rules. It fIrstly fInds the rules contained in F which satisfy min_sup and min_conf threshold. Then, it will determined the rules whether belong to the set of positve class correlation rules P _ AR or the set of negative class correlation rules N AR The algorithm of generating positive and negative class association rules is shown as follow Algorithm 3.3 Input: training dataset T, min_sup, min_conf Output: P_AR, N_AR I 2 any frequent itemset X in F and Ci in C  538  if \(sup\(X---+cJ>min_sup and conf\(X---+cJ if\( corr\(X, cJ> 1   else if corr\(X, cJ<I  N_AR= N_AR U {X---+ -c  3 In this algorithm, we use Apriori method generates the set of frequent itemsets F, In F, there are some itemsets passing certain support and confidence thresholds. And the correlation between itemsets and class labels is used as an important criterion to judge whether or not the correlation rule is positve. Lastly, P_AR and N_AR are returned B. Classification After P_AR and N_AR are selected for classification, the algorithm is ready to classify new objects. Given a new data object, the algorithm collects the subset of rules matching the new object. In this section, we discuss how to determine the class label based on the subset of rules 


First, the algorithm finds all the rules matching the new object, generates PL set which includes all the positive rules from P _ AR and sorts the itemset by descending support values. The algorithm also generates NL set which includes all the negative rules from N_AR and sort the itemset by descending support values. Second, the algorithm will compare the positive rules in PL with the negative rules in NL and decides the class label of the data object The algorithm of classification is shown as follow Algorithm 3.4 Input: data object, P _AR, N_AR Output: the class label of data object Cd J P_AR 2 pL, i NL,j 3  if\(RuleCompare\(p _role, n _role  if\(P _role>n _role   Cd = the label of p_role Break if\(P _role=n _role   Cd = the label of p _role break if\(P _role<n _role  j   if\(!RuleCompare\(pJule, nJule  if\(P Jule>n Jule  Cd = the label ofpJule break   if\(P _ rule=n_ rule 


 i j  if\(P _ rule<n _rule  i  4 In the algorithm of classification, the function Sort\(P _ AR returns PL and the itemsets in PL are sorted by descending support values, the function GetElem\(pL, i rule in the set of PL. Also, we can deduce the returns of the function of Sort{N_AR IV. EXPERIMENTAL RESULTS To evaluate the accuracy and efficiency of the algorithm in this section, we report our experimental results on comparing the algorithm against the popular classification method: CBA All the experiments are performed on a 2.2GHz Core PC with 1 G main memory, running Microsoft Windows Server 2003. CBA was implemented by its authors, respectively We choose a training dataset including 1000 objects which have 12 attributes and 7 class labels In our experiments, min_confis set to 50%. For min_sup it is more complex. min_sup has a strong effect on the quality of the classifier produced. If min _sup is set too high those possible rules that cannot satisfy min_sup but with high confidences will not be included, and also the rules may fail to cover all the training cases. In the experiments reported before, we set min_sup to 6 The results are shown in Table I TABLE!. EXPERIMENT RESULTS  CBA Algorithm in this paper Rule Positve 213 120 Class Association Rule Negative Class Association 0 63 Rule Total 213 183 


Recall Ratio 92.3% 95.1 Precision Ratio 90.2% 92.9 539 As can be seen from the table, the algorithm outperforms CBA on recall ratio and precision ratio. It is clear from these objects that our algorithm produces more accurate classifiers Our Recall Ratio and Precision Ratio is higher than CBA So, it shows that the algorithm outperforms CBA in terms of average accuracy and efficiency There are two important parameters, database coverage threshold and confidence difference threshold. As discussed before, these two thresholds control the number of rules selected for classification In general, if the set of rules is too small, some effective rules may be missed. On the other hand, if the rule set is too large, the training data set may be over fit. Thus, we need to test the sensitivities of the two thresholds for classification accuracy According to our experimental results, there seems no way to pre-determine the best threshold values. Fortunately both curves are quite plain. That means the accuracy is not very sensitive to the two thresholds values V. CONCLUSIONS In this paper, we examined a number of problems that exist in current classification techniques. A new algorithm is presented to generate all positive and negative class association rules and to build an accurate classifier. The method has several distinguished features: \(1 classification is performed based on positive and negative class association rules, which leads to better overall classification accuracy; \(2 and negative class association rules effectively based on correlation between itemsets. Our experiment shows that the algorithm is highly effective at classification and has better average classification accuracy and efficiency in comparison with CBA. In our future work, we will focus on building more accurate classifiers by using more sophisticated techniques and mining more useful positive and negative class association rules REFERENCES I] Wenmin Li, Jiawei Han and Jian Pei, "CMAR: Accurate and Efficient Classification Based on Multiple Class-Association Rules 


Proceedings of the 2001 IEEE International Conference on Data Mining, IEEE Press, Dec. 2001, pp. 123-131 2] B. Liu, W.H su and Y.Ma, "Integrating classification and association rule mining", KDD'98, ACM Press, May. 1998, pp. 256-262 3] Srkant R and Agrawal R, "Mining Quantitative Association Rules in Large Relational Tables," Proceeding of the ACM SIGMOD Conference on Management of Data, ACM Press, Sep. 1996, pp. 1-12 4] Agrawal R, Imielinski T and Swami A, "Mining association rules between sets of items in large database," Proceeding of the 1993 ACM SIGMOD InternationaiConference on Management of Data ACM Press, Dec. 1993, pp. 207-216 5] T. S. Lim, W. Y. Loh, and Y. S. Shih. "A comparison of prediction accuracy, complexity, and training time of thirty-three old and new classification algorithms," Machine Learning, vol. 39, Dec. 2000, pp 1201-1211 6] K.W ang, S. Zhou, and Y.H e. "Growing decision tree on support-less association rules," The Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, A CM Press Aug. 2000, pp. 561-570 7] P.C lark and T. Niblett. "The CN2 induction algorithm," Machine Learning, vol. 3, May. 1989, pp. 261-283 8] Savasere. A , Omiecinski. E , Navathe.S, "Mining for St rong Negative Associations in a Large Database of Customer Transactions," Proceedings of IEEE 14th Internal Conference on Data Engineering, IEEE Press, Dec. 1998, pp. 369-376 540 9] Han J, Pei J, Yin YW, "Mining frequent patterns without candidate generation," DataMining and Knowledge Discovery, vol. 8, Sep 2004, pp. 53-87 10] Liu B, Ma Y and Wong K, "Improving an Association Rule Based Classifier," Proc of the 4th European Conference on Principles and Practice of Knowledge Discovery in Databases, Steween Press, Nov 2000, pp. 1 12-123 


that selecting useful clustering characters based on the rules of the observable world. However, this method has a problem of scarcity of information and lack of testifY. This paper aims to establish an association analysis model based on case study and then discover new knowledge from limited information. This paper is supposed to contribute to the name distinction and some other clustering analysis with more reliable method and theory We obtain the association rules by the experiment as followed: having same coauthor; having same workplace and years; having same publication and years The association rules selected by the experiment could be easily explained and commonsensible. Considering the association rules coming from the objective data and data mining method, they are more reliable Our future research will focus on the negative correlation between information [12] that is the local minimum points of function y=f\(x method into the feature extraction of pattern recognitions which have relative sub-information ACKNOWLEDGMENT This work was supported partly by the Special Fund for Fast Sharing of Science Paper in Net Era by CSTD under Grant No. 20096102410001 REFERENCES I] X.Yin, JHan, P.S Yu. "Object distinction Distinguishing objects with identical names," In Proc. of ICDE, pp. 1242-1246, IEEE Press 2007 2] C.C.Fabris, A.A.Freitas. "Discovering surprising patterns by detecting occurrences of Simpson's paradox," In Proc. of the Irj SeES Inti. Con! on Knowledge-Based Systems and Applied ArtifiCial Intelligence, pp. 148-160, Cambridge, UK, December 1999 3] Xiaoming Fan, Jianyong wang, Bing Lv, Lizhu Zhou, Wei Hu GHOST An Effective Graph-based Framework for Name Distinction," In Proc. of CIKM, pp. 1449-1450, ACM Press, 2008 4] A. Kulkarni and T Pedersen. "Name discrimination and email clustering using unsupervised clustering and labeling of similar contexts," In Proc of the Second Indian International Conference on Artificial Intelligence, pp. 703-722, Pune, India, December 2005 5] A.A.Freitas, "Understanding the crucial differences between classification and discovery of association rules - a position paper SIGKDD ExpIrations, 2\(1 


6] L.Feng, H.JLu, JXYu and JHan. "Minging inter-transaction associations with templates," In Proc of the 8th Inti. Cor\(. On Information and Knowledge Management, pp. 225-233, Kansas City Missouri, Nov 1999 7] E.-HHan, G.Karypis, and V.KumarMin-Apriori, "An Algorithm for Finding Association Rules in Data with Continuous Attributes http://www.cs.umn.edu/-han, 1997 8] Travers, Jeffrey, and Stanley Milgram. "An Experimental Study of the Small World Problem," SOCiometry, Vol. 32, No. 4, pp. 425-443 1969 9] de Sola Pool, !thiel, Kochen and Manfred \(1978-1 979 influence," Social Networks 1\(1 10] Stanley Milgram, "The Small World Problem," Psychology Today 1967, Vol. 2, pp. 60-67 II] Pang-Ning Tan, Michael Steinbach and Vipin Kumar, Introduction to Data Mining, Pearson Education, Inc. pp.132-133, 2006 12] X.Wu, C.Zhang and SZhang, "Mining Both Positive and Negative Association Rules," ACM Trans. on Information Systems, 22\(3 381- 405,2004 V4-289 


3 4  9 9  Suppose 3=s , according to formula \(5 of CID006, BP006  is{ }9,3,10 , as shown in table 4,  which indicates  that  e-shopper  CID006 belonged to the tenth cluster in May and moved into the third cluster in June thereafter reaching the ninth cluster in July According to formula \(6 from e-shoppers path with regard to a minimum support of 0.1 and a minimum confidence of 0.5. The association rules are as shown in table 5. The similarities  between the path of e-shopper CID016  and the derived rules are 22016 =SD and 11016 =SD . Therefore, e-shopper CID016 belongs to the ninth cluster at timeT , since the fitness between CID016 and the rules are =FD2016 0.2 and =FD 1 016 0.2001. Therefore, we predict that the products which e-shopper CID016 is likely to buy are Bread, and Biscuit  TABLE V. THE DERIVED ASSOCIATE RULES Rule 1+? sT  1?T  T  Support Confidence 1 2 3  15 16 10 10 3   10 3 1 10  


 3 9 3 4  9 9 0.3 0.1 0.1 0.2 0.1 0.1 1.0 1.0 1.0 0.5 1.0 1.0 V. CONCLUSION The preferences of e-shopper change over time. In this study, we describe a new approach for mining the changes of e-shopper  purchase behavior over time and discuss solutions to several problems. For predicting e-shoppers purchase behavior, the following concepts are proposed: BP j SDij and FDij . The SOM technique is used to detect the evolving e-shopper purchase sequences as time passes. The purchase sequences are derived from the changes in the cluster number of e-shopper. The sequential purchase patterns over user-specified minimum support and confidence are extracted by using the association rule. Then the sequential purchase patterns are stored in the rule database Finally, we give the example to elaborate the new methodology. The research presented in this paper makes a 155 contribution to mining  e-shoppers purchase behavior basing on transaction data. E-retailer may be able to perform effective  one-to-one marketing campaigns by providing individual target e-shoppers with personalized Product basing on using purchase sequences In the future, some possible extensions to this work are as 


follows. From the results of this study, we know which products target e-shoppers are likely to buy, but we have not yet explored the times at which these purchases are likely to occur. Further research analyzing e-shoppers past purchasing patterns should likewise enable prediction of the most appropriate times. Furthermore, one interesting research extension would be the setting up of a real marketing campaign, in which e-shoppers would be targeted using this methodology, which could then be evaluated with regard to its performance REFERENCES 1]Dhond, Gupta, A., Vadhavkar, S. Data mining techniques for optimizing inventories for electronic commerce[C]. In the Proceeding of the ACM-SIGKDD International Conference on Knowledge Discovery and Data Mining, 2005:480-486 2] Kuo, R. J., Chen, J. H., Hwang, Y. C. An intelligent stock trading decision support system through integration of genetic algorithm based fuzzy neural network and artificial neural network[J]. Fuzzy Sets and Systems, 2001 118\(1 3] Agrawal, D., Schorling, C. Market share forecasting: An empirical comparison of artificial neural networks and multinomial logist model Journal of Retailing[J]. 1997, 72\(4 4] Weigen, A. S., Rumelhart, D. E.Generalization by weight-elimination with application to forecasting. Advances in Neural Information Processing Systems[J]. 1999, 3:875882 5] Chen, M, S, Han, J. Data mining: an overview from a database perspective[J]. IEEE Transactions on Knowledge and Data Engineering, 2006 8\(6 6] Schafer, J. B., Konstan. E-commerce recommendation application[J Journal of Data Mining and Knowledge Discovery, 2001, 16:125153 7] Giudici, P, Passerone, G. Data mining of association structures to model e-shopper behavior. Computational Statistics and Data Analysis[J]. 2002 38:533541 8]Changchien, S. Mining association rules procedures to support on-line recommendation by e-shoppers and products fragmentation[J]. Expert Systems with Applications, 2001, 20\(4 9] Song, H, Kim, J. Mining the change of e-shopper behavior in an Internet shopping mall[J]. Expert System with Applications, 2001, 21\(3 10] Anand, S, Patrick, A. A data mining methodology for cross-sales[J Knowledge-Based Systems, 2006, 10:449-461 11] G. Adomavicius, A. Tuzbilin. Using data mining methods to build e-shopper profiles[J]. IEEE Computer, 2006, 34 \(2 


12] Dhond, Gupta, A., Vadhavkar, S. Data mining techniques for optimizing inventories for electronic commerce[C]. In the Proceeding of the ACM-SIGKDD International Conference on Knowledge Discovery and Data Mining, 2005:480-486 13]Chui-Yu Chiu , Yi-Feng Chen. An intelligent market segmentation system using k-means and particle swarm optimization[J]. Expert Systems with Applications, 2009, 36: 45584565 14]Tzung-Shi Chen , Shih-Chun Hsu. Mining frequent tree-like patterns in large datasets[J]. Data & Knowledge Engineering, 2007,62:6583 15]H. Tsukimoto, Extracting rules from trained neural networks[J]. IEEE Trans.Neural Networks, 2000, 11 \(2 156 


http://datamining.buaa.edu.cn/TopKCos.pdf 14] M. Zaki, Scalable algorithms for association mining, TKDE, vol. 12, pp. 372390, 2000 


enhance item-based collaborative filtering, in 2nd IASTED International Conference on Information and Knowledge Sharing, Scottsdale, Arizona, 2003 476 2010 10th International Conference on Intelligent Systems Design and Applications 


Basi Association Rles Basi c  Association R u les Association is basically connecting or tying up occurrences of Association is basically connecting or tying up occurrences of events Ol dib t f ilt t O n l y d escr ib e se t s o f s i mu lt aneous even t s Cannot describe patterns that iterate over time e g  itemset a  0  b  0  g    Eg If you sense higher data rates on the downlink than normal AND New Route generated Implies high chances of Intrusion AND New Route generated Implies high chances of Intrusion Associative IDS for NextGen Frameworks Dr S Dua LA Tech 20 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


