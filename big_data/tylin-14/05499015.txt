1 INTRODUCTION Rough sets theory \(RST scientist Pawlak Z. in 1982 is a new mathematical tool for fuzzy and uncertain knowledge. In this theory knowledge is regarded as partition of the Universe by defining the knowledge from a new angle of view. Knowledge is discussed by equivalence relation in algebra. RST has been successfully applied in such fields as artificial intelligence knowledge discovery, data mining, pattern recognition and fault diagnose in the recent 20 years [1-4]. RST is very suitable for data analysis because of its intrinsic characteristics At present the reduction algorithm always focuses on reducing attributes and aims at obtaining the best attributes reduction, although in practice attributes reduction is not especially important because we only need the satisfactory value reduction [5]. Potential knowledge contained in data is always targeted when we analyze the database. The complexity of the information system can be reduced by attributes reduction, although not all attribute values of each rule are not necessary in the reduced information table, so the value reduction of information or decision table is needed. Value reduction is a process of deleting all redundant values of condition attributes that have no influence on expression rule [6 It is now proved theoretically that obtaining value reduction of objects of interest is an NP-hard problem and that it is difficult to obtain minimal value reduction by enumeration. In this paper a new rough sets value reduction RSVR association rules by combining association rules mining with RST based on literatures [7,8]. Perfect reduction results of the given decision table were obtained by this  This work is supported by Natural Science Foundation of Zhejiang Province under Grant Y1080854 and National Hi-Tech Research and Development Program \(863 algorithm and the advantages can be seen by comparing this algorithm with least value reduction algorithm 2 THEORETICAL FOUNDATION There is a mature theory of rough sets via more than 20 years development and the basic concepts of RST can be 


consulted in literature [9]. This section mainly introduces concepts such as support, reduction ratio and so on Agrawal and Srikant [10] put forward Apriori algorithm, which can compress greatly candidate sets. The concepts can be defined as follows via the support concept in association rules Definition 1 In the decision table, t and s are condition and decision attributes respectively. The cardinality card\(t? s marked as sup\(t? s t is called support of t, which is marked as sup\(t Definition 2 If the support sup\(t? s satisfies sup\(t ? s t determinate rule; if the support of a determinate rule is greater than the least support minsup appointed by user then the rule is strong determinate rule The concepts of reduction ratio here come from literature [11 Definition 3 Let the number of condition attributes of the initial database be Na, the number of reduced attributes be Nc, then the attribute reduction ratio is 1 The attribute reduction ratio denotes decrease of involved factors after data reduction Definition 4 Let the initial databases number of rules be Ns, the number of reduced rules be Nr, then the rule reduction ratio is 1 The rule reduction ratio denotes decrease of rules in a given database Value reduction in rough sets based on Apriori algorithm Yuliang Ma, Zhizeng Luo School of Automation, Hangzhou Dianzi University, Hangzhou, 310018 E-mail: mayuliang@hdu.edu.cn Abstract: Aiming at value reduction, a kind of RSVR algorithm was presented based on support in association rules via Apriori algorithm. A more effective reduction table can be obtained by deleting those rules with less support according to least support---minsup. The reduction feasibility of this algorithm was achieved by reducing the given decision table Testing by UCI machine learning database and comparing this algorithm with least value reduction algorithm indicate 


the validity of RSVR algorithm Key Words: Association rules; Value reduction; Support; Apriori algorithm; Rough sets 468978-1-4244-5182-1/10/$26.00 c2010 IEEE Definition 5 Let the data quantity of initial database be N, the reduced data quantity be M, then the data reduction ratio is 1 The data reduction ratio denotes decrease of information in database 3 ALGORITHM DESIGN Based on Apriori algorithm, if the rule t? s is not strong, then the extended rule t?p? s is not strong either The reduction table is obtained by deleting the rules whose support is less than the least support minsup appointed by user The description of the algorithm is as follows Input: decision table DT, the least support minsup Output: rules set Rk Step 1: Attribute reduction for decision table Step 2: Set k as 1 Step 3: Calculate attribute support and rule support of every attribute in candidate set Ck Step 4: Delete the rules from Ck if its rule support is less than or equal to the least support minsup, transfer the rule into rule sets Rk if its attribute support is equal to rule support Step 5: Expand Ck into Ck+1. Scan Ck first, combine every two items in Ck into candidate item with k+1 attributes and insert the candidate item into Ck+1. Then check every item C in Ck+1, if an item is in k-subset of C but not in Ck, delete C; if C is antipathic, delete C too Finally obtain Ck+1 and set k as k+1 Step 6: Repeat steps 3 to 5 until Ck is empty Step 7: End 4 EXAMPLE OF ALGORITHM In the original decision Table 1, U is the concerned universe, a, b, c, d are condition attributes, e is decision attribute. The least support is minsup=1 Reduce the decision table according to the algorithm presented in the above section Attribute reduction. Only attribute a is e-omissible in the original table, so delete attribute a to form a new 


decision table Value reduction. Calculate attribute support and rule support of every attribute to form candidate set C1 with 1 condition attribute. Check every item in the new table, if rule support of an item is less than or equal to the least support minsup, delete the item from C1; if attribute support of an item is equal to rule support, transfer the item to rule set R as determinate rule Form candidate set C2 with 2 condition attributes Combine two items that have the same decision attribute in C1 to form new item with 2 condition attributes by extending C1. Then deal with them according to the above method Form candidate set C3 with 3 condition attributes When candidate set C4 with 4 condition attributes is empty, stop the algorithm At last we get the reduced decision Table 2 yielding the following rules 1 d,1 e,3 2 b,2 c,2 e,3 3 b,1 c,1 e,3 4 b,2 c,1 d,2 e,2 5 b,1 c,2 d,2 e,1 Table 1  The original decision table U a b c d e 1 1 2 1 1 3 2 1 2 2 1 3 3 1 2 1 2 2 4 1 1 1 1 3 5 2 1 2 2 1 6 2 2 2 1 3 7 2 2 2 2 3 8 2 1 2 1 3 9 2 1 1 1 3 10 3 2 2 2 3 11 3 2 1 2 2 12 3 1 1 2 3 13 3 1 1 1 3 14 3 2 2 1 3 15 3 1 2 2 1 16 3 1 2 1 3 Table 2  The reduced decision table 


U b c d e Rule support 1 ? ? 1 3 9 2 2 2 ? 3 5 3 1 1 ? 3 4 4 2 1 2 2 2 5 1 2 2 1 2 5 COMPARISON OF ALGORITHMS The 8 discrete datasets in UCI machine learning database are used to test this algorithm and the least value reduction algorithm is used for comparison. Let minsup=2 and minsup=3 in the two algorithms with the reduction results listed in Table 3 and Table 4, where only rule reduction ratio and data reduction ratio are listed because the attribute reduction of the two algorithms is the same Generally speaking, the satisfactory values of Ea, Ei Ew are Ea>30%, Ei>60%, Ew>85% respectively 2010 Chinese Control and Decision Conference 469 Table 3  Reduction results when minsup=2 Data sets Number of objects Number of attribute s RSVR algorithm Least value reduction algorithm Rule reduction ratio Data reduction ratio Runtime s Rule reduction ratio Data reduction ratio Runtime s 


monk1 124 7 87.9% 93.7% 0.011 86.3% 92.9% 0.01 monk3 122 7 77% 88.5% 0.019 81.1% 90.6% 0.011 mux6 64 7 71.9% 83.5% 0.084 84.4% 93.3% 0.004 led7 200 8 93.5% 96.1% 0.677 61.5% 68.6% 0.04 parity5+5 100 11 81% 89.6% 0.072 64% 81.7% 0.018 iris-disc 100 5 88% 93.4% 0.002 94% 96.4% 0.003 Table 4  Reduction results when minsup=3 Data sets Number of objects Number of attributes RSVR algorithm Least value reduction algorithm Rule reduction ratio Data reduction ratio Runtime s Rule reduction ratio Data reduction ratio Runtime s monk1 124 7 89.5% 94.6% 0.009 86.3% 92.9% 0.01 monk3 122 7 82% 91.3% 0.015 81.1% 90.6% 0.01 mux6 64 7 71.9% 83.5% 0.086 84.4% 93.3% 0.004 led7 200 8 94% 96.4% 0.458 61.5% 68.6% 0.056 parity5+5 100 11 88% 93.5% 0.062 64% 81.7% 0.018 iris-disc 100 5 89% 94% 0.002 94% 96.4% 0.003 470 2010 Chinese Control and Decision Conference By comparing the two algorithms, we can see from Table 3 and Table 4 that a 6 datasets in RSVR algorithm are greater than those in least value reduction when minsup=3 \(denoted by * in 


front of dataset It shows that the reduction ratio increases with improvement of minsup value. The reduction ratio of RSVR algorithm must be greater than that of least value reduction algorithm if the value of minsup is increased b users. The algorithm mainly focuses on applied system instead of on reduction ratio c longer than that of least value reduction algorithm especially when the quantity of objects and attributes is large, because RSVR algorithm adopts times-iterative method and complicated structure database 6 CONCLUSION This paper presents an RSVR algorithm based on support in association rules mining via Apriori algorithm The more effective reduction table can be obtained by deleting those rules with less support according to least support minsup. The reduction feasibility of this algorithm was achieved by reducing the given decision table Comparing this algorithm with least value reduction algorithm reveals the characteristics and advantages of RSVR. Testing by UCI machine learning database showed the validity and feasibility of this algorithm REFERENCES 1] [1] Pan J.L., Ye X.H. Wang H.X. Node Fault Diagnosis in WSN Based on the Rough set and Bayes Decision-Making Chinese Journal of Sensors and Actuators, 2009, 22\(5 734-738 2] [2] Zhang Z.Y., Yuan R.X., Yang T.Z. Rule Extraction for Power System Fault Diagnosis Based on the Combination of Rough Sets and Niche Genetic Algorithm. Transactions of China Electrotechnical Society, 2009, 24\(1 3] [3] Zhou X.S., Wang Z.M. Application of Rough Set and Neural Network in Data Mining. Computer Engineering and Applications, 2009, 45\(7 4] [4] Wang H. Customer Value Analysis Based on Rough Set and Data Mining Technique. 4th International Conference on Wireless Communications, Networking and Mobile Computing, 2008: 1-4 5] [5] Jiang W.J., Xu Y.H., Xu Y.S. Research on the Nature of Reduction to Simplifying Reduction Algorithm 


Proceedings of the Fourth International Conference on Machine Learning and Cybernatics, Guangzhou, 2005 1800-1805 6] [6] Yang Z.F., Guo J.F., Chang F. A Value Reduction Method Based on Rough Sets. Computer Engineering 2003, 29\(9 7] [7] Agrawal R., Imielinski T., Swami A. Mining Association Rules between Sets of Items in Large Databases. Proceedings of the ACM SIGMOD Conference on Management of Data. 1993: 207-216 8] [8] Lin T.Y. Rough Set Theory in Very Large Databases Proceedings of CESA96. Lille, 1996: 936-941 9] [9] Pawlak Z. Rough Sets: Theoretical Aspects of Reasoning About Data. Kluwer Academic Publishers Boston. 1991 10] [10] Agrawal R., Srikant, R. Fast Algorithms for Mining Association Rules. Proceedings of the 20th VLDB Conference, Santiago, 1994: 487-499 11] [11] Wang J., Wang R., Miao D.Q. Data Condensation Based on Rough Set Theory. Computer Transaction, 1998 21\(5 2010 Chinese Control and Decision Conference 471 


o  a1, b1}, {a1, b2}, {a1, c1 a1, c2}, {a1, d1}, {a1, d2 a2, b1}, {a2, b2}, {a2, c1 a2, c2}, {a2, d1}, {a2, d2 2-itemset a1,b1,c1},{a1,b1,b2 a1,b1,d1},{a1,b1,c1 a1,b1,c2},{a1,b1,d1 a1,b1,c1},{a1,b1,c2 3-itemset computer 3-predicate set. We sign frequent predicate set in the same way of frequent itemset. Just like using Lk to represent the set of frequent k-predicate set, C k to represent the set of Candidate k-predicate set [1 In the former, there existed many algorithms by using the idea of Apriori and its several variations [5]. One of them can mine frequent predicate set from data cube, we call it Apriori_Cube. The difference between them is calculating the degree of predicate support instead of itemset. When mining multi-dimension association rules from data cube, one predicate set is the very set of dimension members from different dimensions in data cube d1  d n | count age-1, business-1, buys-1 the supporting number of predicate set is the count storage in the cell of data cube 2.3 The Formation Of Multi-Dimension Rules With the frequent predicate set having been found out, it is easy to pick up association rules [1 1 subsets 2 set, calculate the confident degree of rules s? \(I-s word, confidence = the confident number of I/the confident number s, if confidence ? PLQFRQI WKHQ WKH rules s? \(I-s 3. The Improve Of Apriori_Cube Algorithm 3.1 The Main Idea Of Improvements First using OLAP to simplify the data cube; second improving function gen_candidate \( Lk 


do 1. The dimensions and levels of mining task are determined by users requiring when data cube is built up. Sometimes it cannot certainly find out strong association rules or it may mine many rules which users arent interested in based on the levels of dimensions. Such as dimension area, when we talk about the problems of the world, the country level will be more useful than the province level. One solution of this situation is that adjusting the levels based to the amount and proximity of the rules which have been mined. But it means mining the data cube once again and while mining multi-dimension association rules, it also cant determine the dimensions which need to be adjusted accurately at the same time. So we think that analyzing the levels of dimensions at the same time of data mining, then using operations roll-up and drill-down of OLAP to adjust the levels, this is called On-Line Analysis Mining, at last carrying on mining process which will be more propriety and efficient Assume that users make an n-dimensions data cube and always hope the multi-dimension association rules mined can contain these n dimensions, on the contrary they arent interested in the rules containing only \(n-1 dimensions or even less [6]. In a word, it must find out the frequent n-predicate set. According to Apriori, every 1 subset of frequent n-predicate set must be frequent 1-set That means every dimension has a frequent 1-predicate set Obviously saying, once if some dimension doesnt contain a frequent 1-predicate set, the frequent n-predicate set must not exist So we check the frequent 1-predicate set after mining them from every dimension by algorithm: we can know that it shows the partition level of dimension we made is too low if some dimension doesnt have frequent 1-predicate set so we should raise the level of this dimension by roll-up; in the opposite, if every 1-predicate set of some dimension is frequent 1-predicate set, we should drill-down to drop the level of this dimension [3]. All of these adjustments will be embedded in mining process in order to strengthen the flexibility and targeting 2. In recent years, we also meet many problems like this when mining rules, most quantitative dimensions can be 


scattered, for example: dimension deposit can be parted in the form of interval as [0, 20K], [21k, 30k]and so on This discretization is commonly completed before mining 1]. So we can easily know that [0,20k], [21k, 30k] and the other intervals which belong to one dimension cant exist at the same time [4]. But there are still some special situations such as year which is parted by quarter level\(Q1,Q2,Q3,Q4 problem what about the sales in both quarter Q1 and Q2, so when we improve the algorithm we must think about it too First of all, it is obviously that when mining multi-dimension association rules, one predicate set contains no more than one different level from the same dimension [5] without special requirements of users Combine this point with Apriori_Cube algorithm can help reduce the amount of useless predicate set, especially with the great growing of the amount Example 1 There are 4 dimensions of the database: A, B, C D, when the discretization of them has been complete, we can built a table of them like Table 1 A B C D a1 b1 c1 d1 a2 b2 c2 d2 Table 1:Examp1       Graphic1: Connection of example 1 When the 3-itemset come out, itemset {a1, b1, b2 must be contained, according we discussed before it is obviously unsubstantiated. We will use one predicate set contains no more than one different level from the same dimension to do the first pruning, so that this condition will not appear ever Second, when users make special requirements, we also should do some improvements. Such as the example before, when the algorithm discovered that one predicate set contains different levels from the same dimension as {a1 b1, b2} and checked out that this is what the users are interested in, so we will calculate the sum of the counts of a1, b1} and {a1, b2}, then assign it to {a1, b1, b2} and keep it in the frequent 3-itemset. We call this situation SP The algorithm improved also has steps connection and pruning, but the difference is that we use one predicate set contains no more than one different level from the same dimension and the SP to do the first pruning in function 


gen_candidate \( Lk second pruning by using minsup o  B4 B3 B2 B1 To sum up the above arguments, we put forward the improved algorithm 3.2 Apriori_Cube_Improved Algorithm Algorithm: Apriori_Cube_Improved Input: N-Dimension Data Cube \( d1  d n | count support: minsup Output: The set of frequent predicate set L Process 1 2 i=1n up the candidate 1-predicate set and we use symbol C1 to represent the set of them 3 C1 4 1-predicate set of every one dimension 5 1 2 3 6 1 2 2 3 4 5 the new improvement of step 1 7 8 k ?Q\fDQG Lk ??\f'R 1 Lk 1 The new function will be expanded below 2 C k 3 4 9 New Function gen_candidate_improved \( Lk 1 1 2 


Lk 1 Do 1 there exists k-2 items in the same between I1 and I 2 a If there exits requirements of the users Then sup_count of C go to b b C k = C k ?c; Else delete c 3 Function gen_frequent \( C k 1 2 I1  I k come from the set do 1 n-dimension data cube\( I1  I k Support=sup_count/total_count 2 3 4. Performance Analysis Example 2 Lets talk about a practical problem just like the status of sales. Assume that we will mine the association rules involved 4 dimension attributes of sales, the minsup=25%. First of all, using OLAP technology to build a 4-D data cube and the 4 dimension attributes are: time location, item, and supplier. For location dimension which contains area, country and so on, we choose province level We use brand level for item dimension, company level for supplier dimension. Time dimension can be divided as Q1 1-3 4-6 7-9 10-12 location\(P1,P2,L1,L2 York, item\(B1,B2,B3,B4 C1,C2,C3 sales data cube can be generalized like this Graphic 2: The 4-Dimension Data Cube of Sales The details of this sales data cube are in the table follow: The amount of cells is 100 Location Time Item Supplier Count Cell-1 P1 Q1 B1 C2 5 Cell-2 P1 Q3 B1 C1 2 Cell-99 L1 Q3 B1 C1 3 Cell-100 L2 Q4 B1 C3 11 Table 2: The details data table of sales data cube We use original Apriori_Cube Algorithm to find 


frequent predicate set with minsup_num= 25%*100=25 According the data table we calculate that sup_count of every member of dimension L is \(P1:8, P2:5, L1:1, L2:24 and also T, I, S. So there is the process Graphic 3: The processes of old algorithm As we know, through comparing with the minsup_num dimension location has no one frequent 1-predicate set, so that there have no frequent 4-predicate set in the output by the original algorithm. But users are interested in the Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate set T I S Q1 Q3 B2 B3 C1 C2 I1 I 2 C1              C2                 C3 2 12 1 3 5 1 


6 2 12 8 11 Q 1  Q 2  Q 3  Q 4 P 1 P 2 L 1 L 2 Candidate 2-Predicate set Q1,B2},{Q1,B3 Q1,C1},{Q1,C2 Q3,B2},{Q3,B3 Q3,C1},{Q3,C2 Candidate 3-Predicate set Q1,B2,B3}{Q 1,Q3,B2},{Q3 C1,B2 Frequent 2-Predicate set Q1,B2}{Q1,B 3},{Q3,B2 Q3,C1 Output: 1L U 2L U 3L Frequent 3-Predicate set Q1,B2,C1},{Q1,B3,B2 Q1,B2,B3  B4 B3 B2 B1 To sum up the above arguments, we put forward the improved algorithm 3.2 Apriori_Cube_Improved Algorithm Algorithm: Apriori_Cube_Improved Input: N-Dimension Data Cube \( d1  d n | count support: minsup 


Output: The set of frequent predicate set L Process 1 2 i=1n up the candidate 1-predicate set and we use symbol C1 to represent the set of them 3 C1 4 1-predicate set of every one dimension 5 1 2 3 6 1 2 2 3 4 5 the new improvement of step 1 7 8 k ?Q\fDQG Lk ??\f'R 1 Lk 1 The new function will be expanded below 2 C k 3 4 9 New Function gen_candidate_improved \( Lk 1 1 2 Lk 1 Do 1 there exists k-2 items in the same between I1 and I 2 a If there exits requirements of the users Then sup_count of C go to b b C k = C k ?c; Else delete c 3 Function gen_frequent \( C k 1 2 I1  I k come from the set do 1 


n-dimension data cube\( I1  I k Support=sup_count/total_count 2 3 4. Performance Analysis Example 2 Lets talk about a practical problem just like the status of sales. Assume that we will mine the association rules involved 4 dimension attributes of sales, the minsup=25%. First of all, using OLAP technology to build a 4-D data cube and the 4 dimension attributes are: time location, item, and supplier. For location dimension which contains area, country and so on, we choose province level We use brand level for item dimension, company level for supplier dimension. Time dimension can be divided as Q1 1-3 4-6 7-9 10-12 location\(P1,P2,L1,L2 York, item\(B1,B2,B3,B4 C1,C2,C3 sales data cube can be generalized like this Graphic 2: The 4-Dimension Data Cube of Sales The details of this sales data cube are in the table follow: The amount of cells is 100 Location Time Item Supplier Count Cell-1 P1 Q1 B1 C2 5 Cell-2 P1 Q3 B1 C1 2 Cell-99 L1 Q3 B1 C1 3 Cell-100 L2 Q4 B1 C3 11 Table 2: The details data table of sales data cube We use original Apriori_Cube Algorithm to find frequent predicate set with minsup_num= 25%*100=25 According the data table we calculate that sup_count of every member of dimension L is \(P1:8, P2:5, L1:1, L2:24 and also T, I, S. So there is the process Graphic 3: The processes of old algorithm As we know, through comparing with the minsup_num dimension location has no one frequent 1-predicate set, so that there have no frequent 4-predicate set in the output by the original algorithm. But users are interested in the Candidate 1-Predicate set L T I S P1 P2 L1 


L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate set T I S Q1 Q3 B2 B3 C1 C2 I1 I 2 C1              C2                 C3 2 12 1 3 5 1 6 2 12 8 11 Q 1  Q 2  Q 3  Q 4 P 1 P 2 L 1 L 2 Candidate 2-Predicate set Q1,B2},{Q1,B3 Q1,C1},{Q1,C2 Q3,B2},{Q3,B3 


Q3,C1},{Q3,C2 Candidate 3-Predicate set Q1,B2,B3}{Q 1,Q3,B2},{Q3 C1,B2 Frequent 2-Predicate set Q1,B2}{Q1,B 3},{Q3,B2 Q3,C1 Output: 1L U 2L U 3L Frequent 3-Predicate set Q1,B2,C1},{Q1,B3,B2 Q1,B2,B3  6 level from the same dimension and SP to pruning Candidate 3 Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special requirement on location B by users 7 1 max_support of every dimension 1 frequent 4-predicate set which contains location dimension And the candidate 3-predicate set {Q1,Q3,B2} is useless because when we mine multi-dimension association rules we should follow that one predicate set contains no more than one different level from the same dimension even the sup_count >minsup_num when there are no requirements from users. But this time, users require that different members from dimension item can exist at the same time So we redo this mining process by the improved Algorithm Graphic 4: The processes of improved algorithm At last, we got the frequent 4-predicate set which the users are interested in, and then can format the rules 5. Conclusion In this paper, we have studied issues and methods on efficient mining of multi-dimension association rules based on data cube. With the development of technology, the size 


of database has become much huge than ever before unimaginable. So the multi-dimension model of database based on data cube has become useful and popular relatively how to mine useful multi-dimension association rules based on this model has been important too. Among the algorithms for the problems we choose the most classify one to improve, to make it more efficient and useful An efficient algorithm, Apriori_Cube_Improved, has been developed which explores the multi-dimension association rules. First, we use max_support and min_support of every frequent 1-predicate set to check the levels of dimensions, and then adjust the data cube by roll-up and drill-down operation immediately. This step is embed in the process of mining association rules, so it makes the rules much more useful and flexible; second applying one predicate set contains no more than one different level from the same dimension and SP pruning can help reduce the amount of predicate set, especially with the great growing of the number. So the speed of algorithm improved is faster than ever At last, there are also many other interesting issues and flaws of the algorithm which call for further study including efficient mining multi-dimension association rules of complex measures and so on. Finally, I should thank all the people who have give me help for this paper Reference 1] Jiawei Han, Micheline Kamber. Data Mining Concepts and Techniques. China Machine Press, 2007 2] Agrawal R, Imielinski T and Swami A. Mining association rules between sets of items in large database. Proc. of the ACM SIGMOD Conf., Washington DC, 1993 3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional Association Rule Mining Method Based on Data Cube Computer Engineering, China,2003 4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research Multi-dimensional Association Rule Ming Based on Apriori Science Technology and Engineering, China, 2009 5] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proc. of the 20th Int.Conf. on VLDB Santiago, Chile, 1994 6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang Mining Multi-Dimension Constrained Gradients in Data 


Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy 2001 7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE Transactions on Knowledge and Data Engineering, 2000 Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Candidate 1-Predicate set L T I S P L Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate L T I S P L 


Q1 Q3 B2 B3 C1 C2 Frequent 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3 Output L1 U L2 U L3 U L4 The Dimension need to be adjust Dimension: Location Operation: Roll-up to increase the level of this dimension Result P\(P1,P2 L1,L2 2 3 2 3 The Features Of Every 1-Predicate Set Location: min_support=13 max_support=25 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 4 5 4 max_support of every dimension 5 minsup_num=25, and no one has to be adjusted Frequent 3-Predicate set Q3,C1,B2},{Q1,B2,L Q1,B3,L},{Q3,P,B2 Candidate 4-Predicate set 


Q3,C1,B2,P},{Q1,B2,B3,L}\(7 The Features of Every 1-Predicate Set Location: min_support=1 max_support=24 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 Candidate 2-Predicate set Q1,B2},{Q1,B3},{Q1,C1 Q1,C2},{Q3,B2},{Q3,B3 Q3,C1},{Q3,C2},{P,Q1 P,Q3},{P,B2},{P,B3},{P,C1 P,C2},{L,Q1},{L,Q3 L,B2},{L,B3},{L,C1},{L,C2 Frequent 2-Predicate set Q1,B2}{Q1,B3},{Q3,B2 Q3,C1},{P,Q3},{L,Q1 6 Candidate 3 -Predicate set Q3,C1,B2},{Q3,C1,P},{Q1,B 2,L},{Q1,B3,L},{Q3,P,B2},{Q 1,B2,B3  6 level from the same dimension and SP to pruning Candidate 3 Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special requirement on location B by users 7 1 max_support of every dimension 1 frequent 4-predicate set which contains location dimension And the candidate 3-predicate set {Q1,Q3,B2} is useless because when we mine multi-dimension association rules we should follow that one predicate set contains no more than one different level from the same dimension even the sup_count >minsup_num when there are no requirements from users. But this time, users require that different 


members from dimension item can exist at the same time So we redo this mining process by the improved Algorithm Graphic 4: The processes of improved algorithm At last, we got the frequent 4-predicate set which the users are interested in, and then can format the rules 5. Conclusion In this paper, we have studied issues and methods on efficient mining of multi-dimension association rules based on data cube. With the development of technology, the size of database has become much huge than ever before unimaginable. So the multi-dimension model of database based on data cube has become useful and popular relatively how to mine useful multi-dimension association rules based on this model has been important too. Among the algorithms for the problems we choose the most classify one to improve, to make it more efficient and useful An efficient algorithm, Apriori_Cube_Improved, has been developed which explores the multi-dimension association rules. First, we use max_support and min_support of every frequent 1-predicate set to check the levels of dimensions, and then adjust the data cube by roll-up and drill-down operation immediately. This step is embed in the process of mining association rules, so it makes the rules much more useful and flexible; second applying one predicate set contains no more than one different level from the same dimension and SP pruning can help reduce the amount of predicate set, especially with the great growing of the number. So the speed of algorithm improved is faster than ever At last, there are also many other interesting issues and flaws of the algorithm which call for further study including efficient mining multi-dimension association rules of complex measures and so on. Finally, I should thank all the people who have give me help for this paper Reference 1] Jiawei Han, Micheline Kamber. Data Mining Concepts and Techniques. China Machine Press, 2007 2] Agrawal R, Imielinski T and Swami A. Mining association rules between sets of items in large database. Proc. of the ACM SIGMOD Conf., Washington DC, 1993 3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional Association Rule Mining Method Based on Data Cube 


Computer Engineering, China,2003 4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research Multi-dimensional Association Rule Ming Based on Apriori Science Technology and Engineering, China, 2009 5] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proc. of the 20th Int.Conf. on VLDB Santiago, Chile, 1994 6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang Mining Multi-Dimension Constrained Gradients in Data Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy 2001 7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE Transactions on Knowledge and Data Engineering, 2000 Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Candidate 1-Predicate set L T I S P L Q1 Q2 Q3 Q4 B1 B2 


B3 B4 C1 C2 C3 Frequent 1-Predicate L T I S P L Q1 Q3 B2 B3 C1 C2 Frequent 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3 Output L1 U L2 U L3 U L4 The Dimension need to be adjust Dimension: Location Operation: Roll-up to increase the level of this dimension Result P\(P1,P2 L1,L2 2 3 2 3 The Features Of Every 1-Predicate Set Location: min_support=13 max_support=25 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 4 5 


4 max_support of every dimension 5 minsup_num=25, and no one has to be adjusted Frequent 3-Predicate set Q3,C1,B2},{Q1,B2,L Q1,B3,L},{Q3,P,B2 Candidate 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3,L}\(7 The Features of Every 1-Predicate Set Location: min_support=1 max_support=24 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 Candidate 2-Predicate set Q1,B2},{Q1,B3},{Q1,C1 Q1,C2},{Q3,B2},{Q3,B3 Q3,C1},{Q3,C2},{P,Q1 P,Q3},{P,B2},{P,B3},{P,C1 P,C2},{L,Q1},{L,Q3 L,B2},{L,B3},{L,C1},{L,C2 Frequent 2-Predicate set Q1,B2}{Q1,B3},{Q3,B2 Q3,C1},{P,Q3},{L,Q1 6 Candidate 3 -Predicate set Q3,C1,B2},{Q3,C1,P},{Q1,B 2,L},{Q1,B3,L},{Q3,P,B2},{Q 1,B2,B3  


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


