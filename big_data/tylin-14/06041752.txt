Research of Mining Algorithm based on  1NF Strongly Correlated Item Pair Luo Mingying University of Electronic Science and Technology of China School of Computer Science and Engineering Chengdu, China Library Xichang College Xichang, China Abstract In order to reduce the computation cost of candidate pairs, we have developed the Taper algorithm according to 1NF property. The developed TaperR algorithm can cut the number of candidate pairs to improve efficiency Experimental results exhibit that the new algorithm is wellworked in the mining of all-strong-pairs. So it is more suitable for real relation database system Index Terms - Data Mining; Association Rules; Top-K Strongly Correlated Item Pair; Multidimensional Structured Database I I 
NTRODUCTION Association rules in data mining is hot issue. At present, the framework of mining association rules is based on support, so that will not only lead to a real correlation between the data Correlation\ failed to be found, and in the resulting rules are also many do not have real relevance, it is an inescapable fact Therefore, in order to make up for lack of association rules more and more researchers have begun using statistical correlation methods. In recent years, people are strongly related to projects \(in the transaction database with statistical significance\f the excavation to give a certain attention Strong problem of mining-related projects should be relevant in a given minimum threshold and be the premise of mining transaction databases, to identify all meet the Pearson's 
correlation coefficients of not less than the project right Xiong et al [1 r o p os ed the Tap er alg o rithm can  effectively meet the conditions of strong mining-related projects right. Pearson correlation coefficient based on the technology sector, pruning is the key of the algorithm. From the literature [2 Th e an aly s i s an d t es t r esu l t s can be seen  Taper pruning algorithm used to filter out a large number of candidate item pairs. However, if the number of projects and transactions are very large, then even after the reduction still need to test very high computational cost of projects that the remaining candidates, extreme cases, lead to memory is not enough, the algorithm can not run properly completed Since a large number of business data is stored in a relational database, while most of the data mining applications 
are for a relational database, so strong in the relational database research project related to mining, its theoretical value and practical significance are very significant. This paper studies the strong relational database on the problem of mining-related projects, Pearson correlation coefficient for the relevant measure, with a statistical correlation instead of the traditional association rules, and then design efficient mining algorithms 3  II P EARSON CORRELATION COEFFICIENT AND T APER ALGORITHM From view of statistical point, the strength of association between variables can be used to describe the correlation measure. For discrete variables, the Pearson correlation coefficient indicated a relationship between variables. Pearson 
correlation coefficient is the correlation coefficient of binary variable in a special form[4-6 Suppose that two binary variables A and B, the distribution of their values shown in Table 1 Tab.1 The distributing figure of variables’value B 0 1 Line sum 0 P\(00\ P\(01\ P\(0 A 1 P\(10\ P\(11\ P\(1 Column sum P\(+0 P\(+1 N 
Then, correlation coefficient is calculated as  1   0   1   0   10   01   11   00       002 002 002 002 003 004 P P P P 
P P P P 005 Among them, that   ij P meet the A == i & & B == j conditions the number of objects \(i = 0,1; j = 0,1\In addition   002 i P meet the A == i said the number of objects \(regardless of the values of B\, that satisfy B == j the number of objects regardless of the values of A\. So, the relationship between   002 i P    j P 002 and   ij P can be expressed as follows 006 004 002 004 1 0 
    j ij i P P and 006 004 002 004 1 0     i ij j P P N is the total number of all objects, that is  1   0   1   0  002 002 002 002 002 004 002 004 P P P P N  
2011 International Conference on Future Computer Science and Education 978-0-7695-4533-2/11 $26.00 © 2011 IEEE DOI 10.1109/ICFCSE.2011.135 530 
2011 International Conference on Future Computer Science and Education 978-0-7695-4533-2/11 $26.00 © 2011 IEEE DOI 10.1109/ICFCSE.2011.135 530 
2011 International Conference on Future Computer Science and Education 978-0-7695-4533-2/11 $26.00 © 2011 IEEE DOI 10.1109/ICFCSE.2011.135 530 


On the formula of correlation coefficient 005 further deduced  1   0   1   0   10   01   11   00       002 002 002 002 003 004 P P P P P P P P 005  1   0   1   0   10   01   11   11   10   01         002 002 002 002 003 003 003 003 004 P P P P P P P P P P N  1   0   1   0   11   01   10   11   11           002 002 002 002 002 002 003 004 P P P P P P P P P N  1   0   1   0   1   1   11       002 002 002 002 002 002 003 004 P P P P P P P N N P N P N P N P N P N P N P  1   0   1   0   1   1   11      002 002 002 002 002 002 003 004 So, if you use the degree of support for mining association rules of representation, then the item \(set\ A, B and AB respectively, the degree of support can sup \(A\, sup \(B\d sup \(A, B\ indicate, then N P A  1   sup 002 004  N P B  1   sup 002 004  N P B A  11    sup 004 Re-use N P A  1   sup 002 004 formula to substite, end up, for the transaction database, A and B on the two projects, with the coefficient can be calculated form the formula \(1  sup 1    sup 1    sup   sup  sup   sup   sup B A B A B A B A 003 003 003 004 005 1 Which, sup \(A\, sup \(B\ and sup \(A, B\ denote the item \(set A, B and AB of support In addition, the set {A, B} is a given project, sup \(A\is the A of the support, sup \(B\is B's support, we may assume that sup A\so, because sup B\= sup \(A, B\, therefore according to the formula \(1\, the project of {A, B} of the correlation coefficient can be the upper bound of the following formula  sup 1    sup 1    sup   sup  sup   sup  sup      B A B A B A B upper B A 003 003 003 004 005 The above equation can be further pushed and simplification  sup 1    sup 1    sup   sup  sup 1    sup B A B A A B 003 003 003 004  sup 1  sup 1   sup  sup B A A B 003 003 004 Therefore, the upper bound      B A upper 005 of the correlation coefficient 005 of  the project {A, B}  can finally be expressed as equation \(2  sup 1  sup 1   sup  sup      B A A B upper B A 003 003 004 005 2 By the formula \(2\d the process, the upper bound      B A upper 005 of the correlation coefficient 005 of  the project A, B}  is only with A, B related to their support; other words the calculation of {A, B} of the correlation coefficient 005 does not need to know the upper bound of the project on the {A, B of the support. Therefore, in the Taper algorithm, that can not meet the conditions of the project can use this upper bound to filter, which has been pr uned out of the pr oject no longer need to calculate the degree of support, the efficiency of the algorithm can be improved III NORMALIZATION THEORY ON RELATION DATABASE Relational database is one of the common database structure. Relational database normalization theory in 1971 first proposed by Dr. EFCodd. Since then, the theory can be continuously deepened and improved. Normalized relational schema design theory as the theoretical guide and a powerful tool, but also other data model database based on the logical design theory, and in the relational database for data mining provides a useful constraint information Let relational schema R, X, Y, if r 007 Ins \(R\d any two tuples u, vr, by u   v X   i n troduced u Y v Y  call ed XY is a function dependent on R, say X or Y function depends on the X function to determine Y Different types of paradigms can be used to define the concept of functional dependency, including 2NF, 3NF and BCNF. As we all know, the first paradigm is that each relationship must meet the most important paradigm. It is defined as If a domain unit is indivisible unit, then the field is atomic A relational schema R if all the attributes of the domain are atomic, we call the relational schema R is the first paradigm 1NF In association mining, the first important application of a paradigm is: a project of the two projects can not be from the same property. Thus, we can use this property to reduce the mining process the number of candidate projects, the efficiency of the algorithm can be improved 
531 
531 
531 


IV MINING ALGORITHM B ASED ON THE STRONG CORRELATION 1NF T APER R ALGORITHM To further reduce the relational database project on the candidate's test cost, we use the nature of 1NF improved Taper algorithm, designed to improve the TaperR algorithms, data mining process to reduce the number of candidate projects, to increase the efficiency of the algorithm [4  First, we introduce the subsequent chapters will use the symbols and concepts Let A1, ..., Am are, respectively, the domain D1, ..., Dm on the property. Order data set D = {X1, X2, ..., Xn} is a collection of tuples. Attribute Ai of the set of attribute values expressed by Vi, the property values of Ai on the number of different attribute i p is used ie   i i V p 004 Require special attention, where the definition data set D and the traditional relationship between the tables are equivalent. The general meaning has been more clearly, we are no longer given special emphasis and distinction TaperR algorithm is composed of two steps: candidate generation and pruning project. In the production process of the candidate projects, the use of the nature of 1NF project to reduce the number of candidates; in the pruning process, using the upper bound to filter out those who can not meet the conditions of the project, so that pruning out of the project's support the calculation of avoided costs First, according to Theorem 4 Theorem to 1 and a series of theorems, starting with the advantages of the algorithm is proved theoretically TaperR Theorem 1: Algorithm for pruning the original Taper produce a candidate 2  1     1 1 003 006 006 004 004 m i i m i i p p before the process of the project right Proof: In a relational table, a total 006 004 m i i p 1 items, Taper algorithm produces candidate operation by linking the project right By Theorem 1 we can see, the original Taper algorithm will not take into account the special nature of relations between tables, it generates all possible combinations Theorem 2: Before pruning, TaperR algorithm generates a candidate item pairs   2  1    2  1      1 1 1 006 006 006 004 004 004 003 003 003 m i i i m i i m i i p p p p  Proof: Since there is a project 006 004 003 m i i i p p 1  2  1    that does not meet the 1NF, so TaperR algorithm does not produce these items right. Therefore, TaperR pruning algorithm produced only candidate before the project   2  1    2  1      1 1 1 006 006 006 004 004 004 003 003 003 m i i i m i i m i i p p p p  By Theorem 1 and Theorem 2 shows that, in the generation of candidate projects on this step, TaperR algorithm to produce 006 004 003 m i i i p p 1  2  1    less than the Taper algorithm had a project, they are not in a relational database on the right Number of projects to reduce the size of Ai by the property attribute values on the number i p of different decisions. If the relational data table contains a lot of property and the number of attribute values are more than the Taper algorithm then TaperR algorithm with more obvious advantages In addition, the relational database is not the correct item on the correlation coefficient may not be able to use 005 the upper bound pruning process is cut off, therefore, computational cost increase. In other words, the original Taper Algorithm in the process of pruning algorithm than TaperR need to see more projects, such as the theorems 3 Theorem 3: Let Taper algorithm and TaperR algorithm is not cut out of the number of projects \(related to a given minimum threshold 010 d, respectively 002 002 010 1 R and 002 002 010 2 R the representative 006 004 003 m i i i p p 1  2  1    of a relational database that is not the correct item has not been lost on the the number of projects on, there Proof: Let Taper algorithm and TaperR algorithm before pruning process of the project that were used for the collection and said that according to Theorem 1 and 2, there is a relational database and the project is not set correctly. Thus, R1 \(\ = R2 R3 \(\ was established proof of the theorem Theorem 3 shows that, Taper algorithm not only in the process of pruning redundant checks a project must also scan the database more checks in the items right. Therefore, 1NF nature of the algorithm makes TaperR pruning and scan the database in two steps in both the lower computational cost which for large data sets, data mining is particularly important Pruning algorithm for TaperR upper bound operation those who can not meet the conditions of the project on this upper bound can be filtered out of the project to avoid pruning the support of the calculation of cost efficiency of the algorithm can be improved. However, this upper bound is always greater than 0, it is easy to see from the formula. So, if the minimum threshold value related to very small, such as 0.01, then by pruning out the number of candidate projects will be very small, so the upper bound on the effect of pruning techniques will become very poor Therefore, for this problem, through the use of the special structure of relational tables, we further reduce the number of candidate projects. The basic idea is described below May wish to set Vi = {u1, u2, ..., up} and Vj = {v1, v2 vq} attributes Ai and Aj are the property of values. These two properties can generate p * q project right. However, this p * q is not a project are necessary, because sup \(uk\ = sup \(uk v1 
532 
532 
532 


sup \(uk v2\+ ... + sup \(uk vq\ sup 003 Or, sup \(uk vq\ = sup \(uk sup \(uk vq-1 003  003 sup \(uk v2 003 uk v1\ other words, can uk v2\ sup \(uk vq-1\export support sup \(uk vq\. Furthermore, the project contains vq of support without the need for scanning the database directly in the process of calculation. Therefore, in scanning the  1 project 003 q 003 database, just calculate \(p  right Optimization effect of the technology is given by Theorem 4 Theorem 4: In TaperR algorithm, assuming that does not use pruning techniques, then scan the database for the process we need to check the items up on the support Proof: For each property's attributes Ai and Aj, the use of this technology, = degree of support for a project do not have to scan the database in the process of inspection. Consider all the attributes, a total project right. According to Theorem 3-2 TaperR algorithm to generate a project before the cut right Therefore, the maximum degree of support items required for the database in the process of scanning checks Theorem00204 shows that, even in technology-based pruning effect on the community is not an ideal situation, the new algorithm can still effectively reduce the number of projects. That is, even in the smallest little relevance threshold when, TaperR algorithm can still be effective on the excavation project[8 In summary, TaperR algorithm described in Figure 2.1, the overall process can be used. Algorithm contains three steps: 1 generate candidate projects; 2\ pruning of candidate projects; 3 Scan the database to get the final result V E XPERIMENTAL RESULTS We use TaperR algorithm to test on real data sets and related results are given in the algorithm execution time and number of projects on the two dimensions compared with the original Taper algorithm. The two data sets from the experimental point of view the experimental results prove the validity and superiority Taper mentioned in this section are algorithms and TaperR through Java programming. Experiment operating environment are a Pentium4-2.4G, 512 M RAM running Windows 2000 Professional PC A Experimental Data Sets In the experiment, two data sets used in this article Mushrooms \(Mushroom\ data set and soybean \(Soybean\ data sets, are from the well-known UCI database \(Http www.ics.uci.edu/ ~ mlearn / MLRR epository . html\owing a brief overview of the two data sets soybean \(Soybean\ data set: In 011 this data set, each record is a sample of soybeans, 35 feature attributes; the same time each record has a class label, marked as 4 in one disease namely: Diaporthe Stem Canker, Charcoal Rot, Rhizoctonia Root Rot, and Phytophthora Rot. This data set 47 records including records of Phytophthora Rot, 17, and the remaining 10 diseases have a record of each mushrooms \(Mushroom\ata set 011 In this data set, each record is a sample of mushrooms, there are 22 characteristics of properties, including color, shape, skin, etc.; the same time each record has a class label, that is toxic mushrooms is nontoxic. This data set a total of 8124 records, the number of toxic and non-toxic mushrooms 4208 and 3916, respectively B Experimental Results First, we observe the algorithm associated with the minimum threshold for a given increase in the project need to check the number of changes Figure 1 and Figure 2 are given in the mushrooms Mushroom\ data set and soybean \(Soybean\data sets, Taper algorithm and TaperR algorithm associated with the minimum threshold for a given increase in the project need to check the number of comparison. Consistent with previous theoretical proof, Taper algorithm always need to check the project for more than TaperR algorithm. Therefore, the superiority of this algorithm is proved by the experiment Figure.1 Comparison of the number of items desired to be checked in Mushroom dataset Figure. 2 Comparison of the number of items desired to be checked in Soybean dataset These results show that the algorithm designed TaperR relational database on the strong correlation of the excavation project, showing good results, more suitable for practical application of relational database systems VI C ONCLUSION Since a large number of business data stored in relational databases, and most of the data mining is also against the relational database, so strong in the relational database on the project of mining-related research, its theoretical value and practical significance are very significant. This paper studies the strong relational database on the problem of mining-related projects, Pearson correlation coefficient for the relevant measure, with a statistical correlation instead of the traditional association rules, and then design an efficient mining algorithm Theoretical analysis and experimental results show that the proposed algorithm in a relational database TaperR be strong on the mining-related projects on time, showing good results 
533 
533 
533 


and various aspects are better than existing similar Taper algorithm R EFERENCES 1 A s hraf  Abaz ee d, A li Mam a t, Mohm d Nas ir, e t al. Min ing A s s o cia tion Rules from Structured XML data. 2009 International Conference on Electrical Engineering and Inform atics. Selangor, Malaysia, 2009: 376379  QI N Li-ping BAI Mei. Predictin g Tren d  in Futures Prices Tim e Series Using a New Association Rules Algorithm. 2009 International Conference on Management Science & Engineering. Moscow, Russia 2009: 1511-1517 3 Z. Yan g, W H Ta ng A Shintem irov e t al As s o ciatio n R u le MiningBased Dissolved Gas Analysis for Fault Diagnosis of Power Transformers 4 IEEE TRANSAC TI ONS ON SYST EMS, MAN, AND CYBERNETICS—PART C: APPLICATIONS AND REVIEWS, 2009 39\(6 5 Yufe i H u ang  Ji any in Wa ng Jia nqiu  Zhang e t a l  Ba y e s i a n  Infe renc e of Genetic Regulatory Networks from Time Series Microarray Data Using Dynamic Bayesian Networks. JOURNAL OF MULTIMEDIA, 2007, 2\(3 6 Y ap Ghim E ng  Tan  Ah H we e, Pang Hwe e-Hw a. Explainin g inf e r e nce s  in Bayesian Networks. Applied Intelligence, 2008, 29\(3\: 263-278 7 Bin X i e A n u p K u m a r  P a dm ana bhan R a m a sw am y  et al Soc ial B e ha vior Association and Influence in Social Networks. Symposia and Workshops on Ubiquitous, Autonomic and Trusted Computing, 2009: 434-439 8 Pasq uier N, Bastide Y, Taou il R et al. Efficient Mining of Association Rules Using Closed Itemset Lattices. Information Systems, 1999, 23\(1 24-45 
534 
534 
534 


wired environment and one from a wireless. Waikato Internet Traffic Storage \(WITS Auckland University. The data is anonymized with only TCP UDP and ICMP and any payload within 64 bytes is zeroed 3]. Community Resource for Archiving Wireless Data At Dartmouth \(CRAWDAD  headers from every wireless packet were sniffed form four of the campus buildings. The buildings were among the most popular wireless locations, and included libraries, dormitories academic departments and social areas. The MAC address were sanitized by randomizing bottom six hex digits. IP address are anonymized using prefix-preserving IP sanitizer as described in [14]. For a given data set, we label the training data set by using port based classification B. Application Behavior Profiling The association rule mining technique help us derive valuable relations within the datapoints. In our case, such association will help us recognize pattern in a particular application The goal of the data mining and classification models is to build a heuristic for predictive recognizing the occurrence of application class from the datapoints The association rules composes of two item sets called an antecedent and consequent. Antecedent is the preceding event and Consequent is an event associated and followed after antecedent. In other words, an event occurring due to antecedent is followed by the consequent is depicted by the association rules for a particular class. Each rule is associated with three parameters, [2 1 be applied to \(the percentage of transactions, in which it is correct 2 is correct relative to the number of cases in which it is applicable \(and thus is equivalent to an estimate of the conditional probability of the consequent of the rule given its antecedent 3 consequent occur together, to the multiple of the two individual probabilities for antecedent and consequent There are conditions where both support and confidence is high, and still result in to invalid rule. Therefore Lift indicates the strength of a rule over random occurrence 


of antecedent and consequent, given their individual support. It provides information about improvement and increase in probability of consequent for a given antecedent. In the other words, Lift is given as Lift RuleSupport Support\(Antecedent Consequent TABLE I ASSOCIATION RULES FROM APRIORI ASSOCIATION ALGORITHM Traffic Rules C Lift DNS dip=16 => dport=56322 1 21.8 dport=4207 => dip=43 1 17.7 Mail dip=8 => dport=53 1 6.7 dip=16 => dport=53 1 6.7 HTTP dip=1 & dport=1273 => srcip=233 1 81.5 dip=1 0.7 1 IRC sip=162 => dport=37273 1 69 dip=16 0.7 1 SMTP sip=262 & dport=4868 => dip=28 1 2.8 dip=28 & dport=4868 => sip=262 1 15.3 Rules with lower lift and confidence values are filtered out Confidence is measured for certainty of the rule. It measures events itemset that matches the antecedent of the implication in the association rule and also matches consequent. Table I clearly mentions flow parameters in antecedent and consequent format for building association rules. The rules in the table are represented as Antecedent => Consequent The association rule indicates an affinity between antecedent and consequent with evaluation parameters such as Support Confidence and Lift For example, for HTTP, destination IP with index 1 and destination port number equal to 1273 will have a flow from source IP of index 233 with confidence \(noted as C lift of 81.5. When we trace back this flow, we are accurately able to classify this flow as HTTP. Moreover, in the future, if we observe any particular flow with this association then we would be certain to classify it as HTTP with confidence value of 1 C. Classification  http mail smtp dns irc 


0 10 20 30 40 50 60 70 80 90 100 Application A cc ur ac y Fig. 2. Accuracy for K-mean clustering and rule-based classification In Fig. 2, we show accuracy values of each application78 after clustering and rule-based classification. The number of cluster \(k a particular application. This is because, the number of flows in each clusters decrease and mean square error reduces for each iteration. This causes flow data to be tightly clustered With mean square error diminishing, misclassified flows are eliminated from the application class. We can observe an increase in precision for all the application classes. However with the increase of the number of clusters the processing time also increases. After several runs, we noted diminishing returns when the number of clusters was greater than 300. In order to get the optimum accuracy, we have averaged it for all values of k between 300 and 500 However, there is a need to estimate the number of clusters k for each clustering run. As we know same application class observes different behavior, therefore the value of k cannot simply be equal to the number of application classes. Hence it is difficult to estimate the k value for each new dataset as well as to maintain high accuracy and precision. One of the reasons is that K-mean clusters are partitioned by mean values iteratively. The mean values of random selected centers converge to the appropriate cluster. Finally, we observe that the overall accuracy is poor with a combination of K-mean 


with transductive and rule-based classification. Hence, we have used Model-based clustering that as will be shown, solves both issues Fig. 3. Bayesian Information Criterion distribution From Fig.3, we observe that Mclust uses BIC to estimate models and number of clusters. Model-based clustering estimates the cluster contour. For Mclust, geometric features are determined by covariances ?k. Each covariance matrix is parameterized by eigenvalue decomposition in the form [6 k = ?kdkAkD T k where, Dk = orthogonal matrix of eigenvectors, Ak = diagonal matrix whose elements are proportional to the eigenvalues of k, ?k = scalar quantity In the model, orientation for ?k depends on Dk. Also, the shape of the density contours is determined by Ak. The volume of the corresponding ellipsoid is proportional to ?dk | Ak where d represents data dimension. The characteristics of the geometric features of the cluster ? orientation, volume and shape are estimated from the data. These can vary between clusters, or be equal for all clusters. Hence we are able to estimate cluster contour for achieving precise model-based hierarchical clustering In one dimension dataset, equal variance and varying variance is represented by E and V respectively. For multiple dimensions, geometric characteristics of the model such as volume, shape and orientation is taken into consideration The volume, shape and orientation represents clustering of flow data. In our case, VEV model represents volumes of all clusters as varying \(V E V an approximation to the Bayes factor. It adds a penalty term to the log-likelihood estimation depending on the number of parameters BIC\(k x|?, k k n where, k is the number of clusters, p is the conditional probability; pi stands for different variance-covariance structures and v\(k clusters and covariance structure ?; ? represents maximum likelihood estimate of the parameters in the model \(k and n is the number of observations. BIC for each model is 


evaluated, and model with the highest BIC is the best model for the flow dataset As shown in Fig. 3, the best estimated model is VEV. VEV indicates volume and orientation is variable \(V equal \(E in Fig.3 that BIC is strongest for VEV model for number of clusters equal to 9. The number of clusters is very close to the number of applications, as opposed to K-mean technique Note that to get the highest accuracy from K-mean, more than 300 clusters were required In Fig.4, it is clearly shown that the model based clustering has performed better than K-mean classification for both data sets. Model-based clustering produces strong application clusters in the data. The Apriori based association rule classifier finds stronger association between flow parameters. The rules with high lift and confidence value, represent stronger correlation to the application. Hence, the rule set help us derive behavior pattern for a particular application class. We trace back the flows to the main trace file and we observe a strong probability that those flows belong to a particular application class. The overall accuracy of this method is around 94%, thus much higher than K-mean clustering method. We tested our model for CRAWDAD data and achieved similar accuracy of 93%.79   http mail snmp dns irc 0 10 20 30 40 50 60 70 80 90 100 ModelbasedandRulebasedClassificationAccuracyGraph WAND Crawdad Application A 


cc ur ac y   Fig. 4. Accuracy for Model-based clustering and rule-based classification Fig. 5. Distribution of flow attributes in scatter plot matrix In Fig.5, the distribution of IP and ports for source and destination is depicted for Model-based clustering method. For HTTP \(blue traffic to few destination IPs. HTTP behavior is prominent because a number of source hosts are communicating with small number of destination hosts, i.e., HTTP servers. In DNS application, source hosts communicate with a set of DNS servers. Each host interacts with different domains such as www, .org and .net for resolving IP addresses. The DNS behavior is prominent because large group of source hosts communicate with small repeated set of destination IPs. In the IRC application, a group of hosts communicates with other group of hosts. There is roughly one-to-one correspondence between source and destination IP. This behavior is prominent because IRC \(cross with approximately equal number of destination IPs. Moreover, source IP and destination port distribution show that destination port numbers are in the range of 30000 for SMTP green where as HTTP and MAIL have destination ports number in lower range of 1000. Model-based clustering with the Apriori algorithm provides reliable and accurate classification model as compared to K-mean method. The association rules helps us predict flows for particular application with high confidence and lift values IV. CONCLUSION In conclusion, we have presented a classification model that achieves high flow classification accuracy with application behavior profiling. The use of the K-mean algorithm for Netflow classification was shown to be inefficient. On the other hand, Model based clustering with association rule mining techniques provided a much better accuracy. Moreover the rule heuristics are produced automatically, making the algorithm modular and independent of the dataset. In addition 


our model is able to detect new behavior patterns for next generation applications. As a future work, we are planning to analyze a bigger database of traces and define behavior patterns for a wider range of applications REFERENCES 1] F. Baker, B. Foster, and C. Sharp. Cisco architecture for lawful intercept in IP networks. Internet Engineering Task Force, RFC, 3924, 2004 2] C. Borgelt and R. Kruse. Induction of association rules: Apriori implementation. In Compstat: Proceedings in Computational Statistics 15th Symposium Held in Berlin, Germany, 2002, page 395. Physica Verlag, 2002 3] WAND Trace Catalogue. http://www.wand.net.nz/wits/catalogue.php 4] C. Dewes, A. Wichmann, and A. Feldmann. An analysis of Internet chat systems. In Proceedings of the 3rd ACM SIGCOMM conference on Internet measurement, pages 5164. ACM, 2003 5] J. Erman, A. Mahanti, and M. Arlitt. Internet traffic identification using machine learning. In Proceedings of IEEE GlobeCom. Citeseer, 2006 6] C. Fraley and A.E. Raftery. MCLUST version 3 for R: Normal mixture modeling and model-based clustering. Technical report, Citeseer, 2006 7] M. Iliofotou, H. Kim, M. Faloutsos, M. Mitzenmacher, P. Pappu, and G. Varghese. Graph-based P2P traffic classification at the internet backbone. In IEEE Global Internet Symposium. Citeseer, 2009 8] T. Karagiannis, A. Broido, and M. Faloutsos. Transport layer identification of P2P traffic. In Proceedings of the 4th ACM SIGCOMM conference on Internet measurement, pages 121134. ACM, 2004 9] David Kotz, Tristan Henderson, Ilya Abyzov and Jihwang Yeo. CRAWDAD trace dartmouth/campus/tcpdump/fall01 \(v. 2004-11-09 http://crawdad.cs.dartmouth.edu/dartmouth/campus/tcpdump/fall01 November 2004 10] A.W. Moore and K. Papagiannaki. Toward the accurate identification of network applications. Passive and Active Network Measurement, pages 4154, 2005 11] TTT Nguyen and G. Armitage. A survey of techniques for internet traffic classification using machine learning. IEEE Communications Surveys Tutorials, 10\(4 12] I. Papapanagiotou and M. Devetsikiotis. Aggregation Design Methodologies for Triple Play Services. In IEEE CCNC 2010, Las Vegas, USA pages 15, 2010 13] V. Paxson. Bro: A system for detecting network intruders in real-time Comput. Networks, 31\(23 14] J. Xu, J. Fan, M. Ammar, and S.B. Moon. On the design and 


performance of prefix-preserving IP traffic trace anonymization. In Proceedings of the 1st ACM SIGCOMM Workshop on Internet Measurement, page 266. ACM, 2001.80 


denote the input attribute with the minimum integrated cost after the split, and let the set of allocated sample sizes, computed by the sample allocation method explained later, be ASM . Then DM | children are generated for the node M . For each child CHi, i = 1, . . . , |DM |, the associated query is QN ?{M = mi sample size is asi ? ASM , and potential splitting attributes is PSN ? M . The process of splitting is then recursively applied to children of node N In the process of calculating costs during the strati?cation process, we need to perform sample allocation, i.e., divide the parent nodes sample size among the potential children nodes This is required for calculating the integrated cost for the potential split. This is based on our sample allocation method, which we 328 describe next in Section IV-B. Furthermore, for calculating the integrated cost, the variance of target value ?2i and probability of output attribute A = a, ?i, for each stratum is computed based on the pilot sample Initially, the strati?cation process on the query space begins by calling Stratification\(R, null, F IA, n, null root node. The process of strati?cation would stop if there is no leaf node with integrated cost larger than a prede?ned threshold Each leaf node in the tree is a ?nal stratum for sampling, and the associated sample size denotes the number of data records drawn from the subpopulation of the stratum B. An Optimized Sample Allocation Method Now, we introduce our optimized algorithm for sample allocation which integrates variance reduction and sampling cost As introduced in section IV-A, integrated cost is de?ned by taking into account of variance of estimation and sampling cost The goal of our sample allocation algorithm is to minimize the integrated cost by choosing the sample size, ni, for each stratum In our algorithm, we adjust the value of SampCost and ?2s so that their in?uences on the integrated cost are in the same unit SampCost SampCost SmpCost\(r where SmpCost\(r entire population, and ?r denotes the probability of A = a being true for the entire population 2 2s 


2r where ?2r 2 n denotes the variance of estimation of the target value on the entire population The key constraint on the values of the sample sizes for each strata is that their sum should be equal to the total sample size A vector n = {n1, n2, . . . , nH} is used to represent the sample sizes, where the ith element, ni, is the sample size for the ith stratum By including sampling cost and variance of estimation into the integrated cost, our sample size determination task leads to the following optimization problem Minimize Cost\(n  i\(?s ni i v N2i niN2 2 i subject to  i ni = n 6 where Ni denotes the population size of data records under the space of A = a in ith stratum. Note that this value may not be known if A is an output attribute. However, the total number of records in the ith stratum is typically known, and can be denoted as DNi. Then Ni can be estimated by ?iDNi, and the population size of A = a on the entire population is estimated by N  i ?i DNi For ?nding the minimum of integrated cost, we utilize Lagrange multipliers, a well know optimization method. Lagrange multipliers aims at ?nding the extrema of a function object to constraints. Using this approach, a new variable ? called a Lagrange multiplier is introduced and de?ned by n n  


 i ni ? n If n is a minimum solution for the original constrained problem then there exists a ? such that \(n Lagrange function. Stationary points are those points where the partial derivatives of ?\(n n,??\(n 7 In our problem, by conducting partial derivatives on Formula 7 a group of equations are yielded as follows s i v N 2 i n2iN2 2 i + ? = 0 i = 1, ..H i ni = n 8 where the solution n leads to the minimum value of integrated cost in Formula 5 However, it is dif?cult to solve the group of equations directly Thus, we use numerical analysis to approximate the real solution. Newtons method is utilized for ?nding successively better approximations to the zeroes \(or roots Given an equation f\(x x tive of function f\(x method iteratively provides, xt+1, a better approximation of the root, based on xt, the previous approximation of root according to the following formula xt+1 = xt ? f\(xt f ?\(xt The iteration is repeated until a suf?ciently accurate value is reached, i.g. |f\(xt In our problem of Formula 8, there are H + 1 equations F \(xt xt xt n1, .., nH , ?}, where the equations in F \(xt fi\(xt s i v N 


2 i n2iN2 2 i + ? i = 1, .., H fH+1\(xt  i ni ? n The Newtons method is also applied iteratively via the system of linear equations JF \(xt xt+1 ? xt xt 9 where JF \(xt H + 1 H + 1 equation system F \(xt vector xt. The entry JF \(i, j d\(fi\(x dxj where fi\(x xt x From the Expression 9, a better approximation xt+1 is obtained based on previous approximation xt. The iterative procedure would be stopped if ?i|fi\(xt threshold, and then the sample size ni is allocated for each stratum so that the integrated cost is minimized. In reality, we are required to pick an integral number of records from each stratum during the sampling step. Thus, we round down each ni to its nearest integer, ni + 0.5 In the example shown in Table I, suppose we set both weights v and ?s to be 0.5. Further, assume the variance of the entire population, ?2r , to be 80000. The probability of A = a over the entire population ?r is 0.242. By using the proposed optimized sample allocation method, the sample sizes for the three strata 329 are 162, 299, and 139, respectively. In this case, the variance of estimation according to the Expression 2 is 93.66, and the estimated cost is 1943.7. We can see that, compared with Neyman allocation, the sampling cost is decreased by 42.1%, but results some increase in variance. Overall, this example shows that we can achieve lower sampling cost by trading off some accuracy C. Overall Sampling Process Algorithm 2 DiffRuleSampling\(DW1, DW2, F IA, t, St 1: PS ? a pilot sample from DW1, DW2 


2: DR ? identi?ed rules from PS 3: OA ? output attributes of DW1, DW2 4: for all R : X ? DW1\(t t 5: if X ?OA = null then 6: Acquire St data records from the space of X 7: else 8: R ? root node 9: Lf ? null 10: Strati?cation\(R,null, F IA, St, Lf 11: for all N ? Lf do 12: s ? sample size of N 13: Draw s data records from the subpopulation of N 14: end for 15: end if 16: Update the mean value of DW1\(t t 17: end for Algorithm 2 shows the overall sampling process for differential rule mining on two deep web data sources, DW1 and DW2 and with differential attribute t. The inputs of the algorithm also contain the full set of input attributes FIA as well as the sample size St. The algorithm starts with a pilot sample PS, from which the differential rules are identi?ed. For the rule R : X ? DW1\(t t St data records are directly drawn from the space of X \(Lines 5-6 t t containing output attributes, query spaces of DW1 and DW2 are strati?ed and sample is recursively allocated to each stratum with corresponding query subspace \(Line 10 of the tree built by strati?cation, a sample is drawn according to its sample size \(Line 13 t t is updated by the further sample \(Line 16 for association rule mining is very similar and not shown here V. EVALUATION STUDY We evaluate our sampling methods for association mining and differential rule mining on the deep web using two datasets described below US Census data set: This is a 9-attribute real-life data set obtained from the 2008 US Census on the income of US households. This data set contains 40,000 data records with 7 categorical attributes about the race, age, and education level of the husband and wife of each household and 2 numerical attributes about the incomes of husband and wife 


Yahoo! data set: The Yahoo! data set, which consists of the data crawled from a subset of a real-world hidden database at http://autos.yahoo.com/. Particularly, we download the data on used cars located within 50 miles of a zipcode address. This yields 30,000 data records. The data consists of 7-attribute with 6 categorical attributes about the age, mileage, brand, etc, of the cars and one numerical attribute, which is the price of the car Variance of Estimation is estimated for the target value \(i.e mean value in differential rule mining, and con?dence in association rule mining is calculated according to the Expression 2. Since variance of estimation reveals the variation of the estimated value from the true value, smaller variance suggests better estimation Sampling Cost is estimated by the number of queries submitted to data sources in order to acquire a certain number of data records containing target output attributes, i.e. A = a. Larger sample size implies higher sampling costs in a deep web setting where the queries are executed over the internet Estimate accuracy is estimated by Absolute Error Rate \(AER Small AER value indicates higher accuracy. For an estimator on variable Y with true value y and estimated value y, the AER of the estimator is calculated by AER\(y A. Association Rule Mining In this section, we present the results of our method for association rule mining. Using our overall approach, we have created four different versions, which correspond to four different sets of weights assigned to variance of estimation and sampling costs. 1 the weight ?v = 1.0 and ?s = 0.0, 2 the weight ?v = 0.7 and ?s = 0.3, 3 v = 0.5 and ?s = 0.5, and 4 weights ?v = 0.3 and ?s = 0.7. In addition, we also compare these approaches with a simple random sampling method, which is denoted by Random We focus on the queries in the form of A = a ? B = b where A and B are output categorical attributes. Other categorical attributes in the data set are considered as input attributes Our goal is to estimate Supp\(A=a,B=b A=a association rules are randomly selected from the datasets. Each of the 50 association rules are re-processed 100 times using 100 different \(pilot sample, sample iterations result is the average result for 5000 executions 


In all charts reported in this section, the X-axis is k, which denotes the size of sample under the space of a target rule drawn from deep web. The sample size for each point on X-axis is k x, where x is a ?xed value for our experiment, and depends upon the dataset. At each time, queries are issued to obtain kx data records under the space of a target rule. Overall, all our experiments show the variance of estimation, sampling costs and sampling accuracy with varying sample size Figure 1 shows the result from our strati?ed sample methods on the US census data set. The size of pilot sample is 2000, from which all of the 50 initial rules are derived. In this experiment the ?xed value x is set to be 300, which means the smallest sample size at k = 1 is 300, and the largest sample size at k 10 is 3000. Figure 1 a the ?ve sampling procedures. Figure 1 b cost for the sampling procedures. In order to better illustrate the experiment result, in each execution of sampling, the variance of 330 6DPSOLQJ9DULDQFH            9D UL DQ FH R I V WL PD WL RQ  


9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW           6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF            5 


 9DU 9DU 9DU 5DQG c Fig. 1. Evaluation of Sampling Methods for Association Rule Mining on US Census Dataset estimation and sampling cost for the sampling procedures var7 var5, var3, and rand are normalized by the corresponding values of Full Var. Thus, in our experiment, the values of sampling cost and variance of estimation for sampling procedure Full Var are all 1. Furthermore, Figure 1 c sampling procedures From Figure 1 a pared with sampling procedures Var7, Var5 and Var3, Full Var has the lowest estimation variance and the highest sampling cost. From sampling procedures Var7, Var5, and Var3, we can see a pattern that the variance of estimation increases, and the sampling cost decreases consistently with the decrease of the weight for variance of estimation. At the largest sample size of k = 10, the estimation variance of sampling procedure Var3 is increased by 27% and the sampling cost is decreased by 40 compared with sampling procedure Full Var. The experiment shows that our method decreases the sampling cost ef?ciently by trading off a percent of variance of estimation. Similar to variance of estimation, the sampling accuracy of these procedures also decreases with the decrease of the weight on variance of estimation. For the largest sample size at k = 10, we can see that the AER of sampling procedure Var3 is increased by 20 compared with sampling procedure Full Var. However, for many users, increase of the AER will be acceptable, since the sampling cost is decreased by 40%. By setting the weights for sampling variance and sampling cost, users would be able to control the trade-off between the variance of estimation, sampling cost, and estimation accuracy In addition, compared with sampling procedure of Full Var Var7, Var5, and Var3, sampling procedure Random, has higher estimation of variance, sampling cost and lower estimation accuracy. Thus, our approach clearly results in more effective methods than using simple random sampling for data mining on the deep web Figure 2 shows the experiment result of our proposed strati?ed 


sampling methods on the Yahoo! data set. The size of pilot sample on this data set is 2,000, and the ?xed value x for sample size is 200. The results are similar to those from the US census dataset. We can still see the pattern of the variance of estimation increasing with the decrease of its weight. Besides, the sampling accuracy is also similar to the variance of estimation. However although the variance estimation of sampling procedure Random is 60% larger than sampling procedure Full Var, the sampling cost of Random is 2% smaller than Full Var. This is because Full Var does not consider sampling cost. It is possible that Full Var assigns a large sample to a stratum with low ?, which denotes the probability of containing data records under the space of A = a, resulting the larger sampling cost than that of simple random sampling. Sampling procedures Var7, Var5, Var3 consider sampling cost as well, and have smaller variance estimation and sampling cost, compared with Random. Furthermore, Random has smaller sampling accuracy than Full Var, Var7 and Var5, but has larger sampling accuracy than Var3. This is because Var3 assigns much more importance to the sampling cost, and loses accuracy to a large extent To summarize, our results shows that our proposed strati?ed sampling are clearly more effective than simple random sampling on the deep web. Moreover, our approach allows users to tradeoff variance of estimation and sampling accuracy to some extent while achieving a large reduction in sampling costs B. Differential Rule Mining In this section, we present results from experiments based on differential rule mining. Particularly, we look at the rules of the form A = a ? D1\(t t categorical attribute and t is an output numerical attribute, while other categorical attributes in the data set are considered as input attributes In this experiment, we also evaluate our proposed method with different weights assigned to variance of estimation and sampling cost. Five sampling procedures, Full Var, Var7, Var5,Var3 and Random, have same meanings with those in the experiments of association rule mining. Similarly, 50 rules are randomly selected from the datasets, and each of the 50 differential rules are reprocessed 100 times using 100 different \(pilot sample, sample iterations 5000 runs First, we evaluated the performance of these procedures on 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


