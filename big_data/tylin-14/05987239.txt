 Research on Application of Data Mining Based on  FP-Growth Algorithm for Digital Library  Yunlong Song  Library, Shandong University at Weihai, Weihai City, Shandong Province, P. R.China songyl@sdu.edu.cn Ran Wei Department of Computer Science, Shandong University at Weihai, Weihai City, Shandong Province, P. R.China  biweijian731@126.com   Abstract\227 In the field of the library, there is a huge amount of data from readers\222 borrowing records every day. We can find an 
interesting network from the relationship of books by doing data mining works on these records, especially on the analysis of association rules, which can help us to find the needs of readers more clearly. FP-growth \(frequent pattern growth\ses an extended prefix-tree \(FP-tree\cture to store the database in a compressed form. FP-growth adopts a divide-and-conquer approach to decompose both the mining tasks and the databases In this paper, we use FP-growth algorithm to analyze the association rule of library circulation records. The results can 
make great sense to help to improve the quality of library collections Keywords-Data mining; FP-growth; Digital library; Algorithm  I. INTRODUCTION With the development of office automation technology now almost all the libraries have adopted the library management systems to direct the library and have achieved automation management of acquisition, cataloging, circulation and other process. With the library work, the system will produce huge amounts of data every day, such as book order history, records of readers in 
to the library, operation records from the staff, etc. in which the most important and the maximum amount of data is the record of the book circulation of borrowing and returning. We can find useful information as follows with the analysis on the degree of association of the borrowing record   Books which are easily be borrowed by many readers at the same time  Groups of readers who prefer to borrow certain groups of books  Ratio between the number of books borrowed by readers at the same time 
 Features of the book contents in different groups of readers  Characteristics of the borrowing cycle in different groups of readers We can find out the relationships between books and readers which is an important premise to establish the purchasing system for the library with data mining on the association rules of the borrowing records II. ABOUT DATA MINING Data mining \(sometimes called data or knowledge discovery\ is the process of analyzing data from different perspectives and summarizing it into useful information 
information that can be used to increase revenue, cuts costs, or both m i n i n g s o f t w a re is on e of a n u m b er of an al y tic al tools for analyzing data. It allows users to analyze data from many different dimensions or angles, categorize it, and summarize the relationships identified. Technically, data mining is the process of finding correlations or patterns among dozens of fields in large relational databases  223Diapers and beer\224 is the most classic cases in the field of data mining, which can help us understand the concept of data mining. Wal-Mart uses its data warehouse system to analyze the customers\222 shopping behavior and find the 
unexpected results that along with the diapers, beer was actually the goods purchased up! After a survey analysis, they found out the reason for this phenomenon is: U.S. wives often told their husbands to buy diapers for the kids after work, and the husbands always brought their favorite beer after buying the diapers. So the Wal-Mart stores set the counter of diapers and beer side by side, brought the results that both diapers and beer sales growth A. Common Methods of Data Mining There are two types of knowledge discovery of data mining: \223Verification-driven\224 and \223Discovery-driven\224 223Verification-driven\224 refers to that users make their own 
assumptions first, and then use various tools to verify or deny them through repeated, recursive search queries 223Discovery-driven\224 means to discover new hypotheses through the techniques of Machine-Learning, statistics and data visualization and so on which play a very important and positive role in discovering new knowledge. But this process is not entirely automatic that it still need analysts involved.. It also includes descriptive and predictive two categories Descriptive data mining involves visualization, aggregation 1525 978-1-4244-9439-2/11/$26.00 \2512011 IEEE 


correlation, statistical and other techniques. Predictive data mining mainly refers to decision trees, rule induction, neural network techniques and etc. And such descriptive and predictive type of discovery-driven data mining is that we generally mentioned technology of data mining.[2   1  Classification. Classification is to identify the common characteristics of a set of data objects in the database and to divide them into different classes follow the classification model with the purpose to map the data in the database to a given category. It can be applied to analyze the readers\222 classification, properties, characteristics, satisfaction and forecast the trend of readers\222 loan, such as to divide the readers into different groups according to their reading hobbies 2  Regression Analysis. Regression analysis method reflects the features of property values in the transaction database in time. The main research issues include the trend characteristics of data series, the forecast of data series and the relationship between data and so on. It can be applied to all aspects of library management, such as to catch the readers\222 dynamic needs of books, analyze the book\222s life cycle forecast the trend of demand on books and make training activities targeted  3  Association Rules. Association rules are the discipline to describe the relationship that exists between data items in the database, which is also the mutual relations hidden in the data. In the management of readers\222 relationship we can find out a large number of interesting relationship and identify the key factors that affect books borrowing which can supply a foundation for us to purchase new books, train the readers and promote the service through data mining on the great quantity of data stored in the library management system 4  Deviation Analysis. Deviation includes many potential and interesting knowledge, such as the unusual instance of classification, exceptional mode and deviation between observations and the expectation, with the aim to find out some significant differences between observations and references. In the management and warning system, managers are more interested in these unexpected rules which can be used in the discovery, analysis, identification, assessment and early warning of abnormalities  B. The Process of Data Mining Data mining process generally consists of four phases identification of business objects, data preparation, data mining and evaluation of results, as shown in Figure 1 1 Make the business objects. The business objects being researched is the basis of the whole process in data mining. It drives the entire process and tests the final results to guide staff to complete data mining. To define the business problem clearly and identify the purpose of data mining is an important step in data mining. The final structure is unpredictable, but the issue be explored should be foreseen. Doing data mining just for conducting data mining is blindness and will not be succeed  Figure 1. Structure of Data Mining 2 Data Pre-processing. Data pre-processing is the second phase of data mining with great importance. The quality of data preparation will directly affect the efficiency and accuracy of data mining and the validity of the final excavating pattern. This stage can be further divided as data integration, data extraction, data pre-processing and data conversion of four steps 3 Data Mining. Data mining is the core phase of the process of knowledge discovery, the purpose of which is to extract the hidden and unknown in advance but potentially useful information and knowledge. At this stage, it needs to adopt some intelligent methods to extract data models including the main points as  Decide how to generate hypothesis  There are two ways to generate hypotheses, one is using the data mining system to generate hypothesis for the user and the other is that users bring up hypotheses of knowledge that may be contained in the database   Select the appropriate mining tools. Different mining tools are applicable in different scope, so it needs to choose effective mining models and methods based on targeted areas in practice  Operate knowledge of mining. Data mining on the                                        1526 


transformed data to find and confirm significant knowledge 4 Evaluate the results. After building the model through data mining techniques, we must evaluate the results and explain its value. The accuracy obtained from the test set is only useful for the data used to establish a model. In practice the model\222s accuracy will certainly change with different data from applications. Therefore, we need to modify the prediction model repeatedly to select the best III. DATA MINING ALGORITHM A. Concept of Association Rules In 1993, R.Agrawal and Swami first proposed the concept of associate rules h ic h  w e re f i r s t us ed to an al yze the correlation between items in the basket and later be extended to solve many related questions. The general definition of association rules is as follows  J={ I 1 I 2 205, I m is a set of items, D is a transaction database, in which each transaction JT 005  contained in D is s, we say that the support of the association rule BA  in D is s, which can also be expressed as the probability P B A 005  if the ratio that A and B both contained in D is c, we say that the confidence of the association rule BA  is c, expressed as conditional probability P \(B|A\That is  Support BA P B A 005  Confidence BA B|A Association rules have two important properties as follows Support: P\(A B\, which is the probability that A or B appears in the transaction set D Confidence: P\(B A  which is the probability that A and B both appears at the same time in the transaction set D Rules that can meet the minimum support threshold and minimum confidence threshold at the same time are called strong rules  B. Apriori Algorithm Aprior algorithm is the basic algorithm of association rules, which was designed by Agrawal in 1993. [4 T h i s is a s e t  of method based on the thought of two-stage frequency which can be designed in two subproblems  Find all the itemsets whose support is greater than the minimum support, and we call them frequent itemsets  Use the frequent itemsets found in step 1 for generating rules desired Apriori algorithm has follow features   Apriori algorithm needs to scan the database repeatedly. So one direction to improve it is to reduce the number of database scans  Apriori algorithm will produce a large number of candidate itemsets. The count and storage of these frequent itemsets costs a lot. Therefore, another direction to improve the algorithm is to reduce the number of candidate itemsets  Apriori algorithm\222s main operation is to count the support which can be improved with several techniques C. FP-growth Algorithm In many cases, Apriori frequent set generate-check method significantly reduces the size of frequent sets with good performance. But there is still some inherent weaknesses   It may produce a large number of frequent sets  It may need to scan the database repeatedly and check a lot of frequent sets through pattern matching Using frequent-pattern growth, the FP-growth, can effectively address the issu T h e m a i n idea of th e  algorithm is to take the following divide and conquer strategy compress the database which provide frequent sets to a frequent-pattern tree \(or FP-tree\t still retain relational information of itemsets; then divide this compressed database into a set of conditional database \(a special type of project database\ associated with a frequent set, and do data mining on each database. Related concepts are as follows  FP-Tree Sort data items in the transaction data table by support, then insert the data items in each transaction into a tree with NULL as its root by descending turn and record the support of each node occurs  Conditional pattern base Contains the set of prefix path which appears together with the suffix pattern set in the FP-Tree  Condition tree Construct the conditional pattern base into a new FP-Tree according to the principles of the formation of FP-Tree Specific process is divided into the following steps  1 Enter a transaction database DB and a minimum support threshold to construct FP-Tress. Specific for Scan the database DB to get the frequent itemsets F and the support for each item. Order F according to descending support and note the result as L 1527 003 and 003  JB 002 we say that transaction T contains A An association rule is an implication relationship of the form BA  in which JA 004  BA  If the ratio that B A 002 Each transaction has its own identifier called TID. If A is a set of items, if and only if TA 


TABLE 1 BORROW LOG DATABASE TID Item List T1 1,2,5 T2 1,2 T3 2,4 T4 1,2,4 T5 1,3 T6 1,2,3,5 T7 1,2,3 T8 2,5 T9 2,3,4 T10 3,4  Create the root of FP-Tree noted T, and marked as \221Null\222 Then do the following steps to each transaction Trans in DB select and sort the items of Trans according to the order of L mark the list of affair items sorted in Trans as [p|P w h ere p is  the first element and P is the rest of the list; call insert_tree\([p|P T  F u n c tion ins e rt_tree\([p P T  ru n as follows: if T has a child node N where N.item-name=p.item-name, then increase the threshold of N by 1; otherwise, create a new node N and make its count as 1 and its parent node as T, and string its node_link with the field which has the same item_name together. If P is not empty recursively call insert_tree\(P,N T h e FP T ree c o n s tru ct ed based on table 1 is as shown in Figure 2  Figure 2. FP-TREE  2 Do data mining on the FP-Tree, specifically described as follows procedure FP-Growth \( Tree, x   if \(Tree only contains a single path P\ then     for each combination of nodes on the path P mark as B      generate mode B and x, support = the minimum support of all nodes in B  else for each ai on the head of Tree, do      generate mode B = ai and x, support = ai.support     construct the conditional model libraries of B and the conditional FP-Tree for B, TreeB if TreeB != empty set     then call FP-Growth \( TreeB , B   In the FP-growth algorithm, compressed store the frequent itemsets in each transaction into the FP-Tree in descending order of support through scanning the transaction database twice. In the future, when develop the frequent items we won\222t need to scan the transaction database but only check the FP-Tree, and generate frequent itemsets directly through recursice call FP-growth method. Therefore, it also need not generate candidate itemsets in the discovery process. This algorithm overcomes the problems appeared in Apriori and performs significantly better in the implementation of efficiency V.  EXPERIMENT DESIGN     The library office system I am working on is developed by Jiangsu Huiwen Software Limited Company which use ORACLE to store data in the background. Table LEND_HIST records the history of readers borrowing books. I try to write code for generating association rules with C# on the Microsoft Visual Studio 2008 platform. The results are shown in Figure 3  Figure 3. Result of Data Mining  VI. CONCLUSIONS After reviewing the theory of data mining and association rules, we build data mining on the association rules of data of book loan based on FP-growth algorithm targeted on the readers borrowing records to analyze which books are borrowed at the same time to bring up some constructive comments for the library collection and book purchase. We found that the FP-growth algorithm can implement in a short time and get high accuracy when analyze the association rules of books borrowed records through the final test and analysis  References 1 L i hu i Ch en g G a n g  223R es ea rc h  on th e M o d e l of E n t e rp ri s e C o m p eti t i v e Intelligence System Based on Data Mining Technology\224, Information Studies:Theory & Application, An Hui, 2011.1, pp.95-99 2  C hun ji a   M a o Z h i y on g 223R es ea rc h o n A s s i s t a n t Deci s i on m a k in g i n  Formulating University Library Book Purchasing Plan Based on Data Mining\224, Journal of Modern Information, 2009.7, pp.108-110 3  A g ra w a l a n d  R  S rik a n t  F a s t A l go ri t h ms for Mi ni n g A s s o c i a ti on Rules", Proc.of the 20th Int'l Conference on Very Large Databases Santiago, Chile, 1994  i u Y a n xi  223S tu d y on app l i c a t i o n of ap ri ori a l gori t hm  i n d a t a m i ni ng\224  ICCMS 2010, January 22, 2010, pp. 111-114 5 Zha n g W e i  L i a o Hon g zh i  Zha o Na  223R es ea rc h o n th e F P gro w t h  algorithm about association rule mining\224, ISBIM 2008, December 19,2008, pp. 315-318  W a n g Mi n gm i n g 223A s s oc i a t i on R u le of B orro wed B ook s 224  Q i n g da o  Ocean University of China, 2007               1528 


Smoke attribute Map to integer items using rule base and dictionaries Figure 1. Data transformation of medical data 3. The proposed algorithm The main theme of this algorithm is based on the following two statements. Interesting relationships among various medical attributes are concealed in subsets of the attributes, but do not come out on all attributes taken together. All interesting relationships among various medical attributes have not same support and confidence. The algorithm constructs a candidate   itemsets based on groups constraint and use the corresponding support of each group in candidate selection process to discover all possible desired itemsets of that group. The goals of this algorithm are the following: finding desired rules of medical researcher and running fast. The features of this proposed algorithm are as follows x It allows grouping of attributes to find relationship among medical attributes. This provides control on the search process x Minimum confidence and support can vary from one group to another group x One item can belong to several groups x Attributes are constrained to appear on either antecedent or consequent or both side of the rule x It does not generate subsets on full desired itemset, but generates subsets for items that can appear in both consequent and antecedent x Uninteresting relationships among medical attributes are avoided in the candidate generation phase which reduces number of rules, finds out only interesting relationships and makes the algorithm fast Confidence is not the perfect method to rank symmetric medical relationships because it does not account for the consequent frequency with the antecedent. For the ranking of medical relationship, a direct measure of association rule between variables 


is a perfect scheme. For a medical relationship s ? t s is a group of medical items where each item is constrained to be appear in antecedent or both and t is a group of medical attributes where each item is appear to be in consequent or both. Moreover s ? t = . For this relationship, the support is defined as support = P?s, t? and the confidence is defined as = P?s, t?/P?t? where P is the probability The correlation coefficient \(also known as the ?coefficient between two random variables by measuring the degree of linear interdependency. It is defined by the covariance between the two variables divided by their standard deviations:?st = Cov\(s, t Here Cov\(s, t two variables and ?X and ?Y are stand for standard 82 deviation .The covariance measures how two variables change together Cov?s, t? = P?s, t? ? P?s?P?t As we know, standard deviation is the square root of its variance and variance is a special case of covariance when the two variables are identical.?s = ?Var?s? = ?Cov?s, s P?s, s? ? P?s?P\(s s Similarly, ?t = ?P?t? ? P\(t s t Here P?s, t? is the support of itemset consists of both s and t. Let the support of the itemset be Sst Here p?s? and p?t? is the support of antecedent s and antecedent t respectively. Let the support of antecedent s and consequent t be Ss andSt . The value of Sst , Ss and St are computed during the desired itemset generation of our proposed algorithm. Using these values, we can calculate the correlation of every medical relationship rule between a group of medical items to another group of medical items. The correlation value will indicate medical researchers how strong a medical relationship is in perspective of historical data.  ?st = Sst ? Ss St?Ss ? Ss2 ? St ? St2 So putting the value of ??? , ?? and ?? in association rule generation phase, we have found the single metric, correlation coefficient, to represent how much antecedent and consequent are medically 


related with each other. For each medical relationship or rule, this metric has been used to indicate the degree of strong relationship between a group of items to another group of items to support medical qualitative research. The ranges of values for ??? is between -1 and +1. If two variables are independent then ??? equals 0. When ??? equals +1 the variables are considered perfectly positively correlated. A positive correlation is the evidence of a general tendency that when   a group of  attribute values s for a patient happens, another group of attribute values y for the same patient happens. More positive value means the relationship is more strong When ??? equals -1 the variables are considered perfectly negatively correlated Figure 2 shows the association-mining algorithm to support medical research. Like Apriori, our algorithm is also based on level wise search. The major difference in our proposed algorithm is candidate generation process with Apriori. Each item consists of attribute name and its value. Having retrieved information of a 1-itemset, we make a new 1-itemset   if this 1-itemset is not created already otherwise update its support. The 1-itemset can belong to zero or more groups. 1-itemset   is selected if it has support greater or equal to one of its corresponding group support. As medical attribute value contains patient information that is multidimensional, the algorithm performs the count operation by comparing the value of attributes instead of determining presence or absence of values of attributes to calculate support 3.1. Candidate Generation and Selection The intuition behind candidate generation of all level-wise algorithms like Apriori is based on the following simple fact: Every subset of a frequent itemset is frequent so that they can reduce the number of itemsets that have to be checked However, the idea behind candidate generation of proposed algorithm is   every item in the itemset has to be in the same group. This idea makes the new candidates that consist of items in the same group 


and keeps itemsets consist of both rare items and high frequent items. If all the items in a new candidate set are in the same group, then it is selected as a valid candidate, otherwise the new candidate is not added to valid candidate itemsets Here for each group there are different support and confidence. Each candidate itemset belongs to a particular group. After finding group id of a candidate itemset, the algorithm uses corresponding support for candidate selection where as Apriori uses a single support threshold for all the candidate itemsets. By this way, itemsets are explored which are desired   to medical researchers 3.2. Generating association rules Let AC\(item of three  values: 1 if item   is constrained to be  in the antecedent of a rule, 2 if it  is constrained to be  in the consequent and 0 if it can be in either. Using this function, itemset is partitioned into antecedent set consequent set and both set. Moreover, it does not use subset generation to itemsets to form rules like conventional association mining algorithm; it only uses subset generation to both set. Each subset of both set is added in antecedent part in one rule and is added in consequent part in another rule. Each itemset belongs to a particular group. In addition to there is a different confidence for each group whereas Apriori uses a single confidence for all the itemsets. After finding group id of an itemset, the algorithm uses corresponding confidence to form rules. By this way, rules are explored which are desired of medical researchers 83 Algorithm:  Find itemsets which has high support and are in the same group Input: Data and metadata files Output :  Itemsets which are desired to Medical Researchers 1. K=1 2. Read the metadata about which attributes can only appear in the antecedent of a rule,  can only appear in the consequent and   can appear in either 


3. Read Groups Information along with each group support and confidence from configuration file and make  dictionary , here key is the attribute number and value is a list of group numbers on whcih the corresponding attribute belongs to 4. Ik = Select 1-itemsets that have support greater or equal to one of its corresponding group support 5. While\(Ik ??\f 5.1 K 5.2 CK = Candidate_generation\(Ik-1 5.3 CalculateCandidatesSupport\(Ck 5.4 Ik  = SelectDesiredItemSetFromCandidates\(CK GroupSupports 5.5 I  = I U Ik 6. return I procedure Candidate_generation\(Ik-1: frequent \(k-1 itemsets 1. for each Itemset i1 ?,k-1 1.1for each Itemset i2 ?,k-1 1.1.1 newcandidate, NC = Union\(i1,i2 1.1.2 if  size of  NC  is k 1.1.2.1  isInSameGroup =TestWhetherAll TheItemsInSameGroup\(NC 1.1.2.2  if \(isInSameGroup == true 1.1.2.2.1 add  NC to Ck othewise remove it 2. return Ck  procedure  SelectDesiredItemSetFromCandidates CK, GroupSupports 1.1 j=FindGroupNoWhichHasMinimum SupportIfMultipleGroupsExist \(c 1.2  If c.support >=   GroupSupports[j 1.3 Add it to I 2. return I Algorithm : Find assosiation rules for decision supportability of medical reasearcher Input:  I  : Itemsets , GroupConfidences Output: R: Set of rules 1. R 2. For each X ? I 2.1 j =FindGroupNoWhichHasMinimum 


ConfideceIfMultipleGroupsExist\(X 2.2 Both Set B  = \(b1, b2En X   and AC\(bi QWHFHGHQWVHW$6 DVDVDVQ\f where   asi ?;DQG$&DVi 2.4 Consequent  set CS = \(cs1, cs2FVn where  csi ? X and AC\(csi 2.5 For each subset Y  of  B 2.5.1 Y1 = B-Y 2.5.2 AS1 =AS U Y 2.5.3 CS1 = CS U Y1 2.5.4   if  \(support \(AS1 ? CS1 AS1  2.5.4.1  AS1 ? CS1 is a valid rule 2.5.4.2 R = R U \(AS1 ? CS1 2.5.5 AS2 =AS U Y1 2.5.6 CS2 = CS U Y 2.5.7   if  \(support \(AS2 ? CS2 AS2  2.5.7.1  AS2 ? CS2 is a valid rule 2.5.7.2 R = R U \(AS2 ? CS2 Figure 2: Association mining algorithm to support medical research 3.2.1. Lemma 1. Number of rules is equal to   ? 2L\(D2i itemsets and L is function, which determines number of items in an itemset. D2 is the both set. Number of discarded rules =  mp ? ? 2L\(D2i Proof: Let I = {i1, i2Ln} be the set of items. Let G= {g1,g2,g3Jq} be the set of groups.  Let R r1,r2,r3Us} be the set of restrictions. GS is the function, which finds groups with the smallest confidence. If not all items are in the same group, the GS returns NULL. 1-itemset is selected if S\( 1itemset GS\(1-itemset which returns support for an itemset. Let C= {c1, c2 c3Fx} be the set of  candidate itemsets. A new candidate NC is added to C LI*61&\f?18 ci is selected for rule generation if S\(C GS\(C three parts. D = {D0, D1, D2}. D0 is mapped to anticipated items, D1 is mapped to consequent items D2 is mapped to both. Each subset of D2, d, is added to both antecedent and consequent. When d is added 


to antecedent then D2-d is added to consequent. On the other hand, when d is added to consequent then D2-d is added to antecedent. L is a function, which determines number of items in a itemset. Number of rules from D =2?\(?2 2 itemsets. Let m is the average number of distinct value, each multidimensional attribute holds. P is the number of attributes. Number of possible different rules = ?? . Number of discarded rules =  ?? ?? 2?\(?2 4. Results and discussion The experiments were done using PC with core 2 duo processor with a clock rate of 1.8 GHz and 3GB of main memory. The operating system was Microsoft Vista and implementation language was c#.  We used 1 dataset to verify our method. The data set of interest is patient dataset collected and preprocessed from Bangladeshi hospitals, which has 50273 instances and 514 attributes \(included 150 discrete and 364 numerical attributes categories of healthcare data: ratio, interval, decimal integer, percentage etc. All these data are converted into mineable items \(integer representation domain dictionary and rule base. We have taken an 84 average value from 10 trials for each of the test result Table 1. Test result for patient dataset Number of groups 4 8 Support for  each group .55 64 76,.45 47,.84, .66 55,.85, .94 86,.35 Correlation for each group .71 41 51,.61 63, .85,.82 76,.91, .73 82, .71 Number of Items to be constrained in antecedent for 


each group 4,4,4,4 5,4,5,6 4,5,5,7 Number of Items to be constrained in consequent  for each group 1,2,2,1 1,2,2,1 1,2,2,1 Number of Items to be constrained in both for each group 0,0,0,0 1,1,1,1 1,1,1,0 Total number of desired itemsets 125 311 Total number of desired rules 21 28 Time\(Seconds Table 1 shows test result for patient dataset, after running the program of the  proposed algorithm with different parameters. Second column of the table presents the test result, where we used 4 groups minimum support of 45%-76% and correlation of 41-.71 to mine symmetric association rules for medical researcher. The maximum number of items in a rule  was 6.  125 desired itemsets were generated in total. 21 rules were discovered in total. It took about 3461 seconds to find these rules. Third column of the table presents the test result, where we used 8 groups, minimum support of 35%-94% and correlation of .63-.91 to mine symmetric association rules for medical researcher. The maximum number of items in a rule  was 8. 311 desired itemsets were generated in total. 28 rules were discovered in total It took about 11122 seconds to find these rules Figure 3: Time comparison of the proposed algorithms for the patient dataset based on number of groups Figure 3 shows how time is varied with different number of groups for the medical research algorithm We measured the performance of Medical Research algorithm in terms of number of groups keeping group size constant, support and confidence of each group constant, antecedent and consequent 


constrains on attributes constant. Time is not varied significantly because the number of groups has no lead to reduce disk access. This is because number of groups has no lead to the number of candidate generations phases and to the number of support calculation phases. The number of groups has only lead to the number of valid candidate generations and it can save some CPU time Figure 4: Time comparison of the proposed algorithms for the patient dataset based on Group Size Figure 4 shows how time is varied with different group size for medical research algorithm. Here we measured the performance of Medical Research algorithm in terms of group size keeping number of groups constant, support and confidence of each group constant, antecedent and consequent constrains on attributes constant. Time is varied significantly because group size has lead to reduce disk access. This is because group size has   lead to the number of candidate generations phases and to the number of support calculation phases Figure 5: Accuracy of test result for the patient dataset based on correlation Figure 5 illustrates accuracy results for our proposed algorithm. The value of correlation for each presented result is also indicated. For accuracy measurement, we intentionally discovered relationships among attributes for which trends are known. Here we calculated accuracy as the ratio between the number of correct discovered relationships and total number of discovered relationships. A discovered relationship is correct if it is one of the known trends of medical domain. It shows that an average accuracy of 55% is achieved with correlation 0.5. The proposed algorithm with correlation 0.7 achieves an average accuracy of 85.66%. The proposed algorithm with correlation 0.7 achieves an average accuracy of 94.66%. As 0 1000 2000 


4 8 12 T im e S e co n d s Number of Groups Group Size 4 Group Size 10 Group Size 18 0 2000 4 8 12 T im e S e co n d s Group Size 4 Groups 8Groups 12 Groups 0 0.5 1 0.5 0.7 0.85 A cc u ra cy Correlation Group Size 4 Group size 10 Group Size 18 85 accuracy refers to the rate of correct values in the data, the figure represents the success of our 


proposed  data mining algorithm 5. Conclusion Medical Researchers are interested to find relationship among various diseases, lab tests symptoms, etc. Due to high dimensionality of medical data, conventional association mining algorithms discover a very high number of rules with many attributes, which are tedious, redundant to medical researchers and not among their desired set of attributes. In this paper, we have proposed an association rule mining algorithm for finding symmetric association rules to support medical qualitative research. The main theme of this algorithm is based on the following two statements interesting relationships among various medical attributes are concealed in subsets of the attributes but do not come out on all attributes taken together and all interesting relationships among various medical attributes have not same support and correlation. The algorithm constructs a candidate item sets based on groups constraint and use the corresponding support of each group in candidate selection process to discover all possible desired item sets of that group. We propose measuring interestingness of known symmetric relationships and unknown symmetric relationships via the correlation measure of antecedent items and consequent items. The proposed algorithm has been applied to a real world medical data set. We have shown significant accuracy in the output of the proposed algorithm. Although we have used levelwise search for finding symmetric association rules each step of our algorithm is different from any level-wise search algorithm. Rules generation from desired item sets is also different from conventional association mining algorithms 6. References 1] R. Agrawal and R. Srikant, "Fast Algorithms for Mining Association Rules in Large Databases," in Proceedings of the 20th International Conference on Very Large Data Bases, San Francisco, CA, USA 1994, pp. 487 - 499 


2] S. Brin, R. Motwani, J. D. Ullman, and S. Tsur Dynamic Itemset Counting and Implication Rules for Market Basket Data," in Proceedings of the 1997 ACM SIGMOD international conference on Management of data, Tucson, Arizona, United States 1997, pp. 255-264 3] J. S. Park, M. S. Chen, and P. S. Yu, "An Effctive Hash based Algorithm for mining association rules in Prof. ACM SIGMOD Conf Management of Data New York, NY, USA, 1995, pp. 175 - 186 4] R. Agrawal, T. ,PLHOL?VNL DQG $. Swami, "Mining Association Rules between Sets of Items in Very Large Databases," in Proceedings of the 1993 ACM SIGMOD international conference on Management of data, Washington, D.C., 1993, pp. 207-216 5] H. Mannila, H. Toivonen, and A. I. Verkamo Efficient Algorithms for Discovering Association Rules," in AAAI Workshop on Knowledge Discovery in Databases, 1994, pp. 181-192 6] R. Srikant and R. Agrawal, "Mining Generalized Association Rules," in In Proc. of the 21st Int'l Conference on Very Large Databases, Zurich Switzerland, 1995 7] R. Srikant, Q. Vu, and R. Agrawal, "Mining association rules with item constraints," in In Proc 3rd Int. Conf. Knowledge Discovery and Data Mining, 1997, pp. 67--73 8] A. Savasere, E. Omiecinski, and S. B. Navathe, "An Efficient Algorithm for Mining Association Rules in Large Databases," in Proceedings of the 21th International Conference on Very Large Data Bases 1995, pp. 432 - 444 9] H. Mannila, "Database methods for data mining," in The Fourth International Conference on Knowledge Discovery and Data Mining, 1998 10] B. Liu, W. Hsu, and Y. Ma, "Mining Association Rules with Multiple Minimum Supports.," in SIGKDD Explorations, 1999, pp. 337--341 11] H. Yun, D. Ha, B. Hwang, and K. H. Ryu, "Mining association rules on significant rare data using relative support.," Journal of Systems and Software archive vol. 67, no. 3, pp. 181 - 191, 2003 


12] M. Hahsler, "A Model-Based Frequency Constraint for Mining Associations from Transaction Data Data Mining and Knowledge Discovery, vol. 13, no 2, pp. 137 - 166, 2006 13] L. Zhou and S. Yau, "Association rule and quantitative association rule mining among infrequent items," in International Conference on Knowledge Discovery and Data Mining, San Jose, California 2007, pp. 156-167 14] C. Ordonez, C. Santana, and L. d. Braal, "Discovering Interesting Association Rules in Medical Data," in Proccedings of ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, 2000, pp. 78-85 15] L. J. Sheela and V. Shanthi, "DIMAR - Discovering interesting medical association rules form MRI scans," in 6th International Conference on Electrical Engineering/Electronics, Computer Telecommunications and Information Technology 2009, pp. 654 - 658 16] C. Ordonez, N. Ezquerra, and C. A. Santana Constraining and summarizing association rules in medical data," Knowledge and Information Systems vol. 9, no. 3, pp. 259 - 283, September 2005 17] H. Pan, J. Li, and Z. Wei, "Mining Interesting Association Rules in Medical Images," Lecture Notes In Computer Science, vol. 3584, pp. 598-609, 2005 18] S. Doddi, A. Marathe, S. S. Ravi, and D. C Torney Discovery of association rules in medical data Medical Informatics and the Internet in Medicine, vol 26, no. 1, pp. 25-33, January 2001 86 


the time needed for execution exceeded 100000 seconds Thus, from this analysis we see that FPrep, which uses FCM clustering, clearly outperforms the CLARANS and CURE based methods on the basis of speed. The execution times for CLARANS and CURE mentioned in fig. 7 and Table II do not include the time required to create fuzzy sets, and calculate the membership value  for each numerical data point in every fuzzy set for the numerical attribute under consideration. These times also do not take into account the time required to transform crisp numerical attributes to fuzzy attributes, and derive the fuzzy dataset from the original crisp dataset The fuzzy partitions generated for each of the five numerical attributes for the USCensus1990raw dataset are shown in Table III. Coincidentally, generating three fuzzy partitions for each numerical attribute seemed a perfect fit In addition to the superior speeds achieved by FPrep, as illustrated in fig. 7 and Table II, Table III indicates the semantics and the quality of the fuzzy partitions generated by FPrep. Moreover, the number of frequent itemsets generated by a fuzzy ARM algorithm \(like fuzzy ARMOR and fuzzy Apriori minimum support threshold, is illustrated in fig. 8   Fig. 7. Algorithm, numerical attribute comparison based on speed \(log10 seconds   Fig. 8. Number of frequent itemsets for various minimum support values  B. Results from Second Dataset We have also applied FPrep on the FAM95 dataset http://www.stat.ucla.edu/data/fpp transactions. Of the 23 attributes in the dataset, we have used the first 18, of which six are quantitative and the rest are binary. For each of the six quantitative attributes, we have generated fuzzy partitions using FPrep. A thorough analysis with respect to execution times, has already been performed on the USCensus1990raw dataset \(which is manifolds bigger in size than the FAM95 dataset both on the basis of number of transactions and number of unique values for numerical 


attributes dataset has been done solely to provide further evidence of the quality and semantics of the fuzzy partitions generated by FPrep. The details of the same are in Table IV. In this case, the number of fuzzy partitions is different for different numerical attributes. Thus, the number and type of fuzzy partitions to be generated is totally dependent on the attribute under consideration. A graphical representation of the fuzzy partitions generated for the attribute Age has already been provided in fig. 5, and clearly shows the Gaussian nature of the fuzzy partitions. The nature and shapes of fuzzy partitions for the rest of the attributes are also similar. Last, the number of frequent itemsets generated for different minimum support values is illustrated in fig. 8  C. Analysis of Results With FPrep, we can analyze and zero in on the number and type of partitions required based on the semantics of the numerical attributes, which the methods detailed in [19 20] do not necessarily facilitate. Then, FPrep, backed by FCM clustering, takes care of the creating the fuzzy partitions, especially assigning membership values for each numerical data point in each fuzzy partition. In section 8.A we have already shown that FPrep is nearly 9 to 44 times faster than the CURE-based method, and 2672 to 13005 times faster than the CLARANS-based method. FPrep is not only much faster than other related methods, but also generates very high quality fuzzy partitions \(Table III and IV much user-intervention. We have created a standard way of representing any fuzzy dataset \(converted from any type of crisp dataset efficacy of the same is corroborated by the successful implementation of Fuzzy Apriori and Fuzzy ARMOR on the fuzzy dataset \(converted from crisp version of FAM95 dataset an initial implementation of Fuzzy ARMOR, are very encouraging. FPrep, when used in conjunction with these fuzzy ARM algorithms, generates a pretty good number of high-quality frequent itemsets \(fig. 8 frequent itemsets generated for a particular minimum support is same, irrespective of the fuzzy ARM algorithm 


used IX. CONCLUSIONS In this paper we have highlighted our methodology, called FPrep, for ARM in a fuzzy scenario. FPrep is meant for seamlessly and holistically transforming a crisp dataset into a fuzzy dataset such that it can drive a subsequent fuzzy ARM process. It does not rely on any non-fuzzy techniques and is thus more straightforward, fast, and consistent. It facilitates user-friendly automation of fuzzy dataset 1 0 1 2 3 4 5 Age - 91 Hours - 100 Income3 4949 Income2 13707 Income1 55089 Ti m e lo g1 0 se co nd s Numerical Attribute - Number of Unique Values FCM CURE CLARANS 0 500 1000 1500 2000 2500 


3000 0.075 0.1 0.15 0.2 0.25 0.3 0.35 0.4 N o o f F re qu en t I te m se ts Minimum Support USCensus1990 FAM95 generation through FCM, and subsequent steps in preprocessing with very less manual intervention and as simple and straightforward manner as possible. This methodology involves two distinct steps, namely creation of appropriate fuzzy partitions using fuzzy clustering and creation of fuzzy records, using these partitions, to get the fuzzy dataset from the original crisp dataset FPrep has been compared with other such techniques, and has been found to better on the basis of speed. We also illustrate its efficacy on the basis of quality of fuzzy partitions generated and the number of itemsets mined by a fuzzy ARM algorithm which is preceded by FPrep. This preprocessing technique provides us with a standard method of fuzzy data \(record that it is useful for any kind of fuzzy ARM algorithm irrespective of how the algorithm works. Furthermore, this pre-processing methodology has been adequately tested with two disparate fuzzy ARM algorithms, Fuzzy Apriori and Fuzzy ARMOR, and would also work fine with other fuzzy ARM algorithm REFERENCES 1] Zadeh, L. A.: Fuzzy sets. Inf. Control, 8, 338358 \(1965 2] Chen G., Yan P., Kerre E.E.: Computationally Efficient Mining for Fuzzy Implication-Based Association Rules in Quantitative Databases. International Journal of General Systems, 33, 163-182 


2004 3] Hllermeier, E.: Fuzzy methods in machine learning and data mining Status and prospects. Fuzzy Sets and Systems. 156, 387-406 \(2005 4] De Cock, M., Cornelis, C., Kerre, E.E.: Fuzzy Association Rules: A Two-Sided Approach. In: FIP, pp 385-390 \(2003 5] Yan, P., Chen, G., Cornelis, C., De Cock, M., Kerre, E.E.: Mining Positive and Negative Fuzzy Association Rules. In: KES, pp. 270-276 Springer \(2004 6] De Cock, M., Cornelis, C., Kerre, E.E.: Elicitation of fuzzy association rules from positive and negative examples. Fuzzy Sets and Systems, 149, 7385 \(2005 7] Verlinde, H., De Cock, M., Boute, R.: Fuzzy Versus Quantitative Association Rules: A Fair Data-Driven Comparison. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics 36, 679-683 \(2006 8] Dubois, D., Hllermeier, E., Prade, H.: A systematic approach to the assessment of fuzzy association rules. Data Min. Knowl. Discov., 13 167-192 \(2006 9] Dubois, D., Hllermeier, E., Prade, H.: A Note on Quality Measures for Fuzzy Association Rules. In: IFSA, pp. 346-353. Springer-Verlag 2003 10] Hllermeier, E., Yi, Y.: In Defense of Fuzzy Association Analysis IEEE Transactions on Systems, Man, and Cybernetics - Part B Cybernetics, 37, 1039-1043 \(2007 11] Agrawal, R., Imielinski, T., Swami, A.N.: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Record, 22, 207216 \(1993 12]  Agrawal, R., Srikant, R.: Fast Algorithms for Mining Association Rules. In: VLDB, pp. 487-99. Morgan Kaufmann \(1994 13] Han, J., Pei, J., Yin, Y.: Mining Frequent Patterns without Candidate Generation. In: SIGMOD Conference, pp. 1-12. ACM Press \(2000 14] Han, J., Pei, J., Yin, Y., Mao, R.: Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach. Data Mining and Knowledge Discovery, 8, 5387 \(2004 15] Pudi V., Haritsa J.R.: ARMOR: Association Rule Mining based on Oracle. CEUR Workshop Proceedings, 90 \(2003 16] Dunn, J. C.: A Fuzzy Relative of the ISODATA Process and its Use in Detecting Compact, Well Separated Clusters. J. Cyber., 3, 32-57 1974 17] Hoppner, F., Klawonn, F., Kruse, R, Runkler, T.: Fuzzy Cluster Analysis, Methods for Classification, Data Analysis and Image Recognition. Wiley, New York \(1999 


18] Bezdek J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Kluwer Academic Publishers, Norwell, MA \(1981 19] Fu, A.W., Wong, M.H., Sze, S.C., Wong, W.C., Wong, W.L., Yu W.K. Finding Fuzzy Sets for the Mining of Fuzzy Association Rules for Numerical Attributes. In: IDEAL, pp. 263-268. Springer \(1998 20] Kaya, M., Alhajj, R., Polat, F., Arslan, A: Efficient Automated Mining of Fuzzy Association Rules. In: DEXA, pp. 133-142. Springer \(2002 21] Mangalampalli, A., Pudi, V. Fuzzy Association Rule Mining Algorithm for Fast and Efficient Performance on Very Large Datasets In FUZZ-IEEE, pp. 1163-1168. IEEE \(2009 22] Kaya, M., Alhajj. Integrating Multi-Objective Genetic Algorithms into Clustering for Fuzzy Association Rules Mining. In ICDM, pp. 431434. IEEE \(2004  Table II. Algorithm, numerical attribute comparison based on speed \(seconds  Algorithm Age - 91 Hours - 100 Income3 - 4949 Income2 - 13707 Income1 - 55089 FCM 0.27 0.3 3.13 6.28 79.4 CURE 0.25 0.25 28.67 163.19 3614.13 CLARANS 1.3 1.34 8363.53 78030.3 Table III. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions Age Old Middle Aged Young Hours More Average Less Income1 High Medium Low Income2 High Medium Low Income3 High Medium Low  Table IV. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions AGE Very old Around 25 Around 50 Around 65 Around 35 HOURS Very High Zero Around 40 Around 25 INCHEAD Very less Around 30K Around 50K Around 100K INCFAM Around 60K Around 152K Around 96K Around 31K Around 8K TAXINC Around 50K Around 95K Around 20K Very less FTAX Around 15K Very less Around 6K Very high Around 33K  


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


