A Framework for Weighted Association Rule Mining from Boolean and Fuzzy Data   Li Guang-yuan  School of Computer and Information Engineering  Guangxi Teachers Education University Nanning, China guangyuanli2004@163.com Hu Qin-bin  School of Computer and Information Engineering  Guangxi Teachers Education University Nanning, China huqinbin@126.com   Abstract 227Association rules mining is one of the most important tasks in the field of data mining. It aims at searching for interesting relationship among items in a large data set. In this paper, we present a novel approach for mining the fuzzy 
weighted association rule from boolean and fuzzy data in large data set, where a weighted value is assigned to each item, we develop a novel approach to calculate the support and confidence of the weighted items, experimental results show that the proposed method is efficient and scalable  Keywords-association rule mining; fuzzy; fuzzy association rule weighted I   I NTRODUCTION  Association rule mining\(ARM\ searches for interesting relationship among items in a large data set. Market basket analysis, a typical example of ARM, analyzes buying habit of customers by finding association between the different items 
that customers put in their shopping cart \(basket\. Apriori algorithm is an influential algorithm for mining frequent itemset for generating association rules, and most of existing algorithms are based on it. Typically, most algorithms first generates all large \(frequent\ item-sets from which association rule sets are derived, finding the large item-sets is a time consuming, so prune techniques are needed to limit the search space for finding large item-sets Unlike the classical ARM, fuzzy ARM was proposed using fuzzy sets such that quantitative and categorical attributes can be handled   A  f u zzy qu an ti t ativ e ru le re p r es en ts ea ch item  as  item, value\ pair. Fuzzy association rules are expressed in the 
 and 
002  I Y 
following form  If  X is A satisfies Y is B, For example, if \(age is young salary is low Given a database T attributes I with item-sets I X 
 and   21 n yyyY 
 YX we can define fuzzy sets 2,1 
002 and  21 n xxxX 
fxnfxfxA 
  and  2,1 fxnfxfxB 
 Associated to X and Y respectively In classical ARM, it is assumed that all of the items have the same importance, the weight of each item in the data set is the same\(weight=1\, but there are not always the case. Take the shopping in the supermarket for example, we may not consider the rings and dentifrices as the same important goods, because the rings are usually more profit per unit sale that that of dentifrice. According to the importance of each item, a weight is assigned to each item, so, weighted ARM had been proposed 
II  R ELATED WORKS  In weighted fuzzy ARM, there are two approaches for analyzing data sets with weighted settings prean d pos t processing 
pre- and post-processing. Post processing handles firstly the non-weighted problem \(weights=1\ and Then perform the pruning process later. Pre-processing prunes the non-frequent item-sets after each of iteration using weights. In pre-processed classical ARM, item-sets are pruned by checking frequent ones against we ighted support after every scan. This results in less rules being produced as compared to post processing because many potential frequent super sets are missed  o s t proces s i ng mode l i s propos ed. Tw o al gori t h ms  were proposed to mine item-sets with normalized and un 
normalized weights. The k-support bound metric was used to ensure validity of the \221\221downward closure property\(DCP\\222 but still there is no guarantee that every subset of a frequent set will be frequent unless the k-support bound value of \(k-1 subsets was higher than \(k\ An efficient mining methodology for Weighted Association Rules \(WAR\s proposed in A  Numerical attribute was assigned for each item where the weight of the item was defined as part of a particular weight domain. Similar techniques for weighted fuzzy quantitative ARM are presented in [5,6,7 In   a two-fol d pre p r oces s i ng  approach is used where quantitative attributes are discrete into different fuzzy linguistic intervals and weights assigned to each linguistic label. A mining al gorithm is applied then on the resulting dataset by applying two support measures for 
normalized and un-normalized cases novel ap pro ach  is presented for mining weighted association rules from binary and fuzzy data, it solve the problem of DCP in weighted ARM with the novel approach to calculate the support and confidence of the weighted items, but we think it had two drawbacks , one is that it could be very sensitive as the support threshold vary for the generation of the frequent item-sets another drawback is that the approach is unreasonable for some item-sets, for example, there are two item-sets: \223ab\224 and 223cdef\224, suppose the weight of each item is the same\(weight=0.5\he support threshold is 0.3%, the total 978-1-4244-7255-0/11/$26.00 \2512011 IEEE 


number of transactions in data set is 1000000, so, according to the approach present in [2 m set  a b  is f r eq u e n t if its  support count is equal to or more than  12000, but to the itemset “cdef ”, it is not  frequent unless its support count equal to or exceed 48000, that seems to be unreasonable for this case III  PROBLEM  DEFINITION the problem definition consists of terms and basic concepts to define item’s weight, item-set transaction weight, weighted support and weighted confidence for both boolean  and  fuzzy data. In the following, we discuss the problem for fuzzy ARM mining for the two types of data respectively, and it can be used in the real data set which consists of these data A  Boolean Weighted Association Rule Mining Let the input data D have transactions      2 1 n t t t T  with a set of items      2 1 m i i i I  a set of positive real number weights     2 1 m w w w W  which is attached to each item i Each th i transaction i t is some subset of I and a weigh w is attached to each item   j i i t  th j item in the th i  transaction\, thus each item j i will have associated with it a weight corresponding to the set W weight for the th j item in the th i transaction is given by    w i t j i We use table 1 and table 2 to illustrate the following definitions and some other issues Definition 1 2 Item-set Weight IW is a non-negative real value given to each item j i ranging w i t h s o m e deg r ee of  importance, a weight  w i j  Definition 2. Item-set Transaction Weight ITW is the aggregated weight of all the items in the item-set present in a single transaction. Item-set transaction weight for an item-set X can calculated as:  vote for i t satisfying X    X w i t X ITW k i X k X w i      1         1 Item-set transaction weight of item-set\(A,B\ is calculated as 75  0 2 9  0 6  0       B A ITW  Definition 3. Weighted Support WS is the aggregated sum of item-set transaction weight ITW of all the transactions in which item-set is present, divided by the total number of transactions. It is calculated as n X w i t X WS n i k i X k X w i      1 1           2 WS of item-set\(A,B\ is calculated as: 0.7 3/8=0.256 Definition 4. Weighted Confidence WC is the ratio of sum of voets satisfying both Y X to the sum of votes satisfying X It is formulated\(with  Y X Z  as           n i X k k i X w i Z k k i z w z X w x t Z w z t X WS Z WS Y X WC 1 1      1                 3 Weighted Confidence of item-set \(A,B\ calculated as 937  0 4 6  0 3 75  0        B A WC  TABLE I  TRANSACTIONAL DATABASE  T Items T Items t1 ABCD t5 ABDE t2  BD t6 ABCDE t3 AD t7 DE t4 C t8 BCE TABLE II  ITEMS  WITH  WEIGHTS Items i Weights IW  A 0.60 B  0.90 C 0.30 D 0.10 E 0.20  B  Fuzzy Weighted AssociationRule Mining A fuzzy dataset  D consists of fuzzy transactions        2  1  n t t t T  with fuzzy sets associated with each item in      2 1 m i i i I  which is identified by a set of linguistic labe      2 1 l l l l L  for example  arg    e l medium small L   we assign a weight w to each l in L associated with i Each attribute    j i i t is associated to some degree with several fuzzy sets. The degree of association is given by a membership degree in the range[0,1 w h i c h in d i c a t e s th e c o r r e sp o n d e n c e  between the value of a value of a given   j i i t and the set of fuzzy linguistic labels. The th k weighted fuzzy set for the  th j item in the th i fuzzy transaction is given by      w l i t k j i We use table 3 and 4 to illustrate the fuzzy ARM definition terms and concepts Definition 5. Fuzzy Item Weight FIW is a value attached with each fuzzy set. It is a non-negative real number value in 


0,1 W e i ght o f a fuzz y se t for  a n  i t e m  j i is denoted as    w l i k j  Definition 6. Fuzzy Item-set Transaction Weight FITW is the aggregated weights of all the fuzzy sets associated with items in the item-set present in a single transaction. Fuzzy item-set transaction weight for an item-set    A X can be calculated as Vote for  i t satisfying  L w l i t X L k k j i X w l i     1            4 Y,Small denoted by\(X,Medium\as A and \(Y, Small\as B. Fuzzy itemset transaction weight FITW of item-set\(A,B\ in transaction 1 is calculated as  225  0 2  5  0 2  0   7  0 5  0          B A FITW  Definition 7. Fuzzy Weighted Support FWS is the aggregated sum of FITW of all the transaction’s item-sets present divided by the total number of transactions, represented as n L w l i t X FWS n i L k k j i w l i    11             5 FWS of item-set\(A,B\ is calculated as 188  0 4 755  0      B A FWS  Definition 8. Fuzzy Weighted Confidence FWC is the ratio of sum of votes satisfying both Y X to the sum of votes satisfying X and Y X Z  and given as           n i X k k j i X w l i Z k k j i Z w l i X w l i t Z w l i t X FWS Z FWS Y X FWC 1 1        1                      6 FWC of item-set\(A,B\ calculated as 188  0 4 188  0      B A FWS   TABLE III  F URRY TRANSACTION DATABASE  Tid X Y Small Medium Small Medium t1 0.5 0.5 0.2 0.8 t2  0.9 0.1 0.4 0.6 t3 1.0 0.0 0.1 0.9 44 0.3 0.7 0.5 0.5  TABLE IV  ITEMS  WITH  WEIGHTS Fuzzy Items   l i  Weights IW  X, Small 0.9 X, Medium  0.7 Y, Small 0.5 Y, Medium 0.3  IV  EXPERIMENTAL  RESULTS To demonstrate the effectiveness of the proposed approach we had performed several experiments compared with other similar approach, experiments were done on data set which was described in [2 e i ght s w e r e ge ner a t e d r a n d o m l y a n d assigned to all items in the data set to show their importance they were undertaken using two different association rule mining techniques, namely classical fuzzy ARM and FWARM present in   T h e r e s u l t s ar e as fo ll o w s    From the experimental results showed, we can see that the proposed approach had been generated more frequent item-sets and rules than that of the classical ARM and FWARM done the reason is that compared with the other two methods, the proposed method can yield a relatively little variation of the                 


weight Item-set Transaction between item-sets. The proposed method and FWARM have higher execution time because they deal with the fuzzy data with the way as discussed previously  V  CONCLUSIONS  In this paper, we have presented a novel framework for the weighted fuzzy association rule mining from Boolean and fuzzy data. we defined a novel way to calculate the item\222s weight, item-set transaction weight, fuzzy weighted support and fuzzy weighted confidence, experimental results show that the proposed method is efficient and scalable  A CKNOWLEDGMENT  This work has been supported by the fund for scientific research in Guangxi Teachers University , Guangxi, China R EFERENCES   1 K u o k  C M., F u A W o ng M.H 223 M i n i n g f u z z y asso ciat io n r u l e s in  databases,\224  SIGMOD Record 27\(1\, pp. 41\22646,  1998 2 M S u lai m an K h an Ma y b i n Mu y e b a  an d F r a n s  C o en en  223W ei gh t e d association rule mining from binary and fuzzy data,\224 ICDM, pp. 200-212 2008 3  Ca i, C  H F u A  W  C., Che n g  C.H  an d K w o n g  W  W 223 M ini n g  association rules with weighted items,\224. In: Proceedings of Intl. Database Engineering and Applications Symposium \(IDEAS 1998\, Cardiff Wales, UK, July 1998, pp. 68\22677  1998 4  Wang, W., Yang, J., and Yu, P.S., \223Efficient mining of weighted association rules,\224  In: Proceedings of the KDD, Boston, August, pp 270\226274  2000 5 W a ng  B.Y   Z h ang  S  M 223 A Min ing  al g o r ithm f o r f u z z y  w e ighte d  association rules,\224 In: IEEE Conference on Machine Learning and Cybernetics, vol. 4, pp. 2495\2262499 , 2003 6 S h u Y J   T s an g E    a n d Yeung D S  223M in in g fu zz y a s s o c i a t i o n r u les  with weighted items,\224 In: IEEE International Conference on Systems Man, and Cybernetics , 2000 7 L u J  J   Mi ni ng Bo o l e a n a n d G e ne r a l F u zzy W e ig hte d A s s o ciat io n Ru l e s  in Databases. Systems Engineering-Theory & Practice 2, pp. 28\22632 2002 8  G y en es ei  A   223 Mi ni n g w e i g h t ed a s s o ci at i o n r u les f o r f u zz y q u an t i t a ti ve  Items,\224  In:Proceedings of PKDD Conference, pp. 416\226423 , 2000              


projection are employed. Although GC tree algorithm solves the problem of complicated construction, the construction of left skew tree still wastes the memory space III. SORTED COMPRESS TREE ALGORITHM In this paper, we propose a new algorithm called Sorted Compressed Tree \(SC Tree problems suffered in CATS and GC tree algorithm. We focus on two aspects. One is to simplify the process of tree construction, and another is to simplify the rule mining method SC tree algorithm mines the frequent itemset in three stages 1 2 3 mining in SC tree. These stages will be explained in the followings Identify applicable sponsor/s here. \(sponsors 329 A. Data Preprocessing Sorting brings benefits for mining as it in the binary searching tree. In our previous work proposed in 2007 [6], we actually improved the Apriori algorithm with sorting technique Arrangement of sorted transactions is consistent, so it is unnecessary to adjust the tree structure as in CATS tree Efficiency of tree constructing and rule mining can be improved. Searching space can be reduced because useless nodes can be skipped by comparing their indexes For a better performance and consistent arrangement, items in SC tree will be sorted by their occurred frequency in descendent. The preprocessing is shown in Fig. 2 1 2 items. Create a frequency table to record the occurred frequency of each item 3 decreasing order and build an index table. This new frequency table will be treated as Header table in the mining process 4 C will be translated to index 1 and item A be translated to index 3 5 Finally, the new transaction database is obtained B. SC Tree Constructing After Data Preprocessing stage, we obtain a sorted 


transaction database and then start to construct the SC tree. The constructing process is similar to FP-growth tree. An example of constructing SC tree for TID 1 and TID 2 is shown in Fig. 3 1 2 branch in SC tree 3 in SC tree, merge it and accumulate its count. Otherwise create a new branch. In TID 2, items 1, 2 and 3 already exist in SC tree, so their counts will be accumulated. Then a new branch beneath the last common item 3 is created for the remaining part 4 compressed into the tree structure. The SC tree with the five transactions is shown in Fig. 4 By the way, a Header table will be constructed. Each item in Header table will be linked to the corresponding node where it was first occurred in SC tree. The nodes with same index value \(item name linked list. The linked list will be used to mining frequent itemset in different branches C. Mining in SC Tree Nodes \(items direction in SC tree can be single direction. In order to satisfy the constraint of minimum support \(min_sup direction in SC tree is bottom up. The mining processes in SC tree are Figure 2. Database pre-processing in SC tree algorithm Figure 3. Insert TID 1 and TID 2 in SC tree Figure 4. Example of SC Tree 330 1 According to the given minimum support value, SC tree will be scanned by mining algorithm and items whose count is larger than or equal to the minimum support constraint min_sup than the min_sup will not be considered and their linkages will also be ignored. Because these linkages are ignored, the items will be filtered out in mining process. For instant, a Header table with min_sup?3 is shown in Fig. 5 2 In order to mine the frequent itemset in SC tree, each item 


whose counts is large than or equal to min_sup will be considered. The conditional condensed SC tree of item X will be constructed. Item X in SC tree will be found by the linkage recorded in Header table. Then the path along item X to the root will be constructed. Because all the items with same index item name item X in different branch will be constructed either. By the way, a Header table for mining process will be created An example of mining frequent itemset including item 6 with min_sup?3 is shown in Fig. 6. There are two paths, \(6, 5 3, 2, 1 6, 4, 1 these two paths, counts of items along the path will be recorded in a Mining_Header table 3 In mining table, a node whose count is less than the min_sup will be deleted. Then, a conditional SC tree of eliminated itemset will be created. Finally, the preserved paths are the frequent itemsets As shown in Fig. 6, nodes whose count is less than three will be deleted. Only item 6 and item 1 are preserved but others are deleted. Beneath item 1, there are two preserved items \(6:2 6:1 1:3 6:3 with min_sup?3 is \(1, 6 Stage \(2 3 the items in Header table whose count is large than or the same as min_sup are considered Figure 5. A Header table with min_sup?3 Figure 6. . Frequent itemset mining with min_sup?3 and including item 6 IV. EXPERIMENT AND EVALUATION In this section, we will verify the performance of SC tree CATS tree, and GC tree algorithm. We will focus on executing performance and memory requirement in different minimum support and transaction size. The equipment we used is a PC with Pentium IV 2.8GHz processor with 2GB memory. Testing data was generated by data generator from IBMs Almaden project [12]. The meaning of parameters is shown as follows N : Number of data items D : Number of transactions in the database T : Average size of transactions I : Average size of the maximal potentially frequent itemset L : Number of maximal potentially frequent itemset 


A. Efficiency Evaluation For evaluating the performance of three algorithms, the experiments focus on two aspects 1 There are 100K transactions and average size \(length transactions is 10 and average size of the maximal potentially frequent itemset is 4. The result is shown as Fig. 7 These algorithms are minimum support independent, so the results are not influenced by different minimum supports. The constructing time of SC tree is shorter than CATS tree and GC tree. Then we evaluate the influences of different transaction sizes with a fixed minimum support 0.4%. As shown in Fig. 8 SC tree still outperforms the other two algorithms 0 1 2 3 4 5 6 0.20% 0.30% 0.40% 0.50% 0.60 Minimum Support Ex ec uti ng T im e se c CATS Tree GC Tree SC Tree Figure 7. Tree construction time with different minimum support in T10I4D100K database 331 0 20 40 60 80 100 100K 250K 500K 750K 1000K 


Transaction Size E xe cu tin g T im e s ec  CATS Tree GC Tree SC Tree Figure 8.  Tree construction time with fixed minimum support 0.4% in T10I4D100K~1000K database 2 In this part, we focus on the efficiency of mining process Three algorithms are verified with different minimum supports under 500K transactions and different transaction sizes under fixed minimum support 0.2%. The results are shown in Fig. 9 and Fig. 10 As shown in Fig. 9 and Fig. 10, the average mining time of SC tree is always much less than the CATS tree and GC tree algorithm. By concluding the experiments in this part, we can say that SC tree algorithm outperforms CATS tree and GC tree algorithm, especially in low minimum support cases 0 2 4 6 8 10 12 14 16 18 0.20% 0.30% 0.40% 0.50% 0.60 Minimum Support E xe cu tin 


g T im e s ec  CATS Tree GC Tree SC Tree Figure 9. Mining efficiency with different minimum support in T10I4D500K database 0 10 20 30 40 100K 250K 500K 750K 1000K Transaction Size Ex ec ut in g Ti m e s ec  CATS Tree GC Tree SC Tree Figure 10. Mining efficiency with different transaction size and fixed minimum support 0.2 B. Memory Requirements Evaluation For evaluating the memory requirements of three algorithms, the experiments focus on two aspects 1 We verify the memory requirements of tree construction among three algorithms with different minimum supports in T10I4D100K database. The result is shown in Fig. 11 As shown in Fig. 11, the results are not influenced by different minimum supports because theses algorithms are minimum support value independent. Nevertheless, for constructing T10I4D100K data set, CATS tree spent 15,654KB 


GC tree spent 19,881KB, and SC tree only spent 13,030KB SC tree algorithm saves about 17% and 34% than CATS tree and GC tree Then, we evaluate the influences of different transaction sizes with a fixed minimum support 0.4%. As shown in Fig. 12 we can find that the average constructing space for SC tree saves about 16% and 34% than CATS tree and GC tree 2 In this part, we focus on memory requirements of mining process. Three algorithms will be verified with different minimum support under 500K transactions and different transaction sizes under fixed minimum support 0.2%. The results are shown in Fig. 13 and Fig. 14 Obviously, the average mining space for SC tree is less than both CATS tree and GC tree 0 5000 10000 15000 20000 25000 0.20% 0.30% 0.40% 0.50% 0.60 Minimum Support M em or y Sp ac e K B CATS Tree GC Tree SC Tree Figure 11. Tree construction space requirement with different minimum support in T10I4D100K database 0 50000 100000 150000 200000 100K 250K 500K 750K 1000K Transaction Size 


M em or y Sp ac e K B  CATS Tree GC Tree SC Tree Figure 12. Tree construction space requirement with fixed minimum support 0.4% in T10I4D100K~1000K database 332 0 200 400 600 800 1000 1200 0.20% 0.30% 0.40% 0.50% 0.60 Minimum Support M em or y Sp ac e K B  CATS Tree GC Tree SC Tree Figure 13. Memory requirements under different minimum support in T10I4D500K database 0 200 400 600 800 1000 


1200 1400 100K 250K 500K 750K 1000K Transaction Size M em or y Sp ac e K B CATS Tree GC Tree SC Tree Figure 14. Memory requirements with fixed minimum support 0.2 T10I4D100K~1000K database V. CONCLUSION AND FURTURE WORKS In this paper, a new tree structure called SC tree and a mining algorithm for association rule are proposed. Advantages of several algorithms are combined in this algorithm, and experimental results also show that the proposed algorithm outperforms CATS tree and GC tree mining algorithms For the SC tree algorithm, the mining model \(tree structure does not need to be re-constructed while the minimum support is changed. This property enables users to adjust the minimum support dynamically to find out satisfied large itemsets Because the database is sorted before mining, the SC tree can be mined only from the bottom-up direction, which is quite different from the bi-directional mining in CATS tree and thus the mining time for SC tree is shorter than CATS tree Moreover, the arrangement of sorted transactions is consistent so it is unnecessary for SC tree to adjust the tree structure as in CATS tree. That further improves the efficiency of tree construction in SC tree In addition, the memory requirements for constructing and mining the tree structure of SC tree are also less than both the CATS tree algorithm and the GC tree algorithm. Experimental results show that SC tree algorithm saves 16%~ 34% of memory space Although SC tree algorithm is more efficient than the other approaches, there are still some aspects can be considered for further improvement. One is the process of pre-sorting. When 


the data set is updated, the whole database has to be re-sorted If only partial tree structure rather than the whole tree structure is to be adjusted, execution time can be reduced substantially Another aspect is the scalability. When applying the SC tree algorithm in a large database, employing the technique of parallel and distributed computing can not only lightens the loading of processers and the requirement memory space but also suits for the architecture of disturbed computing environment in enterprises REFFERENCE 1] Peng, Y., Kou, G., Shi, Y., and Chen, Z., A Descriptive Framework for the Field of Data Mining and Knowledge Discovery , International Journal of Information Technology and Decision Making, Vol. 7, Issue: 4, Page 639 - 682, 2008 2] R. Agrawal & R. Srikant, Fast Algorithms for Mining Association Rules, Proceedings of the 20th VLDB Conference Santiago, pp. 487-499, September 1994 3] J. Han, J. Pei, Y. Yin and R. Mao, et al., Mining frequent patterns without candidate generation: A frequent-pattern tree approach, Data Mining and Knowledge Discovery, vol. 8, no. 1 2004, pp. 53-87 4] W. Cheung and O.R. Zaiane, Incremental mining of frequent patterns without candidate generation or support constraint Citeseer, 2003, pp. 111-116 5] G. J. Hwang, W. F. Tsai and J. C. R. Tseng, A Minimal Perfect Hashing Approach for Mining Association Rules from Very Large Databases, Proc. The 6th IASTED International Conference on Internet and Multimedia Systems and Applications, 2002, pp. 80-85 6] C.-K. Chiou, and J. C. R. Tseng, A Scalable Association Rules Mining Algorithm Based on Sorting, Indexing and Triming Proc. International Conference on Machine Learning and Cybernetics, 2007, pp. 2257-2262 7] W. Pei-ji, S. Lin, B. Jin-niu and Z. Yu-lin, Mining Association Rules Based on Apriori Algorithm and Application, Proc International Forum on Computer Science-Technology and Applications, 2009, pp. 141-143 8] J. Pei, J. Han, B. Mortazavi-Asl, J. Wang, H. Pinto, Q. Chen, U Dayal and M. C. Hsu, Mining sequential patterns by patterngrowth: The prefixspan approach, IEEE Transactions on Knowledge and Data Engineering, 2004, pp. 1424-1440 9] G. Grahne and J. Zhu, Fast algorithms for frequent itemset 


mining using FP-trees, IEEE Transactions on Knowledge and data Engineering, vol. 17, no. 10, 2005, pp. 1347-1362 10] L. Qihua, Z. Defu and W. Bo, A New Algorithm for Frequent Itemsets Mining Based on Apriori and FP-Tree, Proc. WRI Global Congress on Intelligent Systems, 2009, pp. 360-364 11] S.Y. Liu, An Efficiency Incremental Mining with Grouping Compress Tree, Unpublished masters thesis, National Central University Taoyuan Country, Taiwan, 2004 12] IBM Almaden Research Center http://www.almaden.ibm.com/cs/disciplines/iis 13] S. K. Tanbeer, C. F. Ahmed and J. Byeong-Soo., Parallel and Distributed Frequent Pattern Mining in Large Databases, Proc 11th IEEE International Conference on High Performance Computing and Communications, 2009, pp. 407-414 333 


ec  d Fig. 5: Computation Performance Comparison Tab. 4: Computation Savings by TOP-MATA K Connect K Retail K Wap La12 50 58.35% 100 0.01% 200 0.83% 23.04 150 55.91% 400 2.65% 400 30.12% 45.38 250 53.61% 700 1.84% 800 20.03% 25.95 350 48.28% 1100 3.95% 1600 13.06% 27.89 450 43.12% 1400 1.48% 3200 6.14% 12.70 550 39.36% 1700 4.00% 6400 5.63% 7.11 Second, Fig. 5 shows the results of four data sets computed by TOP-MATA and TOP-DATA, respectively. As can be seen, in general, TOP-MATA shows a better performance than TOP-DATA. And as the increase of the ? value, the advantage tends to be even more impressive for these four data sets 4.3. The Computation Saving of TOP-MATA As can be seen in the Tab. 4, four data sets, enjoy signi?cant computation savings brought by TOP-MATA. We can conclude that the computation saving is a major factor for the performance of TOP-MATA. That is, compared with TOP-DATA, a higher computation saving implies a much better performance of TOP-MATA. Since this saving is more signi?cant as the increase of the items, TOP-MATA works better for large scale data sets with a large number of items 5. Conclusion In this paper, we studied the problem of searching for top? item pairs with the highest cosine values among all item pairs. Speci?cally, we provided a novel algorithm TOPMATA which employ a Max-First traversal strategy for ef?ciently performing top-? cosine similarity search. Extensive experimental results veri?ed the effectiveness of the algorithms, And TOP-MATA algorithm is superior to TOPDATA for large-scale data sets with multiple items Acknowledgment This research was partially supported by the National Natural Science Foundation of China \(NSFC No. 70901002 and the Ph.D. Programs Foundation of Ministry of Education of China \(No. 20091102120014 


REFERENCES 1] R. Agrawal, T. Imielinski, and A. Swami, Mining association rules between sets of items in large databases, in SIGMOD 1993 2] C. Alexander, Market Models: A Guide to Financial Data Analysis. John Wiley & Sons, 2001 3] W. Kuo, T.-K. Jensen, A. Butte, L. Ohno-Machado and I. Kohane, Analysis of matched mrna measurements from two different microarray technologies Bioinformatics, vol. 18, p. 405C412, 2002 4] H. Xiong, X. He, C. Ding, Y. Zhang, V. Kumar, and S. Holbrook, Identi?cation of functional modules in protein complexes via hyperclique pattern discovery in PSB, 2005 5] J. Han, H. Cheng, D. Xin, and X. Yan, Frequent pattern mining: Current status and future directions DMKD, vol. 15, no. 1, pp. 5586, 2007 6] P.-N. Tan, M. Steinbach, and V. Kumar, Introduction to Data Mining. Addison-Wesley, 2005 7] S. Brin, R. Motwani, and C. Silverstein, Beyond market basket: generalizing association rules to correlations, in SIGMOD 1997, Tucson, AZ, 1997, pp 265276 8] E. Omiecinski, Alternative interestmeasures formining associations, TKDE, vol. 15, pp. 5769, 2003 9] H. Xiong, S. Shekhar, P.-N. Tan, and V. Kumar Exploiting a support-based upper bound of pearsons correlation coef?cient for ef?ciently identifying strongly correlated pairs, in KDD 2004, 2004, pp 334343 10] I. Ilyas, V. Markl, P. Haas, P. Brown, and A. Aboulnaga, Cords: Automatic discovery of correlations and soft functional dependencies, in SIGMOD 2004 2004, pp. 647658 11] J. Zhang and J. Feigenbaum, Finding highly correlated pairs ef?ciently with powerful pruning, in CIKM 2006, 2006, pp. 152161 12] H. Xiong, W. Zhou, M. Brodie, and S. Ma, Top-k correlation computation, JOC, vol. 20, no. 4, pp 539552, 2008 13] S. Zhu, J. Wu, and G. Xia, Top-k cosine similarity interesting pairs search, in 


http://datamining.buaa.edu.cn/TopKCos.pdf 14] M. Zaki, Scalable algorithms for association mining, TKDE, vol. 12, pp. 372390, 2000 


enhance item-based collaborative filtering, in 2nd IASTED International Conference on Information and Knowledge Sharing, Scottsdale, Arizona, 2003 476 2010 10th International Conference on Intelligent Systems Design and Applications 


Basi Association Rles Basi c  Association R u les Association is basically connecting or tying up occurrences of Association is basically connecting or tying up occurrences of events Ol dib t f ilt t O n l y d escr ib e se t s o f s i mu lt aneous even t s Cannot describe patterns that iterate over time e g  itemset a  0  b  0  g    Eg If you sense higher data rates on the downlink than normal AND New Route generated Implies high chances of Intrusion AND New Route generated Implies high chances of Intrusion Associative IDS for NextGen Frameworks Dr S Dua LA Tech 20 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


