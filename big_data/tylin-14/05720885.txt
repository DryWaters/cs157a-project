Combination of Data Mining and Ant Colony Algorithm for Reactive Power Optimization Gong Jinxia, Xie Da, Zhang Yanchi, Jiang Chuanwen Shanghai Jiao Tong University, Shanghai, 200240, China jxgong@sjtu.edu.cn Abstract The management of reactive resources plays an important role in maintaining voltage stability and system reliability. This paper presents a new method to find the optimal solution to reactive regulation in power system, using the daily data collected in power substations. The new algorithm is combined with improved ant colony algorithm and Apriori data mining technique. The mathematic models of reactive optimization are described and applied to the reactive optimal compensation in an example electric system. Test results show that the application of the new algorithm 
proposed in this paper for determining the plan of reactive optimization operation can raise the system’s operation efficiency and reduce the power loss Keywords- Ant colony algorithm; Data mining; Reactive power optimization I I NTRODUCTION Electric power system is a large-scale nonlinear interconnected system. It is difficult to extract the useful information from the accumulated continuously running data for operators in power system. The data mining technique can take full advantage of these operating data to reveal the principles and rules that the power system contains through association analysis, classification and prediction, clustering analysis, outlier analysis, and so on 1-3 
Data mining technology has been applied in many fields such as credit card management, churning analysis and so on. Most researchers focus on the study of data mining models 4-6  The application of traditional data mining techniques is continually facing new challenges in power system because an ever increasing amount of data is still being produced at high rates in power system and the analyses of the data often needs to be conducted in real-time and under time constraints Ant colony algorithm \(ACA\ is a new method for solving the optimal combination problem 7 In recent years researches on the ant colony focus on improving the traditional ant colony algorithm, such as TSP optimal 
problems and its extended application of the ant colony algorithm to other areas, such as data mining and knowledge discovery 8. Paper [11  a d j u s t s t h e a n t  c o l ony phe r o m o n e  adaptively under the limitation of pheromone to further solve the stagnation problem and improve the searching ability of ACA. Paper a ppl ie s AC A to opti m iz e the  r a pid microgrid power management problem given complex constraints and objectives including: environmental fuel/resource availability, and economic considerations Reactive power plays an important role in supporting the real power flow by maintaining voltage stability and system reliability. The available reactive power capabilities of the system have to be optimally deployed so that bus voltages 
are kept within specified limits. The purpose of reactive power dispatch is to determine the proper amount and location of reactive support with several constraints. Paper   f o c u se s on t h e  volta g e  r e a c t i v e po we r p r oble m  ke e p in g  the real power flows fixed to values determined from a base case load flow analysis. In paper [1 o p t i m a l  po w e r  dispatch is solved by time-varying acceleration coefficients particle swarm optimization \(TVAC-PSO\. It proposes a comprehensive model for reactive power pricing in an ancillary services market. Paper [15  p r es en ts an  ef fi ci en t Genetic Algorithm \(GA\ based reactive power optimization approach to minimize the total support cost from generators and reactive compensators 
This paper focuses on the problem of extracting useful data for effective decision-making of reactive power optimization. It describes the concepts and improvements of association rules algorithm - Apriori algorithm and ant colony algorithm. The improved Apriori algorithm is applied to extract the useful information for the ACA from the large number of running data in the substation operation process The overall model based on Apriori algorithm and ant colony algorithm is established for reactive power optimization. An example power substation is used to illustrate the application of the proposed models in the voltage and reactive power automatic control system. Based on historical data, the proposed method is used to get the optimal operating conditions of the optimal solution to guide 
the practical operation II D ATA MINING A Principle of  Association Rules Method Association rules method is represented simply as B 002 A  Where I A 002  I B 002  003  004 B A  The support level of B 002 A is support B 002 A P\(A B 002 The confidence level of B 002 A is   _ sup   _ sup  
  A cofidence A count port B A count port A B P B 005   002 1 Where   _ sup B A count port 005 is the record number of the items which include B A 005    _ sup A count port is the record number of the items which include A 
2011 Third International Conference on Measuring Technology and Mechatronics Automation 978-0-7695-4296-6/11 $26.00 © 2011 IEEE DOI 10.1109/ICMTMA.2011.182 720 


The support level indicates the statistical importance of association rules in the whole data set. The confidence level indicates the credibility of the association rules. Generally the useful association rules are the ones with high support level and confidence level. The data mining process can be divided into two parts: \(i\ mining the large items set whose general support level is higher than the pre-set value; \(ii\ get the association rules whose support level is higher than the pre-set minimum support frequency B Improved Apriori Algorithm The Apriori algorithm proposed by Agrawal in year 1994 is recursive and includes two main steps i\ Get the frequent K item on the frequent K1\- item ii\ Calculate the support level of the candidate set on the database scanning and pattern matching It can be included that the candidate set is too large and the database is scanned repeatedly in the Apriori algorithm A improved method without these two drawbacks is applied to the data mining in the historical database of the substations. It is described as follow i\ Preprocess the original data based on partition. It divides the database of the substation into 9-zones according to the requirement of reactive power and bus voltages. Then it focuses on the data in the area except the normal running area. So it is time-saving and fast-accessing because it only scans the corresponding area in the database without scanning the whole database ii\ Classify with similarity search, according to central substation operation conditions. The association level of the selected data is improved to meet the requirements of practical operation III O NLINE O PTIMAL A LGRITHM AND O VERALL M ODEL A Model of Ant Colony Algorithm Let m be the number of the ants   b i t the number of the ants at moment t and element i    ij t 006 the information in path i  j at moment t  ij d  i,j 1,2,…, n\ the distance between cities i and j At the beginning C   0  ij 006 C is constant When a ant k  k 1,2 002 m\ is moving, it collects the information in the path to choose the next path. The state transition probability of ant’s shift from city i to city j at moment t is represented as 003 003 004 003 003 005 006 007    007 002 0               k allowed s is is ik k ij k ij allowed j t t t t t p k 003 010 011 010 011 012 006 012 006 2 Where allowd k 0,1 002 n-1}; tabu k represents the possible cities allowed to choose in the next step The artificial ants have the function of memory. Tabu k k=1,2 002 m\ecords the cities the ant has gone to in the last k steps. And it is updated dynamically as the evolutionary process. After a circle with n times, the ant passes all the cities. Each path traversed by an ant is a solution. The information in each path is updated as      1    t t n t ij ij ij 006 006 013 006 014        3 Where 007  014  014 m k k ij ij t t 1     006 006    4  1  0  002 013 is volatile factor 013  1 is information residual factor   t k ij 006 014 is the residual information between city i and city j and can be represented as 003 004 003 005 006  014 else  0  j  i path pass k ant  if     k k ij L Q t 006 5 Where Q indicates the pheromone intensity L k is the total length of the path the ant k passed in this cycle. After several cycles, the calculation ends based on the stop condition B Improvement of Ant Colony Algorithm The improvement of the ant colony includes i\ Selection of parameters: The parameters are dynamically adjusted. At the beginning, the parameters are set at a small value, to avoid "false positive feedback" and solution loss". When the calculation is running after a certain number of cycles, the parameters are increased to improve the solution quality ii\ Modification of the parameters: The state transition probability in \(2\ is modified according to the results of data mining. The higher the confidence level and the pheromone concentration are, the greater the probability that ants choose In the ant k passes path i  j    t k ij 006 014 is represented as   t k ij 006 014 004 k L p Q  1   6 Where p is the confidence level. The tabu table is established according to the results of data mining. And it is updated after each ant’s choice until the new optimal strategy is found iii\ Selection of paths: First, calculate the reactive power supplied by the capacitor sets in all the substations to establish all the working states. The probable strategies are found out when the reactive power shortfall is compared with the calculated reactive power. The strategies with great difference are aborted. Number the left states and find out the confidence level through data mining Second, the path selection strategy in the basic ant colony algorithm is adjusted. The probability of paths that ants choose is set as the confidence levels of the mined association rules. The tabu table of probable choice is listed The next path is calculated by the tabu table without randomness. And the original establishment of tabu table is related to the results of the offline data mining C Overall Model For a substation in centralized control mode in China, the proposed control strategy of switching capacitors for optimal 
721 


allocation of reactive power is described as Fig.1. First, it establishes the association rules of the central station and controlled stations based on historical databases.  Second, it compares the established results and the measured data Then it calculates the optimal solution according to evaluation function, namely, optimization goals 002\003\004\005\006\007\010\007 011\012\013\010\014\015\012\016\007\017\005\006\007\010\007 020\015\003\021\007\015\007\010\012\014\022\005\014\023\005 010\024\003\005\006\007\010\007 025\026\017\003\013\005\007\022\006\005 027\022\014\004\017\003\006\030\003 007\022\010\005\016\014\017\014\022\031\005 007\017\030\014\015\012\010\024\032 033\024\003\005\014\021\010\012\032\007\017\005 013\010\015\007\010\003\030\031 014\023\023\017\012\022\003 014\022\017\012\022\003 006\007\010\007\005\032\012\022\012\022\030\005\004\012\010\024\005 034\021\015\012\014\015\012    033\024\003\005\007\016\010\026\007\017\005 021\014\004\003\015\005\030\015\012\006 014\026\010\021\026\010  035\014\022\023\012\006\003\022\016\003 005\017\003\036\003\017 014\021\010\012\032\012\037\007\010\012\014\022\005 030\014\007\017\013 025\003\007\017 \010\012\032\003\005\016\014\017\017\003\016\010\003\006\005\006\007\010\007\005 Figure 1 Proposed strategy The proposed strategy can be divided into two parts offline and online. The input of the offline part is the historical databases and the output is the associate rule and the confidence level of the historical data calculated by the Apriori algorithm. The frequent items are mined according to the principle that their frequencies are not less than the pre-set minimum support frequency. Based on the frequent items, the corresponding strong association rules are gained Ant colony algorithm is used to find the optimal strategy of reactive power regulation, based on the output association rules of the offline part. And the renew output of the offline part interact with the online strategy D Target Function The power loss between two points i, j can be represented as i ij i i l U P f 2 010 010 011 012 013 013 014 015  011   7 Where ij P is the transported power between i and j  l i is the length of the transmission line i 011 is the related comprehensive coefficient The total power loss can be represented as 007   n i i f F 1 1    8 The node voltage deviation is sp j sp j j j U U U f 014   2    9 The total voltage deviation of all nodes is 007  014   n j sp j sp j j U U U F 1 2   10 Where n is the number of the nodes except the slack bus nodes sp j U is the set value of the node voltage sp j U 014 is the set value max deviation of the node voltage The mathematical model of the reactive power optimization can be represented as 007 007   k N k F C  F  min 2 2 1 1 011 011   11 Where 1 011 and 2 011 are the weight coefficients N k is a group of the numbers of the available capacitors T n e e e E     2 1  is the group of the states of the available capacitors f 1 and f 2 are the functions of  E  004 005 006  d disconecte  is  i capacitor 0 switched  is  i capacitor 1 i e  The constraints can be represented as following i\ The constraint of power balance 003 003 004 003 003 005 006       007 007     0  cos sin  0  sin cos  1 1 ij ij ij ij n i j j i i ij ij ij ij n i j j i i B G U U Q B G U U P 015 015 015 015  12 Where P i is the injected active power Q i is the injected reactive power U i  and U j are the node voltage G ij is the conductance between i and j  B ij is the susceptance between i and j  ij 015 is the electrical angle difference between i and j  ii\ The constraint of node voltage max min Ci Ci Ci Q Q Q 016 016  max min i i i U U U 016 016  max min ij ij ij 015 015 015 016 016  max min i i i T T T 016 016  max min i i i C C C 016 016 13 Where Q Cimin is the min available reactive power Q Cimax is the max available reactive power U imin is the min voltage amplitude of node i  U imax is the max voltage amplitude of node i  T imin  T imax   i s ad ju s t m e n t ra n g e  o f th e ad ju s t ab le transformer i  n i  2  1   C i is the switching frequency  C min and C max are the limits of C i If C i reaches to C max the capacitor is disabled in the left time E Calculation of Target Function i\Target function for TSP method: The problem of reactive power optimization in substations can be regarded as a TSP problem. A capacitor set can be regarded as a city in TSP method. The switching state is the path between two cities. The function in \(11\ can be described as       min 1 1 007    n s n i e s ts e s ts  14 
722 


Where    1  n e s ts represents the change of target function if there is injected reactive power in the new-added node n  ii\ Constraint conditions: Considering the representation of the constraint conditions of \(13\ in tabu table, the constraints on voltage and the change of the transformer taps can be ignored The switching frequency of capacitor sets is max min i i i C C C 016 016 If max i i C C 017 and last for a period time the capacitor i C will be not allowed to switched again and the value is set to zero in the left time IV C ASE STUDY The improved algorithm is applied to an example system The diary operating data are available 002 Fig.2 shows the simplified study example. A center substation \(C, as in Fig.1 has nineteen controlled substations, three 110KV substations and sixteen 35KV substations. All these substations are equipped with reactive compensators and on-load tapchanging transformers as shown in Table I The parameters are 5  0 004 011 003 1 004 010 003 013 0.4 before the 1/4 calculation period and 1 004 011 003 3 004 010 003 013 0.8 later   t k ij 006 014 is calculated by \(6\. So the information in the path is enlarged and the computational complexity is reduced to find the optimal solution quickly Fig.3\(a\, \(b\ and \(c\ are the evaluation results when the reactive difference of 110kV buses changes continuously Where, \(I\aims at the min of the net loss; in other words 1 1  011  0 2  011 in \(11\; \(II\ aims at the min node voltage deviation; in other words 0 1  011  1 2  011 in \(11  002\002\003 002 002 002\004 002\005 002\006 007 005 006 010\002 010\011 Figure 2 A real electric system TABLE I T HE CONFIGURATION OF THE COMPENSATED REACTIVE POWER IN EXAMPLE SUBSTATION Node No Distance Available Var 1 35km 24kVar 2 25km 36kVar 3 100km 24kVar 4 78km 36kVar 5 43km 24kVar 6 65km 36kVar 7 73km 24kVar 8 53km 36kVar 9 67km 30kVar 10 36km 30kVar 11 36km 12kVar 12 37km 18kVar 13 56km 12kVar 14 38km 18kVar 15 47km 12kVar 16 56km 18kVar 17 67km 12kVar 18 86km 18kVar 19 33km 12kVar 17/18 28km 0 kVar The evaluation function is as 007 007 007 007    NL i i i N k f C F F k 2 2 2 1 1  F  011 011   15 Where 1 F is shown in \(8 j f 2 is shown in \(9   1 N 2 017  j f L If the node voltage exceeds a given maximum deviation voltage of the node, the corresponding coefficient C i increase as a punitive options When the 35 kV bus coupler switcher S1 is disconnected and 110 kV bus coupler switcher S2 is closed, the compensating results are shown in Fig. 3 \(a\ When S1 is closed and S2 is disconnected, the compensating results are shown in Fig.3 \(b\. When S1 and S2 are disconnected, the compensating results are shown in Fig.3 \(c a 
723 


b c Figure 3 The comparison of reactive compensation From Fig.3, it can be concluded that the overall compensation result with optimized strategy is better than that of the old switching method \(III\. The evaluation coefficient is equal to zero when fully compensated. The reactive power is over-compensated because of the step reactive power regulation with capacitors in Table.I V C ONCLUSIONS In this paper, Apriori algorithm has been improved and applied to substation data mining process. Ant colony algorithm is applied to get the optimal solution of reactive power allocation in substations. The state transition probability formula is amended and parameters are dynamically adjusted in this ant colony algorithm. The choice of the ant’s path to the next node is determined by the tabu table formulated according to the confidence level of the data mining. The switching strategy of the capacitor sets are given by online algorithm. An example substation system is described to test the algorithm proposed in this paper. Experimental results show that, reactive power optimization method based on data mining system can improve the system efficiency, reduce power loss, and have a great significance of stable operation R EFERENCES  Q i  L uo  A dv a n c i ng K n o w l e d g e D i s c o v e r y a nd D a t a  M i ni ng  Knowledge Discovery and Data Mining,” WKDD 2008. 23-24 Jan 2008, pp.3-5 2 X i n d o n g W u   D ata m i n i n g  a r t i ficia l  i n te l l i g en ce i n  d a t a an aly s is    Proceedings. IEEE/WIC/ACM International Conference on Intelligent Agent Technology, 2004. \(IAT 2004\.  pp.7-7  A i hua  L i L i ngl i n g  Z h a n g    A S t ud y o f t h e G a p f r om D a t a  M i ni ng t o  Its Application with Cases, Business Intelligence and Financial Engineering,”  BIFE '09. International Conference on 24-26 July 2009 pp.464 - 467 4 S J  A  tee l e J  R  M c Do n a l d   an d C  D  A r c y   K n o wl ed g e d i s c o v e r y in  databases: applications in the electrical power engineering domain IT Strategies for Information Overload \(Digest No: 1997/340\ IEE Colloquium on 3 Dec. 1997, pp.8/1 - 8/4 5 L I J i an q i an g   N I U C h en g l i n  L I U J i z h en    A p p l icat i o n o f  Data  Mining Technique in Optimizing the Operation of Power Plants Joumal of Power Engineering.  Vol.26,No.6, pp.830-835 6 C es ar i o  E   Ta l i a  D   Dis t r i b u t ed  Dat a  Min i n g Mo d e l s as Ser v i c e s  on the Grid,” International Conference on Data Mining Workshops 2008, pp.486 - 495  D i ngl i  S o ng  B i ngru Y a n g   Z h e n P e n g   a nd We i w e i  F a ng   S t u d y of cost-sensitive ant colony data mining algorithm,” Industrial Mechatronics and Automation, ICIMA 2009. International Conference on15-16 May 2009, pp.488 - 491 8 L  A d m an e  K  B e n a tch b a  M Ko u d i l   M Dri a s   S  G h a r o u t N  Ha m a n i   Using ant colonies to solve data-mining problems,” IEEE International Conference on Systems, Man and Cybernetics, 2004 4\:3151-3157 9 P   S  Sh e l o k ar V  K  J a y a ram a n   B   D Ku l k arn i    A n an t co lo n y  classifier system: application to some process engineering problems Computers and Chemical Engineering, 2004 \(28\: 1577-1584 1 WA N G Z h i g a n g   Y A N G L i x i  CH EN G e n y ong   A nt  C o l o n y  Algorithm for Distribution Network Planning,” Proceedings of the EPSA. 2002, 14\(6\73-76 1 Y i S h e n   M i ng x i n Y u a n  Y unf e n g Bu  S t u d y o n a d a p t i v e pl a nni ng  strategy using ant colony algorithm based on predictive learning Control and Decision Conference, 2009, pp: 3030 - 3035 1 C o l s o n   C  M    N e hr i r  M  H   Wa ng  C    A nt c o l o n y  o p t i m i z a t i o n  f o r microgrid multi-objective power management. Power Systems Conference and Exposition, 2009, pp: 1 - 7 13 A b ay at ey e J   S e k a r  A     D e t er m i n a t i o n o f  o p t i m al  r e ac t i v e p o w e r  generation schedule using line voltage drop equations and genetic algorithm,” 41st Southeastern Symposium on System Theory, 2009 pp:  139 - 143 14 A c h a y u th ak an  C    On g s ak u l  W  TVA C PSO  b a s e d  o p ti m a l  reactive power dispatch for reactive power cost allocation under deregulated environment,” Power & Energy Society General Meeting 2009, pp: 1 - 9 15 S u re s h R  K u m a r a ppa n  N    G e n e t i c a l gor i t h m ba se d r e a c t i ve  power optimization under deregulation,” IET-UK International Conference on Information and Communication Technology in Electrical Sciences, 2007, pp:  150 - 155 
724 


monk1 124 7 87.9% 93.7% 0.011 86.3% 92.9% 0.01 monk3 122 7 77% 88.5% 0.019 81.1% 90.6% 0.011 mux6 64 7 71.9% 83.5% 0.084 84.4% 93.3% 0.004 led7 200 8 93.5% 96.1% 0.677 61.5% 68.6% 0.04 parity5+5 100 11 81% 89.6% 0.072 64% 81.7% 0.018 iris-disc 100 5 88% 93.4% 0.002 94% 96.4% 0.003 Table 4  Reduction results when minsup=3 Data sets Number of objects Number of attributes RSVR algorithm Least value reduction algorithm Rule reduction ratio Data reduction ratio Runtime s Rule reduction ratio Data reduction ratio Runtime s monk1 124 7 89.5% 94.6% 0.009 86.3% 92.9% 0.01 monk3 122 7 82% 91.3% 0.015 81.1% 90.6% 0.01 mux6 64 7 71.9% 83.5% 0.086 84.4% 93.3% 0.004 led7 200 8 94% 96.4% 0.458 61.5% 68.6% 0.056 parity5+5 100 11 88% 93.5% 0.062 64% 81.7% 0.018 iris-disc 100 5 89% 94% 0.002 94% 96.4% 0.003 470 2010 Chinese Control and Decision Conference By comparing the two algorithms, we can see from Table 3 and Table 4 that a 6 datasets in RSVR algorithm are greater than those in least value reduction when minsup=3 \(denoted by * in 


front of dataset It shows that the reduction ratio increases with improvement of minsup value. The reduction ratio of RSVR algorithm must be greater than that of least value reduction algorithm if the value of minsup is increased b users. The algorithm mainly focuses on applied system instead of on reduction ratio c longer than that of least value reduction algorithm especially when the quantity of objects and attributes is large, because RSVR algorithm adopts times-iterative method and complicated structure database 6 CONCLUSION This paper presents an RSVR algorithm based on support in association rules mining via Apriori algorithm The more effective reduction table can be obtained by deleting those rules with less support according to least support minsup. The reduction feasibility of this algorithm was achieved by reducing the given decision table Comparing this algorithm with least value reduction algorithm reveals the characteristics and advantages of RSVR. Testing by UCI machine learning database showed the validity and feasibility of this algorithm REFERENCES 1] [1] Pan J.L., Ye X.H. Wang H.X. Node Fault Diagnosis in WSN Based on the Rough set and Bayes Decision-Making Chinese Journal of Sensors and Actuators, 2009, 22\(5 734-738 2] [2] Zhang Z.Y., Yuan R.X., Yang T.Z. Rule Extraction for Power System Fault Diagnosis Based on the Combination of Rough Sets and Niche Genetic Algorithm. Transactions of China Electrotechnical Society, 2009, 24\(1 3] [3] Zhou X.S., Wang Z.M. Application of Rough Set and Neural Network in Data Mining. Computer Engineering and Applications, 2009, 45\(7 4] [4] Wang H. Customer Value Analysis Based on Rough Set and Data Mining Technique. 4th International Conference on Wireless Communications, Networking and Mobile Computing, 2008: 1-4 5] [5] Jiang W.J., Xu Y.H., Xu Y.S. Research on the Nature of Reduction to Simplifying Reduction Algorithm 


Proceedings of the Fourth International Conference on Machine Learning and Cybernatics, Guangzhou, 2005 1800-1805 6] [6] Yang Z.F., Guo J.F., Chang F. A Value Reduction Method Based on Rough Sets. Computer Engineering 2003, 29\(9 7] [7] Agrawal R., Imielinski T., Swami A. Mining Association Rules between Sets of Items in Large Databases. Proceedings of the ACM SIGMOD Conference on Management of Data. 1993: 207-216 8] [8] Lin T.Y. Rough Set Theory in Very Large Databases Proceedings of CESA96. Lille, 1996: 936-941 9] [9] Pawlak Z. Rough Sets: Theoretical Aspects of Reasoning About Data. Kluwer Academic Publishers Boston. 1991 10] [10] Agrawal R., Srikant, R. Fast Algorithms for Mining Association Rules. Proceedings of the 20th VLDB Conference, Santiago, 1994: 487-499 11] [11] Wang J., Wang R., Miao D.Q. Data Condensation Based on Rough Set Theory. Computer Transaction, 1998 21\(5 2010 Chinese Control and Decision Conference 471 


o  a1, b1}, {a1, b2}, {a1, c1 a1, c2}, {a1, d1}, {a1, d2 a2, b1}, {a2, b2}, {a2, c1 a2, c2}, {a2, d1}, {a2, d2 2-itemset a1,b1,c1},{a1,b1,b2 a1,b1,d1},{a1,b1,c1 a1,b1,c2},{a1,b1,d1 a1,b1,c1},{a1,b1,c2 3-itemset computer 3-predicate set. We sign frequent predicate set in the same way of frequent itemset. Just like using Lk to represent the set of frequent k-predicate set, C k to represent the set of Candidate k-predicate set [1 In the former, there existed many algorithms by using the idea of Apriori and its several variations [5]. One of them can mine frequent predicate set from data cube, we call it Apriori_Cube. The difference between them is calculating the degree of predicate support instead of itemset. When mining multi-dimension association rules from data cube, one predicate set is the very set of dimension members from different dimensions in data cube d1  d n | count age-1, business-1, buys-1 the supporting number of predicate set is the count storage in the cell of data cube 2.3 The Formation Of Multi-Dimension Rules With the frequent predicate set having been found out, it is easy to pick up association rules [1 1 subsets 2 set, calculate the confident degree of rules s? \(I-s word, confidence = the confident number of I/the confident number s, if confidence ? PLQFRQI WKHQ WKH rules s? \(I-s 3. The Improve Of Apriori_Cube Algorithm 3.1 The Main Idea Of Improvements First using OLAP to simplify the data cube; second improving function gen_candidate \( Lk 


do 1. The dimensions and levels of mining task are determined by users requiring when data cube is built up. Sometimes it cannot certainly find out strong association rules or it may mine many rules which users arent interested in based on the levels of dimensions. Such as dimension area, when we talk about the problems of the world, the country level will be more useful than the province level. One solution of this situation is that adjusting the levels based to the amount and proximity of the rules which have been mined. But it means mining the data cube once again and while mining multi-dimension association rules, it also cant determine the dimensions which need to be adjusted accurately at the same time. So we think that analyzing the levels of dimensions at the same time of data mining, then using operations roll-up and drill-down of OLAP to adjust the levels, this is called On-Line Analysis Mining, at last carrying on mining process which will be more propriety and efficient Assume that users make an n-dimensions data cube and always hope the multi-dimension association rules mined can contain these n dimensions, on the contrary they arent interested in the rules containing only \(n-1 dimensions or even less [6]. In a word, it must find out the frequent n-predicate set. According to Apriori, every 1 subset of frequent n-predicate set must be frequent 1-set That means every dimension has a frequent 1-predicate set Obviously saying, once if some dimension doesnt contain a frequent 1-predicate set, the frequent n-predicate set must not exist So we check the frequent 1-predicate set after mining them from every dimension by algorithm: we can know that it shows the partition level of dimension we made is too low if some dimension doesnt have frequent 1-predicate set so we should raise the level of this dimension by roll-up; in the opposite, if every 1-predicate set of some dimension is frequent 1-predicate set, we should drill-down to drop the level of this dimension [3]. All of these adjustments will be embedded in mining process in order to strengthen the flexibility and targeting 2. In recent years, we also meet many problems like this when mining rules, most quantitative dimensions can be 


scattered, for example: dimension deposit can be parted in the form of interval as [0, 20K], [21k, 30k]and so on This discretization is commonly completed before mining 1]. So we can easily know that [0,20k], [21k, 30k] and the other intervals which belong to one dimension cant exist at the same time [4]. But there are still some special situations such as year which is parted by quarter level\(Q1,Q2,Q3,Q4 problem what about the sales in both quarter Q1 and Q2, so when we improve the algorithm we must think about it too First of all, it is obviously that when mining multi-dimension association rules, one predicate set contains no more than one different level from the same dimension [5] without special requirements of users Combine this point with Apriori_Cube algorithm can help reduce the amount of useless predicate set, especially with the great growing of the amount Example 1 There are 4 dimensions of the database: A, B, C D, when the discretization of them has been complete, we can built a table of them like Table 1 A B C D a1 b1 c1 d1 a2 b2 c2 d2 Table 1:Examp1       Graphic1: Connection of example 1 When the 3-itemset come out, itemset {a1, b1, b2 must be contained, according we discussed before it is obviously unsubstantiated. We will use one predicate set contains no more than one different level from the same dimension to do the first pruning, so that this condition will not appear ever Second, when users make special requirements, we also should do some improvements. Such as the example before, when the algorithm discovered that one predicate set contains different levels from the same dimension as {a1 b1, b2} and checked out that this is what the users are interested in, so we will calculate the sum of the counts of a1, b1} and {a1, b2}, then assign it to {a1, b1, b2} and keep it in the frequent 3-itemset. We call this situation SP The algorithm improved also has steps connection and pruning, but the difference is that we use one predicate set contains no more than one different level from the same dimension and the SP to do the first pruning in function 


gen_candidate \( Lk second pruning by using minsup o  B4 B3 B2 B1 To sum up the above arguments, we put forward the improved algorithm 3.2 Apriori_Cube_Improved Algorithm Algorithm: Apriori_Cube_Improved Input: N-Dimension Data Cube \( d1  d n | count support: minsup Output: The set of frequent predicate set L Process 1 2 i=1n up the candidate 1-predicate set and we use symbol C1 to represent the set of them 3 C1 4 1-predicate set of every one dimension 5 1 2 3 6 1 2 2 3 4 5 the new improvement of step 1 7 8 k ?Q\fDQG Lk ??\f'R 1 Lk 1 The new function will be expanded below 2 C k 3 4 9 New Function gen_candidate_improved \( Lk 1 1 2 


Lk 1 Do 1 there exists k-2 items in the same between I1 and I 2 a If there exits requirements of the users Then sup_count of C go to b b C k = C k ?c; Else delete c 3 Function gen_frequent \( C k 1 2 I1  I k come from the set do 1 n-dimension data cube\( I1  I k Support=sup_count/total_count 2 3 4. Performance Analysis Example 2 Lets talk about a practical problem just like the status of sales. Assume that we will mine the association rules involved 4 dimension attributes of sales, the minsup=25%. First of all, using OLAP technology to build a 4-D data cube and the 4 dimension attributes are: time location, item, and supplier. For location dimension which contains area, country and so on, we choose province level We use brand level for item dimension, company level for supplier dimension. Time dimension can be divided as Q1 1-3 4-6 7-9 10-12 location\(P1,P2,L1,L2 York, item\(B1,B2,B3,B4 C1,C2,C3 sales data cube can be generalized like this Graphic 2: The 4-Dimension Data Cube of Sales The details of this sales data cube are in the table follow: The amount of cells is 100 Location Time Item Supplier Count Cell-1 P1 Q1 B1 C2 5 Cell-2 P1 Q3 B1 C1 2 Cell-99 L1 Q3 B1 C1 3 Cell-100 L2 Q4 B1 C3 11 Table 2: The details data table of sales data cube We use original Apriori_Cube Algorithm to find 


frequent predicate set with minsup_num= 25%*100=25 According the data table we calculate that sup_count of every member of dimension L is \(P1:8, P2:5, L1:1, L2:24 and also T, I, S. So there is the process Graphic 3: The processes of old algorithm As we know, through comparing with the minsup_num dimension location has no one frequent 1-predicate set, so that there have no frequent 4-predicate set in the output by the original algorithm. But users are interested in the Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate set T I S Q1 Q3 B2 B3 C1 C2 I1 I 2 C1              C2                 C3 2 12 1 3 5 1 


6 2 12 8 11 Q 1  Q 2  Q 3  Q 4 P 1 P 2 L 1 L 2 Candidate 2-Predicate set Q1,B2},{Q1,B3 Q1,C1},{Q1,C2 Q3,B2},{Q3,B3 Q3,C1},{Q3,C2 Candidate 3-Predicate set Q1,B2,B3}{Q 1,Q3,B2},{Q3 C1,B2 Frequent 2-Predicate set Q1,B2}{Q1,B 3},{Q3,B2 Q3,C1 Output: 1L U 2L U 3L Frequent 3-Predicate set Q1,B2,C1},{Q1,B3,B2 Q1,B2,B3  B4 B3 B2 B1 To sum up the above arguments, we put forward the improved algorithm 3.2 Apriori_Cube_Improved Algorithm Algorithm: Apriori_Cube_Improved Input: N-Dimension Data Cube \( d1  d n | count support: minsup 


Output: The set of frequent predicate set L Process 1 2 i=1n up the candidate 1-predicate set and we use symbol C1 to represent the set of them 3 C1 4 1-predicate set of every one dimension 5 1 2 3 6 1 2 2 3 4 5 the new improvement of step 1 7 8 k ?Q\fDQG Lk ??\f'R 1 Lk 1 The new function will be expanded below 2 C k 3 4 9 New Function gen_candidate_improved \( Lk 1 1 2 Lk 1 Do 1 there exists k-2 items in the same between I1 and I 2 a If there exits requirements of the users Then sup_count of C go to b b C k = C k ?c; Else delete c 3 Function gen_frequent \( C k 1 2 I1  I k come from the set do 1 


n-dimension data cube\( I1  I k Support=sup_count/total_count 2 3 4. Performance Analysis Example 2 Lets talk about a practical problem just like the status of sales. Assume that we will mine the association rules involved 4 dimension attributes of sales, the minsup=25%. First of all, using OLAP technology to build a 4-D data cube and the 4 dimension attributes are: time location, item, and supplier. For location dimension which contains area, country and so on, we choose province level We use brand level for item dimension, company level for supplier dimension. Time dimension can be divided as Q1 1-3 4-6 7-9 10-12 location\(P1,P2,L1,L2 York, item\(B1,B2,B3,B4 C1,C2,C3 sales data cube can be generalized like this Graphic 2: The 4-Dimension Data Cube of Sales The details of this sales data cube are in the table follow: The amount of cells is 100 Location Time Item Supplier Count Cell-1 P1 Q1 B1 C2 5 Cell-2 P1 Q3 B1 C1 2 Cell-99 L1 Q3 B1 C1 3 Cell-100 L2 Q4 B1 C3 11 Table 2: The details data table of sales data cube We use original Apriori_Cube Algorithm to find frequent predicate set with minsup_num= 25%*100=25 According the data table we calculate that sup_count of every member of dimension L is \(P1:8, P2:5, L1:1, L2:24 and also T, I, S. So there is the process Graphic 3: The processes of old algorithm As we know, through comparing with the minsup_num dimension location has no one frequent 1-predicate set, so that there have no frequent 4-predicate set in the output by the original algorithm. But users are interested in the Candidate 1-Predicate set L T I S P1 P2 L1 


L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate set T I S Q1 Q3 B2 B3 C1 C2 I1 I 2 C1              C2                 C3 2 12 1 3 5 1 6 2 12 8 11 Q 1  Q 2  Q 3  Q 4 P 1 P 2 L 1 L 2 Candidate 2-Predicate set Q1,B2},{Q1,B3 Q1,C1},{Q1,C2 Q3,B2},{Q3,B3 


Q3,C1},{Q3,C2 Candidate 3-Predicate set Q1,B2,B3}{Q 1,Q3,B2},{Q3 C1,B2 Frequent 2-Predicate set Q1,B2}{Q1,B 3},{Q3,B2 Q3,C1 Output: 1L U 2L U 3L Frequent 3-Predicate set Q1,B2,C1},{Q1,B3,B2 Q1,B2,B3  6 level from the same dimension and SP to pruning Candidate 3 Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special requirement on location B by users 7 1 max_support of every dimension 1 frequent 4-predicate set which contains location dimension And the candidate 3-predicate set {Q1,Q3,B2} is useless because when we mine multi-dimension association rules we should follow that one predicate set contains no more than one different level from the same dimension even the sup_count >minsup_num when there are no requirements from users. But this time, users require that different members from dimension item can exist at the same time So we redo this mining process by the improved Algorithm Graphic 4: The processes of improved algorithm At last, we got the frequent 4-predicate set which the users are interested in, and then can format the rules 5. Conclusion In this paper, we have studied issues and methods on efficient mining of multi-dimension association rules based on data cube. With the development of technology, the size 


of database has become much huge than ever before unimaginable. So the multi-dimension model of database based on data cube has become useful and popular relatively how to mine useful multi-dimension association rules based on this model has been important too. Among the algorithms for the problems we choose the most classify one to improve, to make it more efficient and useful An efficient algorithm, Apriori_Cube_Improved, has been developed which explores the multi-dimension association rules. First, we use max_support and min_support of every frequent 1-predicate set to check the levels of dimensions, and then adjust the data cube by roll-up and drill-down operation immediately. This step is embed in the process of mining association rules, so it makes the rules much more useful and flexible; second applying one predicate set contains no more than one different level from the same dimension and SP pruning can help reduce the amount of predicate set, especially with the great growing of the number. So the speed of algorithm improved is faster than ever At last, there are also many other interesting issues and flaws of the algorithm which call for further study including efficient mining multi-dimension association rules of complex measures and so on. Finally, I should thank all the people who have give me help for this paper Reference 1] Jiawei Han, Micheline Kamber. Data Mining Concepts and Techniques. China Machine Press, 2007 2] Agrawal R, Imielinski T and Swami A. Mining association rules between sets of items in large database. Proc. of the ACM SIGMOD Conf., Washington DC, 1993 3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional Association Rule Mining Method Based on Data Cube Computer Engineering, China,2003 4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research Multi-dimensional Association Rule Ming Based on Apriori Science Technology and Engineering, China, 2009 5] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proc. of the 20th Int.Conf. on VLDB Santiago, Chile, 1994 6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang Mining Multi-Dimension Constrained Gradients in Data 


Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy 2001 7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE Transactions on Knowledge and Data Engineering, 2000 Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Candidate 1-Predicate set L T I S P L Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate L T I S P L 


Q1 Q3 B2 B3 C1 C2 Frequent 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3 Output L1 U L2 U L3 U L4 The Dimension need to be adjust Dimension: Location Operation: Roll-up to increase the level of this dimension Result P\(P1,P2 L1,L2 2 3 2 3 The Features Of Every 1-Predicate Set Location: min_support=13 max_support=25 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 4 5 4 max_support of every dimension 5 minsup_num=25, and no one has to be adjusted Frequent 3-Predicate set Q3,C1,B2},{Q1,B2,L Q1,B3,L},{Q3,P,B2 Candidate 4-Predicate set 


Q3,C1,B2,P},{Q1,B2,B3,L}\(7 The Features of Every 1-Predicate Set Location: min_support=1 max_support=24 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 Candidate 2-Predicate set Q1,B2},{Q1,B3},{Q1,C1 Q1,C2},{Q3,B2},{Q3,B3 Q3,C1},{Q3,C2},{P,Q1 P,Q3},{P,B2},{P,B3},{P,C1 P,C2},{L,Q1},{L,Q3 L,B2},{L,B3},{L,C1},{L,C2 Frequent 2-Predicate set Q1,B2}{Q1,B3},{Q3,B2 Q3,C1},{P,Q3},{L,Q1 6 Candidate 3 -Predicate set Q3,C1,B2},{Q3,C1,P},{Q1,B 2,L},{Q1,B3,L},{Q3,P,B2},{Q 1,B2,B3  6 level from the same dimension and SP to pruning Candidate 3 Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special requirement on location B by users 7 1 max_support of every dimension 1 frequent 4-predicate set which contains location dimension And the candidate 3-predicate set {Q1,Q3,B2} is useless because when we mine multi-dimension association rules we should follow that one predicate set contains no more than one different level from the same dimension even the sup_count >minsup_num when there are no requirements from users. But this time, users require that different 


members from dimension item can exist at the same time So we redo this mining process by the improved Algorithm Graphic 4: The processes of improved algorithm At last, we got the frequent 4-predicate set which the users are interested in, and then can format the rules 5. Conclusion In this paper, we have studied issues and methods on efficient mining of multi-dimension association rules based on data cube. With the development of technology, the size of database has become much huge than ever before unimaginable. So the multi-dimension model of database based on data cube has become useful and popular relatively how to mine useful multi-dimension association rules based on this model has been important too. Among the algorithms for the problems we choose the most classify one to improve, to make it more efficient and useful An efficient algorithm, Apriori_Cube_Improved, has been developed which explores the multi-dimension association rules. First, we use max_support and min_support of every frequent 1-predicate set to check the levels of dimensions, and then adjust the data cube by roll-up and drill-down operation immediately. This step is embed in the process of mining association rules, so it makes the rules much more useful and flexible; second applying one predicate set contains no more than one different level from the same dimension and SP pruning can help reduce the amount of predicate set, especially with the great growing of the number. So the speed of algorithm improved is faster than ever At last, there are also many other interesting issues and flaws of the algorithm which call for further study including efficient mining multi-dimension association rules of complex measures and so on. Finally, I should thank all the people who have give me help for this paper Reference 1] Jiawei Han, Micheline Kamber. Data Mining Concepts and Techniques. China Machine Press, 2007 2] Agrawal R, Imielinski T and Swami A. Mining association rules between sets of items in large database. Proc. of the ACM SIGMOD Conf., Washington DC, 1993 3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional Association Rule Mining Method Based on Data Cube 


Computer Engineering, China,2003 4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research Multi-dimensional Association Rule Ming Based on Apriori Science Technology and Engineering, China, 2009 5] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proc. of the 20th Int.Conf. on VLDB Santiago, Chile, 1994 6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang Mining Multi-Dimension Constrained Gradients in Data Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy 2001 7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE Transactions on Knowledge and Data Engineering, 2000 Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Candidate 1-Predicate set L T I S P L Q1 Q2 Q3 Q4 B1 B2 


B3 B4 C1 C2 C3 Frequent 1-Predicate L T I S P L Q1 Q3 B2 B3 C1 C2 Frequent 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3 Output L1 U L2 U L3 U L4 The Dimension need to be adjust Dimension: Location Operation: Roll-up to increase the level of this dimension Result P\(P1,P2 L1,L2 2 3 2 3 The Features Of Every 1-Predicate Set Location: min_support=13 max_support=25 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 4 5 


4 max_support of every dimension 5 minsup_num=25, and no one has to be adjusted Frequent 3-Predicate set Q3,C1,B2},{Q1,B2,L Q1,B3,L},{Q3,P,B2 Candidate 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3,L}\(7 The Features of Every 1-Predicate Set Location: min_support=1 max_support=24 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 Candidate 2-Predicate set Q1,B2},{Q1,B3},{Q1,C1 Q1,C2},{Q3,B2},{Q3,B3 Q3,C1},{Q3,C2},{P,Q1 P,Q3},{P,B2},{P,B3},{P,C1 P,C2},{L,Q1},{L,Q3 L,B2},{L,B3},{L,C1},{L,C2 Frequent 2-Predicate set Q1,B2}{Q1,B3},{Q3,B2 Q3,C1},{P,Q3},{L,Q1 6 Candidate 3 -Predicate set Q3,C1,B2},{Q3,C1,P},{Q1,B 2,L},{Q1,B3,L},{Q3,P,B2},{Q 1,B2,B3  


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


