A Fuzzy Set Theoretic Approach to Discover User Sessions from Web Navigational Data Zahid Ansari 003  A Vinaya Babu y  Waseem Ahmed 003 and Mohammad Fazle Azeem z 003 Dept of Computer Science Engineering P.A College of Engineering Mangalore India y Dept of Computer Science Engineering Jawaharla Nehru Technological University Hyderabad India z Dept of Electronics and Communication Engineering P.A College of Engineering Mangalore India Abstract 227Due to the continuous increase in growth and complexity of WWW web site publishers are facing increasing dif\002culty in attracting and retaining users In order to design attractive web sites designers must understand their users needs Therefore analysing navigational behaviour of users is an important part of web page design Web Usage Mining WUM is the application of data mining techniques to web usage data in order to discover the patterns that can be used to analyse the user's navigational behaviour Preprocessing knowledge extraction and results analysis are the three main steps of WUM Due to large amount of irrelevant information present in the web logs the original log 002le can not be directly used in the WUM process During the preprocessing stage of WUM raw web log data is to transformed into a set of user pro\002les Each user pro\002le captures a set of URLs representing a user session This sessionized data can be used as the input for a variety of data mining tasks such as clustering association rule mining sequence mining etc If the data mining task at hand is clustering the session 002les are 002ltered to remove very small sessions in order to eliminate the noise from the data But direct removal of these small sized sessions may result in loss of a signi\002cant amount of information specially when the number of small sessions is large We propose a 224Fuzzy Set Theoretic\224 approach to deal with this problem Instead of directly removing all the small sessions below a speci\002ed threshold we assign weights to all the sessions using a 224Fuzzy Membership Function\224 based on the number of URLs accessed by the sessions After assigning the weights we apply a 224Fuzzy c-Mean Clustering\224 algorithm to discover the clusters of user pro\002les In this paper we provide a detailed review of various techniques to preprocess the web log data including data fusion data cleaning user identi\002cation and session identi\002cation We also describe our methodology to perform feature selection or dimensionality reduction and session weight assignment tasks Finally we compare our soft computing based approach of session weight assignment with the traditional hard computing based approach of small session elimination I I NTRODUCTION The World Wide Web as a large and dynamic information source is a fertile ground for data mining principles or Web Mining  Web mining is primarily aimed at deriving actionable knowledge from the Web through the application of various data mining techniques W eb Usage Mining is the disco very of user access patterns from Web server access Web Usage Mining analyses results of user interactions with a Web server including Web logs clickstreams and database transactions at a Web site Web usage mining includes clustering to 002nd natural groupings of users or pages associations to discover the URLs requested together and analysis of the sequential order in which URLs are accessed Web Usage Mining WUM consists of three main steps preprocessing knowledge extraction and results analysis The goal of the preprocessing step is to transform the raw web log data into a set of user pro\002les Each such pro\002le captures a sequence or a set of URLs representing a user session Web usage data preprocessing exploit a variety of algorithms and heuristic techniques for various preprocessing tasks such as data fusion data cleaning user identi\002cation session identi\002cation etc Data fusion refers to the merging of log 002les from several Web servers Data cleaning involves tasks such as removing extraneous references to embedded objects style 002les graphics or sound 002les and removing references due to spider navigations User identi\002cation refers to the process of identifying unique users from the user activity logs User Session identi\002cation is the process of segmenting the user activity log of each user into sessions each representing a single visit to the site Once user sessions are discovered this sessionized data can be used as the input for a variety of data mining tasks such as clustering association rule mining 5 sequence mining etc.If the data mining task at hand is clustering the session 002les are 002ltered to remove very small sessions in order to eliminate the noise from the data But direct removal of these small sized sessions may result in loss of a signi\002cant amount of information specially when the number of small sessions is large We propose a 224Fuzzy Set Theoretic\224 approach to deal with this problem Instead of directly removing all the small sessions below a speci\002ed threshold we assign weights to all the sessions using a 224Fuzzy Membership Function\224 based on the number of URLs accessed by the sessions After assigning the weights we apply 224Fuzzy c-Mean Clustering\224 to discover the clusters of user pro\002les 978-1-4244-9477-4/11/$26.00 ©2011 IEEE 879 


Fuzzy clustering techniques perform non-unique partitioning of the data items where each data point is assigned a membership value for each of the clusters This allows the clusters to grow into their natural shapes A membership value of zero indicates that the data point is not a member of that cluster A non-zero membership value shows the degree to which the data point represents a cluster Fuzzy clustering algorithms can handle the outliers by assigning them very small membership degree for the surrounding clusters Thus fuzzy clustering is more robust method for handling natural data with vagueness and uncertainty Rest of the paper is organized as follows in section-II we provide a detailed review of various techniques to preprocess the web log data including data fusion data cleaning user and session identi\002cation In Section III we describe our methodology for feature selection or dimensionality reduction and session weight assignment In this session we also discuss In this section we also discuss the work how to apply Fuzzy cMean Clustering algorithms to weighted user sessions Section IV gives the results of our methodology applied to a real Web site Finally section V provides the conclusion II P REPROCESSING OF W EB L OG D ATA In accordance with the W3C's web characterization terminologies and 10 pro vided de\002nitions for the main WUM terms which are described in Table 1 TABLE I D EFINITION OF WUM T ERMS WUM Term Description Resource It can be anything that has identity URI A compact string of characters to identify a resource e.g an HTML 002le an image etc Web resource A resource accessible through the HTTP protocol Web server Server that provides access to Web resources Web page Set of data constituting one or several Web resources that can be identi\002ed by a URI Page View Set of 002les that contribute to the display of a web page on a web browser at a speci\002c moment in time Web Browser Client software that sends Web requests handles the responses and display requested URIs User An individual that is accessing 002les from one or more web servers using a Web browser Web request Is a request a Web client makes for a Web resource Click stream Is a sequential series of a page view requests User session Click-stream of page views for a single user across across one or more Web servers Server session The set of page views in a user session for a particular Web site Episode is a subset of a visit constituted from related clicks The goal of the preprocessing stage in Web usage mining is to transform the raw web log data into a set of user sessions Each session consists of a sequence or a set of pageviews This sessionized data can be further transformed and abstracted or used as the input for various data mining algorithms Web usage data preprocessing exploit a variety of algorithms and heuristic techniques for various preprocessing tasks such as data fusion data cleaning user and session identi\002cation etc Data preprocessing step of Figure 1 depicts the primary Fig 1 Web log processing to extract weighted user sessions tasks involved in web log data preprocessing in order to discover the user sessions A Data Fusion Many important organizations have several Web servers for their Web sites Tanasa and Brigitte TrousseIn order to perform analysis of the of the users behaviours on the entire Web site web log 002les generated by different web servers should be combined Data fusion refers to the mer ging of log 002les from several Web servers This requires global synchronization across these servers The Web server's name is added in the requests before the 002le path Web server clocks are synchronized across various time zones The original host name is replaced with an identi\002er B Data cleaning Data cleaning involves tasks such as removal of irrelevant references to embedded objects style 002les graphics or sound 002les and references due to spider navigations Data cleaning also identi\002es Web robots and removes their requests Popular Web sites generate gigabytes of web log data per hour Processing such huge 002les is a tedious task By performing data cleaning log 002le sizes can be reduced to enhance the subsequent mining tasks On the other hand client or proxy side caching can often result in missing references to those pages or that have been cached In order to infer these missing references Cooley et al have described a path completion process which utilizes the knowledge of site structure and referrer information from server log 002les Elimination of image 002le requests Most Web pages contain images Whether to retain or eliminate the log 002les for these images depends much on the goal of the Web Usage Mining Elimination of the image and other multimedia requests can be accomplished by checking the suf\002x of the URL name For example log entries with 002le name suf\002xes such as gif jpeg GIF JPEG jpg JPG and map can be discarded A default list of suf\002xes can be maintained for this purpose 880 


Elimination of robots requests Web robots or spiders are software tools that automatically traverse hyperlink structure of world wide web in order to locate an retrieve information Sessions consisting of Web robot request make the effective Web usage analysis more dif\002cult Elimination of log entries corresponding to the Web robot requests 002lters non useful sessions and enhances the subsequent mining tasks In order to identify a Web Robot request following heuristics are being used  i Find all the hosts that ha v e requested the page 223robots.txt.\224 ii Refer a list of all user agents known as robots iii Compute the browsing speed of the host If the browsing speed and the number of pages visited during the current visit exceed some speci\002ed threshold then the host is considered as web robot C User identi\002cation Next unique users must be identi\002ed For Web sites requiring user registration the web logs contain the user login name In such cases this information can be used for user identi\002cation For those cases where user login information is not available each IP might be considered as a user Pirolli et al suggest that each dif ferent agent type for an IP address represents a different user Another approach described in uses the access log along with the referrer log and site topology to construct browsing paths for each user If a requested page is not directly reachable from any of the pages visited by the user then this represents another user with the same IP address D User session identi\002cation User Session identi\002cation is the process of segmenting the user activity log of each user into sessions each representing a single visit to the site It is a complicated task due to the presence of proxy servers and cases of multiple users accessing the same computer Web sites without user authentication information or embedded session ids mostly rely on heuristics methods for sessionization The sessionization heuristic helps in extracting the actual sequence of actions performed by one user during one visit to the site Generally sessionization heuristics are either time-oriented or structure oriented A formal framework for measuring the effectiveness of such heuristics has been proposed in  Berendt et al in v estigated the impact of site different heuristics on the quality of constructed user sessions Time-oriented heuristics consider an upper bound on the time spent in the entire site during a visit or an upper bound on page-stay time If the time between page requests e xceeds a certain threshold it is assumed that the user is starting a new session Based on emperical data Cateledge and Pitkow  proposed a timeout of 25.5 minutes based on empirical data A default time threshold of 30 minutes is commonly used by many tools  A second type of time-oriented heuristics use a threshold on the total page-stay time There is no commonly used threshold for page-stay time due to the fact that page-stay time is affected by the page information content page loading time and by data transfer rate of the communication Navigation-oriented heuristics considers Web navigational behaviours of the W eb users typically reach pages by following hyperlinks rather than by typing URLs Therefore if a page request that is unreachable through the pages visited by the user so far is likely to have been initiated by another user This strategy is used to partition a Web server log into sessions According to Cooley et al a requested W eb page P need not be accessible from the page immediately accessed before it Rather it might be reachable from a page the user backtracked to Web server logs may not contain these backward moves because of the client catching mechanism If a page request is not directly linked to the last requested page and if the referrer of this page is present in the user's recent request history then the user might have backtracked receiving cached versions of the pages until a new page was requested Missing page references are then added to the user session 002le III D ISCOVERY OF U SER S ESSION C LUSTERS Clustering techniques can be applied to cluster the user sessions identi\002ed in the preprocessing stage Detailed description of our web data preprocessing algorithms to discover the user sessions can be found in A Feature Subset Selection of User Sessions Each user session can be thought of a single transaction of many URL references We map the user sessions as vectors of URL references in a n dimensional space Let U be a set of n unique URLs appearing in the preprocessed log then U  f u 1  u 2  001 001 001  u n g and let S be a set of m user sessions discovered by preprocessing the web log data Then S  f s 1  s 2  001 001 001  s m g where each user session s i 2 S can be represented as a bit vector s  f w u 1  w u 2  001 001 001  w u n g where w u i  1  if w u i 2 s and w u i  0  otherwise Instead of binary weights feature weights can also be used to represent a user session These feature weights may be based on frequency of occurrence of a URL reference within the user session the time a user spends on a particular page or the number of bytes downloaded by the uses from a page However the URLs appearing in the access logs and could number in the thousands Distance-based clustering methods often perform very poor when dealing with very highdimensional data Therefore 002ltering the logs by removing references to low support URLs i.e that are not supported by a speci\002ed number of user sessions can provide an effective dimensionality reduction method while improving clustering results B Assiging Weights to User Sessions If the data mining task at hand is clustering the session 002les can be 002ltered to remove very small sessions in order to eliminate the noise from the data But direct remo v al of these small sized sessions may result in loss of a signi\002cant amount 881 


Fig 2 Fuzzy membership function for session weight assignment of information specially when the number of small sessions is large We propose a 224Fuzzy Set Theoretic\224 approach to deal with this problem Instead of directly removing all the small sessions below a speci\002ed threshold we assign weights to all the sessions using a 224Fuzzy Membership Function\224 based on the number of URLs accessed by the sessions Figure 2 depicts a linear Fuzzy membership function for session weight assignment Here LB represents a lower bound on the number of URLs accessed in a session and U B represents an upper bound on the number of URLs accessed in a session Let j s i j be the number of URLs accessed in session s i  then the fuzzy membership function takes the following values   s i   0  if j s i j 024 LB   s i   1  if j s i j 025 U B   s i   j s i j\000 LB U B 000 LB  otherwise C Clustering the User Sessions Once use sessions are represented in the form of a vector clustering algorithm can be run against them The goal of this process is to discover session clusters that represent similar URL access patterns Clustering aims to divide a data set into groups or clusters where inter-cluster similarities are minimized while the intra cluster are similarities maximized Details of various clustering techniques can be found in survey articles 23][24 The k means clustering algorithm is one of the most commonly used method for partitioning the data This algorithm partitions a set of m objects into k clusters The algorithm proceeds by computing the distances between a data point and various cluster centres in order to assign the data item to one of the clusters so that intra-cluster similarity is high but inter-cluster similarity is low Euclidian distance can be used as a measure to calculate the distance between various data points and cluster centres d  x i  v j   n X k 1 r r r x i k 000 v j k r r r 2 where v i  is the i th data point v j  is the j th cluster center d  x i  v j   is the distance between X i and center of v j n  is the number of dimensions of each data point x i k  is the value of k th dimension of x i v j k  is the value of k th dimension of center of v j The k means clustering 002rst initializes the cluster centres randomly Then each data point x i is assigned to some cluster v j which has the minimum distance with this data point Once all the data points have been assigned to clusters cluster centres are updated by taking the weighted average of all data points in that cluster This recalculation of cluster centres results in better cluster center set The process is continued until there is no change in cluster centres Although k means clustering algorithm is ef\002cient in handling the crisp data which have clear cut boundaries but in real world data clusters have ill de\002ned boundaries and often overlapping clusters This happens because many time the natural data suffer from Ambiguity Uncertainty and Vagueness 25 Fuzzy c-means clustering incorporates fuzzy set theoretic concept of partial membership and may result in the formation of overlapping clusters The algorithm calculates the cluster centres and assigns a membership value to each data item corresponding to every cluster within a range of 0 to 1 The algorithm utilizes a fuzziness index parameter q where q 2 1  1   which determines the de gree of fuzziness in the clusters As the value of q reaches to 1 the algorithm works like a crisp partitioning algorithm Increase in the value of q results in more overlapping of the clusters Let X  f x i j i  1 001 001 001 m g be a set of n dimensional data point vectors where m is the number of data points x i  f x i 1  x i 2  001 001 001  x i n g8 i  1 001 001 001 m  Let V  f v j j j  1 c g represent a set of n dimensional vectors corresponding to the cluster center corresponding to each of the c clusters v j  f v j 1  v j 2  001 001 001  v j n g8 j  1 001 001 001 c  Let u ij represent the grade of membership of data point x i in cluster j  u ij 2 0  1  8 i  1 001 001 001 m and 8 j  1 001 001 001 c  The n 002 c matrix U   u ij  is a fuzzy c partition matrix which describes the allocation of the data points to various clusters The objective function J  U V X  of fuzzy c mean clustering can be speci\002ed as the weighted sum of distances between the data points and the corresponding centres of the clusters In general it takes on the form J  U V X   c X j 1 m X i 1 u q ij d 2 ij  x i  v j  Where q 2 1  1  is the fuzziness index of the fuzzy clustering d 2 ij  x i  v j  is the distance between the data point 026 x i and cluster center 026 v j  d 2 ij  x i  v j   n X k 1 w  x i  r r r 026 x i k 000 026 v j k r r r 2 where w  x i  is the weight of the data point x i Clusters formed by the applications clustering algorithms represents a group of user sessions that are similar based on co-occurrence patterns of URL references Clustering of user 882 


sessions results in a set C  f c 1  c 2  001 001 001  c k g of clusters where each c i is a subset of S  i.e a set of user sessions Each cluster represents a group of users with similar navigational patterns IV E XPERIMENTAL R ESULTS In order to discover the clusters that exist in user accesses sessions of a web site we carried out a number of experiments The Web access logs were taken from the P.A College of Engineering Mangalore web site at URL http://www.pace.edu.in The site hosts a variety of information including departments faculty members research areas and course information The Web access logs covered a period of one months from February 1 2011 to March 1 2011 There were 74,924 logged requests in total TABLE II R EDUCTION IN L OG S IZE AFTER E LIMINATING I RRELEVANT E NTRIES Site 1 Site 2 Site 3 Access log initial size 74,924 66,426 22,433 Access log size after cleaning 30,644 65,983 20,801 Percentage of Initial Size 40.90 99.33 92.72 Total number of unique URLs of the Web Site present in the log 002le entries are 6850 Figure 3 shows the percentage of the URLs against how many times they are accessed in the log 002le It is clear from the graph that 78 of URLs were accessed only once 16 of them were accessed twice and only 6 of them are accessed three or more times Maximum access count for a URL is 2234 On average each URL is accessed 4.47 times Fig 3 Percentage of URLs versus URL Access Frequency As far as clustering of the User Sessions is concerned those URLs which are accessed only once do not play any signi\002cant role in forming the clusters since they appear in only one of the user sessions Therefore we eliminate all such URL requests from our further anlaysis This type of URL 002ltering is important in removing noise from the data Since a user session is represented by an n dimensional vector where n represents the number of the site URLs accessed in the log 002les Reductions in the number of URLs also reduces the session vector dimentions The count of the URLs which are accessed only once is 5372 After eliminating them the total number of unique URLs for sub sequent analysis are 1478 In order to identify the user sessions we applied two different kinds of time oriented heuristics T OH 1 and T OH 2  Details of these results and the comparisons of these approaches can be found from our previous work The results of application of T OH 1 is given in Table 3 TABLE III R ESULTS OF U SER S ESSION I DENTIFICATION Items Count No of User Sessions 968 Minimum no of URLs accessed in a session 1 Maximum no of URLs accessed in a session 545 Average no of URLs accessed in a session 26.12 Minimum no of unique URLs accessed in a session 1 Maximum unique URLs Accessed in a session 158 Average unique URLs Accessed in a session 6.5 Figure 4 shows the number of URLs and their corresponding session support count Our result shows that 396 URLs have a session support count of one We eliminate these URLs since they can not play any signi\002cant role clusters formation This type of session support 002ltering provides a form of dimensionality reduction in subsequent clustering tasks where URLs appearing in the session 002le are used as features Table 4 shows the results of user session identi\002cation after the elimination of these low support URLs TABLE IV Items Count No of User Sessions 902 Average no of unique URLs in a session 5.5 Fig 4 No of URLs versus no of sessions they are associated with Figures 5 depicts the session counts against various URL counts Our results show that there are there are quite a large number of user sessions conatining only few URLs For example there are 67 sessions containing one only URL 134 containing two URLs and 56 sessions containing three URLs User sessions with samaller number of URLs are less signi\002cant for the purpose of clustering We may impose 883 


certain constraints desirable for better clustering performance and outcome The 002rst is that we are interested in only those sessions that access more than a certain number of URLs For this purpose we are using a Fuzzy set theortic approach to assign the weights to various user sessions based on the number of URLs they contain Fig 5 Number of sessions Vs Number of URLs V C ONCLUSION In this paper we provided a detailed review of various techniques to preprocess the web log data including data fusion data cleaning user identi\002cation and session identi\002cation We also described our methodology to perform feature subset selection of session vectors and session weight assignment tasks Finally we compared our soft computing based approach of session weight assignment with the traditional hard computing based approach of small session elimination Our results shows the soft computing based approach provides more accurate user pro\002les than the hard clustering based approaches R EFERENCES  P  K olari and A Joshi 223 W eb mining research and practice 224 Computing in Science and Engineering  vol 6 no 4 pp 49\22653 2004  R Coole y  B Mobasher  and J Sri v asta v a 223W eb mining Information and pattern discovery on the world wide web,\224 in Ninth IEEE International Conference on Tools with Arti\002cial Intelligence 1997 Proceedings  1997 pp 558\226567  A Joshi and R Krishnapuram 223Rob ust fuzzy clustering methods to support web mining,\224 1998  Y  Fu K Sandhu and M Shih 223 A generalization-based approach to clustering of web usage sessions,\224 Lecture Notes in Computer Science  pp 21\22638 2000  H L T  Mobasher  B.and Dai and M Nakag a w a 223Ef fecti v e personalization based on association rule discovery from web usage data.\224 in In Proceedings of the 3rd ACM Workshop on Web Information and Data Management WIDM01 Atlanta Georgia November 2001  2001  M Spiliopoulou and L C F auls tich 223W um A web utilization miner  224 in In Proceedings of EDBT Workshop WebDB98 Valencia Spain LNCS 1590 Springer Verlag  1999  B Mobasher  R Coole y  and J Sri v asta v a 223 Automatic personalization based on web usage mining,\224 Commun ACM  vol 43 pp 142\226151 August 2000  F  Kla w onn and A K eller  223Fuzzy clustering based on modi\002ed distance measures,\224 in Advances in Intelligent Data Analysis  ser Lecture Notes in Computer Science D Hand J Kok and M Berthold Eds Springer Berlin  Heidelberg 1999 vol 1642 pp 291\226301  D T anasa and B T rousse 223Data preprocessing for wum 224 Intelligent Systems IEEE  vol 23 no 3 pp 22\22625 2004  R W  Coole y  223W eb usage mining Disco v ery and application of intresting patterns from web data,\224 Ph.D dissertation Th e Graduate School of the University of Minnesota 2000  J Sri v asta v a R Coole y  M Deshpande and P  T an 223W eb usage mining Discovery and applications of usage patterns from web data,\224 SIGKDD explorations  vol 1 no 2 pp 12\22623 2000  D T anasa and B T rousse 223 Adv anced data preprocessing for intersites web usage mining,\224 IEEE Intelligent Systems  vol 19 no 2 pp 59\22665 2004  R Coole y  B Mobasher  J Sri v asta v a et al  223Data preparation for mining world wide web browsing patterns,\224 Knowledge and Information Systems  vol 1 no 1 pp 5\22632 1999  P N T an and V  K umar  223Di sco v er y of web robot sessions based on their navigational patterns,\224 Data Mining and Knowledge Discovery  vol 6 pp 9\22635 January 2002  P  Pirolli J Pitk o w  and R Rao 223Silk from a so w s ear e xtracting usable structures from the web,\224 in Proceedings of the SIGCHI conference on Human factors in computing systems common ground  ser CHI 96 New York NY USA ACM 1996 pp 118\226125  B M M S J W  Berendt B 223Measuring the accurac y of sessionizers for web usage analysis,\224 in Proc.of the Workshop on Web Mining First SIAM Internat.Conf on Data Mining Chicago IL  2001 pp 7\22614  M Spiliopoulou B Mobasher  B Berendt and M Nakag a w a 223 A framework for the evaluation of session reconstruction heuristics in webusage analysis,\224 INFORMS J on Computing  vol 15 pp 171\226190 April 2003  B Berendt B Mobasher  M Nakag a w a and M Spiliopoulou 223The impact of site structure and user environment on session reconstruction in web usage analysis,\224 in WEBKDD 2002 MiningWeb Data for Discovering Usage Patterns and Pro\002les  ser Lecture Notes in Computer Science Springer Berlin  Heidelberg 2003 vol 2703 pp 159\226179  L D Catledge and J E Pitk o w  223Characterizing bro wsing strate gies in the world-wide web,\224 Computer Networks and ISDN Systems  vol 27 no 6 pp 1065\2261073 1995 proceedings of the Third International World-Wide Web Conference  B Berendt and M Spiliopoulou 223 Analysis of na vig ation beha viour in web sites integrating multiple information systems,\224 The VLDB Journal  vol 9 pp 56\22675 2000  Z Ansari M F  Azeem A V  Bab u and W  Ahmed 223Preprocessing users web page navigational data to discover usage patterns,\224 in The Seventh International Conference on Computing and Information Technology Bangkok Thailand  May 2011  P  Berkhin 223Surv e y of clustering data mining techniques 224 Springer  2002  B P a v el 223 A surv e y of clustering data mining t echniques 224 in Grouping Multidimensional Data  Springer Berlin Heidelberg 2006 pp 25\22671  R Xu and I W unsch D 223Surv e y of clustering algorithms 224 Neural Networks IEEE Transactions on  vol 16 no 3 pp 645\226678 May 2005  M Chau R Cheng B Kao and J Ng 223Uncertain data mining An example in clustering location data,\224 in Advances in Knowledge Discovery and Data Mining  ser Lecture Notes in Computer Science W Ng M Kitsuregawa J Li and K Chang Eds Springer Berlin  Heidelberg 2006 vol 3918 pp 199\226204  X L Xie and G Beni 223 A v alidity measure for fuzzy clustering 224 IEEE Trans Pattern Analysis and Machine Intelligence  vol PAMI-13 p 841847 1987 884 


Table III A VERAGE RANKS OF THE QUALITY MEASURES  All datasets Binary datasets Measure k 1 k  100 k 1 k  100 004 2 4.435 4.038 4.694 3.889 Jaccard 5.224 5.622 5.361 7.028 Correlation 5.235 4.679 5.361 4.667  WRAcc  5.288 4.571 5.306 4.333 G-measure 5.312 5.538 5.417 6.750 F-measure 5.582 5.718 5.250 6.778 WRAcc 5.800 5.027 5.417 4.722 Condence 6.506 6.865 7.333 7.028 Laplace 6.553 6.654 7.278 6.139 Specicity 7.465 8.455 8.306 7.806 Purity 10.235 10.141 8.389 7.361 Sensitivity 10.365 10.692 9.889 11.500 004 2 F  003 1  261.916 292.001 40.674 57.618 CD  003 1  2.069 2.160 4.496 4.496 Purity Jaccard Specicity Sensitivity Laplace F-measure G-measure and Correlation Details on these measures and their origins can be found in the paper by F  urnkranz and Flach For each dataset we perform steps I and II of our method the same way as in the previous section with each of the 12 quality measures We then compare the measures in step III by comparing the p-values of the k best subgroups for both k 1 and k  100 for k  100 we take the average p-values over the top 100 groups Hence for all measures we obtain for both choices of k one test score for each combination of dataset and target value within that dataset For k 1 this leads to a grand total of 85 test scores for each quality measure On both the Car and the Contact-lenses dataset no 100 subgroups are found that satisfy the minsup constraint Hence there are no results on these datasets for k  100  leaving a total of 78 test cases for k  100  The measures are subsequently ranked where a lower test score p-value is better The resulting average ranks can be found in the second and third columns of Table III This table also displays the results of the Friedman tests the values for 006 2 F  With a signicance level of 002 1 we need 006 2 F to be at least 24  73 to reject the null hypothesis that all quality measures perform equally good Hence we comfortably pass this test Since the Friedman test is passed we can now perform Nemenyi tests to see which quality measures outperform others For the k 1 setting the critical difference CD equals 2  069 with signicance level 002 1  For each pair of measures we compute from Table III whether their difference is larger than CD  and if so the one with the smaller average rank is better than the other The corresponding CD chart can be found in Figure 2 Such a chart features a horizontal bar of length CD for each quality measure 004 i  starting at its average rank Hence 004 i is signicantly better than each quality measure whose bar starts to the right of the bar of 004 i  For instance in Figure 2 we see that 006 2 Table IV A VERAGE RANKS FOR CORRELATION MODEL MEASURES  Measure Average rank 005 ent 1  75 r 3  00 r 2 3  75 005 abs 4  25  r 4  75  r 2 5  25 005 scd 5  25 004 2 F 21  96 CD  003 1  4  114 is signicantly better than Laplace Specicity Purity and Sensitivity Figure 3 displays the CD chart for the k  100 setting When we have a dataset with many distinct target values we repeatedly let one of the target values correspond to positive examples and the rest to negative examples Hence the more distinct target values we have the lower the average fraction of positive examples in the dataset To see whether certain quality measures suffer from this effect we also computed the average ranks considering only the 9 datasets with a binary target The results can be found in the last two columns of Table III Again the average ranks easily pass the Friedman test Now that we have only 18 test cases the critical difference for the Nemenyi test becomes CD 4  496 with signicance level 002 1  C Beyond subgroup discovery So far we have illustrated our method with measures for subgroup discovery over a single discrete target We now turn to a variant of EMM 12 an e xtension of subgroup discovery incorporating complex target concepts This variant strives to nd subgroups for which the correlation between two attributes is signicantly different from their correlation on the whole dataset Several quality measures have been proposed for this problem W e validate measures for this setting on the datasets and target concepts used in the original paper The resulting average ranks over the two datasets  Windsor Housing and Gene Expression  can be found in Table IV The Friedman test value for these ranks is 006 2 F 21  96  where 16  81 would be enough with 7 measures so we can proceed with the Nemenyi test The critical difference is CD 4  114 with signicance level 002  10 when testing 7 measures on 4 test cases aggregating over the results for k 1 and k  100  In these modest experiments we nd that no signicant conclusions can be drawn VI D ISCUSSION The previous section displayed the results experimentally obtained with our new method in this section we will interpret them We start with the results obtained by the technique for validating subgroups in a set S found through subgroup discovery 
157 


 Fmeasure Gmeasure WRAcc Correlation Jaccard Chi^2 12 11 10 9 8 7 6 5 4 Sensitivity WRAcc Confidence Laplace Specificity Purity Figure 2 CD chart for k 1  CD 2  069    Jaccard Gmeasure WRAcc Correlation WRAcc Chi^2 Confidence Laplace Fmeasure Purity Specificity Sensitivity 456789101112 Figure 3 CD chart for k  100  CD 2  160  A Validating subgroups From Table II we nd that we cannot refute any subgroups from S in several datasets Adult Credit-a Ionosphere Pima-indians and Wisconsin To explain this result we crafted a metalearning dataset from Tables I and II We selected the columns from Table I as attributes of our metalearning dataset and added three new columns representing the total number of attributes in the dataset a boolean column representing whether the dataset has discrete attributes and a boolean column representing whether the dataset has numeric attributes As target column we added the last column of Table II the fraction of subgroups retained when insignicant subgroups are removed with signicance level 002 1  On this metalearning dataset we performed a shallow using search depth d 1  but exhaustive subgroup discovery run using Kl  osgens mean test as quality measure The resulting metasubgroups should consist of those datasets with a relatively high fraction of kept subgroups The best metasubgroup is dened by the condition that the datasets have more than ve numeric attributes The eight datasets belonging to this metasubgroup are Adult Credita Glass Ionosphere Labor Pima-indians Wisconsin and Yeast This set includes all datasets for which we cannot refute any of the subgroups from S  This makes sense since for each dataset we have only considered the top 1000 subgroups a xed number independent of dataset characteristics Numeric attributes usually have many different values resulting in a hypothesis space that is much larger than it would have been if the attributes would have been discrete Hence in datasets with relatively many numeric attributes it is more likely that the 1000 best subgroups represent relatively rare spikes in a quality distribution consisting mainly of low values Therefore it is less likely that the random baseline incorporates some of these spikes and thus the baseline is more likely to be relatively weak B Validating quality measures The results we obtained by the technique for validating quality measures show that 006 2 achieves the best performance of all quality measures in distinguishing the top k subgroups from false discoveries Many of the relations between quality measures however are not signicant For k 1  all other quality measures perform signicantly better than Purity and Sensitivity Additionally Specicity performs signicantly worse than Jaccard Correlation  WRAcc   and the Gmeasure and 006 2 signicantly outperforms Laplace For k  100  we see some slight changes 006 2 and 
158 


 WRAcc  now also perform signicantly better than Condence and Specicity is now additionally outperformed by the F-measure and WRAcc while it no longer performs signicantly better than Purity Finally Correlation signicantly outperforms Condence Obviously some measures might be better than others in distinguishing the top k subgroups from false discoveries when k 1  while others might be better when k  100  The observed changes are not very dramatic and we consider the selection of k a user-derived parameter in the method One of the signicant relations that seems somewhat peculiar is the result that for both k 1 and k  100  Condence performs signicantly better than Purity while the latter is dened to be max  Condence  1  Condence   While there is a good theoretical reason to consider the Purity of a subgroup we can see from the denition that Purity has a lower bound of 0  5  hence the random baseline will generate higher values with Purity than with Condence Apparently the quality of the subgroups found with Purity does not increase enough compared to those found with Condence to compensate for this effect By comparing the second and third columns of Table III with the last two columns we can see that  WRAcc   WRAcc and particularly Purity perform better when we restrict the tests to datasets with a binary target These measures benet from the fact that in these test cases we have a better balance between positive and negative examples in the data compared to test cases on other datasets We can also read from the table that we have fewer measures that are signicantly better than others on datasets with a binary target This is mainly because because signicance is hard to achieve in an experiment with only 18 test cases as opposed to 85 or 78 on all datasets With 18 test cases the critical difference for the Nemenyi test with signicance level 002 1 is CD 4  496  rather than CD 2  069 with 85 test cases Since the average ranks range from 1 to 12  a critical difference of 4  496 is substantial More signicant differences between the quality measures can be expected when tested on more datasets with a binary target The results for the EMM variant were generated on a modest number of test cases As a result the critical difference for the Nemenyi test is quite high and one could not expect to nd many signicant results Expensive experimentation may give a signicant reason to prefer one measure over another in this setting For now what matters is that this illustrates that our method is applicable in more general settings than just traditional subgroup discovery VII C ONCLUSIONS We propose a method that deals with the multiple comparisons problem in subgroup discovery i.e the problem that when exploring a vast search space one basically considers many candidates for a statistical hypothesis hence one will inevitably incorrectly label some candidates as passing the test Our method tackles this problem by building a statistical model for the false discoveries the Distribution of False Discoveries DFD This distribution is generated by given a dataset and quality measure repeatedly running a subgroup discovery algorithm on a swap-randomized version of the data In this swap-randomized version while the distribution of the target variable is maintained its correlation with the attributes is destroyed Hence the best subgroup discovered on this dataset represents a false discovery The DFD is then determined by applying the central limit theorem to the qualities of these false discoveries Having determined the DFD one can solve many practical problems prevalent in subgroup discovery For any given discovered subgroup one can determine a p-value corresponding to the null hypothesis that it is generated by the DFD refuting this null hypothesis implies that the subgroup is not a false discovery Given a set of quality measures one can use the DFD to determine which quality measures are better than others in distinguishing the top k subgroups from false discoveries This gives an objective criterion for selecting a quality measure that is more likely to produce exceptional results Finally given some desired signicance level 002  one could extract from the DFD a minimum threshold for the quality measure at hand When validating single subgroups we see that our method removes insignicant subgroups found on datasets that have few numeric attributes From metalearning we nd that on large datasets for instance with more than ve numeric attributes the random baseline is more likely to accept many patterns This is reasonable because of the associated larger hypothesis space Table II shows that our method can remove insigincant subgroups on some of the datasets with more than ve numeric attributes but not on all of them When we validate quality measures we have outlined that the method we described determines the extent to which a quality measure is also an exceptionality measure We have seen that of the twelve measures for subgroup discovery we tested 006 2 is the best exceptionality measure and Purity and Sensitivity are by far the worst For the EMM correlation model variant no signicant conclusions can be drawn from the modest experiments In this paper we have presented a technique making extensive use of swap randomization Notice that we do not by any means claim to have invented this particular randomization method Also its use in step I of the method we introduced in this paper is not the only option available We have extensively explained why using swap randomized data leads to a good model for false discoveries but it comes at a price for every result of a subgroup discovery run one wishes to validate one has to run the same subgroup discovery algorithm an additional m times where m needs to be large enough to satisfy the constraints of the Central Limit Theorem In the more traditional subgroup discovery setting one can usually afford this extra computation time 
159 


For more complex settings for instance the EMM variant using Bayesian networks introduced in this becomes problematic When computation time becomes an issue one might consider different randomization techniques to generate R 1 R m  for instance by simply drawing a random sample from 002 of a certain size for each R i  Before such a technique can be employed its theoretical ramications need to be explored In future work we also plan to empirically investigate the effect of certain parameters on the outcome of the method A CKNOWLEDGMENTS This research is nancially supported by the Netherlands Organisation for Scientic Research NWO under project number 612  065  822 Exceptional Model Mining R EFERENCES  J Friedman N Fisher  Bump-Hunting in High-Dimensional Data Statistics and Computing 9\(2 pp 123143 1999  W  Kl  osgen Subgroup Discovery Handbook of Data Mining and Knowledge Discovery ch 16.3 Oxford University Press New York 2002  S D Bay  M J P azzani Detecting group dif ferences Mining contrast sets Data Mining and Knowledge Discovery 5\(2 pp 213246 2001  G Dong J Li Ef cient mining of emer ging patterns Discovering trends and differences Proc KDD pp 4352 1999  P  Kralj No v a k N La vra 031 c G I Webb Supervised Descriptive Rule Discovery A Unifying Survey of Contrast Set Emerging Pattern and Subgroup Mining Journal of Machine Learning Research 10 pp 377403 2009  Y  Hochber g A T amhane Multiple Comparison Procedures Wiley New York 1987  J F  urnkranz P A Flach ROC n Rule Learning  Towards a Better Understanding of Covering Algorithms Machine Learning 58 1 pp 39-77 2005  N La vra 031 c P Flach B Kav 031 sek and L Todorovski Rule induction for subgroup discovery with CN2-SD Proc ECML/PKDD 2002  H Grosskreutz Cascaded subgroups disco v ery with an application to regression Proc ECML/PKDD 2008  B F  I Pieters A Knobbe S D 031 zeroski Subgroup Discovery in Ranked Data with an Application to Gene Set Enrichment Proc Preference Learning workshop PL2010 at ECML PKDD 2010  W  Dui v esteijn A Knobbe A Feelders M v an Leeuwen Subgroup Discovery meets Bayesian networks an Exceptional Model Mining approach Proc ICDM pp 158167 2010  D Leman A Feelders A Knobbe Exceptional Model Mining Proc ECML/PKDD Part II pp 116 2008  Y  H Xu A Fern On Learning Linear Ranking Functions for Beam Search Proc ICML 2007 ACM International Conference Proceeding Series vol 227 pp 10471054 ACM New York  H Grosskreutz S R  uping On Subgroup Discovery in Numerical Domains Data Mining and Knowledge Discovery 19\(2 pp 210226 2009  A Gionis H Mannila T  Mielik  ainen P Tsarapas Assessing data mining results via swap randomization Proc KDD pp 167176  2006  A M L yapuno v  Nouv elle forme du th  eor  eme sur la limite de probabilit  e St Petersburg 1901  K Pearson L Filon Mathematical contrib utions to the theory of evolution iv on the probable errors of frequency constants and on the inuence of random selection on variation and correlation Phil Trans A 191 pp 229311 1898  J Dem 031 sar Statistical Comparisons of Classiers over Multiple Data Sets Journal of Machine Learning Research 7 pp 130 2006  M Friedman The use of ranks to a v oid the assumption of normality implicit in the analysis of variance Journal of the American Statistical Association 32 pp 675701 1937  M Friedman A comparison of alternati v e tests of signicance for the problem of m rankings Annals of Mathematical Statistics 11 pp 8692 1940  P  B Nemen yi Distrib ution-free multiple comparisons PhD thesis Princeton University 1963  M Ojala G C Garriga A Gionis H Mannila Ev aluating Query Result Signicance in Databases via Randomizations Proc SDM pp 906917 2010  N Me giddo R Srikant Disco v ering Predicti v e Association Rules Proc KDD pp 274278 1998  W  Kl  osgen Explora A multipattern and multistrategy discovery assistent Advances in Knowledge Discovery and Data Mining pp 249-271 1996  C Silv erstein S Brin R Motw ani Be yond Mark et Bask ets Generalizing Association Rules to Dependence Rules Data Min Knowl Discov 2 1 pp 3968 1998  P N T an V  K umar  J Sri v asta v a Selecting the right inter estingness measure for association patterns Proc KDD pp 3241 2002  G I W ebb Disco v ering Signicant P atterns Machine Learning 68 1 pp 133 2007  M Meeng A J Knobbe Fle xible Enrichment with Cortana  Software Demo Proc Benelearn pp 117119 2011  D Ne wman S Hettich C Blak e C Merz UCI repository of machine learning databases 1998 
160 


cc ur ac y   Fig. 4. Accuracy for Model-based clustering and rule-based classification Fig. 5. Distribution of flow attributes in scatter plot matrix In Fig.5, the distribution of IP and ports for source and destination is depicted for Model-based clustering method. For HTTP \(blue traffic to few destination IPs. HTTP behavior is prominent because a number of source hosts are communicating with small number of destination hosts, i.e., HTTP servers. In DNS application, source hosts communicate with a set of DNS servers. Each host interacts with different domains such as www, .org and .net for resolving IP addresses. The DNS behavior is prominent because large group of source hosts communicate with small repeated set of destination IPs. In the IRC application, a group of hosts communicates with other group of hosts. There is roughly one-to-one correspondence between source and destination IP. This behavior is prominent because IRC \(cross with approximately equal number of destination IPs. Moreover, source IP and destination port distribution show that destination port numbers are in the range of 30000 for SMTP green where as HTTP and MAIL have destination ports number in lower range of 1000. Model-based clustering with the Apriori algorithm provides reliable and accurate classification model as compared to K-mean method. The association rules helps us predict flows for particular application with high confidence and lift values IV. CONCLUSION In conclusion, we have presented a classification model that achieves high flow classification accuracy with application behavior profiling. The use of the K-mean algorithm for Netflow classification was shown to be inefficient. On the other hand, Model based clustering with association rule mining techniques provided a much better accuracy. Moreover the rule heuristics are produced automatically, making the algorithm modular and independent of the dataset. In addition 


our model is able to detect new behavior patterns for next generation applications. As a future work, we are planning to analyze a bigger database of traces and define behavior patterns for a wider range of applications REFERENCES 1] F. Baker, B. Foster, and C. Sharp. Cisco architecture for lawful intercept in IP networks. Internet Engineering Task Force, RFC, 3924, 2004 2] C. Borgelt and R. Kruse. Induction of association rules: Apriori implementation. In Compstat: Proceedings in Computational Statistics 15th Symposium Held in Berlin, Germany, 2002, page 395. Physica Verlag, 2002 3] WAND Trace Catalogue. http://www.wand.net.nz/wits/catalogue.php 4] C. Dewes, A. Wichmann, and A. Feldmann. An analysis of Internet chat systems. In Proceedings of the 3rd ACM SIGCOMM conference on Internet measurement, pages 5164. ACM, 2003 5] J. Erman, A. Mahanti, and M. Arlitt. Internet traffic identification using machine learning. In Proceedings of IEEE GlobeCom. Citeseer, 2006 6] C. Fraley and A.E. Raftery. MCLUST version 3 for R: Normal mixture modeling and model-based clustering. Technical report, Citeseer, 2006 7] M. Iliofotou, H. Kim, M. Faloutsos, M. Mitzenmacher, P. Pappu, and G. Varghese. Graph-based P2P traffic classification at the internet backbone. In IEEE Global Internet Symposium. Citeseer, 2009 8] T. Karagiannis, A. Broido, and M. Faloutsos. Transport layer identification of P2P traffic. In Proceedings of the 4th ACM SIGCOMM conference on Internet measurement, pages 121134. ACM, 2004 9] David Kotz, Tristan Henderson, Ilya Abyzov and Jihwang Yeo. CRAWDAD trace dartmouth/campus/tcpdump/fall01 \(v. 2004-11-09 http://crawdad.cs.dartmouth.edu/dartmouth/campus/tcpdump/fall01 November 2004 10] A.W. Moore and K. Papagiannaki. Toward the accurate identification of network applications. Passive and Active Network Measurement, pages 4154, 2005 11] TTT Nguyen and G. Armitage. A survey of techniques for internet traffic classification using machine learning. IEEE Communications Surveys Tutorials, 10\(4 12] I. Papapanagiotou and M. Devetsikiotis. Aggregation Design Methodologies for Triple Play Services. In IEEE CCNC 2010, Las Vegas, USA pages 15, 2010 13] V. Paxson. Bro: A system for detecting network intruders in real-time Comput. Networks, 31\(23 14] J. Xu, J. Fan, M. Ammar, and S.B. Moon. On the design and 


performance of prefix-preserving IP traffic trace anonymization. In Proceedings of the 1st ACM SIGCOMM Workshop on Internet Measurement, page 266. ACM, 2001.80 


denote the input attribute with the minimum integrated cost after the split, and let the set of allocated sample sizes, computed by the sample allocation method explained later, be ASM . Then DM | children are generated for the node M . For each child CHi, i = 1, . . . , |DM |, the associated query is QN ?{M = mi sample size is asi ? ASM , and potential splitting attributes is PSN ? M . The process of splitting is then recursively applied to children of node N In the process of calculating costs during the strati?cation process, we need to perform sample allocation, i.e., divide the parent nodes sample size among the potential children nodes This is required for calculating the integrated cost for the potential split. This is based on our sample allocation method, which we 328 describe next in Section IV-B. Furthermore, for calculating the integrated cost, the variance of target value ?2i and probability of output attribute A = a, ?i, for each stratum is computed based on the pilot sample Initially, the strati?cation process on the query space begins by calling Stratification\(R, null, F IA, n, null root node. The process of strati?cation would stop if there is no leaf node with integrated cost larger than a prede?ned threshold Each leaf node in the tree is a ?nal stratum for sampling, and the associated sample size denotes the number of data records drawn from the subpopulation of the stratum B. An Optimized Sample Allocation Method Now, we introduce our optimized algorithm for sample allocation which integrates variance reduction and sampling cost As introduced in section IV-A, integrated cost is de?ned by taking into account of variance of estimation and sampling cost The goal of our sample allocation algorithm is to minimize the integrated cost by choosing the sample size, ni, for each stratum In our algorithm, we adjust the value of SampCost and ?2s so that their in?uences on the integrated cost are in the same unit SampCost SampCost SmpCost\(r where SmpCost\(r entire population, and ?r denotes the probability of A = a being true for the entire population 2 2s 


2r where ?2r 2 n denotes the variance of estimation of the target value on the entire population The key constraint on the values of the sample sizes for each strata is that their sum should be equal to the total sample size A vector n = {n1, n2, . . . , nH} is used to represent the sample sizes, where the ith element, ni, is the sample size for the ith stratum By including sampling cost and variance of estimation into the integrated cost, our sample size determination task leads to the following optimization problem Minimize Cost\(n  i\(?s ni i v N2i niN2 2 i subject to  i ni = n 6 where Ni denotes the population size of data records under the space of A = a in ith stratum. Note that this value may not be known if A is an output attribute. However, the total number of records in the ith stratum is typically known, and can be denoted as DNi. Then Ni can be estimated by ?iDNi, and the population size of A = a on the entire population is estimated by N  i ?i DNi For ?nding the minimum of integrated cost, we utilize Lagrange multipliers, a well know optimization method. Lagrange multipliers aims at ?nding the extrema of a function object to constraints. Using this approach, a new variable ? called a Lagrange multiplier is introduced and de?ned by n n  


 i ni ? n If n is a minimum solution for the original constrained problem then there exists a ? such that \(n Lagrange function. Stationary points are those points where the partial derivatives of ?\(n n,??\(n 7 In our problem, by conducting partial derivatives on Formula 7 a group of equations are yielded as follows s i v N 2 i n2iN2 2 i + ? = 0 i = 1, ..H i ni = n 8 where the solution n leads to the minimum value of integrated cost in Formula 5 However, it is dif?cult to solve the group of equations directly Thus, we use numerical analysis to approximate the real solution. Newtons method is utilized for ?nding successively better approximations to the zeroes \(or roots Given an equation f\(x x tive of function f\(x method iteratively provides, xt+1, a better approximation of the root, based on xt, the previous approximation of root according to the following formula xt+1 = xt ? f\(xt f ?\(xt The iteration is repeated until a suf?ciently accurate value is reached, i.g. |f\(xt In our problem of Formula 8, there are H + 1 equations F \(xt xt xt n1, .., nH , ?}, where the equations in F \(xt fi\(xt s i v N 


2 i n2iN2 2 i + ? i = 1, .., H fH+1\(xt  i ni ? n The Newtons method is also applied iteratively via the system of linear equations JF \(xt xt+1 ? xt xt 9 where JF \(xt H + 1 H + 1 equation system F \(xt vector xt. The entry JF \(i, j d\(fi\(x dxj where fi\(x xt x From the Expression 9, a better approximation xt+1 is obtained based on previous approximation xt. The iterative procedure would be stopped if ?i|fi\(xt threshold, and then the sample size ni is allocated for each stratum so that the integrated cost is minimized. In reality, we are required to pick an integral number of records from each stratum during the sampling step. Thus, we round down each ni to its nearest integer, ni + 0.5 In the example shown in Table I, suppose we set both weights v and ?s to be 0.5. Further, assume the variance of the entire population, ?2r , to be 80000. The probability of A = a over the entire population ?r is 0.242. By using the proposed optimized sample allocation method, the sample sizes for the three strata 329 are 162, 299, and 139, respectively. In this case, the variance of estimation according to the Expression 2 is 93.66, and the estimated cost is 1943.7. We can see that, compared with Neyman allocation, the sampling cost is decreased by 42.1%, but results some increase in variance. Overall, this example shows that we can achieve lower sampling cost by trading off some accuracy C. Overall Sampling Process Algorithm 2 DiffRuleSampling\(DW1, DW2, F IA, t, St 1: PS ? a pilot sample from DW1, DW2 


2: DR ? identi?ed rules from PS 3: OA ? output attributes of DW1, DW2 4: for all R : X ? DW1\(t t 5: if X ?OA = null then 6: Acquire St data records from the space of X 7: else 8: R ? root node 9: Lf ? null 10: Strati?cation\(R,null, F IA, St, Lf 11: for all N ? Lf do 12: s ? sample size of N 13: Draw s data records from the subpopulation of N 14: end for 15: end if 16: Update the mean value of DW1\(t t 17: end for Algorithm 2 shows the overall sampling process for differential rule mining on two deep web data sources, DW1 and DW2 and with differential attribute t. The inputs of the algorithm also contain the full set of input attributes FIA as well as the sample size St. The algorithm starts with a pilot sample PS, from which the differential rules are identi?ed. For the rule R : X ? DW1\(t t St data records are directly drawn from the space of X \(Lines 5-6 t t containing output attributes, query spaces of DW1 and DW2 are strati?ed and sample is recursively allocated to each stratum with corresponding query subspace \(Line 10 of the tree built by strati?cation, a sample is drawn according to its sample size \(Line 13 t t is updated by the further sample \(Line 16 for association rule mining is very similar and not shown here V. EVALUATION STUDY We evaluate our sampling methods for association mining and differential rule mining on the deep web using two datasets described below US Census data set: This is a 9-attribute real-life data set obtained from the 2008 US Census on the income of US households. This data set contains 40,000 data records with 7 categorical attributes about the race, age, and education level of the husband and wife of each household and 2 numerical attributes about the incomes of husband and wife 


Yahoo! data set: The Yahoo! data set, which consists of the data crawled from a subset of a real-world hidden database at http://autos.yahoo.com/. Particularly, we download the data on used cars located within 50 miles of a zipcode address. This yields 30,000 data records. The data consists of 7-attribute with 6 categorical attributes about the age, mileage, brand, etc, of the cars and one numerical attribute, which is the price of the car Variance of Estimation is estimated for the target value \(i.e mean value in differential rule mining, and con?dence in association rule mining is calculated according to the Expression 2. Since variance of estimation reveals the variation of the estimated value from the true value, smaller variance suggests better estimation Sampling Cost is estimated by the number of queries submitted to data sources in order to acquire a certain number of data records containing target output attributes, i.e. A = a. Larger sample size implies higher sampling costs in a deep web setting where the queries are executed over the internet Estimate accuracy is estimated by Absolute Error Rate \(AER Small AER value indicates higher accuracy. For an estimator on variable Y with true value y and estimated value y, the AER of the estimator is calculated by AER\(y A. Association Rule Mining In this section, we present the results of our method for association rule mining. Using our overall approach, we have created four different versions, which correspond to four different sets of weights assigned to variance of estimation and sampling costs. 1 the weight ?v = 1.0 and ?s = 0.0, 2 the weight ?v = 0.7 and ?s = 0.3, 3 v = 0.5 and ?s = 0.5, and 4 weights ?v = 0.3 and ?s = 0.7. In addition, we also compare these approaches with a simple random sampling method, which is denoted by Random We focus on the queries in the form of A = a ? B = b where A and B are output categorical attributes. Other categorical attributes in the data set are considered as input attributes Our goal is to estimate Supp\(A=a,B=b A=a association rules are randomly selected from the datasets. Each of the 50 association rules are re-processed 100 times using 100 different \(pilot sample, sample iterations result is the average result for 5000 executions 


In all charts reported in this section, the X-axis is k, which denotes the size of sample under the space of a target rule drawn from deep web. The sample size for each point on X-axis is k x, where x is a ?xed value for our experiment, and depends upon the dataset. At each time, queries are issued to obtain kx data records under the space of a target rule. Overall, all our experiments show the variance of estimation, sampling costs and sampling accuracy with varying sample size Figure 1 shows the result from our strati?ed sample methods on the US census data set. The size of pilot sample is 2000, from which all of the 50 initial rules are derived. In this experiment the ?xed value x is set to be 300, which means the smallest sample size at k = 1 is 300, and the largest sample size at k 10 is 3000. Figure 1 a the ?ve sampling procedures. Figure 1 b cost for the sampling procedures. In order to better illustrate the experiment result, in each execution of sampling, the variance of 330 6DPSOLQJ9DULDQFH            9D UL DQ FH R I V WL PD WL RQ  


9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW           6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF            5 


 9DU 9DU 9DU 5DQG c Fig. 1. Evaluation of Sampling Methods for Association Rule Mining on US Census Dataset estimation and sampling cost for the sampling procedures var7 var5, var3, and rand are normalized by the corresponding values of Full Var. Thus, in our experiment, the values of sampling cost and variance of estimation for sampling procedure Full Var are all 1. Furthermore, Figure 1 c sampling procedures From Figure 1 a pared with sampling procedures Var7, Var5 and Var3, Full Var has the lowest estimation variance and the highest sampling cost. From sampling procedures Var7, Var5, and Var3, we can see a pattern that the variance of estimation increases, and the sampling cost decreases consistently with the decrease of the weight for variance of estimation. At the largest sample size of k = 10, the estimation variance of sampling procedure Var3 is increased by 27% and the sampling cost is decreased by 40 compared with sampling procedure Full Var. The experiment shows that our method decreases the sampling cost ef?ciently by trading off a percent of variance of estimation. Similar to variance of estimation, the sampling accuracy of these procedures also decreases with the decrease of the weight on variance of estimation. For the largest sample size at k = 10, we can see that the AER of sampling procedure Var3 is increased by 20 compared with sampling procedure Full Var. However, for many users, increase of the AER will be acceptable, since the sampling cost is decreased by 40%. By setting the weights for sampling variance and sampling cost, users would be able to control the trade-off between the variance of estimation, sampling cost, and estimation accuracy In addition, compared with sampling procedure of Full Var Var7, Var5, and Var3, sampling procedure Random, has higher estimation of variance, sampling cost and lower estimation accuracy. Thus, our approach clearly results in more effective methods than using simple random sampling for data mining on the deep web Figure 2 shows the experiment result of our proposed strati?ed 


sampling methods on the Yahoo! data set. The size of pilot sample on this data set is 2,000, and the ?xed value x for sample size is 200. The results are similar to those from the US census dataset. We can still see the pattern of the variance of estimation increasing with the decrease of its weight. Besides, the sampling accuracy is also similar to the variance of estimation. However although the variance estimation of sampling procedure Random is 60% larger than sampling procedure Full Var, the sampling cost of Random is 2% smaller than Full Var. This is because Full Var does not consider sampling cost. It is possible that Full Var assigns a large sample to a stratum with low ?, which denotes the probability of containing data records under the space of A = a, resulting the larger sampling cost than that of simple random sampling. Sampling procedures Var7, Var5, Var3 consider sampling cost as well, and have smaller variance estimation and sampling cost, compared with Random. Furthermore, Random has smaller sampling accuracy than Full Var, Var7 and Var5, but has larger sampling accuracy than Var3. This is because Var3 assigns much more importance to the sampling cost, and loses accuracy to a large extent To summarize, our results shows that our proposed strati?ed sampling are clearly more effective than simple random sampling on the deep web. Moreover, our approach allows users to tradeoff variance of estimation and sampling accuracy to some extent while achieving a large reduction in sampling costs B. Differential Rule Mining In this section, we present results from experiments based on differential rule mining. Particularly, we look at the rules of the form A = a ? D1\(t t categorical attribute and t is an output numerical attribute, while other categorical attributes in the data set are considered as input attributes In this experiment, we also evaluate our proposed method with different weights assigned to variance of estimation and sampling cost. Five sampling procedures, Full Var, Var7, Var5,Var3 and Random, have same meanings with those in the experiments of association rule mining. Similarly, 50 rules are randomly selected from the datasets, and each of the 50 differential rules are reprocessed 100 times using 100 different \(pilot sample, sample iterations 5000 runs First, we evaluated the performance of these procedures on 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


