 Abstract  Mining generalized association rules with fuzzy taxonomic structures has been recognized as a important extension of generalized associations mining problem To date most work on this problem however required the taxonomies to be static ignoring the fact that the taxonomies of items cannot necessarily be kept unchanged For instance some items may be reclassified from one hierarchy tree to another for more suitable classification abandoned from the taxonomies if they will no longer be produced or added into the taxonomies as new items Additionally the membership degrees expressing the fuzzy classification may also need to be adjusted Under these circumstances effectively updating the discovered generalized association rules is a crucial task In this paper we examine this problem and propose two novel algorithms called FDiff_ET and FDiff_ET2 to update the discovered frequent generalized itemsets I I NTRODUCTION ince Srikant and Agrawal in 1995  i n t r od u c e d th e problem of mining generalized association rules from transaction database with taxonomy information lots of subsequent researches on algorithmic improvement or model extension have been conducted One of the prosperous avenues on extending generalized association mining is to consider fuzzy taxonomy 1  i n w hic h the c hil d n o d e c a n partially belong to its parent-node with a certain degree in 0 1 For example consider the fuzzy taxonomy in Fig 1 wherein HP Officejet is regarded as being both Printer and Scanner  but to different membership degrees Laser HP Officejet ScanMaker Printer Scanner Peripheral  PC Desktop Notebook 1 1 11 1 10.70.3 Fig 1 An example of fuzzy taxonomy T  To our best knowledge all work to date on generalized association rules with fuzzy taxonomies assumes that the taxonomic structure is static not considering the situation that the taxonomy may change as time passes some items will be W.Y Lin is with the National University of Kaohsiung Kaohsiung City Taiwan corresponding author to provide phone 886-7-5919517 fax 886-7-5919514 e-mail wylin@nuk.edu.tw M.C Tseng is with the Refining Business Division CPC Corporation Taiwan e-mail clark.tseng@msa.hinet.net J.H Su is with the National Cheng Kung University Tainan City Taiwan e-mail bb0820@ms22.hinet.net shifted from one classification to another some trees of the taxonomies will be merged together or be split into smaller trees if the items on the trees cannot meet the demands in a new classification items will be abandoned if those items do not be produced any more and newborn items are added Under these circumstances how to update the discovered generalized association rules effectively becomes a critical task Our previous work 16 h a s s t u die d t h i s p r obl e m of updating generalized association rules but the taxonomy structure is crisp In this paper we extend our work to deal with the issue that the taxonomy is fuzzy Two algorithms FDiff_ET and FDiff_ET2 modified from our previously proposed algorithms Diff_ET and Diff_ET2 16  are proposed The remainder of the paper is organized as follows A review of related work is given in Section 2 The problem of updating previously discovered generalized association rules when the fuzzy taxonomic structure is evolved is formalized in Section 3 In Section 4 we elaborate on the proposed two algorithms and give an example to illustrate the ideas Finally our conclusions are stated in Section 5 II R ELATED W ORK The problem of mining association rules in presence of taxonomy information was first addressed in 6 an d  1 3   independently In 13  t h e p r o b l em w a s ref er red t o m i n i n g generalized association rules aiming to find associations among items at any crossing levels of the taxonomies under the minimum support and minimum confidence constraints In 6 t h e problem o f c on cern w a s s om e w h a t dif f eren t i n t h a t they generalized the uniform minimum support constraint to a level-wise assignment i.e items at the same level received the same minimum support The objective is to develop mining associations level-by-level in a fixed hierarchy That is only associations among items on the same level are examined progressively from the top level to the bottom Since then several algorithmic improvements 8 1 or pr o ble m extensions 4 15 h a v e b e e n p r op os e d  The problem of mining generalized association rules with fuzzy taxonomic structures was first proposed by Wei and Chen   L a t e r i n  2  t h e y p r opos e d a n e x t e n s ion of t h e Apriori algorithm to accomplish this mining task Various extensions of their work have been made recently including the concept of introducing generalized weights to distinguish the importance of items in the taxonomy 9 12  qu a n ti t a ti v e attributes 7  m u lt i ple m i n i m u m s u ppo r t s  10   a t t r ibu t e Updating Generalized Associat ion Rules with Evolving Fuzzy Taxonomies Wen-Yang Lin Ming-Cheng Tseng and Ja-Hwung Su S 978-1-4244-8126-2/10/$26.00 ©2010 IEEE 


generalization 1 11  Compared with the substantial amount of research on mining generalized association rules there is a lack of effort devoted to the problem of mining generalized associations with taxonomy evolution In 16 17  w e h a v e co n s i d ered t h i s problem and some of its extensions and have proposed efficient Apriori-like algorithms III P ROBLEM S TATEMENT A Mining generalized association rules with fuzzy taxonomies Let I   i 1  i 2    i m beasetofitemsand DB  t 1  t 2    t n  be a set of transactions where each transaction t i  TID A  has a unique identifier TID and a set of items A  A  I  We assume that the fuzzy taxonomy of items T  is available and is denoted as a directed acyclic graph on I  J  where J  j 1  j 2    j p  represents the set of generalized items derived from I Anedgein T denotes an is-a relationship that is if there is an edge from j to i  we call j a parent generalization of i and i a child of j  For example in Fig 1 I  Laser  HP Officejet  ScanMaker  Desktop  Notebook and J  Scanner  Printer  PC  Peripheral  In a crisp taxonomy we assume that the child item belonging to its ancestor has a membership degree with 1 But in a fuzzy taxonomy the membership degree of an item may relate to all nodes of the taxonomy For any two nodes x and y in the taxonomy the membership degree  xy of an item y belonging to its ancestor node x can be calculated as follows    xy  y x l   012   l e on    le 1 where l x  y is one of the paths of accessing x from y  e on l is oneoftheedgesonpath l and  le is the membership degree on the edge e on l Operators 012 and  depend on the problem of concern For illustrative purposes max stands for 012 and min for   Furthermore  xx is set to 1 indicating that any item is fully belonging to itself Next consider calculating the support of items under a fuzzy taxonomy If a is an item in a certain transaction t  DB  and x is an item in certain itemset X  then the membership degree  xa with which a belongs to x can be obtained according to 1 Finally the support sup  X  count  X  DB  where count  X  denotes the accumulated degree that every transaction supports X in DB  calculated as follows   max  min   xa DB t t a X x DB t tX X count   015 015       2 Example 1 Consider Fig 1 The membership degree  Peripheral  Laser  min    Printer  Laser    Peripheral  Printer   min 1 1  1 Similarly  Peripheral  Officejet  max min    Printer  officejet    Peripheral  Printer  min    Scanner  officejet    Peripheral  Scanner   max min 0.7 1 min 0.3 1  0.7 Now consider the transaction database in Table 1 Let X  Peripheral  PC  The degree that the first transaction supports X is min max   Peripheral  Laser   Peripheral  HP Officejet  max   PC  Laser   PC  HP Officejet   min max 1 0.7 max 0 0  0 TABLE 1 A TRANSACTION DATABASE  DB  TID Items Purchased 11 Laser HP Officejet 12 HP Officejet  Desktop 13 ScanMaker  Desktop 14 HP Officejet  Notebook 15 Notebook 16 ScanMaker  Notebook Given a user-specified minimum support ms and a minimum confidence mc  the problem of mining generalized association rules is to discover all generalized association rules whose support and confidence levels are larger than the specified thresholds This problem is usually reduced to the problem of finding all frequent itemsets for a given minimum support After an initial discovery of all the generalized association rules in DB let L be the set of all generalized frequent itemsets with respect to ms  As time passes some update activities may occur to the taxonomies due to various reasons 5  L et T  denote the updated taxonomies The problem of updating discovered generalized association rules in DB is to find the set of frequent itemsets L  with respect to the refined taxonomies T   B Types of taxonomy updates Our previous work 1 ha s i de nt if i e d f o u r b a s ic t y pe s o f item updates that will cause taxonomy evolution item insertion  item deletion  item rename and item reclassification  The essence of frequent itemsets update for each type of taxonomy evolutions is further clarified in what follows New type of taxonomy update specific to the fuzzy taxonomic structure is introduced as well Note that hereafter the term  item  refers to a primitive or a generalized item Type 1 Item insertion The strategies to handle this type of update operation are different depending on whether the inserted item is primitive or generalized When the new inserted item is primitive we cannot process this item until there is an incremental database update because the new item does not appear in the original set of frequent itemsets On the other hand if the new item represents a generalization then the insertion itself also has no effect on the discovered associations until the new generalization incurs some item reclassification Fig 2 shows an example of this type of taxonomy evolution where a new item  J  is inserted as a primitive item or a generalized item Note that in Fig 2b item  E  is reclassified to the generalization represented by  J   


A G C I D B  F H E J A G C I D B F H J E a b Fig 2 An example of taxonomy evolution caused by item insertion The in 012\012\015\012\012\012\012\012 neralized Type 2 Item deletion This case is similar to the insertion case Nothing has to be done with the deletion of a primitive item if there is no transaction update to the original database Notably the removal of a generalization may lead to items reclassification Fig 3 shows an example of this type of taxonomy evolution Type 3 Item rename When items are renamed we do not have to process the database Instead we just replace the frequent itemsets with new names and so the association rules A G C I D B F H A G C I D B F H E E a b Fig 3 An example of taxonomy evolution caused by item deletion a The primitive 012\012\012 e 012\012\012	\012\012\012\012		\012	\012 012\012	\012\012\012 !	\012"\012 Type 4 Item reclassification Among the four types of taxonomy updates this is the most profound operation Once an item primitive or generalized is reclassified into another category all of its ancestor generalized items in the old and the new taxonomies are affected In other words the supports of these affected generalized items have to be updated and so do the frequent itemsets containing any one of the affected generalized items For example in Fig 4 the two shifted items E and G will change the support counts of generalized items A B and F and also affect the support counts of itemsets containing A B or F A G C I D B F H E A E C I G B F H D Fig 4 An example of taxonomy evolution caused by item reclassification Intuitively these four basic types of operations work on a fuzzy taxonomic structure as well But an additional operation has to be included to accomplish the situation of membership adjustment Type 5 Membership adjustment This operation refers to adjusting the membership degree of an item in the taxonomy For example one may change in Fig 1 the degrees that HP Officjet belongs to Printer and Scanner to 0.8 and 0.2 respectively Unlike item reclassification adjusting the membership of an item x has no effect on the support counting of x itself no matter x is primitive or generalized It instead would change the support counting of the ancestors of x  In this paper we assume that there is no transaction update to the original database That is the transaction database is unchanged before and after the evolution of taxonomic structure IV T HE PROPOSED METHODS Let ED denote the extended version of DB by adding in taxonomies T the ancestors of each primitive item to each transaction while ED  denotes the extension of DB by adding generalized items in the updated taxonomies T  A straightforward method to find updated generalized frequent itemsets would be to run the algorithm proposed by Chen and Wei 2 f o r f i n d i ng ge ne r a liz e d f r e que nt i t e m se ts o n the updated extended transactions ED   This simple way however ignores the fact that many discovered frequent itemsets would not be affected by the taxonomy evolution If we can identify the unaffected itemsets then we can avoid unnecessary computations in counting their supports In view of this we adapt the Apriori-like maintenance framework we used in 17  Each pass of mining the frequent k itemsets involves the following main steps 1 Generate candidate k itemsets C k from the discovered set of frequent  k 1\-itemsets  k  2 Differentiate in C k the affected itemsets   k C romthe unaffected ones   k C  3 Incorporate L k   k C and  k C to determine whether a candidate itemset is frequent or not in the resulting database ED   4 Scan DB with T  i.e ED   to count the support of itemsets that are undetermined in Step 3 Below we elaborate on each of the kernel steps i.e Steps 2and3 A Differentiation of affected and unaffected itemsets Definition 1 An item primitive or extended is called an affected item if its support would be changed with respect to the taxonomy evolution otherwise it is called an unaffected item   Consider an item x  T  T  and the three independent subsets T  T   T   T and T  T   There are three different cases in differentiating whether x is an affected item or not The case that x is a renamed item is neglected for which can be simply regarded as an unaffected item Case 1 x  T  T   That is x is an obsolete item Then the support of x in the updated database should be counted as 0 and so x is an affected item Case 2 x  T   T That is x denotes a new item If x is a primitive item it doest not appear in the database and so can be regarded as an unaffected item Otherwise x should be 


regarded as an affected item since its count shall change from zero to nonzero Case 3 x    T   T   This case also depends on whether x is a primitive or generalized item Obviously if x is a primitive item there is no change on its support and so x is an unaffected item On the other hand if x is a generalized item its count may change or not depending on whether any of its descendants is involved in any evolution operation of item reclassification or membership adjustment Recall that in 2 the support counting of an itemset is primarily determined by calculating the membership degree in 1 Thus we can derive a simple rule for determining a generalized item as affected or not as clarified in the following lemma Let Q  x nd Q   x enotein T and T  respectively the set of primitive descendants of x  each along with their membership degrees with respect to x  More specifically Q  x    y 1   xy 1   y 2   xy 2    y n   xy n  and Q   x    y  1   xy  1   y  2   xy  2    y  m   xy  m  where  y 1  y 2    y n and y  1  y  2    y  m  are the descendants of x in T and T   respectively Lemma 1 Consider a generalized item x  T  T  If Q  x   Q   x  then count ED  x   count ED   x  where Q  x   Q   x  means m  n  y 1  y  1    y n  y  n and  xy 1   xy  1     xy n   xy  n  Proof Consider a transaction t in DB If t supports x with respect to ED  there exists at least one item being primitive descendant of x in T say d 1  d 2    d i and  d 1  d 2    d i    y 1  y 2    y n   Likewise if t supports x with respect to ED  then there must exist at least one item being primitive descendant of x in T  say d  1  d  2    d  j and d  1  d  2    d  j    y  1  y  2    y  m   Since m  n and y 1  y  1    y n  y  n  it follows that i  j and d 1  d  1    d i  d  i  Further  xd 1   xd  1     xd i   xd  i  According to 2 the degree that t supports x in ED is equal to that in ED   The lemma then follows  Example 2 Suppose that the taxonomy in Fig 1 is refined to that in Fig 5 with the inclusion of a new subcategory of peripheral Fax aswellastheadjustingofmembership degrees of HP Officejet  Clearly Fax is an affected item Case 2 Now consider the generalized items satisfy Case 3 i.e Printer  Scanner  Peripheral and PC  PC is unaffected since it possesses the same set of primitive descendants each of which retains the same membership degree However Printer  Scanner and Peripheral are affected items Although each of them has the same set of primitive descendants at least one of its primitive descendants changes the membership degree Fig 5 A refined fuzzy taxonomy of T in Fig 1 Definition 2 For a candidate itemset A wesay A is an affected itemset if it contains at least one affected item  Definition 3 A transaction is called an affected transaction if it contains at least one of the affected items with respect to a taxonomy evolution  B Inference of Frequent and Infrequent Itemsets Now that we have clarified how to differentiate the unaffected and affected itemsets we will show how to utilize this information to determine in advance whether or not an itemset is frequent before scanning the updated extended database ED   We observe that there are four different cases 1 If A is an unaffected itemset and is frequent in ED thenit is also frequent in ED   2 If A is an unaffected itemset and is infrequent in ED then it is also infrequent in ED   3 If A is an affected itemset and is frequent in ED thenit may be frequent or infrequent in ED   4 If A is an affected itemset and is infrequent in ED thenit may be frequent or infrequent in ED   Note that only cases 3 and 4 need further database scan to determine the support c ount of A  Table 2 summarizes the above discussion TABLE 2 F OUR CASES FOR INFERRING WHETHER A CANDIDATE ITEMSET IS FREQUENT OR NOT  T  T  LED  Action Case  frequent no 1 unaffected  infrequent no 2   scan ED  3 affected   scan ED  4 C Algorithm FDiff_ET Based on the aforementioned concepts the proposed FDiff_ET algorithm is described in Fig 6 First let candidate 1-itemsets C 1 be the set of items in the new item taxonomies T   Next derive the membership degree among all items in C 1 represented as a matrix and identify affected items for dividing candidate itemsets Then load the original frequent 1-itemsets L 1 and divide C 1 into two subsets  1 C and  1 C   1 C consists of unaffected 1-itemsets in L 1 and  1 C contains affected 1-itemsets where  1 C is for Cases 1 and 2 and  1 C is for Cases 3 and 4 According to Case 2 all itemsets in  1 C that is not in L is infrequent and so are pruned Then compute the support counts of each 1-itemset in  1 C over only transactions that contain affected items in ED   After this we create new frequent 1-itemsets 1 L  by combining  1 C and those itemsets being frequent in  1 C The next cycle is that we generate candidates 2-itemsets C 2 from 1 L  and repeat the same procedure until no frequent k itemsets k L  are created Laser HP Officejet ScanMake r Printer Scanner Peripheral  PC Desktop Notebook 1 1 11 1 10.6 Fax 1 0.2 0.2 


Input 1 DB  the original database 2 ms  the minimum support 3 T the old fuzzy taxonomy 4 T   the new fuzzy taxonomy 5 L thesetofold frequent itemsets Output L   the set of new frequent itemsets  Steps 1 C 1 be set of all items in T  excluding new primitive items 2 Create MD   the table of membership degree using 1  3 C 1   iden_AItem MD  MD   T  T   Identify all affected items  4 k  1 5 repeat 6 if k 1 then 7 C k  apriori-gen L  k  1  8 delete any candidate in C k that consists of an item and its ancestor 9 endif 10 load original set of frequent k itemsets L k  11 divide C k into two subsets  k C and  k C  12 for each A   k C do 13 if A  L k then sup ED   A   sup ED  A   Cases 1 14 else delete A from  k C Case2 15 scan ED  to count count ED   A  according to 2 for each itemset A in  k C  Cases3&4 16 L  k   k C U  A  A   k C and sup ED   A   ms  k  17 until L  k  1  18 L   U k L  k  Fig 6 Algorithm FDiff_ET D Algorithm FDiff_ET2 The second algorithm FDiff_ET2 follows the same idea of Diff_ET2 Instead of scanning ED  as a whole for cases 3 and 4 FDiff_ET2 adopts a stratify paradigm first scanning the affected transactions both in ED and ED  identified according to Definition 3 hopefully saving the cost for scanning the rest of ED   This idea is further clarified in the following lemma Lemma 2 If an affected itemset A  L and count    A   count   A   0 then A  L   where  and   denote the sets of transactions in ED and ED   respectively affected by taxonomy update Proof If A  L then count ED  A    ED   ms Notethat ED     ED    ED          dueto        Hence count ED   A   count ED  A  count    A   count   A    ED   ms   ED    ms  Thus A  L    Thus in case 4 we scan the affected transactions in ED and ED  to count the appearances of A  If the support count of A in ED  s affected transactions is greater than that in ED  thenwe have to scan the rest of ED  to decide whether A is frequent or not Example 3 Consider the transaction database in Table 1 again The corresponding extended transaction database ED by inserting generalized items in taxonomies T and the updated extended transaction ED  by inserting generalized items in taxonomies T  are shown in Table 3 and Table 4 respectively where those shaded transactions denote the affected part TABLE 3 T HE EXTENDED TRANSACTION DATABASE ED  TID Items Purchased Extended Items 11 Laser HP Officejet Printer Scanner Peripheral 12 HP Officejet  Desktop Printer Scanner Peripheral PC 13 ScanMaker  Desktop Scanner Peripheral PC 14 HP Officejet  Notebook Printer Scanner Peripheral PC 15 Notebook PC 16 ScanMaker  Notebook Scanner Peripheral PC TABLE 4 T HE UPDATED EXTENDED TRANSACTION DATABASE ED   TID Items Purchased Extended Items 11 Laser HP Officejet Printer Scanner Fax Peripheral 12 HP Officejet  Desktop Printer Scanner Fax Peripheral PC 13 ScanMaker  Desktop Scanner Peripheral PC 14 HP Officejet  Notebook Printer Scanner Fax Peripheral PC 15 Notebook PC 16 ScanMaker  Notebook Scanner Peripheral PC V C ONCLUSIONS In this paper we have investigated the problem of updating generalized association rules with evolving fuzzy taxonomies We have elaborated on the design of two algorithms FDiff_ET and FDiff_ET2 for updating previously discovered generalized frequent itemsets yet the implementation and performance study of the proposed two algorithms will be accomplished in the future Although our work in this study has advanced the research in generalize associations mining there are many unexplored issues deserved further investigation For example the study can be extended to a more general model that incorporates multiple minimum supports weights of items quantitative database and even more complicated fuzzy ontological structure such as that exploiting both classification and component relationships Another important direction is on embedding the frequent pattern maintenance scheme into an online data mining platform R EFERENCES 1 R A  A ng ry k a nd F  E P e t r y   Mining multi-level associations with fuzzy hierarchies  Proc 14th IEEE Int Conf on Fuzzy Systems  pp 785  790 2005 2 G  C he n a nd Q  W e i   Fuzzy association rules and the extended mining algorithms  Information Sciences  vol 147 no 1-4 pp 201  228 2002 3 J  M D e G r a a f W  A K o s t e r s a n d J  J  W W i t t e m a n   Interesting fuzzy association rules in quantitative databases  Lecture Notes in Computer Science  vol 2168 2001 4 M A  D omi n gu e s a n d S O  R e z e n d e   Using taxonomies to facilitate the analysis of the association rules  Proc 2nd Int Workshop on Knowledge Discovery and Ontologies  pp 59  66 2005 5 015$\012'\012	\012\($\012\*\012 Dynamic generation and refinement of concept hierarchies for knowledge discovery in databases 012 Proc AAAI  94 Workshop on Knowledge Discovery in Databases  pp 157  168 1994 6 015$\012'\012	\012\($\012\*\012+ ",\012"!\012 level association rules from 012 012 Proc 21st Int Conf Very Large Data Bases  pp 420  431 1995 


7 T P  H o ng K  Y  L i ng  S L  W an g  012\,\012 data mining for interesting generalized association r 012 Fuzzy Sets and Systems  vol 138 pp 255  269 2003 8 Y  F  H ua ng a n d C.M W u  Mining generalized association rules using pruning techniques  Proc IEEE Int Conf on Data Mining  pp 227  234 2002 9 M Kaya and R Alhajj  Effective mining of fuzzy multi-cross-level weighted association rules  Proc 16th Int Symp on Foundations of Intelligent Systems  Lecture Notes in Computer Science  vol 4203 pp 399  408 2006 10 Y.C  L e e T P Ho n g  a n d T  C  012-\012 level fuzzy mining with multiple minimum supports Expert Systems with Applications vol 34 pp 459  468 2008 1 F  E P e t r y a nd L  Z h ao  Data mining by attribute generalization with fuzzy hierarchies in fuzzy databases  Fuzzy Sets and Systems vol.165 no 15 pp 2206  2223 2009 1 B Shen M Yao and Y Bo 012 weighted generalized fuzzy association rules with fuzzy t 012 Proc Int Conf Computational Intelligence and Security  Lecture Notes in Computer Science  Vol 3801 pp 704  712 2005 1 0$\012 12\012 012 0$\012 3\012 012 012 012 012 Proc 21st Int Conf Very Large Data Bases  pp 407  419 1995  K  S r i phae w an d T  T hee r amunko ng  Fast algorithms for mining generalized frequent patterns of generalized association rules  IEICE Transaction on Information and Systems  vol 87 no 3 pp 761  770 2004  M  C T s e n g a nd W  Y  L i n 012 Efficient mining of generalized association rules with non-uniform minimum support  Data and Knowledge Engineering  vol 62 no 1 pp 41  64 2007 16 M  C  Ts en g  W  Y  L i n  a n d R  J en g   Updating generalized association rules with evolving taxonomies  Applied Intelligence  vol 29 no 3 pp 306  320 2008  M  C T s e ng W  Y  L i n and R  J e n g  4 \012 maintenance of generalized association rules under taxonomy evolution  Journal of Information Science  vol 34 no 2 pp 174  195 2008  S  W a ng  K F  L  C hung  a n d H  S h e n 012 Fuzzy taxonomy quantitative database and mining ge 012 012 012 Intelligent Data Analysis  vol 9 no 2 pp 207  217 2005  Q  W e i a nd G  C h e n   Mining generalized association rules with fuzzy taxonomic structures  Proc 18th Int Conf North American Fuzzy Information Processing Society  pp 477  481 1999 


old field points to the nodes traversed or a Share-table constructed Based on the definition mentioned above, we have the following Share-struct construction algorithm Algorithm 1 Share-struct-Construction Input:  a database DB, and a minimum support threshold s Output:  a Share-struct, S Method:Call Share-struct-Construction \(DB, s, S Procedure Share-struct-Construction \(DB, s, S L1 = {the set of all frequent items sort items in L1 in support descending order create a FP-tree, T call Initialize \(S?, T  The procedure Initialize is shown in the following Procedure Initialize \(S?, new, old for each child ci of new/old add ci to the relevant new field in S ci.local-count= ci.local-count + ci.count  Example 3 Table 2 shows the example transaction database which is borrowed from [9]. Let the minimum support threshold be 2 TABLE II.  A TRANSACTION DATABASE AS RUNNING EXAMPLE, DB  Because only the frequent items will play a role in the frequent pattern mining, it is necessary to scan the DB once to identify the set of frequent items \(L1 compactness, we order L1 in support descending order L1={f:4, c:4, a:3, b:3, m:3, p:3, l:2, o:2 DB for a second time to construct the FP-tree. The construction algorithm of a FP-tree is proposed in [10 Readers are referred to the cited paper for details. The FP-tree in a Share-struct is almost the same as the one in [10] except for each notes parent and next fields. They are redundant in Share-struct, so they are removed. Afterwards, we create the first Share-table S? according to L1. Fig. 1\(a original Share-struct \(The FP-tree with Share-table S the convenience of later discussion, the number before each node in the FP-tree represents its address in memory and all blank grids in S? indicate 0 or null  


a  Figure 1.  The original Share-struct and the steps of S?s change C. Share-FPM algorithm In this subsection, we will develop an efficient algorithm for mining all frequent patterns Algorithm 2 Share-FPM Input: S?, the Share-struct constructed based on Algorithm 1 and s, the minimum support threshold Output: The complete set of frequent patterns Method:Call Share-FPM \(S?,?, s Procedure Share-FPM \(S? ,?, s 1 for each entry si in S 2   if \(si.local-count<s and si->new! =NULL 3     add all si ->news children to the new fields of relevant entries in S 4   else ? =?si; ?-SD = ?-SD 5     if \(si ->new != NULL 6       if si is the only active entry 7         ?-SD=?-SDsi 8         add all children of si to relevant entries in S 9         si ->old= S 10      else ?-Postfix = ?-Postfix ?si 11        if \(si ->old == S 12          call Inherit\(si, S TID Items Bought \(Ordered 100 f, a, c, d, g, i, m, p f, c, a, m, p 200 a, b, c, f, l, m, o f, c, a, b, m, l, o 300 b, f, h, j, o f, b, o 400 b, c, k, s, p c, b, p 500 a, f, c, e, l, p, m, n f, c, a, m, p, l 1431 13        else call Initialize \(S?, si->new, si->old 14        if \(si ->old == NULL 15          flag_upload =TRUE 16        si->old = S 17        call Share-FPM \(S?, ?, s 18        if \(flag_upload ==TRUE 19          call Upload \(S?, S 20    else if \(si ->old ? S 21      ?-SD=?-SD?the items that appear after ? in ?-SD 22      ?-Postfix=the items that appear after ? in ?-Postfix 


23      for each pi in ?-Postfix 24        ? = ??pi 25        ?-SD = the items that appear before pi in ?-SD 26        call Share-FPM \(pi->old, ?, s 27      call Generate-FP \(?, ?-SD, ?-Postfix 28 call Generate-FP \(?,?-SD, ?-Postfix the end of Share-FPM The procedures Inherit, Upload and Generate-FP are shown in the following Procedure Inherit \(s, S if s->old can be released then //memory management S? = s->old else create a new Share-table S? by inheriting s->old call Initialize \(S?, s->new  Procedure Upload \(S?, S for each entry si in S upload all new and old fields in S? to old fields in S add si.local-count in S? to si.local-count in S  Procedure Generate-FP \(?,?-SD, ?-Postfix for each nonempty combination ? of the items in ?-SD generate pattern ?? ? with support minimum support of items in it for each item pi in ?-Postfix for each combination ? of the items which appear before pi generate pattern ?? ? ? pi with support minimum support of items in it  D. Share-UFPM Algorithm According to our mining model description, each utility frequent pattern is also frequent. After Share-FPM algorithm finds all frequent patterns, the Share-UFPM algorithm scans the database once to check whether each frequent pattern candidate algorithm is as follow Algorithm 3 Share-UFPM Input:   S?, s Output:   UFP, utility frequent patterns in DB Method:  Call Share-UFPM\(S?, s 


Procedure Share-UFPM\(S?, s UFP FP = Share-FPM \(S?,?, s for each transaction Ti ? DB for each candidate c ? FP if \(c ? Ti and u \(c, Ti c.support for each candidate c ? FP if \(c.support ? s UFP = UFP + c return UFP  IV. PERFORMANCE STUDY To evaluate the efficiency and effectiveness of our algorithms, we have done extensive experiments on various kinds of datasets with different features. The experiments are based on a 2.4GMHz Pentium IV PC with 512MB main memory and 60 GB hard driver, running on Microsoft Windows 2000 Server. All the programs are written in Microsoft/Visual C++6.0 The measured performance is algorithms execution time on the datasets with different minimum support threshold. The execution time only includes the disk reading time \(scan datasets output frequent patterns speed of disk writing A. Datasets and characteristics We use real world and synthetic data for our performance study. The basic characteristics of datasets are listed in the following The real world dataset called Retail is achieved from a retailing company. Retail contains products from various categories. There are 16469 items and 88162 transactions in the dataset. Each transaction consists of the products purchased by a customer at a time point. Its average transaction size and average maximal potentially frequent patterns size are 10.3 and 3. The size of this dataset is 4M The synthetic data sets which were used for the experiments were achieved from the online FIMI repository See the RUL: http://fimi.cs.helsinki.fi/. The data sets are T10I4D100K and T40I10D100K. In T10I4D100K, the average record size and average maximal potentially frequent patterns 


size are 10 and 4. In T40I10D100K, they are 40 and 10. The numbers of transactions in both two dataset are set to 100K There are exponentially numerous frequent patterns when the support threshold goes down B. Experimental results In order to mine the utility frequent patterns, we randomly generate the count of each item between 1 and 6. In fact, most items are in the low profit range, we synthetically generate utility values of each item from 0.01 to 10.00, using a log normal distribution. For instance, Fig.2 shows the distribution of the utility values of items in T10I4D100K 1432 0 1 2 3 4 5 6 7 8 9 10 0 20 40 60 80 100 120 140 160 180 N um be r o f i te m s Utility value  Figure 2.  Utility value distribution in T10I4D100K For selecting appropriate utility thresholds, we use the average transaction utility value to constraint the utility threshold instead of randomly choosing it. For example, in Table 1, where the average transaction utility value is 40. If the utility threshold is equal to 25%, it represents that ? = 10 40  25% =10 We compare the performance of Share-FPM with BUUFM [5], an up to date algorithm for utility frequent patterns 


mining. Fig.3 through Fig.5 show the performance curves of two algorithms on three datasets respectively. We can see that the Share-UFPM algorithm outperforms BU-UFM on all datasets, and the performance gap becomes significant when the minimum support threshold drops low enough 0.0 0.2 0.4 0.6 0.8 1.0 1.2 0 10 20 30 40 50 60 70 80 90 100 110 120 Ti m e S ec on ds  Minimum support \(%0 Share-UFPM BU-UFM Utility threshold=5  Figure 3.  Fig.12 Utility frequent patterns mining on Retail 0.0 0.2 0.4 0.6 0.8 1.0 0 50 100 150 200 250 300 350 


400 450 500 550 600 Ti m e S ec on ds  Minimum support \(%0 Share-UFPM BU-UFM Utility threshold=5  Figure 4.  Fig.13 Utility frequent patterns mining on T10I4D100K 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0 50 100 150 200 250 300 350 400 450 500 550 Ti m e S ec on ds  Minimum support Share-UFPM 


BU-UFM Utility threshold=5  Figure 5.  Utility frequent patterns mining on T40I10D100K V. CONCLUSIONS In this paper, we introduce a utility frequent pattern mining model based on a share strategy to find the combination of items with high frequencies and utilities. This model first find all patterns with a given minimum support threshold. In this step, a share strategy gives a way to share most of the results from the previous mining process instead of separating them distinctively, thereby dramatically reducing the cost of computation. And then all patterns that do not satisfy a minimum utility threshold are pruned The extension of our technique, for maintenance of the already mined utility frequent patterns when updating databases, is an interesting topic for future research REFERENCES 1] R. Agrawal, T. Imielinski, A. Swami, Mining association rules between sets of items in large databases, In: Proceedings of the 1993 ACM-SIGMOD, Washington, DC, 1993, 207216 2] J. Han, H. Cheng, D. Xin, X. Yan, Frequent pattern mining: current status and future directions, Data Min knowl Disc 2007, 55-86 3] Y. Liu, W. Liao, A.Choudhary, A two-phase algorithm for fast discovery of high utility itemsets, Lecture Notes in Artificial Intelligence 2005, 3518:689-695 4] Y. Liu, W. Liao, A. Choudhary, A fast high utility itemses mining algorithm, In: Proceeding of the 2005 ACM SIGKDD workshop on utility-based data mining, Chicago, Illinois, USA, 2005, 90-99 5] J. Yeh, Y. Li, C. Chang, Two-phase algorithms for a novel utilityfrequent mining model, Lecture Notes in Artificial intelligence 2007 4819: 433-444 6] C, Aaron, F. John, Association mining, ACM Computing Surveys 2006, 38\(2 7] H.Yao, H. Hamilton, C. Butz, A foundational approach to mining itemset utilities from databases, In: Proceeding of the 4th SIAM International Conference on Data Mining, Lake Buena Vista, Florida 2004, 428-486 8] H. Yao, H. Hamilton, L. Geng, A unified framework for utility based measures for mining itemsets, In: Proceedings of ACM SIGKDD 2nd workshop on utility-based data mining, New York, NY, 2006, 28-37 9] J. Han, M. Kamber, Data mining: concepts and techniques, 2nd edn 


Morgan Kaufmann. 2006 10] J. Han, J. Pei, Y. Yin, Mining frequent patterns without candidate generation, In: Proceeding of the 2000 ACM-SIGMOD international conference on management of data, Dallas, TX, 2000, 112 


2] S. Brin, R. Motwani, J. D. Ullman, and S. Tsur Dynamic Itemset Counting and Implication Rules for Market Basket Data," in Proceedings of the 1997 ACM SIGMOD international conference on Management of data, Tucson, Arizona, United States 1997, pp. 255-264 3] J. S. Park, M. S. Chen, and P. S. Yu, "An Effctive Hash based Algorithm for mining association rules in Prof. ACM SIGMOD Conf Management of Data New York, NY, USA, 1995, pp. 175 - 186 4] R. Agrawal, T. ,PLHOL?VNL DQG $. Swami, "Mining Association Rules between Sets of Items in Very Large Databases," in Proceedings of the 1993 ACM SIGMOD international conference on Management of data, Washington, D.C., 1993, pp. 207-216 5] H. Mannila, H. Toivonen, and A. I. Verkamo Efficient Algorithms for Discovering Association Rules," in AAAI Workshop on Knowledge Discovery in Databases, 1994, pp. 181-192 6] R. Srikant and R. Agrawal, "Mining Generalized Association Rules," in In Proc. of the 21st Int'l Conference on Very Large Databases, Zurich Switzerland, 1995 7] R. Srikant, Q. Vu, and R. Agrawal, "Mining association rules with item constraints," in In Proc 3rd Int. Conf. Knowledge Discovery and Data Mining, 1997, pp. 67--73 8] A. Savasere, E. Omiecinski, and S. B. Navathe, "An Efficient Algorithm for Mining Association Rules in Large Databases," in Proceedings of the 21th International Conference on Very Large Data Bases 1995, pp. 432 - 444 9] H. Mannila, "Database methods for data mining," in The Fourth International Conference on Knowledge Discovery and Data Mining, 1998 10] B. Liu, W. Hsu, and Y. Ma, "Mining Association Rules with Multiple Minimum Supports.," in SIGKDD Explorations, 1999, pp. 337--341 11] H. Yun, D. Ha, B. Hwang, and K. H. Ryu, "Mining association rules on significant rare data using relative support.," Journal of Systems and Software archive vol. 67, no. 3, pp. 181 - 191, 2003 


12] M. Hahsler, "A Model-Based Frequency Constraint for Mining Associations from Transaction Data Data Mining and Knowledge Discovery, vol. 13, no 2, pp. 137 - 166, 2006 13] L. Zhou and S. Yau, "Association rule and quantitative association rule mining among infrequent items," in International Conference on Knowledge Discovery and Data Mining, San Jose, California 2007, pp. 156-167 14] C. Ordonez, C. Santana, and L. d. Braal, "Discovering Interesting Association Rules in Medical Data," in Proccedings of ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, 2000, pp. 78-85 15] L. J. Sheela and V. Shanthi, "DIMAR - Discovering interesting medical association rules form MRI scans," in 6th International Conference on Electrical Engineering/Electronics, Computer Telecommunications and Information Technology 2009, pp. 654 - 658 16] C. Ordonez, N. Ezquerra, and C. A. Santana Constraining and summarizing association rules in medical data," Knowledge and Information Systems vol. 9, no. 3, pp. 259 - 283, September 2005 17] H. Pan, J. Li, and Z. Wei, "Mining Interesting Association Rules in Medical Images," Lecture Notes In Computer Science, vol. 3584, pp. 598-609, 2005 18] S. Doddi, A. Marathe, S. S. Ravi, and D. C Torney Discovery of association rules in medical data Medical Informatics and the Internet in Medicine, vol 26, no. 1, pp. 25-33, January 2001 86 


the time needed for execution exceeded 100000 seconds Thus, from this analysis we see that FPrep, which uses FCM clustering, clearly outperforms the CLARANS and CURE based methods on the basis of speed. The execution times for CLARANS and CURE mentioned in fig. 7 and Table II do not include the time required to create fuzzy sets, and calculate the membership value  for each numerical data point in every fuzzy set for the numerical attribute under consideration. These times also do not take into account the time required to transform crisp numerical attributes to fuzzy attributes, and derive the fuzzy dataset from the original crisp dataset The fuzzy partitions generated for each of the five numerical attributes for the USCensus1990raw dataset are shown in Table III. Coincidentally, generating three fuzzy partitions for each numerical attribute seemed a perfect fit In addition to the superior speeds achieved by FPrep, as illustrated in fig. 7 and Table II, Table III indicates the semantics and the quality of the fuzzy partitions generated by FPrep. Moreover, the number of frequent itemsets generated by a fuzzy ARM algorithm \(like fuzzy ARMOR and fuzzy Apriori minimum support threshold, is illustrated in fig. 8   Fig. 7. Algorithm, numerical attribute comparison based on speed \(log10 seconds   Fig. 8. Number of frequent itemsets for various minimum support values  B. Results from Second Dataset We have also applied FPrep on the FAM95 dataset http://www.stat.ucla.edu/data/fpp transactions. Of the 23 attributes in the dataset, we have used the first 18, of which six are quantitative and the rest are binary. For each of the six quantitative attributes, we have generated fuzzy partitions using FPrep. A thorough analysis with respect to execution times, has already been performed on the USCensus1990raw dataset \(which is manifolds bigger in size than the FAM95 dataset both on the basis of number of transactions and number of unique values for numerical 


attributes dataset has been done solely to provide further evidence of the quality and semantics of the fuzzy partitions generated by FPrep. The details of the same are in Table IV. In this case, the number of fuzzy partitions is different for different numerical attributes. Thus, the number and type of fuzzy partitions to be generated is totally dependent on the attribute under consideration. A graphical representation of the fuzzy partitions generated for the attribute Age has already been provided in fig. 5, and clearly shows the Gaussian nature of the fuzzy partitions. The nature and shapes of fuzzy partitions for the rest of the attributes are also similar. Last, the number of frequent itemsets generated for different minimum support values is illustrated in fig. 8  C. Analysis of Results With FPrep, we can analyze and zero in on the number and type of partitions required based on the semantics of the numerical attributes, which the methods detailed in [19 20] do not necessarily facilitate. Then, FPrep, backed by FCM clustering, takes care of the creating the fuzzy partitions, especially assigning membership values for each numerical data point in each fuzzy partition. In section 8.A we have already shown that FPrep is nearly 9 to 44 times faster than the CURE-based method, and 2672 to 13005 times faster than the CLARANS-based method. FPrep is not only much faster than other related methods, but also generates very high quality fuzzy partitions \(Table III and IV much user-intervention. We have created a standard way of representing any fuzzy dataset \(converted from any type of crisp dataset efficacy of the same is corroborated by the successful implementation of Fuzzy Apriori and Fuzzy ARMOR on the fuzzy dataset \(converted from crisp version of FAM95 dataset an initial implementation of Fuzzy ARMOR, are very encouraging. FPrep, when used in conjunction with these fuzzy ARM algorithms, generates a pretty good number of high-quality frequent itemsets \(fig. 8 frequent itemsets generated for a particular minimum support is same, irrespective of the fuzzy ARM algorithm 


used IX. CONCLUSIONS In this paper we have highlighted our methodology, called FPrep, for ARM in a fuzzy scenario. FPrep is meant for seamlessly and holistically transforming a crisp dataset into a fuzzy dataset such that it can drive a subsequent fuzzy ARM process. It does not rely on any non-fuzzy techniques and is thus more straightforward, fast, and consistent. It facilitates user-friendly automation of fuzzy dataset 1 0 1 2 3 4 5 Age - 91 Hours - 100 Income3 4949 Income2 13707 Income1 55089 Ti m e lo g1 0 se co nd s Numerical Attribute - Number of Unique Values FCM CURE CLARANS 0 500 1000 1500 2000 2500 


3000 0.075 0.1 0.15 0.2 0.25 0.3 0.35 0.4 N o o f F re qu en t I te m se ts Minimum Support USCensus1990 FAM95 generation through FCM, and subsequent steps in preprocessing with very less manual intervention and as simple and straightforward manner as possible. This methodology involves two distinct steps, namely creation of appropriate fuzzy partitions using fuzzy clustering and creation of fuzzy records, using these partitions, to get the fuzzy dataset from the original crisp dataset FPrep has been compared with other such techniques, and has been found to better on the basis of speed. We also illustrate its efficacy on the basis of quality of fuzzy partitions generated and the number of itemsets mined by a fuzzy ARM algorithm which is preceded by FPrep. This preprocessing technique provides us with a standard method of fuzzy data \(record that it is useful for any kind of fuzzy ARM algorithm irrespective of how the algorithm works. Furthermore, this pre-processing methodology has been adequately tested with two disparate fuzzy ARM algorithms, Fuzzy Apriori and Fuzzy ARMOR, and would also work fine with other fuzzy ARM algorithm REFERENCES 1] Zadeh, L. A.: Fuzzy sets. Inf. Control, 8, 338358 \(1965 2] Chen G., Yan P., Kerre E.E.: Computationally Efficient Mining for Fuzzy Implication-Based Association Rules in Quantitative Databases. International Journal of General Systems, 33, 163-182 


2004 3] Hllermeier, E.: Fuzzy methods in machine learning and data mining Status and prospects. Fuzzy Sets and Systems. 156, 387-406 \(2005 4] De Cock, M., Cornelis, C., Kerre, E.E.: Fuzzy Association Rules: A Two-Sided Approach. In: FIP, pp 385-390 \(2003 5] Yan, P., Chen, G., Cornelis, C., De Cock, M., Kerre, E.E.: Mining Positive and Negative Fuzzy Association Rules. In: KES, pp. 270-276 Springer \(2004 6] De Cock, M., Cornelis, C., Kerre, E.E.: Elicitation of fuzzy association rules from positive and negative examples. Fuzzy Sets and Systems, 149, 7385 \(2005 7] Verlinde, H., De Cock, M., Boute, R.: Fuzzy Versus Quantitative Association Rules: A Fair Data-Driven Comparison. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics 36, 679-683 \(2006 8] Dubois, D., Hllermeier, E., Prade, H.: A systematic approach to the assessment of fuzzy association rules. Data Min. Knowl. Discov., 13 167-192 \(2006 9] Dubois, D., Hllermeier, E., Prade, H.: A Note on Quality Measures for Fuzzy Association Rules. In: IFSA, pp. 346-353. Springer-Verlag 2003 10] Hllermeier, E., Yi, Y.: In Defense of Fuzzy Association Analysis IEEE Transactions on Systems, Man, and Cybernetics - Part B Cybernetics, 37, 1039-1043 \(2007 11] Agrawal, R., Imielinski, T., Swami, A.N.: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Record, 22, 207216 \(1993 12]  Agrawal, R., Srikant, R.: Fast Algorithms for Mining Association Rules. In: VLDB, pp. 487-99. Morgan Kaufmann \(1994 13] Han, J., Pei, J., Yin, Y.: Mining Frequent Patterns without Candidate Generation. In: SIGMOD Conference, pp. 1-12. ACM Press \(2000 14] Han, J., Pei, J., Yin, Y., Mao, R.: Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach. Data Mining and Knowledge Discovery, 8, 5387 \(2004 15] Pudi V., Haritsa J.R.: ARMOR: Association Rule Mining based on Oracle. CEUR Workshop Proceedings, 90 \(2003 16] Dunn, J. C.: A Fuzzy Relative of the ISODATA Process and its Use in Detecting Compact, Well Separated Clusters. J. Cyber., 3, 32-57 1974 17] Hoppner, F., Klawonn, F., Kruse, R, Runkler, T.: Fuzzy Cluster Analysis, Methods for Classification, Data Analysis and Image Recognition. Wiley, New York \(1999 


18] Bezdek J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Kluwer Academic Publishers, Norwell, MA \(1981 19] Fu, A.W., Wong, M.H., Sze, S.C., Wong, W.C., Wong, W.L., Yu W.K. Finding Fuzzy Sets for the Mining of Fuzzy Association Rules for Numerical Attributes. In: IDEAL, pp. 263-268. Springer \(1998 20] Kaya, M., Alhajj, R., Polat, F., Arslan, A: Efficient Automated Mining of Fuzzy Association Rules. In: DEXA, pp. 133-142. Springer \(2002 21] Mangalampalli, A., Pudi, V. Fuzzy Association Rule Mining Algorithm for Fast and Efficient Performance on Very Large Datasets In FUZZ-IEEE, pp. 1163-1168. IEEE \(2009 22] Kaya, M., Alhajj. Integrating Multi-Objective Genetic Algorithms into Clustering for Fuzzy Association Rules Mining. In ICDM, pp. 431434. IEEE \(2004  Table II. Algorithm, numerical attribute comparison based on speed \(seconds  Algorithm Age - 91 Hours - 100 Income3 - 4949 Income2 - 13707 Income1 - 55089 FCM 0.27 0.3 3.13 6.28 79.4 CURE 0.25 0.25 28.67 163.19 3614.13 CLARANS 1.3 1.34 8363.53 78030.3 Table III. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions Age Old Middle Aged Young Hours More Average Less Income1 High Medium Low Income2 High Medium Low Income3 High Medium Low  Table IV. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions AGE Very old Around 25 Around 50 Around 65 Around 35 HOURS Very High Zero Around 40 Around 25 INCHEAD Very less Around 30K Around 50K Around 100K INCFAM Around 60K Around 152K Around 96K Around 31K Around 8K TAXINC Around 50K Around 95K Around 20K Very less FTAX Around 15K Very less Around 6K Very high Around 33K  


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


