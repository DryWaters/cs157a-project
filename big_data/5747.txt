 ICETACS 2013     ii  Keynote Address  Strategy and the Future of Software Applications  Dr. Brian Subirana  Associate Professor MIT Sloan School of Management  Abstract  We will first provide a framework to analyze the evolution of software applications and the derived opportuniti es for market disruption that emerge We will then apply the framework first to well know success stories and then to the future. Our window into the future will be based on recent research mostly from the MIT Field Intelligence Laboratory We will focus on Big Data and various engineering solutions that are currently being researched including sensors that enable new applications \(light scanning smart skins water leakage temperature RFID architectural solutions that enable new data sharing models for  applications such as cloud car RFID middleware digital graffiti and the potential disruptive business impact in various business processes and industries including the supply chain massive on line open courses smart cities auto industry and social  media  en-IN  en-IN  


in Section VIII and we conclude the paper with Section IX II A UTOMATED P ERFORMANCE M EASUREMENT I NFRASTRUCTURE We address the above challenges by leveraging automated techniques for performance measurement More concretely we have developed Expertus  an automated infrastructure to fully automate the performance measurement process In our approach a user provides the conìguration le for the experiment and the infrastructure generates all of the required resources shell scripts and other conìguration les runs the experiments i.e deploy and conìgure applications run the workloads and collects and uploads the data to the data warehouse Finally the user can analyze the data using either command line tools  R or other means or a web portal The complete process for experiment measurement using our approach is illustrated in Figure 1 A brief description for each item in the gure is provided below  Code generator is the core which generates all the necessary resources to automate the experiment management process In a nutshell code generator takes experiment conìguration les as the input and generates resources to automate the process  Experiment Driver is designed to use the generated resources and controls the experimentation ow which involves application deployment conìguration initialization workload execution and data collection Code generator generates all the scripts and a special script called run.sh  which maintains the sequence for script execution Experiment driver uses run.sh to nd the order of execution It connects to all the nodes through SSH/SCP and executes the scripts on the corresponding nodes  Data Extraction Each experiment produces gigabytes of heterogeneous data for resource monitors e.g CPU Memory thread pool usage and etc  response time throughput and application logs The structure and amount of collected data vary based on sundry factors including system architecture  64-bits vs 32-bit  2-core vs 4-core  monitoring strategy and monitoring tools e.g sar  iostat  dstat  oprofile  logging strategy e.g Apache access logs and the number of deployed nodes and workloads Data extractor is written to help users easily import experiment data into the data warehouse It supports the most commonly used data formats and has built-in exibility to extend to new data formats  Data Warehouse Due to the nature of large-scale performance experiments creating a priori xed schema to store measurement data is difìcult Even if one could be deìned data processing becomes extremely inefìcient due to the magnitude of the data To overcome these challenges we have created a exible data warehouse tailored to handle performance measurement data  Data Analysis The reason for conducting large-scale experiments is to nd and resolve performance issues  Code Generator Experiment Driver Data Extractor Data Warehouse Data Analyzer Fig 1 A Typical Performance Measurement Process with Our Approach To this end data analysis plays an integral role yet due to the magnitude of the data and structure of the data warehouse data analysis becomes a non-trivial task To address these challenges we have provided two types of tools a web portal for graphical users and R scripts for command line users Both of these tools understand the internal data structure and they help to make data analysis efìcient III A UTOMATION THROUGH C ODE G ENERATION In our approach we enable automated experiment measurement through code generation which generates all the necessary resources to automate the measurement process From an architectural viewpoint our code generator adopts a compilerbased approach of multiple serial transformation stages  a code generation pipeline The intuition behind this approach is to deliver more extensibility and exibility by dividing the larger problem into smaller pieces and processing them one at a time The hallmarks of our approach are twofold the stages typically operate on an XML document that is an intermediate representation and XSLT performs the code generation We address challenges that originate from differences among clouds applications users and other cross cutting requirements e.g monitoring through aspect oriented programming AOP techniques Use of XML provides the code generator with a high degree of extensibility This stems from XMLês simple well-deìned syntax requirement and its ability to accept arbitrary new tags thereby bypassing the overhead encountered when managing both XSLT templates and AOP For example a template can add an arbitrary element to the intermediate XML however unless the processing code is written to process this new tag the newly added tag remains untouched XSLT transformation is the process of converting an XML document into another document through the use of XSL Typically XSLT converts an XML document into another XML document e.g HTML or any other type of document Expertus consists of two types of templates namely Resource templates and Aspect templates  The former is used to generate application/platform independent part of a resource and the latter is used to modify weave the generated resource for the target application/platform e.g Emulab vs EC2 Expertus takes an XML document and produces another XML document through XSLT transformation Expertus treats the rst and last stage differently as compared to the rest 188 


of the pipeline In the rst stage it takes the experiment speciìcation as the input and in the nal stage it generates the automation resources for the target le system in lieu of an intermediate XML At each stage Expertus uses the intermediate XML document created from the previous stage as the input to the current stage It uses the intermediate XML le to retrieve the names of the templates that are needed for the current stage and it transforms the intermediate XML document which produces yet another XML document If needed AOP pointcuts are added to the intermediate XML during the transformation phase Consequently Aspect Weaver is used to weave such pointcuts into the intermediate XML Aspect Weaver processes the pointcuts through Aspect templates and creates the woven XML The woven le is then written to the le system using the le writer During the nal stage of the pipeline the automation scripts are written to the le system while at each of the other intermediate stages of the pipeline an intermediate XML is generated and the next stage in the pipeline is called IV A PPROACH TO P ERFORMANCE D ATA G ENERATION In our approach to performance measurement we execute workloads by deploying actual or representative applications e.g benchmarks like RUBBoS R UBiS 11 Cloudstone c on actual or representati v e deplo yment platforms e.g Amazon EC2 These large-scale experiments produce a huge amount of heterogeneous performance data The heterogeneity of the data arises from the assemblage of applications clouds monitoring tools and monitoring strategies used We conduct large-scale experiments and collect data by fully automating the process and our code generator generates all the necessary resources to automate this process Experiment driver takes care of the experimental deployment and conìguration and once deployed the driver executes the workload against the speciìc deployed conìguration In this step we run the planned experiments according to the availability of hardware resources For example we usually run the experiments by increasing the workload For each workload we run the easily scalable browse only scenario rst followed by read/write scenarios After each batch of experiments we collect data ramp-down the system stop all servers and start the next batch of experiments This sequence allows for sufìcient ramp-up time which minimizes cache inter-dependencies across experiments The iterations continue until all of the experiments have been completed During experiment execution the experiment driver collects information about system resources e.g CPU memory application speciìc data e.g thread pool usage application logs e.g apache logs high level data like throughput and response time and any other data that the user wants to collect This process continues for each and every workload In fact experiments in our domain consist of 50 to 60 workloads and each workload runs for approximately 30 minutes The framework is capable of collecting managing and storing data without any help from the user The data extractor as the name implies extracts this data and stores it in the data warehouse after the experiment has completed V F LEXIBLE D ATA W AREHOUSE During large-scale performance measurement researchers do not know beforehand which resources need to be monitored whether it be high-level data like response time or throughput or low-level data like resource utilization data and application logs Monitoring all the potential resources is infeasible because of the performance overhead Researchers may choose different monitoring regimens change testing strategies or software and test on hetergenous platforms Because of this type of variability experimental data cannot be feasibly stored in a set of static tables Moreover failures during experimentation usually lead to incomplete or faulty data that waste database resources and slow data processing Even without failures these data tables tend to be quite large so processing becomes very expensive if these tables cannot be loaded into primary memory To address above challenges we have designed Experstore  a special data warehouse designed to store performance measurement data Expestore is fully dynamic that is its tables are created and populated on-the-îy based on the speciìc experimental data At the end of each experiment we create a set of tables to store the data and the resultant schema is solely based on the structure of this data e.g how many columns tables relations etc The experiment measurements in our domain consist of multiple workloads running against a deployed system a unique conìguration of hardware and software hence each experiment produces measurement data for each and every workload We have designed the experiment driver to store measurement data for each experiment in a separate directory In most cases each directory follows the same structure names and number of les During the data loading stage the data loader iteratively processes all of the directories i.e it recurses over all of the directories and loads the corresponding data contained in each The data loading conìguration maps directories to workloads and the loader uses the information about the data parser to process data les contained in a given directory As mentioned earlier large-scale experiments commonly result in failures and storing failures is incredibly wasteful During the data loading stage the data loader creates db scripts to remove all of the failure data for a given experiment The data engine uses these same scripts to recover in the event of a loading failure To minimize the possibility of such a failure we reduce data loading overhead by not loading data in a transaction During data parsing each le is matched to a proìle and the parser uses this proìle to update the database accordingly More concretely data processors provide an API for the parser such that the parser only needs to provide values for each row and the data engine does the rest of the work This approach enables the parser and the data engine to be loosely coupled 189 


Fig 2 Experstore Static and Dynamic Tables The data loading conìguration le provides all the necessary data for the data loader and the data engine For example a user can specify how to format a given data eld e.g datetime where to begin and end in a le and how to relate a data column row to columns in the database table and etc    This conìguration le can be reused across experiments as long as the directory structures are identical across experiments For each resource type we create a proìle which maps a leês structure to an applicable schema i.e relating the columns in a CSV le to a particular database table Next we have a mapping which speciìes what proìles apply to a given node A mapping contains node name le name and corresponding proìle A sample proìle and a mapping le is shown below Listing 1 Code Listening for Proìle and Mapping  profile   separator     separator   resource  name  CPU0   resource  name   processor  class  dataimport  f i l t e r  CSVFileProcessor   processor  class   column index   0   colname    user   datatype=èèdouble     column index   1   colname    system   datatype=èèdouble     start  index  10   start  index   end  index  0  end  index    profile   mapping nodename  Apache   filname   169.254.100.3 csv   startwith=èèfalse  e ndwith false   profiles=èèCPU0,DISK,CPU1 NETWORK SYSTEM   The structure of the data warehouse is shown in Figure 2 As shown in the gure it consists of four static tables that store experimental metadata e.g experiment name platforms node and workload information which are typically xed across experiments As shown in the gure the highlighted tables are the tables that are created on-the-îy Resource Mapping Table stores the names of the dynamically created tables along with the resource names For example it has a record for CPU utilization for experiment ID EXP  ID and the value is EXP ID CPU Likewise all the monitoring data for a given experiment is stored In fact it has a record for each unique node workload and resource VI A UTOMATED D ATA E XTRACTION In general the problem of extracting data from various log le formats reduces to a problem of attempting to disambiguate presentation concerns from those related to data While this reduction narrows the thinking around this problem it does not account for the numerous points of variability that occur particularly related to any given log leês layout and structure e.g the presentation of the embedded data At the highest level les can be described as containing unstructured semistructured or structured data Most of the log les presented in our domain fall into the semi-structured category the remaining portion are structured Hence an approach that accommodates semi-structured les could be used for structured so we focused on solving semi-structured les Laender et al suggests wrapper inducti v e approaches might be particularly relevant for this problem because of their reliance on format and presentation This observation serves as the foundation for the intuition for our design In short wrapper inductive approaches rely on format and structure to impart order when order is not explicit The extractor begins by creating a replica of the log le in memory Next it performs a rst pass to probabilistically encode rows of the le such that they are coded as containing the header header rows data data rows or some other type of information misc rows such as generic batch job information Next it attempts to match the data rows to headers many of the les contain more than one header in the le To accomplish this matching the extractor uses order and presentation information particularly invisible ASCII characters to compute the probability that a given row of data corresponds to a given header in the log le of interest Once we have a match we use the presentation information of the header row layout and structure to extract data from the matched data rows Once the data has been extracted it is loaded into a data warehouse loading le During this entire process the operator is asked to evaluate or validate rows that do not have signiìcant statistical power e.g the rows received low encoding or matching probabilities In this case the operator provides input to the extractor to either encode or match the row depending on the speciìc algorithm based on his judgment This design primarily relies on two algorithms the Row-Encoding Algorithm and the Matching Algorithm  Row-Encoding Algorithm works by prompting a user only when the system thinks the row is a header row Headers have two distinct characteristics They contain more alphabetic and more special characters relative to the total length of a given string Misc rows i.e rows that should be ignored for later processing have one of these two properties but not both which differentiates them from header rows The core of this algorithm works by calculating string length-weighted character frequencies The second the Header-to-Data Row Matching Algorithm operates similarly to Row-Encoding First it computes character frequencies and scales the weights corresponding to these frequencies by the type of character identiìed e.g visible ASCII vs invisible ASCII The algorithm makes a match by calculating two metrics the bytewise difference of invisibles between a header row and a given data row in addition to the vertical distance between a header row and a given data row The lowest sum of these two metrics yields a match 190 


VII E FFECTIVENESS OF THE I NFRASTRUCTURE We have used Expertus extensively to perform a large number of experiments on different computing clouds through experimentation we have collected a huge amount of data with various data formats stored these in the data warehouse and observed interesting performance phenomena In this section we evaluate the success of our approach managing performance measurement data A Usability of the Tool Here we present how quickly a user can change an existing speciìcation to run the same experiment with different settings e.g MySQL Cluster vs C-JDBC on different clouds e.g Emulab vs EC2 with different numbers of nodes e.g two vs four app servers or entirely different applications e.g RUBBoS vs Cloudstone In our analysis we created a speciìcation say a.xml  to run the RUBBoS application on Emulab with a total of 16 nodes and generated automated resources using Expertus We then changed a.xml to generate automated scripts for EC2 which required only a single line change i.e param name=èèplatform value=èèEC2 n a.xml and an IP address modiìcation Even though only a few lines changed in the conìguration le the changes to the generated code were material and non-trivial We followed the same procedure and modiìed a.xml to change the database middleware from C-JDBC to MySQL Cluster This change required modifying only 36 lines mostly MySQL Cluster-speciìc settings but the differences in generated code were huge Similarly by changing only 4 lines in a.xml  we were able to move from 2 to 8 Application servers Furthermore with only 52 template line changes we were able to extend the support from RUBBoS to Cloudstone B Generated Script Types and Magnitude The biggest advantage of our approach becomes apparent when automating experiments for complex applications The number of resources generated by Expertus depends on the application e.g RUBBoS RuBiS software packages e.g Tomcat JBOSS deployment platform e.g Emulab EC2 the number of experiments the number of servers and the number of conìguration parameters To show the difference in the generated code six different hardware conìgurations on Emulab were selected and the number of generated lines for each conìguration was counted When the number of nodes increases the size of the generated code grows signiìcantly as do the differences among the generated code bases The magnitude of the generated code implies two conclusions the effectiveness of our approach and the enormous hurdles confronting manual approaches For example an experiment with 43 nodes would require approximately 15K lines of shell scriptsÖa non-trivial undertaking for manual-based approaches C Richness of the Tool Richness is considered as the breadth and depth of supported software packages clouds and applications the infrastructure supports Expertus has been used over three years to conduct a large number of experiments spanning ve clouds Emulab EC2 Open Cirrus Wipro and Elba three applications RUBBoS RUBiS and Cloudstone ve database management systems C-JDBC MySQL Cluster MySQL PostgreSQL Oracle various resource monitoring tools dstat sar vmstat and varying numbers and types of nodes D Success of Data Generation Table I provides a high level summary of the many different experiments performed using the RUBBoS RUBiS and Cloudstone benchmarks In the table experiment refers to a trial of a particular experiment i.e execution of a particular workload against a combination of hardware and software conìgurations Typically a trial of an experiment takes one hour which is the aggregated value of reset time start time sleeping time ramp-up time running time ramp-down time stop time and data copy time As such in Emulab we have spent approximately 8,000 hours running experiments In the table nodes refer to the total number of machines we have used during our experiments We calculated the number of nodes by multiplying the number of experiments by the number of nodes for each experiment Conìguration means the number of different software and hardware conìgurations that have been used in our experiments Finally the number of data points collected describes the amount of data we have collected from executing these experiments TABLE I N UMBER OF E XPERIMENTS P ERFORMED WITH E XPERTUS  Type Emulab EC2 Open Cirrus Elba Wipro Experiments 8124 1436 430 2873 120 Nodes 95682 25848 4480 8734 430 Conìgurations 342 86 23 139 8 Data points 3,210.6M 672.2M 2.3M 1328.2M 0.1M E Testing for Heterogeneous Data Formats For the purpose of evaluating the robustness of the extractor or parser the following le patterns were tested 1 one header 2 multiple header rows with sequentially corresponding data 3 multiple header rows with non-sequential corresponding data and 4 multiple header rows appearing randomly in the le with data occurring non-sequentially e.g data does not correspond to the header it follows These patterns were distilled by sampling the known domain of log les During testing we used actual collected performance data that adhered to these aforementioned patterns and Table II outlines the observed results The le patterns also differed in header structure Based on the sample rows designated as headers contained either one row or two rows A header with one row Only Field Row Header  only contained data elds Alternatively a header with two rows Record  Field Row Header contained a row which enumerated the data records and another row which listed the corresponding data elds for each record For this latter case the numbers of records and elds were varied from 1 to 8 number of records and 16 number of elds respectively If the headers were correctly matched to the applicable row of data the speciìed test received a PASS grade otherwise it received a FAIL grade 191 


TABLE II E VALUATION S UMMARY OF S UPPORTED F ILE F ORMATS  Pattern Only Field Record  Field Row Header Row Header One header PASS PASS Multiple header sequentially data PASS PASS Multiple header non-sequential data PASS PASS Multiple header randomly headers N/A FAIL VIII R ELATED W ORK Benchmarking is an essential approach used in both academia and industry to gain an understanding of some or all of the following system behavior hypothesis formulation and testing systems conìguration and tuning solution development and performance bottleneck resolution However few efforts have had dual aims of building software tools for large-scale testing of distributed applications and reducing the complexity associated with benchmarking  The ZOO 3 has been designed to support scientiìc experiments by providing experiment management languages and supporting automatic experiment execution and data exploration Zenturio on the other hand is an experiment management system used for parameter studies performance analysis and software testing of cluster and grid architectures One of the closest approaches to ours is Weevil which also focuses on w orkload generation and script creation In their later studies the Weevil team observed some of the limitations in their approach and obstacles for reaching higher levels of conìdence with their results To our knowledge these efforts havenêt explored the issues of extensibility exibility or modularity that is presented in this paper IX C ONCLUSION Expertus our automated experiment management framework has been developed to minimize human errors and maximize efìciency when evaluating computing infrastructures experimentally We have used the framework for a large number of experimental studies and through these we have collected a huge amount of data which we have used for identifying interesting performance phenomena In this paper we discussed the use of the infrastructure for efìciently creating storing and analyzing performance measurement data The code generator generates the necessary resources to fully automate the experiment measurement process and using these generated scripts users can run experimental studies to actually generate performance data The automated data processor processes heterogeneous data and stores this data in a exible data warehouse built speciìcally for measurement data We evaluated the proposed automation framework based on its usage the amount of data it can accommodate different monitoring and logs formats it supports and nally the overall effectiveness of the approach based on the needs of the scientiìc community Our future work includes extending the data parser to support additional data formats extending the data warehouse to use No-SQL databases and extending the visualization tool to support more customizable graphing capabilities A CKNOWLEDGMENT This research has been partially funded by National Science Foundation by IUCRC/FRP 1127904  CISE/CNS 1138666 RAPID 1138666 CISE/CRI 0855180 NetSE 0905493 programs and gifts grants or contracts from DARPA/I2O Singapore Government Fujitsu Labs Wipro Applied Research and Georgia Tech Foundation through the John P Imlay Jr Chair endowment Any opinions ndings and conclusions or recommendations expressed in this material are those of the author\(s and do not necessarily reîect the views of the National Science Foundation or other funding agencies and companies mentioned above R EFERENCES 1 Y Ioannidis M Shivani and G Ponnekanti ZOO A Desktop Experiment Management Environment In Proceedings of the 22nd VLDB Conference  Mumbai\(Bombay India 1996 2 K.L Karavanic and B.P Miller Experiment management support for performance tuning In Proceedings of the 1997 ACM/IEEE conference on Supercomputing  Mumbai\(Bombay India 1996 3 R Prodan and T Fahringer ZEN A Directive-based Language for Automatic Experiment Management of Distributed and Parallel Programs In ICPP 2002  Vancouver Canada 4 R Prodan and T Fahringer ZENTURIO An Experiment Management System for Cluster and Grid Computing In Cluster 2002  5 Y Wang A Carzaniga and A.L Wolf Four Enhancements to Automated Distributed System Experimentation Methods In ICSE 2008  6 S Babu N Borisov S Duan H Herodotou and V Thummala Automated Experiment-Driven Management of Database Systems In HotOS 2009 Monte Verita Switzeland 7 W Sobel S Subramanyam A Sucharitakul J Nguyen H Wong A Klepchukov S Patil A Fox and D Patterson Cloudstone MultiPlatform Multi-Language Benchmark and Measurement tools for Web 2.0 In CCA 2008 8 Y Wang M.J Rutherford A Carzaniga and A L Wolf Automating Experimentation on Distributed Testbeds In ASE 2005  Emulab N etw ork E mulation T estbed http://www emulab net 10 RUBBoS Bulletin board benchmark http://jmob.objectweb.org/rubbos html  R U BiS Rice University Bidding System http://rubis.ow2.org 12 Open Cirrus Open Cloud Computing Research Testbed https opencirrus.org  W IPRO Technologies www.wipro.com  Amazon Elastic Compute Cloud http://a ws.amazon.com 15 S Malkowski M Hedwig and C Pu Experimental evaluation of N-tier systems Observation and analysis of multi-bottlenecks In IISWC 2009  16 D Jayasinghe S Malkowski Q Wang J Li P Xiong and C Pu Variations in performance and scalability when migrating n-tier applications to different clouds CLOUD 2011 17 P Vassiliadis A Survey of Extract-Transform-Load Technology Integrations of Data Warehousing Data Mining and Database Technologies Innovative Approaches 2011 18 R Baumgartner G Wolfgang and G Gottlob Web Data Extraction System Encyclopedia of Database Systems 2009 3465-3471 19 R Kohavi R.M Henne and D Sommerìeld Practical guide to controlled experiments on the web Listen to your customers not to the HiPPO In ACM KDD 2007 20 S Malkowski D Jayasinghe M Hedwig J Park Y Kanemasa and C Pu Empirical analysis of database server scalability using an n-tier benchmark with read-intensive workload ACM SAC 2010 21 S Malkowski Y Kanemasay H Chen M Yamamotoz Q Wang D Jayasinghe C Pu and M Kawaba Challenges and Opportunities in Consolidation at High Resource Utilization Non-monotonic Response Time Variations in n-Tier Applications IEEE Cloud 2012 22 A Laender B Ribeiro-Neto A.S da Silva and J.S Teixeira A Brief Survey of Web Data Extraction Tools ACM Sigmod Record 31.2 2002 23 G Linden Make Your Data Useful Amazon November 2006 http://home.blarg.net  glinden/StanfordDataMining.2006-11-29.ppt 192 


  7  Lorenz, R  D., Experi m e n t s i n Timel a pse C a m e r a  Observations of Dust Devil Activity at El Dorado Playa Nevada, Abstract #1573, 42nd Lunar and Planetary Science Conference, Lunar and Planetary Institute Houston, TX, 2011  Lorenz R  D., B  Jackson and J. B a rnes, Inexpensi v e Timelapse Digital Cameras for Studying Transient Meteorological Phenomena : Dust Devils and Playa Flooding, Journal of At mospheric and Oceanic Technology, 27, 246-256, 2010  C a st ano, A., A. F ukanag a J. B i esadecki, L. Neakrase, P Whelley, R. Greeley, M. Lemmon, R. Castano, S. Chien Automatic detection of dust devils and clouds on Mars Machine Vision and Applications, 19, 467-482, 2008  Lorenz R  D. and A Val d ez, Var i abl e W i nd R i p p l e  Migration at Great Sand Dunes National Park, Observed by Timelapse Imagery, Geomorphology, 133, 1-10, 2011 19 B a l m e  M. R A  Pa th a r e, S.M Me tzg e r  M C. To w n er  S.R. Lewis, A. Spiga, L.K. Fenton, N.O. Renno, H.M Elliott, F.A. Saca, T.I. Michae ls, P. Russell, J. Verdasca Field measurements of horizontal forward motion velocities of terrestrial dust devils: Towards a proxy for ambient winds on Mars and Earth, Icarus, 221, 632ñ645 2012  Koch, W  On B a y e si an Tracki ng and Dat a Fusi on  A Tutorial Introduction with Examples, IEEE Aerospace and Electronics Systems, 25, 29-51, 2010  Biographies Ralph Lorenz is a planetary scientist at the Johns Hopkins University Applied Physics Laboratory, with interests in atmospheres surfaces and their interactions, especially on Titan and Mars.  He worked for the European Space Agency on Phase B of the development of the Huygens probe to Titan, and subsequently built part of the instrumentation of the probeís Surface Science Package SSP\. Prior to joining APL in 2006, he spent 12 years in various positions at the Lunar and Planetary Laboratory at the University of Arizona, where he led observation planning for the Cassini RADAR investigation, and served on the science team of the New Millennium DS-2 mission to Mars. He is th e author of several books including ëSpinning Flightí, ëTitan Unveiledí, and ëSpace Systems Failures. He has a B.Eng in Aerospace Systems Engineering from the University of Southampton \(UK and a Ph.D. in Physics from the University of Kent at Canterbury \(UK\. He is the recipient of 5 NASA Group Achievement Awards   


  8  


Virtual Social Networks Analysis in Computational Social Network Analysis  ser Computer Communications and Networks A Abraham A.-E Hassanien and V Sn  ael Eds London Springer London 2010 ch 1 pp 3Ö25  J K orner  Fredman-k olmo s bounds and information theory   SIAM J Algebraic Discrete Methods  vol 7 no 4 pp 560Ö570 Oct 1986  T  Leighton and S Rao Multicommodity max-îo w min-cut theorems and their use in designing approximation algorithms J ACM  vol 46 no 6 pp 787Ö832 Nov 1999  M Bastian S He ymann and M Jacomy  Gephi An open source software for exploring and manipulating networks 2009  A.-L Barabasi and R Albert Emer gence of scaling in random networks Science  vol 286 no 5439 pp 509Ö512 1999 


application or middleware platform to collect request ows Thus it is much easier to deploy FChain in large-scale IaaS clouds Blacksheep correl a t e s t he change poi nt of s y s t em-l e v el metrics e.g cpu usage with the change in count of Hadoop application states i.e events extracted from logs of DataNodes and TaskTrackers to detect and diagnose the anomalies in a Hadoop cluster Kahuna-BB correl a t e s b l ack-box dat a system-level metrics and white-box data Hadoop console logs across different nodes of a MapReduce cluster to identify faulty nodes In comparison FChain is a black-box fault localization system which is application-agnostic without requiring any knowledge about the application internals We believe that FChain is more practical and attractive for IaaS cloud systems than previous white-box or gray-box techniques V C ONCLUSION In this paper we have presented FChain a robust blackbox online fault localization system for IaaS cloud computing infrastructures FChain can quickly pinpoint faulty components immediately after the performance anomaly is detected FChain provides a novel predictability-based abnormal change point selection scheme that can accurately identify the onset time of the abnormal behaviors at different components processing dynamic workloads FChain combines both the abnormal change propagation knowledge and the inter-component dependency information to achieve robust fault localization FChain can further remove false alarms by performing online validation We have implemented FChain on top of the Xen platform and conducted extensive experimental evaluation using IBM System S data stream processing system Hadoop and RUBiS online auction benchmark Our experimental results show that FC hain can achieve much higher accuracy i.e up to 90 higher precision and up to 20 higher recall than existing schemes FChain is light-weight and non-intrusive which makes it practical for large-scale IaaS cloud computing infrastructures A CKNOWLEDGMENT This work was sponsored in part by NSF CNS0915567 grant NSF CNS0915861 grant NSF CAREER Award CNS1149445 U.S Army Research Ofìce ARO under grant W911NF-10-1-0273 IBM Faculty Awards and Google Research Awards Any opinions expressed in this paper are those of the authors and do not necessarily reîect the views of NSF ARO or U.S Government The authors would like to thank the anonymous reviewers for their insightful comments R EFERENCES   A m azon E las tic Com pute Cloud  h ttp://a w s  a m azon com ec2   V i rtual c om puting lab  http://vcl ncs u  e du  P  Barham  A  D onnelly  R I s aacs  a nd R M o rtier   U s ing m agpie f or request extraction and workload modelling in 
 2004  M  Y  Chen A  A ccardi E  K icim an J  L lo yd D  P atters on A  F ox and E Brewer Path-based failure and evolution management in  2004  R F ons eca G  P o rter  R H  K atz S  S h enk e r  and I  S toica X T race A pervasive network tracing framework in  2007  I  Cohen M  G o lds z m i dt T  K elly  J  S ym ons  a nd J  S  Chas e Correlating Instrumentation Data to System States A Building Block for Automated Diagnosis and Control in  2004  I  C ohen S  Z h ang M  G o lds z m i dt J  S ym ons  T  K elly  a nd A  F ox Capturing indexing clustering and retrieving system history in  2005  S  D uan S  Bab u  a nd K  M unagala F a A s ys tem for a utom ating failure diagnosis in  2009  S  K andula R Mahajan P  V erkaik S  A garw al J  P a dhye a nd V  Bahl Detailed diagnosis in computer networks in  2009  A  J  O liner  A  V  K ulkarni and A  A ik en  U s ing c orrelated s u rpris e to infer shared inîuence in  2010  P  Bahl R Chandra A  G r eenber g  S  K andula D  A  M altz and M Zhang Towards highly reliable enterprise network services via inference of multi-level dependencies in  2007  Z  G ong X  G u  a nd J  W ilk es   P RE S S  P Redicti v e E las tic ReS ource Scaling for Cloud Systems in  2010  H  N guyen Y  T a n and X  G u P A L  P ropagation-a w are a nom aly localization for cloud hosted distributed applications in  2011  B Gedik H Andrade K L  W u P  S  Y u and M  D oo SP ADE  t he system s declarative stream processing engine in  2008  A pache H adoop S y s tem   http://hadoop apache  or g/co re   Rice uni v e rs ity bidding s y s tem   http://rubis  objectw eb  o r g   M Ben-Y e huda D  B reitgand M F actor  H  K o lodner  V  K r a v ts o v  and D Pelleg NAP a building blo ck for remediating performance bottlenecks via black box network analysis in  2009  Y  T a n X  G u  a nd H  W a ng  A dapti v e s ys tem anom aly prediction f or large-scale hosting infrastructures in  2010  D  L  M ills   A b rief his t ory o f N T P tim e m e m o irs o f a n i nternet timekeeper  2003  Y  T a n H  N guyen Z  S h en X  G u C V e nkatram ani and D  R ajan PREPARE Predictive Performance Anomaly Prevention for Virtualized Cloud Systems in  2012  M  Bas s e ville and I  V  N ikiforo v   Prentice-Hall Inc 1993  L  Cherkaso v a  K  O zonat N Mi J  S ym ons a nd E  Sm irni  Anom aly application change or workload change towards automated detection of application performance anomaly and change in  2008  P  Barham and e t al   X e n a nd the a rt of virtualization  i n  2003  T he ircache p roject  h ttp://www.ircache.net  H ttperf  h ttp://code google com  p htt p er f  S  K u llback  T h e ku llback-leibler distance  1987  X  Chen M  Z hang Z  M  M a o and P  B ahl  A utom ating n etw ork application dependency discovery experiences limitations and new solutions in  2008  M Y u  A  G reenber g  D  M altz J  Re xford L  Y u an S  K andula and C Kim Proìling network performance for multi-tier data center applications in  2011  M K  A guilera J  Mogul J  W iener  P  R e ynolds  a nd A  Muthitacharoen Performance debugging for distributed systems of black boxes in  2003  S  A g arw ala F  A l e g re K  S chw a n and J  M ehalingham  E 2E P r of A utomated end-to-end performance management for enterprise systems in  2007  P  Re ynolds  J  L  W iener  J  C M ogul M  K  A guilera and A  V ahdat  WAP5 black-box performance debugging for wide-area systems in  2006  R Apte L  Hu K  S chw a n and A  G hosh L ook W ho s T alking Discovering dependencies between virtual machines using cpu utilization in  2010  G Khanna I  L aguna F  A rshad an d S Bagchi Distr ibuted diagnosis of failures in a three tier e-commerce system in  2007  R R S a m b as i v an A  X  Z heng M  D e Ros a  E  K re v at S  W h itm an M Stroucken W Wang L Xu and G R Ganger Diagnosing performance changes by com paring request ows in  2011  J  T a n a nd P  N a ras i m h an  RA M S and B lackS h eep I nferring w h ite-box application behavior using black-box techniques CMU Tech Rep 2008  J  T a n X  P a n E  Marinelli S  K a vulya R  G andhi a nd P  N a ras i m h an Kahuna Problem diagnosis for mapreduce-based cloud computing environments in  2010 
OSDI NSDI NSDI OSDI SOSP ICDE SIGCOMM DSN SIGCOMM CNSM SLAML SIGMOD ICAC PODC Computer Communication Review ICDCS Detection of abrupt changes theory and application DSN SOSP The American Statistician OSDI NSDI SOSP DSN WWW HotCloud SRDS NSDI NOMS 
207 
30 
30 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





