HETER0 ASSOCIATIVE NEURAL NETWORK FOR PATIERN RECOGNITION Taiwei Lu Xin Xu Shudong Wu and Francis T S Yu Department of Electrical Engineering The Pennsylvania State University University Park PA 16802 ABSTRACT We present an Inter-Pattern Association IPA neural network model in which basic logical operations are used to determine the association among common and special features of reference patterns i.e inter-pattern association Hetero and auto-associative memory are synthesized by applying a generalized logical rule Computer 
simulations for pattern recognition by using the IPA model have shown a better performance and a higher storage capacity than Hopfield model A 2-D adaptive optical neural network is used to perform parallel neurocomputations Since the Interconnection Weight Matrix \(IWM\for IPA model has a tri-state structure the dynamic range imposed on a Spatial Light Modulator SLM is rather relaxed and the interconnections are much simpler than the Hopfield model In this paper we shall present a neural network model based on the association between reference patterns First this model determines the common and 
special features among patterns by applying logical operations on the pixels in the pattern space Based on the relationship between the special and common features two equivalent logical rules are presented in Sec I1 to construct the excitatory and inhibitory interconnections in the network In Sec 111 the Inter-Pattern Association IPA model is compared with the Hopfield model in the performance versus number of reference patterns in a noisy environment Computer simulations of IPA model are conducted for recognizing tools of different orientations In Sec IV a 2-D hybrid parallel optical neural 
network is used for the implementation of IPA and Hopfield models The experimental results show that IPA model performs more effectively than Hopfield model I INTRODUCTION 11 IPA MODEL AND ASSOCIATIVE MEMORY The superior capability of human perception in pattern recognition has stimulated a great enthusiasm among neural network researchers Mathematical modeling of various neural networks has been developed during the past several decades l-41 However for recognition of a given set of reference patterns many neural network algorithms construct the Interconnection Weight Matrix IWM by emphasizing on the association of 
the elements within each reference pattern \(i.e., the intra-pattern association while paying little attention to the association between patterns i.e inter-pattern association For example in Hopfield type neural  the outer-products of the reference patterns are added to form the IWM This type of approach is based on the assumption that reference patterns are significantly different However in practice the reference patterns are usually not independent and the differences among the patterns are often very small e.g human faces finger prints handwritten characters etc Thus it may create an unstable or ill-conditioned network Under these circumstances the special features 
of the patterns play an important role in pattern recognition Therefore it is necessary to consider the relationships between the common and the special features among the reference patterns in constructing the IWM The information obtained from pixels in the special areas of a reference pattern is generally more important than that from the common areas i.e areas shared by more than one pattern in terms of constructing the associative memory matrix for a neural network The basic concept of IPA model is to determine whether the pixels in the pattern space 
belong to special or common spaces of the reference patterns and then set the excitatory or inhibitory interconnections according to a logical relationship Figure 1 shows an example of a training set that consists of three overlapping patterns A B and C in space S1 see Fig.l\(a and their corresponding desired output patterns A B and C in space S2 see Fig.l\(b These patterns can be divide into 7 subspaces For instance in space SI subspaces I I1 and I11 are the special areas of 
patterns A B and C respectively IV V and VI are the common areas of A and B B and C C and A respectively while VI1 is the common area of A B and C And the rest can be defined as an empty space a Similarly for the subspaces in S2 as illustrated in Fig.l\(b The subspaces of si and S2 can be expressed by the fo 1 Io w in g logical functions  658 


number of reference patterns Similarly the logical operation in subspace X in S2 can also be written in the same form such as X=PAQ 2 where P  p'1 A p'2 A    A pIn  3 Q=qll v 4'2 v  v q'm  4 The pixels in area X must excite  i.e having positive connections with all the pixels in area P inhibit i.e having n9ative connections with all the pixels in area Q AP and have no connection with the remaining areas in S2 For simplicity the connection strengths  i.e weights  are defined as 1 for positive connections 1 for negative connections and 0 for no connection Thus the IPA neural network can be constructed in a simple tri-state structure where P is defined by P  p'1 v p'2 v  v PIn 5 If patterns in S2 are the same as the patterns in SI then Rule A can be used to construct the auto associative memory To illustrate further the construction of the IPA model we shall synthesize an auto-associative memory for A B and C three 2x2 array patterns as shown in Figs 2\(a b and c respectively The pixel-pattern relationship is given in Table I It is apparent that pixel 1 represents the common feature of A B and C pixel 2 is the common feature of A and B pixel 3 the common feature of A and C and pixel 4 represents the special feature of C By applying Rule A to the three reference patterns A B and C a tri-state neural network can be constructed as illustrated in Fig 2\(d This is a single layer neural network with 4 neurons at the input end and 4 neurons at the output end Each neuron is matched to one pixel of the input/output patterns For example the 1st input neuron corresponds to pixel 1 of the input pattern and excites only the 1st output neuron The 2nd input neuron the corresponding pixel belongs to patterns A anb B excites both the 1st and the 2nd output neurons while inhibiting the 4th output neuron, which belongs to the special area of pattern C It is interesting to analyze the structure of the IWM Since the reference patterns are in 2-D form the weight matrix becomes a 4-D matrix We can partition the 4-D IWM into a 2-D submatrix array 6 as illustrated in Fig.2\(e The IWM can be divided into 4 blocks Each block corresponds to one output neuron The 4 elements in one block represent the 4 neurons in the input end For example since all the 4 elements in the upper-left block have value 1 it indicates that either one of the 4 input neurons can excite the 1st ouput neuron In the upper-right block as another example, the 1st and 3rd elememts are 0 the 2nd element has value 1 and the 4th element is 1 From this example we can determine that the 1st and 3rd input neurons have no connection to the 2nd output neuron the 2nd input neuron excites the 2nd output neuron and the 4th input neuron inhibits the 2nd output neuron In order to simplify the algebraic operations an equivalent rule Rule B is developed as follows which can be used to construct the IWM by examining the pixel-pattern relationships  A VII v VI v V 111 111 s1 s2 a b Fig.1 a Common and special areas of three reference patterns b\corresponding desired output patterns  I=AA\(B I'=AA\(B'V e  I1  B A A v C 11  B A A v C I11  C A A v B 111  c A A v B  IV  A AB AC W  A AB AF v  B A C A iC v  B A c A iC VI=\(CAA W=\(CAA AFT 1 vn  A A B A C A 5 VII  A A B A c AA where  A I  v  and I stand for the logic AND  OR and NOT operations respectively When a pixel in area VI1 is on i.e the pixel has a value 1 it implies that there is an input but we can not judge whether it belongs to any of the A B and C patterns Thus this pixel can only excite the pixels within area VII for which it has no connection with the pixels in other areas belong to space S2 it implies that the input pattern is not A but we can not tell whether it belongs to pattern B or C Thus this pixel should excite the pixels in areas V and VII but inhibit the pixels in area 1 Similarly, logical operations can also be applied to pixels in area IV or VI For the case when a pixel is on in area I it implies that pattern A is appeared at the input end Therefore this pixel can excite all the pixels in pattern A i.e areas 1 IV VI and VII  but inhibit the pixels in areas  B v C  AT i.e areas 11 111 and V  Similarly for the pixels in area I1 or I11  they must excite area B or C and inhibit areas 1 111 and VI or areas 1 11 and IV respectively the descriptive logic can be summarized in the following rule which is extented for any number of reference patterns Rule A An arbitrary subspace X in input space S1 can always be represented by the following expression When a pixel in area V is on In view of eq.\(l X=PAQ 2 where P  p1 A p2A  A pn  3 Q=ql v 42 v  V Sm  4 p1 p2     pn are input reference patterns that occupy the subspace X  ql q2   qm are input reference patterns that do not belong to X n and m are two positive integers and n+m  M the total 659  7 


 A B C Input layer output layer 4 1234 1110 1100 1011 Fig.2 Constructing an auto-associative memory matrix using IPA model a b and c represent three 2x2 reference patterns d a tri-state neural network, \(e\interconnection weight matrix Table I reference patterns in Fig.2 Pixel-pattern relationship of the three RUkE Let us define Dl.i as a 2-D matrix that corresponds to the 2-D 'pixel-pattern array in table I where 1 and i denote the row and column number and Di,i as the corresponding pixel-pattern matrix for desired output patterns Let di be the number of patterns that are bright i.e in state 1  at the ith pixel then it can be determined by summing the elements in the ith column of table I i.e di  c D,,i I I 1 d  g I L1 Let us also define Then we shall now construct an IPA neural network by applying the following logical rules 1 If Ki  di this means that the patterns sharing the ith pixel in SI are included by patterns of which the corresponding patterns in S2 share the jth pixel thus pixel i in S1 should excite pixel j in 2 If 0  K  di this implies that some patterns sharing the ith pixel do not share thejrh pixel thus the ith pixel in S1 can not excite the jth pixel in S2 s2 J 3 If Kij O when di  0 and d z 0 which means that no J common pattern shares pixels i and j in S1 must inhibit pixel j in S2 when di  0 and/or d  0 then pixel i must have no connection to pixel j We stress that, these logical rules for constructing the IWMs are rather straight forward which are suitable for computer implementations Notice that the IWMs computed by Rule B are equivalent to those obtained by Rule A then pixel i J 111 COMPARISON WITH THE HOPFIELD MODEL It is the differences rather than the similarities among patterns used for pattern recognition For example the outline of the eyes nose and mouth are common features in all human faces People distinguish different persons by the differences rather than the detail of faces Similar to many other neural network algorithms Hopfield model constructs the IWM by correlating the elements within each pattern however ignoring the relationships among the reference patterns The IWM of Hopfield model for three reference patterns A B and C can generally be expressed as where   stands for the transpose of the vectors and I is the unit matrix which makes the weight matrix zero-diagonal If input pattern A is applied to the neural system then the output would be T  AAT  BBT ccT  31 9 V=TA  A AT A  B BT A  C CT A  3A 10 where AT A represents the autocorrelation of pattern A while BT A and CT A are the respective cross-correlation between A and B A and C If the differences between A B and C are sufficiently large the autocorrelation of A B or C would be much larger than the cross-correlation between them i.e Notice that from eq 10 pattern A has a larger weighting factor than patterns B and C for which patterns B and C can thus be considered as noise By choosing the proper threshold value pattern A can be reconstructed at the output end of the neural network On the other hand if A B and C are closely similar the inequalities of Eq 11 are no longer held since the weights of patterns B and C are comparable with those of pattern A Patterns B and C can no longer be treated as noise Thus the AT AX-BT A AT AX-CT A 1 1 660 71 


threshold value for Hopfield model can not be easily defined and the neural network would become unstable Computer simulations of a 2-D neural network with an 8 x 8 array neurons at the input and output ends have been conducted for both Hopfield and IPA models The reference patterns considered are the 26 capital English letters lined up in sequence based on the similarities of the letters  B I  P   R    F I    and each letter occupies with an 8 x 8 array pixels Figure 3 shows the output error rates as a function of reference patterns for Hopfield and IPA models under noisy conditions The input noise levels are set at about 0 5 and 10 respectively We notice that Hopfield model becomes unstable when four patterns  B   P   R  and F are stored in the IWM wheras IPA model is quite stable with 12 stored letters Under the condition of noiseless input, the IPA model can produce correct results for all 26 stored letters in the IWM wheras Hopfield model starts making significant errors when the number of reference patterns is increased to 6 From Fig 3 we see that the IPA model is more robust and it has a larger storage capacity as compared with the Hopfield model IPA model can be used to train neural networks to perform shift and rotation invariant pattern recognition since the IPA neural network has large storge capacity and high robustness HoDf ield 4 0 0 10 20 Reference Pattern Number 30 Fig.3 Comparison of IPA and Hopfield models input noise levels I 0 11 5 and 111 10 The neural network is trained to recognize tools with different orientation as shown in Fig.4 IPA model is used to construct an Hetero-associative memory of these tools The first row in Fig.4\(a consists of 10 training patterns The desired output must be the correct tool and always aligned in the up right direction as shown in the second row of Fig.4\(a The interconnection weight matrix built by Rule B are rather simple as shown in Fig.4\(b the dark parts are inhibitory weights i.e 1 the bright parts are exitatory weights i.e I and the medium gray level represents no connection i.e 0 Figure 4\(c shows some simulated results performed by the IPA neural network The patterns in the first and third columns are input patterns either incomplete or with noise After parallel processing by IPA neural network the output patterns i.e the patterns in the 2nd and 4th columns become complete patterns with correct orientation Due to limited size eg 64 neurons the neural network can only be trained to recognize simple patterns with limited orientation However if we build a neural network with aboutlOOxlOO neurans 661 T Fig.4 Computer simulated results for recognizing tools of different orientations a a set of training patterns, \(b an IWM generated with IPA model c input/output results of the neural network IV OPTICAL IMPLEMENTATION One of the important properties of neural networks is the massive interconnection among large number of simple processing elements i.e neurans The 2-D and 3-D parallel processing capabilities of optical systems make it an important candidate in solving the complex interconnection problems in neural networks An adaptive optical neural is used to implement IPA model The optical architecture is illustrated in Fig.5 In this system a high resolution video monitor is used to display the weight matrix which was constructed with the IPA model This proposed system differs from the matrix-vector processor of Farhat and Psaltis[9 for which the position of the input SLM and the IWM have been exchanged We note that this arrangement makes it possible to use a video monitor for associative memory matrix generation instead of a low 7   


Thus the sequencial electronic bottleneck can be alleviated to some extend with the feedback loop Although displaying the memory matrices using a video monitor is relatively slow however the programming speed of IWM is not required to match the iteration speed in the feedback loop With reference to a 1024x1024 pixels video monitors which are available commercially it is possible to build a parallel hybrid optical neural network with 32x32 i.e 1024 neurons Fig.5 A 2-D hybrid optical neural network resolution and low contrast SLM A lenslet array consisting of NxN small lenses is used to establish the optical interconnections between the IWM and the input pattern, where a moderate sized SLM with NxN binary pixels is served as the input device As depicted in Fig.6 the light beam emitted for each submatrix from the video screen would be imaged by a specific lens of the lenslet array onto the input SLM Thus an NxN number of submatrices would be added onto the input SLM The overall transmitted light through SLM can be imaged at the ouput plane to form an NxN output array distribution which represents the product of the 4-D matrix T and the 2 D input pattern The output pattern can be picked up by an NxN photo-detector array for thresholding and feedback iterations Video Lenslet Input output Monitor Array Device Detector I I  Fig.6 Illustration of optical interconnections of the neural network To form a closed loop neural network operation the output signals from the detector array are fed back to the input SLM via a thresholding circuit It is apparent by the intervention of a computer the proposed optical neural network can be made adaptive We propose that a ferroelectric liquid crystal SLM would be used as the input device of the system Accordingly its contrast ratio can be as high as 125:l ll Since the SLM can be addressed in parallel the detector and the input arrays can communicate in parallel Thus the electro-optical feedback loop can be performed in a completely parallel manner This arrangement would allow the system to operate at high-speed asynchronous mode Fig.7 Experimental demonstrations obtained with the proposed optical neural network of Fig 5 using an LCTV as an input SLM a three similar English letters as reference patterns b  c positive and negative parts of the IWMs of the IPA model, \(d  e positive and negative parts of the Hopfield model f Input pattern  SNR  7dB g pattern reconstruction using IPA model h pattern reconstruction obtained with Hopfield model 662 


Since the resolution requirement of the lenslet array is rather relaxing An array of 32x32 lenses with 2.5 mm in diameter can provide at least 10 times the resolution of a commercially the TV monitor which has a resolution of about 3 lines/mm However the alignment of the optical system is critical for the matrix-vector operations The submatricies on TV screen have to be precisely imaged onto the input SLM by the lenslet array in superimposing position Since the proposed optical neural network is essencially a close-loop feedback system the precise alignment can be corrected by adjusting the position of each submatrix on TV screen by computer intervention and the intensity of the TV screen can also be adjusted Thus the proposed optical system can indeed perform in adaptive mode In experiments Hopfield and IPA models are chosen to perform pattern recognition using noisy input patterns  B    P  and  R  are three letters stored in the weight matrix as shown in Fig 7\(a The positive and negative parts of IWMs for IPA model are shown in Figs 7\(b and c while those for Hopfield model are displayed in Figs 7\(d and e In comparison between these two sets of IWMs it can be seen that the IPA model has two major advantages over the Hopfield model, namely 1 less interconnections and 2 fewer gray levels The later is significant because the IPA model requires only 3 gray levels to represent the IWM whereas the Hopfield model needs 2M+1 gray levels where M denotes the number of stored reference patterns The experimental results of these two models are obtained based on an input pattern  B  embedded in 30 random noise SNR  7 dB as shown in Fig.7\(f The output patterns of the IPA model and the Hopfield model are shown in Figs.7\(g and h respectively Because of the curvature of the video monitor screen the output results are somewhat distorted Nevertheless the results obtained from the IPA model have been shown better than those obtained from the Hopfield model  V CONCLUSIONS We have illustrated IPA neural network model for pattern recognition By using a simple logical rule the common features and the special features of the reference patterns can be obtained and the positive negative or no interconnections can be assigned to each neuron The IWM can be easily formulated which requires merely 3 gray levels An adaptive optical neural network is used to carry out the parallel processing Computer simulations and experimental results have shown that the IPA model can perform more effectively in terms of pattern recognition among similar patterns than the Hopfield model. The basic reason is that the IPA model puts more emphasis on the inter-pattern relationships while the Hopfield model deals only the intra-pattern association ACKNOWLEDGEMENT We acknowledge the support of the U S Army Research office contract DAAL03-87-0147 References l].Rumelhart D E and D Zipser Feature Discovery by competitive Learning Rumelhart D E and McClelland J L eds Parallel Distributed Processing Explorations in the Microstructure of ition  Chapter 5 Vol 1 pp151-193 MIT Press  1986 2 Kohonen T Self-Or~~ion and Associative Memory Springer-Verlag 1984 3 Fukushima K A Neural Network for Visual Pattern Recognition Computer Vol 21 No 3 65  1988 41 Carpenter G A and Grossberg S A Massively Parallel Architecture for a Self-organizing Neural Pattern Recognition Machine Computer Vision Graphics and Image Processing Vol 37 54 1987 5 Hopfield J J Neural Network and Physical System with Emergent Collective Computational Abilities," 79, 2554 1982 6].McClelland J L and Rumelhart D E An Interactive Activation Model of Context Effects in Letter Perception Part I An Account of Basic Findings I Psychological Review Vol 88 No 5 375\( 1981 7 P Smolensky Information Processing in Dynamical Systems: Fundations of Harmony Theory Rumelhart D E and McClelland J L eds Parallel istributed Processine ExDiorations in the Microstructure of Cognitiqn   Chapter 6 Vol 1 pp194 281 MIT Press 1986 8 Lu T Wu S Xu X and Yu F T S A 2-D Programmable Optical Neural Network to appear in Applied Optics 9].Farhat N H and Psaltis D Optical Implementation of Associative Memory Based on Models of Neural Networks Optical S ienal procesu ed by Horner J L Academic Press pp lo Athale R A Szu H H and Friedlander C B Optical Implementation of Associative Memory with Controlled Nonlinearity in the Correlation Domain Opt Lett Vol 11 No 7 482 1986 ll Johnson K M Handschy M A and Pagano Stauffer L A Optical Computing and Image Processing with Ferroelectric Liquid Crystals Opt Eng Vol 26 No 5 385\(1987      129-162 1987 663 


 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 percent of rules generated above confidence threshold ave clique sparseness/density 220complex_colocations\220  220simple_colocations\220  Figure 8 comparison of ef\223ciency from sparse to dense data features and the presence of multiple features apply the maxPI algorithm to the transactions as described in section 4 automatically pruning trivial/nonsensical collocations such as 000 000 001 000  For an analysis of the ef\223ciency and application across different spatial data sets see 6 and return a set of colocations and their con\223dences and calculate the signi\223cance of the con\223dences of the mined relationships with respect to their signi\223cance as described in section 6.1 7.2 Test Sets Synthetic data sets were created similar to those described in 1 b u t w i t h t h e s peci 223 c propert i e s o f s pat i a l data such e occurrence of a single item in many cliques and the occurrence of many items representing a single feature in one clique Set constitu ency was varied according to sparseness the number of features and the number of items The mining of relationships was varied according to the participation and con\223dence thresholds A comprehensive set of tests corresponding was completed across approximately 100,000 different set/parameter combinations A summary of results is given below Testing was undertaken to compare the ef\223ciency of mining complex relationships to the mining of simple relationships with maxPI and to investigate the relative frequencies of the different relationship types 7.3 Results Ef\336ciency As Figure 8 shows the ratio of rules generated to con\223dent rules found is typically more ef\223cient for the mining of 0 10 20 30 40 50 60 70 0 10 20 30 40 50 60 70 80 90 100 ave no. of rules confidence threshold 220complex\220  220one-to-many\220   220self-excl_col\220   220mulit-excl\220   220positive\220 Figure 9 comparison of relationship type frequencies complex rules especially when the data is sparse Although it was never the case here we do not rule out the possibility of the existence of a set such that the mining of simple relationships is more ef\223cient than the mining of complex relationships The results in Figure 8 are the average ratios for approximately 10,000 randomly generated data sets which were varied according to sparseness/density the average probability of a feature occurring in a given clique The maxPI and con\223dence were held constant at 0.6 and 0.8 respectively Varying the maximal participation index had little impact on the respective ratios Varying the con\223dence threshold varied the scale of the ratio but did not affect the scale of the two distributions with respect to each other A constant maintained across the generation of all sets were the inclusion of skews in the data such as 217the probability of 001 appearing in a clique increases by 0.15 if 000 and 002 are present\220 These were originally generated randomly then maintained as averages about which all random sets were created It is the interaction of such skews with the various thresholds that cause the unevenness in distributions in Figure 8 7.4 Results Frequency of relationship types The results in Figure 9 are averages for approximately 1000 separate data sets each with 10 features The number of features is the most sensitive variable in the relative frequencies due to the fact that there is the possibility of exponentially more exclusive and therefore complex sets with respect to the number of features in a clique as discussed in section 5.3 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


Typically the number of complex relationships found was greater than but correlated with the number of other relationship types found As Figure 9 shows the number of complex relationships at a given con\223dence threshold was sensitive to the variance in the number of the other relationship types Self-exclusion and self-colocation were modeled together in Figure 9 emphasize the complementary relationship between the two as described in section 5.2 This is revealed in the corresponding steepness f gradient for self-exclusion/colocation at con\223dence 000 000 001 001 and con\223dence 002 000 001 002  7.5 Limitations/Strengths of the representation While there are representational issues with any type of data appropiate representation is particularly important in the spatial domain 9 Limitations In one-to-many relationships this model doesn\220t capture interesting ranges or distributions in the 217many\220 which is a task better suited for mixture modelling or the techniques described in As pointed out in 10 t h e cos t o f ful l y t r ans cri b i n g s pat i a l data into a transactional representation can in some cases be more expensive than the mining of the colocations but as a full representation is necessary to accurately add the features representing absent and multiple items a solution to this in the current representation may be problematic Strengths The most obvious strength of this representation is that currently it is the only model that allows the mining of complex relationships in spatial data A major strength of a transactional representation of spatial data not explored here is that it may be combined with non-spatial data and so the addition of nonspatial data to the representation described here would be uncomplicated 8 Conclusions  Future Work We have de\223ned the concept of complex relationships in spatial data We have described how even in transactional representations spatial data is undamentally erent from other forms of data making the need to mine complex relationships of inherent interest We have demonstrated that even when simple relationships are the goal of mining spatial data the mining of complex relationships is necessary for determining the signi\223cance of those relationships We have implemented and demonstrated a transactional representation of spatial data that allows the ef\223cient mining of complex relationships and discussed its limitations and strengths 8.1 Future Work Apart from investigating improvements to the representation to address the limitations mentioned in 7.5 there are several future directions evident such as the application to other types of data with a spatial component such as spatiotemporal data and to a lesser extent natural language and biological systems One important step would be the combination of spatial coordinate features with spa tial volume features this is especially important in Geographic Information Systems where a volume may represent the area of a lake valley etc As we have demonstrated that with a purely coordinate system 003 in 004 000 003 must be treated as a volume the inclusion of features that explicitly represent volumes should prove interesting References  R  A gra w al and R  S r i kant  F ast al gori t h ms for m i n i n g a ssociation rules In J B Bocca M Jarke and C Zaniolo editors Proc 20th Int Conf Very Large Data Bases VLDB  pages 487\205499 Morgan Kaufmann 12\20515 1994 2 T  C  B aile y a n d A T  Gatrell Interactive spatial data analysis  Longman Scienti\223c  Technical 1995 3 S  B rin  R  R asto g i  a n d K Sh im M i n i n g o p timized g a in rules for numeric attributes IEEE transactions on knowledge and data engineering  15 2003 4 G  P iatetsk y Sh ap iro  Discovery analysis and presentation of strong rules AAAI/MIT Press 1991 5 J  H an J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In W Chen J Naughton and P A Bernstein editors 2000 M SIGMOD Intl Conference on Management of Data  pages 1\20512 ACM Press 05 2000 6 Y  H uang H Xi ong and S  S hekhar  M i n i n g con\223 dent colocation rules without a support threshold In Proc 18th M Symposium on Applied Computing ACM SAC  2003  K K operski and J Han Di sco v e ry of spat i a l a ssoci at i o n rules in geographic information databases In M J Egenhofer and J R Herring editors Proc 4th Int Symp Advances in Spatial Databases SSD  volume 951 pages 47\205 66 Springer-Verlag 6\2059 1995 8 R  M unr o S  C h a w l a  a nd P  S un C o mpl e x spat i a l r el at i onships University of Sydney School of Information chnologies chnical Report 539  2003 9 D  J  P euquet  Representations of space and time  Guilford Press 2002  S  S h ekhar and S  C ha wl a Spatial Databases A Tour  2002  S  S h ekhar and Y  H uang Di sco v e r i ng spat i a l c ol o cat i o n patterns A summary of results Lecture Notes in Computer Science  2121 2001  X  W u  C  Z hang and S  Z hang Mi ni ng bot h posi t i v e and negative association rules In 19th International Conference on Machine Learning ICML-2002  2002 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


 A A A A A A A A B B B B B B B A B A B A B A B AB A B A A A A B B B A B A B A A B B B B A B A B A B A B A B A B A disjoint B A inside B A contains B A equals B A meets B A covered by B A covers B A overlaps B A B A B A B A B A B AB Figure 4 Topology and resolution increase with minimum bounding circles 64Mb of main memory Since the Apriori algorithm uses the number of transactions as support and we wanted to compare our algorithm with Apriori we have implemented MaxOccur and the na\250 021ve with transaction based support MaxOccur1 The second version of MaxOccur MaxOccur2 used the object-based support as presented in Algorithm 3.1 Table 9 shows the average execution times for the four algorithms with different image set sizes and 033 0 0  05 for Apriori 223Na\250 021ve\224 and MaxOccur1 and 0  0035 for MaxOccur2 The results are graphically illustrated in Figure 5 Clearly MaxOccur scales well with both versions treating one thousand images in 1.3 seconds on average regardless of the size of the data set The running time for 002ltering the frequent item-sets with 033 0  the maximum support threshold line 16 of Algorithm 3.1 is negligible since it is done in main memory once the frequent item-sets are determined Moreover the calculation of the total number of items line 4 of Algorithm 3.1 is done during the 002rst scan of the data set and has limited repercussion on the algorithms execution time The major difference between Apriori and MaxOccur is in ascertaining the candidate item-sets and counting their repeated occurrences in the images Obviously MaxOccur discovers more frequent item-sets The na\250 021ve algorithm also 002nds the same frequent item-sets but is visibly capable of less performance in execution time The left graphic in Figure 6 shows the average number of frequent item-sets discovered with the three algorithms Apriori found on average 109 different frequent k-item-sets while MaxOccur1 and Na\250 021ve found 148 on the same data sets and MaxOccur2 found 145 on average The discrepancy between MaxOccur1 and MaxOccur2 is basically due to the different de\002nition of support The price we pay in performance loss with MaxOccur is gained by more frequent item-sets and thus more potentially useful association rules with recurrent items discovered ofimages Apriori Na\250 021ve MaxOccur1 MaxOccur2 10K 6.43 70.91 13.62 13.68 25K 15.66 176.69 32.35 34.11 50K 30.54 359.38 66.07 67.44 75K 44.93 514.33 97.27 101.23 100K 60.75 716.01 130.12 137.81 Table 9 Average execution times in seconds with different number of images 0 100 200 300 400 500 600 700 800 10K 25K 50K 75K 100K Apriori MaxOccur1 MaxOccur2 Na\357ve time images Figure 5 Scale up of the algorithms 6 Discussion and conclusion We have introduced in this paper multimedia association rules based on image content and spatial relationships between visual features in images using coarse to 002ne resolution approach and we have demonstrated the preservation and changes in topological features during resolution re\002nement We have put forth a Progressive Resolution Re\002nement approach for mining visual media at different resolution levels and have presented two algorithms for the discovery of content-based multimedia association rules These rules would be meaningful only in a homogeneous image collection a collection of semantically similar images or received from the same source channel Many improvements could still be added to the multimedia mining process to speed up the discovery or to re\002ne or generalize the discovered results 017 One major enhancement in the performance of the multimedia association rule discovery algorithms is the addition of some restrictions on the rules to be discovered Such restrictions could be given in a metarule form Meta-rule guided mining consists of dis#ofimages 033 0 0  25 0  20 0  15 0  10 0  05 10K 1.43 2.20 2.70 5.06 13.51 25K 2.80 4.78 6.31 11.20 32.35 50K 6.27 9.28 11.59 22.74 66.07 75K 8.24 13.57 17.69 33.94 97.27 100K 11.32 17.63 23.13 46.74 130.12 Table 10 Average execution time in seconds of MaxOccur with different thresholds 


 0 20 40 60 80 100 120 140 160 MaxOccur2 MaxOccur1 Na\357ve Apriori Apriori MaxOccur1 MaxOccur2 Na\357ve F k  Figure 6 Frequent item\255sets found by the dif\255 ferent algorithms covering rules that not only are frequent and con\002dent but also comply with the meta-rule template For example with a meta-rule such as 223 H-Next-to X Y   Colour x red  Overlap Y Z   P  Y Z  224 one need only to 002nd frequent 3-item-sets of the form f HNext-to\(red Y  Overlap Y 003  P  Y 003  g where Y is an attribute value and P a visual descriptor or spatial relationship predicate Obviously such a 002lter would greatly reduce the complexity of the search problem A method for exploiting meta-rules for mining multilevel association rules is given in  017 We have approximated an object in an image to a locale which is an area with a consistent visual feature such as colour Objects in images and videos are obviously more complex In a recent paper 9 re gions and their signatures are used as objects in a similarity retrieval system A computationally ef\002cient way to identify distinct objects in images is however still to be proposed Automatically identifying real objects and using spatial relationships between real objects would reduce the number of rules discovered and make them more signi\002cant for some multimedia applications 017 Object recognition or identi\002cation in image processing and computer vision is a very active research 002eld Accurately identifying an object in a video for example as being an object in itself is a very dif\002cult task We believe that data mining techniques can help in this perspective Multimedia association rules with spatial relationships using the motion vector of locales as a conditional 002lter can be used to discover whether locales moving together in a video sequence are part of the same object with a high con\002dence 017 There are many application domains where multimedia association rules could be applied and should be tested such as global weather analysis and weather forecast medical imaging solar surface activity understanding etc We are investigating the application with Magnetic Resonance Imaging MRI to discover associations between lesioned structures in the brain or between lesions and pathological characteristics Further development and experiments with mining multimedia data will be reported in the future References 1 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules In Proc VLDB  pages 487\226499 1994 2 M  J  E genhof er  Spatial Query Languages  PhD thesis University of Maine 1989 3 M  J  E genhof er and J  S har ma T opol ogi cal r e l a t i ons between regions in r 2 and z 2 In Advances in Spatial Databases SSD'93  Singapore 1993 4 U  M  F ayyad S  G  D j or go vski  a nd N  W e i r  A ut omat i n g the analysis and cataloging of sky surveys In U Fayyad G Piatetsky-Shapiro P Smyth and R Uthurusamy editors Advances in Knowledge Discovery and Data Mining  pages 471\226493 AAAI/MIT Press 1996 5 Y  F u a n d J Han  M e ta-ru le-g u i d e d m in in g o f a sso ciatio n rules in relational databases In Proc 1st Int Workshop Integration of Knowledge Discovery with Deductive and ObjectOriented Databases  pages 39\22646 Singapore Dec 1995 6 J  H an an d Y  F u  Disco v e ry o f mu ltip le-le v el asso ciatio n r u l es from large databases In Proc VLDB  pages 420\226431 1995 7 Z  N  L i  O R Z a 250 021ane and Z Tauber Illumination invariance and object model in content-based image and video retrieval Journal of Visual Communication and Image Representation  10\(3\:219\226244 September 1999 8 R  M iller a n d Y  Y a n g  Asso ciatio n r u l es o v e r i n t erv a l d ata In Proc ACM-SIGMOD  pages 452\226461 Tucson 1997 9 A  N atse v  R Rasto g i  a n d K Sh im W ALR U S A s imilar ity retrieval algorithm for image databases In Proc ACMSIGMOD  pages 395\226406 Philadelphia 1999  R Ng L  V  S  L akshmanan J  H an a nd A Pang E x ploratory mining and pruning optimizations of constrained associations rules In Proc ACM-SIGMOD  Seattle 1998 11 R Srik an t a n d R Ag ra w a l M i n i n g q u a n titati v e asso ciatio n rules in large relational tables In Proc ACM-SIGMOD  pages 1\22612 Montreal 1996  P  S t ol or z H  N a kamur a  E  M esr obi an R  M unt z E  S h ek J Santos J Yi K Ng S Chien C Mechoso and J Farrara Fast spatio-temporal data mining of large geophysical datasets In Proc Int Conf on KDD  pages 300\226305 1995  O  R  Z a 250 021ane Resource and Knowledge Discovery from the Internet and Multimedia Repositories  PhD thesis School of Computing Science Simon Fraser University March 1999  O  R  Z a 250 021ane,J.Han,Z.-N.Li,J.Y.Chiang,andS.Chee MultiMediaMiner A system prototype for multimedia data mining In Proc ACM-SIGMOD  Seattle 1998  O  R  Z a 250 021ane J Han Z.-N Li and J Hou Mining multimedia data In CASCON'98 Meeting of Minds  Toronto 1998 


18001  balancing mechanism which requires further investi gation 4.5 Speedup Figure 12 shows the speedup ratio for pass 2 vary ing the number of processors used, 16 32 48 and 64 where the curve is normalized with the 16 processor execution time The minimum support value was set to 0.4 4.5 0.5 1 1 0 I 10 20 30 40 50 60 70 number of mxessors Figure 12 Speedup curve NPA HPA and HPA-ELD attain much higher lin earity than SPA HPA-ELD an extension of HPA for extremely large itemset decomposition further in creases the linearity HPA-ELD attains satisfactory speed up ratio This algorithm just focuses on the item distribution of the transaction file and picks up the extremely frequently occurring items Transferring such items could result in network hot spots HPA-ELD tries not to send such items but to process them locally. Such a small mod ification to the original HPA algorithm could improve the linearity substantially 4.6 Effect of increasing transaction Figure 13 shows the effect of increasing transac tion database sue as the number of transactions is increased from 256,000 to 2 million transactions We used the data set t15.14 The behavior of the results does not change with increased database size The minimum support value was set to 0.4 The num ber of processors is kept at 16 As shown each of the parallel algorithms attains linearity 5 Summary and related work In this paper we proposed four parallel algorithms for mining association rules A summary of the four database size Sizeup 0 I 0 500 loo0 1500 uxw amount of transaction thousands Figure 13 Sizeup curve algorithms is shown in Table 5 In NPA the candi date itemsets are just copied amongst all the proces sors Each processor works on the entire candidate itemsets NPA requires no data transfer when the supports are counted However in the case where the entire candidate itemsets do not fit within the mem ory of a single processor the candidate itemsets are divided and the supports are counted by scanning the transaction database repeatedly Thus Disk 1/0 cost of NPA is high PDM, proposed in 6 is the same as NPA which copies the candidate itemsets among all the processors Disk 1/0 for PDM should be also high The remaining three algorithms SPA HPA and HPA-ELD partition the candidate itemsets over the memory space of all the processors Because it better exploits the total system's memory, disk 1/0 cost is low SPA arbitrarily partitions the candidate itemsets equally among the processors Since each processor broadcasts its local transaction data to all other pro cessors the communication cost is high HPA and HPA-ELD partition the candidate itemsets using a hash function which eliminates the need for transac tion data broadcasting and can reduce the comparison workload significantly HPA-ELD detects frequently occurring itemsets and handles them separately which can reduce the influence of the workload skew 6 Conclusions Since mining association rules requires several scans of the transaction file its computational requirements are too large for a single processor to have a reasonable response time This motivates our research In this paper we proposed four different parallel algorithms for mining association rules on a shared nothing parallel machine and examined their viabil 29 


Table 5 characteristics of algorithms ity through implementation on a 64 node parallel ma chine the Fujitsu AP1000DDV If a single processor can hold all the candidate item sets parallelization is straightforward It is just suf ficient to partition the transaction over the proces sors and for each processor to process the allocated transaction data in parallel We named this algo rithm NPA However when we try to do large scale data mining against a very large transaction file the candidate itemsets become too large to fit within the main memory of a single processor In addition to the size of a transaction file a small minimum support also increases the size of the candidate itemsets As we decrease the minimum support computation time grows rapidly but in many cases we can discover more interesting association rules SPA HPA and HPA-ELD not only partition the transaction file but partition the candidate itemsets among all the processors We implemented these al gorithms on a shard-nothing parallel machine Per formance evaluations show that the best algorithm HPA-ELD attains good linearity on speedup by fully utilizing all the available memory space which is also effective for skew handling At present we are doing the parallelization of mining generalized association rules described in 9 which includes the taxonomy is-a hierarchy Each item belongs to its own class hierarchy In such mining associations between the higher class and the lower class are also examined Thus the candidate itemset space becomes much larger and its computation time also takes even longer than the naive single level association mining Parallel pro cessing is essential for such heavy mining processing Acknowledgments This research is partially supported as a priority research program by ministry of education We would like to thank the F\221ujitsu Parallel Computing Research Center for allowing us to use their APlOOODDV sys tems References l R.Agrawal T.Imielinski and ASwami 223Min ing Association Rules between Sets of Items in Large Databases\224 In Proc of the 1993 ACM SIGMOD International Conference on Manage ment of Data pp207-216 May 1993 2 R.Agrawal and RSrikant 223Fast Algorithms for Mining Association Rules\224 In Proc of the 20th International Conference on Very Large Data Bases pp.487-499 September 1994 3 J.S.Park M.-S.Chen and P.S.Yu 223An Effec tive Hash-Based Algorithm for Mining Associ ation Rules\224 In Proc of the 1995 ACM SIG MOD International Conference on the Manage ment of Data SIGMOD Record Vo1.24 pp.175 186 June 1995 4 H.Mannila H.Toivonen and A.I.Verkamo 223Ef ficient Algorithms for Discovering Association Rules\224 In KDD-94:AAAI Workshop on Knowl edge Discovery in Databases pp.181-192 July 1994 5 A.Savasere, E.Omiecinski and S.Navathe 223An Effective Algorithm for Mining Association Rules in Large Databases\224 In Proc of the 21th International Conference on Very Large Data Bases pp.432-444 September 1995 6 J.S.Park M.-S.Chen and P.S.Yu 223Efficient Parallel Data Mining for Association Rules\224 In Proc of the 4th International Conference on In formation and Knowledge Management pp.31 36 November 1995 7 T.Shintani and M.Kitsuregawa 223Considera tion on Parallelization of Database Mining\224 In Institute of Electronics Information and Com munication Engineering Japan SIG CPS Y95 88 Technical Report Vo1.95 No.47 pp.57-62 December 1995 8 T.Shimizu T.Horie and H.Ishihata 223Perfor mance Evaluation of the APlOOO Effects of message handling broadcast and barrier syn chronization on benchmark performance-\224  In S WO PP 22292 9.2 ARC 95 Information Processing Society of Japan Vo1.92 No.64 1992 9 R.Srikant and R.Agrawal 223Mining Generalized Association Rules\224 In Proc of the 21th Inter national Conference on Very Large Data Bases pp.407-419 September 1995 30 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


