Improving Information Retrieval Precision by Finding Related Queries with similar Information Need using Information Scent  Suruchi Chawla  Research Scholar  Dr Punam Bedi  Reader   sur_chawla@rediffmail.com  pbedi@cs.du.ac.in    Abstract  Web is a valuable source of information and it is expanding at an enormous speed. Search engines provide the interface to access to this vast pool of information. Users express their information need 
through the input query to retrieve the relevant information which does not prove to be effective as input query entered by the user is too short to get the information need of the user. To retrieve the information according to a particular information need from a big pool of information available on the web is a big challenge. This paper proposes a method to find the related queries which approximate the information need of the input query issued to the search engine This is accomplished using Information scent and content of clicked pages in query sessions mining Information scent is derived from the Information 
Foraging theory in which user behavior in the information environment is guided by information scent. Information Need of the query sessions is modeled using Information Scent and content of clicked URLs and query sessions with similar information need are clustered. The clusters which closely approximate the information need of input query are used to suggest queries with similar information need for a given input query. The suggested queries are ranked in order of their degree of relevance with respect to information need of the input query.  Retrieval   precision of search engine is 
improved as suggested queries help to retrieve document relevant to the need of user efficiently and quickly. Experimental study has been conducted on the dataset collected from "Google" search engine Web History to confirm the improvement of precision of Information Retrieval   Key Words  Information Scent, Information Retrieval, Clustering, Search engine  1  Introduction Current search engines retrieve too many documents of which only a small fraction is relevant to 
a user query N o w a da y t h e w e b s e a r c h e n g i n e s  provide the user-friendly interface by which user can issue queries that are simply a list of keywords. From a study of the log of a popular search engine in i t w a s  concluded that most queries are short and imprecise Due to ambiguity of query terms and short length of query, keywords of query could not determine the information need associated to the query. As a result many documents are retrieved which are not relevant to the input query and retrieval precision is degraded 
In order to overcome this problem work has been done in [4 w h ic h i m p r o v e th e  i n f o r m a tio n  r e t r ie v a l precision by recommending set of similar queries in response to the input query and set of suggested queries are ranked according to the relevance criterion but it is realized that precision can be improved further if recommended queries are selected using the information need of past query sessions issued on the search engine. According to the information foraging theory the user is guided in the information 
environment by information scent. Users tend to click those retrieved pages in search results which match their information need and those pages have Information scent associated with it with respect to their information need. More the page is satisfying the information need of user, more will be the information scent associated to it. High Information Scent pages are more relevant than low scent pages with respect to the information need of the user. Information need of the query sessions is accessed not only from content 
but also from information scent of clicked page with respect to the information need of user which is not considered in [4  I n  1 0  co n cep t o f I n f o r m a t i o n S cen t is introduced in the field of information retrieval to improve the precision by improving the rank of those pages in result set which are relevant to the user information need. In this paper the Information scent is 
First International Conference on Emerging Trends in Engineering and Technology 978-0-7695-3267-7/08 $25.00 © 2008 IEEE DOI 10.1109/ICETET.2008.23 486 


used to infer the information need of the query sessions using Information Scent of the clicked pages in the query sessions and related queries are recommended using the query sessions with the similar information need. Information Scent is used to infer the information need of the query session by identifying the high and low scent clicked documents as every document clicked by the user in the query session is not equally relevant to the information need of query session some clicked documents are more relevant to information need of query session than others which is captured through information scent. Information Scent is used to access the relevancy of the clicked page with respect to the information need of the user. The solution proposed in this paper is used to model the information need of the query sessions using information scent and content of clicked URLs in the sessions. Information Scent helps to infer the information need of the query session using the uniqueness of frequently clicked documents in the session associated with the query. Query sessions modeled using information scent and content of clicked URLs are clustered to generate clusters of query sessions where each cluster represent a unique information need. The input query is then used to select the clusters which closely represent the information need of the input query. The selected clusters contain set of queries similar in information need to be used as recommendation to the user. Web history of “Google” search engine keep track of queries and URLs selected by users when they are finding useful data through the search engines. The data is extracted from the web history and loaded into database which is preprocessed to find the query sessions. The query sessions modeled using Information Scent and content of clicked URLs are clustered to group the query sessions with similar information need. During online searching the queries are recommended in order of their relevancy with respect to the information need of input query Recommended queries help the user to find the relevant documents which are closed to his needs and which he could not get from initial query issued. The recommended queries are those queries which are different from initial query in term of keywords used but they are satisfying the information need similar to initial query. This paper is organized as follows Section 2 describes the Information Scent, Section 3 explains the Clustering Query sessions with similar Information Need using Information Scent, Section 4 gives the proposed algorithm for Improving the Information Retrieval precision using Related Queries with similar Information Need, Experimental Study is presented in Section 5 and Section 6 Concludes the paper  2. Information Scent On the web, users search for information by navigating from page to page along the web links Their actions are guided by their information need Information scent is the subjective sense of value and cost of accessing a page based on perceptual cues with respect to the information need of user. More the page is satisfying the information need of user, more will be the information scent associated to it. The interaction between user needs, user action and content of web can be used to infer information need from a pattern of surfing  8  9   2.1. Information Scent Metric The Inferring User Need by Information Scent IUNIS  s  us e d i n  t h e p r o p o s e d a ppr o a c h t o  weigh each clicked page vector in query sessions using the combined effect of two factors. The factors are page access PF.IPF weight and TIME both of them are used to quantify the information scent associated with the page. In page access PF.IPF the PF is the access frequency of the clicked page in the given query session and the IPF is the ratio of total query sessions in the log to the number of query sessions in which this page is clicked. This helps to reduce the Information Scent of those pages that are accessed in many query sessions and may not be very relevant to the information need associated with the current query session. The second factor that is taken is Time spent on a page in a given query session. By including the time more weightage is given to those pages that consume more user attention. The information scent s id is calculated for each page P id  in a given query session Q i as follows  id s  1..n d  Pid  Time   Pid  PF.IPF 001 002 1   log\(M/m   max\(f f  PF.IPF\(P Pid Pid Pid id    d 002 1..n                   \(2   PF.IPF\(P id PF correspond to the page P id normalized frequency f Pid in a given query session Q i  and IPF correspond to the ratio of total number of query  sessions M in the whole log to the number of query sessions m Pid that contain the given page P id   Time\(P id It is the ratio of time spent on the page P id in a given session Q i to the total duration of session Q i   
487 


3. Clustering Query sessions with similar Information Need using Information Scent Clustering is the process of grouping the data into classes or clusters so that objects within a cluster have high similarity in comparison to one another, but are very dissimilar to objects in other clusters Dissimilarities are accessed based on attribute values describing the objects. In this paper weighted query session vector is used for clustering where each page P id in query session Q i is defined as follows  P id Content id for each document id  Content id The content vector of a page P id is a weighted keyword vector \(w 1,id w 2,id w 3,id w v,id  where v is the number of terms in the vocabulary set V describing the content of the page P id Vector Model [3 i s u s ed f o r  r e p r es en t i n g co n t e n t feature of each page P id in all query sessions. Each page P id is represented by vector w 1,id w 2,id w 3,id w v,id where v is the number of terms in the vocabulary set V TF.IDF \(term frequency * inverse document frequency\ term weighing scheme is used to represent the content vector for a given page P id  T h e  importance of each item of V in a given page P id is calculated using TF.IDF item weight. Vocabulary V is a set of distinct terms found in all distinct clicked pages in whole dataset relevant to a content feature The TF.IDF term weight is calculated as number of times a term appears in the given page weighted by the ratio of the number of all pages to the number of the pages that contain the given item The information scent associated with the given clicked page is calculated by using two factors i.e PF.IPF page access and TIME. Each query session is constructed as linear combination of vector of each page P id scaled by the weight s id which is the information scent associated with the page P id in session i. That is  n Q i  000 s id P id 3 d=1 In above formula n is the number of distinct clicked pages in the data set and s id information scent\ is calculated for each page P id present in a given session Q i  Each query session Q i is obtained as weighted vector using formula \(3\. This vector is modeling the information need associated with the query session Q i   3.1. Clustering Queries  Query sessions are clustered using k-means algorithm because of its good performance for document clustering [5   Q u e r y s e s s i o n s i n o u r approach are similar to the vectors of web pages and hence can be clustered using the methods for clustering pages A score or criterion function measures the quality of resulting clusters. This is used by common vector space implementation of k-means algorithm [13 Th e  function measures the average similarity between vectors and the centroid of clusters that are assigned to Let C p be a cluster found in a k-way clustering process\(p 001 1..k\ and let c p be the  centroid of pth cluster The criterion function I is defined as follows k I=1/M 003  003 sim \(v i c p 4 p=1     v i 001 C p  where v i is the vector representing some query session belonging to the cluster C p and centroid  c p of the cluster C p is defined as given below  c p  003 v i C p 5 v i  001 C p where M is the total number of query sessions in all clusters and |C p denotes the number of query sessions in cluster C p sim \(v i c p is calculated using cosine measure  4. Algorithm for Finding Related Queries with similar Information Need  The algorithm is based on clustering process that defines neighborhood of similar query sessions driven by similar information need using information scent and content vector of clicked URLs/pages in the query sessions. Each query session consist of a query along with the clicked URLs in its answer  querysession=\(query,\(clicked URLs    where clicked URLs are those URLs which user clicked before submitting another query  4.1. Algorithm  1  Offline Preprocessing phase at regular and periodical intervals 1.1  Extract the queries and associated clicked URLs from the data set 1.2  Preprocess the Extracted Queries to find the query sessions 
488 


1.3  Model the Information need associated to each query session using information scent and weighted vector of content of 3 1.4  Cluster the Query sessions using information need associated to each query session using k-means 1.5  For each cluster C j create a list of queries Q j in cluster C j  2  Online searching 2.1  Find the C j cluster to which input query q belongs 2.2  if   no cluster found then 2.2.1  Find the C j cluster which is most similar to term weight vector of input query q as per the threshold value set for similarity measure  2.3  Rank the list of queries Q j associated with selected cluster C j in order of their relevance to input query q upto certain similarity threshold value 2.4. Return the ranked set of queries The rank of queries in set Q j is calculated using similarity measure of each query vector x in Q j to input query vector q such that those queries with high value of similarity to input query q are ranked higher than those queries with low value of similarity to input query q where sim\(x, q\ is calculated using cosine measure between vector x and q  Rank\(x\ = {sim\(x, q 002 x 001 Q j 6  5. Experimental Study  Experiment was performed on the data set containing the clicked documents associated with queries issued on the Google search engine. The data set was collected from the web history of Google search engine. The data set was generated by users who had expertise in specific domains The web history contains the following fields  1.  Time of the Day 2.  Query terms 3.  Clicked URLs The data set generated from web history contains several thousand entries out of which we have extracted 5325 entries. In the experiment only those queries in data set were selected which had at least one click in their answer. The data set generated from web history is loaded into database to be processed further On the submission of input query, Google search engine returns result page consists of URLs with information about URLs. Query sessions considered consists of query terms along with clicked URLs. The clicked URLs are those URLs which user clicked before he submits another query. The number of distinct URLs in data set was found to be 3795.The data set was preprocessed to get 895 query sessions. In this experiment similarity of any two query sessions was calculated using cosine measure. The query sessions were clustered using k-means algorithm and it was executed several times for different values of k and criterion function was computed for each value of k. The criterion function was found to have maximum value at k=67.The threshold value set for similarity of two vectors was 0.5. The experiment was performed on randomly selected test queries which were categorized into trained queries set and untrained queries set. The trained queries were those input queries which had sessions associated with them in data set and untrained queries were those input queries which did not have sessions associated with them in data set. Some of the test queries in each of the category are given  in Table 1  Table 1  Sample of Queries taken in each of the category  The experiment was performed on Pentium IV PC with 512 MB RAM under Windows XP using Java and Oracle database. WebSphinx crawler was used to fetch the clicked documents of query sessions in the data set Each query session was transformed into the vector representation using Information Scent and content of clicked URLs. The k-means algorithm was executed to generate cluster of query sessions. Each cluster of Category Queries Untrained Set Moviesong,Spacefood,novels magazine movies,Numbness, Nature familyplay Games, movie pictures, software download online tutorial Trained  Set Homeloan distanceeducation online, free pics, cgi perl tutorial, moons of neptune how to play .vcd files, .vcd file, .api com, mpeg movies dragonball,intranet helpdeskmanagerjob description, free software 
489 


query sessions was represented by mean value of vector of terms. The Table 2 shows the cluster to which query "Games" and "hollywood video store belongs. The first column shows the input query and second column shows the queries which belong to the selected cluster for a given input query. The queries in cluster are shown in order of their relevance to input query  The relevance of recommended queries in the selected cluster to input query is decided by some anonymous user having knowledge in domain to which input query belongs. The relevance is judged by analyzing the answer of recommended queries from the result set showing top 10 answers and determined the URL in answers which are relevant to input query  The experiment was performed on randomly selected 21 trained queries and 35 untrained queries The average precision of queries of trained and untrained query set was calculated for different number of recommended queries from the result set showing the top 10 answers. The experimental result shows that ordering the recommended queries according to their relevance to input query with respect to its information need using information scent shows promising result  The Fig 1 and Fig 2 shows the precision of search results without using query recommendation and proposed approach on the trained and untrained set of queries and it shows that recommended queries help the user to find the relevant document which the user could not find it from initial input query as keywords of the query given by him were few to retrieve the relevant documents. Recommended queries help to handle ambiguity by expressing the information need using alternative queries that are satisfying the information need similar to the input query but different in the keyword used    Trained queries 0 0.2 0.4 0.6 0.8 02 468 NoOfRecommendedQueries Avgprecision proposed approach w ith query recommendation w ithout query recommendation    Fig 1. AvgPrecision of Trained Queries for different number of Recommended Queries  UntrainedQueries 0 0.2 0.4 0.6 0.8 02468 NoOfRcommendedQueries AvgPrecision proposed approach w ith query recommendation w ithout query recommendation    Fig 2. AvgPrecision of Untrained Queries for different number of Recommended Queries    Table 2. Queries in Selected Cluster for input Query “Games” and “Hollywood video store     6. Conclusion  In this paper efforts have been made to satisfy the Information need of user and improve the information retrieval precision by recommending related queries which approximate the information need associated to the input query using Information Scent. The information need associated to the query is modeled using information scent and content feature of clicked pages in the session. The suggested queries help the user to retrieve the documents relevant to his information need which he could not get through his Test Query Other Queries in Cluster Games Games download Internet games PC games Free online games Download play station games Downloadable games Free kid online games Hunting games Skies of arcadia pictures Xbox Mankind game  Hollywood video store Hollywood  movie store Movie store Video store Hollywood long movie Hollywood entertainment Hollywood video 
490 


initial query. Experimental results confirm the improvement of the information retrieval precision    7. References  1  E. Agichtein, E. Brill, S. Dumais,  “Improving Web Search Ranking by Incorporating User behaviour”. In Proceedings of the ACM Conference on Research and Development on Information Retrieval \(SIGIR\2006   2  E H. Chi, P. Pirolli, K. Chen and J. Pitkow Using Information Scent to model User Information Needs and Actions on the Web  In Proc. ACM CHI 2001 Conference on Human Factors in Computing Systems, 2001,pp. 490-497  3  R. Baeza-Yates and B. Ribeiro-Neto,”Modern Information Retrieval “ Addison Wesley, 1999  4  R. Baeza-Yates, C.A. Hurtado and M Mendoza  “Query Recommendation using query logs in Search engines  In Advances in Web Intelligence, Second International Atlantic Web Intelligence Conference, AWIC 2004, pp. 164-175  5  R J. Wen, ,Y J. Nie, J H Zhang, “Query Clustering Using User Logs  ACM Transactions on Information Systems,vol 20,No 1,2002,pp. 59-81  6  J. Heer and E.H. Chi, “Identification of Web User Traffic Composition using Multi-Modal clustering and Information Scent  In Proc of Workshop on Web Mining. SIAM Conference on Data Mining, 2001,pp. 51-58  7  M. Jansen, A. Spink , J. Bateman and T Saracevic, “Real life Information retrieval: a Study of user queries on the web  ACM SIGIR Forum 32\(1\,1998, pp. 5-17  8  P.Pirolli “Computational models of information scent-following in a very large browsable text collection  In Proc. ACM CHI 97 Conference on Human Factors in Computing Systems, 1997,pp. 3-10  9  P. Pirolli ,“ The use of proximal information scent to forage for distal content on the world wide web.” In Working with Technology in Mind Brunswikian. Resources for Cognitive Science and Engineering, Oxford University Press, 2004    Punam Bedi and Suruchi Chawla, “Improving Information Retrieval Precision using Query log mining and Information Scent”, Information Technology Journal 6\(4\:584-588 Asian Network for Scientific Information  2007    V N. Gudivada , V V. Raghavan,  W. Grosky and  R. KasanaGottu ,” Information  Retrieval on World Wide Web  IEEE expert, 1997, pp. 58-68    Y. Zhao and G. Karypis, “Comparison of agglomerative and partitional document clustering algorithms  In SIAM Workshop on Clustering High-dimensional Data   and its Applications, 2002    Y, Zhao and Y, Karypis . “Criterion functions for document clustering  Technical report University of Minnesota, Minneapolis, MN, 2002 
491 


abnormalities between test and training data and also to identify the critical system parameters 5 FAULT ISOLATION DOMINANT VARIABLES USING PROJECTION PURSUIT ANALYSIS The model space is designed to capture the data that varies the most whereas the residual space is designed to capture the data that does not vary but contributes to a faulty state The residual space can therefore detect changes in the distribution from variables that are degrading or have faults and are not effecting the variance Below are the principal components for the entire subspace S Each principal component is composed of the eight parameters with a particular weighting as shown in Table 5 The model/signal subspace is composed of the first four principal components This was chosen based on iterative experimentation to best capture the faults The decision of how many principal components are chosen to represent the model/signal space is based on experience and understanding of the data at hand There are computational/statistical techniques that can provide estimates for the selection of the number of PCs to optimized results The remaining columns span the residual subspace Each variable is represented by each respective row of matrix S The first row shows the contributions of the fan speed and the rest show the CPU temperature motherboard temperature video card temperature C2 state C3 state oCPU usage and 0%OCPU throttle from top to bottom in matrix S Table 5 Principal Component of subspace S and parameter contribution S PCI PC2 PC3 PC4 PC5 PC6 PC7 PC8 Fan 0.999 0.019 0.001 0.001 0.001 0.002 0.004 0.001 Speed CPTm 0.005 0.076 0.042 0.048 0.000 0.484 0.856 0.153 Mother board 0.830 Temp Video card 0.000 0.048 0.107 0.093 0.002 0.670 0.490 0.537 Temp 00C2 0.001 0.060 0.018 0.091 0.994 0.001 0.002 0.001 State 00C3 0.015 0.730 0.384 0.554 0.102 0.005 0.025 0.011 StateI CPU 0.013 0.661 0.271 0.690 0.028 0.114 0.019 0.000 Usage CPU 0.001 0O13 0.872 0.439 0.033 0.166 0.038 0.OOS Throttle From the decomposition of S we can see that the model space variations should be dominated by the fan speed followed by C3 state OCPU throttle and usage In the residual subspace the temperature components are dominant We expect that the temperature variables to be highly dominant Changes in the temperature are expected in turn to be less obvious to changes in system variance and should contribute to the shape of the multivariate data distribution Such a distribution can be modeled as Gaussian mixtures but in general a hard task Intuitively if the fan speed is not functioning we expect that the temperature of the system will rise and become abnormally high This is at first hand validated by the dominance of the temperatures components as observed in the residual subspace in S Mathematically this is also validated through the parameter contribution plots to the T2 and SPE respectively as illustrated in the contribution plots shown in Figure 4 The contribution plots tell us which parameter is contributing the most to the projection onto each subspace 70 70 60 60 50 5040~~~~~~~~~~~~~3 U 4~~~~~~~~~~~~~~0 10 Parameters 0 0.002 0.035 0.077 0.079 0.004 0.523 0.158 Parameter Figure 4 Contribution plot of each parameters towards T2 on right and SPE on left It is shown that on the model space the fan speed is highly dominant and varies the most in terms of standard deviation This phenomenon masks the effect on parameters that are also exhibiting abnormalities but are overpowered by dominant parameters such as the fan speed The residual space statistic SPE captures the inverse information and identifies the parameters that are indeed abnormal but are not dominating in terms of variance Also interesting is the fact that the mathematics validate our intuition that because the fan isn't functioning properly the temperature sensors would be experiencing unusual readings Note that these results are based on picking the model space using k=4 that is the first four PCs in matrix S The selection of more PCs for the model space and consequently fewer PCs for the residual space will change the results slightly If all eight PCs are used to construct the model space then the SPE will be rendered ineffective although the results for the Hotelling T2 will improve Even though the results from the Hotelling T2 improve with the selection of more PCs the information available through the SPE is lost There are ways to select the optimum number of PCs necessary to optimize the information captured from both subspaces often the selection is purely based on experience or experimentation although there are statistical methods such as the maximum likelihood estimator MLE which can estimate the optimum number of PCs to use 6 CONCLUSIONS A set of experiments were conducted to establish the healthy or normal operation on a set of notebook computers subjected to range of usages and environmental conditions A test computer was then subjected to field use conditions and evaluated in-situ using Mahalanobis Distance and Projection Pursuit analysis techniques The Projection Pursuit analysis method was also used to identify 7 


key parameters for root cause analysis of anomalies This study emphasizes that the defined baseline can be used to characterize a new computer model This will allow us to characterize a new notebook computer regardless of the model and reduce the time for analysis In this study PPA and MD were independently used to identify the similarity of new observations to healthy data PPA performed this analysis in a reduced dimension based on an optimization criterion maximum variance It was also found that PPA can identify the faulty parameters based on the data whereas MD requires an understanding of the system The strength of PPA lies in the ability to decompose the signal and extract additional information not originally available used to identify faults in the system PPA overcomes masking effects when working with highly correlated data The strength of the MD method is that it preserves all the information available because it does not reduce the original dimensionality of the data The drawbacks of using just the MD method are that it cannot be used directly for fault identification and it is susceptible to masking effects With the MD results and our understanding of the system functionality four critical parameters were empirically identified the fan speed and the three temperature components CPU temperature motherboard temperature and videocard temperature In parallel in the PPA approach the principal component space also identified the fan speed as the most dominant and from the residual principal component space three temperature parameters were identified to be dominant mathematically confirming the earlier empirical conclusion The cross-validated result shows that these two algorithms can be used for fault detection and isolation The MD method can be used for quick fault detection at a system level and fault isolation can be made if related fault to MD signatures are available PPA can also be used when system faults are not known and where critical parameters need to be identified ACKNOWLEDGEMENT This work is sponsored by the members of the CALCE Prognostics and Health Management Consortium at the University of Maryland College Park REFERENCES 1 N Vichare P Rodgers V Eveloy and M Pecht Environment and Usage Monitoring of Electronic Products for Health Assessment and Product Design International Journal of Quality Technology and Quantitative Management 2\(4 235-250 2007 2 J Gu N Vichare T Tracy and M Pecht Prognostics Implementation Methods for Electronics 53rd Annual Reliability  Maintainability Symposium RAMS Florida 2007 3 G Zhang C Kwan R Xu N Vichare and M Pecht An Enhanced Prognostic Model for Intermittent Failures in Digital Electronics IEEE Aerospace Conference Big Sky MT March 2007 4 N Vichare and M Pecht Enabling Electronic Prognostics Using Thermal Data Proceedings of the 12th International Workshop on Thermal Investigation of ICs and Systems Nice Cote d'Azur France 27-29 September 2006 5 N Vichare P Rodgers and M Pecht Methods for Binning and Density Estimation of Load Parameters for Prognostics and Health Management International Journal of Performability Engineering Vol 2 No 2 April 2006 6 A Fraser N Hengartner K Vixie and B Wohlberg Incorporating Invariants in Mahalanobis Distance based Classifiers Application to Face Recognition in International Joint Conference on Neural Networks IJCNN Portland OR USA Jul 2003 7 J Edward Jackson Govind S Mudholkar Control Procedures for Residuals Associated With Principal Component Analysis Technometrics Vol.21 No.3 1979 8 J Liu Khiang-Wee Lim R Srinivasan and X Doan On-Line Process Monitoring and Fault Isolation Using PCA Proceedings of the 2005 IEEE International Symposium on Mediterranean Conference on Control and Automation pp 658 661 2005 9 G Taguchi S Chowdhury and Y Wu The Mahalanobis-Taguchi System New York McGrawHill 2001 10 E B Martin A J Morris and J Zhang Process Performance Monitoring Using Multivariate Statistical Process CSontrol IEE Proc Control Theory Application Vol 143 No.2 March 1996 11 H Chen G Jiang C Ungureanu and K Yoshihira Failure Detection and Localization in Component Based Systems by Online Tracking KDD 2005 12 H Wang Z Song and P Li Fault Detection Behavior and Performance Analysis of Principal Component Analysis Based Process Monitoring Methods American Chemical Society Vol 41 pp 2455 2464 2002 13 H H Yue S.J Qin Reconstruction-Based Fault Identification Using a Combined Index American Chemical Society Vol 40 pp 4403-4414 2001 8 


BIOGRAPHY Sachin Kumar received the B.S degree in Metallurgical Engineering from the Bihar Institute of Technology and the M.Tech degree in Reliability Engineering from the Indian Institute of Technology Kharagpur He is currently pursuing the Ph.D degree in Mechanical Engineering at the University of Maryland College Park His research interests include reliability electronic system prognostics and health and usage monitoring of systems Vasilis Sotiris received the B.S degree in Aerospace Engineering from Rutgers University in New Brunswick New Jersey and the M S degree in Mechanical Engineering from Columbia University in New York He worked as a Systems Engineer for Lockheed Martin Corporation concentrating on software development projects for the Federal Aviation Administration He is currently pursuing the Ph.D degree in Applied Mathematics at the University of Maryland College Park His research interests are in the field of applied statistics and computational mathematics related to diagnostics and prognostics for electronic systems   _ n Acoustics an M.S in Electrical _  in Engineering Mechanics from the g  University of Wisconsin at Madison He is a Professional Engineer an IEEE Fellow and an ASME Fellow He has received the 3M Research Award for electronics packaging the IEEE Award for chairing key Reliability Standards and the IMAPS William D Ashman Memorial Achievement Award for his contributions in electronics reliability analysis He has written over twenty books on electronic products development use and supply chain management He served as chief editor of the IEEE Transactions on Reliability for eight years and on the advisory board of IEEE Spectrum He has been the chief editor for Microelectronics Reliability for over eleven years and an associate editor for the IEEE Transactions on Components and Packaging Technology He is a Chair Professor and the founder of the Center for Advanced Life Cycle Engineering CALCE and the Electronic Products and Systems Consortium at the University of Maryland He has also been leading a research team in the area of prognostics and formed the Prognostics and Health Management Consortium at the University of Maryland He has consulted for over 50 major international electronics companies providing expertise in strategic planning design test prognostics IP and risk assessment of electronic products and systems 9 


Engineering Education Annual Conference & Exposition 2005 3  Chong N., and M. Yamamoto Collaborative Learning Using Wiki and Flexnetdiscuss: a Pilot Study  Proceedings of the fifth IASTED International Conference on Web-based Education Puerto Vallarta, Mexico, Jan 23-25 2006 4  Engeström, Y. Learning by Expanding. OrientaKonsultit Oy, Helsinki, 1987 5  Forte A., and A. Bruckman From Wikipedia to the Classroom: Exploring Online Publication and Learning  Proceedings of the 7 th International Conference on Learning Sciences, Bloomington, Indiana, 2006, pp. 182-188 6  Grierson H., Nicol D., Littlejohn A., and A Wodehouse Structuring and Sharing Information Resources to Support Concept Development and Design Learning  Proceedin gs of the Networked Learning Conference, 2004 7  Gross Davis, B., Tools for Teaching, Jossey-Bass Publishers, 1993 8  Hon A. and W. Chun The Agile Teaching/Learning Methodology and its E-learning Platform Wenyin Liu, Yuanchun Shi, Qing Li \(Eds Advances in Web-Based Learning - ICWL 2004, Third International Conference, Beijing, China, August 8-11, 2004 9  Johnson, R. and D. Johnson An Overview of Cooperative Learning originally published in J. Thousand A. Villa and A. Nevin \(eds\ativity and Collaborative Learning, Brookes Press, Baltimore, 1994 10  Järvinen, E-M Education about and through Technology. In search of More Appropriate Pedagogical Approaches to Technology Education Acta Universitates Ouluensis, E 50. Oulu: Oulun yliopisto, 2001 11  Kim S., Han H., and S. Han The Study on Effective Programming Learning Using Wiki Community Systems Innovative Approaches for Learning and Knowledge Sharing, Springer Berlin, Vol. 4227/2006, pp 646-651 12  Koufman-Frederick, A., Lillie, M., PattisonGordon, L., Watt, D., and R. Carter Electronic Collaboration: A Practical Guide for Educators The LAB at Brown University, 1999 13  Leuf B., and W. Cunningham The Wiki Way Quick Collaboration on the Web Addison-Wesley, 2001 14  Mirijamdotter A., Somerville M. and Holst M An Interactive and Iterativ e Evaluation Approach for Creating Collaborative Learning Environments The Electronic Journal Information Systems Evaluation, Vol. 9 No. 2, 2006 pp. 83-92 15  Murugesan S Understanding Web 2.0 IT Pro July/August 2007 16  Parker K. and Chao J Wiki as a Teaching Tool  Interdisciplinary Journal of Knowledge and Learning Objects. Vol. 3, 2007 17  Patokorpi, E., Tétard F., Qiao F. and N. Sjövall Mobile Learning Objects to Support Constructivist Learning in Learning Objects: Applications, Implications and Future Directions, Koohang A. and K. Harman \(eds 2007 18  Poikela, E. and A.R. Nummenmaa Ongelmaperustainen oppiminen tiedon ja osaamisen tuottamisen strategiana Problem-based learning as a strategy for knowledge building\ Poikela, E. \(ed Ongelmaperustainen pedagogiikka teoriaa ja käytänt Problem-based pedagogy theory and practice\ Tampere Tampere University Press, 2002 19  Reinhold S., and D. Abawi Concepts for Extending Wiki Systems to Supplement Collaborative Learning in Technologies for E-Learning and Digital Entertainment: Proceedings of the First International Conference on Edutainment, Berlin : Springer, 2006, p. 755 767 20  Richardson, W Blogs, Wikis, Podcasts, and Other Powerful Web Tools for Classrooms Sage Publications: California, US; London UK; New Delhi, India 2006 21  Schaffert S., Bischof D., Bürger T., Gruber A Hilzensauer W., and S. Schaffert Learning with Semantic Wikis in First Workshop SemWiki2006 - From Wiki to Semantics, co-located with the 3rd Annual European Semantic Web Conference \(ESWC\ Budva, Montenegro 11th - 14th June, 2006 22  Schwartz L, Clark S., Cossarin M., and J. Rudolph Educational Wikis: Features and Selection Criteria  International Review of Research in Open and Distance Learning, Vol. 5, No. 1, 2004 23  Stvilia, B., Twidale, M. B., Smith, L. C., and L Gasser Assessing information quality of a communitybased encyclopedia in: F. Naumann, M. Gertz, S. Mednick Eds.\, Proceedings of the International Conference on Information Quality - ICIQ 2005, Cambridge, MA: MITIQ 2005, pp. 442-454 24  Tétard, F. and E. Patokorpi A Constructivist Approach to Information Systems Teaching Journal of Information Systems Education, 16\(2\05, pp. 167-176 25  Viégas F., Wattenberg M., Kriss J., and F. van Hamn Talk Before You Type: Coordination in Wikipedia  Proceedings of the 40th A nnual Hawaii In ternational Conference on System Sciences, 2007, p. 78 26  Wagner, C. and P. Prasarnphanich, P Innovating Collaborative Content Creation: The Role of Altruism and Wiki Technology Proceedings of the 40th Annual Hawaii International Conference on System Sciences, 2007 27  Wang C., and D. Turner D Extending the Wiki Paradigm for Use in the Classroom Proceedings of the International Conference on Information Technology Coding and Computing, 2004, p. 255 28  Wheeler S., Yeomans P. and D. Wheeler D. \(2008 The good, the bad and the wiki: evaluating studentgenerated content for collaborative learning in British Journal of Education Technology, 2008 29  Wiki EduTech Wiki retrieved April 9, 2008 from http://edutechwiki.unige.ch/mediawiki/index.php?title=Wiki oldid=17132   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


impodt Top Leve M d Ile Level Dom1ain Lave i 1po Scenwd1o Level Figure 8 Ilium Ontology Suite typical It is possible and in fact common to employ only a few of these ontologies in particular studies For example many studies are only concerned with physical platforms and therefore only need the MilAsset ontology which imports IlumAsset IliumFramework and DUL Figure 8 illustrates the existing Ilium Ontology Suite and a typical import pattern for a complex Modeling and Simulation application scenario Currently the illustrated suite less Millnfo MilComm and UAV defines 1240 OWL Classes 274 OWL Object Properties and 188 OWL Datatype Properties 6 AN EXAMPLE APPLICATION We are currently using the described approach to conduct systems requirements and other engineering analyses for military aerospace systems In these investigations it is useful to consider detailed aspects of the system design or software prototype behavior in the context of large scale network-centric military operations In the past year in particular we have assembled a collection of well known and widely accepted military simulations to prepare for an extensive investigation of requirements for autonomy in unmanned military platforms It is believed that useful insights in the study will be sensitive to important details of system behavior supporting sensor platform and communications technologies as well as the combined effects of these technologies on an advanced highly networked military force Thus the study environment must provide a means to reliably model and evaluate significant technical detail and to measure the effects as well as propagation of those effects throughout a broad operational context 11 


I r Z  Figure 9 A Military Modeling and Simulation Configuration of the Ilium Framework To do that we have assembled a collection of trusted military simulations that span the range of such applications from fine-grained simulations that focus on detailed interactions between two entities to coarse-grained simulations of military campaigns Figure 9 illustrates the Ilium Framework configured to accommodate those simulations as well as representative prototype systems In this case the legacy applications are simulations analysis systems and design tools Prototypes are notional UAV autopilots route planners decision support systems and similar applications The Framework augments these applications with control objects and agents that coordinate computations in the composed system and specialize agents that typically represent the characteristics of a new system or technology that cannot be easily or adequately simulated in any of the component legacy systems In our current study the following systems have Ilium Framework plugins and are used as components in the study environment 12 Al f<cOmmi.ain FIa 1A  


THUNDER is a campaign simulation that models national military forces and military operations that extend over months 30  Characteristics of individual systems are evaluated statistically and their effects on the overall campaign are not explicitly known Political social and cultural objects and concepts are not included in the model In this configuration THUNDER is used to generate force level tasking mission orders and to evaluate and adjust for the results of missions with respect to campaign plans SEAS is a force level simulation that simulates battles between major forces in combat operations that typically last for hours and as much as a day SEAS features a flexible rules based decision logic that can influence behavior at both the commander and individual combatant level SEAS executes the missions requested by THUNDER and provides a manageable dynamic context for examining the behavior of prototypes in a range of typical operational situations  31 A dynamic plug-in supports interaction with the Framework and other pugged-in components EADSIM is a trusted model of air defense systems that is in particular sensitive to many important design attributes of individual systems In certain modified forms it can reference advanced sensor and engagement decision models In our study configuration EADSIM simulates enemy air defenses in selected e.g significant to our investigation portions of the virtual battle A dynamic plug-in supports system control as well as interaction with the Framework and other pugged-in components A prototype aircraft mission planning system plug-in supports virtual real time mission plan creation and updates In particular the system provides automatic route planning for selected platforms that have been assigned missions by THUNDER and that are subsequently executed in simulation in SEAS and EADSIM The Ilium Framework itself provides software agents that are used to model notional or experimental UAV characteristics and behaviors Depending on the objectives of a particular study the Framework may also provide agents that address advanced Command and Control concepts to coordinate the interaction of the various component systems We maintain a semantic consistency among the plugged-in component applications by developing a single operational scenario as initial input for a study and deriving the necessary application configuration data from that source We create an RDF model of the scenario based on the Ilium suite of ontologies that includes political context issues objectives sensitivities etc military context centers of gravity campaign objectives etc geophysical environment military units order of battle unit equipment lists etc command and control assets platforms weapons ISR systems etc Figure 11 below is an excerpt from an operational scenario set in the Southwest U.S depicting a description of an Air Force Wing assigned to a notional Joint Task Force The Wing is based at Bishop Air Base and has six Fighter Squadrons assigned to it Additional detail about each of those squadrons as well as the base is found in the model in this case an rdf:resource  associated with it morg:AirForceWing rdf:ID="USAF_366 Air_Exp Wg dc:creator>Doug Holmes</dc:creator mgeo:basedAt rdf:resource="#BishopAB geo:positionedAt rdf:resource="#BishopAB_pos morg:assignedOrg rdf:resource="#USAF 390_Ftr_Sq morg:assignedOrg rdf:resource="#USAF_494_Ftr_Sq dc date I 0/20/07</dc date morg:assignedOrg rdf:resource="#USAF_496 Ftr_Sq rdfs:comment>366 AEW F-22 F-15 KC-135 130]</rdfs:comment morg:assignedOrg rdf:resource="#USAF 389_Ftr_Sq morg:assignedOrg rdf:resource="#USAF 391 Ftr_Sq morg:assignedOrg rdf:resource="#USAF 495_Ftr_Sq rdfs:label>366 Air Expeditionary Wing</rdfs:label morg:AirForceWing ab9 7P.b ii uI1 L L CFigure 11 An RDF model of a notional USAF Wing Figure 12 is another excerpt from the same scenario illustrating a model of a particular Fighter Squadron and one of the F-15E aircraft it operates Figure 10 Legacy Components in the Ilium Framework 13 suka 


morg:AirliorceSquadron rd:lD U SAF _8 htr Sq rdfs:label>8th Fighter Squadron</rdfs:label geo:positionedAt rdf:resource="#GeorgeAB_pos msim:hasSeasModel rdf:resource="#BAFGA mast:hasModel rdf:resource="#BAFGA rdfs:comment>An F15E Squadron</rdfs:comment dc:date>9/26/07</dc:date dc:creator>Doug Holmes</dc:creator mgeo:basedAt rdf:resource="#GeorgeAB morg:AirForceSquadron F1 5E rdf:ID="F1 5E 05 dul:isReferenceOfRealization rdf:resource="#AirObj ect_2007 mast:equipment-of rdf:resource="#USAF 7 Ftr_Sq dc:creator>Doug Hollmes</dc:creator dc:date>9/26/07</dc:date msim:hasSeasPlane rdf:resource="#F15E mgeo:basedAt rdf:resource="#GeorgeAB geo:positionedAt rdf:resource="#GeorgeAB-pos rdfs:label>F-15E 005</rdfs:label F-15E Figure 12 An RDF model of a Fighter Squadron and one of its aircraft Note that these models also have associated SEAS models that contain information peculiar to the SEAS simulation about these entities This information is used to configure SEAS to properly represent these particular entities We are also able to insert objects and information that may be of interest in our study that is not represented in any of the component simulations or other applications Where those objects are related to an object that is simulated that relationship permits inferences about the effects on them as a result of actions that are computed in a simulation For example it is possible to indicate that GeorgeAB defends the capital city and lend special significance to the actions of aircraft that are based there even though none of the simulations in the composite system have any notion of capital city Finally the operational scenario provides an explicit record of the assumptions that underly the study and can also include and explicit representation of system and study goals This practice improves analysis and may enable future knowledge based analytical tools Throughout the process of constructing the operational scenario and when it is complete we use one or more Description Logic Reasoners to ensure the logical consistency of the the model A number of these automatic theorem provers are freely available including Pellet 32 33 FaCT  34 and OWLIM 35 are used in the Framework to compute logical entailments and to complete RDF models as well as to ensure the consistency of models In the later capacity frequent checks will identify errors in the construction some e.g Pellet also indicate the source of the error and aid in repairing it As a result we are confident that the operational scenario is a sound model The primary use of the Reasoners allows us to significantly extend the explicit RDF model that is created For example a squadron is explicitly specified as assignedTo a wing and that relationship is the inverse of assignedOrg then the system can infer that the wing has the squadron as an assigned organization even though that fact has never been asserted Similarly if the same relationship is defined as a transitive relationship it is possible to infer that a flight that is assigned to the squadron is also assigned to the wing Once we have an operational scenario that has been classified by a Reasoner we then use the completed model to configure the composite system Ilium and the pluggedin systems to execute the simulation We use SPARQL a W3C standard query language designed to access RDFL to extract data from the scenario to create the input files needed to configure the various applications.[36 SELECT name pos long lat alt vis W1HLERE name rdf:type mast SeasLocation name geo:positionedAt pos pos pos:long long pos pos:lat lat pos pos:alt alt name msim:Visible vis  Figure 13 A typical SPARQL query In a similar fashion SPARQL is used to extract data from the scenario to create the necessary Ilium java surrogate control and agents SPARQL can also be used to review the scenario and answer questions that have arisen since the inception of the study At this point we are able execute the scenario with confidence that it will produce reliable results 7 CONCLUSIONS We have developed a methodology and supporting tools for creating an operational scenario that supports the semantic interoperability of an ad hoc collection of legacy applications and extends their capabilities The method depends on and ensures the logical integrity of the composite system Therefore when we assemble as a collection of legacy component systems that were not originally intended to interoperate with other systems we can be confidant that the composite system will produce consistent results Logical consistency implies that to the degree that we trust the interpretation underlying the model the results of operations on the model are trustworthy If for example the model is based on a Newtonian interpretation of physics the model ought to provide reliable answers to questions about automotive and even aeronautical engineering but probably not to all questions in astronomy or cosmology This methodology extends the utility of trusted simulations allowing integration of finegrained simulations that are sensitive to design requirements with high level coarse-grained simulations that are sensitive to acquisition issues and policy The potential benefits of 14 


this sort of interoperation may range from obvious production efficiencies to clearer insights into system requirements Finally an important side-effect of the approach is the OWL/RDF knowledge base formed by the combination of the operational scenario and the results of the operations of the legacy systems That knowledge base and the use of SPARQL queries and SWRL rules effectively expands the functionality of the system and greatly improves the analysis of the output of the system of cooperating simulations and tools We have prepared a foundation for the application of Semantic Web and other knowledge based tools in the analysis and design of unmanned systems We anticipate the development and application of these tools and the addition of autonomy directed Multiple Agent Systems MAS that use RDF/XML inter-agent communications in the coming year REFERENCES 1 NAFCAM 2001 Exploiting EManufacturing:Ineroperability of Software Systems Used by U.S Manufacturers available at 12 Protege Ontology Editor documentation available at http proteae.stanf6rd.edu 13 Top Braid Composer Datasheet available at 14 W and Nicola Guarino 2001 Support for Ontological Analysis of Taxonomic Relationships J Data and Knowled 39\(1 October 2001 15 Natalya F Noy and Deborah L McGuinness Ontology Development 10 1 A Guide to Creating Your First Ontology  Stanford University Stanford CA 94305 16 Alan Rector Modularisation of Domain Ontologies Implemented in Description Logics and related formalisms including OWL K-CAP'03 October 23-25 2003 Sanibel Island Florida USA pp 121-8 2003 17 Cyc Homepage available at htc.c 18 Open Cyc home page available at 19 SUMO Description and Home Page available at 19 SENSUS Description and Home Page available at 2 Bemers-Lee Tim Hendler Jim Lasilla Ora The Semantic Web Scientific American available at 20 DOLCE A Descriptive Ontology for Linguistic and Cognitive Engineering ontology and documents available at 3 Bemers-Lee Tim Blog on Design Issues available at 4 RDF Primer available at s chema/#ref-rdf-primer 5 Lacey Lee OWL Representing Information Using the Web Ontology Language Trafford Publishing 2005 6 OWL Web Ontology Language Guide available at Jten pHomewPage availal adt 7 Jena Home Page available at 8 Protege Home Page available at h1ttp  protegest nftanford.edu,X 1 9 Baader Calvanese McGuiness Nardi and PatelSchnieder Description Logic Handbook 10 Oberle Daniel Semantic Management of Middleware Springer 2006 OWL Web Ontology Language Guide available at 21 Nicola Gauarino Claudio Masolo Stefano Borgo Aldo Gangemi and Alessandro Oltramari Ontology Infrastructure for the Semantic Web Wonder World Deliverable DI 8 Laboratory for Applied Ontology Trento Italy 2001 available on line at ht X1t1 22 DUL.owl ontology available at www.1oa23t og DLl 23 Amy Knutilla Steven Polyak Craig Schlenoff Austin Tate Shu Chiun Cheah Steven Ray and Richard Anderson Process Specification Language An Analysis of Existing Representations NIST report available at http llwww.mel.nlist.gov/msid.librarZ/d.oc/psl-1 _.df 24 Process Specification Language Ontology available at http-//www,55,me l.nit gov/psl 25 Ontology for Geography Markup Language GML3.0 owl ontology available at ok i.cae.drexel.edu./-wbs/onltology/2004/09/ogc-gmI1 26 Ontology for Geography Markup Language GML3.0 of Open GIS Consortium OGC Home Page available at 15 


 Peter Maguire Using THUNDER for Campaign Studies DSTO-TN-0303 DSTO Melbourne August 2000 28 User Manual SEAS Version 3.7 U.S Air Force SMC/XR February 2007 29 Bijan Parsia and Evren Sirin Pellet and OWL DL Reasoner MINDSWAP Research Group University of Maryland College Park available at 30 Pellet Home Page available at 31 FaCT Home Page available at 32 OWLIM Home Page available at 3 A SPARQL Tutorial available at BIOGRAPHY Douglas Holmes is co-founder and Senior Partner of Java Professionals Inc In the past twenty-two years he has managed and participated in numerous artificial intelligence and knowledge-based programs for DARPA and other research agencies as well as commercial applications in the petroleum and other sectors He is currently developing ontologies and applying Semantic Web technology to support research and development of military unmanned systems He also has over twenty years experience as an Air Force Fighter Pilot and Fighter Weapons School Instructor Mr Holmes has a B.S in Mathematics and Basic Sciences from the U.S Air Force Academy and a M.S in Management Information Systems from Golden Gate University Richard Stocking is the lead Program Investigator/PM for Net Centric Operations Warfare Analysis efforts for Lockheed Martin Aeronautics Advanced Development Programs The Skunk WorksTM He is currently leading efforts researching autonomous UAV operations Current efforts include the integration of Multiple Agent Systems and other autonomy systems within the Ilium Framework He has over thirty years experience and over 11,000 flight hours with multiple C4ISR systems in the US Army and US Navy Mr Stocking has a M.S in Systems Technology from the Naval Postgraduate School 16 


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


