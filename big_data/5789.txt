Image Mining Using Association Rule Shaikh Nikhat Fatma                                                         Madhu Nashipudimath Department Of Computer , Mumbai University                     Department Of Computer , Mumbai University Pillaiês Institute Of Information Technology,                           Pillaiês Institute Of Information Technology New Panvel                                                                            New Panvel nikhats10@yahoo.com                                                    madhu.nashipudi@yahoo.co.in Abstract  Image mining deals with the extraction of image patterns from a large collection of images.  Clearly image  mining is different   from  low-level  computer vision and image processing  techniques because the focus of image mining is in extraction of patterns from large collection  of images, whereas  the focus of computer  vision and image  processing techniques  is  in  understanding  and or extracting  specific features from a single  image. While there seems to be some overlaps between image mining and content-based retrieval \(both are dealing with large collection of images\mage mining goes beyond the problem of retrieving relevant images. In image mining, the goal is the discovery of image patterns that are significant in a given collection of images Keywords I mage  mining \(IM\; function-driven; knowledge driven; information driven; knowledge remounting I I NTRODUCTION Image mining deals with extraction of implicit knowledge image data relationship or other patterns not explicitly stored in images and uses ideas from computer vision, image processing, image retrieval, data mining, machine learning databases and AI. The fundamental challenge in image mining is to determine how low-level, pixel representation contained in an image or an image sequence can be effectively and efficiently processed to identify high-level spatial objects and relationships. Typical image mining process involves preprocessing, transformations and feature extraction, mining \(to discover significant patterns out of extracted features evaluation and interpretation and obtaining the final knowledge. Various techniques from existing domains are also applied to image mining and include object recognition learning, clustering and classification, just to name a few Association rule mining is a well-known data mining technique that aims to find interesting patterns in very large databases. Some preliminary work has been done to apply association rule mining on sets of images to find interesting patterns  In the remaining paper  in section II we take the review of how  Image Mining is actually done actually work. In section III we take an example. In section IV we take an over view of Image Mining Frameworks. In section V we make a conclusion for Image Mining and  the last section  includes all references for this paper II M ETHOD FOR IMAGE MINING In this section, we present the algorithms needed to perform the mining of associations within the context of images. The four major image mining steps are as follows 1.Feature extraction. Segment images into regions identifyable by region descriptors \(blobs\y one blob represents one object. This step is also called segmentation 2.Object identification and record creation. Compare objects in one image to objects in every other image. Label each object with an id. We call this step the preprocessing algorithm 3.Create auxiliary images. Generate images with identified objects to interpret the association rules obtained from the following step 4.Apply data mining algorithm to produce object association rules The idea of this method is selecting a collection of images that belong to a specific field \(e.g. weather\e selection stage we will extract the objects from each image and indexing all the images with its objects in transaction database, the data base contain image identification and the objects that belong to each images with its features. After creating the transaction data base that contains all images and its feature we will use the proposed data mining methods to associate rules between the objects. This will help us for prediction \(e.g. if image sky contain black clouds then  it will rain \(65   The following block diagram presents the proposed IM method 587 978-1-4673-0126-8/11/$26.00 c  2011 IEEE 


Figure 1 Block diagram of image mining method 1 Select a collection of images that belong to the same field \(E.g. medical images, geographical images, persons images, etc 2 Image Retrieval. Image mining requires that images can be retrieved according to some requirement specifications In the proposed work we comprise image retrieval by derived or logical features like objects of a given type or individual  objects or persons using edge detection techniques  After we extract object we will encoded it as follows O1: circle O2: triangle O3: square Figure 2 Example of an image Figure 3  O bject extraction using edge detection 3 Image Indexing. Image mining systems require a fast and efficient mechanism for  the  retrieval of image data Conventional database systems such as relational databases facilitate indexing on primary or secondary key\(s\e will create two databases The first one contains all the objects that have been extracting from the images and its    features Table 1 First database contains the objects and its features Therefore the association rule with spatial relationships could be V-Next-to \([red, circle, small  HNext-to \([red, circle yell ow  lar ge    gre en  3 4  In this example, only three dimensions were needed and we made use of the wildcard * to replace absent values The second Database contains all the images and the objects that belong to each image Table 2  Second Database Contains Each  Image and its Objects 588 2011 World Congress on Information and Communication Technologies 


4 Finally, the last step is applying the proposed mining techniques using the data of the        images that has been index to the database 5\fter that we will use the first a proposed algorithm to find the frequent item sets     from the specific table and the result will be the following 6\The final step we will use the second proposed  algorithm to find association rules between the objects and we will have the following results 1 O2,O2} [100 2\,O4 O2} [100  3 O2} [10  4 O2,O4} [10  5 O4} [10  6 O2} [10  7 O2} [100  8 O4} [100  III E XAMPLE A simple example illustrating how image mining algorithms work with n=10. The original images and their corresponding blobs are shown on Figure 4 and Figure 5 Association rules corresponding to the identified objects are also shown .  10 representative images are chosen from the image set created for the experiments Figures 4 and 5 shows the original image at the left with several geoemetric shapes and white background. These images are labeled with an image id. These images are the only input data for the program; no domain knowledge is used. Then a series of blob images are shown, each containing one blob. These images are labeled with the id obtained by preprocessing . Each blob has a close \(most times equal\geometric shape There are some cases in which one blob corresponds to several geometric shapes. For instance in image 013 object 2 corresponds to the triangle and object 3 corresponds to the circle. In image 131 object 4 corresponds to all the shapes in the image The data mining was done with a 20% support and 70 confidence. The output is a set of association rules whose support and confidence are above these thresholds The 66 rules obtained by the program are shown . Let us analyse some of these rules. The first rule  3 2 means that if there is circle in the image then there is also a triangle. In fact with these simple images, there was never a circle without a triangle. In this case the rule actually has a higher support and a higher confi dence than that obtained by the program \(50% and 83 % respectively\. This happened because the circle had two different object identifiers: 3 and 7. The rule {2,3,5 8 says if there is a circle, a triangle and an hexagon then there is also a square Once again images containing the first three shapes always contained the square. Another interesting rule is  3,11 2 .In this case the rule says that if there are a circle and an ellipse then there is also a tria ngle; once again the rule is valid; note that this rule has a low support It important to note that several incorrect or useless blob matches such as 9, 10, 13, 14, 16 are altered out by the 30% support. That is the case for images 029, 108, 119 131 and 144. There are no rules that involve these identified objects \(matched blobs Figure 4 F irst part of images and blobs 2011 World Congress on Information and Communication Technologies 589 


FIGURE 5 S ECOND P ART OF I MAGES AND BLOBS RULES GENERATED IV I MAGE M INING F RAMEWORKS 1 Function-Driven Image Mining   Framework Model Function-driven Image Mining Framework Model is usually organized by modules with different functions. It divides the function model into two modules 1\a obtaining, pre-treatment and saving  module Which is mainly used for image pick-up, original image storage and searching[8  2\mage mining module  Which is used for mining image model and  meanings There are 4 function modules included in this system Figure 6  F unction-driven image mining framework   model 1\age data acquisition Get image from multimedia database 2 Pre processor Get image ch  3 Searches engine use the image characters   for matching inquire 4\ Knowledge discovery module Mining   image 2 Information Driven Image Mining Framework Model This model emphasize different roles of  different image arrangement, that incarnate description mechanism of vary arrangement of image data,  mark of 4 layers The Four Information Levels We will describe the four information levels in our proposed framework 1 Pixel Level The Pixel Level is the lowest layer in an image mining system. It consists of raw image information such as image 590 2011 World Congress on Information and Communication Technologies 


pixels and primitive image features such as color, texture and edge information  2 Object Level The focus of the Object level is to identify     domainspecific features such as objects and homogeneous regions in the images. An object recognition module consists of four components: model database, feature detector hypothesizer and hypothesis verifier. The model database contains all the models known to the system. The models contain important features that describe the objects. The detected image primitive features in the Pixel Level are used to help the hypothesizer to assign likelihood to the objects in the image The verifier uses the models to verify the hypothesis and refine the object likelihood. The system finally selects the object with the highest likelihood as the correct object Figure 7 Information driven image mining framework   model To improve the accuracy of object recognition, image segmentation is performed on partially recognized image objects rather than randomly segmenting the image. The techniques include: çcharacteristic mapsé to locate a particular known object in images, machine learning techniques to generate recognizers automatically , and use a set of examples already labelled by the domain expert to find common objects in images . Once the objects within an image can be accurately identified, the Object Level is able to deal with queries such as çRetrieve images of round tableé and Retrieve images of birds flying in the blue skyé. However, it is unable to answer queries such as çRetrieve all images concerning Graduation ceremonyé or çRetrieve all images that depicts a sorrowful mood 3 Semantic Concept Level While objects are the fundamental building blocks in an image there is çsemantic gap between the Object level and Semantic Concept level. Abstract concepts such as happy, sad, and the scene information are not captured at the Object level. Such information requires domain knowledge as well as state-ofthe-art pattern discovery techniques to uncover useful patterns that are able to describe the scenes or the abstract concepts Common pattern discovery techniques include image classification, image clustering and association rule mining With the Semantic Concept Level, queries involving highlevel reasoning about the meaning and purpose of the objects and scene depicted can be answered. Thus, we will able to answer queries such as: çRetrieve the images of a football matché and çRetrieve the images depicting happinessé. It would be tempting to stop at this level. However, careful analysis reveals that there is still one vital piece of missing information Ö that of the domain knowledge external to images. Queries like: çRetrieve all medical images with high chances of blindness within one monthé, requires linking the medical images with the medical knowledge of chance of blindness within one month. Neither the Pixel level, the Object level, nor the Semantic Concept level is able to support such queries 4 Pattern and Knowledge Level At this level, we are concerned with not just the information derivable from images, but also all the domain-related alphanumeric data. The key issue here is the integration of knowledge discovered from the image databases and the alphanumeric databases. A comprehensive image mining system would not only mine useful patterns from large collections of images but also integrate the results with alphanumeric data to mine for further patterns. For example, it is useful to combine heart perfusion images and the associated clinical data to  discover rules in high dimensional medical records that may suggest early diagnosis of heart disease. IRIS an Integrated Retinal Information System, is designed to integrate both patient data and their corresponding retinal images to discover interesting patterns and trends on diabetic retinopathy BRAin-Image Database is another image mining system developed to discover associations between structures and functions of human brain . The brain modalities were studied by the image mining process and the brain functions deficits/disorders\btainable from the patientsê relational records. Two kinds of information are used together to perform the functional brain mapping. Discovering knowledge from data stored in  alphanumeric  databases, such as  relational databases, has been the focal point of much work in data mining. However, with advances in secondary storage capacity, coupled with a relatively low storage cost, more and more nonstandard data is being accumulated. One category of non-standardé data is image data \(others include free text video, sound, etc\here is currently a very substantial collection of image data that can be mined to discover new and valuable knowledge. The central research issue in image mining is how to pre-process image sets so that they can be represented in a form that supports the application of data mining algorithms 2011 World Congress on Information and Communication Technologies 591 


A common representation is that of feature vectors were each image is represented as vector. Typically each vector represents some subset of feature values taken from some global set of features. A trivial example is where images are represented as primitive shape and colour pairs  Thus the global set of tuples might be blue square}, {red square}, {yellow square}, {blue circle red circle}, {yellow circle which may be used to describe a set of images blue square}, {red square}, {red circle red square}, {yellow square} {blue circle}, {yellow circle red box}, {red circle}, {yellow circle However, before this can be done it is first necessary to identify the image objects of interest \(i.e. the squares and circles in the above example\ common approach to achieving this is known as çsegmentationé. Segmentation is the process of finding regions in an image \(usually referred to as objects\at share some common attributes \(i.e. they are homogenous in some sense\[9   The process of image segmentation can be helped /enhanced for many applications if there is some application dependent domain knowledge that can be used in the process. In the context of the work described here the authorês are interested in MRI çbrain scansê, and in particularly a specific feature within these scans called the Corpus Callosum An example image is given in Figure 8. The Corpus Callosum is of interest  to researchers for a number of reasons 1. The size and shape of the Corpus Callosum are shown to be correlated to sex, age,  neuro degenerative diseases \(such as epilepsy\d various lateralized behaviour in people 2. It is conjectured that the size and shape of the  Corpus Callosum reflects certain human characteristics \(such as a mathematical or musical ability 3. It is a very distinctive feature in MRI brain scans Several studies indicate that the size and shape of the Corpus Callosum in human brains are correlated to sex , age , brain growth and degeneration, handedness and various types of brain dysfunction Figure 8  C orpus callosum in a midsagittal brain MRI image In order to find such correlations in living brains, Magnetic Resonance Imaging \(MRI\arded as the best method to obtain cross-sectional area and shape information about the Corpus Callosum. In addition, MRI is fast and safe, without any radiation exposure to the subject such as with x-ray CT  Since manual tracing of Corpus Callosum in MRI data is time consuming, operator dependent, and does not directly give quantitative measures of cross-sectional areas or shape, there is a need for automated and robust methods for localization delineation and shape description of the Corpus Callosum [9 3 Knowledge Driven Image Mining Framework Model Function-driven model is formed from image mining application, and information-driven model is considered from different layer. The essential of image mining is to find knowledge, the above two model doesnêt consider the using of mining knowledge, besides, in the whole course, user is on a passive position to receive the mining module and knowledge Due to the image data itself is an unstructured or semi structure data, so the remounting may happen in image mining, how to mine the maximum knowledge from the mining course. We should know the knowledge user wanted is the knowledge significant[6 1\age choosing The aim of image choosing is to confirm the object of image mining which is an original image data in image database as the userês requirement 2\age disposal It refers to digital image management and image identification. For example, to remove noises from the image or to proof read the anamorphic image, to recover the low information image[10   3\ Character pickup The character information, such as color , shape, Position are picked up, and stored. Character base is very important because it should support the  inquire of image data 4\aracter choosing Optimize The storage of the image character may be overabundant and this factor may affect the operation of the key mining approach so the character choose should be taken before the image mining. If we mark a character with eigenvector, this approach we call dimension decrease. Besides character choosing, sometimes, we should optimize the choosing, including data noise decrease sequence data dispersion and dispersing data continuum 5\age mining Use image mining to mine the data in the image to find related modules. At present, commonly used ways are all from traditional data mining area, such as stat analyses, associate rule analyses, machine learning. etc 6\xplain and comment combining In module/knowledge base, it stores the knowledge units which represent image logic concept; we need integrate data to find more potential modules or knowledge. When mining the module, all redundant or useless modules should be removed, the useful modules  converse to the knowledge which can be understand by the user 592 2011 World Congress on Information and Communication Technologies 


7\age sample training Through the image sample training, the validity and veracity can be highly improv 8\ating learning Users can learn domain knowledge by system mining, and also can input the domain knowledge to the system, which includes how to split the non figurative knowledge into knowledge unit[6  9\main knowledge In the course of mining, all the former approaches, models or the episteme can be used in discovery of the new system Figure 9 Knowledge-driven image mining framework V CONCLUSIONS Image mining is the advanced field of Data mining technique and it has a great   challenge to solve the problems of various systems. The main objective of the image mining is to remove the data loss and extracting the meaningful information to the human expected needs. It retrieves the most matching images from the collection of the images, with respect to the query image. The  framework  models represents the first step towards capturing the different levels of information present in image data and addressing the  question of what are the issues and challenges of discovering useful patterns/knowledge from each level REFERENCES 1   J  H a n a n d M K a m b e r D a ta Mi ning  Co nc e p ts a n d T e c hniqu e s  Morgan Kaufmann,   USA, 2001 2 Ma rg a r e t H  D unha m  D a t a  m i ning Intro duc ti ona r y  a n d   Advanced Topics",  Southern Methodist University  R G o n zalez an d R W o o d s  Digital Im a g e   P r o cessin g   Addison-Wesley Publications  Co, March 1992 4 J i Z h a n g  W y n n e H s u  M o n g L i L e e   An  Information-driven Framework for Image  Mining in Proceedings of 12th International Conference on Database and Expert  Systems Applications \(DEXA\, Munich, Germany, 2001 5   Hila l M Yo usif A b d u l R a hm a n A l Huss a ini, M oha m m a d A   Al-Hamami Using  Image Mining to Discover Association Rules between Image Objects  Yu C h an gj i n   X i a Hon gxi a  The Investigation of Image Mining Framework WUHAN University of Technology Wuhan, China 7 J o s e ph R ode n Mic ha e l  B u rl a n d C h a r le s s Fow lk e s  The Diamond Eye Image Mining System Jet Propulsion Laboratory  J Z h an g W  Hsu and  M  L  L ee I m a ge M i n i n g  Issues  Frameworks and Techniquesé,  Proc. of Second International Workshop on Multimedia Data Mining \(MDM/KDD'2001 San Francisco, CA, USA, August, 2001  A s h r a f E l sa yed  F r an s Co en en  M a rt a G a rc a-F i  a n a an d Vanessa Sluming Segmentation for Medical Image Mining: A Technical Report The University of Liverpool, Liverpool L69 3BX, UK 1 D r V M oh an  A.Kannan Color Image Classification and Retrieval using Image mining Techniques International Journal of  Engineering Science and Technology  Vol. 2\(5 2010, 1014-1020 2011 World Congress on Information and Communication Technologies 593 


algorithm process. In practice, the adversary may achieve part sensitive rules by greedily replacing unknown values with knowns\(0 or 1 V. DISTRIBUTED PRIVACY PRESERVING ASSOCIATION RULE MINING The growth of Internet has triggered tremendous opportunities for distributed data mining, where people jointly conducting mining tasks based on the private inputs they supplies. These mining tasks could occur between mutual un-trusted parties, or even between competitors. So privacy will become a primary concern. In distributed setting privacy-preserving association rule mining algorithms require collaboration between parties to compute the results or share no-sensitive mining results, while provably leading to the disclosure of any sensitive information In general, data distribution has two kinds of forms horizontally partitioned and vertically partitioned Horizontally partitioned data: each site has complete information on a distinct set of entities, and an integrated dataset consists of the union of these datasets. In contrast vertically partitioned data has different types of information at each site; each has partial information on the same set of entities In [14], the authors have studied the privacy-preserving association rule mining problem over horizontally partitioned data. Their methods incorporate cryptographic techniques to minimize the information shared, while adding little overhead to the mining task. The problem of privately mining association rules in vertically partitioned data was addressed in [15-16]. All these methods are almost based on Secure Multiparty Computation \(SMC In distributed privacy preserving association rule mining areas, SMC technology is mainly consist of a set of secure sub-protocols, such as, Secure Sum, Secure Comparison, Dot Product Protocol, Secure Intersection, Secure Set Union and so on. In the following, we will briefly describe the basic idea of two kinds of secure sub-protocols used in horizontally partitioned and vertically partitioned setting A. Secure Sum Secure Sum can securely calculate the sum of values from different sites. Assume that each site i  has some value iv  and all sites want to securely 


compute nv,,vvS \f\f\f 21 , where iv is known to be in the range [0..m]. In horizontally partitioned setting, we can securely calculate the global support count of an itenset by the secure sum sub-protocol B. Dot Product Protocol Many secure dot product protocols have been proposed in the past. The problem is defined as follows: Alice has a ndimensional vector X n21  while Bob has a ndimensional vector Y n21 . At the end of the protocol, Alice should get ba rYXr \f  where br is a random number chosen from uniform distribution that is known only to Bob, and nn yx,,yxyxYX \f\f\f 2211 Through the dot product protocol we can securely calculate the global support count of an itemset whose items are located at different sites in vertically partitioned setting VI.  METRICS FOR MEASURING SIDE-EFFECTS Let R be the set of rules mined from D and sR be the set of sensitive rules that must be protected according to some privacy policies. The goal is to transform D into 'D so that sensitive rules are hidden. Let 'R be the set of rules mined from 'D Ideally, s RRR \b . Nevertheless, undesired side-effects e.g., non-sensitive rules falsely hidden and spurious rules falsely generated, may be produced in the sensitive rule hiding process. So in reality, s RRR \b . To measure the amount of side-effects resulted from privacy preserving process, we introduce one related metric named as Sideeffects Factor \(SEF     R SEF s s  b f\b 3 Where R  is the size of the set R . It is obvious that SEF equals to 0 when s RRR \b holds. The bigger the 


distance between SEF and 0, the larger the side-effects Hence, we should try to decrease the side-effects through optimizing the privacy preserving algorithm VII.   CONCLUSIONS AND FUTURE WORK In this paper, we carried out a wide survey of the different approaches for privacy-preserving association rule mining, and analyzed the major algorithms available for each method and pointed out the existing drawback. However, it was proved that Optimal Sanitization Is NP-hard[5]. All the purposed methods are only approximate to our goal of privacy preserving. We need to further perfect those approaches or develop some new methods for achieving the trade-off: privacy and accuracy. Hence, we recognize that the following problems should be concentrated on 1 assumptions. We should generalize these algorithms so that they can be more widely used in practice 2 mining areas, we should explore more effective algorithms and look for a balance between disclosure cost, computation cost and communication cost 3 improving one usually incurs a cost in the other. How to apply various optimizations to achieve a trade-off should also be paid more attention 4 process. How to measure and decrease its impact on privacy preserving needs to be considered carefully. We also need to define some metrics for measuring 5 association rule mining to other data mining contexts? such as classification, clustering, etc ACKNOWLEDGEMENTS This paper was supported by the Natural Science Foundation of Education Department of Anhui Province in China \(KJ2009B075Z, KJ2009B128Z REFERENCES 1] R. Agrawal, T. Imielinski, and A. Swami, Mining Association Rules between Sets of Items in Large Databases, ACM SIGMOD Record New York,1993,pp. 207-216 2] Han Jiawei, and M. Kamber, Data Ming: Concepts and Techniques Beijing: China Machine Press,2006, pp. 227-250 


3] S. Warner, Randomized Response: A Survey Technique for Eliminating Evasive Answer Bias, J. Am. Stat. Assoc. 1965,60\(309 pp. 63-69 4] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke, Privacy Preserving Mining of Association Rules, Information System, vol 29, Apr. 2004, pp. 343-364 5]  M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim3, V. Verykios Disclosure Limitation of Sensitive Rules, Proc. IEEE Knowledge and Data Engineering Workshop, Chicago, Illinois,1999, pp. 25-52 6] E. Dasseni, V. S. Verykios, A. K. Elmagarmid, and E. Bertino Hiding Association Rules by Using Confidence and Support, Proc the 4th Information Hiding Workshop, Pittsburg, PA, 2001, LNCS 2137, pp. 369-383 7] S. R. M. Oliveira, and O. R. Zaiane, Privacy Preserving Frequent Itemset Mining, Proc. IEEE ICDM Workshop on Privacy, Security and Data Mining, Maebashi, Japan, 2002, pp. 43-54 8] S. R. M. Oliveira and O. R. Zaiane, Algorithms for Balancing Privacy and Knowledge Discovery in Association Rule Mining, Proc the 7th International Database Engineering and Applications Symposium, Hong Kong, China, 2003, pp. 54-63 9] Y. Wu, C.M. Chiang, and A.L.P. Chen, Hiding Sensitive Association Rules with Limited Side-effects, IEEE Transactions on Knowledge and Data Engineering, vol. 19,Jan. 2007,pp. 29-42 10] S.R.M. Oliveira, O.R. Zaiane, and Y. Saygin, Secure AssociationRule Sharing, Advances in Knowledge Discovery and Data Mining Springer Berlin, Heidelberg,Vol. 3056,2004, pp. 74-85 11]  V.S. Verykios, A.K. Elmagarmid, E. Bertino, Y. Saygin, E. Dasseni Association Rule Hiding, IEEE Transactions on Knowledge and Data Engineering, vol. 16, Apr. 2004, pp, 434-447 12] R. Agrawal, and R. Srikant, Privacy-preserving data mining, ACM SIGMOD Record, New York, vol. 29, Feb. 2000,pp.439-450 13] Y. Saygin, V. S. Verykios, and C. Clifton, Using Unknowns to Prevent Discovery of Association Rules, ACM SIGMOD Record New York, vol. 30, Apr. 2001, pp. 45-54 14] M. Kantarcioglu, and C. Clifton, Privacy-Preserving Distributed Mining of Association Rules on Horizontally Partitioned Data, IEEE Transactions on Knowledge and Data Engineering, vol. 16, Sep. 2004 pp. 1026-1037 15] J. Vaidya, and C. Clifton, Privacy Preserving Association Rule Mining in Vertically Partitioned Data, Proc.  the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining Washington, D.C. 2002,pp. 639-644 


16] I. Ioannidis, A. Grama, and M.J. Atallah, A Secure Protocol for Computing Dot-Products in Clustered and Distributed Environments, Proc. the 31st International Conference on Parallel Processing, Vancouver, B.C., Canada,2002, pp. 379-384  


   TABLE III THE ASSOCIATION RULES RESULTS FOR CASE 2 Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Sup. \(74 74 79 79 81 81 72 72 94 94 Hi <- Ma  \(78.2, 95.1 93.9, 95.2 94.7, 94.4 92.9, 97.5 100.0, 100.0 Ma <- Hi  \(94.9, 78.4 95.5, 93.7 94.7, 94.4 96.5, 93.9 100.0, 100.0 Av <- Ma  \(78.2, 98.4 93.9, 95.2 94.7, 100.0 92.9, 97.5 100.0, 100.0 Ma <- Av  \(97.4, 78.9 95.5, 93.7 100.0, 94.7 97.6, 92.8 100.0, 100.0 Vl <- Ma  \(78.2, 98.4 95.5, 95.2 94.7, 100.0 92.9, 100.0 100.0, 100.0 Ma <- Vl  \(98.7, 77.9 95.5, 95.2 100.0, 94.7 100.0, 92.9 100.0, 100.0 Av <- Hi  \(94.9, 97.3 89.4, 94.9 89.5, 100.0 96.5, 97.6 Yo Av  \(100.0, 100.0 Hi <- Av  \(97.4, 94.7 89.4, 94.9 94.7, 94.4 97.6, 96.4 Yo Vl  \(100.0, 100.0 Vl <- Hi  \(94.9, 98.6 90.9, 93.3 94.7, 94.4 96.5, 100.0 Av Vl  \(100.0, 100.0 Hi <- Vl  \(98.7, 94.8 100.0, 96.5 Vl <- Av  \(97.4, 98.7 97.6, 100.0 Av <- Vl  \(98.7, 97.4 100.0, 97.6 Av <- Ma Hi  \(74.4, 98.3 90.6, 97.4 Hi <- Ma Av  \(76.9, 95.0 90.6, 97.4 Ma <- Hi Av  \(92.3, 79.2 94.1, 93.8 Vl <- Ma Hi  \(74.4, 98.3 90.6, 100.0 Hi <- Ma Vl  \(76.9, 95.0 92.9, 97.5 Ma <- Hi Vl  \(93.6, 78.1 96.5, 93.9 Vl <- Ma Av  \(76.9, 98.3 90.6, 100.0 Av <- Ma Vl  \(76.9, 98.3 92.9, 97.5 Ma <- Av Vl  \(96.2, 78.7 97.6, 92.8 Vl <- Hi Av  \(92.3, 98.6 94.1, 100.0 Av <- Hi Vl  \(93.6, 97.3 96.5, 97.6 Hi <- Av Vl  \(96.2, 94.7 97.6, 96.4  


TABLE IV THE ASSOCIATION RULES FROM COMBINING 5 CLUSTERS FOR CASE 2 Combining 5 clusters All Datasets Sup. \(30 30 Hi <- Ma  \(78.2, 95.1 92.3, 79.2 40.5, 75.0 71.2, 41.7 Ma <- Hi  \(94.9, 78.4 74.4, 98.3 73.5, 41.3 30.4, 98.9 Av <- Ma  \(78.2, 98.4 76.9, 95.0 40.5, 96.0 39.2, 76.7 Ma <- Av  \(97.4, 78.9 93.6, 78.1 96.4, 40.3 72.2, 41.6 Vl <- Ma  \(78.2, 98.4 76.9, 98.3 40.5, 96.8 38.9, 96.6 Ma <- Vl  \(98.7, 77.9 76.9, 98.3 96.7, 40.5 39.2, 95.8 Av <- Hi  \(94.9, 97.3 96.2, 78.7 59.5, 72.5 93.1, 40.4 Hi <- Av  \(97.4, 94.7 92.3, 98.6 73.5, 58.7 43.1, 96.2 Vl <- Hi  \(94.9, 98.6 93.6, 97.3 59.5, 96.7 57.5, 72.2 Hi <- Vl  \(98.7, 94.8 96.2, 94.7 96.4, 59.7 71.2, 58.3 Vl <- Av  \(97.4, 98.7 89.5, 100.0 59.5, 96.7 43.1, 97.7 Av <- Vl  \(98.7, 97.4 94.7, 94.4 96.7, 59.5 57.5, 73.3 Yo <- Av  \(94.7, 100.0 94.7, 94.4 73.5, 96.9 72.2, 58.4 Av <- Yo  \(100.0, 94.7 90.6, 100.0 96.4, 73.9 57.5, 96.6 Yo <- Vl  \(94.7, 100.0 92.9, 97.5 73.5, 98.2 57.5, 96.6 Vl <- Yo  \(100.0, 94.7 96.5, 93.9 96.7, 74.7 93.1, 59.6 Yo <- Hi  \(92.9, 100.0 90.6, 100.0 96.4, 96.6 71.2, 98.2 Hi <- Yo  \(100.0, 92.9 92.9, 97.5 96.7, 96.3 72.2, 96.8 Av <- Ma Hi  \(74.4, 98.3 97.6, 92.8 30.4, 97.8 93.1 75.1 Hi <- Ma Av  \(76.9, 95.0 38.9, 76.5  


to be a well-suited performance measure for MC tasks where the number of negative instances significantly exceeds the number of positive instances [6]. Another advantage of AUPRC is its global nature and independence of a certain threshold value. The closer the AUPRC value is to 1, the better the performance Since these measures are based on the comparison of multi-labels, they depend on a transformation from rankings to classes. As a contrast we also used four wellknown ranking-based performance measures: One-Error OE RL C cision \(AP  for all ranking-based performance measures except AP. OneError evaluates how many times the top-ranked label is not in the set of proper labels of the instance. Ranking Loss is defined as the average fraction of pairs of labels that are ordered incorrectly. Coverage evaluates how far we need, on average, to go down the list of labels in order to cover all the proper labels of the instance. Average Precision evaluates the average fraction of labels ranked above a particular label i ? mt which actually are in mt And finally, the special hierarchical loss function H-loss 2] were utilized. Following [3], normalized costs were calculated: ci := 1/|c\(p\(i i ? L i of all direct children of i. Hierarchical loss \(H-loss consider mistakes made in subtrees of incorrectly predicted labels and penalizes only the first mistake along the path from the root to a node. The smaller the H-loss value, the better the performance B. 20 Newsgroups Dataset We modified the popular single-label dataset 20 Newsgroups [18], [19] by considering eight additional labels corresponding to the intermediate levels of the hierarchy faith, recreation, recreation/sport, recreation/vehicles, politics, computer, computer/hardware, science. This dataset is a collection of almost 20,000 postings from 20 newsgroups sorted by date into training \(60 40 data were preprocessed by discarding all words appearing only in the test documents and all words found in the stop word list [20]. Afterwards, all but the 2%-most-frequent words were eliminated to reduce the dimensionality. Documents were represented using the well-known TF-IDF \(Term 


Frequency  Inverse Document Frequency scheme [19]. The TF-IDF weights were then normalized to the range of [0, 1]. Conversion to TF-IDF and normalization were performed separately for training and test data. This resulted in the 1,070-dimensional dataset with 11,256 training instances, 7,493 test ones and 28 labels To test the performance of two HE algorithms, we first extracted hierarchies from the True Test Multi-Labels \(TTML and calculated the corresponding proximity measures. Both algorithms successfully extracted the original hierarchy We studied the performance of the multi-label classifiers and their ability to infer the class hierarchies in the presence of only partly available hierarchical information. We performed a series of HE experiments with multi-labels having a decreasing number of inserted non-leaf labels describing the levels in the hierarchy. We randomly removed such labels from 20%, 30%, and 40% of the training instances leaving them single-labeled. The results for predicted test multilabels are shown in Table I where the bold face marks the best classifier, and the first column \(left result of HE by Voting and the second \(right Thresholding \(referred as GT Comparing classification performance, one can see that the ART-based networks are superior to both the other classifiers in terms of most performance measures and that ML-FAM slightly outperforms ML-ARAM. Taken together they win on at least 6 and at most 8 out of 9 evaluation measures. BoosTexter has the second best performance, but its predictive power degrades more quickly with the increase in the number of single-label instances. The poorest MC results were shown by ML-kNN, its performance decreased very fast with any reduction in the number of multi-labels For example, F1 decreased by 15% while removing 40% of labels instead of 30%. It is also interesting to note that when trained on the dataset with 40% removed labels, ML-FAM and ML-ARAM significantly outperformed ML-kNN trained on the original dataset with all labels The hierarchy proximity measures confirm the good quality of predictions produced by the ART-based networks: The hierarchies were correct extracted by both HE algorithms of Section II-B even with 40% removed labels. The predictions of ML-kNN were the worst: The Voting variant of the HE 


algorithm could not extract the correct hierarchy with 30 assigning five labels incorrectly to the root label. None of the HE algorithms could extract the correct hierarchy in the absence of 40% multi-labels. With 40% and Voting, the number of labels falsely assigned to the root was 13, while with GT it was only three. For BoosTexter, Voting assigned two labels wrongly to the root label in the experiment with TABLE I 20 NEWSGROUPS ALL, -20%, -30% AND -40% RESULTS Measure all 20% 30% 40%ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN BoosT A 0.635 0.638 0.429 0.549 0.613 0.633 0.383 0.456 0.596 0.619 0.322 0.412 0.563 0.591 0.255 0.387 F1 0.694 0.696 0.565 0.677 0.675 0.688 0.528 0.604 0.662 0.677 0.469 0.566 0.640 0.657 0.392 0.542 F 0.691 0.692 0.480 0.605 0.671 0.688 0.429 0.507 0.658 0.676 0.364 0.465 0.630 0.652 0.296 0.441 OE 0.221 0.220 0.336 0.222 0.259 0.236 0.387 0.275 0.273 0.259 0.415 0.301 0.301 0.291 0.434 0.316 RL 0.100 0.098 0.124 0.073 0.108 0.110 0.128 0.077 0.098 0.103 0.132 0.079 0.101 0.106 0.135 0.082 C 4.188 4.168 6.080 4.164 4.397 4.446 6.184 4.286 4.246 4.340 6.326 4.328 4.334 4.463 6.397 4.379 AP 0.789 0.790 0.677 0.778 0.774 0.782 0.657 0.758 0.769 0.774 0.645 0.747 0.759 0.764 0.638 0.740 AUPRC 0.775 0.772 0.618 0.749 0.743 0.727 0.581 0.691 0.733 0.722 0.555 0.671 0.715 0.708 0.535 0.660 H-loss 0.103 0.123 0.121 0.094 0.102 0.098 0.124 0.108 0.106 0.103 0.132 0.117 0.115 0.111 0.145 0.122 Wins 1 5 0 3 1 6 0 2 2 6 0 1 2 6 0 1 LCAPD 0 0 0 0 0 0 0 0 0 0 0.12 0 0.05 0 0 0 0.51 0.26 0.17 0 CTED 0 0 0 0 0 0 0 0 0 0 0.14 0 0.07 0 0 0 0.50 0.21 0.21 0 TO* 0 0 0 0 0 0 0 0 0 0 0.11 0 0.05 0 0 0 0.39 0.18 0.15 0 30% removed labels and and six labels in the experiment with 40% removed labels. GT resulted in zero distances in the both cases. Assigning more labels to the root creates more shallow and wider hierarchies \(trivial case as stated before The good hierarchy extraction with ART networks demonstrates the system robustness  even with strongly damaged data the system can rebuild the original hierarchy C. RCV1-v2 Dataset The next experiment was based on the tokenized version of 


the RCV1-v2 dataset introduced in [21]. Only the topics label set consisting of 103 labels arranged in a hierarchy of depth four is examined here. Documents of the original training set of 23,149 were converted to TF-IDF weights and normalized Afterwards the set was splitted in 15,000 randomly selected documents as training and the remaining as test samples In this case, the Voting variant of HE applied to the TTML resulted in the LCAPD, CTED and TO* values 0.12, 0.15 and 0.13, respectively. The corresponding values of the GT variant are 0.05, 0.07 and 0.05. The poor performance of the Voting method is due to the fact that for the TTML only very high threshold values succeed in removing enough noise The Voting results are thus dominated by bad hierarchies extracted for all but the highest thresholds The classification and HE results for this dataset are shown in Table II. ML-ARAM has better performance results on this data set in all points than ML-FAM except for RL being the best of all classifiers in terms of the multi-label performance measures. BoosTexter is the best in terms of all ranking measures For both HE algorithms the distances of BoosTexter are the best, those of ML-FAM second, followed ML-ARAM and ML-kNN. All three distance measures correlate. Interesting is also that for ML-kNN the distance values obtained by both HE methods are almost the same The hierarchy extracted by GT from the TTML has much lower distances values as compared with the hierarchies extracted by both methods from predicted multi-labels. This reflects a specific problem of HE, since only a small fraction of the incorrectly classified multi-labels can prevent building of a proper hierarchy. For example, 16.5% of misassigned labels in the extracted hierarchy are responsible for about 80% of LCAPD calculated from the predictions of MLARAM. This large part of the HE error is caused by only 4% of the test data. Under these circumstances the other distances behave analogically. Most labels were not assigned making them trivial edges, but six labels were assigned to a false branch. This can happen when labels have a strong correlation and in the step Hierarchy Construction of the basic algorithm the parent is not unique in the confidence matrix. BoosTexters results suffer less from this problem because it generally sets more labels for each test sample 


Both HE algorithms behaved similarly on the predictions of the ART networks. They constructed a deeper hierarchy than the original one and wrongly assigned the same 11 labels to the root node. The higher distances come from Voting assigning more labels to the wrong branch. For MLkNN both algorithms again create very similar hierarchy trees, both misassigned 28 labels to the root label. For BoosTexter it was seven with Voting and eight with GT Voting produced a deeper hierarchy here D. WIPO-alpha Dataset The WIPO-alpha dataset1 comprises patent documents collected by the World Intellectual Property Organization WIPO ments. Preprocessing was performed as follows: From each document, the title, abstract and claims texts were extracted stop words were removed using the list from [20] and words were stemmed using the Snowball stemmer [22]. All but the 1%-most-frequent stems were removed, the remaining stems were converted to TF-IDF weights and these were normalized to the range of [0, 1]. Again, TF-IDF conversion and normalization were done independently for the training and the test set. The original hierarchy consists, from top to bottom, of 8 sections, 120 classes, 630 subclasses and about 69,000 groups. In our experiment, only records from the sections A \(5802 training and 5169 test samples H \(5703 training and 5926 test samples 1http://www.wipo.int/classifications/ipc/en/ITsupport/Categorization dataset/wipo-alpha-readme.html August 2009 TABLE II RCV1-V2 RESULTS Measure ARAM FAM kNN BoosT A 0.748 0.731 0.651 0.695 F1 0.795 0.777 0.735 0.769 F 0.805 0.787 0.719 0.771 OE 0.077 0.089 0.104 0.063 RL 0.087 0.086 0.026 0.015 C 11.598 11.692 8.563 5.977 AP 0.868 0.860 0.839 0.873 AUPRC 0.830 0.794 0.807 0.838 H-loss 0.068 0.077 0.097 0.081 Wins 4 0 0 5 LCAPD 0.29 0.22 0.25 0.20 0.34 0.34 0.21 0.18 


CTED 0.32 0.23 0.28 0.22 0.38 0.37 0.24 0.20 TO* 0.27 0.18 0.22 0.17 0.31 0.30 0.21 0.17 document in the collection has one so-called main code and any number of secondary codes, where each code describes a group the document belongs to. Both main and secondary codes were used in the experiment, although codes pointing to groups outside of sections A and H were ignored. We also removed groups that did not contain at least 30 training and 30 test records \(and any documents that only belonged to such small groups 7,364 test records with 924 attributes each and a label set of size 131 In this case, the Voting variant of the HE algorithm applied to the TTML resulted in the LCAPD, CTED and TO* values of 0.13, 0.12 and 0, respectively. GT showed the same values. Remarkable are the TO* distances, which are equal to 0. This is due to the fact that the WIPO-alpha hierarchy contains 16 single-child labels that are not partitioned by the true multi-labels: whenever a single-child label j is contained in a multi-label, so is its child, and vice versa. It is therefore theoretically impossible to deduce from the multilabels which of them is the parent of the other. As a result the HE algorithms often choose the wrong parent, resulting in higher LCAPD and CTED values. TO*, as described above is invariant under such choices The results obtained on the WIPO-alpha dataset are shown in Table III. The classification performance of the ART-based networks on this dataset is slightly worse than that of BoosTexter. Mostly in the terms of OE, RL, C, AP, AUPRC, and H-loss measures BoosTexter is better because its rankings are better and it assigned more labels to each sample. But the ART networks have better HE results because their predicted labels are more consistent with the original hierarchy. MLkNN has the worst classification results and distance values again. The reason for the high relative difference between LCAPD as well as CTED and TO* obtained for the ART networks or BoosTexter as compared to the results of the other datasets is because most of the labels were assigned in the right branch but not exactly where they belong Both HE algorithms extracted the same hierarchy from the predictions of ML-ARAM and a very similar hierarchy for ML-FAM. About 5% labels were assigned wrongly to the 


root label in the hierarchies of the ART networks. For MLTABLE III WIPO-ALPHA\(AH Measure ARAM FAM kNN BoosT A 0.588 0.590 0.478 0.564 F1 0.694 0.691 0.614 0.693 F 0.682 0.682 0.593 0.679 OE 0.052 0.057 0.110 0.042 RL 0.135 0.136 0.056 0.025 C 25.135 25.269 22.380 11.742 AP 0.790 0.785 0.724 0.802 AUPRC 0.720 0.684 0.688 0.762 H-loss 0.090 0.093 0.149 0.079 Wins 1 2 0 6 LCAPD 0.16 0.16 0.17 0.17 0.32 0.38 0.21 0.21 CTED 0.18 0.18 0.19 0.19 0.38 0.53 0.27 0.27 TO* 0.05 0.05 0.07 0.07 0.24 0.32 0.08 0.08 kNN both HE methods wrongly assigned about the half of the labels and about 20% of total labels were assgined to the root label. Here, GT extracted a much worse hierarchy as shown by CTED being 0.15 higher for GT than for Voting For BoosTexter both HE methods built the same hierarchy and no label was wrongly assigned to the root. All extracted hierarchies were one level deeper than the original one Although Voting produced worse hierarchies than GT on two previous datasets, this time its distance values were comparable or even better. In comparison to Voting, GT has higher values for all distances on the multi-labels of MLkNN. Voting has the advantage of being a much simpler method and of being more dataset independent. Still the tree distances have the same ranking order for all classifiers for both HE methods VI. CONCLUSION In this paper we studied Hierarchical Multi-label Classification \(HMC tive was to derive hierarchical relationships between output classes from predicted multi-labels automatically. We have developed a data-mining-system based on two recently proposed multi-label extensions of the FAM and ARAM neural networks: ML-FAM and ML-ARAM as well as on a Hierarchy Extraction \(HE algorithm builds association rules from label co-occurrences 


and has two modifications. The presented approach is general enough to be used with any other multi-label classifier or HE algorithm. We have also developed a new tree distance measure for quantitative comparison of hierarchies In extensive experiments made on three text-mining realworld datasets, ML-FAM and ML-ARAM were compared against two state-of-the-art multi-label classifiers: ML-kNN and BoosTexter. The experimental results confirm that the proposed approach is suitable for extracting middle and large-scale class hierarchies from predicted multi-labels. In future work we intend to examine approaches for measuring the quality of hierarchical multi-label classifications REFERENCES 1] M. Ruiz and P. Srinivasan, Hierarchical text categorization using neural networks, Information Retrieval, vol. 5, no. 1, pp. 87118 2002 2] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni, Incremental algorithms for hierarchical classification, The Journal of Machine Learning Research, vol. 7, pp. 3154, 2006 3] , Hierarchical classification: combining Bayes with SVM, in Proceedings of the 23rd international conference on Machine learning ACM New York, NY, USA, 2006, pp. 177184 4] F. Wu, J. Zhang, and V. Honavar, Learning classifiers using hierarchically structured class taxonomies, in Proceedings of the 6th International Symposium on Abstraction, Reformulation And Approximation Springer, 2005, p. 313 5] L. Cai and T. Hofmann, Hierarchical document categorization with support vector machines, in Proceedings of the thirteenth ACM international conference on Information and knowledge management ACM New York, NY, USA, 2004, pp. 7887 6] C. Vens, J. Struyf, L. Schietgat, S. Dz?eroski, and H. Blockeel Decision trees for hierarchical multi-label classification, Machine Learning, vol. 73, no. 2, pp. 185214, 2008 7] E. P. Sapozhnikova, Art-based neural networks for multi-label classification, in IDA, ser. Lecture Notes in Computer Science, N. M Adams, C. Robardet, A. Siebes, and J.-F. Boulicaut, Eds., vol. 5772 Springer, 2009, pp. 167177 8] M. Zhang and Z. Zhou, ML-kNN: A lazy learning approach to multilabel learning, Pattern Recognition, vol. 40, no. 7, pp. 20382048 2007 9] R. Schapire and Y. Singer, BoosTexter: A boosting-based system for text categorization, Machine learning, vol. 39, no. 2, pp. 135168 


2000 10] K. Zhang, A constrained edit distance between unordered labeled trees, Algorithmica, vol. 15, no. 3, pp. 205222, 1996 11] A. Maedche and S. Staab, Measuring similarity between ontologies Lecture notes in computer science, pp. 251263, 2002 12] G. Carpenter, S. Martens, and O. Ogas, Self-organizing information fusion and hierarchical knowledge discovery: a new framework using ARTMAP neural networks, Neural Networks, vol. 18, no. 3, pp. 287 295, 2005 13] A.-H. Tan and H. Pan, Predictive neural networks for gene expression data analysis, Neural Networks, vol. 18, pp. 297306, April 2005 14] G. Carpenter, S. Grossberg, N. Markuzon, J. Reynolds, and D. Rosen Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps, IEEE Transactions on Neural Networks, vol. 3, no. 5, pp. 698713, 1992 15] Y. Freund and R. Schapire, A decision-theoretic generalization of online learning and an application to boosting, Journal of computer and system sciences, vol. 55, no. 1, pp. 119139, 1997 16] K. Zhang and T. Jiang, Some MAX SNP-hard results concerning unordered labeled trees, Information Processing Letters, vol. 49 no. 5, pp. 249254, 1994 17] G. Tsoumakas and I. Vlahavas, Random k-labelsets: An ensemble method for multilabel classification, Lecture Notes in Computer Science, vol. 4701, p. 406, 2007 18] K. Punera, S. Rajan, and J. Ghosh, Automatic construction of nary tree based taxonomies, in Proceedings of IEEE International Conference on Data Mining-Workshops. IEEE Computer Society 2006, pp. 7579 19] T. Joachims, A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization, in Proceedings of the Fourteenth International Conference on Machine Learning. Morgan Kaufmann Publishers Inc. San Francisco, CA, USA, 1997, pp. 143151 20] A. McCallum, Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering, 1996 http://www.cs.cmu.edu/ mccallum/bow 21] D. Lewis, Y. Yang, T. Rose, and F. Li, RCV1: A new benchmark collection for text categorization research, The Journal of Machine Learning Research, vol. 5, pp. 361397, 2004 22] M. Porter, Snowball: A language for stemming algorithms, 2001 http://snowball.tartarus.org/texts/introduction.html 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


