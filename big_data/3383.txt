 Sequential Pattern Mining with Ina ccurate Event in Temporal Sequence   Jiadong Ren College of Information Science and Engineering, Yanshan University Qinhuangdao, P.R.China, 066004 jdren@ysu.edu.cn  Haiyan Tian College of Information Science and Engineering, Yanshan University Qinhuangdao, P.R.China, 066004 haiyan7155012@sohu.com    Abstract   In true-life, the existence of many events which are 
occurred in the interval may cause uncertainty in events ordering. The inaccurate event has been introduced for sequential pattern mining to improve accuracy of computing support threshold. In this paper we store a sequence in the chain table. Sequence with inaccurate event can be expressed  expediently  Besides precise support is introduced to evaluate the probability of a patte rn contained in a sequence. And  the probabilistic ordering is employed to handle 
overlapping events. In essence, pr obabilities of generated candidate pattern contained in an inaccurate sequence are co mputed. The maximal value in probabilities is selected as precise support of the sequence. The sum of all in accurate sequence precise supports in the database is computed. If the ratio value between the sum and the length of database is not less than predefined minimum threshold, the generated 
candidate pattern is called frequent pattern. So some infrequent patterns might be turned to be frequent and some interesting patterns could not be missed Performance analysis shows the accuracy of discovering  frequent patterns is improved     This work is supported by the Natural Science Foundation of Hebei Province P.R.China, NO. F2008000888 
 Keyword inaccurate event, temporal sequence sequential pattern  1. Introduction  The problem of sequential pattern mining was first developed by Agrawal and Srikant 1 Since then mining works had been done in sequential pattern discovery 2 The time  attribute was introduced for 
the transaction by Srikant and Agrawal 4 To relax the rigid definition of supermarket transactions, they dynamically combined transactions happened inside a marginal interval \(ignore their orders\ However, the order of such events may not be ignored in many other domains. The frequent episodes were discovered from telecommunication event sequences  by Mannila et al 
2  But they only considered events with an accurate timestamp. Subsequently, the uncertainty was introduced into pattern discovery in temporal event sequences by Maria E.Orlowsk et.al 5 The uncertainty model was defined and an algorithm was presented to discover interesting pattern in the sequence database with inaccurate event. But they only considered one 
type of inaccurate event Recently, study of uncertain temporal information also has evolved greatly  Efficient methods were presented by P.Papaterou et al 6 to find frequent arrangements of temporal intervals using both breadth 
Fourth International Conference on Networked Computing and Advanced Information Management 978-0-7695-3322-3/08 $25.00 © 2008 IEEE DOI 10.1109/NCM.2008.93 659 


 first and depth first search techniques. A new algorithm ARMADA also was presented to discover frequent temporal patterns and to generate richer interval-based temporal association rules by E. Winarko et al 7  Subsequently, a maximum gap time constraint was introduced to get rid of insignificant patterns and rules so that the number of generated patterns and rules can be reduced by E. Winarko and J.F. Roddick 8 In addition, a unifying view of temporal concepts and data models to distinguish time point-based methods and interval-based methods was proposed by F Morchen 9  Traditional sequential mining approaches 1,2,4 are based on support model. They only work for accurate events. So the uncertainty of the events order is not considered. If this kind of uncertainty is ignored, some frequent patterns might be missed in some application fields. In this paper, the structure which employs chain table is developed to represent the data which contain inaccurate events expediently  And to improve accuracy of computing inaccurate sequence support threshold, precise support is employed to evaluate the significance of a pattern  The rest of the paper is organized as follows: the section 2 illustrates design and construction of chain table. Section 3 discusses categories and probability computation of inaccurate events. Section 4 describes the algorithm of handling inaccurate events. Section 5 shows the performance analysis. Finally, a conclusion is given in Section 6  2. Design and construction of chain table  Each event in a sequence is a node in the chain table Name and occurred time of event are recorded in the node. Nodes are linked according to time sequence Node of inaccurate event is recorded with start time and end time. They are inserted into chain table by start time. And the node with end time of inaccurate event also is inserted, therefore the end position of inaccurate event can be found So a chain table is defined as follows  It consists of nodes  A node in the chain table has two fields: item and link: item records the name and occurred time of the event. link links to the next item in the chain table The nodes are linked in the chain table by time The node which contains event \(e,ts,te\s the node of inaccurate event, where [ts en ts occu rrin g  interval of inaccurate event e Based on the definition, construction algorithm of the chain table is as follows S is a sequence with inaccurate events. L is a pointer Algorithm: the chain table construction Input: sequence database with inaccurate events Output: chain table CT Method 1. Create header node: the link is labeled as “null 2. For each item i S 3.  while\(L!=null 4   If \(find item i’s position\ then   5    Create the node of item i 6.       Item i is inserted 7       L points the first item of chain table 8.         i 9.   Else  L 10. end  Sequences with inaccurate events are given as follows  a             b      e    c S 1 t1    t3    t2     t3    t4    t a  S 1 with an inaccurate event b Chain table of S 1 Figure 1 S 1 with an inaccurate event and its chain table b         c         a     e   f   g S 2 t1  t4  t2  t6  t3   t4   t5  t6  t a  S 2 with overlapping intervals  a:t 1 e:t3 t3 b:t 2 e t 3 c t 4 
660 


  b  Chain table of S 2  Figure 2 S 2 with overlapping intervals and its chain table Obviously, interval [t6, t4 v erlap p e d in terv al o f  inaccurate events e and g   3. Categories and probability computation of inaccurate events  3.1. Categories of inaccurate events  The events which are occurred in intervals have four possibilities  The interval of inaccurate event is contained in two certain neighborhood timestamps, then the order of events is certain, the computation is not needful The timestamp of certain event is contained in the interval of inaccurate event, then the uncertainty of the order between events is generated. When intervals of inaccurate events are not overlapping, sequence is segmented several sections and probabilities of possible order are computed When intervals of inaccurate events are overlapping  probabilities of possible order are computed according to probabilistic ordering  Precise support of generated candidate pattern is computed in the whole database, then compared with predefined minimum support threshold \(denoted as s if the precise support is not less than s, the potential frequent pattern is discovered Given an accurate event \(e i t i d an inaccurate event \(e j t js t je he order between them is uncertain iff t i t js t je erwise, it is certain. Sequence S is called a flexible-order sequence, iff at least an inaccurate event \(e,t s t e contained in sequence S. Otherwise, S is called a fix-order sequence. A symbol i s  u s ed i n  the sequence to delimit uncertain intervals definitely Given S=S 1 S 2 S m and P=p 1 p 2 p m where the length of S i equals to the length of p i P  stands for sequential pattern \he precise support of P in S is defined as  P_sup\(P,S ii 1i m s\(P S  3-1 The range of precise support is  P_sup\(P,S\>0, we call S contains P  If there are several support values which generated candidate pattern in a sequence, we select the maximal value as the sequence precise support  MP_sup\(P,S\{P_sup\(P,S i _sup\(P,S j 3-2 where 1 i<j m  3.2. The probability computation of non overlapped intervals   In figure 1, the sequence S 1 has the uncertain interval \(t3 t3\flexible-order sequence Chain table of S 1 b,t2\\(e t3 t3 c,t4  als o repr es en ted as c 1 c 2 c 3 where c 1 a,t1 2 e,t3 t3 c 3 c,t4\. Assume ts= t3 and te=t3. Because event b occurs in the interval of inaccurate event e  becomes a discrete random variable x. P\(x\ is the probability of possible pattern. d\(e,t\ the density function of the inaccurate event. The probability of p =<b,e,c> equals to one of e happened in the interval between b and c Ta bl e 1  The  probability of possible pattern x <e,b,c b,e,c P\(x t2 t3s d\(e,t dt  t3e t2 d\(e,t dt  Given a sequence S, it can be segmented as s 1 b,t2\>, s 2 c,t3\\(e,t4,t4 3 d,t5 s 4 g,t7,t7 S a  b       c     e  d         f    g t1 t2  t4  t3  t4   t5  t7  t6   t7  t Figure 3  Inaccurate sequence of non overlapped intervals Given a pattern P=<b,a,e,c,d,g,f>,it can be segmented as P 1 P 2 P 3 P 4 b,a>.<e,c>.<d>. <g,f>, P_sup\(P,S\0  b t1 e t 4 t4 c t 2 g:t6,t6 a t 3 e:t4 f:t5 g:t6 
661 


 because P_sup\(P 1 s 1 0. If the pattern P’=<a,b,e,c,d,g f>, P_sup\(P,S t3 t6 t4 t7 d\(e,t\dt d\(g,t\dt  because P_sup\(P 1 s 1   Assume two inaccurate events e are contained in S S’ a  b        c    e  d         f     e t1 t2  t4  t3  t4  t5  t7   t6  t7  t Figure 4  Inaccurate sequence with two inaccurate events e  Given a pattern P=<c,e>, S 1 c,t3\\(e,t4 t4   and S 2 e, t7, t7\ntain pattern P, the precise support of P in S’ is Max{P_sup\(P,S 1  P_sup\(P,S 2  t4 t3 d\(e,t\dt 1} =1 In the fix-order sequence S, the sequence segment of S S, is a pattern. Based on above depiction P_sup\(P,S\ if P is contained in S. Otherwise P_sup\(P,S\ identical to the traditional support 3.3. The probability computation of overlapped intervals  Uncertain instant of inaccurate event occurring can be represented by a certain interval, so temporal relation of inaccurate event occurring can be translated into relation of certain intervals  Definition  Probabilistic ordering  Let any two inaccurate events e and g uncertain intervals be s1 d s2, e2 P r o b ab ilistic ordering of e and g is PO_R  ijij PO _ R  P P i   P  j     3-3  PO_R is used to compute the possibility of occurring before In this paper, PO_R is called assuring threshold, PO_R is a real number in s  a rule, the more the value of PO_R is bigger\(approaching 1\he more occurs before   and P_sup\(P,S\=PO_R Definition assuring threshold of simultaneous relation When inaccurate event e with the short interval is contained in inaccurate event g with the long interval we call it as simultaneous relation Let any two inaccurate events e and g uncertain instants be s1 s2 A s s u m e  th at and meet simultaneous relation and compute assuring threshold S_R ijij S_R P P  i   P  j     3-4 S_R is called assuring threshold, S_R is a real number in s a ru le, t h e  m o re th e v a l u e of  S_R is  bigger\(approaching 1\, the more the possibility of simultaneous relation between and is bigger. And their overlapping uncertain instant   s h ere s 3  m a x s 1,s 2  in e1,e2  Considered chain table of sequence shown in figure 2, the precise support threshold is computed as follow Given any two inaccurate events \(e,t1\d \(g,t2 Case When t1 t4,t2], P 1 b,e,c>, P\(e t2 t4 d\(e,t\dt there are three possibilities for g 1\en a pattern P=<b,e,c,g,a,f>,P 2 g,a,f t2 t6 g   t3 t6 d\(g,t\dt then p_sup\(P,C P\(e\P\(g 2\en a pattern P=<b,e,c,a,g,f>, P 2 a,g,f t2   g   t5 t3 d\(g,t\dt then P_sup\(P,C P\(e\P\(g 3\en a pattern P=<b,e,c,a,f,g>, P 2 a,f,g t2    P  g   t6 t5 d\(g,t\dt then P_sup\(P,C P\(e\P\(g Case When t1 t2,t6 1 b,c,e>, P\(e t6 t2 d\(e,t\dt there are three possibilities for g 1\en P 2 g,a,f>, t2 t6  g   t3 t6 d\(g,t\dt then P_sup\(P,C\= P\(e\* P\(g 2\en P 2 a,g,f>, t2   g   t5 t3 d\(g,t\dt  then P_sup\(P,C\= P\(e\* P\(g 3\en P 2 a,f,g>, t2   g   t6 t5 d\(g,t\dt  then P_sup\(P,C\= P\(e\* P\(g Case When t1 t6,t3], P 1 b,c,e>, there are three possibilities for g 1\en t2 t6 b,c,e,g a,f th e   
662 


 probability of e before g is computed according to 3-3\en P_sup\(P,C\=PO_R e.g  2\en t2  P 2 a,g,f>, P\(e t3 t6 d\(e,t\dt  P\(g t5 t3 d\(g,t\dt then P_sup\(P,C\ = P\(e\ * P\(g 3\en t2  P 2 a,f,g>, P\(e t3 t6 d\(e,t\dt  P\(g t6 t5 d\(g,t\dt then P_sup\(P,C\ = P\(e\ * P\(g Case When t1   1\en P=<b,c,g,a,e,f>, t2 t6 e  t4 t3 d\(e,t\dt P\(g t3 t6 d\(g,t\dt then P_sup\(P,C P\(e\ * P\(g 2\en P=<b,c,a,e,g,f>, t2 t3,t4 e p r o b a b ility  of e before g is computed according to \(3-3\en P_sup\(P,C\=PO_R e.g  3\en P=<b,c,a,e,g,f>, t2   P  e   t4 t3 d\(e,t\dt P\(g t5 t4 d\(g,t\dt then P_sup\(P,C\= P\(e P\(g 4\en P=<b,c,a,e,f,g>, t2   P  e   t4 t3 d\(e,t\dt P\(g t6 t5 d\(g,t\dt then P_sup\(P,C\= P\(e P\(g Case Assume that events e and g meet simultaneous relation. If P=<b,c,e,a,f> or P=<b,c,g,a,f t6 g to \(34  P _ s u p\(P  C  S_R e.g  If P=<b,c,a, e,f> or P=<b,c,a,g,f   according to \(3-4\_sup\(P,C\=S_R e.g  Given a pattern P and a sequence database D, the precise support of P in D is defined as     P_sup\(P,D  1i N i P_sup\(P,S N 3-5 where N=|D Given a predefined minimum support threshold s, if P_sup\(P,D  s, P is a frequent pattern  4. The sequential pattern mining algorithm with inaccurate events \(IE-SPM  The algorithm is an improvement upon U_Apriori in referen accu rate ev e n ts  w i th  n onov erlapped and overlapped intervals are considered Algorithm: IE-SPM Input: a database D with chain table, e and f types events with density functions d\(e,t\ and d\(g,t\,and a threshold s Output: pattern whose precise support is not less than s Method 1.F 1 frequent 1-patterns 2  If \( e and g is overlapping 3.    Pre-compute intervals of overlapping consider two inaccurate events and overlapping intervals 4.  Else  Pre-compute intervals 5. For \(k =2;F k-1  k 6  Generate candidate pattern set C k from F k-1  7.  for sequence S i D do 8.    for P j C k do 9.    P j P_sup+= Max_match \(P j S i  10. F k P j P j C k  P j P_sup/ |D s 11. Frequent pattern set k k F  12. end When intervals are not overlapping, we represent the sequence the same as refere h en in ter v als are overlapping, we add [s_j, e_j to repres en t t h e m  w h ere s_j is the index of the first frequent event in overlapped intervals and e_j  is index of overlapped intervals position. For each sequence S, an overlapped intervals list o_ intervals is used to represent intervals of overlapping. After pre-computing, a sequence is represented as a triplet [sid, S,o_interva   5. Performance analysis  We analyze the performance of algorithm by an exam ple  a           b     e     c S 1 t1   t3   t2    t3     t4   t a             e    b    c S 3 t1     t2   t2    t3   t4  t a      e    b     c S 4 t1     t2    t3    t4 Figure 5  A mining dataset  A mining dataset with 3 sequences is shown in  
663 


 Figure 5. Let the minimum support count be 3 Considering a sequential pattern P = <e, b, c>, if we ignore the uncertainty, only S 3 and S 4 contain P. The support count of P  is 2, which is less than 3. So P  is infrequent pattern and will not be in the mining result However, besides S 3 and S 4 S 1 may also have some probabilities to contain P. If these probabilities are considered and the number of S 1 is two, P  could be frequent. Obviously, if we ignore the uncertainty in sequential pattern mining, the potential frequent pattern may be missed. Especially, when the number of inaccurate sequences contained in the database is large then large numbers of potential interesting patterns may be missed. Therefore, by algorithm IE-SPM, the accuracy of discovering frequent patterns can be improved But we can discover algorithm IE-SPM to be much more complex than traditional sequential mining algorithm. So the runnin g time may be increased Especially the more the predefined minimum support is lower, the more the potential patterns are handled, so the running time increases. Owing to considering  probabilities of inaccurate events order, some interesting patterns can not be missed. So the computation of discovering frequent patterns is more accurate, however, at the cost of increasing the running time  6. Conclusions  In this paper, by employing storage structure of chain table, sequence which contains inaccurate events can be stored  expediently. Compared with previous method of sequential pattern mining with uncertainty two different inaccurate events are considered. The precise support is introduced to evaluate the probability of inaccurate events order. The probabilistic ordering is employed to compute sequence precise support of two overlapped intervals, and simultaneous relation between two inaccurate events is considered to evaluate possible simultaneous events. By an improved algorithm IE-SPM, the potential frequent pattern can be discovered, so some interesting patterns can not be missed. Performance analysis shows that computation of discovering frequent patterns is more accurate by considering uncertainty. But the running time is increased  7. References  1 R A g ra wa l a nd R  S r ik a n t M ini n g se que n tia l pa tte rns   In Proc.11thICDE 1995, pp.3-14  2 H  Ma nn ila H  T o iv one n, a nd A  I   V e r k am o Discovery of frequent episodes in event sequences Data Mining and Knowledge Discovery 1997, pp. 259 -289 3 J  Y a ng W  W a ng P  S  Y u  a nd J  H a n M i n i n g long  sequential patterns in a noisy environment In SIGMOD Conference 2002, pp.406-417 4 R.Srik a n t a n d R.A g ra wa l M ini n g se que ntia l pa tte r n s  Generalizations and performance improvements In Proc 5thEDBT 1996, pp. 3-17 5  Xing z h i S u n  Ma r i a E O r low s ka a nd X u e L i   Introducing Uncertainty into Pattern Discovery in Temporal Event Sequences Proceedings of the Third IEEE International Conference on Data Mining Florida, USA 2003, pp. 299-306 6  P   P a pa te ro u, G  Kollios S. S c la rof f  a nd D. G uno po ulos   Discovering frequent arrangements of temporal intervals In Proc. ICDM 2005,pp.354-361 7 E W i n a r ko an d J F  R o dd i c k Di s co ver i n g R i ch er  Temporal Association Rules from Interval-based Data Proc of the 7th International Conference on Data Warehousing and Knowledge Discovery Copenhagen, Denmark, 2005 pp.315-325  E  W i n a rko and J  F  R o dd i c k  A r m ad a - an al go ri t h m  for discovering richer relative temporal association rules from interval-based data Data & Knowledge Engineering  Amsterdam, Netherlands, 2007, pp. 76-90 9 F   Mor c he n   U ns upe r v is e d pa tte r n m i ning f r om  symbolic temporal data ACM SIGKDD Explorations Newsletter ACM, New York, USA, 2007, pp.41-55  
664 


7 5769 l00 97.67 l005 I00 Avg Accuracy ______ __95.59 95.98 89.5 8954 BSTC/RCBT To keep comparisons fair we ran SVM and randomForest on the same genes selected by our entropy discretization except with their original undiscretized gene expression values SVM was run with its default radial kernel We ran randomForest 10 times with its default 500 trees for ALL LC and OC and its accuracy was constant For PC we had to increase randomForest's number of trees to 1000 before its accuracy stabilized over the 10 runs Table III contains the number of class 0/1 samples in the clinically determined training set the number of genes selected by our entropy discretization and our experimental results As shown in this table the overall average accuracies of BSTC and RCBT are again best at about 96 each When compared against RCBT SVM and randomForest on the individual tests we can see that BSTC is alone in having 100 accuracy on the majority of datasets However BSTC's performance on the preliminary AML/ALL dataset test is relatively poor This is likely due to over fitting Every error BSTC made mistook a class 0 AML test sample for a class 1 ALL test sample i.e all errors were made in this same direction And the ALL training data has both i about 2.5 times as many class 1 samples as class 0 samples and ii a small number of total samples/genes When the training set is more balanced and the number of samples/genes is larger we can expect that cancellation of errors will tend to neutralize/balance any over fitting effects in BSTC And BSTC is a method meant primarily for large training sets where CAR-mining is prohibitively expensive As we will see below in Section V-B.1 BSTC s performance is much better for larger AML/ALL training set sizes B Holdout Validation Studies Holdout validation studies make comparisons less susceptible to the choice of a single training dataset and provide performance evaluations that are likely to better represent program behavior in practice We next present results from a thorough holdout validation study completed using 100 different training/test sets from each of the ALL LC PC and OC data sets For these holdout validation tests we benchmark BSTC against Top-k/RCBT because i BSTC/RCBT perform hest in our preliminary experiments ii Top-k/RCBT is the fastest/most accurate CAR-based classifier for microarray data and iii we are interested in BSTC's CAR-related vs Topk/RCBT s CAR based scalability For the holdout validation study we generated training sets of sizes 40 60 and 80 of the total samples Each training set was produced by randomly selecting samples from the original combined dataset We then used the standard R dprep package's entropy minimized partition 17 to discretize the selected training samples Finally the remaining dataset samples were used for testing the two classifiers after rule/BST generation on the randomly selected training data For each training set size we produced 25 independent tests In addition to these training sets we created an additional 25 1-x/0-y tests To create these tests we chose training data by randomly selecting x class 1 samples and y class 0 samples to be used as training data As before the remaining samples were then used to test both classifiers For each dataset the x and y values are chosen so that the resulting 25 classification tests have the exact same training/test data proportions as the single related dataset test reported in section V-A For each training set size we plot our results using a boxplot Boxplot Interpretation Each boxplot that we show in this section can be interpreted as follows The median of the measurements is shown as a diamond and a box with boundaries is drawn at the first and the third quartile The range between these two quartiles is called the inter-quartile range IQR Vertical lines a.k.a whiskers are drawn from the box to indicate the minimum and the maximum value unless outliers are present If outliers are presents the whiskers only extend to 1 5 x IRQ The outliers that are near i.e within 3 x IRQ are drawn as an empty circle and further outliers are drawn using an asterisk 1 ALIJAML ALL Experiment Figure 4 shows the classification accuracy for the ALL/AML dataset As can be seen in this figure BSTC and RCBT have similar accuracy across the ALL/AML tests as a whole BSTC outperforms RCBT in terms of median and mean accuracy on the 40 and 80 training set sizes while RCBT has better median/mean accuracy on the 1-27/0-11 training size tests And both classifiers have the same median on the 60 training set size Over the 100 ALL/AML tests we see that BSTC has a mean accuracy of 92.13 while RCBT has a mean accuracy of 91.39 they are very close It's noteworthy that BSTC is 100 accurate on the majority of 80 training size tests However BSTC appears to have slightly higher variance than RCBT on all but the 40 training tests Considering all the results together both BSTC and RCBT have essentially equivalent classification accuracies on the ALL/AML dataset 2 Lung Cancer LC Experiment The results for the Lung Cancer dataset are reported in Figure 5 Here again both BSTC and RCBT have similar classification behavior RCBT has higher mean and median accuracies on the 40 and 60 tests while BSTC outperforms RCBT on the 1-16/0-16 tests Meanwhile both classifier have the same median on the 80 training test Over all 100 LC tests we find that BSTC has a mean accuracy of 96 32 while RCBT has a mean accuracy of 97.08 again they are very close As before BSTC is alone in having 100 accuracy more 1068 TABLE III USINC GIVEN RES Ul TS TRAINING DATA  Class I  Class 0 Genes random Training Training After BSTC RCBT SVM Forest Dataset Samnples Samnples Discr Ac or Accr Ac o Accuracy ALL 27 11 866 82.35 91 18 91 18 85.29 LC 16 16 2173 100 97.99 93.29 99.33 PC 52 50 1554 100 97.06 73.53 73.53 OC 133 


TABLE IV AVERACGE RUN TIMLES FOR THE PC TESTS IN SEC-NI t INDICATES nl WAS LOWERED TO 2 Training Median  Mean 260 Near outliers  25/25t T O Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 80 Training 1 27/0-11 Training 1.0 1.0 1 0]0.95 0.9 0.9 0.7 0.7 0.850.8 0.80.80.80.750.7 0.70.70.7 650.6 0.60.60.6Holdout Validation Results 25/25t cJ CZ C O Far outliers 40 Training 60 Training 80 Training 1-16/0-16 Training h0 1.0 T 1.0 1.00.95 0.9 0.9 0 0.8 0.8 0.80.80.8LC Holdout Validation Results THE PC TESTS THAT RCBT FINISHED Training BSTC RCBT 40U 75.08 79.27 60 78.18 85.45 80 84.98 1-52/0 50 81.65%o 1069 cJ CZ C 5.06 120.63 21.32 RCBT  7110  7200  7200  RCBT DNF 0 P 00 24/25 t per training set test Finally the  RCBT DNF column gives the number of tests RCBT was unable to finish in  the cutoff time over the number of tests for which Top-K finished mining rule group upper bounds Explanation for varying nd values Run time cutoffs were necessary to mitigate excessive holdout validation CARmining times Even with a cutoff of 2 hours these 100 PC experiments required about 11 days of computation time with most experiments not finishing For the 80 and 1-52/0-50 training set sizes RCBT with nl  20 failed to finish lower bound rule mining for all 50 tests within 2 hours Thus RCBT's nl parameter was lowered from the default value of 20 to 2 in an attempt to improve its chances of completing tests Not surprisingly decreasing nl i.e mining fewer lower bound rules per Top-k rule group decreases RCBT s runtime However RCBT was still unable to finish lower bound rule mining for any tests Classification Accuracy Figure 6 contains accuracy results for BSTC on all four Prostate Cancer test sets Prostate Cancer boxplots for RCBT weren't constructed for training set sizes that RCBT was unable to complete all 25 tests within the time cutoffs In contrast BSTC was able to complete each of the 100 PC classification tests in less than 6 seconds Table V contains mean accuracies for the PC dataset with 40 60 80 and 1-52/0-50 training For each training set the average accuracies were taken over the tests RCBT was able to complete within the cutoff time Hence the 40 row means were taken over all 25 results Since RCBT was unable to complete any 80 or 1-52/0-50 training size tests we report these BSTC means over all 25 tests RCBT has slightly better accuracy then BSTC on 40 training For 60 training TABLE V MEAN AcCURACIES FOR 40 60 80 1.52/0.50 BSTC 3 4.93 5.78 5.57 Top0.09 BSTC F a RCBT BSTC RCBT BSTC RCBT b c Fig 5 BSTC RCBT BSTC RCBT b c ALL BSTC RCBT a Fig 4 BSTC RCBT d then half the time for any training set size see Figure 5 d However RCBT has smaller variance for 3 of the 4 training set sizes Therefore as for the ALL/AML data set both BSTC and RCBT have about the same classification accuracy on LC 3 Prostate Cancer PC Experiment RCBT begins to run ilnto a comiputational difficulties on PC's larger training set sizes This is because before using a Top-k rule group for classification RCBT must first mine nt lower bound rules for the rule group RCBT accomplishes rule group lower bound mining via a pruned breadth-first search on the subset space of the rule group's upper bound antecedent genes This breadthfirst search can be quite time consuming In the case of the Prostate Cancer PC dataset all 100 classification tests 25 tests for each of the 4 training set sizes generated at least one top10 rule group upper bound with more than 400 antecedent genes Due to the difficulties involved with a breadth-first search over the subset space of a several hundred element set RCBT began suffering from long run times on many PC classification tests Table IV contains four average classification test run times in seconds for each PC training size The BSTC column run times reflect the average time required to build both class 0 and class I BSTs and then use them to classify all the test samples Each Top-k column run time is the average time required for Top-k to mine the top 10 covering rule groups with minimum support 0.7 for each training set Table IV's RCBT column gives average run times for RCBT using a time cutoff value of 2 hours for all the training sets For each classification test if RCBT was unable to complete the test in less than the cutoff time it was terminated and it s run time was reported as the cutoff time Hence the BSTC RCBT d RCBT column gives lower bounds on RCBT s average run time 


TESTS IN SECOND t INDICATES nl WAS LOWERED TO 2 Training BSTC Top-k RCBT 7 OC Holdout Validation Results RCBT outperforms BSTC on the single test it could finish by more then 7 although it should be kept in mind that RCBT's results for the 24 unfinished tests could vary widely Note that BSTC's mean accuracy increases monotonically with training set size as expected At 60 training BSTC's accuracy behaves almost identically to RCBT's 40 training accuracy see Figure 6 4 Ovarian Cancer OC Experiment For the Ovarian Cancer dataset which is the largest dataset in this collection the Top-k mining method that is used by RCBT also runs into long computational times Although Top-k is an exceptiounally fast CAR group upper bound miner it still depends on performing a pruned exponential search over the training sample subset space Thus as the number of training samples increases Top-k quickly becomes computationally challenging to tune/use Table VI contains four average classification test run times in seconds for each Ovarian Cancer\(OC training size As before the second column run times each give the average time required to build both class 0/1 BSTs and then use them to classify all test's samples with BSTC Note that BSTC was able to complete each OC classification test in about 1 minute In contrast RCBT again failed to complete processing most classification tests within 2 hours Table VI's third column gives the average times required for Top-k to mine the top 10 covering rule groups upper bouhnds for each training set test with the same 2 hour cutoff procedure as used for PC testing The fourth column gives the average run times of RCBT on the tests for which Topk finished mining rules also with a 2 hour cutoff Finally the  RCBT DNF column gives the number of tests that RCBT was unable to finish classifying in  2 hours each THE OC TESTS THAT RCBT FINISHED Training BSTC RCBT 40 92.05 97.66 60 95.75 96.73 80 94 12 98.04 1-133/077 9380 96.12 1070 cJ CZ C 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 BSTC RCBT d Median Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 0.90.80.70.6BSTC RCBT a 80 Training 1-52/0-50 Training 0.9DNFI 0.80.70.6BSTC RCBT b 1 u0.9DNFI 0.80.70.6BSTC RCBT  RCBT DNF 40 30.89 0.6186 273.37 0/25 60 61.28 41.21  5554.37 19/25 80 71.84  1421.80  7205.43 t 21/22 TIMES FOR THE OC 9 Mean 0 Near outliers  Far outliers 1.01 11 01 1.0 d Fig 6 PC Holdout Validation Results BSTC RCBT a Fig 0.80.8 0.8BSTC RCBT BSTC RCBT b c c i DNF cJ CZ C 40 Training 60 Training 80 Training 1-133/0-77 Training 0.95 DNF DNF DNF 0.9 0.90.90.90.85 0.8 BSTC RCBT TABLE VI AVERAGE RUN 1 133/0-77 70.38  1045.65  6362.86 t 20/23 over the number of tests for which Top-k finished Because RCBT couldn't finish any 80 or 1-133/0-77 tests within 2 hours with nl  20 we lowered nl to 2 Classification Accuracy Figure 7 contains boxplots for BSTC on all four OC classification test sets Boxplots were not generated for RCBT with 60 80 or 1-133/0-77 training since it was unable to finish all 25 tests for all these training set sizes in  2 hours each Table VII lists the mean accuracies of BSTC and RCBT over the tests on which RCBT was able to produce results Hence Table VII's 40 row consists of averages over 25 results Meanwhile Table VII's 60 row results are from 6 tests 80 contains a single test's result and 1-133/0-77 results from 3 tests RCBT has better mean accuracy on the 40 training size but the results are closer on the remaining sizes   4 difference over RCBT's completed tests Again RCBT's accuracy could vary widely on its uncompleted tests CAR Mining Parameter Tuning and Scalability We attempted to run Top-k to completion on the 3 OC 80 training and 2 OC 1-133/0-77 training tests However it could not finish mining rules within the 2 hour cutoff Top-k finished two of the three 80 training tests in 775 min 43.6 sec and 185 min 3.3 sec However the third test ran for over 16,000 mnm  11 days without finishing Likewise Top-k finished one of the two 1-133/0-77 tests in 126 min 45.2 sec but couldn't finish the other in 16,000 min  11 days After increasing Top-k's support cutoff from 0.7 to 0.9 it was able to finish the two unfinished 80 and 1-133/0-77 training tests in 5 min 13.8 sec and 35 min 36.9 sec respectively However RCBT with nl 2 then wasn't able to finish lower bound rule mining for either of these two tests within 1,500 min Clearly CAR-mining and parameter tuning on large training sets is TABLE VII MEAN AcCU1ACIES FOR 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


