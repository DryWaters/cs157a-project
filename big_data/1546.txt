The Applied Research of Kalman in the Dynamic Travel Time Prediction Huifeng Ji  Aigong Xu, Xin Sui, Lanyong Li School of Geomatics, Liaoning Technical University Fuxin, Liaoning, China jhf_sy@126.com  Abstract The dynamic travel time prediction is important contents of The intelligent Transportation System. Dynamic travel time is updating the travel time by the prediction model on the same path or segment of the journey. Different forecasting models are corresponded to different methods, and different methods are corresponded to different prediction accuracy. Contrast to the existing methods, such as historical trends method, non-parametric regression, time series, neural networks, travel time prediction method, The Kalman filtering is best in dynamic information 
forecasts. Due to the unpredictability of traffic Factor impacting the travel time, the change of dynamic travel time have not strict laws. while the Kalman filter can full use of travel time variation to reflect changes. The all parameters of the kalman filter are carefully analyzed in this float car test, the adjacent speed of the float car is selected as the state vector, the observation vector is obtained by converting coordinates and observation time because the speed of floating cars is instantaneous speed whose error is big and other parameters are introduced in the article. Then the method of establishing the Kalman filter equation is proposed by the long interval GPS datum in the urban. For the continuous driving vehicles, the rear travel time can be constantly updated when the observation value of the travel time is obtain in certain 
segments due to continuation of time and space. Then the updated travel time will be closer to the real value. So the parameter regression smoothing approach is proposed based on the result of kalman. For Verifying accuracy of Kalman filter, three periods are selected for time of the GPS data. From 5 error targets that included the mean relative error, the mean absolute relative error the maximum absolute relative error, the equal coefficients and the mean square root of the relative error squared, the error of the kalman filter is researched. So the results are shown that the maximum value is the maximum absolute relative error of the 14:15-14:30 time period, but its value is also only 0.015. It is shown that the forecast model has highly predicting accuracy in all periods. To further prove this conclusion, the all time periods are 
also studied. It can be seen from the all periods that the predicted result are lower than the statistics travel time. The main reason is excluding the data that the speed is zero in the raw data, and it is always assumed that the vehicle is running when the speed of the float car is calculated. Therefore the results of Kalman filter need to be carried out the necessary smoothing process. As the known data of three segments is used in the second improved model, the data processing should be started form the fourth segment. The predicting result shows that the accuracy of improved model is higher than the kalman Keywords- Kalman filter; Kalman filter; ITS; GPS datum dynamic travel time prediction; observation white noise  I   I NTRODUCTION  Dynamic travel time is updating the travel time by the 
prediction model on the same path or segment of the journey Different forecasting models are corresponded to different methods, and different methods are corresponded to different prediction accuracy 1  Contrasted to the existing methods, such as historical trends method, non-parametric regression, time series and neural network 2 the Kalman filtering is best in dynamic information forecasts. Due to the unpredictability of traffic Factor impacting the travel time, the change of dynamic travel time have not strict laws. While the Kalman filter can full use of travel time variation to reflect changes II  D ISCRETE TIME K ALMAN FILTER  Rudolf Emil Kalman who is mathematician of United States 
proposed Kalman filter. The Kalman filter was extracted from the signals measured through the observation method to estimate the required signal and can handle one-dimensional or non-stationary or multi-dimensional random process. The data storage capacity of kalman filter is also small. Because of the advantages of Kalman filter, the theory has been applied in many engineering fields after it was proposed. The theory of Kalman filter is introduced to forecast travel time in the article, and the improved algorithm is established. Kalman filtering was divided into two kinds that the one is continuous-time and the other is discretetime. Because the collected information of the GPS floating car is discrete points, the Discrete-time of Kalman filter is used in predicting the travel Time 3  
The Discrete-time of Kalman filter uses the state-space model that includes the state equation and observation equation composed to describe the filter. According to the criteria of linear unbiased minimum mean square error estimation, the state variable are given the best estimate by using the recursive nature of state equations and a set of recursive algorithm. The discretetime of Kalman filter model is as follows             k k k k k k k k k k k V S H Z W S S 1 1  1 1  1 Where S k is the n-dimensional state vector Z k is the mdimensional observation sequence 1   
 k k  n is the n  n dimensional state transition matrix 1    k k is the n  p dimensional noise input matrix H k is the m  n dimensional observation matrix W k-1 is the p-dimensional system process noise V k is the Supported by the Projects of Liaoning Province University Innovation Team 2008T085, 2007T072\,  Liaoning Province University Key Laboratory\(2009S04 9 


m-dimensional observation noise sequence W and V are assumed to be zero mean white noise and not relevant 4  The core of Kalman filter is the evaluation model that is also one of recursive algorithms. The value of state estimation is determined by the state estimates and the current observed values. It is problem of Kalman filtering that the linear minimum variance estimation n j S   f the state S j culated based on observed value Z n Z n-1  Z 1 The minimum target is as follows     n j j T n j j S S S S E J      2  The formula is the Kalman filter when j is equated to t; the formula is smoother when j is smaller than t; the formula is predictor when j is greater than t. The process of Kalman filter is introduced as example in the following From the priori estimates 1    k k S and prediction error covariance matrix P\(n\. The third equation of the formula 3 is used to calculate the optimal gain factor K k Then the new observation vector Z k is used to calculate the updated parts     1    k k k k k S H Z K The second equation of the formula 3 is used to update estimation k S  the state vector S k  The several important equation of Kalman filter is as follows                                 1  1  1 1  1  1 1  1  1 1  1  1  1  1 1  1           k k k k k k k k k k k k k k k k k k T k k k k T k k k k k k k k k k k k k k k k k P H K I Q P R H P H H P K S H Z K S S S S   3 The formal 3 is a filtering cycle of stochastic linear discretetime Kalman filter. The Kalman filter has two distinct information update process, one is time update process, and the other is observation update process. The first equation of the formula 3 is the method that the k state is estimated by the k-1  state, and the fourth equation of the formula 3 is used to make a quantitative description for the quality of forecasts. The other equations of the formula 3 are used to calculate the rectification value of time-updated, the rectification value is reduced from the quality of time update P k,k-1 the quality of observing information R k the relationship between observation and state  H k and the specific observation information Z k All equations is used to a purpose that observation information Z k is correct and rational used. Therefore, the formula 3 is describing the observation update process of Kalman filter III  I MPROVED M ODEL  OF K ALMAN  For the continuous driving vehicles, the rear travel time can be constantly updated when the observation value of the travel time is obtain in certain segments due to continuation of time and space 5 Then the updated travel time will be closer to the real value. The formula is as follows k k S S k k T       T  1   1    4 Where k S  is the predicting travel time of the k segment by Kalman Filter T  k the statistics travel time of the k segment  1    k T is the revised travel time of the k 1 segment   Because the vehicle speed is affected by a lot of accidental factors, this continuation will be destroyed when the speed changes 6 The ratio is changed suddenly by the sudden and accidental events. The ratio between true value and estimate value in three continuous segments are used in the formula 5. The ratio is also allocated a certain weight. The date will be removed when the ratio is more than two     2     1          3  T 2 3 1 2 1 3              k k k K S k T q S k T q S k T q S k 5 IV  T HE REALIZATION OF DYNAMIC PREDICTION MODEL  Variables and parameters of Kalman filter need to be explained in travel time prediction, based on the introduction of the Kalman filter and its improved model A  The Update Frequency of Dynamic Travel Time The Kalman filter need the time update, then the continuous time is processed to discrete time. For dynamic vehicle navigation systems, the service and operational quality are directly affected by the update frequency and data quality of traffic information 7 First, the system can not withstand the high update frequency. Such as the update time is 3 or 5 minutes, due to the instability of the traffic parameters in the short term and the frequent changes in the high update frequency of traffic information, the navigation center server is overloaded by the redesigning the path of navigation devices; Second, the update time can’t also be a long time. Such as the update time is 20 to 30 minutes, the role of the dynamic and real-time traffic information can’t be showed  The sampling interval of the GPS floating car is 2min in this paper. In excluding the problem data, the update time is 15min in the paper for the changing stability of traffic parameters and ensuring to exist at least two consecutive GPS points in each segment B  State Vectors and Observation Vectors The travel time of segment is affected by many factors, such as speed, road share, road grade, road width, weather conditions unexpected traffic incidents etc 9 Excluding the neural network model, the most relevant factor with speed is chosen. According to the characteristics of GPS datum, the speed of the adjacent time is selected as the state vector S 0 the  S k is expressed as  n k i k k k S S S S   2 1  h ere  i k S is the travel time of the k time period in the i segment The GPS floating car can directly collect the information including the coordinates and the car speed 10 But the speed of floating cars is instantaneous speed, the error is big. So the observation vector Z k s obtained by converting coordinates and observation time  C  Other Parameters The initial value of S 0 and P 0 can’t yet have been determined by a mature and objective way. With increasing of time S k and P k  will gradually get rid of the respective initial impact. But the major deviation are existed in the initial value, the filtering result still will have a greater error. Therefore the starting state vector of the same cycle is selected as S 0  P 0 is given by the specific 


data, such as the analysis result of test datum, the experience. The system noise Q k nd the observation noise R k e also determined by the analysis result of test datum. It can’t be come true and can’t even be done in some condition that the system noise and the observation noise is accurate given, Because the error source of the float car in the urban is more complex including instruments, satellite distribution, regional condition traffic condition and so on S o the system noise and the observation noise are given a more reasonable value depending on the analysis of specific data D  Experiment All GPS datum that are collected in 9:45-18:30 June 25, 2007 and relevant with Haining Road, Yalu River Road and Zhouiiazui Road and the division of segments are shown in fig. 1  Figure 1.  The segments and the associated GPS points Each segment of the travel time is showed in the following table TABLE I  T HE TRAVEL TIME OF EACH SEGMENT  Time The travel time of each segment\(s  1 2 3 4 5 6 7 8 9 10 11 12 13 15:15-15:30 82 173 88 115 78 78 83 83 51 51 81 81 45 15:30-15:45 64 91 98 108 159 64 46 128 178 37 131 166 34   is 0.5342 and  is 27.9523 in the state equation based on the data in the table The value of other groups are omitted due to  is greater than 1 or  is too large,  The change travel time of the previous 20 segments are showed in fig. 2, Each color lines represent different segment. If  is greater than 1, it represents that S k of the state equation of is rapidly increased. With the time gone, the travel time of the segment becomes longer. But the change travel time is not incremental in fig. 2. So it is error that  is greater than 1. It isn’t consistent with the distribution of the travel time that  is too small than 1. The minimum value of  is about 30 based on the fig. 2. Because the first item of state equation is always positive  can’t lager than 30. The data of table can only meet the two conditions in all groups datum  Figure 2.  The change of travel time  Due to the accuracy of travel time, two places rear decimal is retained S 0 the state equation is as follows k k k W S S 28 53  0 1    6 The mean of observation matrix is 1.0453 based on the data of all 35 time segments. For facilitating the operation H is equal to 1. Then the observation equation is as follows k k k V S Z    1 7 S 0  is [48 53 76 76 45 61 61 70 105 34 29 114 113 t h e m a t r i x  is the travel time of all segments in 9:45 ~ 10:00. Then the variance of S 0 is 797.5769, so P 0 is 798 The values of Q and R are determined by experimental Analysis based on the conditions of the above information 0 2 4 6 8 10 12 14 16 18 20 2.5 2 1.5 1 0.5 0 0.5  0 2 4 6 8 10 12 14 16 18 20 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18  a b Figure 3. The change condition of Q value In fig. 3 Q is selected from 0.1 to 2 with 0.1 intervals when R is remained the same value. The fig. 3 is the change condition of travel time in the 20th time period \(14:30-14:45\. It is shown in a diagram that the travel time of all 13 segments is changed when Q is given different values. The b diagram indicates the change condition of variance when Q is given different values. It can be seen from the diagram that the travel time and variance are close to stability when the abscissa is 10. So Q 0.1 +10 × 0.1 = 1.1 After Q is determined, the observation white noise R is equal to 2 in the discrete Kalman filter model through the same method V  C OMPARATIVE A NALYSIS OF A CCURACY  According to the above S 0  P 0  R  Q and the equation of the Kalman filter, it is all segments of fig. 1 form10:00 to 11:00 that the forecast travel time and the statistics travel time is showed in table    


TABLE II  T HE CONTRASTIVE OF TRAVEL TIME  The No. of segments  The forecast travel time \(s the statistics travel time \(s 10:00 10:15 10:15 10:30 10:30 10:45 10:45 11:00 10:00 10:15 10:15 10:30 10:30 10:45 10:45 11:00 1 137.5 50.6 72.6 60.5 247 55 140 108 2 44.0 51.7 40.7 48.4 92 37 85 46 3 163.9 80.3 107.8 50.6 149 80 95 88 4 86.9 49.5 50.6 48.4 96 94 101 86 5 86.9 60.5 69.3 117.7 95 103 128 66 6 73.7 49.5 73.7 83.6 65 69 69 51 7 73.7 57.2 81.4 50.6 66 62 50 51 8 46.2 58.3 50.6 38.5 43 76 53 47 9 46.2 104. 5 50.6 60.5 74 102 51 119 10 96.8 80.3 37.4 39.6 49 192 45 45 11 119.9 64.9 45.1 44.0 59 72 82 62 12 72.6 128.7 40.7 55.0 71 67 47 85 13 42.9 96.8 177.1 119.9 172 71 165 146  According to Table the comparative condition of the prediction and statistical travel time is shown in Fig. 4  Figure 4.  Comparative travel time of forecast and statistics  Figure 5.  The difference distribution of four periods The forecast situation of 13 segments within the study area in the four time period is listed in table It can be seen from fig. 4 and fig. 5 that the bias is existed between the predicting results of Kalman filter and the corresponding statistical time. The travel time is analyzed when the collecting data is only started by the float car in table Then three periods are selected for time of the GPS data in table The period \(10:30-10: 45\ is near the start of collecting GPS datum, the period \(14:15-14: 30\ is near the middle of collecting GPS datum, the period \(18:00 -18:15\ is close to the end of collecting GPS datum. From 5 error targets that included the mean relative error \(mrerr\, the mean absolute relative error \(marerr\, the maximum absolute relative error mxarer\, the equal coefficients \(EC\nd the mean square root sum of the relative error squared \(nmrerr\ , the error of the travel time is researched. So the results are shown in table  TABLE III  THE PREDICTING E RROR OF THE TRAVEL TIME  Error targets Time periods 10:30-10:45 14:15-14:30 18:00-18:15 mrerr  0.9043 2.2000 1.6000 marerr  1.4000 2.9000 2.1000 mxarer  5.7000 15.0000 8.1000 nmrerr  2.1000 4.9000 3.5000 EC 0.9994 0.9991 0.9989  The error between result of Kalman filter and the observed travel time are analyzed in table The maximum value is the maximum absolute relative error of the 14:15-14:30 time period but its value is also only 0.015. It is shown that the forecast model has highly predicting accuracy in all periods. To further prove this conclusion, the mxarer distribution of all 32 time periods is shown by fig. 6 Figure 6. The mxarer of Kalman filter  TABLE IV  T HE PREDICTION RESULTS OF TWO IMPROVED METHODS  The No. of segments The first improved model The second improved model 10:00 10:15 10:15 10:30 10:30 10:45 10:45 11:00 10:00 10:15 10:15 10:30 10:30 10:45 10:45 11:00 4 79 49.3 44.6 84.2 126.1 45.4 73.8 71.8 5 96 114. 9 138.3 209.1 104.6 84.7 113.6 191.4 6 80. 6 84.3 136.1 46.9 78.6 81.7 127.9 97.2 7 65 79.7 76.2 30.9 72.9 90.4 115.3 39.9 8 41 63.2 31. 1 38.8 42.7 75.2 46.9 30.89 9 43 136.2 53 73.8 42. 1 130.1 44.7 63.4 10 155 78.4 37. 7 77. 9 122. 88.6 35.7 61.7 11 60.7 155.2 54.3 50 112.9 112.8 50.2 62.8 12 35.7 142.8 74 77.5 49.5 194.9 60.2 77.6 13 41.9 50.4 204.5 185.3 31.6 99.6 245.1 171.7  It can be seen from Fig. 4 and Fig. 5 that the most predicted results are lower than the statistics travel time. The main reason is excluding the data that the speed is zero in the raw data, and it is always assumed that the vehicle is running when the speed of the float car is calculated. Therefore the results of Kalman filter need to be carried out the necessary smoothing process. As the known data of three segments is used in the second improved model, the data processing should be started form the fourth segment. The weight is given by range principle. For the k segment, the weight of the k-1 segment is 3/6, the weigh of the k-2 segment is 2/6 and the weigh of the k-3 segment is 1/6. There are two ways to 


improve the forecast results of two improved models is shown in table   Figure 7.  The result of the first improved model   Figure 8.  The result of the second improved model From the results of prediction, the effect of smoothing is not obvious. As it is mainly reason that the self-continuity of the observation data is poor, the internal regular is destroyed However, the forecast result of Fig. 8 is better than the one of Fig 7. Form the case, it is shown that the forecast result is fluctuated when the proportion of the accidental factors is too large in a single segment VI  C ONCLUSION  In this paper, Kalman filter model is applied to the predicting field of the dynamic travel time. The method of establishing the Kalman filter equation is proposed based on GPS datum of the float car. And the two improved models of kalman filter are established by the correlation of the adjacent segments. Through the error of the Kalman filter model and two improved methods is researched by comparative analysis in the prediction accuracy the discrete-time Kalman filter model is lower than the smoothing model of parameter regression in accuracy, the second improved model is superior to the first improved model R EFERENCES  1  Z. Yang and L. Chu, “The Advance of Dynamic Route Guidance System Journal of Highway and Transportation Research and Development, vol 17 pp.  34-38,  2000 2  W. Zhang, J. Xu, and H. Wang Urban Traffic Situation Calculation Methods Based on Probe Vehicle Data Journal of Transportation Systems Engineering and Information Technology, vol  1, pp. 43-49, July 2007 3  M. Hang, X. Yang, and G. Peng Study of Predicted Travel Time in Urban Expressway Based on Kalman Filter Journal of  Tongji university , Vol  30, pp.  1068-1072, September 2002 4  M. Fu, Z. Deng, and J. Zhang, The Theory and Application of Kalman Filter in Navigation System. The Science Publishing House, 2003 5  Y. Li, and M. McDonald, “Link Travel Time Estimation Using Single GPS Equipped Probe Vehicle,” Proc. Intelligent Transportation Systems., pp 932-937, September  2002 6  A. Zhu, Research on Link Travel Time Prediction Method Based on Data Collected by Floating Car. Beijing Jiaotong University,  2007 7  B. Young, “GPS-GIS Integrated System for Travel Time Surveys,” The Civil Enginerring University of Toronto, July 2005 8  J. Weng, Research on Key Techniques of Short-Term Traffic Forecasting Toward Vehicle Navigation System Application. Beijing University of Technology,  2007 9  A. Torday, A.G. Dumont Parameters Influencing Probe Vehicle Based Travel Time Estimation Accuracy The 4th Swiss Transport Research Conference, pp.168–173, 2004   B.S. Kerner, C. Demir, R.G. Herrtwich, S.L Klenov, H. Rehborn, M Aleksic, and A. Haug, “Traffic State Detection with Floating Car Data in Road Networks,” Proc. The 8th International IEEE Conference on Intelligent Transportation Systems, pp. 700-705, September  2005         


gain   Test A  Info  S   Info  Test A  S  21 gainRatio   Test A  gain   Test A   splitInf o   Test A  22 Now we can use the equation  22 to choose a split point and an attribute Please note that the main difference between the original equations used in C4.5 and ours is that we use the probability of being in a certain partition as a weight in the formulas The rational behind this modi“cation is the following If we write the original formulas in clear form we can see that there exists an implicit indicator function that assigns 0 or 1 based on the membership instances to set S 1 and set S 2  Since we can not be sure whether a certain instance is in S 1 or in S 2  we use the probability of being in S 1 or in S 2 as a weight 6.5 Splitting Training Data Set Using Random Path Selection This method is an alternative way to split the training data into two after nding a split point and an attribute Again in our implementation we use a random split based on the p S 1  w j  and p S 2  w j  values splitRandom  w j t   S 1  with prob p S 1  w j  S 2  with prob p S 2  w j  23 Pseudo-code is shown in Algorithm 3 6.6 Classifying the Perturbed Instance Using Random Path Selection For each test instance w j in the perturbed data set W and for each chosen split point in the constructed tree we calculate p S 1  w j  t  p 1  Next we place the instance to the left child of the node with prob p 1 and to the right child with prob 1  p 1  We continue with this until we reach a leaf node The Pseudo-code is shown in Algorithm 4 7 Experimental Results In our experiments we use the data extracted from the census database 1994 Census Income or Income which can be downloaded from University of California Irvine UCI machine learning database repository 1  This data set has fourteen attributes six continuous and eight nominal It altogether has 48842 instances separate as training data 32561 instances and testing data 16281 instances The data is used to predict whether the income exceeds 50 K annually We choose this data set to have fair 1 http://www.ics.uci.edu mlearn/MLSummary.html Partition Node N  1 if Stopping Criteria is Met then 2 return 3 else 4 Using RandomSplittingCriterion Compute 5 the Best Attribute Best A and the Best Splitting Point t  for each Instance w j in Node N do 6 Calculate the p S 1  w j  t  p 1 7 Based on attribute Best A and splitting 8 point t  Let R be a uniform random value between 9 0  1  if R  p 1 then 10 addChild  N.leftChild w j  11 else 12 addChild  N.rightChild w j  13 end 14 end 15 end 16 Partition N.leftChild  17 Partition N.rightChild  18 Algorithm 3  Partition Training Instances Using Random Criteria for each Instance w j in W from the root node do 1 Classify\(Node N Instance w j  2 if N is a leaf node then 3 use the rule given at the leaf 4 return class value 5 else 6 Calculate the p S 1  w j  t  p 1 7 Based on the attribute A i used in N 8 and the split point t  9 Let R be a uniform random value between 10 0  1  if R  p 1 then 11 return Classify N.leftChild w j  12 else 13 return Classify N.rightChild w j  14 end 15 end 16 end 17 Algorithm 4  Classify Noisy Instances Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 
 


comparison with reconstruction based techniques that require relatively large data sets Since in this paper we focus on the numeric continuous attributes we only keep the six numeric continuous attributes in our data set Also for ef“ciency purposes we randomly choose 10000 instances from the training data set and keep all the instances in the testing data set We use the noise addition frame work proposed in and add both Gaussian and Uniform random noise to each attribute When using Gaussian random noise we know that the variance  2 can dramatically affect the results We use four different Gaussian distribution noise data with different variance values To quantify the relative amount of noise added to actual data we used the Signal-to-Noise Ratio SNR that is the ratio of variance  2 of actual data to variance  2 of noise data W e also use the pri v a c y measure mentioned in section 3 to quantify the privacy loss The table 1 shows the ve perturbed data sets with their SNR values and privacy measures In our experiments we only use one data set which perturbed by uniform noise shown as data5 We can see from table 1 when the SNR value is higher the variance  2 of noise data is lower thus the perturbed data preserves less privacy The uniform distributed noise is generated by a given data range We can calculate the SNR for each attribute for uniform noise data in our experimental data set the SNR values for six attribute are 1.3 2.7 1.2 53.6 1.9 and 1.1 respectively Table 1 Privacy measure of different data sets  Data1 Data2 Data3 Data4 Data5 Noise Distribution Gaussian Gaussian Gaussian Gaussian Uniform SNR 1  7 1  3 1  0 0  5 N/A Privacy loss 0  2183 0  1909 0  1619 0  1026 0  2604 7.1 Local vs Global Data Mining First note that by local data mining each participant mines its own data By global data mining we mean that the participants share the data and mine to obtain global patterns As we have mentioned before our approach is suitable for the scenarios where many parties are participating to perform global data mining without compromising their privacy The data sets distributed among each party can be horizontally or vertically partitioned Horizontally partitioned data means the instances are split across the parties and vertically partitioned data means the attributes are split across the parties Experimental results show that for both types of partitioning local data mining results are less accurate compared with those obtained from global data mining This supports the fact that extracting information from globally shared data is better Table 2 C4.5 decision tree classi“er accuracy over horizontally partitioned data Accuracy data1 data2 data3 data4 data5 50 Instance 73.47 74.23 73.13 73.23 73.63 100 Instance 78.54 73.33 77.43 78.13 75.63 Accuracy data6 data7 data8 data9 data10 50 Instance 74.63 72.37 74.23 76.27 77.73 100 Instance 78.77 75.4 78.13 76.2 77.77 We use the data set described in the previous sub-section with six attributes We randomly choose instances to form small data sets with different sizes denoted as group 1 group 2 to group 10 We apply standard C4.5 classi“er on these data sets and the accuracy numbers are shown in Table 1 It is clear that when the number of instances are increased the C4.5 decision tree algorithm has better performance Table 3 C4.5 decision tree classi“er accuracy over vertically partitioned data Accuracy 2 Attributes 3 Attributes 4 Attributes 5K Instance 77.54 77.84 78.9 32K Instance 78.06 78.51 79.05 Similarly we have removed some attributes from the Income data set and then applied standard C4.5 decision tree classi“er on the new data sets and the accuracy of the classi“cation results are shown in Table 2 We can see that when the attribute number is increased the C4.5 decision tree algorithm performs better 7.2 Reconstruction Based Approaches Results For comparison purposes we report the data mining results obtained by using original data distribution reconstruction methods We apply two notable reconstruction techniques to the perturbed data set The rst technique is Bayesian inference estimation BE based approach proposed by Agrawal et al The second technique is the principal component analysis PCA based approach proposed by Kargupta et al Please refer to the original work for the algorithms details We apply the two techniques on the ve data sets We rst reconstruct the original distribution and then use this Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 
 


Table 4 Data Mining accuracy of applying data mining techniques directly on 10k perturbed training data set  Data mining on perturbed data set Decision Tree C4.5 Classi“er Accuracy  Data Set Data1 Data2 Data3 Data4 Data5 Test on Original 79  61 79  04 77  73 77  32 80  29 Test on Perturbed 78  56 78  14 77  45 77  05 80  47 Naive Bayes Classi“er Accuracy  Data Set Data1 Data2 Data3 Data4 Data5 Test on Original 78  45 78  21 78  17 77  99 80  45 Test on Perturbed 78  08 77  78 77  46 76  47 80  29 Table 5 Data Mining accuracy with BE based reconstruction technique BE based reconstruction technique Decision Tree C4.5 Classi“er Accuracy  Data Set Data1 Data2 Data3 Data4 Data5 Test on BE-recon 91  91 90  50 86  26 93  97 86  35 Test on Original 24  45 25  17 69  32 35  96 78  72 Test on Perturbed 38  29 36  47 58  45 43  63 74  06 Original Data Mining Accuracy  83  40 Naive Bayes Classi“er Accuracy  Data Set Data1 Data2 Data3 Data4 Data5 Test on BE-recon 88  68 87  09 84  32 94  59 77  53 Test on Original 26  30 21  66 23  01 23  08 76  46 Test on Perturbed 37  19 39  91 33  09 37  30 49  66 Original Data Mining Accuracy  79  87 estimated distribution to build the data mining models We perform three different tests to compare data mining accuracy In the rst case we test the classi“er on the reconstructed test data in the second case we test the classi“er on the original test data and in the third case we test the classi“er on the perturbed test data The data mining models prediction accuracy is shown in the table 5 and table 6 As comparison table 4 shows the data mining accuracy obtained directly from the perturbed data sets Our results indicate that both reconstruction techniques fail to produce good data mining models This result is not surprising since in general estimating data distributions on nite data is a very hard problem If we use this original data distribution reconstruction phase as a intermediate step to do privacy preserving data mining we may not always get good performance results In the work the authors have investigated three different real world data sets and Table 6 BE based reconstruction technique data mining accuracy PCA based reconstruction technique Decision Tree C4.5 Classi“er Accuracy  Data Set Data1 Data2 Data3 Data4 Data5 Test on PCA-recon 99  23 98  16 98  34 95  10 99  61 Test on Original 70  31 71  71 72  35 68  42 76  86 Test on Perturbed 54  76 63  49 59  17 61  08 61  49 Original Data Mining Accuracy  83  40 Naive Bayes Classi“er Accuracy  Data Set Data1 Data2 Data3 Data4 Data5 Test on PCA-recon 97  83 98  10 97  42 94  33 98  63 Test on Original 66  12 64  29 63  21 59  84 64  81 Test on Perturbed 46  08 41  71 39  07 28  05 51  08 Original Data Mining Accuracy  79  87 Table 7 Proposed PPDTC4.5 data mining accuracy Our Proposed PPDTC4.5 Classi“er Accuracy  PPDTC4.5 Threshold Method Data Set Data1 Data2 Data3 Data4 Data5 Test on Original 80  74 79  69 76  63 77  02 80  29 Test on Perturbed 76  09 76  14 74  41 76  03 80  52 PPDTC4.5 Random Path Selection Method Data Set Data1 Data2 Data3 Data4 Data5 Test on Original 78  72 77  21 77  67 78  01 80  31 Test on Perturbed 78  40 77  77 77  30 77  06 80  32 the reconstruction based approaches have failed on all those data sets These results support our motivation of nding direct ways to perform privacy preserving data mining from perturbed data 7.3 PPDTC4.5 Classi“er Accuracy Using the data sets described earlier we perform different experiments Applying WEKA C4.5 algorithm on the original training data set to build the decision tree and classify the original testing data set we get 83  40 accuracy table 4 shows the data mining accuracy when apply data mining tools directly on the perturbed data sets Table 7 shows data mining accuracy of our proposed PPDTC4.5 algorithms We can see when we use our proposed PPDTC4.5 Threshold Method on these ve data sets to build the decision tree and classify on the original data set We get higher accuracy than which classify on the perturbed data for data 1 and data 2 equivalent accuracy for Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 
 


Figure 1 Threshold Method of PPDTC4.5 Classi“er Accuracy on 10K Uniform Perturbed Training Data 16K Original Test Data Figure 2 Threshold Method of PPDTC4.5 Classi“er Accuracy on 32K Gaussian Perturbed Training Data 16K Original Test Data data 5 lower accuracy on data 3 and data4 The reason is SNR value plays an important role here Table 1 shows different perturbed data sets with different SNR values When SNR less than 1  0 means the variance  2 of noise data greater than the variance  2 of actual data In other words data 3 and data 4 are introduced more noise Our algorithm get good results when the SNR value is greater than 1  0  When The threshold method builds a decision tree classi“er which is not suitable to classify the perturbed data set because our algorithm estimates the splitting point and partition the training data as the original data would have To build a classi“er to classify the perturbed data we use the probability as weight to nd the best splitting point and use random path selection to partition the training data so far the classi“er accuracy has not been improved much compared with directly applying WEKA C4.5 algorithm The reason is when partitioning the training data set and classifying the test data set the random path selection method does not bound the random noise R well In the future we would like to nd a better way to bound the R value to build a better classi“er for classifying the perturbed data As we have seen in our experimental results our proposed PPDTC4.5 classi“ers may not get very excited high accuracy comparing with those obtained from directly applying data mining techniques to the perturbed data sets But comparing with reconstructed based approaches our methods obtain very good results We try to represent the message here is avoiding to solving the hard distribution problem in stead mapping the data mining functions to construct privacy preserving data mining methods This is a promising direction Furthermore our experimental results have also indicated that when huge data set is available white noise is no longer can prevent data mining tools to abstract patterns So directly mining the perturbed data set is also a good approach when the data set is big enough In the PPDTC4.5 threshold method we know that choosing different threshold values affect the data mining accuracy Choosing the threshold to get good data mining results is related to the distribution of the random noise added to the data and the data itself In our experiments when using Uniform distribution random noise to distort the data 0  5 is a good threshold to get a classi“er with high accuracy when using Gaussian distribution random noise to distort the data 0  3 is a good threshold to get a classi“er with high accuracy The relationship between data mining accuracy and threshold values are shown in gure 1 and gure 2 The best threshold should change from data to data In other words this is dependent on the data property 7.4 Algorithm Complexity Given n instances m attributes and p label values the number of potential splitting points t of numeric continuous attribute at most is n  1  The complexity C4.5 algorithm on training phase is O  nlgn  tmp   Our algorithm evaluates the probability for instance w j for every given potential splitting point t  which increases the complexity of algorithm in the worst case scenario to O  ntmp   Since our algorithm skips the steps of reconstruction the original distribution for each attribute the running time is very reasonable comparing with the BE reconstruction algorithm given in In BE reconstruction algorithm there is a stop parameter to determine when to stop the calculation of the estimated distribution The fact is more loops calculation more running time and better accuracy of the estimated distribution In our experiments based on different choice of the stop parameter the running time of the BE reconstruction algorithm is raged from three to ve times longer than our proposed algorithm running on the same con“guration computers Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 
 


8 Conclusion We have proposed a modi“ed C4.5 decision tree classi“er which is suitable for privacy preserving data mining The classi“er is built from the perturbed data set and the model can be used to classify the original data with high accuracy In the scenarios where many parties are participating to perform global data mining without compromising their privacy our algorithm decreases the costs of communication and computation compared with the cryptographybased approaches Our algorithm is based on the perturbation scheme but skips the steps of reconstructing the original data distribution The proposed technique has increased the privacy protection with less computation time In the future we will investigate various ways to build the classi“ers which can be used to classify the perturbed data set As we have mentioned before with a better bound of the random noise data R  using the probability as weighting is an approach that needs further investigation As pointed out before some data mining techniques can be directly applied to perturbed data due to the perturbation process still preserve some nature of the data Naive Bayes classi“er can be directly applied to the additive perturbation data and Euclidean based data mining tools e.g k Nearest Neighbor Classi“er Support Vector Machines and Perceptrons Neural Network can be applied to the multiplicative perturbation data But the data mining accuracy is reduced due to the information loss in the process and some data mining methods themselves may not have good performance As we know k Nearest Neighbor is a simple but poor performance classi“er Do we have the exibility to choose different data mining tools In this paper we provide a new direction which is modifying data mining functions to suit the perturbed data This absolutely enable us more choices Our proposed method skips the reconstructing the original data distribution from the perturbed data In this way the method performs privacy preserving data mining without solving the hard distribution problem Data mining techniques are used to derive patterns and high level information from data Data mining results do not cause the violation of privacy One thing bring to our notice is that when data set is big enough perform data mining techniques directly on the perturbed data sets can obtain good data mining accuracy For example applying decision tree classi“er to additive perturbation data can get good data mining accuracy This can be observed in our experimental results Privacy as a security issue in data mining area is still a challenge References  C C Aggarw al and P  S Y u  A condensation approach to privacy preserving data mining  D Agra w a l and C C Aggarw al On the design and quanti“cation of privacy preserving data mining algorithms In PODS  ACM 2001  R Agra w a l and R Srikant Pri v a c y-preserving data mining In SIGMOD Conference  pages 439…450 2000  K Chen and L Liu Pri v a c y preserving data classi“cation with rotation perturbation In ICDM  pages 589…592 2005  C Clifton M Kantarcioglu J V aidya X Lin and M Y  Zhu Tools for privacy preserving data mining SIGKDD Explorations  4\(2 2002  W  Du and Z Zhan Using randomized response techniques for privacy-preserving data mining In KDD  pages 505 510 2003  A V  Ev“mie vski R Srikant R Agra w al and J Gehrk e Privacy preserving mining of association rules In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pages 217…228 2002  M Kantarcioglu and C Clifton Pri v ately computing a distributed k-nn classi“er In J.-F Boulicaut F Esposito F Giannotti and D Pedreschi editors PKDD  volume 3202 of Lecture Notes in Computer Science  pages 279…290 Springer 2004  H Kar gupta S Datta Q W ang and K Si v akumar  O n the privacy preserving properties of random data perturbation techniques In ICDM  pages 99…106 IEEE Computer Society 2003  Y  Lindell and B Pinkas Pri v a c y preserving data mining In M Bellare editor CRYPTO  volume 1880 of Lecture Notes in Computer Science  pages 36…54 Springer 2000  K Liu H Kar gupta and J Ryan Random Projection-Based Multiplicative Data Perturbation for Privacy Preserving Distributed Data Mining IEEE Transactions on Knowledge and Data Engineering TKDE  18\(1 January 2006  L Liu M Kantarcioglu and B Thuraisingham The applicability of the perturbation based privacy preserving data mining for real-world data Data and Knowledge Engineering Journal  2007  T  M Mitchell Machine Learning  mcgraw-hill 1997  J R Quinlan C4.5 Programs for Machine Learning  Morgan Kaufmann 1993  J R Quinlan Impro v e d use of continuous attrib utes in c4.5 J Artif Intell Res JAIR  4:77…90 1996  S Rizvi and J R Haritsa Maintaining data pri v a c y in association rule mining In VLDB  pages 682…693 Morgan Kaufmann 2002  B M Thuraisingham Pri v a c y constraint processing in a privacy-enhanced database management system Data Knowl Eng  55\(2 2005  J V aidya and C Clifton Pri v a c y-preserving means cluster ing over vertically partitioned data In KDD  pages 206…215 2003  I H W itten and E Frank Data Mining Practical Machine Learning Tools and Techniques  Morgan Kaufmann San Francisco 2 edition 2005  S Xu J Zhang D Han and J W ang Singular v alue decomposition based data distortion strategy for privacy protection Knowl Inf Syst  10\(3 2006 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 
 


target item falls in TH category. To improve effectiveness of a segment attack, we select filler items from the set of items which are highly rated by those users who have rated target item at a higher Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 scale. The strategy used is similar to Strategy IH Filler items are selected the way explained in section 6.2  8. Experimental evaluation and discussion  We performed the experimental evaluation of our strategies on the publicly available MovieLens data set [8]. This is the most widely used dataset in recommender systems research. MovieLens consists of 100,000 ratings made by 943 users on 1682 movies. Each user in the data set has rated at least 20 movies and each movie has been rated at least once A timestamp value is associated with each user movie, and rating combination. The data set also contains information on the demographic detail \(age sex, occupation, and zip code information \(genre and release date The ratings are made in a scale of 1 to 5, where 5 indicate extreme likeness for an item and 1 dislike We evaluated effectiveness of the proposed strategies on user-based and item-based collaborative algorithm. For similarity calculation and prediction in user-based CF algorithm, equations 1 and 2 stated in section 3 were used. Similarly, equations 3 and 4 stated in section 3 were used for computing similarity and prediction value for item-based CF algorithm We used a neighborhood size of k = 20 for prediction calculation. Case amplification value of 10 was used while calculating correlation and only positive correlations values were considered for computing predictions To conduct our evaluation, we selected a sample 20 items. Out of the 20 items, 10 items belonged to TL  category while remaining 10 items to TH category All the 20 items were selected randomly from a larger set of items belonging to each category. We also randomly selected a sample of 50 target users Target users selected were those who have never rated any of the 20 test items. Each of the target items was attacked individually and the prediction shift was calculated by averaging the prediction shift observed for each user. The final prediction shift for the attack is the average prediction over all items used in the test. Equation 6 was used to calculate the metric For implementation of segmented attack we followed the same guidelines as stated in [3]. Horror segment was selected as the target segment. Five of the most popular horror movies were selected to represent the segment. These five movies selected formed the selected item set in the attack profiles constructed. The five movies are Alien, Psycho, The Shining, Jaws, and The Birds. Users who have given a rating of 4 or 5 to at least any 3 of the five movies were identified as the target segment against which the attack was focused. For calculating prediction shift we selected 50 of the users from this target segment to form the test user set. While implementing the segment attack, selected items were given a rating of 5 and the randomly selected filler items were assigned a value 1 All experiments were conducted for ?Size of attack? values 1%, 3%, 6%, 12%, and 15%.  ?Size of attack? represents number of attack profiles added as a percentage of pre-attack profiles. 1% ?Size of attack? implies 10 attack profiles were added to a 


attack? implies 10 attack profiles were added to a system of 1000 genuine users. On the basis of the results reported in [4] that best results are reported when a filler size of 3% is used in an average attack we used a filler size of 3% for all our tests i.e., 3 % of 1682 items which is approximately 50 filler items For attacks against user-based collaborative filtering systems we used six strategies: Strategy UL, Strategy UH, Strategy SUL, Strategy SUH, segment attack and average attack. Similarly, for attacks against item based collaborative filtering systems we used six strategies: Strategy IL, Strategy IH Strategy SIL Strategy SIH, segment attack and average attack. For average attack, filler item strategy used was the same as in an average attack i.e., the mean of the filler item was assigned to it. Segment attack was implemented as explained earlier. Category TL, Category TH Strategy UL, Strategy UH, Strategy IL, Strategy IH Strategy SUL, Strategy SUH, Strategy SIL and Strategy SIH were implemented the way explained earlier in section 4, section 5, section 6 and section 7 respectively. For attacks against item-based CF while selecting filler items from set IF, only items with minimum frequency count of 10 were considered Figure 2 and Figure 3 show the effectiveness of our attacks when calculated for all users against systems using user-based collaborative filtering for recommendations.  Figure 2 shows the prediction shift values of attacks Strategy UL and average attack for items belonging to TL category. From the graph it?s obvious that for items in TL category, Strategy UL outperforms average attack model for all values of attack size. Similarly, Figure 3 shows the prediction shift values for the attack strategies Strategy UH and average attack for items belonging to TH category From the graph it can be concluded that for items belonging to TH category, Strategy UH performs much better than average attack over lower values of attack size. At attack size of 12 % and 15% both attack have similar effectiveness Figure 4 and Figure 5 show the effectiveness of our attacks when calculated for all users against systems using item-based collaborative filtering for recommendations.  Figure 4 shows the prediction shift values of Strategy IL and average attack for Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 items belonging to TL category. Similarly, Figure 5 shows the prediction shift values for the Strategy IH and average attack for items belonging to TH category. From Figure 4 and 5 it can be concluded that both Strategy IL and Strategy IH perform substantially better than average attack over all values of attack size. It can also be observed that our attack strategies are more effective against itembased systems than user-based systems Figure 6, 7, 8, and 9 show the effectiveness of our filler based attack strategies for in-segment users. We observe that for attacks against both user-based and item-based CF systems the effectiveness of our filler based strategies is comparable to the best available attack model for in-segment attacks i.e., segment attack. However, in Figure 8 we observe that filler item strategy SIL performs better than segment attack. Because of the low knowledge cost involved in segment attack, we can conclude that for most scenarios segment attack is a better attack model for in-segment attacks than filler based attack models Experimental results clearly show that our approach of selecting a strategy based on target item rating distribution outperforms the best available attack model i.e., average model. One drawback of 


attack model i.e., average model. One drawback of our attack strategies is its high knowledge cost However, automated software agents can help diminish the cost. One approach that can be used to decrease the cost is to use a subset of users while selecting filler items. For example, in attacks against item-based systems, while implementing Strategy IH instead of selecting all users who have rated target item as 4 or 5 as members of the set UH. , we only select 20 users. Selection of items for set IF will then be performed using the data of the 20 users in set UH Similarly, in case of attacks against user-based systems, while implementing Strategy UH instead of assigning a filler item IF the average rating given to it by the set of users UH. , we assign IF the average rating given to it by a subset of 5 randomly selected users from UH. In future work we plan to experimentally verify the effectiveness of these cost reduction approaches    Figure 2:   Attack on TL category of items against user-based collaborative filtering system   Figure 3:   Attack on TH   category of items against user-based collaborative filtering system   Figure 4:   Attack on TL category of items against item-based collaborative filtering system  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8  Figure 5:   Attack on TH category of items against item-based collaborative filtering system   Figure 6:   Attack on TL category of items against user-based collaborative filtering system   Figure 7:   Attack on TH   category of items against user-based collaborative filtering system    Figure 8:   Attack on TL category of items against item-based collaborative filtering system   Figure 9:   Attack on TH category of items against item-based collaborative filtering system  9. Conclusion  This paper provides an effective approach towards constructing attack models. We show the importance of target item and filler items in construction of successful attack strategies. Through experiments we show that our approach of intelligent selection of filler items based on target item rating distribution results in substantial improvement over the baseline average attack. We also compare our approach with the well known in-segment approach and conclude that our approach gives slightly improved results. In future, we plan to examine the filler items strategies for other attack models, and also create algorithms to improve robustness and stability of recommender systems against shilling attacks 


systems against shilling attacks  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9  10. References  1] Lam, S., and Riedl, J. 2004. Shilling Recommender Systems for Fun and Profit, In Proceedings of the 13th International WWW Conference 2] Mehta, B., Hofmann, T., and Nejdl, W. 2007. Robust Collaborative Filtering, In Proceedings of the 2007 ACM Conference on Recommender Systems, 49-56 3] Mobasher, B., Burke, R., Bhaumik, R., and Williams C. 2007. Towards Trustworthy Recommender Systems: An Analysis of Attack Models and Algorithm Robustness, ACM Transactions on Internet Technology, 7\(2007 4] Burke, R., Mobasher, B., and Bhaumik, R. 2005 Limited Knowledge Shilling Attacks in Collaborative Filtering Systems, In Proceedings of Workshop on Intelligent Techniques for Web Personalization 5] Konstan, J., Miller, B., Maltz, D., Herlocker, J Gordon, L., and Riedl, J. 1997.  GroupLens: Applying Collaborative Filtering to Usenet News Communications of the ACM, 40, 3\(1997 6] Herlocker, J., Konstan, J., Borchers, A., and Riedl J.1999. An Algorithm Framework for Performing Collaborative Filtering, In Proceedings of  SIGIR ACM, 77-87 7] Sarwar, B., Karypis, G., Konstan, J., and Riedl, J 2001. Item-based Collaborative Filtering of Recommendation Algorithms. In Proceedings of the 10th International WWW Conference 8] MovieLens data set,www.grouplens.org  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


 Current Time \(min  Healthy Failure Expected Just-in-time line Actual Remaining Life  Figure 17. Results of failure prognosis 0 100 200 300 400 500 600 700 800 900 1000 0 0.02 0.04 0.06 0.08 0.1 0.12 Time \(min Sp al l S iz e  m m 2 Interpolation of spall growth according to feature values 0 100 200 300 400 500 600 700 800 900 1000 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Time \(min Fe a tu re V a lu e    Interpolation of feature value with noise Interpolation of feature vlaue snapshot with ground truth data 11 CONCLUSIONS This paper shows that enhancements to diagnostic techniques are desirable as well as attainable additions to Health and Usage Monitoring Systems \(HUMS particularly in the case of rotorcraft component monitoring Enhancements like those presented support CBM efforts primarily in two ways: reduce the sensitivity of diagnostic processes to both signal noise and variations in environmental and operating conditions, and improve the performance of detection systems as well as the task of fault identification \(e.g., severity quantification instantiation of reliable prognostics Representative examples, motivated by the interest of the U.S. Army in transitioning from time-based \(using TBO definitions drive train bearing, illustrates the potential benefits of 


pursuing an integrated approach to diagnostics and prognostics, combining technologies for enhanced data preprocessing, advanced diagnostic-support algorithms, fusion at the sensor/feature levels, and an adequate framework for false alarm mitigation and uncertainty management. An architecture for achieving such integration is presented, with emphasis on supporting a robust performance of diagnostics operations, even in the presence of such kinds of disturbances as those observed in data acquired by HUMS vibration sensors. The present study also gives relevance to seeded fault because the technologies discussed can integrate knowledge about damage mechanism interactions or physics-of-failure models, as well as make use of multiple-sensor and multiple-feature data sets representative of known fault conditions For this reason, the team behind this project is evaluating a potential opportunity to perform a series of tests on rotorcraft drive train bearings with varying fault severities and under multiple, though realistic, operating conditions Such tests are being planned to provide algorithm/model validations, as well as diagnostic/prognostic performance assessments, in support of providing the U.S. Army with technologies that make detection systems more robust allow for the implementation of prognostics, and extend the useful life of drive train components. Component degradation testing thus remains as future, follow-up work to the research reported in this document ACKNOWLEDGMENTS This work has been partially supported with a cooperative agreement by the Army Research Laboratory under contract number W911NF-07-2-0075. In addition to the primary authors, we would like to thank government and contractor representatives from organizations supporting the Army Utility \(Blackhawk Estes, Mr. Carlos Rivera, and Dr. Jon Keller. This work has also benefitted greatly from consultations with other Army Research Laboratory and NASA Glenn researchers such as Dr. Timothy Krantz, Dr. David Lewicki, Dr. Harry Decker Dr. Hiralal Khatri, Mr. Ken Ranney and Mr. Kwok Tom REFERENCES 1] Branhof, R.W., Grabill, P., Grant, L., and Keller, J.A  Application of Automated Rotor Smoothing Using Continuous Vibration Measurements  American Helicopter Society 61st annual forum, Grapevine, Texas June 1  3, 2005 2] Dora, R., Wright, J., Hess, R., and Boydstun, B  Utility of the IMD HUMS in an Operational Setting on the UH60L Blackhawk  American Helicopter Society 60th annual forum, Baltimore, Maryland, May 7  10, 2004 3] Zakrajsek, J.J., Dempsey, P.J., et al  Rotorcraft Health Management Issues and Challenges  NASA report TM  2006-214022. February, 2006 4] Suggs, D.T., and Wade, D.R  Vibration Based Maintenance Credits for the UH-60 Oil Cooler Fan Assembly  American Helicopter Society, CBM Specialists Meeting, Huntsville, Alabama, February 13 2008 5] Baker, C., Marble, S., Morton, B.P., and Smith, B.J  Failure Modes and Prognostic Techniques for H-60 Tail Rotor Drive System Bearings  IEEEAC paper #1122 IEEE, 2007 6] Keller, J.A., Branhof, R., Dunaway, D., and Grabill, P  Examples of Condition Based Maintenance with the Vibration Management Enhancement Program   American Helicopter Society 61st Annual Forum Grapevine, Texas, June 1  3, 2005 7] Zhang, B., Sconyers, C., Byington, C.S., Patrick, R Orchard, M.E., and Vachtsevanos, G.J  Anomaly Detection: A Robust Approach to Detection of 


Detection: A Robust Approach to Detection of Unanticipated Faults  International Conference on Prognostics and Health Management, Denver, Colorado October 6-9, 2008 8] Byington, C.S., Watson, M., Lee, H., and Hollins, M  Sensor-level Fusion to Enhance Health and Usage Monitoring Systems  American Helicopter Society, 64th Annual Forum, Montreal, Canada, April 29-May 1, 2008 9] Engel, S.J., Gilmartin, B.J., Bongort, K., and Hess, A  Prognostics, the Real Issues Involved With Predicting Life Remaining  Proceedings of the IEEE Aerospace Conference, Big Sky, Montana, March 18-25, 2000 12 BIOGRAPHY Romano Patrick is a Project Manager at Impact Technologies. He received a Ph.D. in Electrical Engineering from the Georgia Institute of Technology specializing in model-based machine health diagnostics and prognostics. He also holds an MBA from Georgia Tech and degrees from U Texas, Arlington and U. Panamericana, Mexico. With career focus on interdisciplinary integration of technologies, his recent work involves practicable diagnostics/prognostics design for complex systems, such as rotorcraft drive trains Past experience includes automation and design for a variety of industrial and government sponsors \(DARPA, Lockheed Martin, Northrop Grumman, etc and program coordination at U. Panamericana, and some entrepreneurial R&amp;D Matthew J. Smith is a Senior Project Engineer at Impact Technologies. During his tenure with Impact, Matthew has performed multiple efforts pertaining to bearing vibration analysis, diagnostic and prognostic system development, and experimental study of faulted system reponse and fault progression. Previously, as a research assistant at Penn State and the NASA Glenn Research Center, Matthew performed experimental and analytical oil-free bearing analyses Matthew received his B.S. and M.S. degrees in Mechanical Engineering from The Pennsylvania State University. His research interests include: prognostic health assessment for bearing and actuator systems, grease degradation modeling and fault classifier development Bin Zhang received his Ph.D. degree from Nanyang Technological University, Singapore in 2007. He received his BE and MSE degrees from Nanjing University of Science and Technology, China, in 1993 and 1999, respectively. He is a senior member of IEEE. From 2005 to present, he has been a Post-Doc with the School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta GA His current research interests are fault diagnosis and failure prognosis, systems and control, digital signal processing learning control, intelligent systems and their applications to robotics, power electronics and various mechanical systems Carl S. Byington is a Professional Engineer and the Director of Systems Engineering at Impact Technologies. He directs R&amp;D in pursuit of advanced, automated systems health management for land-based, shipboard, and airborne machinery for military and commercial customers. He is Chairman of the Machinery Diagnostics &amp; Prognostics Committee of ASME and a member of IEEE, AIAA, SAE and AHS. He has a BS degree in Mechanical Engineering from the University of Pennsylvania and an MS in Aeronautical Engineering from George Washington University, and has published over 60 papers, book chapters, magazine and journal articles related to diagnostics and prognostics technologies George Vachtsevanos is Professor Emeritus at the Georgia Institute of Technology and also serves as the Chief Scientist at Impact Technologies, LLC. He directed the Intelligent Control Systems laboratory at Georgia Tech for the past 28 years where faculty and students are conducting research in fault diagnosis/prognosis and fault-tolerant control of engineering systems, intelligent control of industrial 


engineering systems, intelligent control of industrial processes, neurotechnology and cardiotechnology, and unmanned systems. His research work has been sponsored by government and industry and has published over 250 technical papers in his area of expertise. He is the lead author of a book on "Intelligent Fault Diagnosis and Prognosis of Engineering Systems" published by Wiley in 2006. He is the recipient of the Georgia Tech Interdisciplinary Activities award and the ECE Distinguished Professor award Romeo de la Cruz del Rosario, Jr. is the Chief of the Electronics Technology Branch at the U.S. Army Research Laboratory. He also serves as the Army Technology Objective ATO P&amp;D Operational Readiness and Condition Based Maintenance He received the B.E.E. degree from the Catholic University of America, Washington, D.C., and the M.S.E. and Ph.D degrees in Electrical and Computer Engineering from the Johns Hopkins University, Baltimore, MD. Since 1991 he has been an engineer at the Harry Diamond Laboratory then U.S Army Research Laboratory working in several areas including high power microwave technology characterization &amp; modeling of heterostructure RF devices and fabrication and failure analysis of electron devices and circuits  pre></body></html 


movies. In the case of the volume of critical reviews however, there was no big difference between mainstream and non-mainstream movies. WOM and critical reviews were usually positive H1 and H2 tested the relationship between WOM and weekly box office revenue, and the results supported the hypotheses. The volume of WOM was positively related to weekly box office revenue, while the valence of WOM had no significant effect. H3 H4a, and H4b tested the impact of critical reviews, and the results also supported the hypotheses except H4b The volume and valence of critical reviews had no consistent significances to weekly box office revenue H3 H4b. Table 7 showed that the number of critical reviews was statistically significant to aggregate box office revenue \(H4a for the attitude of critical reviews \(H4b the result more detail, an additional test was performed using only those factors related to critical reviews as independent variables. The result of the additional test supported H4, but the signs were reversed, i.e. positive critical reviews had minus signs, and negative critical reviews had plus signs. This reversed signs imply that the preference of critical reviewers is very similar to that of normal moviegoers. H5s and H6s tested the different effects of WOM and critical reviews on mainstream and non-mainstream movies. The result failed to determine that WOM give different impact on mainstream and non-mainstream movies, so H5a and H5b were rejected. H6, however, was supported, i.e the effects of critical reviews were different for mainstream and non-mainstream movies. There were no significant relationships between critical reviews and aggregate box office revenue in mainstream movies. For non-mainstream movies, however, the volume of critical reviews and the percentage of negative critical reviews were significant. Nonmainstream movies have fewer sources from which consumers can get information, and this might explain the results The above findings lead to several managerial implications. First, producers and distributors of movies could forecast weekly box office revenue by looking at previous weeks? volume of WOM. It does not matter what attitude people have when they spread WOM, the important factor is its volume. Therefore producers and distributors need to develop an appropriate strategy to manage WOM for their movies For example, the terms related to WOM marketing such as buzz and viral marketing are easily found Second, for the distributors who usually distribute less commercial and more artistic movies, and consequently have a smaller market compared to the major distributors, critical reviews can impact their movies box office revenues in a significant way. There are usually fewer sources for information for nonmainstream movies than mainstream movies, and so small efforts could leverage the outcomes. Finally, for those who are dealing with mainstream movies, the finding that the valence of WOM and critical reviews do not have significant relationship with box office revenue can have certain implications. Particularly, the attitude of critical reviews showed reversed effects Therefore, they may need to concentrate on other features rather than attitude of moviegoers or critical reviews, such as encouraging moviegoers to spread WOM This study contributes to the understanding of the motion picture industry, especially the relationship between box office revenue and WOM including critical reviews. There are existing studies that already 


critical reviews. There are existing studies that already dealt with similar issues, but this study has some differentiated features compare to prior studies. First the data used in this study was collected from South Korea, while most of the relevant studies usually focus on the North American market. This helps to provide the opportunity to understand the international market especially the Asian market, even though South Korea is a small part of it in terms of the motion picture industry. Second, movies were categorized to two groups, i.e. mainstream and non-mainstream and this study attempted to determine how WOM impacts these categories differently by testing several hypotheses In this study, there are also several limitations that could be dealt with in future research. First, using box office revenue as a dependent variable is more meaningful for distributers rather than producers. Due to there is close correlation between box office revenue and number of screens, one of producers? main concerns is how many screens their movies can be played on. Moreover, DVD sales are also important measurement for success of movies these days, and so it also could be a dependent variable. Therefore, it could be possible to give more fruitful managerial implications to various players in the motion picture industry by taking some other dependent variables Second, in this study, movies were categorized simply as mainstream and non-mainstream movies, but there could be further studies with diverse techniques of movie categorizations. For example, it would be possible to study the varying influence of WOM or critical reviews on different genres or movie budgets Third, an interesting finding of this study is that positive critical reviews could have negative relationship with box office revenue while negative critical reviews could have positive relationship. This study tried to provide a reasonable discussion on the issue, but more studies could be elaborate on it  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 References  1] Dellarocas, C., The Digitization of Word of Mouth Promise and Challenges of Online Feedback Mechanisms Management Science, 2003. 49\(10 2] Bone, P.F., Word-of-mouth effects on short-term and long-term product judgments. Journal of Business Research 1995. 32\(3 3] Swanson, S.R. and S.W. Kelley, Service recovery attributions and word-of-mouth intentions. European Journal of Marketing, 2001. 35\(1 4] Hennig-Thurau, F., et al., Electronic word-of-mouth via consumer-opinion platforms: What motivates consumers to articulate themselves on the internet? Journal of Interactive Marketing, 2004. 18\(1 5] Fong, J. and S. Burton, Electronic Word-of-Mouth: A Comparison of Stated and Revealed Behavior on Electronic Discussion Boards. Journal of Interactive Advertising, 2006 6\(2 6] Gruen, T.W., T. Osmonbekov, and A.J. Czaplewski eWOM: The impact of customer-to-customer online knowhow exchange on customer value and loyalty. Journal of Business Research, 2006. 59\(4 7] Garbarino, E. and M. Strahilevitz, Gender differences in the perceived risk of buying online and the effects of receiving a site recommendation. Journal of Business Research, 2004. 57\(7 8] Ward, J.C. and A.L. Ostrom, The Internet as information minefield: An analysis of the source and content of brand information yielded by net searches. Journal of Business Research, 2003. 56\(11 


9] Goldsmith, R.E. and D. Horowitz, Measuring Motivations for Online Opinion Seeking. Journal of Interactive Advertising, 2006. 6\(2 10] Eliashberg, J., A. Elberse, and M. Leenders, The motion picture industry: critical issues in practice, current research amp; new research directions. HBS Working Paper, 2005 11] S&amp;P, Industry surveys: Movies and home entertainment 2004 12] KNSO, Revenue of Motion Picture Industry 2004 Ministry of Culture, Sports, and Tourism, 2004 13] Duan, W., B. Gu, and A.B. Whinston, Do Online Reviews Matter? - An Empirical Investigation of Panel Data 2005, UT Austin 14] Zhang, X., C. Dellarocas, and N.F. Awad, Estimating word-of-mouth for movies: The impact of online movie reviews on box office performance, in Workshop on Information Systems and Economics \(WISE Park, MD 15] Mahajan, V., E. Muller, and R.A. Kerin, Introduction Strategy For New Products With Positive And Negative Word-Of-Mouth. Management Science, 1984. 30\(12 1389-1404 16] Moul, C.C., Measuring Word of Mouth's Impact on Theatrical Movie Admissions. Journal of Economics &amp Management Strategy, 2007. 16\(4 17] Liu, Y., Word of Mouth for Movies: Its Dynamics and Impact on Box Office Revenue. Journal of Marketing, 2006 70\(3 18] Austin, B.A., Immediate Seating: A Look at Movie Audiences. 1989, Wadsworth Publishing Company 19] Bayus, B.L., Word of Mouth: The Indirect Effects of Marketing Efforts. Journal of Advertising Research, 1985 25\(3 20] Faber, R.J., Effect of Media Advertising and Other Sources on Movie Selection. Journalism Quarterly, 1984 61\(2 21] Eliashberg, J. and S.M. Shugan, Film critics: Influencers or predictors? Journal of Marketing, 1997. 61\(2 22] Reinstein, D.A. and C.M. Snyder, The Influence Of Expert Reviews On Consumer Demand For Experience Goods: A Case Study Of Movie Critics. Journal of Industrial Economics, 2005. 53\(1 23] Gemser, G., M. Van Oostrum, and M. Leenders, The impact of film reviews on the box office performance of art house versus mainstream motion pictures. Journal of Cultural Economics, 2007. 31\(1 24] Wijnberg, N.M. and G. Gemser, Adding Value to Innovation: Impressionism and the Transformation of the Selection System in Visual Arts. Organization Science, 2000 11\(3 25] De Vany, A. and W.D. Walls, Bose-Einstein Dynamics and Adaptive Contracting in the Motion Picture Industry Economic Journal, 1996. 106\(439 26] Bagella, M. and L. Becchetti, The Determinants of Motion Picture Box Office Performance: Evidence from Movies Produced in Italy. Journal of Cultural Economics 1999. 23\(4 27] Basuroy, S., K.K. Desai, and D. Talukdar, An Empirical Investigation of Signaling in the Motion Picture Industry Journal of Marketing Research \(JMR 2 295 28] Neelamegham, R. and D. Jain, Consumer Choice Process for Experience Goods: An Econometric Model and Analysis. Journal of Marketing Research \(JMR 3 p. 373-386 29] Lovell, G., Movies and manipulation: How studios punish critics. Columbia Journalism Review, 1997. 35\(5 30] Thompson, K., Film Art: An Introduction. 2001 McGraw Hill, New York 31] Zuckerman, E.W. and T.Y. Kim, The critical trade-off identity assignment and box-office success in the feature film industry. Industrial and Corporate Change, 2003. 12\(1 


industry. Industrial and Corporate Change, 2003. 12\(1 27-67 32] KOFIC, Annual Report of Film Industry in Korea 2006 Korean Film Council, 2006 33] Sutton, S., Predicting and Explaining Intentions and Behavior: How Well Are We Doing? Journal of Applied Social Psychology, 1998. 28\(15 34] Basuroy, S., S. Chatterjee, and S.A. Ravid, How Critical Are Critical Reviews? The Box Office Effects of Film Critics Star Power, and Budgets. Journal of Marketing, 2003. 67\(4 p. 103-117  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 





