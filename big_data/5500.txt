Mining Perfectly Sporadic Rules with Two Thresholds  Cu Thu Thuy Faculty of Economic Information System Academy of Finance Ha noi, Viet Nam E-mail: cuthuthuy@hvtc.edu.vn Do Van Thanh Center for Scio-Economic Information and Forecast Ministry of Planing and Investment Ha noi, Viet Nam E-mail: thanhdv_db@mpi. gov.vn   AbstractA sporadic rule is an association rule which has low support but high confidence. It is divided into two types perfectly and imperfectly sporadic rules. In this paper, we describe an efficient algorithm to mine perfectly sporadic rules by proposing a problem of mining perfectly sporadic rules  with two thresholds and developing a MCPSI \(mining closed perfectly sporadic itemsets with two thresholds Unlike the previous approaches, the development of MCPSI algorithm is based on the pruning of the closed itemset lattice therefore efficiency of the algorithm can be improved via reducing a search space and removing redundant imperfectly sporadic rules with two thresholds. Experiments comparing MCPSI to Apriori-Inverse on the same databases also proved this conclusion Keywords: Rare Association Rule; Perfectly Sporadic Rule Perfectly Sporadic Rule with Two Thresholds I.  INTRODUCTION A sporadic rule is an association rule which has low support but high confidence \(named rare association rule rarely occur but they are very valuable in many cases [3-6 Koh, Rountree, and OKeefe [3-4] have splited sporadic rules into two types including perfectly sporadic and imperfectly sporadic An association rule BA ?  is called a perfectly sporadic rule for maxSup and minConf if  


     maxSup sup maxSup, B A inConf conf  xBAx mBA  An association rule BA ?  is called an imperfectly sporadic rule for maxSup and minConf if       maxSup sup maxSup, B A inConf conf  xBAx mBA In [3], authors have developed the Apriori-Inverse algorithm derived from the Apriori algorithm to discover perfectly sporadic rules. As having reasonable complication level compared with other frequent itemset finding algorithms Apriori and Apriori-Inverse thereafter might not be an efficient algorithm to mine perfectly sporadic rules. Thus, in this paper we introduce a more efficient algorithm to mine perfectly sporadic rules by developing mining sporadic rules as follows       maxSup sup maxSup, B A minSup 


inConf conf  xBAx mBA  Where minSup, maxSup \(minSup < maxSup below minimum support, above minimum support and minimum confidence respectively. The three thresholds are user-defined values. In this paper, such rules are called perfectly sporadic rules with two thresholds minSup is needed to be added by reasons that support of every association rules is always positive and it supports to develop a new algorithm for finding perfectly sporadic itemsets To minimise itemsets with too low support, Apriori-Inverse algorithm in [3] only finds perfectly sporadic itemsets whose supports are not less than minAS \(minimum absolute support The value of minAS depends on specific databases. By adding below minimum support minSup, we can conclude that the mining perfectly sporadic rules can be considered as a specific case of the above mining of perfectly sporadic rules with two thresholds when minSup is chosen equal to minAS Unlike the approach in [3], the MCPSI algorithm for finding perfectly sporadic itemsets with two thresholds was developed under the approach of the CHARM algorithm [8 which is one of the most effective algorithms for finding frequent itemsets from transaction databases. The CHARM algorithm is implemented on the closed itemsets lattice CHARM explores both itemset space and tidset space, and uses depth-first exploration of the Itemset-Tidset tree to find all the closed frequent itemsets that included maximal closed frequent itemsets. Like CHARM the MCPSI is also implemented on the closed itemsets lattice, therefore its search space for perfectly sporadic itemsets is reduced  and many redundant perfectly sporadic rules are removed Experimental results also show the efficiency of MCPSI algorithm in term of comparison with Apriori-Inverse on the same synthetic and real databases. In all experiments, the 978-1-4244-5326-9/10/$26.00 2010 IEEE running times of the MCPSI algorithm is less than ones of the Apriori-Inverse algorithm, and the set of closed perfectly sporadic itemsets with two thresholds is smaller than one of 


perfectly sporadic itemsets The structure of the paper is as follows: Following the Introduction Section, Section 2 will discuss some important properties of perfectly sporadic itemsets with two thresholds This will serve as the base to propose an algorithm to find closed perfectly sporadic itemsets with two thresholds in the next section. Section 3 will present the algorithm and prove its soundness and completeness. Section 4 followed will introduce some experiments to evaluate performances of the proposed algorithm, and experiments to compare the MCPSI algorithm with the Apriori-Inverse algorithm. Some conclusions will be raised in Section 5 II. PERFECTLY SPORADIC ITEMSET WITH TWO THRESHOLDS Suppose that D ? \(O, I universe of items, and O = {t1, t2,..., tm} is the universe of transactions, called a transaction database. Let X ? I, sup\(X be the support of X, that is a number \(or percentage transactions in D containing X. An association rule is a conditional relation among itemsets X  Y, where X ? I, Y I, X? Y = ?. X is referred to as an antecedent of the rule and Y as a consequent. The confidence of an association rule conf\(X  Y or percentage containing X, given that they also contain Y [1,2]. Let minSup maxSup, minConf be below minimum support, above minimum support, and minimum confidence respectively, these values are user-determined with a range in \(0, 1 Definition 1. X is called a perfectly sporadic itemset with two thresholds if minSup ? sup\(X x ? X, sup\(x X is called a maximal perfectly sporadic itemset with two thresholds if it is not a subitemset of any perfectly sporadic itemset with two thresholds The following definitions and Property 1 are developed directly from the related definitions and properties in [7 Definition 2. \(Data mining context is a triple D  = \(O, INF, R INF is a set of items which are infrequent for maxSup but are frequent for minSup in I, and R ? OxINF is a binary relation Each couple \(t,i related to the item i ? INF Definition 3. \(Galois connection O, INF, R 


a data mining context. For O ? O and I ? INF, we define f:  2O  2INF f\(O t,i g:  2INF  2O g\(I t,i f\(O and g\(I couple of applications \(f,g power set of O and the power set of INF The operator h = fog  in  2INF , and h = gof  in 2O  are Galois closure operators Property 1. \(Galois connection and Galois closure operator 1 I1 I2 1 O1 O2 2 I 2 O 3 h\(I I 3 h\(O O 4 I1 I2 4 O1 O2 5 g\(I I 5 f\(O O 6 I O Property 1 is similar to the relative properties in [7]. The proof of this property is quite simple therefore not mentioned here Definition 4. Let X be a perfectly sporadic itemset with two thresholds, X is called a closed perfectly sporadic itemset with two thresholds if it is a closed itemset, i.e., h\(X where h is the Galois connection defined above Property 2. Perfectly sporadic itemsets with two thresholds have the Apriori property, i.e., subset of a perfectly sporadic itemset with two thresholds is a perfectly sporadic itemset with two thresholds Proof. Asume that X is a perfectly sporadic itemset with two thresholds and X ? X, we have to prove that X is a perfectly sporadic itemset with two thresholds Since X ? X  => minSup ? sup\(X X hand, for all x ? X => x ? X, so sup\(x sup\(X x perfectly sporadic itemset with two thresholds From this property we have: all supersets of X which is not a perfectly sporadic itemset with two thresholds is not a perfectly sporadic itemset with two thresholds Remark When minSup = O 


1 where O  is the number of transactions of D  then mining perfectly sporadic rules with two thresholds becomes mining perfectly sporadic rules defined in [3]. When minSup = minAS, where minAS is determined in the Apriori-Inverse algorithm then mining perfectly sporadic rules with two thresholds become mining perfectly sporadic rules specified in the Apriori-Inverse algorithm According to the Definition 1, a perfectly sporadic itemset with two thresholds is an infrequent itemset for the above minimum support maxSup but is a frequent itemset for the below minimum support minSup. In accordance to the Definition 4, a closed perfectly sporadic itemset with two thresholds is a closed frequent itemset for support minSup Property 3. The support of a perfectly sporadic itemset with two thresholds is equal to the support of a smallest closed itemset containing it, i.e. sup\(X h\(X The proof of Property 3 is similar to the proof of related property in [7] so it is not mentioned here Property 4. If X is a maximal imperfectly sporadic itemset with two thresholds then X is a closed itemset Proof. Assume that X is a maximal perfectly sporadic itemset with two thresholds. According to the Property 1 - \(2 we have X ? h\(X perfectly sporadic itemset with two thresholds so that  minSup sup\(h\(X X i On the other hand, for all x ? h\(X x obvious because h\(X ii From \(i ii X itemset with two thresholds contained X. Since X is a maximal perfectly sporadic itemset with two thresholds so X = h\(X Property 5. There is no difference between the association rules generated from perfectly sporadic itemsets with two thresholds and those generated from maximal perfectly sporadic itemsets with two thresholds Proof. We only need to prove that each perfectly sporadic rule with two thresholds can always be generated from a maximal perfectly sporadic itemset with two thresholds Let A  B be such rule, then A?B is a perfectly sporadic itemset with two thresholds and A  B is an association rule 


for the minimum support minSup and the minimum confidence minConf. According to [7], A  B is generated from a maximal frequent itemset for minSup Without losing generality, we can assume that A ? B is a maximal frequent itemset for minSup, and we will prove that A?B is a maximal perfectly sporadic itemset with two thresholds If it is not the case then ?C ? A?B, C is a maximal perfectly sporadic itemset with two thresholds, hence it implies that minSup ? sup\(C A?B maximal frequent itemset for minSup containing A?B. This contradicts the above assumption about A?B These properties above are the basis for developing an algorithm in the section below III. THE MCPSI ALGORITHM A. MCPSI overview The MCPSI algorithm is developed under the approach of the CHARM algorithm [8]. It finds perfectly sporadic itemsets with two thresholds by: Based on the search space including items that are frequent for below minimum support minSup but infrequent for above minimum support maxSup, the MCPSI algorithm finds closed frequent itemsets for minSup using depth-first exploration of the Itemset-Tidset tree likes the CHARM algorithm [8]. All itemsets, which are not perfectly sporadic itemsets with two thresholds or not closed, will be removed by applying one of  the four Itemset x Tidset pairs as follows Assume that I1 x g\(I1 g is the Galois connection defined above I1 combined with  I2 x g\(I2 level in the tree, then there are four following situations If g\(I1 I2 I1 ? I2 I1 I2 implies that we can replace every occurrence of I1 by I1 ? I2 and g\(I1 I1 ? I2 father consideration If g\(I1 I2 I1 ? I2 I1 I2 I1 I2 Thus, we can replace every occurrence of I1 by I1 ? I2, but since g\(I1 I2 If g\(I1 I2 I1 ? I2 I1 I2 I2 I1 Thus, we can replace every occurrence of I2 by  I1 ? I2, and  I1 will be kept If g\(I1 I2 I1 ? I2 I1 I2 I2 I1 


In this case, no itemset can be eliminated; both I1 and I2 lead to different closed itemsets In short, MCPSI algorithm can be briefly described as follows It is started by creating a set of items and a set of tidsets of data mining context D . These items are infrequent for maxSup but frequent for minSup MCPSI-EXTEND function returns the set of closed perfectly sporadic itemsets with two thresholds CHARM-PROPERTY function tests the support of itemsets for minSup and check whether itemset-tidset pairs sastify the four properties of the itemset-tidset above B. MCPSI algorithm MCPSI ALGORITHM\(D ? I x O, minSup, maxSup 1. Nodes = {Ij x g\(Ij Ij Ij minSup 2. MCPSI-EXTEND\(Nodes, C  MCPSI-EXTEND\(Nodes, C 3. for each Xi x g\(Xi 4.    NewN = ? and X = Xi 5.    for each Xj x g\(Xj j i function for sorting items in Nodes 6.         X = X ? Xj and Y = g\(Xi Xj 7.         CHARM-PROPERTY\(Nodes, NewN 8.    if  NewN ? ?  then MCPSI-EXTEND\(NewN 9.    C = C ? X  // if X is not subsumed  CHARM-PROPERTY was developed as in [8 Proposition 1. MCPSI algorithm is sound Proof. We need to prove that every itemsets found out by the MCPSI algorithm is closed perfectly sporadic itemsets with two thresholds. In fact the MCPSI comprises two stages In the first stage \(line 1 frequent itemsets with two thresholds are created. All the items in the search space are ordered In the second stage \(line 2 Nodes,C function is called. This function will find closed frequent itemsets for minSup but infrequent for maxSup in Nodes by applying the approach of the CHARM-EXTEND function in 8]. CHARM-PROPERTY function tests the support of 


itemsets for minSup and check whether itemset-tidset pairs sastify the four properties of the itemset-tidset above. Thus MCPSI-EXTEND returns the set of itemsets called C containing closed itemsets whose support is equal to or greater than minSup and smaller than maxSup. Hence these itemsets are closed perfectly sporadic itemsets with two thresholds according to the Definition 1 Proposition 2. MCPSI algorithm is complete Proof. We need to show that every perfectly sporadic rule with two thresholds is generated from a sporadic itemset found by this algorithm Obviously, according to the Property 5, a perfectly sporadic rule with two thresholds is generated from a maximal perfectly sporadic itemset with two thresholds and according to the Property 4, this itemset is a maximal closed perfectly sporadic itemset with two thresholds. The MCPSI algorithm finds such itemsets IV. EXPERIMENTAL RESULS In order to evaluate the performance of the MCPSI algorithm, we compared the performance of MCPSI algorithm with that of Apriori-Inverse algorithm in [3] for finding out perfectly sporadic itemsets with two thresholds from several synthetic and real databases. Real databases are available from 10]. All experiments were performed on a Lenovo-IBM Codual 2.0ghz with 2GB of memory, running Windows Vista MCPSI and Apriori-Inverse were coded in C A. Experiment on synthetic database The purpose of this experiment is to evaluate the performance of the MCPSI algorithm over a large range of data characteristics. We generated synthetic databases based on the principle proposed by Agrawal R., and Srikant R. [1,2 The synthetic databases simulate transactions in the retailing environment with some defined parameters. To generate the synthetic databases, we takes the following parameters: |D| is the number of transactions, |T| is the average size of the transactions, |L| is the number of frequent itemset and |I| is the number of items. The first step, the size of the next transaction is picked from a Poisson distribution with the mean set to the average size of the transactions. Then we fill the transactions with items. Each transaction is assigned a series of potentially frequent itemsets. The complete detail can be found in [1,2]. Table I shows the characteristics of the synthetic 


databases TABLE I.  THE CHARACTERISTICS OF THE SYNTHETIC DATABASES No Database of Items I of Transactions D The average size of the transaction \(T 1 T5I1000D10K 1 000 10 000 5 2 T10I1000D10K 1 000 10 000 10 3 T15I1000D10K 1 000 10 000 15 4 T20I1000D10K 1 000 10 000 20 5 T25I1000D10K 1 000 10 000 25 6 T30I1000D10K 1 000 10 000 30 To compare the performance of MCPSI algorithm with that of Apriori-Inverse algorithm, we produce  two programs to simulate these algorithms. Table II shows the performance of the MCPSI algorithm for finding closed perfectly sporadic itemsets with two thresholds, and the performance of the Apriori-Inverse algorithm for finding perfectly sporadic itemsets in the same synthetic databases with minSup and maxSup are appropriately chosen, which minSup = minAS. As we know, when minSup = minAS the mining perfectly sporadic itemsets with two thresholds will become the mining perfectly sporadic itemsets according to the Apriori-Inverse approach   TABLE II.  PERFORMANCE OF THE MCPSI AND APRIORI-INVERSE ALGORITHM ON THE SYNTHETIC DATABASES No Database minSup maxSup Apriori-Inverse Algorithm MCPSI Algorithm of perfectly sporadic Time \(sec sporadic Time \(sec 1 T5I1000D10K 0.0005 0.01 3588 67.695 1767 62.015 2 T10I1000D10K 0.0005 0.01 1696 38.691 1272 37.928 3 T15I1000D10K 0.0005 0.01 955 23.917 846 22.681 


4 T20I1000D10K 0.0005 0.01 610 15.614 576 14.890 5 T25I1000D10K 0.0005 0.01 416 10.463 397 9.688 6 T30I1000D10K 0.0005 0.01 347 8.048 334 7.627 TABLE III.  PERFORMANCE OF THE MCPSI AND APRIORI-INVERSE ALGORITHM ON THE T5I1000D10K DATABASE minSup maxSup Apriori-Inverse Algorithm MCPSI Algorithm of perfectly sporadic Time \(sec sec 0.0005 0.01 3588 67.695 1767 62.015 0.001 0.01 757 50.388 714 47.899 0.005 0.01 224 6.865 224 6.585 0.001 0.1 1702 78.553 1438 75.256 0.005 0.1 374 20.492 374 19.787 0.01 0.1 152 4.435 152 4.321 TABLE IV.  PERFORMANCE OF THE MCPSI AND APRIORI-INVERSE ALGORITHM ON THE REAL DATABASES Database of Items I of Transactions D minSup maxSup Apriori-Inverse Algorithm MCPSI Algorithm of perfectly sporadic Time sec of closed perfectly sporadic Time sec Soybean 76 47 0.1 0.5 4275 2.246 154 0.312 Zoo 43 101 0.1 0.5 203 0.187 56 0.094 Bridge 220 108 0.1 0.5 63 0.074 42 0.104 Teaching AE 104 151 0.1 0.5 13 0.031 12 0.015 Flag 310 194 0.1 0.5 235 0.515 210 0.443 Mushroom 118 8124 0.1 0.5 1273 43.028 387 34.336 Experimental results show that the MCPSI algorithm is more effective than Apriori-Inverse algorithm not only in terms of running time but also in terms of the number of closed 


perfectly sporadic itemsets with two thresholds found. For visualizing correlations between the closed perfectly sporadic itemsets with two thresholds and perfectly sporadic itemsets found in the synthetic databases, the Fig. 1 is presented in following graphs 0 500 1000 1500 2000 2500 3000 3500 4000 T5I1000D10K T10I1000D10K T15I1000D10K T20I1000D10K T25I1000D10K T30I1000D10K of perfectly sporadic # of closed perfectly sporadic  Figure 1. Number of closed perfectly sporadic itemsets with two thresholds and perfectly sporadic itemsets found on the synthetic databases For the T5I1000D10K database, Table III shows the performance of the MCPSI algorithm for finding closed perfectly sporadic itemsets with two thresholds, and the performance of the Apriori-Inverse algorithm for finding perfectly sporadic itemsets with appropriate minSup and maxSup When we have done two algorithms many time with different minSup and maxSup values on the T5I1000D10K database appeared cases, in which the number of perfectly sporadic itemsets are equal to the number of closed perfectly sporadic itemsets \(when minSup = 0.005, maxSup = 0.01 minSup = 0.005, maxSup = 0.1; minSup = 0.01, maxSup  0.1 but  the running time of MCPSI  algorithm is less than the running time of Apriori-Inverse algorithm B. Experiment on real database We chose six real databases from the UCI Machine Learning Repository [10] and converted them to transaction databases. Table IV  shows the characteristics of the databases and the result of the MCPSI and Apriori-Inverse algorithms The experimental results show that MCPSI is better than Apriori-Inverse not only on the synthetic databases but also on the real databases V. CONCLUTIONS 


The paper has proposed a more effective algorithm MCPSI sporadic rules by proposing  problem for mining perfectly sporadic rules with two thresholds where the finding of perfectly sporadic rules in [3] becomes a special case of this problem MCPSI algorithm was developed from CHARM algorithm 8]. The soundness and completeness of MCPSI has been proved in the paper. In other says, MCPSI was developed based on Galois connection of the closed itemsets therefore narrowing down the search space of perfectly sporadic rules with two thresholds and removing redundant perfectly sporadic rules with two thresholds. This conclusion has been supported by experiments of MCPSI and Apriori-Inverse algorithms on the same transaction databases. MCPSI algorithm has shorter running time and find less perfectly sporadic rules with two thresholds compared to Apriori-Inverse algorithm, meaning that MCPSI algorithm is more effective and remove more redundant perfectly sporadic rules with two thresholds The complete set of rare association rules is often huge Hence, a question for our future research is that we need to find ways to generate useful perfectly sporadic rules with two thresholds from closed perfectly sporadic itemsets with two thresholds REFERENCES 1] Agrawal R., and Srikant R., Fast Algorithms for Mining Association Rules, Proc. Very Large Database International Conference, Santiago pp. 487498, 1994 2] Agrawal R., Mannila H., Srikant R., Toivonen H., and Inkeri Verkamo A., Fast Discovery of Association Rules, Advances in Knowledge discovery and DataMining, The MIT Press, pp.307-328,1996 3] Koh Y. S., and Rountree N., Finding Sporadic Rules Using AprioriInverse, PAKDD 2005, LNAI 3518, pp 97-106, 2005 4] Koh Y. S., Rountree N.,  and OKeefe R. A., Mining Interesting Imperfectly Sporadic Rules, Knowledge and Information System;,14\(2 5] Kiran R. U., and Reddy P. K., An Improved Multiple Minimum Support Based Approach to Mine Rare Association Rules http://www.iiit.net/techreports/2009_24.pdf 6] Ling Zhou, and Stephen Yau, Association Rule and Quantitative Association Rule Mining among Infrequent Items, MDM07, August 12, 2007, San Jose, California, USA 


7] Pasquier N., Bastide Y.,  Taouil R., and Lakhal L., Efficient Mining of Association Rules Using Closed Itemset Latics, Information Systems Vol 24, No. 1, pp. 20-46, 1999 8] Zaki M. J., and Hsiao C.: CHARM, An Efficient Algorithm for Closed Association Rule Mining, In Proceedings, SIAM-02 International Conference on Data Mining, 2002 9] Zaki M. J., Mining Non-Redundant Association Rules, Data Mining and Knowledge Discovery, 9, 223248, 2004 10] UCI-Machine Learning Repository http://archive.ics.uci.edu/ml/datasets.html 


dataset contains a stream of TCP connection records from two weeks of LAN traffic over MIT Lincoln Labs. It consists of 42 attributes that usually characterize network traffic behavior, both categorical attributes and quantitative attributes such as duration of the connections, protocol type etc. Attribute src_byte denoting the number of data bytes from source to destination and attribute dst_bytes inverse are selected in this experiment.  They are both quantitative attributes The user specified parameters are set as follows 5, 0.3, 0.03, 0.01,W min_sup preMinsup T 0.5, 0.4, 30.min_confidence max_MFB min_num_triples And the number of transactions in each time slot is 250 It is assumed that there are no more than three fuzzy sets or intervals in the datasets i.e. 3F Four different approaches to mine association rules are compared using the following notations: Fuzzy+MFB: the approach that use both fuzzy method and MFB_measure with 1 3.0? = and 2 0 .5 2 0.5? = , Fuzzy+P: the approach using fuzzy method with 1 3.0? = and 2 0.5? = also but repressing the first part of the MFB_measure that ignores the changed rate of the membership function, Discrete1: the approach using discrete method with 1 2 1.5? ?= =  and Discrete2 : the approach using discrete method with 1 2 2.5 TABLE I. RESULT OF EXPERIMENT ONE NUM_C NUM_FI NUM_RULE TIME\(s 1 \(50,50 2 \(60,60 3 \(70,70 4 \(80,80 5 \(90,90 6 \(100,100 7 \(110,110 8 \(120,120 9 \(130,130 10 \(140,140 Volume 4] 2010 2nd International Conference on Computer Engineering and Technology V4-157 0 0.5 1 1.5 2 x 104 


0 200 400 600 800 1000 1200 1400 Size of Databases\(250 Ex ec u tio n Ti m e\(s ec  Fuzzy+MFB Discrete1 Discrete2 Fuzzy+P Figure 3. Comparison of Execution Time Fig. 3 shows the execution time of the four approaches The runtimes of them grow linearly as the data stream grows which confirms that they are scalable with respect to the size of data stream, and it is mainly because of the usage of sliding window model. Fuzzy+P uses the least time Fuzzy+MFB has similar execution time to Discrete2, and Discrete1 has the most execution time. The difference of runtimes between them is mainly influenced by the clustering operations they use. The more clustering operations were executed, the more runtime it was Fig. 4 and Fig. 5 show the number of frequent itemsets and interesting rules found with the data stream increased Fuzzy+P and Fuzzy+MFB used less clustering operations than Discrete1 and Discrete2. Fuzzy+MFB nearly finds the most number of frequent fuzzy sets and interesting rules with the second least of clustering operations. Sometimes Discrete2 returns nearly the same number of fuzzy sets and interesting rules but with more clustering operations and the 


semantics of Discrete2 are meaningless as discussed in experiment one. Furthermore, the number of interesting rules found by Discrete1 is even less than Fuzzy+P that used the clustering operations least which illustrates the superiority of the method using fuzzy sets V. CONCLUSIONS In this paper, a novel fuzzy ARM algorithm called FFI_Stream is presented to tackle quantitative attributes in data streams and some techniques are proposed in the algorithm. Both synthetic and real datasets are used to evaluate the performance of the proposed algorithm. The experimental results show both the effectiveness and efficiency of the proposed algorithm.  In comparison with the discrete method, the proposed algorithm using fuzzy sets and MFB_measure gets a trade-off between the number of interesting rules and efficiency ACKNOWLEDGMENT This work is supported by The National High Technology Research and Development Program of China 863 Program 2008AA042902 Technology Research and Development Program of China 863 Program 2009AA04Z162 Project \(B07031 0 500 1000 1500 2000 2500 3000 3500 4000 0 2000 4000 6000 8000 10000 12000 Size of Databases\(250 N um be r o f F re qu e n t I 


te m se ts Fuzzy+MFB Discrete2 Discrete1 Fuzzy+P Clustering Operation Figure 4. Number of Frequent Itemsets 0 500 1000 1500 2000 2500 3000 3500 4000 0 1000 2000 3000 4000 5000 6000 7000 8000 Size of Databases\(250 Nu m be r o f I n te re st in g Ru le s Fuzzy+MFB Discrete2 Discrete1 Fuzzy+P Clustering Operation Figure 5. Number of Interesting Rules REFERENCE 1] R. Srikant and R. Agrawal, Mining Quantitative Association Rules 


in Large Relational Talbes, Proc. ACM SIGMOD, 1996, pp. 1-12 2] A.W. Fu et al. Finding Fuzzy Sets for the Mining of Fuzzy Association Rules for Numerical Attributes, In Proceedings of the First International Symposium on Intelligent Data Engineering and Learning \(IDEAL'98 3] C. M. Kuok, A. Fu and M. H . Wong, Fuzzy Association Rules in Large Databases with Quantitative Attributes, In ACM SIGMOD Records, vol. 27, 1998, pp. 41-46 4] X. Dang, V. Lee, W. K. Ng and K.L Ong, Incremental and Adaptive Clustering Stream Data over Sliding Window, Database and Expert Systems Applications, vol. 5690, 2009, pp. 660-674 5] M. Kaya,?R. Alhajj, F. Polat, and A. Arslan, Efficient Automated Mining of Fuzzy Association Rules, Database and Expert System Applicaton, vol. 2453, 2002, pp.133-142 6] http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html 7] S. Guha, A. N. Mishra, R. Motwani, L. OCallaghan, Clustering Data Streams: Theory and Practice,  Proc. IEEE Transactions on Knowledge and Data Engineering, vol. 15, May/Jun. 2003, pp. 515528 8] C. Aggarwal, J. Han, J. Wang, P. Yu, A Framework for Clustering Evolving Data Streams,  Proc. VLDB Conference, 2003,  pp. 81-92 9] C. K. S. Leung, B. Y. Hao, Mining of Frequent Itemsets from Streams of Uncertain Data,  Proc. IEEE International Conference on Data Engineering \(ICDE 09 10] C. C. Aggarwal, Y. Li, J. Y. Wang, and J. Wang, Frequent Pattern Mining with Uncertain Data, Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Jun. 2009, pp 29-37 11] P. M. Tsai, Mining Frequent Itemsets in Data Streams using the Weighted Sliding Window Model, Expert Systems and Applications vol. 36, Nov. 2009, pp.11617-11625 V4-158 2010 2nd International Conference on Computer Engineering and Technology [Volume 4 


In all charts reported in this section, the X-axis is k, which denotes the size of sample under the space of a target rule drawn from deep web. The sample size for each point on X-axis is k x, where x is a ?xed value for our experiment, and depends upon the dataset. At each time, queries are issued to obtain kx data records under the space of a target rule. Overall, all our experiments show the variance of estimation, sampling costs and sampling accuracy with varying sample size Figure 1 shows the result from our strati?ed sample methods on the US census data set. The size of pilot sample is 2000, from which all of the 50 initial rules are derived. In this experiment the ?xed value x is set to be 300, which means the smallest sample size at k = 1 is 300, and the largest sample size at k 10 is 3000. Figure 1 a the ?ve sampling procedures. Figure 1 b cost for the sampling procedures. In order to better illustrate the experiment result, in each execution of sampling, the variance of 330 6DPSOLQJ9DULDQFH            9D UL DQ FH R I V WL PD WL RQ  


9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW           6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF            5 


 9DU 9DU 9DU 5DQG c Fig. 1. Evaluation of Sampling Methods for Association Rule Mining on US Census Dataset estimation and sampling cost for the sampling procedures var7 var5, var3, and rand are normalized by the corresponding values of Full Var. Thus, in our experiment, the values of sampling cost and variance of estimation for sampling procedure Full Var are all 1. Furthermore, Figure 1 c sampling procedures From Figure 1 a pared with sampling procedures Var7, Var5 and Var3, Full Var has the lowest estimation variance and the highest sampling cost. From sampling procedures Var7, Var5, and Var3, we can see a pattern that the variance of estimation increases, and the sampling cost decreases consistently with the decrease of the weight for variance of estimation. At the largest sample size of k = 10, the estimation variance of sampling procedure Var3 is increased by 27% and the sampling cost is decreased by 40 compared with sampling procedure Full Var. The experiment shows that our method decreases the sampling cost ef?ciently by trading off a percent of variance of estimation. Similar to variance of estimation, the sampling accuracy of these procedures also decreases with the decrease of the weight on variance of estimation. For the largest sample size at k = 10, we can see that the AER of sampling procedure Var3 is increased by 20 compared with sampling procedure Full Var. However, for many users, increase of the AER will be acceptable, since the sampling cost is decreased by 40%. By setting the weights for sampling variance and sampling cost, users would be able to control the trade-off between the variance of estimation, sampling cost, and estimation accuracy In addition, compared with sampling procedure of Full Var Var7, Var5, and Var3, sampling procedure Random, has higher estimation of variance, sampling cost and lower estimation accuracy. Thus, our approach clearly results in more effective methods than using simple random sampling for data mining on the deep web Figure 2 shows the experiment result of our proposed strati?ed 


sampling methods on the Yahoo! data set. The size of pilot sample on this data set is 2,000, and the ?xed value x for sample size is 200. The results are similar to those from the US census dataset. We can still see the pattern of the variance of estimation increasing with the decrease of its weight. Besides, the sampling accuracy is also similar to the variance of estimation. However although the variance estimation of sampling procedure Random is 60% larger than sampling procedure Full Var, the sampling cost of Random is 2% smaller than Full Var. This is because Full Var does not consider sampling cost. It is possible that Full Var assigns a large sample to a stratum with low ?, which denotes the probability of containing data records under the space of A = a, resulting the larger sampling cost than that of simple random sampling. Sampling procedures Var7, Var5, Var3 consider sampling cost as well, and have smaller variance estimation and sampling cost, compared with Random. Furthermore, Random has smaller sampling accuracy than Full Var, Var7 and Var5, but has larger sampling accuracy than Var3. This is because Var3 assigns much more importance to the sampling cost, and loses accuracy to a large extent To summarize, our results shows that our proposed strati?ed sampling are clearly more effective than simple random sampling on the deep web. Moreover, our approach allows users to tradeoff variance of estimation and sampling accuracy to some extent while achieving a large reduction in sampling costs B. Differential Rule Mining In this section, we present results from experiments based on differential rule mining. Particularly, we look at the rules of the form A = a ? D1\(t t categorical attribute and t is an output numerical attribute, while other categorical attributes in the data set are considered as input attributes In this experiment, we also evaluate our proposed method with different weights assigned to variance of estimation and sampling cost. Five sampling procedures, Full Var, Var7, Var5,Var3 and Random, have same meanings with those in the experiments of association rule mining. Similarly, 50 rules are randomly selected from the datasets, and each of the 50 differential rules are reprocessed 100 times using 100 different \(pilot sample, sample iterations 5000 runs First, we evaluated the performance of these procedures on 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


