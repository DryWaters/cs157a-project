Intêl Conf. on Computer Communication Technology  ___________________________________ 978-1-4244-9034-/10/$26.00©2010 IEEE 431  A Comparative Study of Four Feature Selecti on Methods for Associative Classifiers Kavita Das SoS in Computer Science and I.T Pt. Ravishankar Shukla University Raipur, C.G., India mskavitadas@gmail.com O. P. Vyas Professor & Program coordinator \(S/W Engg Indian Institute of Information Technology Allahabad, U. P., India dropvyas@gmail.com Abstract  Feature Selection is a preprocessing step that has optimization effect in data mini ng. The feature set of a dataset generally contains redundant or irrelevant features in order to avoid the risk of incomplete des cription of instances and to provide utility to different purpo ses of the dataset. This may lead to an inefficient Classific ation rule mining process that bears with memory and time overhead. Recently developed Associative Classifiers like CBA CMAR and CPAR are almost equal in accuracy and have outperformed traditional classifiers. CPAR has been found to be most consistently generating results with good average accuracy. So, it is selected to compare the suitability of four popular feature selection methods: GGA, SSGA, LVW and MIFS for classification of data.  The Genetic algorithm is found to be the most suitable Keywordsfeature selection associative classification; CBA CMAR; CPAR; GGA; SSGA LVW; MIFS I I NTRODUCTION Classification is a Machine Learning technique that requires datasets containing many instances. Classification approach uses a dataset in two parts as training set and test set. In training set, each instance is defined with many features \(attributes\ class label. It is used for building the classifier. The test set is us ed to test performance of the classifier. A dataset may contain many impurities that affect the efficiency of a classifier Hence, it requires preprocessing like filtering, transfor ming, summarizing, etc Feature Selection is method of filtering a dataset in which a subset of features or attributes are selected out of original feature set for reducing the dataset to make it more relevant to the concept of classification. While creating a dataset relevant features are generally unknown a priori. There may be many candidate features that represent the domain. Hence the feature set may contain many partially or completely irrelevant attributes and redundant attributes. Such features increase the size of dataset and interfere in the efficiency and accuracy of classifier. It also leads to processing cycle and time overhead. It also compels generation of large sized test sets Feature Selection is important both to speedup learning and to improve concept quality Irrelevant features degrade the performance of concept l earners both in speed \(due to due to irrelevant information\, especially in large dataset. Feature selection methods use various ap proaches to select a feature and test its relevancy to the target concept. Generally a features is taken as irrelevant if on removing it, the classification accuracy does not decrease significantly or if the resulting class distribution is nearly same as the original class distribution in the test set. A general Feature Selection process is shown in figure 1 Original Feature set Subset Feature subset Evaluation for  Generation                                                    irrelevance or redundancy  Goodness of  the subset  Stopping  No  Criteria    Yes   Validation w.r.t  other methods Figure 1.    Feature Selection process Broadly, Feature Selection ap proaches can be categorized as follows Feature Selection Approach Complete                   Heuristic                        Random  Filter                             Wrapper A Feature Selection approach is called complete if the search space is completely search ed for an optimal subset. It may be an exhaustive search Heuristic approach searches for a nearly best solution in incremental manner based on 


Intêl Conf. on Computer Communication Technology  432 some simple criteria. Hence they are fast in producing results Random approach selects candidates based on some parameter values and its optimality depends on availability of the resources Filter approaches obtain f eature subset by maximizing or minimizing a criteria function. They do not take support of any induction algorithm Wrapper approaches are feedback orie nted and evaluate feature selection using a learning algorithm in terms of predictive accuracy. The time required depends on complexity of learning algorithm for fitness evaluation Feature Selection methods have been studied and compared with traditional classification systems, but still they have not been tested on higher ended Associative Classification systems. In this work, Associative Classification approaches CB   CMAR [7 and CP AR  ha ve been st udi ed an d CP AR has been c hosen t o t e st t h e accuracy of popular Feature Selection me  SSGA [3  LV W 4 and M I FS [5  Sect i on II descri bes t h e feature selection methods and the Associative Classifiers are described in section III. Secti on IV gives the comparative study of the feature selection methods and section V concludes this work II M ETHODS OF F EATURE S ELECTION Feature selection methods taken into this work are as follows A Las Vegas with Probabilistic Wrapper\(LVW\ Model This algorithm [4 r ob ab ilistic an d  wrapp er approach version of Las Vegas algorithms.  Las Vegas algorithms make use of probabilistic choices to guide them more quickly to a correct solution. Las Vegas algorithms, use heuristic methods, take long time to reach to decision. LVW adopted a modified approach by adopting estimation of error rate for each subset. The smalle st subset with lowest error rate is chosen as the most relevant feature subset for the target concept. It computes error of a subset both with training set and test set. And then compares with the current minimum error rate. LVW output s every current best. LVW produces good solution or an optimal solution if number of runs is chosen to be large B Mutual Information for Feature Selection \(MIFS This method [5 i nvest i g at es t h e ap pl i cat i o n of M u t u al Information criterion to evaluate a set of candidate features and to select an information subset for neural network classifiers. It uses the problem given an initial set F with n features, find the subset S of set F with k features that minimizes conditional entropy H\(C/S\that maximizes the mutual information I\(C;S\annonís information theory is adopted for suitable formalism of this target. It starts with original set of features. It computes mutual information I\(C;S\feat ure of the selected subset Then it adds the feature f with maximum I\(C;S\to the subset S. It sets F= F\\{f} and S ={f}. Then iteratively MIFS selects greedily feature s from F by computing I\(f;s\oses f with maximum I\(C;S  012 C Generational Genetic Algorithm \(GGA I\(f;s\nd sets  F= F\\{f} and S S U f}. S is the selected feature set Genetic algorithms \(GA\[2 aptiv e search  techniques based on biological evolution of natural selection Sometimes they may take hours to obtain good feature subset on large datasets. To improve this state, GGA adopted filter method that selects based on some criteria and reduced significantly the   computation time. It used inconsistency rate as the filter criteria for a subset of features. It uses binary coding that show the presence or absence of features in the subset and implements standard genetic operators crossover and mutation D Steady State Genetic Algorithm \(SSGA This method[3 has bee n  de v e l ope d f o r f u zz y rul e  based  classification system \(FRBCS FRBCS have a disadvantage that there is exponential growth of fuzzy rule search space with increase in number of f eatures in training process Hence it has big efficiency degradation with datasets. To counter this overhead, this method uses a precision measure provided by the k-NN consider ing only those features that are included in the candidate feature subset . Candidate subdset is selected with any  GA. It uses integer coding scheme with a fixed cardinality. In fact, it combines both filter and wrapper approaches in two steps. First it uses filter approach to find cardinality for the feature subset with minimum number of inconsistencies.Second, the cardinality is used as chromosome length for a wrapper based feature selection process. It provides a variable subset. It used knearest neighbour estimation which is very sensitive to irrelevant variables III A SSOCIATIVE C LASSIFICATION T ECHNIQUES Associative Classification is the integration of Association Rule Mining and Classification Rule Mining by focusing on a special subset of associatio n rules whose right-hand-side are restricted to the classifica tion class attribute. Associative classifiers had been found  9 t o be bet t e r t h an C4 5 an d other traditional classifiers in terms of accuracy and dataset coverage. Associative Classification consists of three steps 1. Generation of frequent item sets or association rules 2. Selection of all the class association rules \(CARs 3. Building a classifier based on the generated CARs Among the associative classifiers: CBA [6   CM AR 7  an d     CP AR had been  f o u n d bet t e r a n d ha ve bee n us ed  in this work But Associative Classifiers are more memory and time consuming The techniques are described as follows A Classification Based on Associations \(CBA This approach [6 g i v e s the first algo rithm th at in teg r ates Association rule mining and Classification. It uses the apriori approach to discover the association rules. Best association rules having any of targeted rules are selected based on confidence, support and size of antecedent. These rules are pruned using ìpessimistic error rateî. Finally the rule list is generated using a variation of ìcoverî principle 


Intêl Conf. on Computer Communication Technology  433 For prediction of a new case, it uses a set of related rules by evaluating the correlation among them B Classification Based onMultiple Association Rules CMAR CM algori t h m uses the FP-g ro wt h appr oach t o  find association rules. The classi fication rules are stored in a prefix tree data structure known as a CR-tree. CR-tree makes effective rule storage and speed retrieval of rules in the classifier.  For classifyin g a new object, the subset of classification rules matching the new object is observed at their class labels. In the case where all rules have a common class, CMAR simply assigns that class to the test object. In cases the classes of the accumu lated rules are not identical the rules are divided into separate groups based on their class values and the effects of every group are compared to identify the strongest one. The strength of the groups is measured by the weighted  2 The class of the strongest group is assigned to the test object C Classification Based on Predictive Association Rules CPAR It [8 g r eed y asso ciative classificatio n appro a ch The best rule condition is measured by FOILgain [1 o f t h e rules generated among the available ones in the dataset FOILgain is used to measure the information gained from adding a condition to the current rule. Once the condition is identified, the weights of th e positive examples associated with it are reduced by a multiplying factor, and the process repeats until all positive examples in the training data set are covered. CPAR generates rules directly from training data instead of generating CARs. It generates and tests more rules to avoid missing important rules. Whereas other approaches select the best rule for prediction, it uses best k rules in prediction in order to avoid over fitting. In the rulegeneration process, CPAR is capable of deriving not only the best condition but also all similar ones. It includes the rules with similar gains IV EXPERIMENT In order to study empirically the performance of various Feature Selection approaches on Associative Classification some easily available Feature Selection methods are selected. KEEL [1 a dat a  m i ni ng so ft wa re t ool  was u s ed for discretizing the datasets. The datasets are also available with this tool. The datasets 1 we used are as shown in Table 1. The methods LVM and SSGA can be directly applied on undiscretized dataset whereas GGA and MIFS require discretized dataset for their application. Number of features to be selected is user-defined in MIFS 1 In each dataset, the data-10-3tra.dat file of data is used in the experiment TABLE I T HE D ATASETS S.no Datasets Size Classes Attributes 1 Iris 150 3 4 2 Wine 143 3 13 3 Glass 191 6 10 4 cleve 268 5 13 5 Breast 614 2 10 6 Pima 692 2 8 Associative Classifiers CBA, CMAR and CPAR can be applied only on discretized data. MDLP [12 d i scretizatio n approach was adopted on the datasets, either before or after feature selection of the datasets depending on the requirements of the feature selection methods. The performances of the three associative cla ssifiers on different datasets with the MDLP discretizer was empirically tested 13 with th e resu lt ob tain ed  as show n in tab l e2  TABLE II P ERFORMANCE W ITH MDLP Figure 2.   Accuracy with MDLP It can be observed that CBA and CMAR are failing to generate any rule in some cases. According to current observation and [14 th e b e st ac curacy see m s to swing among CBA, CMAR and CPAR with change of dataset CPAR is consistently giving relatively good performance in terms of accuracy. Also the num ber of generated rules by Datas ets CBA CMAR CPAR Accurac y No of Rule Accurac y No. of Rules Accurac y No. of Rules Iris 33.33 2 93.33 16 33.33 4 Wine 0 2 74.29 79 47.89 3 Glass 0 0 0 0 36.84 4 cleve 55.97 14 53.14 168 52.24 11 haber man 54.01 2 53.46 4 73.72 2 Breast 96.42 17 94.46 112 95.77 11 Pima 75.43 10 44.94 28 66.47 11 Aver age 45.023 59.089 58.037  


Intêl Conf. on Computer Communication Technology  434 CPAR is neither too large nor too small in comparison to the other two. CPAR seems to be much balanced with respect to number of rules and accuracy. Hence CPAR is adopted for our target test On applying the feature selection methods on the datasets in table 1, the number of features obtained in reduced dataset is shown in table 3 TABLE III N O OF F EATURES IN R EDUCED S ET Application of CPAR on the above dataset produced classification accuracy as shown in table 4 TABLE IV A CCURACIES OF CPAR W  R  T D ISCRETIZERS Figure 3: Accuracies of CPAR w.r.t. Discretizers From the above table and corresponding graph, it can be observed that GGA is giving best performance with CPAR MIFS is also showing relatively good results V CONCLUSION AND DISCUSSION This is an investigative work aimed at finding a suitable combination of feature selection and classification methods in order to get high performance in classification applications. Associative classifiers had been found to be better than C4.5 and other traditional classifiers in terms of accuracy and dataset coverage. Though CBA, CMAR and CPAR have been found similar in accuracy and one shows superiority from the others with respect to datasets, CPAR have been found give little better average accuracy. This work also finds a similar trend. Also CPAR is found to be more reliable to churn out rules. Along with accuracy the number of rules generated by CPAR is more balanced for being applied in problem solutions. But Associative Classifiers are more memory and time consuming Feature Selection provides a way to remove irrelevant features and reduce the memory and time disadvantage considerably. It can also be concluded that in order to choose a high performance classifier for obtaining good results on some target application dataset, CPAR along with Generational Genetic Algorithm \(GGA\is providing a promising combination for an application oriented project Dataset was applied with four types of feature selection approaches: Probabilistic wrapper, Heuristic, Genetic filter Genetic filter-wrapper. Genetic filter method \(GGA\is showing highest performance \(even improved performance with CPAR. It can also be observed that MIFS is also providing competitive performance to GGA. But number of features to be selected by MIFS is user-defined, so its reliability to produce a proper subset of relevant features gets questioned As Genetic algorithms are generally slow and Heuristic methods are much faster. For a static application, one time application of GGA is acceptab le. For dynamic applications that may deal with classification of many datasets, faster method of MIFS may be more suitable for feature selection R EFERENCES  M. D a sh and H. Liu Feature selection for Classisfication Intelligent Data Analysis 1, 1997, pp. 131-156  Pier Luca Lanzi Fast featu re selection with Genetic algorithms: A Filter approachî, in IEEE Intern ational Conference on Evolutionary Computation, IEEE Pre ss, !997, pp 537-540  J Casillas, O. Cordon, M. J Dell Jesus and F. Herrera, ìGenetic feature selection in a fu zzy rule-based classification system learning process for high-dimensional problemsî, Information Sciences 136 2001, pp. 135-157  H uan Liu and Rudy Setiono Feature selection and ClassificationA probabilistic Wrapper approachî, To appear in proc. of IEA-AIE96  R oberto Battiti U sing m u tual info ermation for selecting features in supervised neural net learningî, IEEE transaction on Neural Networks, vol 5, No 4, July 1994 Data sets Original Dataset Reduced Dataset GGA SSGA LVW MIFS Iris 4 1343 Wine 13 3373 Glass 9 5363 cleve 13 8373 Breast 9 3353 Pima 8 1333 Datas ets Original Dataset Reduced Dataset GGA SSGA LVW MIFS Iris 33.33 33.33 33.33 32.84 33.33 Wine 47.89 47.89 40 40 47.89 Glass 36.84 36.84 36.84 36.84 36.84 cleve 52.24 52.24 52.21 48.53 52.24 Breast 95.77 96.42 91.86 94.46 96.42 Pima 66.47 76.59 62.72 62.72 62.72 


Intêl Conf. on Computer Communication Technology  435  B Liu  W Hs u and Y Ma  Integrating Clas sification and Association  rule mining 7 Proc. of the KDD, 1997, New York, NY pp. 80-86,        1998 Wenmin Li, Jiawei Han and Jian Pei  CMAR: accurate and efficient Classification based on multiple class Association rules 8  Proc ICDM 2001, pp. 369-376 9 Xiaoxin Yin, Jiawei Han, ìCPAR: classification based on predictive Association Rules  Proc of SIAM Int Conf on Data Mining SDMí03\, San Fransisco, CA, pp.331-335 F Thabtah, P Cowling and Y Peng  A study of predictive accuracy for four Associative Classifiers  J R Quinlan and R M  C a m e r o nJones  FOIL: A midterm report In Proc. European Conf Machine Learning, pp. 3{20, Vienna Austria, 1993 Journal Of  Digital Information Management, 2005  J. Alcal a-Fdez L   Sanchez, S Ga rci a M. J. del Jesus, S. Ventura, J M. Garrell, et al KEEL: a software tool to assess evolutionary algorithms for data mining problemsî,  Soft Computing A Fusion of Foundations, Methodologies and Applications Springer Berlin  Heidelberg, pp.307-318. Av ailable from anonymous ftp  U.M Fa yyad K.B   Irani  M ulti-Interval Di scretization of  Continuous Valued Attributes for Classification Learningî, 13 th  Kavita Das, O.P Vy as, ìA suitability study of discretization m ethods for Associative Classifiersî, Inter national Journal of Computer Applications \(0975-8887\me 5-No.10, August 2010, pp. 46-51 IJCAI, vol. 2, Chambery, France, 28.8.-2.9.93, Morgan Kaufmann pp. 1022ñ1027  F Thabtah, P. Cowling Y Peng, ìMultiple labels Associative classificationî, Knowledge and Informa tion Systems. Vol. 9, No. 1 pp. 109-129 


we are pursuing: increasing the variation of edges between nodes rather than increasing the number of nodes, allowing different signature patterns to be easily reconstructed Efficient methods for implementing this reconstruction in hardware are an area of further study  4.2  Statistical Verification  In order to verify our premise that the number of primary patterns increases at a much slower rate compared to the rapid growth of signature pattern databases, we extended our analysis of pattern decomposition to determine the variations across multiple versions of the Snort rule database. Since these versions represent the evolution of the Snort database over a period of several years, this experiment presents us a clear view on the relationship between growth in the number of signature patterns and growth in primary patterns We have analyzed the series of signature pattern sets currently available, representing seven major public versions of Snort rule databases. They include V2.1 to V2.4 and V2.6 to V2.8, which were released over a period from April 2005 to October 2008. Although new types of rules were added to rule databases to accommodate the continuous evolution of network intrusion patterns, major parts of Snort rule databases still follow an accumulating update policy. From V2.1 to V2.2 V2.3 to V2.4 and V2.6 to V2 8, the corresponding types of Snort rules were increased from 48 to 50, and later to 52 However, the sizes of rule database inflated from 935Kb to 8.86Mb The number of pattern entities and the size of pattern sets are two key parameters that we are interested in. With these we are better able to determine the growth relationship between signature patters and primary patterns. By extracting the ìcontentî signature pattern rules from the databases, we obtain the original signature pattern sets, summarized in Table 1. The size of the signature patterns grows from 31.1kB in version 2.1 to 164.2kB in version 2.8. We then further decomposed each set of original signature patterns down to primary pattern sets following the two-step decomposition procedure presented in Section 3 Table 1. Summary of parameters of signature pattern set and primary pattern set in different versions Version Signature Pattern Set Primary Pattern Set  Entities Size Entities Size 2.1 2,739 31.1 kB 2,432 11.7 KB 2.2 3,442 33.0 kB 2,501 11.7 KB 2.3 25,802 145.3 kB 5,856 32.9 KB 2.4 26,936 147.9 kB 5,867 32.9 KB 2.6 31,103 164.1 kB 6,482 37.0 KB 2.7 31,097 163.9 kB 6,525 36.9 KB 2.8 31,156 164.2 kB 6,523 37.0 KB The plot in Figure 8 illustrates the incremental increase in the size of the decomposed database as the number of signature pattern entities increases. From the most recent set of rules tested \(version 2.8\he netbios content signature rule set was selected, since it is the largest set of rules  Because the rules do not contain a timestamp indicating when they were added to the database, the rules were randomized to minimize the effect of grouping within the file To generate the plot, one additional rule was added to the database at each step, and the size of the database after two-step decomposition was recorded. Thus, the graph shows the incremental growth in the primary pattern set size as the signature set size increases  Clearly, as the number of signatures increases the growth of the primary pattern set size appears to be decreasing rapidly The individual points on the plot show where the netbios  signature pattern set size, prim ary pattern set size\airs fall on this plot  Note that except for version 2.8, the points do not exactly fall on this curve since the rules were incrementally added in random order and some rules may differ between versions However, the points do show good agreement, justifying the sampling procedure used to generate the plot While Figure 8 supports our premise that after reaching some practical size, the number of primary patterns saturates and is unlikely to increase significantly with incremental increases in the number of signature patterns in the database we cannot say for certainty that this is indeed the case. Many of the signature pattern sets in the snort database are too small to have reached the point of saturation and the database as a whole, being composed of many types of traffic, also have not yet reached the point of saturation  Despite this, the signature pattern sets all exhibited sub-linear incremental growth when analyzed. Furthermore even if the database never reaches the saturation point given changes in service types, the overall reduction in size achieved by our two-step decomposition procedure still justifies the use of static signature pattern decomposition and dynamic reconstruction  Figure 7. Signature Pattern Reconstruction 364 2009 7th International Workshop on the Design of Reliable Communication Networks 


 015 3  5    015   015&\015  015 015  5   5  015     015  015  015 015   015 2   5  015 015 0151  1&&\015>#5\015     015    015 015  3  015&\0126\012<\012   015	\015       0151     015      5  015   015    015\015&>#5\015&&5   0150\015   015 0155\015	2,'\015       0   015   015 015 015  0150                 015 5\015   015&#\015,%\015           015  015   015 2      25\015&\015#,$&&0'5 0155'&&\015   015  0155   5 0155  015&&\015  015  0150\015    015  015\015 015    015      015  015\015 1&$&A\#\015 015\015 015    015\015 015 015&F   015  015  015\015 015   015    015 015   015    015  015\015 015 015\015'#&	\015\015'\015&,J\015K   015 015     015  2&\015\015'\015  015   1\0155   015  2  015     015 015    015 015 015 015 015  015  015   015 015 9 015\015 015 015   015$5  9D     015 015\015\015&&	&\015     015   5  1&\0155  015   015 015   015      015\015 015 015 015&\015 015\015'\015  015\015    015 9C 3 015   015 015\015   015  015    015   4  015	\012=\015  5  015      JK\015,1,'1   015\015     015     2   015\015 015  015*\015&\015%2  015    J$&K      015\015   015   015\015'\015 3\015\015&\015,&&2\015   0151  015   1 015  015     2 015  015,\015&	&\015     5 015  015   1  015  015     5    015 015     015  015 015     5 015  015\015 015 3   015 9C  015  015   015 015\015   015 015 015&&\015\015 015 015\015 015   015       0152  015    015  015  5 1 015 0155\015 015 015  015 015 015   015\015 015    015   015  015 015 015   5 015     5\015   015 015   015\015 015 015   015   015  015 015   015'&\015\015&&,\015 015\015 015 3 0 0155  015&\015   015#\015      015&#\015&&	15#\015$	\015&\015\015  015\015  015 2 015 015   015 015#\015      015   015  0 100 200 300 400 500 600 700 800 0 5000 10000 15000 20000 25000 Primary Pattern Set Size \(Bytes Number of Signature Patterns Primary Pattern Set Size Netbios Rules Releases  8	\012>!\012		\012\012\012\012&\012\012\012 012\012"\012 2009 7th International Workshop on the Design of Reliable Communication Networks 365 


  015 015  015 015    C  015  015  015 015&<	4\015 2&&\015&&,\015 015     015    015\015,&#\015$\015    015 2 015   012\012\012\015\012  9   015 3    015 P	\015 015  015#\015 015   015 P 015 015"!!C 9  Q=G$#J%*3A\015%\015&*\015	\015 3-K 012$	\015  015//&;1 "!!C 9  R$\015O-\015&P/+'	\015 012 015    P  015    012  015  015 012\012\015+\012	\015  B 9B  Q    015 Q  015   O J6\015 0122\015  015 6	\015  K  015   0   015 012\012\(	&*$\(0-1 3 1C  D"!!D 9C   015\015   015 P#\015   015&A/\0155'P 012\012\015  3 B\AB7CC C 9D       1 J-\015\015 015#\015 015   015   015 015   K  23	\015	\012%	\015\012\0154\015  B 9  6\015$\012J,-\015 0151  1 015$&K 015  015  015$\012\012  015 012	\015 B 97   015\015$\015  H\015&\015 3 015  Q G P    015  015&P 015 O"BQ"!!BC"1D 9  6*\015$\0155\015G>&J/&&&#\015 015   6	\015 015#\015K 015    012  015  015$\012\012  015 012	\015   015""1"B 9  H\015G 015	\015\015\015 012\015\015\015\012 3 9   G R R     3  S P&\015  2 015&P  015   015    015%\015	\015\012 015   7  Q 1;"!!7 9  012  015    015  H   J\0122&1 015  015 015 K   012\015\012   015	\012\015\012 3& 0  9    3 015    Q G P\015 015A  015	\015 015#\015  015P  015  6 012\015\012   015 012\012 7\015 012 015	\012\015 7  3\015 C 9B  015&Q&Q+\015&O&&&P6\015 0122\015   6	\015 015#\015P 8 015\012 6#"#&,--9 1""!!7 9C    015 R   015   44\015 J  015#\015 015  015  015&K 23\015	%\015\012 015  1   3 015 015 1 D 9D  3SRR-GJ\0125\015'&\015  015\015 015 015   015$5 K 012\012\012 015 015     7 C "!!7 9  015 3 J\015 015  7P A??###&\015\015?&?&\015L?"!!7 97      0155    J3#1    6 015 012 015#\015 K 015	\012     012\015 T 015\012\0151Q                             366 2009 7th International Workshop on the Design of Reliable Communication Networks 


Application of Chaotic Particle Swarm Optimization Algorithm in Chinese Documents Classification 763 Dekun Tan Qualitative Simulation Based on Ranked Hyperreals 767 Shusaku Tsumoto Association Action Rules and Action Paths Triggered by Meta-actions 772 Angelina A. Tzacheva and Zbigniew W. Ras Research and Prediction on Nonlinear Network Flow of Mobile Short Message Based on Neural Network 777 Nianhong Wan, Jiyi Wang, and Xuerong Wang Pattern Matching with Flexible Wildcards and Recurring Characters 782 Haiping Wang, Fei Xie, Xuegang Hu, Peipei Li, and Xindong Wu Supplier Selection Based on Rough Sets and Analytic Hierarchy Process 787 Lei Wang, Jun Ye, and Tianrui Li The Covering Upper Approximation by Subcovering 791 Shiping Wang, William Zhu, and Peiyong Zhu Stochastic Synchronization of Non-identical Genetic Networks with Time Delay 794 Zhengxia Wang and Guodong Liu An Extensible Workflow Modeling Model Based on Ontology 798 Zhenwu Wang Interval Type-2 Fuzzy PI Controllers: Why They are More Robust 802 Dongrui Wu and Woei Wan Tan Improved K-Modes Clustering Method Based on Chi-square Statistics 808 Runxiu Wu Decision Rule Acquisition Algorithm Based on Association-Characteristic Information Granular Computing 812 JianFeng Xu, Lan Liu, GuangZuo Zheng, and Yao Zhang Constructing a Fast Algorithm for Multi-label Classification with Support Vector Data Description 817 Jianhua Xu Knowledge Operations in Neighborhood System 822 Xibei Yang and Tsau Young Lin An Evaluation Method Based on Combinatorial Judgement Matrix 826 Jun Ye and Lei Wang Generating Algorithm of Approximate Decision Rules and its Applications 830 Wang Yun and Wu-Zhi Qiang Parameter Selection of Support Vector Regression Based on Particle Swarm Optimization 834 Hu Zhang, Min Wang, and Xin-han Huang T-type Pseudo-BCI Algebras and T-type Pseudo-BCI Filters 839 Xiaohong Zhang, Yinfeng Lu, and Xiaoyan Mao A Vehicle License Plate Recognition Method Based on Neural Network 845 Xing-Wang Zhang, Xian-gui Liu, and Jia Zhao Author Index 849 
xiii 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





