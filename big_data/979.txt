    Abstract Dual energy CT \(DECT\as become a hot topic among security inspection recently for its high detection precision and robust material identification ability. Currently, in one typical realization of DECT, two sets of detectors are needed However, the high cost of the system become a big limitation for its wide usage. Moreover, in some particular situation, the detective objects detected in security inspection with regular shapes and  thicknesses donêt need complete sampling to reconstruct. Therefore, a new idea of system setup, dual energy CT with reduced data system \(DECT-RDS\nly a few detector bins instead of complete-sample bins of the second high energy set which can greatly reduced the cost, had been created. And then we proposed a new dual energy CT reconstruction method with reduced data \(DECT-RD\equiring much fewer data to reduce 
the cost of detectors. In this paper, to fully investigate the feasibility of its application in security inspection, we are developing such an experimental imaging system \(DECT-RDS Practical experiments are done by using only 20% detector bins instead of complete-sample bins in each projection. Results give a relative error less than 5% between complete sampling and undersampling. The results demonstrated that detector bins can be greatly reduced in dual energy CT imaging, hence much lower the system cost. We believe this type of system configuration will drive DERT into wide practical applications  Index Terms X-ray tomographic imaging, dual energy CT undersampling, system, security inspection  I  I NTRODUCTION  n recent years, because of its high detection precision and robust material identification ability   4 T 
has played a significant role in security inspection  Currently, in one typical realization of DECT, two sets of detectors are needed: one receives low energy photons, and the other receives high energy photons  as shown in Fig. 1 However, the high cost of the detectors becomes one of the key problems for this kind of DECT imaging system. Furthermore in some particular situation, the detective objects detected in security inspection with regular shapes and  thicknesses donêt  This work was supported by the National Natural Science Foundation of China under the project No. 60871084, No. 60772051 and No. 10575059, and the Program for New Century Excellent Talents in University No NCET-05-0060 Authors are all with the Department of Engineering Physics, Tsinghua University and Key Laboratory of Particle & Radiation Imaging \(Tsinghua University\ Ministry of Education, Beijing, P. R. China, 100084 \(e-mail liuyuanyuan06@mails.tsinghua.edu.cn  For convenience, we use çhigh energyé and çlow energyé meaning the two 
energy levels in a dual energy system. Here, it is not to use specific boundary to define if one is high or low energy, but a relative meaning between two need complete sampling to reconstruct. Maybe only a few dual energy rays passing through the detective objects are enough to obtain good results, as shown in Fig. 2  Detective Object X-ray Sourse Low Energy Detector Layer High Energy Detector Layer 
Angle Changed   
  
Fig. 1. A typical realization of DECT imaging system setup   1 1 l       1 2 l 1  li 2  li 2 j l  j li 2 2 l 
 Fig. 2. The limited dual energy rays passing through the detective objects Provided that a fan-beam angle source, two sets \(the first low energy set mounted is detectors of full resolution while the second high energy set mounted is detectors of reduced number Feasibility Study: Low-Cost Dual Energy CT for Security Inspection Yuanyuan Liu, Jianping Cheng, Zhiqiang Chen and Yuxiang Xing Member, IEEE  I 2009 IEEE Nuclear Science Symposium Conference Record N13-211 9781-4244-3962-1/09/$25.00 ©2009 IEEE 879 


  of bins\ of detectors are equally spaced along a circular arc, as shown in Fig. 3. Only a few detector bins instead of complete-sample bins of the second high energy set, the system cost can be greatly reduced. To fully investigate the feasibility of its application in security inspection, we are developing such an experimental imaging system \(DECT-RDS\ in our lab. An advanced image reconstruction method with redu  utilized in DECT-RDS reconstruction. The system has advantages of high precision and low cost. This paper focuses on the feasibility study of its application to security inspection Through some preliminary experiments, our method and system are validated to be effective for security inspection  Detective Object X-ray Sourse Low Energy Detector Layer High Energy Detector Layer Angle Changed     Fig. 3.  A new low-cost DECT imaging system setup  II  S YSTEM D ESIGN  Why we choose the configuration of Fig. 1 to investigate? As we know, there are three kinds of DECT configurations for circular trajectories. The first one, like the Fig. 1, is with a single energy source and a two sets of detectors \(one receives low energy photons, and the other receives high energy photons rotating during a scan. The second is with a high-low energy switched source and one set detectors which can receive high and low energy photons synchronously rotating during a scan and the last kind is with one pair of high energy source and one set of high energy detectors, and one pair of low energy source and one set of low energy detectors respectively rotating once The first kind of configuration imaging has the capability of examining objects online fast and its source is not complicated Therefore, an experimental DECT system with first kind of configuration imaging is currently being developed in our lab as shown in Fig. 4, to validate the feasibility of applying DECT-RDS to security inspections The parameters of the system are given in TABLE I and the high-low energy spectrum of the experiment system are shown in Fig. 5  Fig. 4. System configuration of the experimental DECT-RDS  TABLE  I P ARAMETERS OF THE DECT-RDS I TEM V ALUE  Voltage of X-ray tube \(keV  160 Maximum operating current of X-ray tube \(mA 3 Minimum operating current of X-ray tube \(mA 0.1   Fig. 5.  The high-low energy spectrum of the experiment system \(detector response counted  III  I MAGE R ECONSTRUCTION S TRATEGY  We proposed DECT-RD reconstruction method requiring much fewer data to reduce the cost of detectors T h e m e t hod  uses low energy complete sampling projections to reconstruct CT image and using segmentation method puts the reconstruction with regular shapes and  thicknesses into a new image with different regions by gray-value difference and marked them with different numbers. With these prior knowledges and dual energy undersampled data, the path length  j li of dual energy ray i through region j, as shown in Fig. 2, can be acquired. Then, it is combined with dual energy undersampled projection data to acquire the integral value of dual effect coefficients A by looking up the full elements H-L curv s h o w n in Fi g  6. Fin a ll y  b y  u s ing pre-reconstruction dual effect decomposition method for dual energy imaging with   j li and A, linear equations e t u p  to obtain the atomic numbers \(Z\he scanned objects. Fig. 7 is the process of the DECT-RD 880 


   Fig. 6.  The full elements H-L curve table   Phantom Low Energy CT Reconstruction Image Segmentation And Mark Dual Energy Undersampling Projections Z Low energy Complete Projections A Aal   l  Fig. 7.  Process of the DECT-RD  IV  E XPERIMENTS  We validate our method DECT-RD and demonstrate the performance of DECT-RDS configuration through the results on the experiments system Suppose the amount of the full detectors is N, and we only use N/5 high energy detectors mounted averagely every 5 detectors instead of complete-sample bins of the second high energy set, as shown in Fig. 8. That means, N/5 high-low energy undersampled projection data are selected to reconstruct In the meanwhile, we set up a phantom, as shown in Fig. 9 Four small \(Al, PMMA, PVC and Graphite\heres are symmetrically mounted in the detective area. Filtered backprojection algorithm \(FBP u s ed to acqu ire a l o w  energy CT reconstruction  Fig. 8.  The location of H-L energy detectors Al PMMA PVC Graphite  Al PMMA PVC Graphite  a\  front view of the phantom             \(b\ side view of the phantom Fig. 9.  The phantom used in experiments By using FBP and threshold segmentation method, we get the low energy CT reconstruction in Fig. 10 \(a\ and segmented image in Fig. 10 \(b  4 1 2 3  a\ low energy CT reconstruction              \(b\ segmentation result Fig. 10.  The low energy CT reconstruction and segmentation results Finally, based on the real experimental data obtained by the DECT-RDS, we get the reconstruction by the DECT-RD method and compare with the reconstruction by the DECT complete sampling method in TABLE II TABLE   C OMPARISON OF D UAL E NERGY C OMPLETE S AMPLING AND D UAL E NERGY U NDERSAMPLING R ECONSTRUCTION  R EGION N UMBER  O BJECT A TOMIC N UMBER Z  R ECONSTRUCTION  F ULL R ESOLUTION S ETTING  U NDERSAMPLED S ETTING FOR H IGH E NERGY D ETECTOR  R ELATIVE E RROR  1 Graphite 6.20 5.89  5.0 2 Al 15.39 15.90 3.3 3 PVC 14.28 13.96 2.2 4 PMMA 7.46 7.31 2.0 From the results, we can see that good results are acquired with relative error between dual energy complete sampling and undersampling is less than 5% while undersampling setting uses only 20% detectors of complete bin samples in each projection. Therefore, it can be seen that reconstructions in a DECT-RDS can be a good approximation to the ideal one Meanwhile, the detector bins can be greatly reduced in dual energy CT imaging, hence much lower the system cost  V  C ONCLUSION  We investigated the feasibility of applying the DECT-RD and DECT-RDS configuration for security inspections in this paper. Through the preliminary experimental results, our system and method were validated to be effective. However only some simple phantoms and situations had been studied More work will be done to improve and further validate this system, such as more complicated phantoms will be attempted 881 


  and more reasonable  location of high-low energy detectors will be investigated, and etc  A CKNOWLEDGMENT  This work is supported by the National Natural Science Foundation of China under the project No. 60871084, No 60772051 and No. 10575059, and the Program for New Century Excellent Talents in University No. NCET-05-0060 R EFERENCES  1  P. Engler and W.D. Friedman, çReview of dual-energy computed tomography techniques Materials Evaluation vol. 48, no. 5, pp 623Ö629, 1990 2  R.E. Alvarez, J.A. Seibert and S.K. Thompson, çComparison of dual energy detector system performance Med. Phys vol. 31, no. 3, pp 556Ö565, 2004 3  S. Oelckers and W. Graeff, çin situ measurement of iron overload in liver tissue by dual-energy methods Phys. Med. Biol vol. 41, no. 7, pp 1149Ö1165, 1996 4  A.J. Coleman and M. Sinclair, çA beam-hardening correction using dual-energy computed. tomography Phys. Med. Biol vol. 30, no. 11 pp. 1251Ö1256, 1985 5  Z.R. Ying, R. Naidu and C.R. Crawford, çDual energy computed tomography for explosive detection Journal of X-ray Science and Technology vol. 14, no. 4, pp. 235Ö256, 2006 6  Y. Liu, Z. Chen, L. Zhang, Y. Xing, J. Cheng and Z. Wang, çDual energy CT Reconstruction Method with Reduced Data The 10 th International Meeting on Fully Three-dimensional Image Reconstruction in Radiology and Nuclear Medicine Sep. 5-10, 2009, Beijing, China 7  W.Y. Bi, Z.Q. Chen, L. Zhang and Y.X. Xing, çA volumetric object detection framework with dual-energy CT,é IEEE Nuclear Science Symposium Conference Record, 19-25 Oct., 2008 8  R.E. Alvarez and A. Macovski, çEnergy selective reconstructions in x-ray computerized tomography Phys. Med. Biol vol. 21, no. 5, pp. 733Ö744 1976 9  A.C. Kak and M. Slaney, Principles of Computerized Tomographic Imaging, New York, NY, IEEE Press, 1988 882 


were resubmitted to the test process for regression testing  Automa tic tests Master RFD p er Master RFD p er Product Manage Develop ment Depar tment Mana Departm ent Manager issue In fo In fo Rep r es entativ Repres entativ Automatic / manual Test O K To test To test Test OK  N OK  Sub Sub In fo In fo ARNO Test Coordination    Standar d Error Figure 3: The Integrated Test Process \(ITP 3.6  Staff At the start of the project, it was recognized that both the development and production staff working with the mainframe environment would also have to be migratedî, i.e. retrained, in order to retain their application and production know-how. It was clear that these employees and their know-how were major assets to the company which needed to be preserved Process knowledge is more important to a user organization than technical knowledge, e.g. the command of a particular programming language or operating system. This required the staff to be thoroughly trained in maintaining and using the target systems. Numerous informational events as well as the establishment of project WIKIs and BLOGs ensured the compilation and distribution of information. This retraining effort simplified the subsequent re-documentation of the existing systems and gave rise to an important change process within th e entire company 3.7  Involving the Line Organizations Since the operation of the computer center and the applications was distributed among different company departments, the migration project team had little influence on the system administration and organization of the computer center. Th e responsibility for the application systems was only temporarily turned over to the migration team for the duration of the project. The project charter specified th at once the migration was completed and validated via system, acceptance and performance tests, the responsibility would be transferred back to the line orga nization. This also meant that the converted sources had to be handed over to the responsible development departments. Involving the affected development, maintenance and production departments in the migration project helped to prepare them for their later respons ibility. Their staff was given roles in the project which would enable them to carry out similar tasks later in the line organization 4  Technical Implementation 4.1  SPL to C As pointed out above, the applications being migrated were programmed in SPL, a procedural programming language to support system level programming under the operating system BS2000. The large volume of source code prohibited manual rewriting There were altogether 25.000 source members with some 2 million source statements and more than 4 million lines of code. There wa s no alternative to an automated conversion process. The target language of choice would have been C because it is the closest to the procedural structure of the SPL and also because it is widely used in the UNIX world. However, since C does not contain equivalent counterparts of all SPL language elements, these had to be provided in the form of a 'SPLCompat' compatibility layer. This meant that not C but C++ was chosen since C++ provides the mechanisms to implement the missing elements Among other things, the compatibility layer contains the data types that are available in SPL, but for which there are no corresponding types in C, e.g. bit strings and hexadecimal strings. In C++ these types could be implemented as classes for string and bit string processing with a complete set of operators to produce exactly the same resu lt as in SPL. The library also includes the IO operations which in SPL are at the system level. These too could be replicated in technical classes [5 Using the SPLCompat library, the STC converter SPL to C++\could generate a UNIX-compatible C code. The conversion addressed numerous non-trivial incompatibilities such as the visibility of variables. For each and every incompatibility a mapping rule was defined for mapping that particular SPL constructs to a functionally equivalent C++ construct. The use of these strict mapping rules e liminated the need to address every individual incompatibility problem. The individual incompatibilities could be resolved with a general solution. This implied that the original SPL source had to be adapted to comply with the general rules. Here again is a case where reengineering is a prerequisite to migration. The SPL code had to first be unified in order to be converted automatically. Compliance with the rules in the SPL code was verified before the conversion and de viations were corrected 
149 
153 


and tested in the SPL code under BS2000, thus providing the basis for an unambiguous conversion. During the conversion phase, details of these rules had to be adjusted several times. This meant that the SPL source also had to be readjusted Another problem arose with respect to the definition of variables and constants created in C++ where the appropriate type could not be defined until their actual usage was determined. This is a problem with all typed languages. Many of these variables and constants were also defined in include members, the SPL equivalent of C header files, which were used in many modules. As it turned out the different SPL modules did not use these variables and constants in exactly same way. Each had its own view of them. This led to the generation of different incompatible C++ includes for one and the same SPL include The first solution to reducing the number of these multiple definitions was to transform them into a common all encompassing definition. Where that was not possible the developers were compelled to adapt the code of the include members to a common view before generating the corresponding C++ header files Here again, reengineering had to precede migration The few remaining problems were then manually corrected in the target language after the conversion Thus, despite all efforts, a fully automated conversion of the code was not possible. Even the comprehensive set of guidelines and the reduction of the language scope did not enable the creation of unambiguous conversion rules for all cases. The STC converter generated alerts for the remaining problem cases which were then manually reviewed and corrected as necessary. This process was managed with a wellorganized configuration management system specifically customized for this job. A high degree of automation was achieved for the language conversion process but it was never 100% \(educated guesses said: more than 95%\bitious goals like the elimination of all GO TO branches and the creation of objects with multiple instances were avoided. A fully automated conversion to support these features would have driven up the costs of the projects. The goal here was to make the conversion as cheap and with the least risk as possible. This is a goal of most technical migrations in industry 4.2  JCL to Perl In addition to the programs, a large number of BS2000 job control procedures had to be migrated to equivalent UNIX scripts. In view of the large number manual conversion was out of the question. In contrast to the source code of the programs, the existing control procedures were far less homogeneous and less welldefined. On the one hand, the Job Control Language JCL\S2000 actually comprises several different languages like ISP and SDF. Furthermore many procedures included utilities such as ëmergeí and sort' routines. Other procedures invoked system specific utilities, such as the EDT editor in BS2000. This required special proprietary commands. To make the command code homogeneous and to create a basis for automated conversion, the first step was to preprocess all procedures with the ai d of the Fujitsu Siemens SDF-CONV converter to transform them into uniform job procedures in a well-defined subset of the SDF language. In the second step, a somewhat simpler JTP JCL to Perl\er was able to convert the preprocessed procedures into Perl For the most common utilities, equivalent routines were written in Perl. Although there are comparable substitutes for most utilities in the Unix environment e.g., the 'sort' utility under Unix, the functional scopes of the products are different. This prohibited the simple substitution of the utility calls. They had to be rewritten by hand. Conversely formally defined conversion rules enabled the JTP converter to insert the newly written Perl utilities in place of the former BS2000 utility calls. Nevertheless, some control procedures still had to be rewritten prior to conversion.  For example, it was cheaper to rewrite those procedures containing the internal procedural language of the BS2000 editor \(EDT\before the conversion than to rewrite the procedures containing these elements in UNIX. Analogous to the STC procedure, JTP generated a complete list of the job control procedures that could not be fully converted. This left it to the developers to manually complete the conversi   4.3  Filehandler to Oracle Data management under BS2000 was handled with a proprietary, low-level program library called the ìFilehandlerî, containing the essential access operations of a database system ñ insert, select, fetch, delete, update, etc. A conversion of these operations would have been extremely costly, due to their dependence on the host operating system. A new development of the library routines would have been equally expensive Some IO routines were written in OS assembler After a thorough evaluation, the approach chosen was to deploy the Oracle 10 database on the Siemens host machine. Since the previous Filehandler had a clearly defined call interface the application logic did not have to be modified. Based on the existing interface, a middleware access layer was inserted, whose task it was to translate the previous Filehandler calls into SQL database queries of the Oracle database. This not only required remodeling the functional scope, but 
150 
154 


also converting, e.g., Oracle error messages to the corresponding Filehandler error codes. Since the existing interface was well-defined, a fully automated conversion was possible. It was also possible to test the new access layer with the relational database on the mainframe prior to the actual conversion The new access layer achieves a clean decoupling of the application layer from the data access layer, thus ensuring data independence of the applications. This encapsulation makes it rela tively easy to replace the data storage system at a late r date, e.g. when opting for a different database supplier 4.4  DCAM Monitor to openUTM The mainframe applications encompassed a proprietary middleware layer, the DCAM monitor ñ based on the basic communication methods provided by BS2000 Data Communication Access Method\nce it had been specifically developed to the requirements of the START applications, it was extremely efficient To achieve its performance goals the DCAM monitor had been implemented using many Assembler level routines built into the SPL code. Besides having to convert the SPL code to C++, it would also have been necessary to translate all of the system calls to their exact equivalents in Unix. This, in turn, would have required an additional middleware layer for the emulation of all BS2000 system calls. Not only would this have given rise to a signi ficant amount of additional work and expense, but it would also have led to a BS2000 communication emulator under Unix. This was contrary to the goals of the project which was to free the application software from all dependencies on the BS2000 operating system Therefore, the decision was made to deploy a standard middleware, openUTM \(Universal Transaction Monitor\ Fujitsu Siemens under both BS2000 and Unix. Fortunately the product was available in both environments. In the first step this transaction monitor was installed under BS2000 and the applications there switched over to it. Thus, the new transaction monitor was already in production, even before the migration began. The applications using it could then be converted without changes to the architecture or middleware interfaces a nd were directly operable under Unix. This platform change was transparent to the communication partners. Consequently, the migration was not burdened by a conversion of the transaction framework. There were no additional costs for running it in the target environment. This also made regression testing a lot easier 5  Migration Tools and their architecture One of the key success factors of ARNO were the dedicated migration tools, specifically developed for this project. They enabled a high degree of automation as has been pointed out in previ   Of particular importance in choosing the right conversion tool is, whether the programming language of the legacy system should be preserved on the target system or not. In the case of COBOL, it is mostly preserved on the target system despite of all criticism. In this case only an adaption of the dialect is necessary For the conversion process a simple task pattern recognition algorithm based on regular expressions is adequate descri bes a t ool whi c h convert s di fferent  COBOL dialects. Controlled by a graphic Eclipse interface the complex pattern recognition algorithms of Perl are used However, when converting one programming language into another, the use of regular expressions falls short.  A complex translation processes requires complex translation techniques. Translators are tools which automatically convert source programs in language A into source programs in language B For the ARNO migration project, a tool to translate SPL into C++ had to be developed. This was accomplished using a transformation architect ure similar to that of a compiler. This transformation model had already been under development for many years. It is displayed in picture 4 and described in   The architecture of a transl ator is based on the classic compiler model.. But there are some differences 1 Preservation of comments: Translators donít discard comments. They are preserved during the whole conversion process to insert them later into the target code 2 Preservation of preprocessor information: According to the classic co mpiler model preprocessor instructions are discarded after they have been resolved. In a translator, they have to be preserved to the end of the conversion process, e.g. macros of the source programs have to appear optionally as macros in the target program and th e include instructions are needed during the generation for the partitioning of target sources into several files 3 Interface between source- and target representation: In translation, there exists a strong separation between the two. The parser provides an attributed syntax tree of the source pr ogram. Out of that the converter generates an attributed syntax tree of the target program 4. Postprocessor: Compar ed to the classic compiler model this component is unique to a translator Among the main tasks of a translator are the division 
151 
155 


of the complete attributed syntax tree of the target program into partial syntax trees, the insertion of preprocessor instructions of the target language and the optional integration of the programís comments. These actions are based on syntax trees rather than on files 5 Generator: The generator writes out source code by traversing separate partial syntax trees. During a conversion to C++ several header files and a main program have to be handled in one pass. Because of maintainability requirements, the formatted output should be configurable. The operation of the postprocessor can be described as follows: Within a classic compiler the task of a postprocessor is to execute preprocessor instructions, which results in a single source code file without preprocessor instructions. Preprocessor instructions contain important information, e.g which include files the source program consists of which macros appear at which position within the include files and which comments occur at which positions. In contrast to classic compiling this information is preserved during the translation. The preprocessor and scanner insert new tokens into the token stream which will be read by the parser. After parsing they will be assigned to the nodes of the syntax tree based on the source code position. The converter put them in the position to the corresponding nodes of the target syntax tree. The postprocesso r uses this information for the subsequent tasks 6 Division of the targ et syntax tree: There comes a partial syntax tree for each include file with semantically equivalent content. For that the target syntax tree must be broken up into partial trees, which are later joined together in the final source  Figure 4: The Translator architecture   7. Placement of macros All partial trees resulting from macros in the source program are annotated in the target syntax tree The postprocessor compares these partial trees, defines a macro of the target language and replaces the syntax trees by syntax trees of the corresponding macro calls. The partial syntax tree of the macro is inserted at the corresponding position within the full syntax tree 8 Reinsertion of comments: Comments are reassigned to the single partial tree of the target program heuristically as pre- and pos t-comments. Then they are placed before or after th e corresponding target code fragment by the generator For the development of the translators in the ARNO project the following, self-developed generator tools Meta tools\n figure 4 BTRACC Backtracking Compiler Compiler, a parser generator based on the backtracking process, which can optionally generate parser in C, Java or Perl CTree A tool for the declarative description of syntax tree models CBack A tool for the generation of structured C/C code from syntax trees. The formatting of the code can be configured optionally Experience has shown that the development effort for translators amounts to 3 to 3.5 man-years. By using the meta-tools described here the effort was reduced by at least 25 6  Test The explicit objective of this migration project was to execute the conversion in such a way that would not negatively affect the customers and, ideally, would go 
152 
156 


unnoticed by them. Since no new functions were added to the system during the conversion, it had to be verified that the new system would be functionally equivalent to the ol  Several t e st s were requi red t o  confirm this. The batch processing tests were comparatively simple, ìonlyî requiring verification that the jobs under UNIX and BS2000 generated identical output files from identical input files. Of course, the different encoding on both systems \(EBCDIC in BS2000 Latin-9 under UNIX\account when comparing the files The online testing presented a greater challenge. Due to the large number of interconnected partner systems each with their own databases, the comparison of the online application results was much more difficult 6.1  Test Procedures for Regular Version Releases For the applications running under BS2000, approximately twelve versions were created and rolled out per year. Other than the routine error corrections and changes, these updates also included newly developed functions. For testing them, an established test procedure had been in place for many decades. In addition to verifying the implementation of the new functions the tests focused on ensuring that the modifications did not adversely affect the other functions. To this end simple regression tests we re performed. For some areas that were rarely modified and had little interaction with new or modified components, the test coverage was not in-depth enough for the migration project For example, negative tests were missing in many cases. Particularly for components that were older, only few automatable test cases were available 6.2  Test Automation To ensure adequate coverage of the migrated systems, test automation was considered essential. The objective was to promptly test each newly migrated application system without having to involve the responsible development departments. To achieve this more than 500 automated test cases were developed by the existing test teams, making it possible to test several program versions a day without the need for setting up a new test team for the duration of the project The newly created automated tests had another positive effect. They could be reused for the further evolution of the migrated sy stems. They are now fully integrated into the regular test runs and contribute to assuring the quality of each new release on the new system platform 6.3  Load Tests To avoid unnecessary costs, the applications in question were not subjected to regular load tests under BS2000. Since the consecutive version releases only slightly influenced the response under load, monitoring was limited to behavior in productive operation. The migration to UNIX completely changed the performance profile of many components. For example, a new database layer was added to the architecture which giving rise to a completely different behavior under load. Therefore, verification that the required load could be handled was an essential prerequisite for going productive under UNIX To assure this, a load test environment had to be set The fact, that individual components had to be tested in a complex application network, made the setup extremely difficult. But, a test was defined that realistically simulated the application environment and enabled a long-term test monitoring. This application environment included among other things, components for the simulation of external partner systems. This test required realistic test scenar ios that could be repeated any number of times. With some test cases, e.g., booking a seat on an airplane, this was difficult to implement. If a seat had been reserved, it was not possible to book it a second time since it was already occupied Despite the difficulties incurred, the tests could be conducted successfully, and the application was in operation for more than twelve hours under the required load. During the long-term test, it was also verified that no leaks existed, e.g. when using memory semaphores or other resources. These tests not only verified compliance with th e requirements, they also facilitated the early detection and correction of several bugs which had not become manifest in the development environments without parallel processing 6.4  Replay of a Productive Online Session Message Protocol Test Even sophisticated tests do not reveal all bugs and errors. Not every data constellation is considered when designing the test cases. Some bugs only become manifest by a coincidence of circumstances. Therefore some problems with complex application can only be detected after productive operation has commenced In order to further enhance test coverage, a further test was designed. All messages generated by the applications being migrated under BS2000 during productive operation were recorded and, using an identical time sequence, subsequently sent to the new applications running under UNIX. In this scenario, the linked partner systems were simula ted since it was not possible to otherwise integrate them into the test run. The results were then compared and the deviations evaluated At first sight, this test might appear to be simple However, the details of it presented tremendous diffi 
153 
157 


culties. For example, prior to each test run, the database of the application had to be reset to precisely the state before the start of the recording. Even small deviations would result in a vast number of errors.  Furthermore, all components involved in the application network had to be simulated. Exact compliance with the recorded time pattern was essential here, as well To this end, some of the simulators from the load test were used. The system time had to be synchronized with the test data stream. Otherwise, requests for flights on a certain date would have triggered an error message instead of a positive response, since the desired departure date of the original message would have referred to a date in the past Comparing the results had similar difficulties. Even for identical test runs, the results did not exactly match the reason being that on multiprocessor systems, there is no deterministic sequence in which sub-tasks at the micro-level are processed if routines with synchronized subtasks are started at the same time. Therefore incoming messages were not always processed in the same order, which resulted in differences in message logs and databases. Consequently, 24-hour test sessions never yielded the exact same final status. An automated analysis to determine whether such differences only arose from these effects proved difficult. In the end, some deviations remained that could only be evaluated manually 6.5  Test Results The many different tests ñ integration testing, functional testing, performance testing and regression testing ñ resulted in high quality software being rolled out on time. Each alternate test method revealed different types of errors that were not detected by the other tests making each test method indisp ensable. Due to the fact that this application had never before been subjected to such in-depth testing, a surprisingly large number of bugs and errors were identified that the application system already contained under BS2000, but which had not become manifest until the migration. Of course, these bugs and errors were also migrated to UNIX along with the entire software. It is typical of migration projects that they reveal many buried errors  7  Conclusions Within the scope of the ARNO project, many questions and issues came up which were addressed and discussed by the experts, specialists and knowledge holders involved. The active involvement of users and developers led to improvements in the software development environment, configuration management, test systematic and the test systems. The software development and rollout processes were also reviewed and optimized. These improvements cannot be conclusively assessed yet with regard to reducing costs and facilitating more rapid software development Decisive success factors of this complex and unique project included consistent change management of the project requirements and the modifications to the objects being migrated during the course of the project utilizing a configuration management system. Other key success factors were the support of top management in the steering committ ee, the practical distribution of tasks, including the outsourcing of the tool development, and a highly motivated project team The factor that contributed most to the success of this project was the constraints on the migration. The responsible managers resisted the temptation to get bogged down by technical issues such as restructuring and objectivizing the code. This was not a reengineering project. Reengineering of the original code and data was limited to what was absolutely necessary to enable the conversion. The lesson learned was that it is very important to separate  The concern of migration is to transform a system from one environment to another with a minimum of side effects. It remained the guiding principle of the ARNO project References and related work  Denert, E.: Software-Engi neering. Springer Berlin 1991  Di jkst ra, E A Di sci p l i n e of Program m i ng Prentice-Hall, Englewood Cliffs, N.J., 1976 3 m en g e r, U.; Kaiser, U.; Lo o s A.; Uh lig  D Methoden und Werkzeuge f¸r die Software Migration   83-97 4 Gim n ich   R., Kaiser, U.; Qu an te, J.; W i n t er, A eds.\ 10th Workshop Software Reengineering \(WSR 2008\. 126, Bonn, 2008  Gut t a g J.V Abst ract dat a  t y pes and t h e devel opment of data structures. CACM, 20\(6 6 as, D.L.: On th e crite ria to be used in decomposing systems into modules. CACM, 15\(12\10531058, 1972  Systemtest, Hanser Verlag M¸nchen, 2008  Sneed, H Aufwandssch‰t z ung von R eengi neeri ng Projekten. Softwaretechnik-Trends, 23\(2  odernizing Legacy Systems Software Technologies, Engineering Processes, and Business Practices. Addison-Wesley, Boston, 2003   Teppe W   Eppi g, R   Das AR NO Projekt  Herausforderungen und Erfahrungen in einem groﬂen industriellen Software-Migrationsprojekt  99-113  B r odi e, M  L. et al  M i grat i ng Legacy Sy st em s  Gateways, Interfaces & Th e Incremental Approach Morgan Kaufmann, San Francisco, 1995 
154 
158 


 11 R EFERENCES     Reddy, M.K. and S.M. Reddy 223Detecting FET Stuck-Open Faults in CMOS Latc hes and Flip-Flops,\224 IEEE Design and Test of Computers Vol. 3 , No. 5 , pp. 17-26, October 1986   2 R Mad g e , M. Vilg is, a nd V. Bhide, "Achieving Ultra High Quality and Reliability in Deep Sub-Micron Technologies using Metal Layer Configurable Platform ASICs", MAPLD 2005    Kewal Sal u ja, \223Di g i t a l Sy st em Fundam e nt al s, Lect ure 11\224, Department of Electrical Engineering, University of Wisconsin Madison    Yu W e i  Papos ng  M oo Ki t Lee Peng W e ng Ng C h i n  Hu Ong, \223IDDQ Test Challenges in Nanotechnologies: A Manufacturing Test Strategy\224, Asian Test Symposium 2007. ATS apos;07. 16 th Volume , Issue , 8-11 Oct. 2007 Page\(s\211 \226 211  NASA GSFC Advi sory NA-GSFC 2004-06   Dan El ft m a nn, Sol o m on W o l d ay and M i nal  Sawant  New Burn In \(BI\ethodology for Testing of Blank and Programmed Actel 0.15 \265m RTAX-S FPGAs MAPLD 2005   M i nal Sawant Dan El ft m a nn,  W e rner van den Abeel an John McCollum, Solomon Wolday and Jonathan Alexander 223Post Programming Burn-in of Actel O.25um FPGA\222s\224 MAPLD 2002  B IOGRAPHY   John worked 2 years at Faichild R&D on bipolar switching performance specifically platinum dopedlife time control and the development of Ion Implantation.  He worked 15 years at Intel developing Intel's first bipolar PROM, Ion Implantation, the world's first 16K DRAM, as well as 64K and 256K DRAMs.  Mr. McCollum developed Intel's first dual layer metal CMOS technology for the 386 microprocessor.  He co-founded Actel and worked the last 20 years on process, antifuse and flash cell development and FPGA Architecture at Actel.  He holds over 50 patents   covering Process Technology, Antifuse and NVM technology, FPGA Architecture, Analog Processing and Radiation Hardening.  He has presented numerous papers at IEDM, MAPLD, CSME, SPWG, and the FPGA Symposium. He is currently a Fellow in the Technology Development Department 


Time Time 50 350   10 0                   10 1                   10 2 12.5 50 350   10 0                   10 1  12 expected from Figure 7, the width of the uncertainty region is compressed by the curvature of the monopulse response resulting in a detection-primitive with greater uncertainty than the variance admits.  A filter lag or so-called cluster tracking can easily result in a 5% or greater offset and degraded consistency.  After 300 seconds the curves peak up because the target is appr oaching a low-elevation beampoint limit.  This occurs anytime a target is tracked into the edge of the radar\222s field of re gard and can lead to radar-toradar handover difficulty         300 300 80  s D 2 k,1 k y    s D 2 k,1 k y   100 150 200 250 10 1                    10 5 1 0  Figure 8 - Consistency versus distance from beam center Monopulse Mismatch The next set of curves plotted in Figure 9 show the sensitivity of detection-primitive consistency to a mismatch in the monopulse slope.  All of these curves were generated using a linear monopulse response derived from the slope of the true monopulse response at beam center.  The slope of the 80% curve is 0.8 times th e beam-center slope; the 90 curve is 0.9 times the beam-center slope; and so on for 100%, 110% and 120%.  Again, the order of curves in the graph is the same as the legend order A steep slope tends to expand y I 222s uncertainty while a gentle slope tends to compress it.  An expanded uncertainty leads to a smaller consistency while a compressed uncertainty leads to a larger consistency.  This behavior can be observed in the family of curves in Figure 9.  Curves for the steeper slopes are on the botto m while curves for more gentle slopes are on top.  The notable feature of this set of curves is that the sensitivity to a mismatch in the monopulse slope is not very significant       100 150 200 250 10 1                    90 100 110 120  Figure 9 - Consistency versus monopulse mismatch Range-Bias Error The complex nature of the monopulse radar models presents ample opportunity to introduce errors in the software implementation.  One such e rror introduced in a \275 rangecell-width bias in the detection-primitive range which in turn resulted in a significant degradation to 2 9 k D The fact that 2 9 k D is measured in different coordinates compared to the bias made it difficult to determine which value or algorithm was to blame.  Examining the intermediate consistency values led directly to the error source A comparison between biased 2 1 k D  2 2 k D and 2 3 k D values and unbiased 2 2 k D values is shown in Figure 10.  The unbiased 2 2 k D is the bottom-most curve and the biased 2 3 k D is the top-most curve with a value around 80.  This large value for 2 3 k D indicates that there is a lot more uncertainty in the range measur ements compared to what is predicted by the range varian ce.  Since the range-variance calculation is easy to confirm, the problem must be in the algorithms that model or manipulate range A notable feature of Figure 10 is the sensitivity of the centroiding algorithm to range bias in the detection primitives.  The range bias is ba rely noticeable in the biased 2 1 k D and 2 2 k D curves.  Of course, if the unbiased 2 2 k D  curve existed as a baseline it would be relatively easy to spot the error 


Time Time Time 50 350   10 0                   10 1                   10 2 50 350   10 1                   10 0                   10 1 50 350   10 0                   10 1  13         Isolated No SNR Adjust  Figure 11 - Centroiding for isolated range cells Filter Tuning Now that the centroided m easurements are reasonably consistent, the parameters that govern track filtering can be examined.  As previously promised, the effects and corrections for atmospheric refr action and sensor bias have been disabled so that 2 8 k D can be analyzed using a sliding window.  Of course the full analysis would include these effects and 2 8 k D at each time step would be collected and averaged over many trials Plots of the effect of changing process noise in a nearlyconstant-velocity filter are shown in Figure 12 and Figure 13 for Cartesian position and velocity respectively.  In both figures, the plotted values have been divided by 3 so that the desired value is always 1.  Increasing the process noise up to a point should increase the updated uncertainty and reduce 2 8 k D values.  Except near th e end of the trajectory when the measurements are off of beam center, the curves in Figure 12 and Figure 13 appear inconclusive for this expected trend If 2 8 k D values are way out of range there are additional intermediate filter values that can be examined.  For example, the state extrapolati on algorithms can be examined by comparing the consistency of 1 210 Isolated With SNR Adjust 300 300 300 0.005 212 212 212 212 212 kkkk T kkkk D xhzSxhz 35        s D 2 k Range   D 2 k,2 biased D 2 k,1 biased D 2 k,2  Figure 10 - Range bias error in detection primitive Centroiding Algorithm From Section 3, assuming that the centroided-range uncertainty for an isolated range cell is the same as its detection-primitive uncertainty may be incorrect Collecting and plotting 2 3 k D values only from isolated range-cell measurements can be used to analyze such assumptions.  The plots in Figure 11 compare differences between the isolated-cell algorithm defined in Section 3, an algorithm that modifies the uncertainty based on the SNR in the isolated cell, and the 2 3 k D values from all measurements 34\was used to modify the range uncertainty for the upper line labeled Isolated with SNR Adjust    4 22  2 2  resRi o R R Rn bdp bm  s D 2 k,3 Range    s D 2 k,8 Position     212 1 can also be examined using \(35 The residual is also commonly used to determine the assignment cost  212 kk z  P  k  k1 with z k The consistency of the innovation covariance k T kkkkk RHPHS 100 150 200 250 10 1                    D 2 k,3 biased 100 150 200 250 10 2                    100 150 200 250 10 1                    0.5 50  Figure 12 \226 Position consistency, filter tuning example  r  t t 34 If the All Centroided curve \(middle\as the baseline doing nothing \(lower\imates the uncertainty and 33\imates the uncertainty.  Dividing by the square root of the observed SNR leads to a more consistent covariance; however, there is currently no statistical evaluation to justify it             210 210 1 1 1 2 All Centroided 


Time 50 350   10 1                   10 0                   10 1  14         300 0.005  s D 2 k,8 Velocity   100 150 200 250 10 2                    0.5 50  Figure 13 \226 Position consistency, filter tuning example 5  C ONCLUSION  Calculating and observing the behavior of covariance consistency at different levels  in the radar signal processing chain represents a very powerfu l tool that can be used to assess the accuracy and softwa re implementation of radar signal-processing algorithms.  Analyzing covariance consistency is applicable to radar systems both in the field and in simulations.  The primary challenge in both arenas comes down to properly accounting for the true target states that contribute to detections, detection primitives measurements, and state estimates For a fielded radar syst em, achieving covariance consistency is usually a s econdary consideration behind achieving and maintaining track s.  Indeed, until recently radar specifications did not even include requirements for covariance consistency.  Recent covariance consistency requirements stem from the fact that the use of radar systems in sensor netting applications is on the rise Currently the combined e ffects of off-beam-center measurements, atmospheric correction, bias correction clustering and centrioding, data association, and filtering on state covariance consistency throughout a target\222s trajectory are not well known.  This is particularly true for radars using wideband waveforms and multiple hypotheses or multiple frame trackers.  Numerical results presented here indicate that algorithms early in the radar signal processing chain can significantly degrad e covariance consistency and that some errors are better tolerated than others For a simulated target in a modeled system, truth relative to some global reference is known. However, transforming truth through different refere nce frames and accounting for changes that occur during various radar processing algorithms is not as simple as it appears.  The techniques in this paper help expose this hidden complexity and provide a framework for discussing and expanding the future development of covariance consistency techniques.  Such future developments include issues related to mapping truth through the convolution operation typically used to simulate wideband signal processing and the fast Fourier transforms typically used in pulse-Doppler processing.  Sophisticated tracking algorithms that carry multiple hypotheses, associate across multiple frames, or weight the association of multiple targets within a single frame pose significant challenges in properly associating truth with state estimates.  Additional work, including an investigation of track-to-truth assignment, is needed before covariance consistency techniques can be applied to these algorithms Another area that needs furthe r analysis is the use of a sliding window to approximate the covariance behavior expected during a set of Monte-Carlo trials.  Various timedependent variables such as the target\222s range and orientation, the transmit waveform, the radar\222s antenna patterns toward the target, missed detections, and false alarms could easily viol ate the assumption that measurement conditions are nearly stationary over the time of the window.  It is importa nt to understand the conditions when this assumption is violated Finally, the examples presented here included a relatively benign arrangement of targets.  Further analysis in dense target environments with the related increase in merged detections, merged measurements, and impure tracks is needed.  Further analysis for targets traveling over different trajectories is also needed Even so, the techniques presented here can be extended to many of these analyses R EFERENCES  1  S. Blackman and R. Popoli Design and Analysis of Modern Tracking Systems Artech House, 1999 2  Y. Bar-Shalom and X. R. Li Multitarget-Multisensor Tracking: Principles and  Techniques YBS Publishing, Storrs, CT, 1995 3  Y. Bar-Shalom, Editor Multi-target-Multi-sensor Tracking: Advanced Applications and  Vol. I Artech House, Norwood, MA, 1990 4  D. B. Reid, \223An Algorithm for Tracking Multiple Targets,\224 IEEE Trans. on Automatic Control Vol. 24 pp. 843-854, December 1979 5  T. Kurien, \223Issues in the Design of Practical Multitarget Tracking Algorithms,\224 in Multitarget-Multisensor Tracking Y. Bar-Shalom \(ed.\43-83, Artech House, 1990 6  R.P.S. Mahler, Statistical Multisource-Multitarget Information Fusion, Artech House, 2007 


 15 7  B.-N. Vo and W.-K. Ma, \223The Gaussian Mixture Probability Hypothesis Density Filter,\224 IEEE Trans Signal Processing Vol. 54, pp. 4091-4104, November 2006 8  B. Ristic, S. Arulampalam, and N. Gordon Beyond the Kalman Filter Artech House, 2004 9  Y. Bar-Shalom, X. Rong Li, and T. Kirubarajan Estimation with Applications to Tracking and Navigation, New York: John Wiley & Sons, pg. 166 2001 10  X. R. Li, Z. Zhao, and V. P. Jilkov, \223Estimator\222s Credibility and Its Measures,\224 Proc. IFAC 15th World Congress Barcelona, Spain, July 2002 11  M. Mallick and S. Arulampalam, \223Comparison of Nonlinear Filtering Algorithms in Ground Moving Target Indicator \(GMTI Proc Signal and Data Processing of Small Targets San Diego, CA, August 4-7, 2003 12  M. Skolnik, Radar Handbook, New York: McGrawHill, 1990 13  A. Gelb, Editor Applied Optimal Estimation The MIT Press, 1974 14  B. D. O. Anderson and J. B. Moore Optimal Filtering  Prentice Hall, 1979 15  A. B. Poore, \223Multidimensional assignment formulation of data ass ociation problems arising from multitarget and multisensor tracking,\224 Computational Optimization and Applications Vol. 3, pp. 27\22657 1994 16  A. B. Poore and R. Robertson, \223A New multidimensional data association algorithm for multisensor-multitarget tracking,\224 Proc. SPIE, Signal and Data Processing of Small Targets Vol. 2561,  p 448-459, Oliver E. Drummond; Ed., Sep. 1995 17  K. R. Pattipati, T. Kirubarajan, and R. L. Popp, \223Survey of assignment techniques for multitarget tracking,\224 Proc  on Workshop on Estimation  Tracking, and Fusion: A Tribute to Yaakov Bar-Shalom Monterey CA, May 17, 2001 18  P. Burns, W.D. Blair, \223Multiple Hypothesis Tracker in the BMD Benchmark Simulation,\224 Proceedings of the 2004 Multitarget Tracking ONR Workshop, June 2004 19  H. Hotelling, \223The generalization of Student's ratio,\224 Ann. Math. Statist., Vol. 2, pp 360\226378, 1931 20  Blair, W. D., and Brandt-Pearce, M., \223Monopulse DOA Estimation for Two Unresolved Rayleigh Targets,\224 IEEE Transactions Aerospace Electronic Systems  Vol. AES-37, No. 2, April 2001, pp. 452-469 21  H. A. P.  Blom, and Y. Bar-Shalom, The Interacting Multiple Model algorithm for systems with Markovian switching coefficients IEEE Transactions on Au tomatic Control 33\(8  780-783, August, 1988 22  M. Kendall, A. Stuart, and J. K. Ord, The Advanced Theory of Statistics, Vol. 3, 4th Edition, New York Macmillan Publishing, pg. 290, 1983 23  T.M. Cover and P.E. Hart, Nearest Neighbor Pattern Classification, IEEE Trans. on Inf. Theory, Volume IT-13\(1 24  C.D. Papanicolopoulos, W.D. Blair, D.L. Sherman, M Brandt-Pearce, Use of a Rician Distribution for Modeling Aspect-Dependent RCS Amplitude and Scintillation Proc. IEEE Radar Conf 2007 25  W.D. Blair and M. Brandt-Pearce, Detection of multiple unresolved Rayleigh targets using quadrature monopulse measurements, Proc. 28th IEEE SSST March 1996, pp. 285-289 26  W.D. Blair and M. Brandt-Pearce, Monopulse Processing For Tracking Unresolved Targets NSWCDD/TR-97/167, Sept., 1997 27  W.D. Blair and M. Brandt-Pearce, Statistical Description of Monopulse Parameters for Tracking Rayleigh Targets  IEEE AES Transactions, Vol. 34 Issue 2,  April 1998, pp. 597-611 28  Jonker and Volgenant, A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems, Computing, Vol. 38, 1987, pp. 325-340 29  V. Jain, L.M. Ehrman, and W.D. Blair, Estimating the DOA mean and variance of o ff-boresight targets using monopulse radar, IEEE Thirty-Eighth SSST Proceedings, 5-7 March 2006, pp. 85-88 30  Y. Bar-Shalom, T. Kirubarajan, and C. Gokberk 223Tracking with Classification-Aided Multiframe Data Association,\224 IEEE Trans. on Aerospace and Electronics Systems Vol. 41, pp. 868-878, July, 2005   


 16 B IOGRAPHY  Andy Register earned BS, MS, and Ph  D. degrees in Electrical Engineering from the Georgia Institute of Technology.  His doctoral research emphasized the simulation and realtime control of nonminimum phase mechanical systems.  Dr. Register has approximately 20 years of experience in R&D with his current employer, Georgia Tech, and product development at two early-phase startups. Dr. Register\222s work has been published in journals and conf erence proceedings relative to mechanical vibration, robotics, computer architecture programming techniques, and radar tracking.  More recently Dr. Register has b een developing advanced radar tracking algorithms and a software architecture for the MATLAB target-tracking benchmark.  This work led to the 2007 publication of his first book, \223A Guide to MATLAB Object Oriented Programming.\224  Mahendra Mallick is a Principal Research Scientist at the Georgia Tech Research Institute \(GTRI\. He has over 27 years of professional experience with employments at GTRI \(2008present\, Science Applications International Corporation \(SAIC Chief Scientist \(2007-2008\, Toyon Research Corporation, Chief Scientist 2005-2007\, Lockheed Martin ORINCON, Chief Scientist 2003-2005\, ALPHATECH Inc., Senior Research Scientist 1996-2002\, TASC, Principal MTS \(1985-96\, and Computer Sciences Corporation, MTS \(1981-85 Currently, he is working on multi-sensor and multi-target tracking and classification bas ed on multiple-hypothesis tracking, track-to-track association and fusion, distributed filtering and tracking, advanced nonlinear filtering algorithms, and track-before-detect \(TBD\ algorithms He received a Ph.D. degree in  Quantum Solid State Theory from the State University of New York at Albany in 1981 His graduate research was also based on Quantum Chemistry and Quantum Biophysics of large biological molecules. In 1987, he received an MS degree in Computer Science from the John Hopkins University He is a senior member of the IEEE and Associate Editor-inchief  of the Journal of Advances in Information Fusion of the International Society of Information Fusion \(ISIF\. He has organized and chaired special and regular sessions on target tracking and classific ation at the 2002, 2003, 2004 2006, 2007, and 2008 ISIF conferences. He was the chair of the International Program Committee and an invited speaker at the International Colloquium on Information Fusion \(ICIF '2007\, Xi\222an, China. He is a reviewer for the IEEE Transactions on Aerospa ce and Electronics Systems IEEE Transactions on Signal Pr ocessing, International Society of Information Fusion, IEEE Conference on Decision and Control, IEEE Radar Conference, IEEE Transactions on Systems, Man and Cybernetics, American Control Conference, European Signal Processing Journal and International Colloquium on Information Fusion ICIF '2007   William Dale Blair is a Principal Research Engineer at the Georgia Tech Research Institute in Atlanta, GA. He received the BS and MS degrees in electrical engineering from Tennessee Technological University in 1985 and 1987, and the Ph.D. degree in electrical engineering from the University of Virginia in 1998. From 1987 to 1990, he was with the Naval System Division of FMC Corporation in Dahlgren, Virginia. From 1990 to 1997, Dr Blair was with the Naval Surface Warfare Center, Dahlgren Division NSWCDD\ in Dahlgren, Virg inia. At NSWCDD, Dr Blair directed a real-time experiment that demonstrated that modern tracking algorithms can be used to improve the efficiency of phased array radars. Dr Blair is internationally recognized for conceptualizing and developing benchmarks for co mparison and evaluation of target tracking algorithms Dr Blair developed NSWC Tracking Benchmarks I and II and originated ONR/NSWC Tracking Benchmarks III and IV NSWC Tracking Benchmark II has been used in the United Kingdom France, Italy, and throughout the United States, and the results of the benchmark have been presented in numerous conference and journal articles. He joined the Georgia Institute of Technology as a Se nior Research Engineer in 1997 and was promoted to Principal Research Engineer in 2000. Dr Blair is co-editor of the Multitarg et-Multisensor Tracking: Applications and Advances III. He has coauthored 22 refereed journal articles, 16 refereed conference papers, 67 papers and reports, and two book chapters. Dr Blair's research interest include radar signal processing and control, resource allocation for multifunction radars, multisen sor resource allocation tracking maneuvering targets and multisensor integration and data fusion. His research at the University of Virginia involved monopulse tracking of unresolved targets. Dr Blair is the developer and coordinator of the short course Target Tracking in Sensor Systems for the Distance Learning and Professional Education Departmen t at the Georgia Institute of Technology. Recognition of Dr Blair as a technical expert has lead to his election to Fellow of the IEEE, his selection as the 2001 IEEE Y oung Radar Engineer of the Year, appointments of Editor for Radar Systems, Editor-InChief of the IEEE Transactions on Aerospace and Electronic Systems \(AES\, and Editor-in- Chief of the Journal for Advances in Information Fusion, and election to the Board of Governors of the IEEE AES Society,19982003, 2005-2007, and Board of Directors of the International Society of Information Fusion   


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


