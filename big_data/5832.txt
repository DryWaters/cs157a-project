Managing Massive Graphs in Relational DBMS Ruiwen Chen Simon Fraser University Burnaby BC Canada Email ruiwenc@sfu.ca Abstract Massive graphs emerge in many real-world applications Practitioners often nd relational databases are inefìcient in graph data management In this paper we investigate the efìciency issue by analyzing both I/O and CPU costs First we nd the storage of a graph in relational DBMS violates the locality principle  graph queries will always reference neighbors however the data locations of neighbors are almost random To solve this problem we introduce partitioned graph storage as a new database design option It combines database partitioning with available graph-partitioning algorithms to restructure the storage such that neighbors are located close to each other Second we nd graph queries expressed with SQL introduce unnecessary overheads To overcome the CPU costs we propose a new storage access method which we call graph scan  to retrieve neighbors in one single operation We show experimentally that partitioned graph storage and graph scan can signiìcantly reduce I/O and CPU costs We conclude that a relational DBMS could be a good graph store as long as the storage respects the locality principle and SQL overheads are eliminated I I NTRODUCTION Many real-world applications are being built on massive graphs for example social networks web graphs geographical networks recommender systems etc It is highly desirable to have a general-purpose graph data management system Such a system needs to address the following characteristics in massive graphs  Large Real-world graphs have grown to have millions to billions of vertices the web graph has hundreds of billions of vertices and the lar gest social netw ork has hundreds of millions of active users  Dynamic The graph structures are frequently updated online mostly by adding new vertices and edges  Irregular Autonomously emerged graphs often exhibit extremal imbalances that is a non-negligible fraction of vertices have very large degrees  Query-intensive Applications based on massive graphs require instant online responses for graph queries Efìcient management of massive graphs is challenging Many application developers resort to relational DBMS by storing graphs as relational tables However they often nd this is not efìcient enough 20 9 The goal of this w ork is to nd approaches to improving the efìciency of massive graph management in relational DBMS We are mainly concerned with the efìciency of local queries on graphs where a local query is one that explores the neighborhood of a vertex We propose a generic algorithmic template for processing such local queries We address I/O and CPU costs of local queries from two perspectives the storage structure of a graph in disk blocks and the accessing method to graph neighbors in query processing First we nd the storage of a graph in a relational DBMS violates the locality principle  which says that data locations accessed in a short period of time often close to each other The violation is in the following sense a local query will always reference neighbors in a graph but the data locations of neighbors are almost unpredictable Conventional optimization techniques such as clustered indexes materialized views or vertical partitioning which respect the locality principle do not work on graphs A rst solution is to apply available graph-partitioning clustering algorithms to restructure the storage such that neighbors are physically located close to each other This approach improves the storage structure However there are still several issues 1 graph-partitioning algorithms usually do not scale to external memory 2 irregular graphs may not have good partitioning at all We propose a hybrid framework which combines database horizontal partitioning with graph partitioning and addresses both the graph connectivity and the visiting frequencies of vertices By a design of partitioned graph storage  our experimental results show that the I/O costs for local queries can be reduced signiìcantly The second issue we tackle is the SQL overheads for graph queries Finding neighbors using recursive self-joins or loops of selections induces unnecessary overheads on the query optimizer and executor We take a different perspective by viewing the neighborhood relationship on vertices as a selection condition and propose a new storage access method which we call graph scan  to retrieve graph neighbors in one single operation We experimentally show that this approach reduces the CPU costs noticeably Figure 1 shows our newly introduced components and their counterparts in a conventional relational DBMS Our conclusion is that a relational DBMS can still be an efìcient graph store as long as its graph storage respects the locality principle and the SQL overheads are eliminated  The paper is organized as follows Section 2 provides background on the data model systems and datasets Section 3 introduces local queries in our theme and also a local algorithm template which underlies our implementation of several graph algorithms Section 4 addresses the I/O costs and introduces the partitioned graph storage Section 5 focuses on the SQL overheads and introduces the graph scan access path This is followed by discussions related work and our conclusion 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 1 


 User Deìned Functions  Graph Package  SQL Query Engine Sequence Scan Index Scan Graph Scan Horizontal Vertical Graph Partitioning Partitioning Partitioning Fig 1 Components of a Relational DBMS II P RELIMINARIES In this section we list our assumptions on graphs and queries We review available techniques of storing and querying graph data in both relational and graph databases We also describe the graph datasets and our test environment A Graphs and Application Scenarios We consider directed graphs with labeled vertices and edges A directed graph is a pair G  V E  where V is a set of vertices V and E is a set of edges each directed edge is an ordered pair of vertices We represent undirected graphs as directed graphs where each undirected edge is replaced by exactly two directed reciprocal edges We allow vertices and edges labeled by attributes following a common schema For instance a typical social network has individuals as vertices and friendships as edges where the vertices may be labeled by name and age and the edges may be labeled by timestamp A walk is a sequence of vertices where each consecutive pair of vertices is an edge of the graph When a walk has no repeated vertices we call it a path  Given two distinct vertices we deìne the distance between them as the length of a shortest path connecting them Given a vertex v  its d hop neighborhood is the subgraph induced by the set of all vertices within distance d from v  In this paper we consider graphs that are massive sparse and dynamic That is the graph may have hundreds of millions of vertices meanwhile vertices on average have relatively small degree We also allow updates on graphs such as adding or removing vertices or edges but we assume such updates are not frequent this means that we can rebuild the storage when a certain fraction say 20 of the graph has changed We note that such assumptions allow us to capture real-world scenarios such as social network applications and recommender systems These assumptions distinguish our work from previous works which assume small or unchangeable graphs For queries on graphs we mainly consider online queries regarding a small neighborhood of some speciìed vertex We call such queries as local queries  As an example in a social network we may wish to nd friends of an individual whose names satisfy a certain condition We assume these queries are very frequent and require instant responses Note that rather than traversing the whole graph these queries will only necessarily access a small part of the graph We consider graph traversals as ofîine operations which will happen infrequently Such operations are useful in activities such as building indexes and generating statistics a c b       e d       g f       h        AdjList v neighbor a b b a c d c a b d   h f a EdgeTable v u a b a c b a   h f b Fig 2 A graph with its a adjacency list and b edge table B Graph Data in Relational DBMS One can store a graph in a relational DBMS either as a table of adjacency lists or as a table of edges as shown in the example in Figure 2 Although the adjacency-list representation does not meet rst normal form 1NF it saves storage and is more efìcient for neighborhood exploration this is supported by most systems with the array data type The edge-table representation is necessary when the edges are labeled by other attributes In this paper we will mainly use the adjacencylist representation however the techniques developed are also applicable to the edge-table representation There are several ways of nd a neighborhood using SQL self-joins recursive queries and loops of selections For the graph in Figure 2 the following self-join query 1 nds vertices of distance 3 to a starting vertex a SELECT DISTINCT t3.v t3.neighbor FROM AdjList t1 AdjList t2 AdjList t3 WHERE t1.v  a AND t2.v  ANY\(t1.neighbor AND t3.v  ANY\(t2.neighbor Practitioners often use self-join due to its simplicity however self-join queries can only express graph traversals up to a small constant distance Recursive queries provide an alternative approach Recursive queries can express graph traversals of arbitrary distance and thus have strictly stronger expressive power than selfjoins The follo wing recursi v e query retrie v e s the 3-hop neighborhood of a in the graph in Figure 2 WITH RECURSIVE SearchAdjList\(v neighbor depth AS  SELECT v neighbor 0 FROM AdjList WHER Ev=êa UNION ALL SELECT DISTINCT A.v A.neighbor S.depth  1 FROM AdjList A SearchAdjList S WHERE A.v  ANY\(S.neighbor AND S.depth  2  SELECT DISTINCT v neighbor FROM SearchAdjList This query rst initializes a result set with the starting vertex and then iteratively joins the result set with the adjacency list until the result set does not change Note that it has explicit duplicate eliminations in each recursion as well as in the nal result 1 In this paper we follow the SQL syntax of PostgreSQL other systems may have slightly different syntax speciìcations 2 


A third approach of neighborhood querying is to use a procedure language to deìne a loop of selections which visit the vertices one at a time Although we have shown three ways of neighborhood querying in relational DBMS the efìciency of these queries is not promising several work 9 5 ha v e sho wn both analytically and experimentally that these queries are not efìcient on large graphs In this paper we establish several ways to reduce the I/O and CPU costs of these queries C Datasets and Test Environment We examine on the massive graphs listed in Table I Except G RID 10 4  10 4   all others are from real-world applications including social networks web graphs road networks internet networks Dataset T WITTER is from the Social Computing Data Repository at ASU and the others are from Stanford Large Network Dataset Collection 2  We converted directed graphs into undirected graphs by adding reciprocal edges the reason is that most applications require nding neighbors in both directions in social network terms both followers and followees The number of vertices in a graph ranges from 0.4 million to 100 million and the number of edges ranges from 4.8 million to 400 million The average degree of vertices is below 20 but in most cases except G RID and R OAD N ET C A  the degree distribution is highly skewed these graphs follow the power-law distribution where a noticeable fraction of vertices have very large degrees We test on the relational DBMS PostgreSQL 9.0.4 conìgured with 512 MB shared buffers and 16 KB block size We store the graphs as adjacency-list tables and build B-Tree indexes on the vertex identiìer attribute The disk sizes of the tables and indexes are also in Table I III L OCAL Q UERIES In this section we give an algorithmic template for processing local queries We also analyze the visiting frequency of vertices based on a model of local queries A A Local-Algorithm Template A local query on a graph nds a neighborhood of a given vertex following certain criteria For example on a social network one may wish to nd friends of friends by exploring a 2-hop neighborhood in recommender systems the user would like to see items that are frequently bought together on a road network one may wish to nd a nearby store location with road connections A signiìcant characteristic of such queries is that for a given query it is only necessary to look at a small part of the graph of constant or sub-linear size Processing local queries on massive graphs could be more involved To name a few obstacles different queries may require different processing algorithms in a highly connected graph a constant-hop neighborhood may cover a big portion of the graph To provide a uniìed algorithmic framework Spielman and Teng initiated a study of local algorithms for massive graphs Roughly speaking a local algorithm starts  2 http://snap.stanford.edu/data Input A graph G  V E  and a vertex v  V  Thresholds t A and t F  and stopping criteria Procedure Initialize A CTIVE   v   F ROZEN    While A CTIVE is not empty 1 Expand A CTIVE with respect to threshold t A  2 Truncate A CTIVE and generate a subset F  3 Append F to F ROZEN  4 Compute a goal based on A CTIVE  F ROZEN  5 Break if the size of F ROZEN exceeds threshold t F  or stopping criteria are met Fig 3 A local-algorithm template with a speciìed vertex and then it iteratively examines only vertices which are neighbors of those seen before There is no computation involving examining all vertices For efìciency guarantees one should provide stopping criteria and a threshold on the size of the explored neighborhood We design a generic template for local algorithms as shown in Figure 3 The algorithm utilizes three sub-procedures Expand  Truncate and Compute  which are dependent on the queries Following the template a local algorithm starts from a given vertex and then it iteratively examines neighbor vertices until thresholds or stopping criteria are reached In particular we divide vertices into three subsets A CTIVE F ROZEN  and the unvisited A CTIVE maintains vertices in the boundary of the neighborhood exploration F ROZEN maintains visited vertices which are not in the boundary but are still useful for computation In each iteration using Expand we add unvisited but promising neighbors to A CTIVE  then we reduce the size of A CTIVE using Truncate  where we may also move certain vertices from A CTIVE to F ROZEN  The iteration terminates when thresholds or stopping criteria are met We implement the collection A CTIVE using a priority queue structure where each vertex is associated with a priority value F ROZEN  which contains vertices with stabilized priority values is append-only We remark that the threshold t A on the size of A CTIVE sacriìces the completeness of the search When the threshold does not exist we can cast many algorithms completely into the template for example breath-ìrst search best-ìrst search Dijkstraês algorithm etc However in order for the algorithm to be feasible on massive graphs the thresholds are necessary to tradeoff efìciency with completeness Several previous works also exploited the idea of truncating the searching boundary The beam search algorithm which is widely used on AI problems with exponentially-growing search space runs a breath-ìrst search but keeps only a small part of the search space at each depth level The graph localclustering algorithm in maintains a small neighborhood by truncating random walks with low probabilities The running time of the algorithm depends on the subprocedures provided In this work we require Expand  Truncate and Compute to be running in time nearly-linear that is O  n log c n  where n is bounded by t A  t F and c is a constant Under such assumptions the whole algorithm runs 3 


 Graph  V   E d   E  AvgDeg MaxDeg Table Index Edge Description G RID 10 4  10 4  100.00 400.00 400.00 4.00 4 7.6 GB 2.2 GB Coordinate Neighbor T WITTER 11.317 85.332 127.11 11.23 564795 1168 MB 253 MB Following L IVE J OURNAL 4.848 68.994 86.221 17.78 20334 635 MB 108 MB Friendship W IKI T ALK 2.394 5.021 9.319 3.89 100029 175 MB 54 MB Talk to R OAD N ET CA 1.965 5.533 5.533 2.82 12 136 MB 44 MB Road connection W EB G OOGLE 0.876 5.105 8.644 9.87 6332 86 MB 20 MB Web link A MAZON P ROD 0.403 3.387 4.887 12.13 2752 53 MB 9MB Product co-purchase TABLE I G RAPH DATASETS T HE NUMBERS OF VERTICES AND EDGES ARE IN MILLIONS  in time nearly-linear in the explored neighborhood size We will use this local-algorithm template to instantiate a local graph-partitioning algorithm in Section IV and a graphscan accessing method in Section V B Local Computation Let G  V E  be a graph where V   1 n  and let M be an n  n matrix which could be a simple transformation from the adjacency matrix or the distance matrix of G  Let x  x 1 x n  be a vector indexed by vertices in the graph Consider the following iterative process Initialize a vector x 0  and then iteratively compute x  t 1  x  t  M  until it converges This iterative process can be instantiated to compute values such as shortest-path lengths and PageRank However it is not efìcient Suppose we run the process for t iterations the total running time is O  tn 2   On sparse graphs the computation simpliìes using the following identity  x  t 1  x  t  M  where we deìne  x  t 1  x  t 1  x  t   One may expect that for sparse graphs most entities in the vector  x  t  vanish and from the computational perspective we only need maintain a small portion of the vector In the local-algorithm template we deìned in Figure 3 we further impose a threshold function  on the changes  x  t 1    x  t  M   which guarantees that only a bounded number of entities  x  t  will not vanish The validity of such threshold functions can be found in 19 for dif ferent computation problems Therefore roughly speaking the collection A CTIVE maintains the changes at each iteration while F ROZEN maintains the necessary history along the computation While the history helps computing the nal goal function in many cases the history can be completely discarded if the goal function depends only on the priority values maintained in A CTIVE  C Vertex Visiting Frequency Now we analyze how frequent a vertex will be visited by local queries We establish a relationship between the visiting frequency of a vertex and its local structure Consider the following random querying model We start from a random vertex v  branch out to a bounded number b of the neighbors of v  and then randomly pick up a fraction r of these neighbors to recursively continue the process each branch terminates when reaching a distance d  The breadth bound b and distance bound d resembles the real-world resource bounds A real-world scenario could be that on a social network a user rst checks information on a bound number of friends and then nds friends of a fraction of these friends vertex degree visiting frequency distance 0 distance 1 distance 2 Fig 4 Vertex visiting frequency vs the degree For an individual vertex its visiting frequency depends on its neighborhood structure For the trivial case d 0  each vertex is visited uniformly randomly with no visits to neighbors This corresponds to the conventional assumption on relational database queries For d 1  r 1 and unbounded b  the visiting frequency is proportional to the degree plus 1 In this case each edge is visited uniformly randomly This is the mostly assumed case in graph-partitioning research For a constant d  r 1 and unbounded b  the visiting frequency is proportional to the size of the d hop neighborhood On average the visiting frequency increases polynomially as the degree increases A complete characterization would need speciìc assumptions on the degree proìle and the connectivity of the graph However we can conclude that for distance d 1  the visiting frequency increases super-linearly as the degree increases this is different from the usual assumption in graph-partitioning research Another simple observation is that for two vertices which are far from each other they will not be visited together in one local query IV P ARTITIONED G RAPH S TORAGE In this section we propose a partitioned graph storage as a database-design option to tackle the I/O costs of graph queries We investigate several graph-partitioning approaches in our querying context and give a guideline on partitioning massive graphs in a relational DBMS In particular we stress that the partitioning should not only depend on the graph connectivity but also on the visiting frequencies of vertices A Sequential Graph Storage With the adjacency-list representation each vertex and its neighbor edges are stored as one tuple in a database table The whole graph is represented by a collection of such tuples in a table and the tuples are distributed over disk blocks At a top level the block-based storage essentially partitions the graph into almost equal-sized parts However there is no rule 4 


guiding such a partitioning the system is unaware of the graph connectivities and this partitioning is far from optimal Let G be a graph with n vertices and m edges and we wish to divide the vertices into disjoint parts each of size at most a constant K  Practically this constant K depends on the database block size A random partitioning is such that vertices are randomly distributed over the blocks Consider an arbitrary edge With a random partitioning the probability that the two endpoints of the edge resides in the same block is K  1  n  1  K n  which is tiny since the number of vertices n could be very large By the linearity of expectation this implies on average a fraction  n  K  n of the edges are crossing blocks This storage structure means that retrieving a neighbor vertex almost always requires reading a new block This strongly violates the locality principle which says that data locations accessed in a short period of time should form clusters The current relational DBMS have no ways to rectify this worst case This situation can be improved signiìcantly by only a simple procedure Consider a graph traversal path which gives an ordering of vertices such that most consecutive pairs are connected We sequentially store the vertices following the order this gives a partitioning such that the fraction of edges with endpoints in the same block is K  1 K  n m  This is much better than a random partitioning In the next two subsections we take graph connectivity and vertex visiting frequencies into consideration and that further improves the storage for local query processing B Graph-Partitioning Algorithms The graph-partitioning problem is to divide a graph into parts such that the parts have about the same size and there are few connections between the parts A good partitioning algorithm tries to minimize the number of edges between vertices in different parts This problem is well-known to be NP-complete Ho we v e r  this hardness result only characterizes the worse case Given its practical importance graph partitioning has been widely studied in many elds such as parallel computing sparse matrix computation circuit design social network analysis etc severial efìcient approximate algorithms have been successfully applied in practice The class of heuristic or approximate graph-partitioning algorithms is diversiìed Below we analyze the applicability of these algorithms in partitioning massive graphs in relational DBMS Global vs Local Algorithms Several well-studied graphpartitioning algorithms produce approximately optimal solutions including the spectral graph partitioning LeightonRao ow-based algorithm and Arora-Rao-V azirani geometric embedding approach Another line of w ork is on heuristic methods such as Kernighan-Lin algorithm Although these algorithms succeed in many applications they are not efìcient enough for processing massive graphs the best implemented Arora-Roa-Vazirani algorithm runs in O  n 1  5  time and K ernighan-Lin algorithm is in O  n 2 log n  time Researchers ha v e resorted to the di vide-and-conquer approach to partitioning large graphs namely the multilevel partitioning  14 which rst coarsen the input graph into a smaller one and then iteratively generate reìned partitioning level by level However this approach uses the abovementioned algorithms as sub-routines which dominates the efìciency Spielman and Tengês local clustering algorithm Nibble nds a cluster via local expansions on graphs The algorithm starts from a given vertex and expands to neighbors via truncated random walks  the cluster produced consists of vertices that are reachable by many walks The graph-partitioning algorithm based on Nibble runs in nearly-linear time and statistically guarantees that the output is approximately optimal Although the algorithm is still not practical due to a large hidden constant it is promising that efìcient local algorithms could be good even for hard problems like graph partitioning Partitioning vs Ordering For storing massive graphs in databases it is more desirable to have an ordering of vertices instead of a collection of parts This is because that data attributes associating with vertices may require varied storage spaces An ordering of vertices provides more exibility in generating partitions Many graph-partitioning algorithms can be viewed as embedding a graph into a low-dimensional metric space A good embedding is such that the connectivity in the graph is reîected by the distance in the metric space with a good embedding a random separation in the metric space should give a good partitioning of the graph Upon this it will be easier to derive an ordering and then a sequential partitioning of the vertices C Partitioning Respecting Power Laws and Vertex Visiting Frequencies Many real-world graphs follow the power-law distribution that is the fraction of vertices having degree k is proportional to 1 k   where  is a number slightly larger than 2 8 For power-law graphs most graph-partitioning algorithms do not work well since the algorithms tend to be dominated by high-degree vertices Even worse almost all graph-partitioning algorithms are based solely on the graph connectivity they do not consider the visiting frequencies of vertices which is essential in our localquery context As shown in Section III-C the query visiting frequency increases super-linearly as the degree increases as long as the distance in the queries is bigger than 1 The means a few portion of high-degree vertices receives the majority of the visits while a large portion of low-degree vertices receive very few visits This characteristic leads to the following partitioning scheme partition the graph into three parts 1 high-degree vertices 2 low-degree vertices and 3 the rest  Let G  V E  be a power-law graph with degree distribution f  k  k    where  is a constant and  is slightly bigger than 2 Assume that the visiting frequency of a k degree vertex follows the distribution g  k  bk c  where b is a constant and c 1 is a number depending on the distance of local queries Figure 5 plots the degree distribution in contrast with the query visiting frequency We wish to nd a splitting degree k  such that the number of vertices with degree greater than k  is small whereas the 5 


 k  vertex degree degree frequency query frequency Fig 5 Partitioning respecting power laws and query visiting frequencies accumulated frequency of visits to such vertices is large More formally let  be a parameter balancing the two goals and we wish to minimize the following   k  k  f  k      k  k  f  k  g  k     k  k  k       k  k  k   bk c     1 k  1    b   1  c k  1 c    In this degree-based partitioning the part of high-degree vertices respects the locality principle since data locations that are frequently accessed cluster together The part with lowdegree vertices receives very few visits and also the visiting cost is small because of the small neighborhood For the rest part with medium-degree vertices the degree distribution tends to be less skewed because of the removal of high-degree vertices this follows from the result on network resilience under targeted attackes 18 This allo ws us to apply the conventional graph-partitioning algorithms discussed in Section IV-B D Implementing Partitioned Graph Storage An important fact in real-world graphs is that vertices are often associated with ground-truth labels  which indicate natural clusters For example in road networks vertices are often associated with geographical locations in the publicationauthor network publication venues such as conferences or journals serve as natural identiìers of communities in whobuys-what relationships product categories are meaningful labels and in web graphs domain names naturally group the URLs Although ground truth provides no guarantees on the partitioning quality Leskovec et al sho ws that ground truth gives good natural clusters communities in many real-world networks For massive graphs that are too large to be processed in memory one may use ground truth to generate parts that are small enough to be fed into in-memory partitioning algorithms Real-world graphs may differ in many aspects size degree connectivity irregularity etc It is impossible to have one single partitioning algorithm which works on all types of graphs A practical approach for a DBMS is to provide a library of schemes and algorithms The user may choose speciìc algorithms and parameters based on the knowledge on the data or even the user can plug in with their own algorithms In our framework we provide the following partitioning options  Partitioning based on ground truth This is useful to partition big graphs into parts that t in memory  Partitioning based on graph statistics such as vertex degree This is useful for power-law graphs  A simpliìed implementation of the local-clustering algorithm Nibble It tra v erses the graph locally by simulating truncated random walks The algorithm runs efìciently and generates an ordering of vertices  The mutilevel heuristic-based graph-partitioning scheme M ETIS  2 which is widely used in experimental settings Our guideline for partitioning a massive graph is the following 1 partition the graph based on some ground-truth labels 2 if the graph is highly skewed partition it based on graph statistics such as the degree proìle 3 apply inmemory partitioning algorithms on the generated subgraphs An implementation of a partitioning requires the user to analyze ground truth and the degree proìle and specify inmemory partitioning algorithms and related parameters The partitioning will either assign each vertex with a part identiìer or output an ordering of all vertices then a post-processing follows to distribute vertices into disk blocks E Experiments on Partitioned Storage In this subsection we present our experimental results on partitioned graph storage 3 Small Graphs We rst applied partitioning on the relatively small graphs storage size less than 512 MB with results in Table II These graphs have about 4  9 millions of edges We compared three types of storage layout random ordering sequential ordering on vertex identiìers and partitioning Since these graphs are small instead of giving the actual I/O costs we show two statistical metrics on a complete analysis of the storage the percentage of edges that are between vertices in the same block and the average number of blocks that contain a vertex and all of its neighbors 1-hop neighborhood As shown in Table II R OAD N ET CA has a very good partitioning and we expect this will scale to larger road networks W EB G OOGLE and A MAZON P ROD also have good partitioning with more than 70 of edges inside blocks and the 1-hop neighborhood clustering in 2  3 blocks The partitioning for W IKI T ALK is not as good as others The result was from the local-clustering algorithm while M ETIS gives a slightly worse result This graph is highly skewed the top 0  05 high-degree vertices 1197 out of 2.4 million connect with 64 of the edges For such graphs we suggest degreebased partitioning as discussed in Section IV-C Big Grid Graph We use G RID 10 4  10 4  as a testbed for big graphs The storage size about 10 GB We built three storage layouts random ordering sequential ordering by rows in the grid and partitioning The partitioning was a hybrid of horizontal partitioning and local clustering we partition the grid by rows into 20 parts and then apply local clustering 3 The tests in this paper were conducted with the following environment operation system OS X V10.6.8 Linux kernel Darwin 10.8.0 CPU 2.3 GHz Intel Core i5 main memory 4 GB 1333 MHz DDR3 hard disk rotational rate 5400 and le system Journaled HFS 6 


 Graph In-Block Edges 16k Blocks 1-Hop Neigh Partition Alg Rand Seq Part Rand Seq Part W IKI T ALK 0.01 0.54 17.1 4.60 3.91 3.36 LocalCluster R OAD N ET CA 0.01 71.2 95.2 3.81 1.68 1.13 M ETIS 10000 W EB G OOGLE 0.02 0.02 73.6 10.72 10.72 2.24 M ETIS 6000 A MAZON P ROD 0.03 12.4 70.4 13.04 8.92 3.34 M ETIS 3000 TABLE II S TATISTICS ON PARTITIONED GRAPH STORAGE  0 0  5 1  0 1  5 2  0 2  5 3  0 3  5 4  0 4  5 5  0 2 5101520  Partitioned  Sequential  Random 5.89 10.1 Seconds Hop a Hop 2 5 10 15 20 Neighbors 13 61 221 481 840 Read Part 1.98 3.78 7.71 12.78 18.25 Read Seq 5.06 11.36 22.87 34.98 47.94 Read Rand 12.84 60.74 216.7 456.4 787.5 b Fig 6 Neighborhood queries on G RID 10 4  10 4   a Running time b Physical I/O counts on each part We tested neighborhood queries starting from random vertices The running time and physical I/O counts are reported in Figure 6 All querying results in this paper were averaged over 300 runs of random queries Large Power-Law Graphs We tested on two large social networks L IVE J OURNAL and T WITTER  which follow the power-law distribution and have large storage size We rst derive some statistics in Table III to assist us choosing appropriate partitioning schemes L IVE J OURNAL has 4.8 million vertices 86 million edges with storage size exceeding 700 MB Single partitioning algorithm does not scale well on this large power-law graph neither local clustering nor M ETIS  Our approach is to rst partition it by vertex degree top 1 high-degree vertices vertices with degree one and the rest and then apply graph partitioning on each part The query we tested is the following given a vertex v  nd vertices u sharing common neighbors with v  and return the top 100 of them ordered by the number of common neighbors In social network terms this is to nd friends via mutual friends We expressed this query using join aggregation and ordering and tested on the storage without and with partitioning Table IV reports the results In this case only we conìgured the database with 64 MB shared buffers in order to better measure the I/O costs We tested each query twice in sequence in which the rst query measures the total time and the second one roughly measures the CPU time T WITTER graph has 11 million vertices and 127 million Total Time ms CPU Time ms No Part 186.85 16.38 Partititoned 56.28 15.77 TABLE IV R UNNING TIME OF FINDING TOP 100 VERTICES WITH COMMON NEIGHBORS ON L IVE J OURNAL  Total Time ms  Physical Read No Part 73.34 6.80 Partititoned 19.57 4.34 TABLE V R UNNING TIME AND PHYSICAL I/O FOR FINDING 2HOP NEIGHBORHOOD ON T WITTER  edges We partitioned the graph into top 0.1 high-degree vertices degree-one vertices and the rest which is further partitioned using local clustering The query is to count the number of vertices within distance 2 from a random vertice Table V shows the running time and the physical I/O counts V G RAPH S CAN In this section we address the CPU costs of graph queries expressed with recursive self-joins and loops of selections We introduce graph scan as a new storage access method to overcome the SQL overheads in graph queries A Graph Scan A New Access Method As seen in Section 2 one may use a recursive self-join query or a loop of selections to retrieve the neighborhood of a vertex This approach is widely adopted by developers due to its simplicity However the SQL overheads with these queries are relatively expensive Using a recursive self-join query to nd a d hop neighborhood requires d rounds of joins together with projections and duplicate eliminations The costs of joins and projections are largely unnecessary And such overheads grow super-linearly as the distance d grows On the other hand using a loop of selections requires running one selection query on each vertex encountered which also introduces unnecessary SQL overheads In contrast the prevalent graph database systems often natively implement graph-traversal functionalities In particular a programmer can invoke some lower-level interfaces to directly retrieve and manipulate neighbors of an individual vertex To overcome the unnecessary SQL overheads we propose a new storage access method which we call graph scan It complements the conventional sequential scan and index scan A graph scan fetches tuples by following a graph-traversal order It starts from a given vertex and then retrieves its neighbors by following its edges it maintains a collection of vertex identiìers as scan keys and recursively proceeds until certain stopping criteria is met This is analogous to a B-Tree index scan for a range query in which we maintain two scan keys namely the lower bound 7 


 Graph AvgDeg MaxDeg Top 0.1 Deg Top1%Deg  Degree-1 Vertices T WITTER 11.23 564795 1207 104 59.6 L IVE J OURNAL 17.78 20334 553 180 21.0 TABLE III S TATISTICS ON LARGE POWER LAW GRAPHS  Hop 2 5 10 15 20 Neighbors 13 61 221 481 840 Recursive Join ms 1.28 2.19 8.89 22.07 48.03 Selection Loop ms 0.79 2.12 7.11 16.07 36.77 Graph Scan ms 0.71 0.84 1.61 3.24 5.26 TABLE VI CPU TIME OF NEIGHBORHOOD QUERIES ON G RID  10 4  10 4  Hop 2 5 10 15 20 Neighbors 8.61 43.1 208 583 1250 Recursive Join ms 0.76 1.49 7.46 27.57 80.18 Selection Loop ms 0.67 1.71 6.56 19.46 51.17 Graph Scan ms 0.43 0.52 1.22 2.27 4.10 TABLE VII CPU TIME OF NEIGHBORHOOD QUERIES ON ROAD NETWORK R OAD N ET CA and the upper bound descend down the tree to locate the rst entry and then follow the link on the leafs to nd other entries until the upper bound is met In a graph scan we maintain a queue of scan keys and also an explicit stopping condition for the traversal Graph scan can be implemented as a local-algorithm instance with customized searching order and stopping condition This follows from the local-algorithm template deìned in Section III As a single operation graph scan does not involve any joins or projections and duplicate eliminations are done along the way B Experiments on Graph Scan We tested graph scan using neighborhood queries starting from random vertices The results show that CPU costs are reduced signiìcantly In particular we compared three expressions of neighborhood queries 1 Recursive join query We use standard SQL recursive query which makes joins iteratively to nd neighbors level by level Projections and duplicate eliminations are done as early as possible in the iteration 2 Loop of selections We create a function using a procedure language it maintains a queue of vertices and iteratively issues a selection query on each vertex 3 Graph scan We implemented graph scan for breath-ìrst search A query speciìes a starting vertex and a distance the graph scan retrieves all vertices up to the distance Internally it maintains a queue of vertex identiìers as scan keys We tested the three approaches on G RID  10 4  10 4  and R OAD N ET CA to nd the neighborhood of a vertex up to a given distance In order to measure the CPU time for G RID  10 4  10 4  we executed each query twice where the rst one measures both I/O and CPU costs and the second one measures only the CPU costs Table VI shows the result For R OAD N ET CA which is of small size we pre-loaded the data in memory and then executed the queries Table VII gives the result We see that for neighborhoods of distance 10 to 20 graph scan is about 5  10 times faster than recursive joins and loops of selections VI C ONCLUSION AND F UTURE W ORK In this paper we propose an algorithmic template for local queries on graphs We introduce two novel features for managing massive graphs in relational DBMS 1 partitioned graph storage which respects the locality principle and reduces the I/O costs and 2 graph scan access method which reduces the CPU overheads We conclude that a relational DBMS can still be an efìcient graph store as long as its storage respects the locality principle and the SQL overheads are eliminated R EFERENCES  S Abiteboul R Hull and V  V ianu Foundations of Databases The Logical Level  Addison-Wesley Longman Publishing Co Inc 1995  A Abou-rjeili and G Karypis Multile v e l algorithms for partitioning power-law graphs In IEEE International Parallel  Distributed Processing Symposium IPDPS In  pages 16Ö575 press 2006  S Arora S Rao and U V azirani Geometry   o ws and graphpartitioning algorithms Commun ACM  51:96Ö105 October 2008  S T  Barnard and H D Simon F ast multile v e l implementation of recursive spectral bisection for partitioning unstructured problems Concurrency Practice and Experience  6\(2 1994  C Beck er  Rdf store benchmarks with dbpedia http://www4.wiwiss.fuberlin.de/benchmarks-200801 2008  R Bisiani Beam search In S Shapiro editor  Encyclopedia of Artiìcial Intelligence  John Wiley  Sons 1987  D Chakrabarti and C F aloutsos Graph mining La ws generators and algorithms ACM Comput Surv  38 June 2006  F  R K Chung Spectral Graph Theory  volume 92 American Mathematical Society 1997  P  Cudr  e-Mauroux and S Elnikety Graph data management systems for new application domains tutorial In VLDB  2011  P  J Denning The locality principle Commun ACM  48:19Ö24 July 2005  D Easle y and J Kleinber g Networks Crowds and Markets Reasoning About a Highly Connected World  Cambridge University Press 2010  M R Gare y a nd D S Johnson Computers and Intractability A Guide to the Theory of NP-Completeness  W H Freeman  Co 1990  A Gulli and A Signorini The inde xable web is more than 11.5 billion pages In WWW 05 pages 902Ö903 ACM 2005  G Karypis and V  K umar  A f ast and high quality multile v e l scheme for partitioning irregular graphs SIAM J Sci Comput  20:359Ö392 December 1998  B W  K ernighan and S Lin An ef cient heuristic for partitioning graphs Bell Systems Technical J  49:291Ö307 1970  T  Leighton and S Rao Multicommodity max-îo w min-cut theorems and their use in designing approximation algorithms J ACM  46:787 832 November 1999  J Lesk o v ec K J Lang A Dasgupta and M W  Mahone y  Statistical properties of community structure in large social and information networks In WWW 08 p 695Ö704  C R P almer  P  B  Gibbons and C F aloutsos Anf A f ast and scalable tool for data mining in massive graphs In ACM KDDê02 1:81Ö90  D A Spielman and S T eng A local clustering algorithm for massi v e graphs and its application to nearly-linear time graph partitioning CoRR  abs/0809.3:1Ö25 2008  C V icknair  M  Macias Z Zhao X Nan Y  Chen and D W ilkins A comparison of a graph database and a relational database a data provenance perspective In ACM SE 10 pages 42:1Ö42:6 ACM 2010  R Zaf arani and H Liu Social computing data repository at ASU 2009 8 


 10  Acknowledgments  The authors are deeply indebted to Isabelle Guyon and her team for designing the problematics of the Causality Challenge, and for providing the Lucas0 test data  11  References  1 A g ra wa l, R. Srik a n t, H   Fa st a l g o rithm s f o r m i ning  association rules in large databases Research Report RJ 9839 IBM Almaden Research Center, San Jose, California June 1994  2 Ba stide Y Data mining : algorithmes par niveau techniques d'implantation et applications Doctoral dissertation, UniversitÈ Blaise Pascal, Clermont-Ferrand 2000  3 Botta M., Bo ulic a u t J.-F  Ma sson C., Me o R A Comparison between Query Languages for the Extraction of Association Rules DaWaK 2002 1-10  4 C a do t, M   2 0 06   Extraire et valider les relations complexes en sciences humaines : statistiques, motifs et rËgles d'association Doctoral dissertation, University of Franche-ComtÈ, France. Available online at http://www.loria.fr/~cadot/cadot_these_2006.pdf  5 C a dot M., Ma j  J  B  Zi a d  T   2005 A s s o c i a tion  Rules and Statistics, in J. Wang \(Ed Encyclopedia of Data Warehousing and Mining pp. 74-77\. Hershey, US, Idea Group Publishing  6 Ch e n Q Mining Exceptions and Quantitative Association Rules in Olap Data Cube Master Thesis, Simon Fraser University, 1999   F a b r is C C  A  A  F r eitas Disco v er y o f su rp risin g  patterns by detecting occurrences of Simpson's paradox Research and d eveloppement in intelligent systems XVI Proc ES99. The 19th SGES Int. Conf. on Knowledge-based systems and applied artificial intelligence\. 148-160 Springer-Verlag, 1999  8 F u Y  Discovery of multiple-Level Rules from large Databases Master Thesis, Simon Fraser University, 1996  9 G u ig ue s J  L  e t D uque nne  V   19 86 Fa m ille s  m i ni m a le s  d'implications informatives rÈsultant d'un tableau de donnÈes binaires Math. Sci. Hum n∞95, 5-18  10 G u ille t F., Ha m ilton H., \(200 7 Quality Measures in data mining Springer  11 G u y on I., Ca usa lity Cha lle ng e 1: Ca usa tio n a n d  Prediction, 2008 http://www.causality.inf.ethz.ch/challenge.php  12 H o c J.-M L'analyse planifiÈe des donnÈes en psychologie PUF, Paris, 1983  13 Ho we l D  C    Statistical Methods for Psychology  Duxbury, A Division of International Thomson Publishing Inc., 1997  1 Jaku l i n A    Attribute Interactions in Machine Learning  Master's thesis University of Ljubljana, Slovenija, 2003  15 L u x e nbur g e r M I m plic a tions  pa r tie lle s da ns u n  contexte MathÈmatiques, Informatique et Sciences humaines n∞113,  35-55, 1991  1 P earl  J   Causality models, reasoning, and inference  Cambridge University Press, 2000, 267 - 279  17 Sim p son  E. H., T h e inte r p re ta tion of inte ra c tion in  contingency tables Journal of the Royal Statistical Society  Series B, 13, 238-241, 1951  18 Suz u k i E., K odr a t of f Y   D i s c ov e r y of Sur p r i s i ng  Exception Rules Based on Intensity of Implication Second European Symposium on Principles of Data Mining and Knowledge Discovery Springer-Verlag, London, UK. Source Lecture Notes In Computer Science. 1998, p. 10-18  19 T o iv o n e n H Kle m e n ttine n  M., R onk a i ne n  P   Ha t ne n  K., Mannila H., Pruning and Grouping Discovered Association Rules ECML'95   20 W i ne r B  J B r ow n D  R   Mic h e l s K  M Statistical principles in experimental design third edition\w York McGrawñHill, 1991  21 Za k i M., Mini ng N onR e dun da nt A s s o c i a tion R u le s   Data Mining and Knowledge Discovery 9, 223-248, 2004 Kluwer Academic Publishers, Netherlands  22 Zh u  H  On-Line Analytical Mining of Association Rules Master Thesis, Simon Fraser University, 1998  
98 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





