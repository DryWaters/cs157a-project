MapReducing GEPETO or Towards Conducting a Privacy Analysis on Millions of Mobility Traces S  ebastien Gambs Universit  e de Rennes 1 INRIA/IRISA Rennes France sgambs@irisa.fr Marc-Olivier Killijian CNRS LAAS Universit  e de Toulouse UPS INSA INP ISAE LAAS Toulouse France marco.killijian@laas.fr Izabela Moise INRIA Rennes France izabela.moise@inria.fr Miguel N  u 037 nez del Prado Cortez 
CNRS LAAS Universit  e de Toulouse UPS INSA INP ISAE LAAS Toulouse France mnunezde@laas.fr 
Abstract 
GEPETO for GEoPrivacy-Enhancing TOolkit is a exible software that can be used to visualize sanitize perform inference attacks and measure the utility of a particular geolocated dataset The main objective of GEPETO is to enable a data curator e.g a company a governmental agency or a data protection authority to design tune experiment and evaluate various sanitization algorithms and inference attacks as well as visualizing the following results and evaluating the resulting trade-off between privacy and utility In this paper we propose to adopt the MapReduce paradigm in order to be able to perform 
a privacy analysis on large scale geolocated datasets composed of millions of mobility traces More precisely we design and implement a complete MapReduce-based approach to GEPETO Most of the algorithms used to conduct an inference attack such as sampling 
I I NTRODUCTION 
Index Terms 
means and DJ-Cluster represent good candidates to be abstracted in the MapReduce formalism These algorithms have been implemented with Hadoop and evaluated on a real dataset Preliminary results show that the MapReduced versions of the algorithms can efìciently handle millions of mobility traces Location Privacy Big Data Mining MapReduce Hadoop Data-Intensive Applications 
k 
The advent of ubiquitous devices and the growing development of location-based services have lead to a massive collection of the mobility data of individuals on a daily basis For instance information about the location of an individual can be 1 deduced from the IP address of his computer 1  2 collected by applications running on a smartphone providing information tailored to the current location or used for collaborative tasks such as trafìc monitoring 2  3 attached in the form of a geotag to a picture he has taken without him noticing or 4 given explicitly by checking-in to a geosocial network such as Foursquare 3  Among all the Personally 
Identiìable Information 
PII learning the location of an individual is one of the greatest threats against privacy In particular an inference attack can use mobility data together with some auxiliary information to deduce the points of interests characterizing his mobility to predict his past current and future locations or even to identify his social network 1 http://whatismyipaddress.com/location-feedback 2 www.waze.com 3 www.foursquare.com Thus we believe that the development of tools that can assess the privacy risks incurred by the collection or dissemination of location data is of paramount importance GEPETO for  is a 
GEoPrivacy-Enhancing TOolkit 
exible software that can be used to visualize sanitize perform inference attacks and measure the utility of a particular geolocated dataset The main objective of GEPETO is to enable a data curator  
 a company a governmental agency or a data protection authority to design tune experiment and evaluate various sanitization algorithms and inference attacks as well as to visualize the following results and evaluate the trade-off between privacy and utility The aim of this paper is to describe a new version of GEPETO based on the MapReduce paradigm This MapReduced version of GEPETO is highly distributed and can efìciently handle up to millions of mobility traces The outline of this paper is the following First we provide 
e.g 
a brief overview on the location data and geoprivacy in Section II before introducing the MapReduce programming model and its Hadoop implementation in Section III Afterwards in Section IV we describe in generic terms how it is possible to implement GEPETO based on the MapReduce paradigm before presenting the testbed and the dataset used during the experiments In the following sections we give concrete examples of algorithms from GEPETO that we have MapReduced namely sampling 
means and DJ-Cluster More precisely the adaption of these algorithms to the MapReduce paradigm are detailed respectively in Section V Section VI and Section VII For each algorithm we rst describe its underlying concepts then we identify the 
k 
map 
and the phases and detail the tasks of the mapper and the reducer in their Hadoop implementation before nally reporting on experimental results Finally we conclude in Section VIII with some possible extensions II G EOLOCATION AND PRIVACY Nowadays the development of geolocated devices and the rapid growth of location-based services have multiplied the potential sources of location data The location data generated by these diverse applications varies in its exact form and content but it also shares some common characteristics 
reduce Location data 
2013 IEEE 27th International Symposium on Parallel & Distributed Processing Workshops and PhD Forum 978-0-7695-4979-8/13 $26.00 © 2013 IEEE DOI 10.1109/IPDPSW.2013.180 1937 


Within the context of GEPETO we focus mainly on location data represented in the form of  A mobility trace is usually characterized by An  which can be the real identiìer of the device   Aliceês phone a pseudonym or even the value unknown when full anonymity is required A pseudonym is generally used as a rst protection mechanism to hide the identity of the user while still being able to link different mobility traces that have been generated by him A  which can be a GPS position   latitude and longitude coordinates a spatial area   the name of a neighborhood in a particular city or even a semantic label   home or work A  which can be the exact date and time or just an interval   between 2PM and 6PM Additional information such as the speed and direction for a vehicle the presence of other geolocated devices or individuals in the direct vicinity or even the accuracy of the estimated reported position A is a collection of mobility traces recording the movements of an individual over some period of time A is generally constituted by a set of trails of traces from different individuals Technically this data may have been collected by recording locally the movements of each geolocated device for a certain period of time or centrally by a server that can track the location of these devices in realtime One of the main challenges for geoprivacy is to balance the beneìt for an individual of using a locationbased service with the privacy risks he incurs by doing so For example if Aliceês car is equipped with a GPS and she accepts to participate in the real-time computation of the trafìc map this corresponds to a task that is mutually beneìcial to all the drivers but at the same time Alice wants to have some privacy guarantees that her individual locations will be protected and not broadly disclosed An is an algorithm that takes as input a geolocated dataset  possibly with some auxiliary information  and outputs some additional knowledge F or example an inference attack may consist in identifying the house or the place of work of an individual The auxiliary information reîects any knowledge that the adversary might have gathered   through previous attacks or by accessing some public data sources that may help him in conducting an inference attack More precisely the objective of an inference attack may be to  called POIs characterizing the interests of an individual A POI may be for instance the home or place of work of an individual or locations such as a sport center theater or the headquarters of a political party Revealing the POIs of a particular individual is likely to cause a privacy breach as this data may be used to infer sensitive information such as hobbies religious beliefs political preferences or even potential diseases such as his past present and future locations From the movement patterns it is possible to deduce other PII such as the mode of transport the age or even the lifestyle 4  According to some recent work 13 our mo v ements are easily predictable by nature from the knowledge of his POIs and movement patterns For instance some mobility models such as  25 do not only represent the evolution of the movements of an individual over time but they also attach a semantic label to the visited places From this semantic information the adversary can derive a clearer understanding about the interests of an individual as well as his mobility behavior than simply from his movement patterns  contained in different geolocated datasets or in the same dataset either anonymized or under different pseudonyms In a geolocated context the purpose of a linking attack might be to associate the movements of Aliceês car contained for instance in dataset  with the tracking of her cell phone locations recorded in another dataset  As the POIs of an individual and his movement patterns constitute a form of ngerprinting simply anonymizing or pseudonymizing the geolocated data is clearly not a sufìcient form of privacy protection against linking or de-anonymization attacks A combination of locations can play the role of a if they characterize almost uniquely an individual in the same way as the combination of his rst and last names between individuals by considering that two individuals that are in contact during a non-negligible amount of time share some kind of social link false positive may happen Location data satisìes the property of  as the data can be partitioned into non-overlapping chunks that can be processed independently from one another Moreover performing inference attacks on large geolocated dataset is generally a long costly and resource-consuming task which may take hours and even days for a geolocated dataset composed of millions of mobility traces These two observations motivate the need for parallel and distributed approaches to these algorithms such as the MapReduce abstraction described in the following section III M AP R EDUCE AND H ADOOP The MapReduce programming model was introduced in 2004 by Google as a possible solution to process extremely large datasets up to Terabytes in size The MapReduce approach is able to efìciently handle such large datasets by taking advantage of data independence A developer designing a MapReduce-based application is left 4 See for instance http://www.sensenetworks.com 
mobility traces identiìer e.g spatial coordinate e.g e.g e.g timestamp e.g trail of traces geolocated dataset Geoprivacy inference attack a priori e.g Identify important places Points Of Interests Predict the movement patterns of an individual Learn the semantics of the mobility behavior of an individual semantic trajectories Link the records of the same individual quasi-identiìer Discover social relations data independence MapReduce 
D D aux A B 
         
1938 


with the task of specifying two primary functions and  The phase applies a lter on the input data outputting only relevant information while the step aggregates the map outputs into a nal one When launching a MapReduce computation the input data is rst partitioned into blocks of equal size called  These chunks are stored in a distributed le system deployed across the participating machines The scheduler launches as many map tasks as possible each chunk being processed by a different map task In the MapReduce approach data is represented as key-value pairs Each mapper applies a lter on its input data selecting only the records satisfying a predeìned condition The mapper processes its associated chunk and outputs intermediate keyvalue pairs for a reduce task This reducer collects and aggregates the data produced by the map phase More precisely all values that have the same key are presented to a single reducer through a data shufîe and sorting step in which the data coming from several mappers is transferred to a particular reducer responsible for dealing with it This phase represents the only communication step in MapReduce The reducers apply the user computation on each intermediate key and its corresponding set of values The result of this phase is also the nal output of MapReduce process All the steps of the computation namely the data partitioning the scheduling of mapper/reducer tasks and the transfers of records across nodes are rendered transparent to the users by MapReduce In particular the system assigns tasks to the nodes in the system based on the locations of the data chunks In practice the reducers are spread across the same nodes as the mappers Hadoop the most popular open-source implementation of the MapReduce programming model 3 is designed to efìciently process large datasets by connecting together many commodity computers and making them work in parallel in a so-called  In a Hadoop cluster the nodes in charge of storing the chunks are called while a centralized is responsible for keeping the le metadata and the location of the chunks The Hadoop Distributed File System HDFS handles possible f ailures of nodes through chunk-level replication by default 3 replicas are created When distributing the replicas to the datanodes the HDFS employs a rack-aware policy the rst copy is always written locally the second one is stored on a datanode in the same rack as the rst replica and the third copy is shipped to a datanode belonging to a different rack chosen at random The namenode maintains the list of datanodes storing the replicas of each chunk The Hadoop architecture follows the master-slave design a single master called the  is in charge of multiple slaves named the  one mapped on each node The input data is split into chunks of equal size usually of 64 MB but the chunk size is parametrable A MapReduce job in the Hadoop implementation is split into a set of tasks executed by the tasktrackers as assigned by the jobtracker Each tasktracker has at its disposal a number of available slots for running tasks Each active task uses one slot thus a tasktracker usually executes several tasks simultaneously When dispatching map tasks to tasktrackers one of the main objectives of the jobtracker is to keep the computation as close as possible to the data This is made possible due to the data-layout information previously acquired by the jobtracker If the work cannot be hosted on the actual node in which the data resides priority is given to neighboring nodes   belonging to the same network rack The jobtracker schedules the map tasks as the reducers have to wait for the completion of the map phase execution to generate the intermediate data Apart from the data splitting and scheduling responsibilities the jobtracker is also responsible for monitoring tasks and handling failures IV M AP R EDUCING GEPETO Efìciently analyzing large geolocated datasets composed of millions of mobility traces requires both distribution and parallelization In particular partitioning the dataset into independent small chunks assigned to distinct nodes will speed up the execution of algorithms implemented within GEPETO Therefore we believe that these algorithms in particular the clustering ones represent good candidates to be abstracted in the MapReduce formalism In the following subsections we describe how some algorithms of GEPETO can be adapted to the MapReduce programming model Basically this adaptation requires to structure the algorithm/application into Map/Reduce phases which is sometimes a highly non-trivial task before implementing it on top of Hadoop More precisely a developer must deìne three classes that extend the native Hadoop classes and interfaces the class implements the phase of the application the class deals with the phase of the application and the class speciìes to the Hadoop framework how to run and schedule the MapReduce processes Our experiments were carried out on the Gridê5000 6 e xperimental platform which is a testbed used by researchers to conduct experiments related to distributed and parallel computing The infrastructure of Gridê5000 consists in a highly-conìgurable environment enabling users to perform experiments under real-life conditions for all software layers ranging from network protocols up to applications The Gridê5000 platform is physically distributed on different sites across several French cities In particular more than 20 clusters spread over 10 sites are available and each cluster includes up to 64 computing multi-core nodes summing up to more than 7000 CPU cores overall The particular testbed that we have used comprises nodes belonging to the Parapluie cluster located in Rennes In Parapluie each node is equipped with 2 AMD 1.7GHz CPUs equipped with 12 cores per CPU 48 GB of RAM and 232 GB of storage per node The standard deployment environment used for our experimental setup allocates one node to the jobtracker one node to the namenode while the rest of the nodes is assigned to datanodes and tasktrackers 
Map Reduce map reduce chunks Hadoop Hadoop cluster datanodes namenode jobtracker tasktrackers i.e Mapper map Reducer reduce Driver Experimental platform 
   
1939 


002\003\004\005\004\006\007\010 002\011\012\013\005\004\006\007\010 014 003\002\004\005\004\006\007\010 007\003\004\010\015\007\003\016\017\020 007\003\004\010 004\005\021\010 022\023\024\023\023\022\025\014\026 027\027\030\024\022\025\030\031\025 014 025\027\026\024\027\023\027\032\023 022\030\032\025\030\024\023\030\023\014\027 025\014\014\014\033\014\027\033\014\027 025\022\034\027\032\034\025\022 
002 002  
In our experiments we used the  28 29 which is a geolocated dataset collected by Microsoft Research Asia from 178 users during a time period of several years from April 2007 to August 2012 The whole dataset is composed of les summing up to GB in total Basically each le contains a single trajectory for a speciìc user and it is contained in a directory named after this userês identiìer this folder stores all the userês GPS trajectories A trail of traces   GPS trajectory consists of a sequence of mobility traces belonging to the same user The generic structure of a line in a GeoLife le is shown in Figure 1 along with a concrete example of a GeoLife log The  and elds specify the spatial coordinate in decimal degrees The third eld in Figure 1 has no meaning for this particular dataset while the fth eld represents the date as the number of days elapsed since  Finally the two remaining elds contain the date and time as string values thus acting as the timestamp of the trace trajectories with an average of approximately traces per user for a total distance of about millions kilometers and a total duration of more than hours This data has been collected from different GPS loggers and GPS-phones at different sampling rates However most of the trajectories are very dense   a mobility trace is recorded every 1 to 5 seconds or every 5 to 10 meters and correspond to outdoor movements of users Apart from daily-life activities such as going home or going to work the collected data also contains activities such as shopping dining hiking sightseeing and cycling V A FIRST ILLUSTRATIVE EXAMPLE  SAMPLING Down is a form of temporal aggregation in which a set of mobility traces that have occurred within a time window are merged into a single mobility trace which is called the trace Thus sampling summarizes several mobility traces into a single one Considering a time window of size composed of a sequence of mobility traces that have occurred during this time window we have implemented two sampling techniques 1 The rst sampling technique takes the trace closest to the upper limit of the time window as the representative one Figure 2 while 2 the second technique chooses the trace closest to the middle of the time interval Figure 3 Both sampling techniques have been implemented as MapReduce applications consisting only of   The initial GeoLife dataset of GB is split into MB-size chunks leading to a total of mapper tasks The experimental testbed consisted of nodes on the Parapluie cluster thus each node executes approximately mapper tasks The user can specify as input parameters the size of the considered time window and the desired sampling technique as well as the input and output folders For a time window of seconds the completion of the sampling process on the overall dataset takes minute and seconds Table I summarizes how the size of the dataset changes after sampling at different rates namely minute minutes and minutes It can be observed that the number of traces decreases drastically even when downsampling with a rate of minute which is not really surprising due to the dense nature of the GeoLife dataset in which the GPS logs were collected every to seconds 
18000 1 61 12 30 1899 17621 124 794 1 2 48 000 1 61 64 26 7 4 10 1 4 1 5 10 1 1 5 
Dataset GeoLife GPS trajectories i.e latitude longitude altitude i.e representative MapReduce sampling Running sampling with Hadoop 
    t  
Fig 1 Example of the structure of a GeoLife mobility trace The dataset contains Fig 2 Sampling by taking the mobility trace closest to the upper limit of the time window as the representative one   Fig 3 Sampling by taking the mobility trace closest to the middle of the time window as the representative one map phases The reduce phase is not necessary as sampling represents a computationally cheap operation and can be performed in a single pass Each map task reads its input chunk and processes each line of the chunk corresponding to a mobility trace In GeoLife each trace is composed of the location and the timestamp plus additional information about the speed associated to this trace All the mapper tasks process in parallel their respective chunks by executing the same code In a nutshell for each time window the mapper artiìcially generates a reference trace which is either the trace located at the end or at the middle of the time window depending on the sampling technique used Afterwards the current mobility trace read from the chunk is compared against the reference trace Then the result of this comparison determines if this trace is kept or not as only the trace closest to the reference trace is outputted by the mapper 
1940 


002 002 002 003 002 002 002 003 002 002 002 002 
e.g i.e initialization assignment update i.e i.e i.e MapReducing means 
th 
setup\(Conìguration conf map\(K key V value for if distance distance emitIntermediate setup\(Conìguration conf reduce\(K key v alues for average emit randomCenters\(Conìguration conf main while if hasConverged stop 
TABLE I N UMBER OF TRACES IN THE G EO L IFE DATASET UNDER DIFFERENT SAMPLING CONDITIONS  NO SAMPLING  SAMPLING RATES OF 1 5 AND 10 MINUTES  2033686 155260 41263 23596 VI C LUSTERING WITH 
Algorithm 1 Algorithm 2 Algorithm 3 
k k k k k d d n k k k k k L k k k k k k k k k k k k i i centroids load from file trace value min MAX VALUE center centroids trace center min min  assign center assign trace centroids load from file v values new centroid key new centroid centers k i true i i i or i maxIter 
MEANS After testing the MapReduced version of a simple functionality such as sampling we chose to MapReduce the means clustering algorithm The means algorithm is a classical clustering algorithm partitioning a set of objects   datapoints into clusters by trying to minimize the average distance between the objects in a cluster and its centroid hence the name means An object is generally represented as a vector of attributes thus corresponding to a datapoint in a dimensional space The algorithm takes as input a dataset composed of points and the number of clusters returned by means The output of means is the clusters as well as their respective centroids The parameter has to be speciìed by the user or inferred by cross-validation The user also deìnes a distance metric that quantiìes how close or far are two points relative to each other Typical examples of distance include the Euclidean distance and the Manhattan distance   norm The generic sketch of means is the following 1 Randomly choose points from the input dataset as initial centroids of the clusters  phase 2 For each point assign it to the cluster corresponding to the closest centroid  step 3 For each cluster compute the new centroid by averaging the points assigned to this cluster  step 4 Repeat from step until convergence   clusters are stable or until after a bounded number of iterations In general averaging the points assigned to a cluster is done directly by computing the arithmetic mean of all the points dimension by dimension   attribute by attribute The algorithm has been proven to converge after a nite number of iterations The clustering generated by means is inîuenced by parameters such as the distance used the method for choosing the initial centers of the clusters as well as the choice of the parameter itself While simple the means algorithm can have a high time complexity when applied on large datasets Other limitations of the algorithm include the possibility of being trapped in a local minimum   nding the optimal means clustering is NP-Hard and its sensitivity to changes in the input conditions In addition if it is manually tuned the number of clusters must be known before the clustering begins which may be difìcult to achieve for some type of data Finally another drawback of using the mean as the center of the cluster instead of the median is that outliers can have a sensible impact on a generated center Consider the situation in which the input dataset is a geolocated one MapReducing the means algorithm amounts to MapReducing each iteration of the algorithm thus implementing each means iteration as a MapReduce job The initialization phase of the algorithm randomly picks mobility traces as initial centroids This phase requires no distribution because it is computationally cheap and can be performed by a single node In contrast the two steps of an iteration the assignment step and the update step are perfect candidates for being MapReduced More precisely the map phase is in charge of assigning each mobility trace to the closest centroid while the reduce phase computes the new centroid of each cluster previously built by the mappers The program iterates over the input points and clusters outputting a new directory clusters containing clusters les for the iteration This process uses a mapper/reducer/main as follows kMeans Mapper    trace center kMeans Reducer v kMeans Main randomly choose centroids write to le submit MapReduce job for iteration All these steps as well as the way centroids are updated during each iteration are summarized in Figure 4 
1 
2         0 1  
Initial dataset 1min sampling 5min sampling 10min sampling 
1941 


k convergencedelta maxIter 
k k k 
k k k k k k k convergencedelta  maxIter k 
number of clusters outputted by the algorithm distanceMeasure name of the metric used for measuring distance between points value used for determining the convergence after each iteration maximum number of iterations The 
11 0 5  150 
In the following we report on the performance obtained when running our MapReduced implementation of means on the GeoLife dataset We designed several testing scenarios in order to measure how varying the input parameters impacts the performance of means For each scenario we tuned parameters such as the dataset size and the distance used For measuring the distance between points we considered two metrics the and the  The squared Euclidean distance uses the same formula as the standard Euclidean distance but without computing the square root part As a consequence clustering with the squared Euclidean distance is faster than clustering with the regular one while preserving the order relationship between different points The Haversine formula computes the distance between tw o points o v er the earthês surface by taking into account the shape of the earth The metric used to assess the performance of the implementation is the time required to complete one iteration Note that the number of iterations required by means to converge depends on the initial selection of centroids In our experiments the number of iterations reported corresponds to a rounded average of 3 to 5 trials The runtime arguments for means in our implementation are the following TABLE II R UNTIME ARGUMENTS FOR means algorithm was run on two datasets that are actually subsets of the original GeoLife dataset the rst dataset is composed of 90 users and has a size of 66 MB while the bigger one is composed of 178 users and is of size 128 MB For these experiments we set the input parameters as following  and  The experiments were carried out on the Parapluie cluster of the Gridê5000 platform For Hadoop the namenode was deployed on a dedicated machine the jobtracker on another machine and nally the datanodes and the tasktrackers on the remaining nodes with one entity per machine leading to a total of 7 nodes overall Table III summarizes our results obtained under different testing scenarios corresponding to different values for the runtime arguments The fth column of the table contains the time measured in seconds required to run a means iteration with our MapReduce implementation TABLE III R ESULTS OF THE M AP R EDUCED Mahout is an open-source project that aims at building scalable machine learning libraries A large class of learning algorithms are implemented as part of the Mahout project such as text classiìcation clustering pattern mining recommender algorithms vector similarity    Most 
002\003\004\005\006\007\010\011\006\012 013\014\015 
Running means on Hadoop squared Euclidean distance Haversine distance Related work 
Fig 4 Workîow for the MapReduced version of means MEANS  argument role input path path to the directory containing the input les output path path to the directory to which the output will be written input le the input le from which the initial centroids will be generated clusters path path to the directory storing the current centroids MEANS EXPERIMENTATIONS  Data MB Nb of traces Distance Chunk MB Iter time sec Nb of iter 66 1.050.000 Haversine 64 57 73 66 1.050.000 Squared Euclidean 64 48 72 66 1.050.000 Squared Euclidean 32 41 70 66 1.050.000 Haversine 32 45 73 128 2.033.686 Squared Euclidean 64 51 85 128 2.033.686 Squared Euclidean 32 45 83 128 2.033.686 Haversine 32 48 89 128 2.033.686 Haversine 64 60 93 We observe that a crucial parameter having a big inîuence on the computational time is the chunk size Usually in Hadoop the chunk size can be set to two values 32 MB and 64 MB A smaller chunk size leads to a larger number of chunks which in turn generates more map tasks Obviously a higher number of mappers working in parallel will improve the computational time Moreover as expected the Haversine distance increases the execution time of an iteration compared to the squared Euclidean distance due to the more complex computations that the Haversine formula requires The deployment of Hadoop on a cluster of nodes begins by installing HDFS on the cluster and then starting up the Hadoop daemon processes the namenode and the jobtracker Afterwards the data is uploaded into the HDFS and then partitioned into chunks that are allocated to datanodes Our experiments report on the overhead brought by these initial steps as being approximately 25 seconds While MapReduce jobs are running on tasktrackers the daemon processes run in the background and for each job they allocate data blocks that are accessed by tastrackers When the job is completed the lease on the block is released so as another tasktracker is able to access it for a new launched job These background processes do not introduce an additional overhead on the job completion time as they are executed in parallel with the map and reduce tasks 
1942 


Fig 5 First Phase of DJ-Cluster with MapReduce In order to measure this reduction of the dataset size we ran a set of experiments that apply the preprocessing phase on the sampled datasets at different sampling rates of respectively 1 5 and 10 minutes see Section V Table I Table IV shows the number of traces remaining after the preprocessing phase The value of 
k k k k k k r MinPts 002 r 002 002 002 002  km/h m s 
combiner Density-Joinable Cluster DJ-Cluster preprocessing phase i.e neighborhood identiìcation phase MinPts outlier i.e merging phase MapReducing DJ-Cluster A Preprocessing phase 
of these algorithms are implemented with Mahout including the means clustering algorithm This version of means is designed similarly to our approach as each iteration of means iteration is split into two phases a map phase responsible for assigning objects to the current centroids and a reduce phase recomputing the centroids However the input data to a clustering algorithm of Mahout must be converted to a speciìc Hadoop le format the SequenceFile format In the authors study the performance of Mahout using a large data set and observed a performance in terms of running time comparable to ours In another work a speed-up w as proposed to the MapReduce version of means based on the observation that as the centroid of a cluster is deìned as the mean value of all the points in the cluster part of the sum required for this computation can be computed in advance even before the reducer tasks start This role is assigned to a special entity called a  The main objective of the combiner is to reduce the amount of intermediate data needed to be transferred from mappers to reducers thus reducing the latency of the whole computation The combiner simply sums all the points outputted by the same mapper stored on the local disk of the host resulting in a communication cost that is null Afterwards the reducer collects all the partial sums and computes the mean values of the samples assigned to a same cluster VII DJC LUSTER After implementing the means algorithm we have moved to MapReduce  which is a density-based clustering algorithm that looks for dense neighborhoods of traces Most of the limitations of means are overcome by density-based clustering algorithms In particular clusters of arbitrary shapes can be discovered with most of the density-based clustering algorithms while traditional ones are usually limited to spherical ones Moreover the resulting clusters usually contain less outliers and noise Finally most of the density-based clustering algorithms are deterministic and therefore their output is stable and not affected by the order in which points are processed The density of the neighborhood is deìned by two parameters the radius of a circle deìning the neighborhood and the number of points contained within this circle which must be greater than a predeìned lower bound of  The rationale of the algorithm is that clusters correspond to areas with a higher density of points than areas outside the clusters At the end of the clustering process the datapoints that do not belong to a cluster are marked as noise The DJ-Cluster algorithm proceeds in three phases 1 The discards all the traces corresponding to a movement   whose speed is above  a small predeìned value and replaces all sequences of repeated stationary traces with a single trace 2 The computes the neighborhood of each trace which corresponds to points that are within distance from the point currently considered with the additional constraint that at least should be contained within this neighborhood If no such neighborhood exists the current trace is labeled as   noise 3 The joins all the clusters sharing at least one common trace into a single cluster Each of the three phases of DJCluster can be expressed in the MapReduce programming model Thereafter we describe how each phase can be implemented as one or several MapReduce jobs The rst phase of DJ-Cluster preprocesses the mobility traces by applying two ltering techniques that remove the traces that are not interesting or might perturb the clustering process These two ltering techniques have been implemented in the form of two MapReduce jobs executed in pipeline More precisely the output of the rst job constitutes the input of the second one During the rst MapReduce job stationary traces are kept while moving ones are discarded Identifying the moving traces amounts to measuring the speed of each trace and then removing the traces whose speed is higher than a predeìned threshold for a small value The speed of a trace is computed as the distance traveled between the previous and the next traces divided by the corresponding time difference The computation of the speed for each trace can be performed only by map tasks Each mapper reads its corresponding data chunk and outputs only the traces whose speed is less than  As no aggregation or additional ltering is required the implementation of this part does not include a reduce phase The second ltering technique removes redundant consecutive traces which correspond to traces that have almost the same spatial coordinate but different timestamps Similarly to the rst ltering technique only the map phase is needed The role of the mapper is simply to output the rst trace from a sequence of traces that are redundant Figure 5 shows the two pipelined MapReduce jobs implementing the preprocessing phase of DJ-Cluster These two ltering techniques reduce considerably the amount of data that needs to be processed by the clustering algorithm  the threshold speed was set to which is equivalent to  
002\003\004 002\003\004 005\006\004\007\010 011\003\010\003 012\007\010\004\007\010 011\003\010\003 
002\003\004\005\006\007\010\011\012\013\003\014\015\010 005\007\016\017\006\020 002\003\004\005\006\007\010\021\022\023\004\003\017\016\005\006\010 005\007\016\017\006\020 010 010 
0 72 2 10 
1943 


Algorithm 4 Algorithm 5 
load from le in distributed cache rTree kNN markAsNoise intersects merge 
TABLE IV N UMBER OF TRACES IN THE SAMPLED DATASETS AFTER THE PREPROCESSING PHASE  Sampling rate Unìltered Filter moving traces Remove duplicates 1 min 155260 86416 85743 5 min 41263 23996 23894 10 min 23596 14207 14174 
002 002 002 002\004 003 003 002 002 003 
B Neighborhood computation and merging of clusters R-Tree i.e C Constructing an R-tree with MapReduce R-Trees minimum bounding rectangle i.e partitioning function 
O n n n r MinPts key value rT ree trace value neighborhood  trace M inP ts r neighborhood.size  M inP ts trace const neighborhood key value key key MinPts clusters neighborhood values cluster clusters neighborhood cluster cluster cluster neighborhood new cluster neighborhood cluster clusters clusterId cluster p 
The main challenge of the second phase of DJ-Cluster is to design an efìcient method for discovering the neighbors of a point We rely on a data structure known as  as computing the neighborhood of a point with such a structure can be done in  for the size of the dataset The construction of an R-Tree is described in subsection VII-C For now we assume that an R-Tree indexing all the traces in the dataset is stored in the distributed cache of Hadoop and can be read by any tasktracker The computation of the neighborhood for each trace in the dataset is a good candidate for parallelization and distribution Indeed splitting the dataset into small chunks processed by different nodes is likely to speed up the computation The second and the third parts of DJ-Cluster can be modeled respectively as a map and as a reduce phases Before processing its chunk a mapper rst loads the R-Tree from the distributed cache while executing its setup method For each trace in the chunk the mapper computes the set of neighbors within a distance from the current trace More precisely the mapper searches for the nearest neighbors of a trace by relying on the R-tree This searching process traverses mainly the branches of the R-Tree in which neighbors may be located If the computed neighborhood has less than elements the mapper marks the current trace as noise The pair outputted by the mapper corresponds to a trace and its associated neighborhood Algorithm 4 describes this process DJ-CLuster Mapper A single reducer implements the last phase of the algorithm as the merging of joinable neighborhoods must be done by a centralized entity that obtains a knowledge about all current neighborhoods built by mappers In the intermediate pair emitted by mappers the eld is set to a constant value such that all pairs are collected by a single reducer   all intermediate pairs that have the same eld are redirected towards the same reducer This reducer collects all neighborhoods outputted by the mappers and then proceeds to building the clusters The construction of clusters is done by merging all joinable neighborhoods By deìnition two neighborhoods are joinable if there exists at least one trace such that both neighborhoods contain it The reducer merges all joinable neighborhoods with existing clusters or creates new clusters if a neighborhood cannot be joined with any of the existing clusters Thus by the end of the clustering process each trace is either assigned to a cluster or marked as noise In addition the clusters are assured to be non-overlapping and to contain at least mobility traces The output of this reduce phase also the output of the algorithm consists in the computed clusters All these steps are shown in Algorithm 5 DJ-Cluster Reducer  are data structures commonly used for inde xing multidimensional data In a nutshell an R-Tree groups datapoints into clusters and represents them through their in their upper level in the tree At the leaf level each rectangle contains only a single datapoint   each rectangle is a point while higher levels aggregate an increasing number of datapoints When querying an R-Tree only the bounding rectangles intersecting the current query are traversed The indexing of large datasets into an R-Tree structure can be implemented as a MapReduce algorithm In this previous work each point in the dataset is deìned by two attributes a location in some spatial domain used to guide the construction of the R-Tree and a unique identiìer used to reference the object in the R-Tree The construction of an R-Tree is a process that can be split into three phases 1 The partition of datapoints into clusters 2 The indexing of each cluster into a small R-Tree 3 The merging of the small R-Trees obtained from the previous phase into a nal one indexing the whole dataset The rst two phases are MapReduced while the last phase is executed sequentially by a single node due to its low computational complexity Figure 6 illustrates how these three phases are executed sequentially More precisely the rst phase computes a assigning each datapoint of the initial dataset to one 
 log                
setup\(Conìguration conf map\(K key V value if emitIntermediate setup\(Conìguration conf reduce\(K key v alues for for if else for emit 
1944 


i.e space-ìlling curves Z-order Hilbert cf cf cf i.e cf 
002\003\004 005\006\004 007 010\003\007\003  011\012\013 014\012 002\003\004 005\006\004 007 010\003\007\003 011\012\013 014\012 
p p  p p p p k 
002 
partitions This partitioning function should yield equally-sized partitions and preserve at the same time data locality   points that are close in the spatial domain should be assigned to the same partition In order to satisfy these constraints the partitioning function has to map multidimensional datapoints into an ordered sequence of unidimensional values In practice this transformation is performed with the help of that precisely map multidimensional data to one dimension while preserving data locality In our R-Tree construction we implemented and tested two types of space-ìlling curves the curve and the curve The MapReduce design for this phase consists of several mappers and one reducer Each mapper Algorithm 6 Appendix samples a predeìned number of objects from its data chunk and outputs the corresponding single-dimensional values obtained after applying the space-ìlling curve Afterwards the reducer Algorithm 7 Appendix collects a set of single dimensional values from all mappers orders this set and then determines partitioning points in the sequence partitioning points delimit the boundaries of each partition The second phase concurrently builds individual R-Trees by indexing the partitions outputted by the rst phase The mappers Algorithm 8 Appendix split the dataset into partitions using the space-ìlling curve computed during the rst phase Each mapper processes its chunk and assigns each object it reads to a partition identiìer The intermediate key is represented by the partition identiìer such that all datapoints sharing the same key   belonging to the same partition will be collected by the same reducer Then each reducer Algorithm 9 Appendix constructs the R-Tree associated with its partition leading to a total of reducers building small R-Trees Finally the last phase merges the small R-Trees into a global one indexing all datapoints of the initial dataset VIII C ONCLUSION In this paper we have proposed to adopt the MapReduce paradigm in order to be able to perform a privacy analysis on large scale geolocated datasets composed of millions of mobility traces More precisely we have developed a complete MapReduce-based approach to GEPETO for GEoPrivacyEnhancing TOolkit a softw are that can be used to design tune experiment and evaluate various sanitization algorithms and inference attacks on location data as well as to visualize the resulting data Most of the algorithms used to conduct an inference attack represent good candidates to be abstracted in the MapReduce formalism For instance we have designed MapReduced versions of sampling as well as the means and the DJ-Cluster clustering algorithms and integrate them within the framework of GEPETO These algorithms have been implemented with Hadoop and evaluated on a real dataset Preliminary results show that the MapReduced versions of the algorithms can efìciently handle millions of mobility traces Currently the clustering algorithms that we have implemented can be used primarily to extract the POIs of an individual from his trail of mobility traces which correspond only to one possible type of inference attack In the future we aim at integrating other inference techniques within the MapReduced framework of GEPETO In particular we want to develop algorithms for learning a mobility model out of the mobility traces of an individual such as Mobility Markov Chains MMCs In a nutshell a MMC represents in a compact way the mobility behavior of an individual and can be used to predict his future locations or even to perform deanonymization attacks thus extending the range of inference attacks available within GEPETO We also want to design MapReduced versions of geo-sanitization mechanisms such as geographical masks that modify the spatial coordinate of a mobility trace by adding some random noise or aggregate several mobility traces into a single spatial coordinate More sophisticated geo-sanitization methods will also be integrated at a later stage such as spatial cloaking techniques and mix zones A CKNOWLEDGMENT This work was funded by the location privacy activity of EIT ICT labs Experiments presented in this paper were carried out using the Gridê5000 experimental testbed being developed under the INRIA ALADDIN development action with support from CNRS RENATER and several Universities as well as other funding bodies see https://www.grid5000.fr R EFERENCES  The Apache Hadoop Project http://www hadoop.or g  The Apache Mahout Frame w ork http://mahout.apache.or g  The Hadoop MapReduce Frame w ork http://hadoop.apache.or g mapreduce  L O Alv ares V  Bogorn y  B K uijpers J A F  de Mac 032 edo B Moelans and A A Vaisman A model for enriching trajectories with semantic geographical information In 
002\003\004\005\006\007\010\007\011\003\012\013\003\014\013\005\010\015\007\011\007\011\003\012\011\012\016\013\014\006\012\017\007\011\003\012\013 013 002\003\012\020\007\015\006\017\007\011\003\012\013\003\014\013\020\004\010\021\021\013\022\023\024\015\025\025\020 013 
002\003\004\005\006\007\010 011 002\003\004\005\006\007\012 002\003\004\005\006\007\013    011 005\014\004\015\015\007\016\017\020\021\006\006\005 005\014\004\015\015\007\016\017\020\021\006\006\005 022\015\023\024\004\015\007\016\017\020\021\006\006  
Proceedings of the 15th ACM International Symposium on Geographic Information Systems Proceedings of the Workshops of the 2nd IEEE Conference on Pervasive Computing and Communications Proceedings of the 6th IEEE/ACM International Workshop on Grid Computing 
010 010 010 010 
 1 
002 
002\003\004\005\006\007\010\005\011\002\003\012\002\013\012 005\014\015\012\016\017\002\020\021\017 022\023\024\006\015\015\012 
Fig 6 Building an R-Tree with MapReduce of the  page 22 2007  A R Beresford and F  Stajano Mix zones User pri v ac y in locationaware services In  pages 127 131 2004  F  Cappello E Caron M Dayde F  Desprez E Jeannot Y  Je gou S Lanteri J Leduc N Melab G Mornet R Namyst P Primet and O Richard Gridê5000 A large scale reconìgurable controlable and monitorable grid platform In  pages 99Ö106 Seattle Washington USA November 2005 
1945 


size 
sample scalar sample const scalar values step total samples R count i values i step count values i count count partitions scalar value i R partitions i  scalar  partitions i partitionId i partitionId value partitionId key tree values tree treeRoot 
      0 mod 0    1   0      1       
 A Cary  Z Sun V  Hristidis and N Rishe Experiences on processing spatial data with mapreduce In  pages 302Ö319 Berlin Heidelberg 2009 Springer-Verlag  J Dean and S Ghema w at Mapreduce simpliìed data processing on large clusters  51\(1 2008  R M Este v es R P ais and C Rong K-means clustering in the cloud  a mahout test In  WAINA 11 pages 514Ö519 Washington DC USA 2011 IEEE Computer Society  S Gambs M.-O Killijian and M N del Prado GEPET O a GEoPri v ac y Enhancing Toolkit In  pages 1071Ö1076 April 2010  S Gambs M.-O Killijian and M N del Prado Cortez Sho w me ho w you move and I will tell you who you are  4\(2 2011  P  Golle and K P artridge On the anon ymity of home/w ork location pairs  pages 390Ö397 May 2009  M C Gonzalez C A Hidalgo and A.-L Barabasi Understanding individual human mobility patterns  453\(7196 June 2008  M Gruteser and D Grunw ald Anon ymous usage of location-based services through spatial and temporal cloaking In  2003  A Guttman R-trees A dynamic inde x structure for spatial searching In B Yormark editor  pages 47Ö57 ACM Press 1984  L Jedrzejczyk B A Price A K Bandara and B Nuseibeh I kno w what you did last summer Risks of location data leakage in mobile and social computing  November 2009  Y  J  egou S Lant  eri J Leduc M N G Mornet R Namyst P Primet B Quetier O Richard E Talbi and T Ir  ea Gridê5000 a large scale and highly reconìgurable experimental grid testbed  20\(4 November 2006  J H Kang B Ste w arta G Borriello and W  W elbourne Extracting places from traces of locations In  pages 110Ö118 2004  J Krumm Inference attacks on location tracks  pages 127Ö143 2007  J K La wder and P  J H King Using space-ìlling curv es for multidimensional indexing In  pages 20Ö35 London UK 2000 SpringerVerlag  J B MacQueen Some methods for classiìcation and analysis of multivariate observations In L M L Cam and J Neyman editors  volume 1 pages 281Ö297 University of California Press 1967  K Shv achk o H K uang S Radia and R Chansler  The Hadoop Distributed File System In  2010  R W  Sinnott V irtues of the ha v ersine In  volume 68 page 159 1984  C Song Z Qu N Blumm and A.-L Barabasi Limits of predictability in human mobility  327\(5968 2010  S Spaccapietra C P arent M L Damiani J A F  de Mac 032 edo F Porto and C Vangenot A conceptual view on trajectories  65\(1 2008  W  Zhao H Ma and Q He P arallel k-means clustering based on mapreduce In  CloudCom 09 pages 674Ö679 Berlin Heidelberg 2009 Springer-Verlag  Y  Zheng Q Li Y  Chen X Xie and W Y  Ma Understanding mobility based on gps data In  UbiComp 08 pages 312Ö321 New York NY USA 2008 ACM  Y  Zheng X Xie and W Y  Ma Geolife A collaborati v e social networking service among user location and trajectory  33\(2 2010  Y  Zheng L Zhang X Xie and W Y  Ma Mining interesting locations and travel sequences from gps trajectories In  WWW 09 pages 791 800 New York NY USA 2009 ACM  C Zhou D Frank o wski P  Ludford S Shekhar  and L T erv een Discovering personal gazetteers An interactive clustering approach In  pages 266Ö273 ACM Press 2004 A PPENDIX 
space lling curve order space lling curve build RTree 
Algorithm 6 Algorithm 7 Algorithm 8 break Algorithm 9 
R-Tree First Phase Mapper randomly sample objects in chunk R-Tree First Phase Reducer R-Tree Second Phase Mapper load ouput of rst phase R-Tree Second Phase Reducer 
map\(K key V value emitIntermediate reduce\(K key v alues for if emit setup\(Conìguration conf map\(K key V value for if emitIntermediate reduce\(K key v alues emit 
002 002 002  002 003 002 002 002 003 002 002 002 
Proceedings of the 21st International Conference on Scientiìc and Statistical Database Management Communications of the ACM Proceedings of the 2011 IEEE Workshops of International Conference on Advanced Information Networking and Applications Proceedings of the International Workshop on Advances in Mobile Computing and Applications Security Privacy and Trust held in conjunction with the 24th IEEE AINA conference Perth Australia Transactions on Data Privacy Proceedings of the 7th International Conference on Pervasive Computing Nature Proceedings of the 1st International Conference on Mobile Systems Applications and Services Proceedings of Annual Meeting of ACM SIGMOD Department of Computing Faculty of Mathematics Computing and Technology The Open University International Journal of High Performance Computing Applications Proceedings of the 2nd ACM international workshop on Wireless mobile applications and services on WLAN hotspots Pervasive Computing Proceedings of the 17th British National Conference on Databases Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies Sky and Telescope Science Data Knowl Eng Proceedings of the 1st International Conference on Cloud Computing Proceedings of the 10th International Conference on Ubiquitous Computing IIEEE Data\(base Engineering Bulletin Proceedings of the 18th International Conference on World Wide Web Proceedings of the 12th ACM International Workshop on Geographic Information Systems 
1946 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





