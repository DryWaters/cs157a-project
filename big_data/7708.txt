Itemset A ppr oximation Using Constrained Binary Matrix F actorization Mirisaee Univ  Gr enoble Alps/CNRS Gr enoble  F r ance Hamid.Mirisaee@ima g fr Gaussier Univ  Gr enoble Alps/CNRS Gr enoble  F r ance Eric.Gaussier@ima g fr ermier Univ  Gr enoble Alps/CNRS Gr enoble  F r ance Ale xandr e T ermier@ima g fr Abstract W e addr ess in this paper the pr oblem of efﬁciently nding a few number of r epr esentati v e fr equent itemsets in transaction matrices T o do so we pr opose to r ely on matrix decomposition techniques and mor e pr ecisely on Constrained Binary Matrix F actorization CBMF which decomposes a gi v en binary matrix into the pr oduct of tw o lo wer dimen 
sional binary matrices called factors W e rst sho w  under binary constraints that one can inter pr et the rst factor as a transaction matrix operating on pack ets of items wher eas the second factor indicates which item belongs to which pack et W e then f ormally pr o v e that one can dir ectly mine the CBMF factors in order to nd appr oximate itemsets of a gi v en size and support in the original transaction matrix Then thr ough a detailed experimental study  we sho w that the fr equent itemsets pr oduced by ou r method r epr esent a sign iﬁcant portion of the set of all fr equent itemsets according to existing metrics while being up to se v eral orders of magnitude less numer ous I ION Frequent pattern mining is a major domain of pattern mining Its goal is to automatically e xtract frequently occur ring patterns in lar ge datasets The fundamental probl em of 
frequent pattern mining is to mine frequent itemsets  i.e sets of item s frequently occurring together in transactional databases This problem originated from the analysis of commercial databases and w as later e xtended to more comple x patterns and a broad range o f domains see 2 xample Frequent pattern mining algorithms ha v e to e xplore a huge combinatorial space of patterns This leads to tw o main problems rst mining algorithms usually need a lar ge amount of time to output the results Second the set of frequent patterns is often huge millions of frequent patterns are commonplace This issue mak es the data analysis task cult There has been a lar ge body of research on condensed representations of frequent patterns such as closed frequent patterns 4 that can usually output one order of mag 
nitude less patterns without loosing information Ho we v er  one order of magnitude reduction is often not enough when dealing with millions of patterns hence a current trend of research focuses on disco v ering a small set of patterns that w ould represent most of the information contained in the set of all frequent patterns and that w ould be small enough to be manageable by the analyst 5 6 Our contrib ution is to i ntroduce a ne w method for computing a reduced set of representati v e frequent patterns that is both computationally ef cient and produces v ery fe w high quality frequent patterns Our method is based on Constrained Binary Matrix F actorization CBMF 7  which has been widely used in data mining applications recently  This technique reduces data dimensionality while k eeping most of its information Exploiting this tool we theoretically study the characteris tics of a matrix decom 
position method adapted to frequent itemset mining W e then pro v e that e xtracting frequent itemsets from the f actors of the decomposition guarantees to nd e xisting frequent itemsets with an approximation support threshold and size Additionally  we s ho w through a detailed e xperime ntal study that the frequent itemsets output by our method represent a signiﬁcant portion of the set of all frequent itemsets according to e xis ting metrics 9 while being up to nine orders of magnitude less numerous The outline of this paper is as follo ws Section II describes the main notions and Section III re vie ws the state of the art The core of the paper is composed of the theoretical analysis in Section IV Section V e xplains the implementation and Section VI reports e xperimental results Lastly  Section VII concludes the paper and discusses some fut ure research directions opened by this w ork II P 
IES A F r equent itemset mining Let I be a set of items An itemset is a subset of I A transaction database TDB is a set of transactions D where each transaction t  D of I  W e will denote by m the size of a transa ction database i.e its number of transactions F or an itemset P  I transaction database D 
the support of P in D denoted sup p or t  D   is the number of transactions of D including P  t  P D   t  t  D  P  t  The 
equency of P is fr e q u e n c y  P D  sup p o r t  D  m  P is equent if its support is greater than a gi v en minimum support threshold p  fr e q u e n c y  P D   p  


B Constr ained Binary Matrix F actorization The general goal of matrix decomposition is to f actorize a matrix into the product of tw o or more smaller matrices The Constrained Binary Matrix F actorization CBMF problem as a special case of matrix decomposition is described en X   0  1  M  N and K  N  n M N                n W  H  X  W  H  p  p 1 or p 2 subject to  i  W   0  1  M  K  H   0  1  K  N  ii  W  H   0  1  M  N where    p the L 1  p 1 the L 2  p 2  Constraint i guarantees that the f actors W and H are binary and constraint ii that the matrix the y  W  H  is also binary  S TA T E O F T H E A R T W e brieﬂy re vie w the major studies in the recent domain of nding an informati v e subset of the frequent patterns In order to output a small set of frequent patterns some studies e xplicitly deﬁne a tar geted number of patterns and criteria that ha v e to be v eriﬁed by the set of patterns Guns et al  for e xample present an e xhausti v e approach where the criteria to be satisﬁed are e xpressed through constraints such as co v erage or area Other approaches such as 10 mak e use of redundanc y-based heuristics to determine the set of patterns Other studies rel y on compression either o v er all the frequent patterns 11 or o v er the dataset 5 In the rst case frequent itemsets that best approximate the set of all frequent itemsets are computed In the second case the authors e xploit the Minimal Description Length principle to compute which set of patterns best compresses the database All of the pre viously cited studies consider the set of all frequent patterns in order to compute the reduced subset of in is a tw o pass approach that rst has to compute all the frequent patterns and then select a subset of them which is computationally inef cient On the in computes in one pass a subset of frequent patterns that optimize a gi v en criterion Ho we v er  to do so it has to e xplore a comple x search space o v er the sets of patterns which is also computationally e xpensi v e Our approach ho we v er  is dif ferent from the pre vious studies in that we rely on main components of the data One should note that the proposed approach is also dif ferent form studies lik e 12 which use the Boolean decomposition and closed itemsets to reduce the dimension Contrary to other pattern set disco v ery methods by mo ving the b urden of reducing the number of patterns from the pattern search space to the input matrix itself one can design mining algorithms that are computational ly ef cient and that yield results IV  T HE ORE TIC AL AN AL YS IS W e rst deﬁne tw o kinds of decompositions str ong decomposition and appr oximately str ong decomposition that are well adapted for frequent itemset mining Then we sho w the relation between the frequent item sets that can be disco v ered in the matrix decomposition f actors and those of matrix W e rst deﬁne the matrix notations used in this paper  F or a gi v en matrix X m  n  X ij denotes the element of the m atrix w i and col umn j  X i  summation w i i.e X i    n k 1 X ik and X  j denotes the the j th of X i.e X  j   m k 1 X kj  The r o w v ector tak en from column-wise summation of X is by X   Similarly  the column v ector obtained from of X is sho wn by X  The i th of X is denoted by X i the j th of X is denoted by X j  The follo wing e xample illustrates the notations that we use in this study  Example 1 Consider the follo wing matrix X this matrix X 3  4 0  X 1  4  X  4 1  X   X   X 2 and X 5 w X    1110100 1111000 1110000   X    3331100  X     4 4 3   X 2   1111000  X 5    1 0 0   W e no w intuiti v ely e xplain ho w itemsets can be found in the f actors of a matrix decomposition Consider a dematrix X with k actors X m  n  W m  k  H k  n  Each of the k of W can be mapped to a set of columns of X through the product with H of X correspond to items columns of W correspond to items  On the other hand in H  one observ es the relation between each pack et of items of W and each original item of X  Using this setting one can easily reconstruct itemsets of X  T o formally deﬁne this setting we start by deﬁning the notion of ansaction matrix  that will be the input of our method 1 V alid transaction matrix matrix X m  n is called a valid tr ansaction matrix if it is binary and ther e is no r ows or columns with all elements equal to zer o Mor e  X m  n is a valid tr ansaction matrix if 1  i j  1  i  m  1  j  n  X ij   0  1  2  j  1  i  m  1  j  n  X i   0  X  j  0 No w we consider dif ferent binary decompositions of a v alid matrix 


decomposition In this case the decomposition is e xact 2 Str ong decomposition matrix X m  n is str ongly decomposable if 1 it is a valid tr ansaction matrix 2  k  1  k  n suc h that X m  n  W m  k  H k  n 3 W   0  1   H   0  1   W  H   0  1  Condition 3 states that the f actors W and H as well as  W  H  are binary  The notion of strong decomposition furthermore implies that one can nd f actors that are all rele v ant in the follo wing sense 1 If X can be decomposed str ongly  then ther e is a str ong decomposition of the form X m  n  W m  k  H k  n suc h that ther e is no columns in W in H with all elements equal to zer o Mor e f ormally  for all t  1  t  k  W  t  0 and H t   0  oof If X and W resp H  contains one or more fully-zero columns resp ro ws then one can remo v e the zero columns of W their in H resp zero ro ws of H and their in W  and still obtain through the product of the simpliﬁed matrices a strong decomposition of X with k  that k  k  Using the abo v e deﬁnitions we are no w ready to pro vide the follo wing theorem which describes a rst link between the itemsets mined from W and H in X  Theor em 1 Let X m  n  W m  k  H k  n strong decomposition of X  If a pack et of items with support v alue f is found in W i.e  p  1  p  k  W  p  f there is at least one itemset with support v alue of at least f in X  oof et p  1  p  k  has support alue f in W i.e  m i 1 W ip  f deﬁnition of decomposition and of Prop 1 there e xists at least one q  1  q  n that H pq 1 all i  1  i  m  we ha v e X iq   k j 1 W ij H jq  W ip H pq thus  m i 1 X iq   m i 1 W ip H pq  H pq  m i 1 W ip nally  m i 1 X iq  f theorem In simpler terms Theorem 1 sho ws that in a strong decomposition if there e xists a frequent pac k et of items in W with of f  then there is a corresponding frequent itemset with support v alue of at least f in X  An important consequence of Theorem 1 is re g arding the size of the captured i temset In this theorem for a p such that W  p  f one q is found such that H pq 1  then there is an itemset of size one a singleton which is not v ery useful Ho we v er  it is easy to pro v e that if se v eral such q  s are found then an itemset and not a singleton in X is captured This is e xpressed in the follo wing corollary  1 If X m  n  W m  k  H k  n is a str ong of X  and ther e e xists a pac k et of items p  1  p  k size l in H  H p   l value f in W  W  p  f  then ther e e xists an itemset of size least l and support value of at least f in X  Corollary 1 can be easily deri v ed from Theorem 1 This result is v ery important since it sho ws that one is able to mine frequent itemsets of an y size using the f actors of the decomposition As one can see once a transaction matrix has been strongly decomposed into latent binary f actors we can ef ciently obtain some frequent itemsets from the decomposition without reloading the data into memory  In practice ho we v er  a strong decomposition of a transaction matrix may not e xist and we no w turn to a more realistic decomposition and reformulate the materials pro vided so f ar  B Appr oximately str ong decomposition A direct e xtension of strong decompositions is to no longer assume that the decomposition is perfect i.e has no error  This can be simply done by adding an error term in the reconstruction of X actors 3  A ppr oximately str ong decomposi tion  A matrix X m  n can be decomposed appr oximately str ongly if 1 it is a valid tr ansaction matrix 2  k  1  k  n suc h that X m  n  W m  k  H k  n   m  n 3 W   0  1  m  k  H   0  1  k  n and W  H   0  1  m  n Note that ha ving v alues of X  W  H and W  H in  0  1  implies     1  0  1  m  n  It should be also noted that Prop 1 holds for the approximate decomposition as well Based on deﬁnition 3 no w we can re write Theorem 1 2 Let X m  n  W m  k  H k  n   m  n an appr oximately str ong decomposition of X let  max be the maximum of absolute values of    If a fr equent pac k et of items with support value of f in W   p  1  p  k  W  p  f  then ther e is at least one itemset with support value of at least f   max in X  oof Suppose that pack et p  1  p  k support v alue of f in W i.e  m i 1 W ip  f the of appr oximately str ong decomposition of Prop 1 there e xists at least one q  1  q  n  such that H pq 1  Then for all i  1  i  m e X iq   k j 1 W ij H jq   iq  W ip H pq   iq And thus  m i 1 X iq   m i 1 W ip H pq   m i 1  iq  H pq  m i 1 W ip   m i 1  iq  f   m i 1  iq nally  m i 1 X iq  f   max which establishes the theorem Considering itemsets of a gi v en size with approximately strong decompositions is ho we v er slightly more dif cult than 


with strong decompositions The follo wing theorem e xtends the corollary of Theorem 1 Corollary 1 to the case of approximately strong decompositions 3 Let X m  n  W m  k  H k  n   m  n be an appr oximately str ong decomposition of X and   a b   W a   b  e  r epr esents dot pr oduct  a   1    k  b   1   n   Also let   p   n 1  j  n H pj    j    p 1  p  k  Now  if ther e e xists a pac k et of items p  1  p  k size l e g   q 1 q 2    q l  in H  H pq 1  H pq 2    H pq l 1 and H p   l  with support of f in W  W  p  f  then ther e e xists an itemset most l and with support value of at least f    p  in X  Furthermor e  if   p  then  q 1 q 2   q l  an size l in X with support value of f  oof First note that   b  reconstruction error for item b et a  and that   p  corresponds to the minimum reconstruction error of all items belonging to pack et p  W ithout loss of generality  we assume that a frequent itemset of size l and support f the rst f ro ws of W rst l columns of H wing illustration W            1  k 11    f 1    0       m 0   H    1  ll 1  n 11  10   0    k   e 1 st item   f i 1 X i 1   f i 1 W i 1 H 11   f i 1  i 1    l th item   f i 1 X il   f i 1 W i 1 H 1 l   f i 1  il which can be re written 1 st item   f i 1 X i 1   m i  1 W i  1 H 11   m i  1 W i  1  i  1    l th item   f i 1 X il   m i 1 W i 1 H 1 l   m i  1 W i  1  i  l Furthermore according to the deﬁnition of   p  y et p e 1 st item   f i 1 X i 1  f   1  1  f   1    l th item   f i 1 X il  f   1 l   f   1 which sho ws that there is an itemset included in  i 1   i l  with support of at least f   1 if   1  0  then  i 1   i l  is an itemset of size l and support v alue f in X  Considering Theorem 3 one can easily mine approximate itemsets using an approximately strong decomposition Algorithm 1 sho ws this procedure note that according to when   p   one is able to identify an itemset in X with its e xact support and size 1 Min-supp Min-size Approximate Itemset Mining Input W  H ector   min-supp f min-size l Output S  a set of i temsets with length of at most l and least f 1 S  2 all ws r in H do 3 if H r   l then 4 if W  r    r   f then 5 P  items corresponding a v ailable in r 6 S  S  P  7 if 8 if 9 end f or I MPL EME NT A T ION The pre vious de v elopment sho ws ho w one can mine a transaction matrix from its decomposition Based on that ha ving a v alid transaction matrix one can easily apply the decomposition to obtain the f actors i.e W and H start mining the itemsets According to the input of Algorithm 1 we need a decomposition method to obtain high quality f actors There are generally tw o approaches to solv e the CBMF problem described in Section II one should note that we are not considering the Boolean matrix decomposition  in this paper since all our theoretical analysis is based on classical matrix multiplication One of these methods has been described in 7 and is called oximus  In this approach a decomposition problem with k is iterati v ely solv ed via simpler problems with k 1 The solution to the original problem is then the aggre g ation of all k 1 decompositions Another more recent approach to solv e the CBMF probin and relies on k-means we refer to as clustering-based ference between the clustering-based approach and oximus that in the former  the problem of decomposing into k f actors is solv ed directly starting with k f actors from the be ginning while the latter solv es the problem indirectly solv es a k 1 time 


In the e xperimental section we use the clustering-based approach since it performs better decomposition comthe oximus  One s hould note that both of these methods obtain binary reconstruction through enforcing the orthogonality on one of the f actors which is a suf cient condition to ha v e a binary reconstruction VI E S In this section we e v aluate the proposed method called DIM Decomposition Itemset Miner based on the quality of produced itemsets and the mining ef cienc y through a set of e xperiments on dif ferent datasets The qualitati v e e xperiments compare the set of itemsets output by DIM with the s et of all closed fre q ue n t itemsets output by algorithm The ef cienc y e xperiments compare the mining time of the LCM F or LCM we use its authors C implement ation while DIM is implemented in Matlab which has v ery ef cient matrix multiplication and decomposition implementations The e xperiments were conduced on an computer with an Intel Xeon E-2630 with 6 cores  2.30 Ghz with 32 GB of RAM No multiprocessing or multithreading has been applied on DIM The datas ets used in the e xperiments are publicly a v ailable datasets from the FIMI repository 1  F ollo wing the methodin we choose pumsb  accidents and T40I10D100k as representati v es of dense  dense and se datasets respecti v ely  W e ha v e chosen one dataset from each cate gory for space reasons ho we v er  one should note that the results are similar for other datasets of each cate gory  valuation Contrary to other frequent itemset mining techniques the comple xity of our approach does not depend on the minimum support threshold i.e once a decomposition is obtained the mining process is quite f ast and straightforw ard This e xperiment may be considered unf air  as LCM aims at nding all closed frequent itemsets while our approach only computes a small set of representati v e frequent itemsets Our approach might thus be compared with approaches computing a representati v e subset of the frequent itemsets Ho we v er  as discussed in the related w ork section section III most e xisting approaches to identify a representati v e subset of the frequent itemsets are tw o-pass approaches that rst compute all closed frequent itemsets and then compute a subset of these frequent itemsets The e xisting one-pass approach 6 is more general than ours and does an e xhausti v e search on a huge search space Its time comple xity is thus much higher than ours Therefor  their running time is higher than the one of LCM alone and comparing our running time to the one of LCM is indeed unf air  b ut to our disadv antage 1 http://ﬁmi.ua.ac.be/data last visited 01/07/2014 Figure 1 present the mining time with a v arying minimum support for both LCM and DIM As e xpected DIM running time is constant Note that for each support v alue we reported the decomposition time clustering-based decomposition time plus itemset mining time Algorithm 1 In practice the decomposition has only to be done once and then it can be e xploited by Algorithm 1 for an y support v alue It can be observ ed that for high support v alues LCM is f aster than DIM Ho we v er  for lo wer support v alues LCM suf fers from the combinatori al e xplosion of the number of results and needs much more computation time than DIM This point is more crucial in structured dense datasets lik e pumsb  where e v en with high support v alues itemset mining task is v ery dif cult In dense datasets lik e accidents LCM becomes v ery slo w for support v alues smaller than 10 In sparse datasets lik e T40I10D100k  it is easy to nd the itemsets with high support v alues Ho we v er  as these datasets contain quite huge itemsets with small support v alues LCM spends a long time to nd closed itemsets B Qualitative e valuation DIM mines v ery fe w itemsets and the ef cienc y e xperiments ha v e sho wn that for lo w support v alues this approach could output a solution f aster than state-of-theart algorithms It thus remains to e v aluate the quality of the itemsets found w r t the complete set of closed frequent itemsets F or this we rely on metrics presented in 9 designed to e v aluate the quality of approximate frequent itemset mining algorithms In these metrics itemsets output by LCM are called base-itemsets  and itemsets outputs by DIM are called found-itemsets  Reco v erability This metric measures ho w well a collection of found-itemsets can co v er base-itemsets Consequently  this met ric is similar to ecall  F or a base-itemset B i  reco v erability is calculated as follo ws among all founditemsets F j we look for the one which has the maximum number of items in common with B i namely F i max The reco v erability of B i as re c o v e r a b i l i t y  B i   B i  F i max   B i  1 The total reco v erability is a weighted a v erage bigger base-itemsets contrib ute more than smaller ones of the reco v erability of all base-itemsets Pr ecision As we can see ha ving one single lar ge founditemset possibly including all items results in a reco v er ability of 1 for all base-itemsets Therefore we need another metric to penalize such cases Spuriousness is a metric to e v aluate the quality of found-itemsets F or a found-itemset F i  spuriousness is deﬁned as follo ws among all baseitemsets we nd the one which has the maximum number with F i namely B i max spuriousness 


0 500 1000 1500 2000 2500 30 35 40 45 50 Time\(s min-supp pumsb LCM DIM   0 50 100 150 200 250 300 350 5 10 15 20 Time\(s min-supp accidents LCM DIM   0 50 100 150 200 250 300 350 400 450 0.1 0.3 0.5 0.7 1 Time\(s min-supp T40I10D100k LCM DIM   Figure 1 LCM and DIM time comparison v arying the min-supp for pumsb  accidents and T40I10D100k of F i is deﬁned as follo ws sp ur iousn ess  F i   F i  F i  B i max   F i  In this case also the total spuriousness is computed by a weighted a v erage bigger found-itemsets contrib ute more than smaller ones of the spuriousness of each found-itemset Precision of an itemset is then deﬁned as 1  sp ur iousn ess  A summary of the results is presented in T able I Number of closed frequent itemsets for both LCM and DIM as well as the precision and reco v erability of DIM itemsets are sho wn in this table The minimum size for the itemsets is set to 2 since singletons are not of interest  The number of latent f actors  k  is set to 30 since lar ger v alues of k results in uninteresting itemsets i.e itemsets with lar ge error v alue F or the sak e of readability  high numbers are suf x ed by M millions or B billions  Cells with a star  report an estimate LCM outputs too man y items resulting in output le of hundreds of Gig abytes and hard disk saturation In these cases only a uniform random sample of 5 million itemsets w as used to compute the metrics Dataset min-supp LCM DIM Prec  Pumsb 40 44.7M 7 71 72 20 7.49B 10 34 76 10  10B 15 27 94 5  10B 17 25 94 1  10B 20 22 94 accidents 10 9.97M 9 75 67 5 64.8M 13 69 68 1 1.62B 15 40 53 T40I10D100k 1 64481 1 88 2.8  1.27M 7 84 9.5  3.56M 8 87 10 0.1 18.4M 9 87 18 T able I Comparing LCM closed itemsets and DIM itemsets T w o points in this table are of particular interest First whereas the number of closed frequent itemsets can get o v er billions of itemsets in some cases it is e v ent hard to count them see pumsb dataset for e xample the number of frequent itemsets output by DIM is al w ays v ery lo w  Thus the frequent patterns output by DIM can be quickly e xamined by an analyst which is not the case for all closed frequent itemsets produced by LCM Second despite being so fe w  DIM frequent itemsets e xhibit signiﬁcant reco v erability v alues This f act becomes more important when the precision is also high lik e accidents with minimum support of 10 and pumsb with minimum support of 40 In these cas es not only we are able to reco v er a lar ge amount of base-itemsets b ut also we can guarantee that these itemsets are not long uninteresting ones according t o the precision v alue In other w ords using DIM itemsets an analyst can ha v e a general prec ise vie w of the ent ire data without being o v erwhelmed by millions or billions itemsets F or instance of accidents with minimum support of 10 we can look at 9 itemsets produced by DIM instead of looking at 9.97M LCM items ets and reco v er 67 of all closed itemsets.This is an important cont rib ution it e xperimentally sho ws that by looking at DIM patterns an analyst will ha v e a signiﬁcant glimpse of the kno wledge that can be e xtracted from the dataset in a very short amount of human analysis time  The case for sparse datasets lik e T40I10D100k is dif ferent F or this dataset DIM e xtracts fe w itemsets with v ery high precision v alues b ut a relati v ely lo w reco v erability 18 for a support of 0.1 This result is in line with the dif ferent nature of T40I10D100k to Pumsb and accidents  T40I10D100k is a synthetic sparse dataset and it does not e xhibit the same strong pattern structure as the tw o other datasets This can be conﬁrmed with LCM results there are fe w patterns comparati v ely to the other datasets and the y appear at v ery lo w support v alues In such a case our method based on matrix decomposition is not the most adapted to get a representati v e vie w of the complete set of itemsets of LCM as matrix decomposition techniques are lik ely to remo v e small groups of ones from the matrix for the sak e of approximation This results in losing some information that appears in the complete output of LCM On the other hand the itemsets captured by DIM are captured with an e xtremely high precision for instance for a support 


of 0.1 the analyst will ha v e to analyze only 9 itemsets and get a good idea of the content of about one fth of the itemsets Since DIM pro vides fe w itemsets one may w onder if the amount of information pro vided by these itemsets can be also obtained by taking the top p here top is considered w r t support v alue itemsets produced by LCM where p is equal to the number of itemsets produced by DIM T able II gi v es reco v erability for the top p for each dataset precision is by deﬁnition 100 in this case the accidents mum support of 10 DIM outputs 9 itemsets so we tak e the top 9 itemsets from the results of LCM and compute their reco v erability  which is 13 The reco v erability of the top p itemsets of LCM is more than 4 times lo wer than those of DIM in real datasets In the synthetic dataset ho we v er  the impro v ement is mar ginal The table in general conﬁrms tha t DIM itemsets con v e y more information than traditional top p techniques Dataset p min-supp LCM DIM pumsb 7 40 15 72 accidents 9 10 13 67 T40I10D100k 9 0.1 12 18 T able II Comparing top-p LCM closed itemsets with DIM itemsets VII C ON CLU SION AN D F UT URE W OR K In this study  we ha v e e xamined the problem of frequent itemset mining through decomposi tion of the input matrix Using theoretic al analysis we ha v e sho wn that matrix decomposition can help us mining frequent itemsets Our e xperiments ha v e sho wn that although for high support v alues our approach is less time ef cient than stateof-the-art algorithms for lo w support v alues our approach is much f aster than those algorithms This approach is also highly scalable with respect to other algorithms according to the f act that the classical itemset mining approaches become v ery slo w since the y need to e xplore a huge combinatorial space as the number of items increases The proposed method only nds a handful of itemsets b ut the e xperiments sho w that these itemsets con v e y a signiﬁcant portion of the information of all frequent itemsets W e adv ocate that contrary to classical methods the mining time is well in vested  what is the utility of a v ery f ast algorithm that outputs millions of patterns that an analyst will tak e hours to mak e some sense of W ith our method the computer does most of the w ork and the analyst is presented with concise reliable and manageable information This w ork opens man y future research directions such as studying the nature of the itemsets not reco v ered by our method or charac terizing the set of frequent itemsets obtained from latent f actors with respect to the sets of frequent itemsets found by methods such as 5 do such methods also re-disco v er the latent f actors R EF E REN CES 1 R Agra w al and R Srikant F ast algorithms for mining assoin Pr oc of the 20th International Confer ence on V ery Lar g e Databases VLDB 94  Chile 1994 2 A Inokuchi T  W ashio and H Motoda  An apriori-based algorithm for mi ning frequent substructures from graph data  in PKDD  2000 pp 13–23 3 N P asquier  Y  Bastide R T aouil L and Lakhal Disco vering frequent closed itemsets for association rules  in oc of 7th international confer ence of database theory 1999 4 T  Uno M Kiyomi and H Arimura Lcm v er  2 Ef cient mining algorithms for frequent/closed/maximal itemsets  in Pr oc of the ICDM W orkshop FIMI 04 2004 5 J Vreek en M v an Leeuwen and A Siebes Krimp mining itemsets that compress  Data Min Knowl Disco v  23 no 1 pp 169–214 2011 6 T  Guns S Nijssen and L D Raedt k-pattern set mining under constraints  KU Leuv en T ech Rep CW 596 2010 7 M K o yuturk A Grama and N Ramakrsihnan Compression clustering and pattern disco v ery in v ery-highdimensional discrete-attrib ute dat a sets  Knowledg e Data Eng  v ol 17 pp 447–461 2005 8 P  Jiang J Peng M Heath and R Y ang  A clustering approach to constrained binary matrix f actorization  in Data Mining and Knowledg e Dis co very for Big Data  2014 pp 281–303 9 R Gupta G F ang B Field M Steinbach and V  K umar  Quantitati v e e v aluation of approximate frequent pattern minin 08  Las V e g as Ne v ada USA 2000 10 B Bringmann and A Zimmermann The chosen fe w On identifying v aluable patterns  in ICDM 63–72 11 F  Afrati A Gionis and H Mannila  Approximating a collection of frequent sets  in 04  Seattle W ashington 12–19 12 P  Kra jca J Outrata and V  Vychodil Using frequent closed itemsets for data dimensionality reduction  in ICDM 2011 13 P  Miettinen The boolean col umn and column-ro w matrix  Data Mining and Knowledg e Disco very  v ol 17 no 1 pp 39–56 2008 14 T  Uno M Kiyomi and H Arimura Lcm v er 3 Collaboration of array  bitmap and preﬁx tree for frequent itemset mining  in 05  2005 pp 77–86 


002 
                          
R. Agrawal and R. Srikant. Fast algorithms for mining association rules in large databases. In Proc. VLDB, pages 487Ö499, 1994 2 R. J. Bayardo, Jr. Efficiently mining long patterns from databases SIGMOD Rec., pages 85Ö93, 1998 3 M. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. Parallel algorithms for discovery of association rules. Data Min. and Knowl. Disc., pages 343Ö373, 1997 4 J. Dean and S. Ghemawat. MapReduce: Simplified data processing on large clusters. In Proc. OSDI. USENIX Association, 2004 5 Apache hadoop. http://hadoop.apache.org/, 2013 6 Jiawei Han and Micheline Kamber. Data Mining, Concepts and Techniques. Morgan Kaufmann, 2001 7 M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley M. Franklin, S. Shenker, and I. Stoica. Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing Technical Report UCB/EECS-2011-82, EECS Department University of California, Berkeley, Jul 2011 8 M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica Spark: Cluster Computing with Working Sets. In HotCloud, 2010 9 J. Han, J. Pei, and Y. Yin: Mining Frequent Patterns without Candidate Generation. In: Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, 29\(2\:1-12, 2000 10 M. J. Zaki. Parallel and distributed association mining: A survey IEEE Concurrency, pages 14Ö25, 1999 11 J. Li, Y. Liu, W.-k. Liao, and A. Choudhary. Parallel data mining algorithms for association rules and clustering. In Intl. Conf. on Management of Data, 2008 12 E. Ozkural, B. Ucar, and C. Aykanat. Parallel frequent item set mining with selective item replication. IEEE Trans. Parallel Distrib Syst., pages 1632Ö1640, 2011 13 B.-H. Park and H. Kargupta. Distributed data mining: Algorithms systems, and applications. 2002 14 L. Zeng, L. Li, L. Duan, K. Lu, Z. Shi, M. Wang, W. Wu, and P. Luo Distributed data mining: a survey. Information Technology and Management, pages 403Ö409, 2012 15 Li L. & Zhang M. \(2011\. The Strategy of Mining Association Rule Based on Cloud Computing. Proceeding of the 2011 International Conference on Business Computing and Global Informatization BCGIN è11\. Washington, DC, USA, IEEE: 475- 478 16 Li N., Zeng L., He Q. & Shi Z. \(2012\. Parallel Implementation of Apriori Algorithm Based on MapReduce. Proc. of the 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel & Distributed Computing SNPD è12\. Kyoto, IEEE: 236 Ö 241 17 Lin M., Lee P. & Hsueh S. \(2012\. Apriori-based Frequent Itemset Mining Algorithms on MapReduce. Proc. of the 16th International Conference on Ubiquitous Information Management and Communication \(ICUIMC è12\. New York, NY, USA, ACM: Article No. 76 18 Yang X.Y., Liu Z. & Fu Y. \(2010\. MapReduce as a Programming Model for Association Rules Algorithm on Hadoop. Proc. of the 3rd International Conference on Information Sciences and Interaction Sciences \(ICIS è10\. Chengdu, China, IEEE: 99 Ö 102 19 S. Hammoud. MapReduce Network Enabled Algorithms for Classification Based on Association Rules. Thesis, 2011 20 Synthetic Data Generation Code for Associations and Sequential Patterns. Intelligent Information Systems, IBM Almaden Research Center http://www.almaden.ibm.com/software/quest/Resources/index.shtml 21 C.L. Blake and C.J. Merz. UCI Repository of Machine Learning Databases. Dept. of Information and Computer Science, University of California at Irvine, CA, USA. 1998 http://www.ics.uci.edu/mlearn/MLRepository.html 22 HadoopApriori. https://github.com/solitaryreaper/HadoopApriori 2 3 H.V. Nguyen, E. Muller, K. Bohm. 4S: Scalable Subspace Search Schema Overcoming Traditional Apriori Processing. 2013 IEEE International Conference on Big Data. 2013 24 S. Moens, E. Aksehirli and Goethals. Frequent Itemset Mining for Big Data. University Antwerpen, Belgium. 2013 IEEE International Conference on Big Data. 2013 25 Y. Bu et al . HaLoop: E cient iterative data processing on large clusters. Proceedings of the VLDB Endowment, 3\(1-2\:285Ö296 2010 26 Frequent itemset mining dataset repository. http://fimi.us.ac.be/data 2004   
002 
Our experiments show that YAFIM is about 18 faster than Apriori algorithms implemented in MapReduce framework Furthermore, we can achieve a better performance in both sizeup and speedup for different datasets. In addition, we also evaluated YAFIM for medical application and revealed that YAFIM outperforms MRApriori about 25 speedup  A CKNOWLEDGMENT  This work is funded in part by China NSF Grants \(No 61223003\, and the USA Intel Labs University Research Program R EFERENCES  1 
002 
1671 


