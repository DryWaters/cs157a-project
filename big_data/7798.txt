A Data Clustering Algorithm for Mining Patterns From Event Logs Risto Vaarandi Department of Computer Engineering Tallinn Technical University Tallinn, Estonia risto.vaarandi@eyp.ee Abstract Today event logs contain vast amomts of data that can easily overwhelm a human Therefore mining patterns from event lags is an important system management task This paper presents a novel clustering algorithm for log file data sets which helps one to detect frequent patterns from log files to build log file profiles and to identify nnomalow log 
file lines Keywords-system monitoring data mining data clustering 1 INTRODUCTION Event logging and log files are playing an increasingly important role in system and network management Over the past two decades the BSD syslog protocol I has become a widely accepted standard that is supported on many operating systems and is implemented in a wide range of system devices. Well-written system applications either use the syslog protocol or produce log files in custom format while many devices like routers switches laser printers etc are able to log their events to remote host using the syslog protocol Normally events are logged as single-line textual messages Since log files are 
an excellent source for determining the health status of the system, many sites have built a centralized logging and log file monitoring infrastructure Because of the importance of log files as the source of system health information a number of tools have been developed for monitoring log files e.g Swatch Z Logsurfer 3 and SEC 4 Log file monitoring techniques can he categorized into fault detection and anomaly detection In the case of faull detection the domain expert creates a database of fault message patterns If a line is appended to a log file that matches a pattern the log file monitor takes 
a certain action This commonly used approach has one serious flaw  only those faults that are already known to the domain expert can be detected If a previously unknown fault condition occurs the log file monitor simply ignores the corresponding message in the log file since there is no match for it in the pattern database Also it is often difficult to find a person with sufficient knowledge about the system In the case of anomaly detection a system profile is created which reflects normal system activity If messages are logged that do not fit the profile an alarm is raised With this approach previously unknown fault conditions are detected but on the other hand 
creating the system profile by hand is time-consuming and error-prone This work IS supported by the Union Bank ofEstonia In order to solve the knowledge acquisition problems various methods have been employed with data mining methods being one of the most popular choices 5 6 7 8 91 In most research papers the focus has been on mining frequent patterns from event logs This helps one to find patterns that characterize the normal behavior of the system and facilitates the creation of the system profile However as pointed out in SI the mining of infrequent patterns is equally important since this might reveal anomalous events that 
represent unexpected behavior of the system e.g previously unknown fault conditions. Recent research papers have mainly proposed the mining of temporal patterns from event logs with various association rule algorithms 5 6 7 8 91 These algorithms assume that the event log has been normalized i.e all events in the event log have a common format. Typically each event is assumed to have at least the following attributes timestamp of event occurrence the event type and the name of the node which issued the event though the node name is often encoded in the event type Association rule algorithms have been often used for detecting temporal associations between event types 5,6,7 8,9 e.g 
if events oftype A and B occur within 5 seconds, they will be followed by an event of type C within 60 seconds each detected temporal association has a certain frequency and confidence Although association rule algorithms are powerhl they often can't he directly applied to log files because log file lines do not have a common format. Furthermore log file lines seldom have all the attributes that are needed by the association rule algorithms For example the widely used syslog protocol does not impose strict requirements on the log message format I A typical sydog message has just the timestamp hostname and program name attributes that are followed by a free-form message 
string hut only the message string part is mandatory I A detailed discussion of the shortcomings of the syslog protocol can be found in I IO One important attribute that log file lines often lack is the event type Fortunately it is possible to derive event types from log file lines since very often the events of the same type correspond to a certain line pattern For example the lines Router myrouterl interface 192.168.13.1 down Router myroulerZ interface IO IO 10 I2 down Router myrouter5 interface 192.168.22.5 down 0-7803-8199-8/03/$17.C\200 0 ZW3 IEEE IPOM2003 Page 119 


represent the event type interface down and correspond to the line pattern Router  interface  down Line patterns could be identified by manually reviewing log files but this is feasible for small log files only One appealing choice for solving this problem is the employment of data clustering algorithms Clustering algorithms II 121 aim at dividing the set of objects into groups clusters where objects in each cluster are similar to each other \(and as dissimilar as possible to objects from other clusters Objects that do not fit well to any of the clusters detected by the algorithm are considered to form a special cluster of outliers When log file lines are viewed as objects clustering algorithms are a natural choice because line patterns form natural clusters  lines that match a certain pattern are all similar to each other and generally dissimilar to lines that match other patterns After the clusters \(event types have been identified association rule algorithms can be applied for detecting temporal associations between event types However note that log file data clustering is not merely a preprocessing step A clustering algorithm could identify many line patterns that reflect normal system activity and that can be immediately included in the system profile since the user does not wish to analyze them further with the association rule algorithms Furthermore the cluster of outliers that is formed by the clustering algorithm contains infrequent lines that could represent previously unknown fault conditions or other unexpected behavior of the system that deserves closer investigation Although data clustering algorithms provide the user a valuable insight into event logs they have received little attention in the context of system and network management In this paper we discuss existing data clustering algorithms and propose a new clustering algorithm for mining line patterns from log tiles We also present an experimental clustering tool called SLCT Simple Logfile Clustering Tool The rest of this paper is organized as follows section 2 discusses related work on data clustering section 3 presents a new clustering algorithm for log file data sets section 4 describes SLCT, and section 5 concludes the paper 11 RELATED WORK Clustering methods have been researched extensively over the past decades and many algorithms have been developed II 121 The clustering problem is often defined as follows given a set of points with n attributes in the data space R find a partition of points into clusters so that points within each cluster are close similar\to each other In order to determine how close similar two points x and y are to each other a distance function d\(x y is employed. Many algorithms use a certain variant of L norm p  1 2  for the distance function Today there are two major challenges for traditional clustering methods that were originally designed for clustering numerical data in low-dimensional spaces where usually n is well below IO Firstly quite many data sets consist of points with categorical attributes where the domain of an attribute is a finite and unordered set of values 13 141 As an example consider a categorical data set with attributes car-manufacturer model type and color and data points Honda Civic hatchback green and Ford Focus sedan red Also it is quite common for categorical data that different points can have different number of attributes Therefore it is not obvious how to measure the distance between data points Though several popular distance functions for categorical data exist such as the Jaccard coefficient 12 13 the choice of the right function is often not an easy task Note that log file lines can be viewed as points from a categorical data set since each line can be divided into words with the n-th word serving as a value for the n-th attribute For example the log file line Connection from 192.168.1.1 could he represented by the data point Connection from 192.168.1.1 We will use this representation of log file data in the remainder of this paper Secondly quite many data sets today are high dimensional where data points can easily have tens of attributes Unfortunately,.traditional clustering methods have been found not to work well when they are applied to high dimensional data As the number of dimensions n increases it is often the case that for every pair of points there exist dimensions where these points are far apart from each other which ,makes the detection of any clusters almost impossible according to some sources this problem starts to be severe when n  15 12 15 161 Furthermore traditional clustering methods are often unable to detect natural clusters that exist in subspaces of the original high-dimensional space 15 161 For instance, data points 1333 1 1 99,25 2033, 1044 12 1 1 724,667 36,2307 and 501 1 I 1822 1749 808 9838 are not seen as a cluster by many traditional methods, since in the original data space they are not very close to each other On the other hand they form a very dense cluster in the second and third dimension ofthe space The dimensionality problems described above are also relevant to the clustering of log file data, since log file data is typically high-dimensional i.e there are usually more than just 3-4 words on every line and most of the line patterns correspond to clusters in subspaces For example, the lines log connectionfrom 192.168.1.1 log RSA key generation complete log: Password authenticationfor john accepted form a natural cluster in the first dimension of the data space, and correspond to the line pattern log  During past few years several algorithms have been developed for clustering high-dimensional data like CLIQUE MAFIA CACTUS and PROCLUS The CLIQUE 1151 and MAFIA I71 algorithms closely remind the Apriori algorithm for mining frequent itemsets IS they start with identifying all clusters in I-dimensional subspaces and after they have identified clusters C  Cm in  subspaces IPOM2003 Page 120 


they form cluster candidates for k-dimensional subspaces from Cl and then check which of those candidates are actual clusters. Those algorithms are effective in discovering clusters in subspaces because they do not attempt to measure distance between individual points which is often meaningless in a high-dimensional data space. Instead their approach is density based where a clustering algorithm tries to identify dense regions in the data space and forms clusters from those regions Unfortunately the CLIQUE and MAFIA algorithms suffer from the fact that Apriori-like candidate generation and testing involves exponential complexity and high runtime overhead 19 201  in order to produce a frequent m-itemset the algorithm must first produce Zm  2 subsets of that m itemset. The CACTUS algorithm I41 first makes a pass over the data and builds a data summary then generates cluster candidates during the second pass using the data summary and finally determines the set of actual clusters Although CACTUS makes only two passes over the data and is therefore fast it is susceptible to the phenomenon of chaining long strings of points are assigned to the same cluster I I which is undesirable if one wants to discover patterns from log files The PROCLUS algorithm I61 uses the K-medoid method for detecting K clusters in subspaces of the original space However in the case of log file data the number of clusters can rarely be predicted accurately and therefore it is not obvious what is the right value for K Though several clustering algorithms exist for high dimensional data spaces they are not very suitable for clustering log tile lines largely because they don't take into account the nature of log file data In the next section we will first discuss the properties of log file data and then we will present a fast clustering algorithm that relies on these properties Data Set 111 CLUSTERING LOG FILE DATA A The nature of the data to be clustered plays a key mle when choosing the right algorithm for clustering Most of the clustering algorithms have been designed for generic data sets such as market basket data where no specific assumptions about the nature of data are made However when we inspect the content oftypical log files at the word level there are two important properties that distinguish log file data fmm a generic data set During our experiments that revealed these properties we used six logfile data sets from various domains HP OpenView event log file mail server log file the server was running sendmail ipopd and imapd daemons Squid cache server log file, Internet banking server log file file and print server log file and Win2000 domain controller log file Although it is impossible to verify that the properties we have discovered characterize every logfile ever created on earth, we still believe that they are common to a wide range of logfile data sets Firstly majority of the words occur only a few times in the data set Table 1 presents the results of an experiment for estimating the occurrence times of words in log file data The results show that a majority of words were very infrequent and a significant fraction of words appeared just once in the data set one might argue that most of the words occurring once are timestamps but when timestamps were removed from data sets we observed no significant differences in the experiment results\Also, only a small fraction of words were relatively frequent i.e., they occurred at least once per every 10,000 or 1,000 lines. Similar phenomena have been observed for World Wide Web data where during an experiment nearly 50 of the words were found to occur once only The Nature ofLog File Data Mail server log file Cache ewer log HP OpenView lntemet banking File and print server Domain controller Li"W file Linux event log file sewer log file log file\(Win2000 logfile\(WinZ000 Solaris Solaris Data set size 1025.3 ME 1088.9 ME 696.9 MB 2872.4 MB 2451.6M6 1043.9 ME 7,657,148 lines 8,189,780 lines 1,835,679 lines 14,733,696 lines 7,935,958 lines 4,891,883 lines IPOM2003 Page 121 


Secondly we discovered that there were many strong correlations between words that occurred frequently As we found this effect is caused by the fact that a message is generally formatted according to a certain format string before it is logged, e.g sprinf\(messuge Connection from 366 port d ipuddress portnumber When events of the same type are logged many times constant parts of the format string will become frequent words which occur together many times in the data set In the next subsection we will present a clustering algorithm that relies on the special properties of log file data B The clustering algorithm Our aim was to design an algorithm which would he fast and make only a few passes over the data and which would detect clusters that are present in subspaces of the original data space The algorithm relies on the special properties of log file data discussed in the previous subsection and uses the density based approach for clustering The data space is assumed to contain data points with categorical attributes where each point represents a line from a log file data set The attributes of each data point are the words from the corresponding log file line. The data space has n dimensions where n is the maximum number of words per line in the data set. A region S is a subset of the data space where certain attributes il  1394 of all points that belong to Shave identical values v1  vk Vx E S x  v  xIk  vk We call the set il,vl  ik,vk the set of fried atlributes of region S If kl i.e there is just one fixed attribute the region is called region A dense region is a region that contains at least N points where N is the support fhreshold value given by the user The algorithm consists of three steps like the CACTUS algorithm I41  it first makes a pass over the data and builds a data summary and then makes another pass to build cluster candidates using the summary information collected before As a final step clusters are selected from the set of candidates During the first step of the algorithm data summarization the algorithm identifies all dense I-regions Note that this task is equivalent to the mining offrequent words from the data set the word position in the line is taken into account during the mining A word is considered frequent if it occurs at least N times in the data set where N is the user-specified support threshold value After dense I-regions frequent words\have been identified the algorithm builds all cluster candidates during one pass The cluster candidates are kept in the candidate table which is initially empty The data set is processed line by line and when a line is found to belong to one or more dense I-regions i.e one or more frequent words have been discovered on the line a cluster candidate is formed If the cluster candidate is not present in the candidate table it will he inserted into the table with the support value I otherwise its support value will he incremented In both cases the line is assigned to the cluster candidate The cluster candidate is formed in the following way if the line belongs to m dense I-regions that have fixed attributes il,v  im,v then the cluster candidate is a region with the set of fixed attributes il,vl  im,vm For example if the line is Connectionfrom 192.168 and there exist a dense I-region with the fixed attribute I Connection and another dense I-region with the fixed attribute 2 from then a region with the set of fixed attributes I Connection 2 from becomes the cluster candidate During the final step of the algorithm, the candidate table is inspected, and all regions with support values equal or greater than the support threshold value Le regions that are guaranteed to he dense are reported by the algorithm as clusters Because of the definition of a region each cluster corresponds to a certain line pattern e.g the cluster with the set of fixed attributes I Password 2 authentication 3 for 5 accepted corresponds to the line pattem Password authentication for  accepted Thus the algorithm can report clusters in a concise way by just printing out line patterns without reporting individual lines that belong to each cluster The CLIQUE algorithm reports clusters in a similar manner 15 The first step of the algorithm reminds very closely the popular Apriori algorithm for mining frequent itemsets since frequent words can he viewed as frequent I-itemsets Then however our algorithm takes a rather different approach generating all cluster candidates at once There are several reasons for that Firstly, Apriori algorithm is expensive in terms of runtime 19 201 since the candidate generation and testing involves exponential complexity Secondly since one of the properties of log file data is that there are many strong correlations between frequent words it makes little sense to test a potentially huge number of frequent word combinations that are generated by Apriori while only a relatively small number of combinations are present in the data set It is much more reasonable to identify the existing combinations during a single pass over the data and verify after the pass which of them correspond to clusters It should he noted that since Apriori uses level-wise candidate generation it is able to detect patterns that our algorithm does not report E.g if words A B C and D are frequent, and the only combinations of them in the data set are A B C and A B D then our algorithm will not inspect the pattern A B although it could have the required support On the other hand by restricting the search our algorithm avoids reporting all subsets of a frequent itemset that can easily overwhelm the user hut rather aims at detecting maximal frequent itemsets only several pattern-mining algorithms like Max-Miner I91 use the similar approach In order to compare the runtimes of our algorithm and Apriori-based algorithm we implemented both algorithms in Per1 and tested them against three small log file data sets Table 2 presents the results of our tests that were conducted on I,5GHz Pentium4 workstation with 256Mf3 of memory and Redhat 8.0 Linux as operating system \(the sizes of log files A B and C were 180KB, 1814KB and 4005KB respectively The results obtained show that our clustering algorithm is superior to the Apriori-based clustering scheme in terms of runtime cost The results also indicate that Apriori-based clustering schemes are appropriate only for small log file data sets and high support thresholds IPOM2003 Page 122 


TABLE 11 THE RUNTIME COMPARISON OF OUR ALGORITHM AN0 APRIORI-BASED ALMRlTHM Although our algorithm makes just two passes over the data and is therefore fast it could consume a lot of memory when applied to a larger data set In the next subsection we will discuss the memory cost issues in more detail C The Memory Cost of The Algorithm In terms of memory cost the most expensive part of the algorithm is the first step when the data summary is built During the data summarization the algorithm seeks for frequent words in the data set by splitting each line into words For each word the algorithm checks whether the word is present in the word table or vocabulary and if it isn't it will be inserted into the vocabulary with its occurrence counter set to 1 If the word is present in the vocabulary its occurrence counter will be incremented If the vocabulary is built for a large data set it is likely to consume a lot of memory When vocabularies were built for data sets from Table 1 we discovered that they consumed hundreds of megabytes of memory with the largest vocabulary occupying 653 MB the tests were made on Sun Fire V480 server with 4 GB of memory and each vocabulary was implemented as a move-to-front hash table which is an efficient data structure for accumulating words 21 As the size of the data set grows to tens or hundreds of gigabytes the situation is very likely to deteriorate further and the Vocabulary could not fit into the main memory anymore On the other hand one of the properties of log file data is that a majority of the words are very infrequent Therefore storing those very infrequent words to memory is a waste of space Unfortunately it is impossible to predict during the vocabulary construction which words will finally be infrequent In order to cope with this problem we use the following technique  we first estimate which words need no1 to be stored in memory and then create the vocabulary without irrelevant words in it Before the data pass is made for building the vocabulary the algorithm makes an extra pass over the data and builds a word summary vector The word summary vector is made up of m counters numbered from 0 to m-I with each counter initialized to zero During the pass over the data a fast string hashing function is applied to each word The function returns integer values from 0 to m-I and each time the value i is calculated for a word the i-th counter in the vector will be incremented Since efficient string hashing functions are uniform 1221, i.e the probability of an arbitrary string hashing to a given value i is llm then each counter in the vector will correspond roughly to W I m words where W is the number of different words in the data set If words WI  Wk are all words that hash to the value i and the words wI  wk occur tl  tk times, respectively, then the value of the i-th counter in the vector equals to the sum tl  ti After the summary vector has been constructed the algorithm starts building the vocabulary but only those words will be inserted into the vocabulary for which their counter values are equal or greater than the support threshold value given by the user. Words that do not fulfill this criterion can't he frequent because their occurrence times are guaranteed to be helow the support threshold Given that a majority of the words are very infrequent this simple technique is quite powerful If the vector is large enough, a majority of the counters in the vector will have very infrequent words associated with them and therefore many counter values will never cross the support threshold In order to measure the effectiveness of the word summary vector technique, we made a number of experiments with data sets from Table 1 We used the support thresholds of I 0.1 and 0.01 together with the vectors of 5,000 20,000 and 100,000 counters respectively each counter consumed 4 bytes of memory The experiments suggest that the employment of the word summary vector dramatically reduces vocabulary sizes, and large amounts of memory will be saved During the experiments vocabulary sizes decreased 9.93-99.36 times, and 32.74 times as an average On the other hand the memory requirements for storing the vectors were relatively small  the largest vector we used during the experiments occupied less than 400 KB of memory If the user has specified a very low support threshold there could be a large number of cluster candidates with very small support values and the candidate table could consume a significant amount of memory In order to avoid this the summary vector technique can also be applied to cluster candidates  before the candidate table is built the algorithm makes an extra pass over the data and builds a summary vector for candidates, which is later used to reduce the number of candidates inserted into the candidate table IV SIMPLE LOGFILE CLUSTERING TOOL In order to implement the log tile clustering algorithm described in the previous section an experimental tool called SLCT Simple Logfile Clustering Tool has been developed SLCT has been written in C and has been primarily used on RedhatX.0 Linux and Solaris8, but it should compile and work on most modem UNIX platforms IPOM2003 Page 123 


SLCT uses move-to-front hash tables for implementing the vocabulary and the candidate table Experiments with large vocabularies have demonstrated that move-to-front hash table is an efficient data structure with very low data access times even when the hash table is full and many words are connected to each hash table slot Zl Since the speed of the hashing function has a critical importance for the efficiency of the hash table SLCT employs the fast and eficient Shift-Add Xor string hashing algorithm 22 This algorithm is not only used for hash table operations hut also for building summary vectors SLCT is given a list of log files and a support threshold as input and after it has detected a clustering on input data it reports clusters in a concise way by printing out line pattems that correspond to clusters e.g Dec IX  myhost mydomain  connect from Dec 18  myhost mydomain  log Connection from port Dec I8  myhostmydomain  log The user can specify a command line flag that forces SLCT to inspect each cluster candidate more closely, before it starts the search for clusters in the candidate table For each candidate C SLCT checks whether there are other candidates in the table that represent more specific line patterns In the above example the second pattern is more specific than the third since all lines that match the second patlem also match the third If candidates C  Ck representing more specific patterns are found for the candidate C the support values of the candidates CI  Ck are added to the support value of C and all lines that belong to candidates CI  CL are also considered to belong to the candidate C In that way, a line can belong to more than one cluster simultaneously and more general line patterns are always reported even when their original support values were below the threshold Although traditional clustering algorithms require that every point must be part of one cluster only there are several algorithms like CLIQUE which do not strictly follow this requirement in order to achieve clustering results that are more comprehensible to the end user 15 By default SLCT does not report the lines that do not belong to any of the detected clusters As SLCT processes the data set each detected outlier line could be stored to memory hut this is way too expensive in terms of memory cost Ifthe end user has specified a certain command line flag SLCT makes another pass over the data after clusters have been detected and writes all outlier lines to a file Also variable parts of cluster descriptions are refined during the pass by inspecting them for constant heads and tails. Fig 1 depicts the sample output from SLCT If the log file is larger running SLCT on the data set just once might not be sufficient because interesting cluster candidates might have very different support values If the support threshold value is too large many interesting clusters will not be detected If the value is too small interesting cluster candidates could be split unnecessarily into many subcandidates that represent rather specific line patterns and have quite small support values in the worst case, there will be no cluster candidates that cross the support threshold Des 1B f myhoar.mydomain 8shdI.I connect from 112.26.242.118 SYpport 262 s YC I outliers 168 oYCllers Figure 1 Sample output from SLCT lauth.critl loginl"*l pam-krbl authenticate error InpurioutpYr error 151 1mil.alerLI llnapdlfffl Fatal disk hoaL-ff I rabx Disk quota exceeded Figure 2 Sample anomalous log file lines IPOM2003 Page 124 


In order to solve this problem an iterative approach suggested in 8 could be applied SLCT is first invoked with a relatively high threshold e.g 5 or IO in order to discover very frequent patterns Ifthere are many outliers after the first run SLCT will be applied to the file of outliers and this process will be repeated until the cluster of outliers is small and contains very infrequent lines which are possibly anomalous and deserve closer investigation Also if one wishes to analyze one particular cluster in the data set more closely SLCT allows the user to specify a regular expression filter that will pass relevant lines only Fig 2 depicts sample anomalous log file lines that we discovered when the iterative clustering approach was applied to one of our test data sets \(the mail server log file from Table 1 Altogether four iterations were used and the cluster of outliers contained 318,166 98,811 22,807 and 5,390 lines after the first second third and fourth the final step respectively At each step the support threshold value was set to 5 The final cluster of outliers was then reviewed manually and the lines representing previously unknown fault conditions were selected The lines in Fig 2 represent various system faults such as internal errors of the sendmail, imapd and syslogd daemon but also unsuccessful attempts to gain system administrator privileges for the reasons of privacy and detecting interesting patterns from log files Table 3 presents the results of some our experiments for measuring the runtime and memory consumption of SLCT The experiments were conducted on I,5GHz Pentium4 workstation with 256MB of memory and Redhat 8.0 Linux as operating system For all data clustering tasks a word summary vector of size 5,000 counters was used. Since SLCT was also instructed to identify outlier points four passes over the data were made altogether during the experiments The results show that our algorithm has modest memory requirements and finds many clusters from large log files in a relatively short amount of time V FUTURE WORK AND AVAILABILITY INFORMATION For a future work we plan to investigate various association rule algorithms in order to create a set of tools for building log file profiles We will be focusing on algorithms for detecting temporal patterns hut also on algorithms for detecting associations between event attributes within a single event cluster SLCT is distributed under the terms of GNU GPL and is available at http:likodu.neti.eel-ristolslcti ACKNOWLEDGMENTS The author wishes to express his gratitude to Mr TGnu Liik Mr Ain Rasva Dr Paul Leis Mr Ants LeitmBe and Mr Kaido Raiend from the Union Bank of Estonia for their kind Also the author thanks Mr Bennett Todd and Mr Jon Stearley for providing feedback about SLCT security real IP numbers host names user names and other such data have been replaced with wildcards in Fig 2 The lines that are not present in Fig 2 represented various rare faults such as scheduled tasks of the crond and named daemon nightly restarts of the syslogd daemon during log rotations unsuccessful login system activities or attempts for regular users, etc We have made many experiments with SLCT and it has proven to be a useful tool for building log file profiles and TABLE 111 RUN71MEANDMEMORY CONWMPTIONOFSLCT IPOM2003 Page 125 


I Sudipto GuhqRajeev Raslogi and Kyuseok Shim, \223ROCK A Robust Clustering Algorithm for Categorical Attributes\224 In/onnalion Syslems Vol 25\(5 2000 I\223 Venkatesh Ganti Johannes Gehk and Raghu Ramakrishnan and Notification With Swatch\224 Proceedings o/lhe USENIX 7\221 Splem 223CACTUS  Clustering Categorical Data Using Summaries\224 Administration Conference 1993 Proceedings o the 5\223 ACM SIGKDD nlerMliOM Con/erence on Knowledge Discovery ondDotn Mining 1999 3I Wolfgang Ley and Uwe Ellerman logsurferll manual page unpublished see http:llwvrrv.cen.dfn.del~~~l~g~~~l 1995 I51 Rakesh Agrawal Jahannes Gehrke Dimitrios Gunopulos and Prabhakar Raghavan 223Automatic Subspace Clustering of High Dimensional Data for Data Mining Applications\224 Proceedings o ihr 4]Risto Vaarandi 221\221SEC  a Lightweight Event Correlation Tool\224 ACM SlGMOD lnlernnlional Conference on Monagemenr o Data Proceedings o ihe T\221 IEEE Workhop on IP Operalions ond REFERENCES I C Lonvick 223Thhe BSD syslog Protocol\224 RFC3164,ZOOl 2 Stephen E Hansen and E. Todd Atkins 223Automated System Monitoring Monogement 2002 I44R 5 H Mannila H Toivonen and A I Verkamo 223Discovery of frequent episodes in event sequences\224 Dam Mining and Knowledge Discovery Vol 1\(3 1997 6]M Klemettinen H Mannila and H Toivonen 223Rule Diseovely in Telecommunication Alarm Data\224 Journal sf Nelwork and Systems Managemem Vol 7\(4 1999 7 Qingguo Zheng Ke Xu Weifeng Lv and Shilong Ma 223Intelligent Search of Correlated Alarms from Database Containing Noise Data\224 Proceeding qf lhe 8\224 lEEE/lFlP Network Operaeons and Managemem Symposium 2002 8]L Bums J L Hellerstein S Ma C S Pemg D A Rabenhorst, and D Taylor 223A Systematic Approach to Discovering Correlation Rulcs Far Event Management\224 Proceedings o the 7\221\221 IFIP/lEEE Inremariunol Xvmposium on lntegrared Network Manogemeni 2001 191 Sheng Ma and Joseph L Hellerstein 223Mining Partially Periodic Event Pattcms with Unknavn Periods\224 Proceedings sf the 16 lniernationol Conference on Dolo Engineermg 2000 IO Matt Bing and Carl Erickson 223Extending UNlX System Logging with SHARP\224 Proceedings o he USENIX 14\221h System Adminirworion Ill David Hand Hcikki Mannila and Padhraic Smyth Principles o/Dat Mining The MIT Press 2001 I21 Pave1 Berkhin 221\221Survey of Clustering Data Mining Techniques\224 unpublished see http:llciteseer.nj.nec.com~~~khinO2~~~~y.html 2002 Con/erence 2000 I61 Chaw C Aggawd Cecilia Procopiuc Joel L Wolf Philip S Yu and long So0 Park, \223Fast Algorithms far Projected Clustering\224 Proceedings o lhe ACM SIGMOD Inlernutionol Con/erence on Management o Dolo 1999 I71 Sanjay Coil Harsha Nagesh and Alok Choudhary, \223MAFIA Efficient and Scalable Subspace Clustering for Very Large Data Sets\224 Technical Report No CPDC-TR-9906-010 Northwestem Universily 1999 1181 Rakesh Agrawal and Ramakrishnan Srikant 223Fast Algorithms for Mining Association Rules\222\222 Proceedings o the 2ffh InlernotioM Con/erenee on Very Large Dnla Bases 1994 I91 Robeno I Bayardo Jr 223Efficiently Mining Long Patterns from Databases\224 Proceedingf o the ACM SIGMOD lntemorionol Confermce on Management qfDolo 1998 20 Jiawei Han Jim Pet and Yiwen Yin 223Mining Frequent Pattems without Candidate Generation\224 Proceedings o/rhr ACM SIGMOD Inlernnrionol Conference on Manogemen1 o/Dolo 2000 2l Justin Zobel Steffen Heinz and Hugh E Williams 223ln-memory Hash Tables for Accumulating Texl Vocabularies\224 In/ormormotion Processing Letiers Vol 80\(6 2001 22 M. V Ramakrishna and Justin Zobel, \223Performance in Practice of String Hashing Functions\224 Proceedings o/rhe 5 Internoliono1 Conference on Dambase Svslemsfor Advanced Applicolions I 997 IPOM2003 Page 126 


100 34 33 34 DSD u u u                          u             Fig.6: Over Hiding problem of setting  1 in S No matter the left-hand or right-hand equation, the support of {1, 2} in D' is 0. That is, item 1 and item 2 never appear toge ther, and they are mutual exclusive! This situation almost never happens in the normal database. The attackers may interest in this situation and infer that {1, 2} is hidden deliberately. To hide the sensitive patterns, only need to make their supports smaller than minimum support and need not to decrease their support to 0. To solve the problem, we inject a probability ? which is called Distortion probability into this approach. Distortion probability is used only when the column j of the sanitization matrix S contains only one  1  i.e. Sjj = 1 0 1 d   m k k j i k  S D  m j n i j i  d d d d   1  1     D  i j  h a s   j probability to be set to 1 and 1  j probability to be set to 0 Lemma 1: Given a minimum support ?, and a level of confidence c. Let {i, j} be a pattern in Marked-Set, nij be the support count of {i, j}. ? is the Distortion probability of column j Without loss of generality, we assume that Sij  1. If ? satisfies    D n i j  u  u  V U   a n d    


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


