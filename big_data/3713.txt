Discovery of High-Dimensional Inclusion Dependencies 001 Andreas Koeller Dept of Computer Science Montclair State University Upper Montclair J 07043 USA Andreas.Koeller@montclair.edu Elke  Rundensteiner Dept of Computer Science Worcester Polytechnic Institute Worcester MA 01609 USA rundenst@cs.wpi.edu Abstract Determining relationships such s functional or inclusion dependencies within and across databases is important for many applications in information integration When such information is not available as explicit meta data it is 
possible to discover potential dependencies from the source database extents However the complexity of such discovery problems is typically exponential in the number of attributes We have developed an algorithm for the discovery of inclusion dependencies across high-dimensional relations in the order of 100 attributes This algorithm is the 336rst to ef\336ciently solve the inclusion-dependency discovery problem This is achieved by mapping it into a progressive series of clique-\336nding problems in k uniform hypergraphs and solving those Extensive experimental studies con\336rm the algo 
rithm\325 s e f 336ciency on a variety of r eal-world data sets 1 Introduction In this work we are concerned with the discovery of meta-information dependencies and interrelationships in databases and in particular with the discovery of inclusion dependencies INDs Due to the nature of data and its generation information is often stored in multiple places with large amounts of redundancy for example across different departments of a large enterprise or across multiple companies in the same 336eld of business When integrating data sources that are likely to be n partly redundant such as in the EVE 
view integration system 6 a method to discover such redundancies is needed 001 This work was performed while Andreas Koeller was a research assistant at Worcester Polytechnic Institute The work was supported in part by the NSF NYI grant IRI 97\32096264 the NSF CISE Instrumentation grant IRIS 97\32029878 and the NSF grant IIS 9988776 Also applications in which data about similar real-world objects is collected independently will bene\336t greatly from redundancy discovery For example medical or pharmacological databases could be compared for similarities or 
overlaps which could provide important information relevant for the discovery of treatments for certain diseases In general the discovery of INDs will be bene\336cial in any ffort to integrate or compare unknown databases A manual extraction of INDs by domain experts is usually not feasible due to the large number of information sources in the world the potentially high number of attributes in real-world relations and a widespread lack of reliable metainformation about legacy databases Furthermore the general IND discovery problem has been shown to be NP-hard as a function of the number of attributes in the relations to 
be compared 4 To the best of our knowledge the algorithm we are presenting is the 336rst that solves the IND-\336nding problem for large numbers of attributes We are aware of only one other solution of the IND discovery problem using a levelwise enumeration algorithm 3 It sho ws good performance for relations with few attributes t does not scale well to numbers of attributes and in particular maximal IND sizes beyond 10  The IND discovery problem is loosely related to the problem of association rule mining ARM 1 Ho we v e r  with the exception of 7 ARM algorithms are 
levelwise algorithms in the sense that a 322frequent itemset\323 of length k is only discovered after all itemsets of lengths 1   k 212 1 have in some sense been evaluated Typically ARM algorithms become feasible by using additional properties of association rules most notably the concept of support  which are not available to us For this reason an adaptation of such algorithms to IND discovery would show signi\336cantly higher complexity than algorithm FIND 
2  In the remainder of this paper e ketch the main idea of the FIND 2 process while a more extensive description including mathematical background and algorithmic details is n in 5     683  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


2 Background Inclusion dependencies are de\336ned as below De\336nition 1 IND Let R  a 1 a 2  a n  and S  b 1 b 2  b m  be projections on two relations Let X be a sequence of k distinct attribute names from R and Y a sequence of k distinct attribute names from S  with 1 001 k 001 min n m   Then an inclusion dependency IND 001 is an assertion of the form 001  R  X  002 S  Y   k is called the arity of 001  An IND 001  R  a 1  a k  002 S  b 1  b k  is valid between two relations R and S if the sets of tuples in R and S satisfy the assertion given by 001  One very important observation on INDs is that a k ary IND with k 1 naturally implies a set of m ary INDs with 1 001 m 001 k  That is for a given valid IND 001  R  A  002 S  B  1  the IND 001 001  R  A 001  002 S  B 001  will be valid for any subset A 001 002 A and its corresponding subset B 001 002 B  Such a set of m ary INDs implied by a k ary IND has a cardinality of 001 k m 002 and is denoted by 001 k m  Note that the validity of all implied k ary INDs of a iven IND 001 is a necessary t not a suf\336cient condition for the validity of 001  For example  R  A 1  002 S  B 1  003  R  A 2  002 S  B 2  003  R  A 3  002 S  B 3  does not imply R  A 1 A 2 A 3  002 S  B 1 B 2 B 3   3 Mapping the IND Discovery Problem to a Graph Problem The worst-case complexity of the problem is determined by the number of possible distinct INDs between two relations which is exponential in the number of attributes in those relations 5 In our w ork we instead mak e use of the fact that it is possible to 336nd a minimal r of valid INDs i.e a set of INDs from which all valid INDs can be d by implication without even enumerating all valid INDs reducing the complexity signi\336cantly We propose a mapping of our problem into a more tractable graph problem We use k uniform hypergraphs which are graphs in which each edge is incident to exactly k nodes Thus standard undirected graphs can be considered 3222-uniform hypergraphs\323 Furthermore we extend the concept of clique maximal connected subgraph to hypergraphs De\336nition 2 hyperclique Let G  V E  be a k hypergraph A hyperclique is a set C 002 V such that for each k subset S of distinct nodes from C  the edge corresponding to S exists in E  The cardinality of a hyperclique C  denoted by  C   is the number of nodes in C  As a special case a single node with no adjacent edges is a hyperclique of cardinality 1 1 The notation A means a set of attributes In analogy to  a clique is a hyperclique in a 2hypergraph The mapping from our problem to a graph problem is d as follows We 336rst map the set of valid INDs to a set of hypergraphs G m 2 001 m<k   by making all k ary valid INDs hyperedges in a k uniform hypergraph The nodes of all hypergraphs for any k  are formed by the unary INDs For example the 336rst hypergraph for k 2 has as its nodes all valid unary INDs and as its edges all valid binary INDs We then show that for m 2 k 212 1  ny set 001 k m of INDs implied by a valid 001 k maps to a hyperclique in the corresponding hypergraph G m  In other words the only candidates for valid high-arity INDs are those that correspond to cliques in k uniform hypergraphs for small k  Those graphs can be constructed after a relatively small number of IND validity checks on INDs of very small arity 3.1 The Clique-Finding Problem The Clique-Finding Problem also called the Maximum Clique Problem is a well known NP-complete graph problem Ef\336cient algorithms for reasonably sized graphs i.e 2 hypergraphs with up to about 100 nodes are n in the literature e.g 2 W ith our de\336nition of hyper cliques Def 2 the Clique-Finding Problem extends naturally to k hypergraphs The NP-complexity of the clique-problem is mainly due to the exponential number of possible cliques in a graph although there are polynomial-time algorithms for some cases We have developed and implemented an algorithm called HYPERCLIQUE that 336nds cliques in k uniform hypergraphs and while NP-complete shows satisfactory performance for relatively sparse graphs with few cliques Due to space limitations we refer to for details 4 Algorithm FIND 2 We now brie\337y sketch out the algorithm FIND 2 Fig 1 which applies cliqueand hyperclique-\336nding techniques to 336nd inclusion dependencies INDs Full details and tions can be found in 5 FIND 2 takes as input two relations R and S  with k R and k S attributes respectively and returns a generating set of INDs between attributes from R and S  The algorithm proceeds by 336rst exhaustively validating unary and binary INDs thus forming the 336rst 2uniform hypergraph Lines 01-02 A clique-\336nding algorithm then determines all higher-arity INDs candidates Line 06 Since the clique property is necessary t not suf\336cient for the validity of a higher-arity IND each IND candidate thus discovered must also be checked for validity Line 09 Each IND that tests invalid but is a clique in the 2-hypergraph is broken down into its implied 3-ary INDs which then form the edges of a 3-hypergraph Line  684  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


11 Edges corresponding to invalid INDs are d from the 3-hypergraph Line 05 Then our algorithm HYPERCLIQUE 336nds new IND candidates in the manner described above Line 06 with invalid INDs broken down into 4-ary subsets and so forth for increasing k  The process is repeated until no new cliques are found At each phase some small INDs might be missed and are discovered in line 13 see 5 In all of our e xperiments using real data sets the algorithm terminated for k 001 6  01  Set V 002 genValidUnaryINDs  R S  02  Set E 2 002 genValidBinaryINDs  R S V   initialize result set with unconnected nodes  i.e cliques of size 1 03  Set res 002 v 003 V  degree  v 0  04  for m 002 2 k S 212 1 05  Graph G m 002  V validINDs  E m  06  Set I 002 genCliquesAndCheckAsINDs  G m  07  Set C tmp 002\004  collect invalid cliques into C tmp 08  forall  c 003 I  09  if  c is valid 005 c 006 m  res 002 res 007 c 10  if  c is invalid 005 c 006  m 1 C tmp 002 C tmp 007 c  generate edges for the next hypergraph G m 1 11  E m 1 002 genKAryINDsFromCliques  m 1 C tmp  12  if  E m 1  004  return res 13  res 002 res 007 genSubINDs  m E m 1  res  14  return res Figure 1 Algorithm FIND 2  A complete explanation of the algorithm\325s functions including a correctness and complexity discussion can be found in 5 Discussion We implemented algorithm FIND 2 in Java r Oracle 8i relational databases using JDBC We ran a large suite of experiments comparing our algorithm to well-known levelwise strategies 322Apriori\323 class of algorithms as well as testing the algorithm\325s performance on multiple data sets obtained from the UC Irvine KDD  We found that algorithm FIND 2 336nds large INDs 50100 attributes in relations of 5,000-100,000 tuples in reasonable time minutes to a few hours As one example the discovery of a 30-ary IND in a training set two relations of 41 attributes each with 4500 and 5000 tuples respectively took about 350 seconds Note that the implementation used standard SQL queries to determine IND validity and signi\336cant speedups are possible with more careful implementation Also the algorithm scales linearly in the size of base relations even in the simple SQL-based implementation used The full set of experiments can be found in 5 6 Conclusions In this paper e ave proposed an algorithm called FIND 2 for the problem of discovering inclusion dependencies r high-dimensional databases With our solution it is possible to automatically compare two databases with known schema t unknown interrelationships and identify inclusion dependencies between their attributes As our algorithm discovers database interrelationships it is useful for a variety of purposes such as the identi\336cation of database duplicates or the comparison of large multidimensional databases The discovery of inclusion dependencies is a hard problem with inherent NP-complexity 4 By mapping the problem to a set of graph problems we achieve a signi\336cant improvement in performance r the na\254\365ve algorithm Due to space limitations we can only give a very brief overview r the work A more detailed treatment can be found in 5 References  R Agra w a l and S Ramakrishnan F ast algorithms for mining association rules In Proc Intl Conf on Very Large Databases VLDB  pages 487\320499 1994  C Bron and J K erbosch Finding all cliques of an undirected graph Communications of the M  16\(9 September 1973  F  de Marchi S Lopes and J.-M Petit Ef 336cient algorithms for mining inclusion dependencies In Proceedings of International Conference on Extending Database Technology EDBT  pages 464\320476 2002  M Kantola H Mannila K J R 254 aih 254 a and H Siirtola Discovering functional and inclusion dependencies in relational databases International J of Intelligent Systems  7:591\320607 1992  A K oeller and E A Rundensteiner  Disco v ery of highdimensional inclusion dependencies Technical Report WPICS-TR-02-15 Worcester Polytechnic Institute Dept of Computer Science 2002  A J Lee A K oeller  A  Nica and E A Rundensteiner  Data Warehouse Evolution Trade-offs between Quality and Cost of Query Rewritings In Proceedings of IEEE International Conference on Data Engineering  Special Poster Session page 255 Sydney Australia March 1999  M J Zaki Scalable algorithms for association mining IEEE Transactions on Knowledge and Data Engineering TKDE  12\(3 May/June 2000  685  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


descriptions exist in Table 1 Some instances of the data are with missing values Ve divided this data into 121 training cases and 19 testing cases. The problem is to find factors important for three decisions: Diagnosis DIAG Detection of bacteria or virus CULT-FIND and Prediction prognosis COURSE The dataset was divided into a training set and testing set and the computations of rules have been done only to training data The results of computations of rules were applied to the classification the objects from the testing dataset To evaluate the classification process we use a measure called classifier\222s error rate 8 which is defined by the ratio of the number of error to the number of all cases We will compare the results of our method with that standard rough set me set we use ROSETTA software to induce the rules from the dataset In the classification process we use the same technique as in C4.5 that the rules are ordered and the first rule that covers a case is taken as the operative one. The default rule rule without conditions that is put in the end of the list of rules comes into play when no other rule covers a case The algorithm requires a set of parameters that have to be manually specified and may have considerable impact on the performance of the algorithm Furthermore, it is desirable to repeat the same process with different sets of parameters We give in details the best set of parameters that we found through the run of our method 36 I I  Neisseria, Strepto, Staphylo Memory-loss N LC Bechet Sinusitis COURSE N P Tb Influenza, Measles Pi El I Varicella Rubella Adeno 31 CULTURE 38 DAR.4-P ABPC+FMOX Amncsia Headche Ataxia DM Hepatits TB N P RISK Grouped 37 I RISK 1 Broncho Myeloma LC-DM I Decision attribute Diagnosis \(DIAG={Bacteria Virus We use 33 attributes \(Personal Information, Present History Physical Examination Laboratory Examination and Therapy and Course and take only group attributes e.g we take attribute COURSE and delete attribute C-COURSE see Table 1 The best set of rules is obtained in generation 2 Table 3 with number of rules fewer and shorter average rule length than which is obtained from C4.5 or standard rough set method The error rate of the set of rules that is obtained from our method is the same as that is obtained from C4.5 method but the rules are obtained from standard rough set method have very high error rate with this data The fitness function here depends on parameters al=0.9 q=O.l a3 0 and O In another run for our method we get from generation 58 the best set of rules, where it has also the same error rate  0.00 but number of rules is 4.00 and average rule length is 1.00 see third row of Table 3 Table 2 shows the run parameters that are used in our experiments where we mainly depend on crossover operator probability rate is 0.8 Two sets of rules that are obtained from our method and C4.5 method are showed in Table 4 Table 2 Run parameters for medical data Decision Diagnosis and Prediction Max generation Po dation size Max de th for trees Crossover rate 0.8 388 Seventh Australian and New Zealand Intelligent Information Systems Conference 18-21 November 2001 Perth Western Australia 


Mutation rate Error I rules 1 Average rule size 1 rate 1 0.1 I c4.5 I 4 I 1.75 I 0.00 I Reproduction rate 0.00 Yo 0.00 Yo 2.00 78.9  Rough Set 0.1 Table 4 Sets of rules are resulted from experiments  with medical data  GRI CELL-POLY 220  Class BACTERIA CT-FIND  Abnormal CELL-MONO  1 2  Class BACTERIA CELL-MONO  1 2 VIRUS  Class  rules Default Class VIRUS Decision attribute Detec Error rate Average rule size cision Diagnosis C4.5 method CELL-POLY 220  Class BACTERIA CT-FIND Abnormal CELL-MONO  1 2 3 Class BACTERIA CELL_POLY<=220 CELL-MONO 12 VIRUS CT-FIND  Normal CELL-POLY 220 VIRUS Default Class: VIRUS on CULT FIND={T  Class  Class I rules Error rate Average rule size Ia.5 I 1 I 6  26.3  GRI I I I I J c4.5 GRI Rough Set medical data decision attribute Detection I GRI I c4.5 5 2.40 21.1 Yo 92 11.00 63.2 Yo AGE<=46 COLD <=7 NAUSEA 9 LOC o ONEST  ACUTE BT 36.1 GCS lo COLD 7 COLD 9 ONEST=CHRONIC AGE 37 COLD 7 Default Class F  attribute Predic  Class F  Class F  Class T  Class T  Class T AGE<=46 COLD <=7 NAUSEA 9 LOC o ONEST  ACUTE BT 36.1  Class F Default Class F ion COURSE In D We use with this data 16 attributes Personal Information Present History Physical Examination and we ignore Laboratory Examination. The rules resulted from our run with new method and C4.5 method are showed in Table 9 The best set of rules is obtained in generation 4 Table 8\with number of rules average rule length and error rate is same as C4.5 method but better than standard rough set method, where from Table 9 we observe that the rules obtained from our method differ than that are obtained from C4.5 method except only one rule is the same The parameters for run are showed in Table 2 as in decision attribute DIAGNOSIS  rr 7 Parameters for run in mi:alii  Decision attribute Detection Max eneration Po ulation size Max de th for trees Crossover rate Mutation rate 5.26 5.26 Yo 14.00 73.7  Rough Set Table 9 Rules resulted from experiments with FOCAL  FEVER 8 SEX M FOCAL  AGE 62 3 Class N 3 89 Seventh Australian and New Zealand Intelligent Information Systems Conference 18-21 November 2001 Perth Western Australia 


3 Class P SEX M AGE 62 LOC 2 BT  38.6 Default Class: N  ClassP  Class P  LOC 2 STIFF  2 AGE 62 FEVER 2 1  Class P FOCAL   ClassN Default Class N Legarding the application we introduced our approach achieves a more accuracy than heuristics of C4.5 and standard rough set theory But C4.5 is faster, however the execution time was not a major tasks involved in its utilization. These results are suggesting that our method can be treated as a promising tool for extracting laws from experimental datasets and its performance is fully comparable with the performance of other classification systems 4 Conclusions On the basis set theory, we have presente ich provides an efficient and for knowledge discovery in database system In the hybrid framework rough set theory and genetic programming are integrated into hybrid system and used cooperatively to generate a-set of rules from database We believe that successful research requires good co-operation between theoreticians and practitioners so we presented in this paper the analysis of structure of the model for our new method and compared standard method for extracting laws from decision table based on rough set theory and that based on C4.5 algorithm with our new method on medical dataset. We observe that the rules extracted by our new method are relatively better predisposed in classification than both C4.5 and standard rough set approaches Reference l Mannila H Inductive Database and Condensed Representations for Data Mining processing of International Logic Programming Symposium 1997 pp 21-30  Ziarako W Ed Rough Sets Fuzzy Sets, and Knowledge Discovery Springer Verlag Berlin 1994  Hassan Y and Tazaki E Knowledge Discovery 221Using Rough Set Combined with Genetic Programming Accepted from JSAI International Workshop on Rough Set Theory and Granular Computing 2001  Dong J.Z Zhong N and Ohsuga S Probabilistic Rough Induction Yamakawa T and Matsumoto G edt Methodologies for the Conception Design and application of SoR Computing and Information/Intelligent Systems LISUKA\22298 World Scientific, 1998,pp 943-946 5 Dong J.Z Zhong N and Ohsuga S Rule Discovery by Probabilistic Rough Induction Japanese Soc Artificial Intelligence, 2000 pp 274 286 6 Garcia A and Shasha D Using Rough Sets to Order Questions Leading to Database Queries In Nagib C Callaos ed Proceedings of the International Conference on Information Systems Analysis and Synthesis ISAS\22296 July 22-26 7 Nakayama H Hattori Y and Ishii, R Rule Extraction based on Rough Set Theory and its Application to Medical data Analysis IEEE SMC\22299 Conference Proceedings 1999 8 Polkowski, L and Showron A Rough sets in Knowledge Discovery, Physica Nerlag, 1998 9 Siromoney A and Inoue K Elementary Sets and Declarative Biases in a Restricted GRS-ILP model Slovene Soc. Informatika24, 2000 pp 125 135 IO Stepaniuk J and Maj M Data Transformation and Rough Sets, Principles of Data Mining and Knowledge Discovery Second European Symposium PKDD\22298, pp 441-9,1998 ll Tsumoto S and Tanaka H Incremental learning of probabilistic rules from clinical databases In Proceedings of the Sixth International Conference Information Processing and Management of Uncertainty in Knowledge-Based Systems IpMU\22296 Granada Spain 1996 pp 1457-1462 I21 Tsumoto S and Tanaka H Discovery of Approximate Medical Knowledge based on Rough Set Model Principles of Data Mining and Knowledge Discovery Second European Symposium PKDD\22298, pp 468-76,1998 13 Ziarako W Discovery Through Rough Set Theory, Communications of the ACM Vol. 42 No I41 Pawlak Z Rough Sets, International Journal of Computer and Information Science Vol 1 1 No  151 Pawlak Z Rough Classification, International Journal of Man-Machine studies 20 1984 pp 469 483 16 Kinnear Jr K.E Advances in Genetic Programming Mit Press, 1994 I71 Koza J.R Genetic Programming 111 Darwinian Invention and Problem Solving San Francisco CA 1999 I81 Dumitrescu D Lazzerini B Jain L.G and Dumitrescu A Evolutionary Computation, CRC Press, 2000 I91 Longdon William B Genetic Programming and Data Structure: Genetic Programming  Data Structure  Automatic Programming Amsterdam Kluwer 1998 20 Quinlan J.R C4.5 Programs for Machine Learning Morgan Kaufinann San Marteo CA 1993 Orlando, USA 1996, pp 555-560 11, 1999 pp 54-57 5 1982, pp 341-356 390 Seventh Australian and New Zealand Intelligent Infomation Systems Conference 18-21 November 2001 Perth Western Australia 


1232.119s 90,146,85 FHUT 185.933 s 20,142,05 HUTMFI PEP 103.492 s 21.582 s 9,150,058 1,331,158 2.904 s Mushroom at 1 support Scaled 1 Ox vertically With Reordering Figure 8 FH+HM 103.323 s 9,150,030 Figures 8 and 9 show the effects of each component of the MAFIA algorithm on the mushroom dataset at 1 minimum support The number of transactions was increased by repeating all transactions in the database by a certain scaling factor We call this form of scaling vertical scaling In this case mushroom was scaled ten times vertically Note that vertical scaling will not change the search space and will only affect the time taken for counting the support of itemsets The components of the algorithm are represented in a cube format where the running times and number of lattice nodes visited during the MAFIA search for all possible combinations are shown The top of the cube shows the time for a simple traversal where the full search space is explored while the bottom of the cube corresponds to all three pruning methods being used Two separate cubes with and without dynamic reordering\rather than one giant cube are presented for readability Note that all pruning components yield some savings in running time but that certain components are more effective than others In particular HUTMFI and FHUT yield very similar results since they use the same type of superset pruning but with different methods of implementation The efficient MFI lookups that HUTMFI uses to check for frequency explain why HUTMFI outperforms FHUT see Section 3 It is also FH+PEP HM+PEP 16.925 s 8.943 s 1,134,863 535,813 Mushroom at 1 support Scaled lox vertically Without Reordering Figure 9 interesting to see that adding FHUT when HUTMFI is already performed yields very little savings i.e from HM to HM+FH or from HM+PEP to ALL the running times do not significantly change HUTMFI checks for the frequency of a node\222s HUT by looking for a frequent superset in the MFI while FHUT will explore the leftmost branch of the subtree rooted at that node Apparently, there are very few cases where a superset of a node\222s HUT is not in the MFI but the HUT is frequent PEP has the biggest effect of the three pruning methods All of the running time of the algorithm occurs at the lower levels of the tree where the border between frequent and infrequent itemsets exists and since PEP is most likely to trim out large sections at the lower levels this pruning yields the greatest results  Dynamically reordering the tail also has dramatic savings cf Figure 8 with Figure 9 It is interesting to note that without PEP dynamic reordering runs nearly an order of magnitude faster than the static ordering while with PEP it is 223only\224 3-5 times faster Since both PEP and reordering remove elements from a node\222s tail it is not surprising that they overlap in their efficacy 5.2 Comparison With Depthproject We tested the algorithm on 223real\224 datasets containing long patterns that have been used in earlier work  1,7 449 


These datasets are publicly available from the UCI Machine Learning Repository http://www.ics.uci.edu/-mlearn/MLRepository html At the lowest supports tested the longest patterns in these databases have over 20 items, making any algorithm that examines all possible subsets of these patterns or a significant portion thereof infeasible This makes the task of finding the patterns computationally intensive despite the small size of the databases For some of the experiments the databases were scaled vertically by Time Comparison on Connect4.data 1000 MAFIA  Depthproject 100 h  U 10  I 1 I 0.1 90 80 70 60 50 40 30 20 10 Min Support  Figure 10 I IlllW wJrlllJ"llsull UII c.lless.ulla 100 MAFIA DP I 60 55 50 45 40 35 30 25 20 Min Support  Figure 12 concatenating copies of the database together This only affects the time counting takes since the bitmaps compressed or not are longer and the search space examined remains constant Figures 10  12 illustrate the results of comparing MAFIA to our implementation of the Depthproject method the state-of-the-art method for finding maximal patterns I The x-axis is the user-specified minimum support, while the y-axis uses a logarithmic scale to show the running time Time Comparison on Mushroom.data 10 MAFIA  Depthproject I 1 h Y cn  i I o 1 0.01 10 8 6 4 2 0 Min Support  Figure 11 scaleup OT r;ness.aata 45 40 35 MAFIA DP 30 rr 25 20 E 15 10 5 0 0 5 10 15 20 25 Scaleup factor Figure 13 Table 1  Reduction Factor of Nodes Considered Due to PEP Pruning 450 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


