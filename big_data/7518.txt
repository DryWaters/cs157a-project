This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination EEE   1 T      Remote sensing imag ys e ears data ation eview ene ting data ale e These oaches t ess. Then           f this aluate e and the ch 8 ORDS      Benchmark data set   e learning I   I N e.g., multi/hyper spectral [1], synthetic aperture radar [2], etc observation 4] generate more and more different types of airborne or satellite images with different resolutions  res olution images, which  space   satellite image analysis, is to categorize scene images into a discrete set of meaningful LULC classes according to the image contents. During the past decades, remarkable efforts have been made in developing various methods  because of its important role for a wide range of appli cations, such as natural hazards detection [5]\226[7], LULC determination [8]\226[43], geospatial object detection [27 44]\226[52], geographic image retrieval [53]\226[63], vegeta tion mapping [64]\226[68], environment monitoring, and urban planning In the early 1970s, the spatial resolution of satellite and hence, the pixel sizes were typically coarser than, or at the best, simi lar in size to the objects of interest [69]. Most of the meth ods for image analysis using remote sensing images devel oped since the early 1970s are based on per-pixel analysis or even subpixel analysis for this conversion [69]\226[72 With the advances of remote sensing technology, the spa  interest and the objects are generally composed of many  l d r t    g d n n   u g e   e  t B y G N g G C N g G J I  H H N    D X N g G L U  E 


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination 2   EEE  variability and single pixels do not come isolated but are knitted into an image full of spatial patterns. In this case, it  images at pixel level purely    to conclude that researchers should focus on the spatial pat     ation and analysis of remote sensing images rather than indi  semantic entities or scene components that are distinguish   the production of a set of nonoverlapping segments \(or poly gons ful geographically based objects or superpixels that share rela   object-level methods have dominated the task of remote sens ing image analysis for decades [70]\226[80  have demonstrated impressive performance for some typi  carry little semantic meanings. For semantic-level under standing of the meanings and contents of remote sensing images, we take eight images from the very popular UC data as examples as shown in Fig   1\(a h residential intersec pixel- and object-level meth ods are distinctly not enough to identify them correctly Fortunately, with the rapid development of machine learn ing theories, recent years have witnessed the emergence of    16], [18], [23], [24], [26], [28], [33], [36]\226[38], [53], [55 59], [81]\226[87]. Here, a scene image usually refers to a local image patch manually extracted from large-scale remote sensing images that contain explicit semantic classes \(e.g commercial area, industrial area, and residential area    data sets 9], [11], [17], [33], [38], [82] \(as summarized in Table 1 presenting various methods [9]\226[11], [15], [16], [18], [24 26], [28], [33], [36]\226[38], [53], [55], [59], [81]\226[87] for the and  almost all existing data sets have a number of limitations such as the small scale of scene classes and images per class the lack of image variations and diversity, and the saturation  most popular UC Merced data set [38] with deep ConvNets features [82 ment of new data-driven algorithms especially deep-learn ing-based methods     ig. 1 ced land use y erpass able      


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination EEE   3 1 which is a freely and publicly avail    total number of 31   500 images, covering 45 scene classes  with 700 images in each class. We highlight three key fea            data set possesses big image variations in translation, spa tial resolution, viewpoint, object pose, illumination, back  diversity and between-class similarity To sum up, the main contributions of this paper are threefold 1   This paper provides a comprehensive review of the  publications we review existing publicly available benchmark data sets and three main categories of  tion, including handcrafted-feature-based methods unsupervised-feature-learning-based methods, and deep-feature-learning-based methods    We propose a large-scale, publicly available   To the best of our knowledge, this data set is of the larg  number of images. The creation of this data set will ena   improve the state of the arts 3   We investigate how well the current state-of-the-art   tested on small data sets. It is unclear how they com pare to each other on a larger data set with a large num ber of representative methods including deep-learn  tion using the proposed data set and the results are reported as a useful performance baseline  reviews several publicly available data sets for remote sens   details on our data set. A benchmarking of state-of-the-art  I   A R   S  sets 11 17 33 38      section t 38 s c o m p o s e d       6 _  _  2 6   p   n             i     8  1 0   1 1   1 4  226  1 6   1 8  226 0  2 4   2 6   2 8  226  3 0   3 6   3 8   5 3   5 5   5 9   6 1   2  8 1   8 2   8 4  226  9 8  t  version of a set of satellite images exported from Google Earth \(Google Inc tial resolution up to 0.5 m and spectral bands of red, green and blue. It contains 19 scene classes, including airport beach, bridge, commercial area, desert, farmland, football  ing lot, pond, port, railway station, residential area, river and viaduct. For each scene class, there are about 50 images The image sizes are 600     600 pixels. This data set is challenging due to the high variations in resolution, scale, orientation, and illu minations of the images. However, the number of images per class of this data set is relatively small compared with 1    


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination 4   EEE UC Merced data set [38]. This data set has also been widely  methods [9], [16], [18], [33], [37], [83]\226[87], [97 t         200 pixels. It was collected from Google Earth \(Google Inc       methods [8], [10], [11], the number of scene classes is rela  and hence lacks diversity and is less challenging t  sensing images which are composed of seven scene classes grass land, forest, farm land, parking lot, residential region there are 400 images collected from the Google Earth \(Google Inc that are cropped on four different scales with 100 images per scale. Each image has a size of 400     400 pixels. The main challenge of the images [14 t  Google Inc           size of 512     512 pixels and a spatial resolution of 0.2 m t  of only two scene classes \(coffee class and noncoffee class  64       sidered the green, red, and near-infrared bands because they are the most useful and representative ones for distinguish  performed manually by agricultural researchers. To be spe  tiles with at least 85% of coffee pixels were assigned to the coffee class; tiles with less than 10% of coffee pixels were  manage ment techniques, different plant ages, and spectral distor tions, there are only two scene classes, which is quite small    A S   S been made to develop various methods for the task of scene clas  tion is usually carried out in feature space, effective feature representation plays an important role in constructing high  categories according to the features they used: handcraftedunsupervised-feature-learningmethods It should be noted that these three categories are not neces sarily independent and sometimes the same method exists with different categories s                mainly focus on using a considerable amount of engineer ing skills and domain expertise to design various humanengineering features, such as color, texture, shape, spa    Here, we briefly review several most representative hand     gradients \(HOG  1 Among all handcrafted features feature is almost the sim plest, yet an effective visual feature commonly used in image  A major advantage of color histograms, apart from their ease to compute, is that they are invariant to translation and rotation about the viewing axis. However, color histograms are not able to convey the spatial information, so it is very   


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination EEE   5 is also sensitive to small illumination changes and quantiza tion errors 2 as gray GLCM Gabor feature  are widely used for analyzing aerial or satellite images [51 56], [62], [100]\226[102]. Texture features are commonly computed by placing primitives in local image subregions and analyzing the relative differences, so they are quite use ful for identifying textural scene images 3         4 _  _  4   grid for which orientation histograms are extracted. Note    popularly used for scene representation [111]\226[113 4     generally has four steps, namely, scale space extrema search       invariant to changes in scale, rotation, and illumination 5  represent objects by computing the distribution of gradi sub regions, which has been acknowledged as one of the best features to capture the edge or local shape information of the objects. It has shown great success for many scene classi  addition, in order to further improve the description ability of HOG for remote sensing images, several extensions are also developed [118]\226[121 These human-engineering features have their advan tages and disadvantages [56], [90], [101], [102]. In brief, the  the overall statistical properties of an entire image scene in terms of certain spatial cues such as color [56], [99], texture [104]\226[106], or spatial structure   feature are local features that are used for the representa tions of local structure [108] and shape information [109 used as building blocks to construct global image features, such as  9], [14], [19], [29], [36], [38], [39], [55], [93], [101], [122 123] and HOG feature-based part models [22], [23], [27 103]. In addition, a number of improved feature encoding in the past few vector coding  84], [86   model \(PTM                  the performance. For example, Zhao et al     et al     Although the combination/fusion of multiple com how to effectively fuse the different types of features is still an open problem. In addition, the features introduced above are handcrafted, so the involvement of human ingenuity in  tional capability as well as the effectiveness for scene clas  challenging, the description ability of those features may become limited or even impoverished s To remedy the limitations of handcrafted features, learn ing features automatically from images are considered as a feature learning from unlabeled input data has become an attractive   26], [28], [33], [37], [54], [63], [87], [95], [126]\226[134 Unsupervised feature learning aims to learn a set of basis  input of the functions is a set of handcrafted features or raw pixel intensity values and the output is a set of learned fea  manually designed features, we can obtain more discrimi native feature that is better suited to the problem at hand Typical unsupervised feature learning methods include, but not limited to, principal component analysis \(PCA    k   means clustering, sparse coding [136], and autoencoder 137]. It is worth noting that some unsupervised feature learning models such as sparse coding and autoencoder can be easily stacked to form deeper unsupervised models 


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination 6   EEE 1 fea  resentative projection matrix such that the projections of the data points can best preserve the structure of the data distribution [135]. To this end, PCA learns a linear transfor of the transformation matrix form a set of orthogonal basis vectors and thus make it possible to calculate the principal compo nents of the input data. Recently, some extensions to PCA have also been proposed, such as PCANet [138] and sparse PCA [126]. In [138], PCA was employed to learn multistage  robust  126], Chaib et al presented an informative feature selec sat  to obtain more abstract representations since the composition operation  coder [137] that will be reviewed below, have been devel oped to extract nonlinear features 2    K   Means Clustering The   k   means clustering is a method of vector quantization that aims to divide a collec into   k   clusters a cluster are more similar to each other than they are to items   k   means clustering is usually carried out when label information is not available concern  algo rithm iteratively repeats two steps: in the assignment step cen troid. In the update step, the centroids are recomputed as clusters The algorithm terminates when the assignments no longer change. To initialize the algorithm, a universal method is to randomly choose   k   data items as the initial centroids. Owing to its simplicity   k   means clustering method is widely used for unsupervised-feature-learning-based scene image classi  methods [8], [9], [14], [19], [29], [86], [87], [89], [91], [122    where the visual dictionaries codebooks are generated by performing   k   means clustering on the set of local features 3 Coding Recently, sparse coding [136] instead of vector quantization has been applied to dictionary learn over complete dictionary from unlabeled training samples, such that we can represent an input sample as a linear combina  26], [33], [37], [95], [128] or reconstruction residuals [20 47] for each sample form their new feature representations The core of sparse coding is to encode each high-dimen sional input vector sparsely by a few structural primitives in a low-dimensional manifold [141]. The procedure of seek ing the sparsest representation for a sample in terms of the overcomplete dictionary endows itself with a discriminative  tors using sparse coding consists of two separate optimiza  over basis vectors across a batch of training samples at once More recently, a large number of sparse coding meth  sensing images [20], [26], [28], [33], [37], [95], [128]. For example, Cheriyadat [26] adopted sparse coding to learn a set of basis functions from unlabeled low-level features. The low-level features were then encoded in terms of the basis  et al 33] introduced a sparse-coding-based multiple feature  et al  28] designed a novel method for satellite image annota tion using multifeature joint sparse coding with spatial rela tion constraint. However, sparse coding is computationally expensive when dealing with large-scale data. Inspired by the  nearby elements in the dictionary, Wang al 142] pro posed locality constrained linear coding \(LLC assumes that a data can be reconstructed by using its   k   near  can be computed through solving a least squares problem The weights for the remaining atoms are set to zero and the sparsity is replaced with locality [85], [142 4 Autoencoder [137] is another famous unsupervised feature learning method that has been suc  133], [134]. It is a symmetrical neural network that is used to learn a compressed feature representation from highdimensional feature space in an unsupervised manner. This is achieved by minimizing the reconstruction error between the input data at the encoding layer and its reconstruction at the decoding layer. The number of nodes in the encod ing layer is equal to that of the decoding layer. To reduce recon the hidden layers. The activations of the hidden layer are usu ally used as the compressed features. Gradient decent with back propagation is used for training the networks. To train a multilayer stacked autoencoder, the most important issue is how to initialize the networks. If initial weights are large, autoencoder tends to converge to poor local minima while small initial weights lead to tiny gradient in early layer, making it infeasible to train such a multilayer net by Hinton al 137], which found that initializing weights  good solution In real applications, the aforementioned unsupervised feature learning methods have achieved good performance 


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination EEE   7  crafted-feature-based methods. However, the lack of seman tic information provided by the category labels cannot guarantee the best discrimination ability between classes because unsupervised feature learning methods do not make  formance, we still need to use labeled data to develop super vised feature learning methods, which will be reviewed below, to extract more powerful features s Most of the current state-of-the-art approaches generally feature representations. Especially in 2006, a breakthrough in deep feature   engineered features with trainable multilayer networks and impressive feature representation capability for a wide range of appli  13], [17], [45], [50], [82], [143]\226[158 On the one hand, in comparison with traditional hand crafted features that require a considerable amount of engi neering skill and domain expertise, deep learning features are automatically learned from data using a general-purpose learning procedure via deep-architecture neural networks This is the key advantage of deep learning methods. On the other hand, compared with aforementioned unsupervised shallow-struc tured models \(e.g., sparse coding that are composed of multiple processing layers can learn more powerful feature representations of data with multiple levels of abstraction [159]. In addition, deep feature learning methods have also turned out to be very good at discovering intricate structures and discriminative information hidden in high-dimensional data, and the features from toper lay ers of the deep neural network show semantic abstracting properties. All of these make deep features more applicable Currently, there exist a number of deep learning mod   convolutional neural networks \(CNNs   on. Limited by the space, here we mainly review two widely  deep learning methods 1  model that has been successfully applied for remote sens  of multiple layers of autoencoders in which the outputs of each layer are wired to the inputs of the successive layer. To   layer on raw input data to obtain parameters and transfer the raw data into an intermediate vector consisting of activa tions of the hidden units. Then, this process is repeated for subsequent layers by using the output of each layer as input the parameters for the remainder of the model. To obtain better results, after  to tune the parameters of all layers at the same time with a smaller learning rate. Compared to a single autoencoder as mentioned in the previous subsection, the feature repre  This can be easily explained: with the composition of multi ple autoencoder that each transforms the representation at one level \(starting with the raw input at a higher, slightly more abstract level, we can learn very powerful representations. This has been proven in literature 13], [134], [169]\226[171 2 CNNs are designed to process data that come in the form of multiple arrays, for example, a multispectral image composed of multiple 2-D arrays containing pixel  the impressive success of AlexNet [163], many representa   been proposed in the literature. There exist four key ideas behind CNNs that take advantage of the properties of natu ral signals, namely, local connections, shared weights, pool ing, and the use of many layers [159 The architecture of a typical CNN is structured as a series of layers 1   Convolutional layers: They are the most important  edges lines, and corners features \(such as structures objects, and shapes 2   Pooling layers: Typically, after each convolutional layer, there exist pooling layers that are created by computing some local nonlinear operation of a par ticular feature over a region of the image. This pro cess ensures that the same result can be obtained even when image features have small translations or  tion and detection 3   Normalization layers: They aim to improve generali zation inspired by inhibition schemes presented in the real neurons of the brain 4   Fully as  straints, they can better summarize the information  decision. As a fully connected layer occupies most of  vent this, the dropout method was employed   163 


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination 8   EEE  CNNs in various remote sensing applications, such as geo      For instance, to address the problem of object rotation vari ations, Cheng et al       explicitly enforces the feature representations of the train  each other. Castelluccio et al 152] and Nogueira et al       best performing strategy on small-scale data sets V   T  T   dedicated toward the construction of various data sets [9], [11], [17], [33], [38  tion. Despite the remarkable progress made so far, as we  all existing remote sensing image data sets have a number of of scene classes numbers the lack of scene variations and diversity, and the saturation  popular UC Merced data set with deep CNN features [82 These limitations have severely limited the development of new data-driven algorithms and also prohibited the wide use of deep learning methods because almost all deep learning models are required to be trained on large training data sets Under such a circumstance, proposing a large-scale data set with big image variations and diversity is highly desirable   which is a freely and publicly available benchmark data set 5 t                 data set. These 45 scene classes are as follows: airplane, air port, baseball diamond, basketball court, beach, bridge, chap     overpass, palace, parking lot, railway, railway station, rec   court, terrace, thermal power station, and wetland   form, including land-use and land-cover classes \(e.g., commer cial area, farmland, forest, industrial area, mountain, and resi    ice terns, some homogeneous with respect to texture, some homo geneous with respect to color, others not homogeneous at all t    includes 700 images with a size of 256     256 pixels in the red\226   except for the classes of island, lake, mountain, and snow         Earth by the superimposition of images obtained from satel       shows two samples of each class from this data set   1        and publicly available benchmark data set, which covers 31   500      


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination EEE   9 ig. 2 eathers anslation c 


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination 10   EEE     the state of the arts 2 image variations  system, be it human or machine. However, most of the existing data sets are not very rich in terms of image vari ations. On the contrary, our images were carefully selected condi tions, imaging conditions, and scales. Thus, for each scene category, our data set possesses much rich variations in translation, viewpoint, object pose and appearance, spatial resolution, illumination, background, occlusion, etc 3 Similarity Many top-performing methods built upon deep  accuracy on most of the existing data sets owing to their sim plicity, or rather the lack of variations and diversity. With this in mind, our new data set is rather challenging with high within-class diversity and between-class similarity. To of conditions  and rectan basket ball court and tennis court, and so on V   B  S    s        Color histograms histograms is almost the simplest handcrafted feature that has been   because of its simplicity. Each channel is quantized into  64 bins for a total histogram feature length of 192. The his tograms are normalized to have an L1 norm of one 2   frequencies of local patterns in subregions. For an image, it   N   neighbors: when the neighbor\222s value is bigger than the value of center pixel output 1, otherwise, output 0. This forms an   N   decimal  obtained by computing the histogram of the decimal num bers over the image and results in a feature vector with   2   N     dimensions. In our implementation, we set   N     8   hence 3      is then averaged over 16 nonoverlapping regions arranged on a   4 _  _  4   grid. The resulting image representation is a 512-dimensional feature vector 4  popular visual features during the last decade. Owing to its  widely used by the community for geographic image clas  a   Patch extraction: With an image as input, the out puts of this step are image patches. This step is implemented via sampling local areas of images in a dense or sparse manner b   Patch the outputs of this step are their feature descriptors such as the c   Codebook generation: The inputs of this step are feature and the output is a visual codebook. The codebook is usually formed by unsupervised   k   means clustering over all feature d   Feature encoding: Given feature descriptors and codebook as input, this step quantizes each feature descrip tor into a visual word in the codebook e   Feature pooling: This step pools encoded local descriptors into a global histogram representation for each image 5 SPM     then concatenates them to represent the image. In our implementation, we divide each image into   1 _  _  1   and   2 _  _  2    subregions. Thus, given a codebook with the size   K   we can obtain a   5 K   dimensional feature vector for each image by 6 LLC variation of sparse coding       


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination EEE   11 simply by   k   means clustering without optimization. Therefore  7  Krizhevsky et al and was the winner of ImageNet large-scale     layers follow both response normalization layers and the   employ nonsaturating neurons, GPU implementation of the  feature from the second fully connected layer, which results in a feature vector of 4096 dimensions 8     former one because of its simpler architecture and slightly   CNN feature was also extracted from the second fully con nected layer to obtain a feature vector of 4096 dimensions 9 GoogLeNet is another representa tive CNN architecture that achieved new state of the art for  The main hallmark of its architecture is the improved utili  carefully crafted design, the depth and width of the network were increased while keeping the computational budget con of  more spatial information; and b to over  inside has 12 times fewer parameters than AlexNet. In our work, we extracted the GoogLeNet CNN feature from the last pooling layer to form a feature vector of 1024 dimensions 10    obtain better performance without using any data augmenta      gress while not clobbering the initialization p To make a comprehensive evaluation, two training\226test ran domly split into 10% for training and 90% for testing \(70 training samples and 630 testing samples per class 20%\22680%: the data set was randomly divided into 20% for training and 80% for testing \(140 training samples and 560 testing samples per class   each image patch with the patch size set to be   16 _  _  16   pix els and the grid spacing to be 8 pixels to balance the speed of 86]. The sizes of visual codebooks were set to be 500, 1000 2000, and 5000, respectively, to study how they affected the  GoogLeNet model, which were pretrained on ImageNet  caffe/wiki/Model-Zoo for deep CNN feature extraction To further improve their generalization capability, we also  Table   2. All three CNN models were implemented on a PC       C     1        scene class by treating the images of the chosen class as posi  s There exist three widely used, standard evaluation met   able 2    


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination 2   EEE  which class they belong to, divided by the total number of   matrix is an informative table used to analyze all the errors and confu sions between different classes which is generated by count  test samples and accumulating the results in the table the same image number per class, so the value of overall accu this paper, we just used the metrics of overall accuracy and  addition, in order to obtain reliable results for the metrics of overall accuracy and confusion matrix, we repeated the and report the mean and standard deviation of the results ts            all based on these optimal parameter settings Tables 3\2266 show the overall accuracies of three hand crafted global features, three unsupervised feature learning  CNN features, respectively, under the training ratios of 10 and 20%. The following can be seen in Tables 3\2266 1   Handcrafted low-level features have the relatively    tures. Actually, they act as mid-level image features that are ig. 3 W+SPM, and  0%; and  able                 


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination EEE   13  hence provide more semantic and more robust represen  semantic gap 3   Deep CNN features outperform all handcrafted features and unsupervised feature learning methods in very big margins \(at least 30% performance improvement  This demonstrates the huge superiority of the current-dom inated deep learning methods in comparison with previous state-of-the-art methods   els, the accuracy was further boosted by at least six percent age points, resulting in the highest accuracy matrices of different methods under the training ratios of 10% and 20%, respec tively, where the entry in the   i   th row and   j   th column denotes the rate of test samples from the   i   as the   j   th class. Limited by the space, we here just report the confusion matrices with the highest overall accuracies selected from features unsupervised feature learning methods, CNN features, and  the following 1 per-class accuracies, unsupervised feature learning methods take the second place, and deep-learning-based CNN features have the highest per-class accuracies 2 sions happen between \223golf court\224 and \223meadow\224 because they are characterized by green color  relatively big confusions happen between \223church\224 and \223pal    may be deep-learning-based methods in combination with dis criminative attributes oriented methods such as [23 I   ON            ig. 4 6 6 


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination 14   EEE        future research    cover in a given region. In fact, the more recent develop    of social media especially the online photo sharing web  collecting sorts of information of ground objects from geo                   Earth using the \223what\224 and \223where\224 aspects of the infor  the ground photos uploaded by user hold higher resolu    additional information is in fact very useful for the classi    future work, we need to explore new methods and sys  information coming from social media and spatial tech     6 6 


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination EEE   15 S            2   H    EEE      3   L   e  EEE      4   P l  EEE        224  s      6   G d                   l 224        9   L gs         r  s        224          s          s       l  s       e         e r 224       7   Q  r 224            g h 224 s        d         h       1   J i l r         2   G  l e  224        l            e   t                6   A e 224        7   G  d n          a l  e         h s         264  264 l g  e                    f 224          e          h          n   s       n                  e         224  t 0    0   M   224    0        t     2   R             g   s      4   G g l e  224        5   G g   224  s   


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination 16   EEE   6   J  e d 224      7   J n l s  d 224           n 224 g      r 224        n d 224        l 224         n l       3   Y g l     s       4   Y   d y  s                 e 224 n P         224        d g 224 e       9   L d d    226       224         g  g      2   E l e  e                4   M  c s    s        224        y  m         e d  e        224       9   L t 224          T S       e  0     tr g s       l d 224         n  a       d n e      g 224 y         n y           s      x         0   H  n d a 224   0     e        2   O s  l n  s        n  S      4   L  n y 224 s       s        n 224 s                   e  g      9   L e s   s       s  g         e  e    


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination EEE   17      e  224         l  h          224 s      264  264 d e           224 h        g y         d e   a                e 224        r n            e       3   G   e   t  1     e  n        g 224       6   T   l 224        7   A e c 224        8   D            t         t         n o l      224 s       224 s         n  t  3         7    6   C e    g      7   G    n          n n 224          d 224  s          224  s      n  224 n e      2   L D e         224         e          r e  e        d    t      g 224        e          r   s       r 224           224 s      2   W    224 s        g e e          e       5   I  s        A  s   


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination 8   EEE     h  e       p 224 s       d 224 n          224        1   G t g  e      2   J  r n   t      l  224 s          e       5   I c 264  t 224         l                  8   Y e  l        9   D            0   M d  g  e       A 224         l e A v a i l a b l e  2    3   K    6 n  A v a i l a b l e  h t t p s   a r x i v o r g  7    4   G r  n   t       g     0      224 n a       g 224        n   s        e       f  ut        r   ut        l h a   s  0     p   t            nt       n  nt      l h   t                  f n l  0      n  o   5                224        n  t      n    s       l n  t       e n E t       224    1 


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination EEE   19 S Gong Cheng om Xidian  in 2007 and the M.S. and technical  3   He is currently an Associate Professor with Northwestern Polytechnical University. His main research interests are computer vision and pat tern recognition ei Han ently techni ch The ersity cher at the Uni His omputer vision, multi and brain imaging analysis. He es such as IEEE T C t T IONS  ON P A t t T ERN  A A YSIS  AND M CHINE  I I N t T ELLIGENCE AMI I I N t T ER NA t T IONAL J OURNAL  OF  C C O m p MP U t T ER V ISION V T C t T IONS  ON  I I m M GE P SSING  TIP C C ONFERENCE  ON  C C O m p MP U t T ER V ISION  AND P A t t T ERN  R R OGNI t T ION VPR I I N t T ERNA t T IONAL  C C ONFERENCE  ON  C C O m p MP U t T ER V ISION V I I N t T ERNA t T IONAL J OIN t T  C C ONFER ENCE  ON  A A R t T IFICIAL  I I N t T ELLIGENCE IJCAI Prof. Han is an Associate Editor of the I E E E IEEE T RANSAC t T IONS  ON  H H U m M AN M ACHINE  S S YS t T E m M S  Neurocomputing   Processing and Machine Vision and Applications  u ently f  tor ests include emote sensing om e eas  international journal, including Neurocomputing Elsevier Cognitive  Computation Springer International Journal of Image and Graphics  World of Scientific 


