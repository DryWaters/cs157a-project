  1 
 
Adaptive Source and Channel Coding for Distributed 
Applications  Philip Tsao, Michael K. Cheng, George Lu  and Clayton Okino Jet Propulsion Laboratory California Institute of Technology 4800 Oak Grove Dr Pasadena, CA  91109 ptsao, mkcheng, cokino}@jpl.nasa.gov Abstract 227Distributed applications are often faced with a choice between improved throughput or improved reliability but not both. We argue that this is not a strict dichotomy and propose a framework that improves both application performance and reliability by adaptively adjusting source and channel coding parameters. For simplicity, we assume 
that the source and channel we work with are memoryless and in general behave in a way such that Shannon's separation theorem holds. A lthough not all sources and channels could be characterized as such, doing so allows us to work this resource allocation problem in parallel. We reduce redundant transmissions and minimize bandwidth utilization through an LZ77 styl e dictionary based source coding approach. To ensure data integrity, we apply rateless forward error correction techni ques at the transport layer Our algorithm works in conjunction with physical layer forward error correction and generates just enough overhead needed to achieve error free transmission without requiring a heavy use of a reverse channel for acknowledgments. We show through simulations that our combined source and channel approach reduces network 
traffic in our experimental platform by a measurable amount while maintaining and at tim es exceeding the Quality of Service \(QoS\hat is obtained without our technique 1 2  T ABLE OF C ONTENTS  1  I NTRODUCTION 1  2  S OURCE C ODING 1  3  A DAPTIVE S OURCE C ODING 
3  4  C HANNEL C ODING 4  5  S OURCE AND C HANNEL C ODING 6  6  S UMMARY AND P OSSIBLE F UTURE W ORK 6  7  A CKNOWLEDGEMENTS 
6  R EFERENCES 7  B IOGRAPHY 7  1  I NTRODUCTION  It is now common practice to assemble many computers together in a network to solve large problems. For so called 223embarrassingly parallel\224 workloads the major determinant of the total runtime is the number of and processing capacity of hosts. Examples of these types of problems   George Lu is affiliated with the Un iversity of California, Berkeley 1 978-1-4244-3888 
4/10/$25.00 \2512010 IEEE 2 IEEEAC paper #1221, Version 4, Updated January 5, 2009 include Monte Carlo methods, serving of stating web content and brute force searches through cryptographic keyspaces [6]. For all other ty pes of workloads, the capacity of the network will eventually dominate as the problem size increases. Some examples of these types of problems include financial transaction clearing, solving large linear systems and large-scale multiagent simulations  Improving the performance of a typical communication bound distributed application \(simplified system diagram in Figure 1\ly involves adding capacity to a network by increasing the bandwidth. Heuristics such as \223Edholm's  aver 
age, every thr ee years there will be an approximate factor of two improvement in bandwidth available at an approximately constant cost. This means one can estimate the ne twork capacity that can be purchased for a given price at a given time. Unfortunately not all problems are patient enough to wait for economic convenience so any way to extend the capacity of existing network equipment would squeeze more return out of the capital that has already been invested. When network capacity is insufficient to satisfy demand, there are existing techniques that improve overall application performance and Quality of Service by reordering or dropping lower priority packet  15 Thi s paper expl ores al t e rnat i v e methods to optimize the performance of distributed 
applications by improving the capacity and reliability of the communications network via adaptive source and channel coding 2  S OURCE C ODING  Source coding \(also known as \223data compression\224\s a method to achieve efficient use of available network bandwidth while avoiding major capital expenditure Implementations fall into two categories: lossless \(in which  Figure 1. Diagram of the simplest nondegenerate example of a distributed application. This consists of a sender, a communications channel \(network link and a receiver 


A B,S},N Probability Symbol Code 1/7 B 1 1/7 S 0 2/7 N  3/7 A   Dictionary Encoded Decoded  2/7 {B S} 1 2/7 N 0 3/7 A  B,S N B B   2 the compressed data contains enough information to reconstruct the original data perfectly\and lossy \(in which the data is compressed in a way that the decompressor can only reconstruct a similar but inexact representation of the original data\e latter is well suited for applications where perfect reconstruction is impractical or imperfect reconstruction is \223good enough\224 examples of both include music and video data\ile the former is better suited for applications in which the data integrity is sacrosanct \(real time stock quotes, executable code, etc\ication of source coding in this study is focused on the lossless category \(system diagram in Figure 2  Techniques for source coding fall into two main categories statistical methods and dictionary methods. Statistical methods rely on the assumption that the data transferred across a network is highly redundant and the constituent symbols have a nonuniform pr obability distribution. As long as this assumption holds true ``often enough" then frequently occurring symbols can be represented by small numbers of bits while less frequently occurring symbols would be represented by larger numbers of bits. An example of this is the Huffm see Tabl e 1 and Fi gures 3 and 4 To construct a Huffman encoding for a given message, first sort the symbols of the messa ge from lowest to highest frequency. Assign the lowest probability symbol the code 2230\224 and the next lowest probability symbol the code \2231\224 Merge the two lowest probability symbols into one symbol with the combined probability of the two symbols. Repeat until there are two symbols le ft. The codeword for each symbol is determined by concatenating the codes encountered through traversi ng the resulting tree from the leaf node \(symbol\o the root Table 1. Illustration of the construction of the Huffman table for the input string \223BANANAS\224 B BA BA BA BAN BAN BAN BAN\(2,3\BANANA BANANA BAN\(2,3\BANANAS   Figure 2. Diagram of the simple distributed application Figure 1 1 st Iteration 2 nd Iteration 3 rd Iteration    3/7 A 1   4/7 {{B,S},N 0 BS root 1 1 1 0 0 0  Figure 3. Tree constructed from Huffman table  Figure 4. Huffman code for the string "BANANAS A" is underlined and marked in red Lempel-Ziv is a popular example of a dictionary coding algorithm in which references to a history buffer \(the dictionary\smitted in lieu of the uncompressed data  Sy m bol s t h at are not found i n t h e di ct i onary  are represented explicitly. For example, the string BANANAS might be transmitted as \223BAN\(2,3\S\224. The pair \(2,3 denotes the string that begins with the 2nd to last symbol in the dictionary \(in this case, \223A\224\hat is 3 symbols long ANA\mbol of the string is not \(yet in the dictionary until the first symbol of the string is appended to the end of the dictionary Table 2. Example encoding a nd decoding of the string 223BANANAS\224 using Lempel-Ziv 


  3 Notice that the lengths of the Huffman and Lempel-Ziv encoded versions of the short string \223BANANAS\224 may not compare favorably to the uncompressed string length \(2 bits per symbol times 7 yields 14 bits\the length of the Huffman tree representation is included then the Huffman encoding results in a significant data expansion. Likewise the Lempel-Ziv encoding of the string \223BAN\(2,3\s at least 17 bits \(3 bits per character symbol and 5 bits for the 2-tuple\ight expect a longer string to be more compressible and empirical data appears to corroborate this                                 10 1                   10 3                   10 4                   10 5 0.45 Ratio of Compressed to Original Size Message Length \(bytes  Figure 5. Compressibility of different length strings from the Cal n g a L empel Z i v  algorithm. Observe how the compression ratio saturates as the message length approac hes infinity. Plotted data is from [8 Figure 5 is a plot of the compressibility of strings of various lengths. The strings that were compressed had statistical properties similar to English text. In general it is true that the larger the average uncompressed message length of a remote data transaction, the better the expected compressibility independent of the statistical distribution of the data. This is because larger message lengths permit Lempel-Ziv style algorithms to analyze larger sets of substrings There are many different ways to achieve more efficient lossless source coding. One well-known technique is to combine statistical and dictionary methods by running a Huffman coder on the output of the Lempel-Ziv encoder This is very similar to the DEFLATE algorithm popularized by the ZIP file form  For short strings, it m a y  be profitable to preset the dictionary or initial Huffman table to best match some expected properties of the data. It is generally true that more effi cient encodings require more processing overhead. This overhead can be due to an increased number of processing passes \(to determine symbol frequencies for Huffman coding\he use of more sophisticated substring matching algorithms. In general finding the longest common substring is NP-hard  Therefore, for practical distributed applications, heuristic substring finding algorithms which trade processing overhead for encoding effici ency are preferred 3  A DAPTIVE S OURCE C ODING  For members of the Lempel-Ziv algorithm family, one can expend more effort \(and conse quently, time\oward finding longer common substrings to achieve better compression results 3 Alternatively, one can opt to expend less effort toward substring matching if one is willing to tolerate an inferior compression ratio. Clearly it doesn\222t make sense to spend more time finding longest common substrings than required to transmit the uncompressed data. Source coding for data transmission is advantageous when the total elapsed time \(compressed T EC is less than the total elapsed time uncompressed T EU We define T EC  T C  T TC sum of time required to compress the data and the time required to transmit the compressed data\d T EU  T TU time required to transmit original uncompressed data  For optimal performance, not only do we want T C  T TC   T TU we seek to minimize T C  T TC In general, this is a multidimensional optimization problem as T EC is a function of several variables \(including network throughput, CPU instruction throughput and choice of longest substring matching algorithm\. Most implementations of Lempel-Ziv style algorithms expose a programming interface that let an application programmer select more or less exhaustive longest substring search algorithms by adjusting a scalar parameter. For example, the ZLIB library \(an implementation of the DEFLATE algorithm\ assigns integer values of 0 through 9 to map to a mode in which no compression is attempted \(0\e way to a mode in which an extremely slow longest substring matching algorithm is used \(9\pically, one dimensional optimization techniques are employed in order to minimize T EC  Figure 7 contains several plots of T EC  x  x 1 to 11 for a 177 megabyte file containing one day of page load statistics for Wikipedia \(1 May 2009\e that for these plots the range x 3 through x 11 correspond to ZLIB compression levels 1 though 9 \(maximum effort\while x 2 maps to a fast Lempel-Ziv implementation known as LZF  3 Other source coding algorithms have analogous parameters that are modifiable  Figure 6. Diagram of distributed application with Ada p tive Source Codin g  10 2                   0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 


  4  and x 1 maps to ZLIB setting 0 \(no compression each plot, the network thr oughput and CPU clock frequency were held constant while the file was compressed and transmitted several times with different compression settings The principal reason why adap tive source coding \(diagram in Figure 6\ is so interesting is the ability to automatically adapt to different channel conditions. From Figure 7 it is clear that different compre ssion settings are optimal at different link speeds. In a busy network, it is unlikely that the rate of a channel will remain perfectly stationary when there are other nodes that share network resources. A gigabit Ethernet link may only yield a few megabits per second if many other applica tions or nodes are transmitting data along the same path Emanuel Jeannot of INRIA wrote an adaptive source coding library called AdOC [10  wh ich attem p ts to fin d th e m i n i m a of T EC by automatically adjusting ZLIB compression settings or even switching to a different algorithm based on the backlog of internal data queues. When AdOC was applied to the experimental setup used for Figure 7, it soundly outperformed the case in which there is no source coding for all link speeds but performed two times worse than the optimal ZLIB compression setting \(level 3\ in the case of the 100 megabit link. The challenge to designing an efficient adaptive source coding implementation is to not spend too much time searching for a better T EC while at the same time finding a good T EC  4  C HANNEL C ODING  Other than processing overhead, the major disadvantage of source coding is the reduction in reliability. The loss of a single symbol of compressed data may result in the loss of multiple symbols of uncompressed data. Channel coding can be employed to protect data from these losses or erasures" \(perhaps due to congestion in wired networks and interference or fading in wireless networks\ adding enough overhead so that the desired payload can be reconstructed \(system diagram in Figure 8  The capacity of a packet drop channel in which the drops which occur with probability p ndependent is 1p For example if, on average, 1/4 of the packets of data transmission are dropped then one can expect 3/4 of the transmitted packets to arrive at the intended destination Error free communications can be guaranteed in uncoded transmissions with the addition of ARQ \(Automatic Repeat reQuest\tially a reverse channel is used to signal acknowledgement of successful reception of one or more intact packets. If the tran smitter does not receive an expected acknowledgement after a predetermined timeout interval, a packet drop is presumed to have occurred and the packet is retransmitted until the acknowledgement is received. Here, capacity is achieved through the use of a reverse channel Another approach to error free communications is the application of forward error correction \(FEC    Figure 7. Plots of T EC  x  levels on different types of Ethernet links between two 3.16 GHz Intel Core 2 Duo computers. The leftmost bars denote no compression T TU  rightmost bars denote maximum compression. The red \(upper\bar represents the amount of time spent compressing T C and the blue lower\of time spent se n d in g th e  d ata a c r oss th e  w ir e  T TC    Figure 8. Diagram of distributed application with Channel Codin g  


message length in symbols average percent of excess transmitted information symbols Error-free transmission on the erasure channel for LT with c=0.06 and 5 15 20 25 30 35 40   5 progress has been made in efficient design of modulation and coding in the physical layer. A new coding technique termed ``fountain" or ``rateless" codes has attracted much attention for its efficacy in wo rking with the erasure channel  The rat e l e ss st rat e gy fi rst random l y pi cks a set  of packets from a fixed block of message packets according to two probability distributions \(one for the size of the set, the other to choose the members of the set\ and then linearly combines these selected packet s into code symbols before transmission. This encoding process is repeated until the destination acknowledges the recovery of the entire message block to the transmitter. The amount of code symbols sent will vary according to the channel erasure probability. The rate of the code adapts to changing channel conditions and the transmitter sends just enough code symbols to achieve correct me ssage delivery and hence the name \223rateless.\224 Decoding amounts to solving a set of linear equations. The decoding complexity will depend on the probability distribution 4  If the probability distribution is the Robust Soliton, this class of rateless codes is known as the Luby Transform \(LT  The overhead of rat e l e ss codes i s  defi ned as t h e amount of excess transmission beyond capacity. The LT overhead has the unique property of being independent of packet drop probability and instead dependent on the message length. The LT overhead also approaches zero as the message length approaches infinity. Therefore, LT codes asymptotically approach the capacity of the packet drop channel                             message size k=1000  Figure 10. The codeword error rate drops a decade for every 10% increase in excess transmission for LT code parameters c 06 0.5  Figure 9. The average excess transmission required for error free operation diminishes with increasing message length To see an example of this we plot Figure 9. Here, the y-axis plots the ratio of the extra number of transmitted code  4 Although rateless codes can replace exis ting channel codes in the physical layer, the scheme does not have to be mutually exclusive with current techniques and could be deployed in the network layer and be complementary to strong physical layer coding symbols required to achieve a complete message block recovery over the length of a message block and the x-axis spans over different message block lengths k from 10 symbols to 100,000 symbols.  In this case the channel is perfect and does not drop pack ets. Because knowledge of what packets the channel w ill drop is not known a priori some overhead is needed to revocer from packet drops and this overhead exists even wh en the channel is perfect Definitions for the code parameters c and 40 60 80   10 4                   002 002 are given in   Each dat a poi nt i s averaged over 1000 codewords The overhead diminishes as the message length increases For non-zero erasure probability channels, the overhead will simply scale in proportion to the packet drop probability LT codes asymptotically appr oach the capacity of the packet drop channel If one is willing to accept a small codeword error rate, the number of excess transmissions can be limited to a constant and the acknowledgement channel can be eliminated altogether. The expected codewo rd error rate is a function of the excess information transmitted. By changing design parameters of the LT code, one can adjust how much the codeword error rate changes with respect to the excess transmission. An example is given in Figure 10. We fix the message size at 1000 symbols and we plot the likelihood of unsuccessful LT decoding for a given number of code symbols transmitted.  Again, the channel here is perfect and does not drop packets.  The average percent of excess information needed will simply scale with the packet drop probability.  For a medium le ngth message \(1000 symbols the average overhead required to reach a 99% message recovery rate is about 45% of the message length.  The overhead and recovery rate can be traded off                     002 002 5 and a message size k 1000 symbols 0 10 3                   10 2                   10 1                   10 0 10 1                   10 3                   10 4                   10 5 10 0.5   average percent of excess information symbols transmitted Word Error Rate \(WER LT Code with c=0.06 and 10 2                   20 


0.5 0.6 0.7 0.8 0.9 1.2 1.3 1.4 Both  Figure 11. Net compression results after both source and channel coding are applied to the data from the Calgary Corpus. The bottommost \(blue\a replot of Figure 3 while the topmost \(red Figure 6. The middle \(black\an estimate of the expected overhead Applying the average excess tr ansmission required for error free decoding of the LT code \(Figure 9\o the Lempel-Ziv compressed Calgary Corpus data \(Figure 5\ields the intermediate curve on Figure 11 labeled \223Both\224 excess transmission of LT codes converges to zero as the message length increases, the overhead imposed by channel coding diminishes toward the right side of the plot. For message lengths as short as 200 bytes \(not counting header the combination of source coding and channel coding still results, on average, in a net reduction of received data. As message lengths increase the excess transmission overhead of channel coding becomes negligible  Note that Figure 12 assumes the T EC of the Calgary Corpus is stationary. From Figure 8 one can conclude that the stationary statistics assumption is not always the case. In that case, an adaptive s ource coding algorithm has the potential to achieve superior performance 6  S UMMARY AND P OSSIBLE F UTURE W ORK  It is possible to employ source and channel coding concurrently to achieve, on aver age, more efficient use of network resources in addition to a certain degree of improved Quality of Service promised by channel coding This results in a significant performance improvement for distributed applications that do not belong to the class of embarrassingly parallel problems. Furthermore, by adapting parameters of the source and channel coding algorithm dynamically to the data, both higher throughput and increased reliability can be achieved simultaneously Improved algorithms for dynamically adjusting parameters of source coding algorithms remain to be discovered. Care must be taken to not spend too much CPU time minimizing T EC to avoid competing for processing resources that are needed for the source and channel coding algorithms as well as user applications that were already running on the CPU Multicore CPU, GPU \(Graphics Processing Unit\d other hybrid architectures have the potential for changing certain design tradeoffs as processing resources can be partitioned and allocated in novel ways. Considerable progress has been made in this area but th ere appears to be plenty of room for improvement 7  A CKNOWLEDGEMENTS  This research was carried out at the Jet Propulsion Laboratory, California Institu te of Technology, and was funded in part by the Department of Defense Test Resource Management Center \(TRMC\ Test and Evaluation/Science and Technology \(T&E/S&T\ through JPL contract number TP 81-11492. Additiona l support was provided by the California Space Grant C onsortium\037and the National Aeronautics and Space Administration. Scott Darden Elizabeth Bodine and Je ffrey Ung made significant contributions to this effort. Thanks to Loren Clare, Aaron Kiely, Matthew Klimesh, Andr ew Parker, Robert Prag and Fabrizio Pollara for their helpful suggestions  Figure 12. Diagram of distributed application with Adaptive Source and Channel Coding   6 5  S OURCE AND C HANNEL C ODING  Perhaps it isn\222t surprising that source coding tends to reduce the amount of traffic that is sent across a network while channel coding results in an increase in redundant traffic One might expect that if both were enabled at the same time system diagram in Figure 12 then the benefits of each would cancel out and one would be fortunate to just break even. However it turns out that it is possible to do better than just break even                                   Message Length \(octets Ratio of Encoded to Original Size   1 10 1                   10 3                   10 4                   10 5 0.4 1.1 Channel Coding Source Coding 10 2                   


  7 R EFERENCES  1 itten an d J. G. Cleary, \223Mo d e lin g fo r text compression,\224 Computing Surveys 21\(4\ 557-591 December 1989  E. A. B odi ne and M K. C h eng, \223C haract eri zat i on of Luby  Transform Codes with Small Message Size for LowLatency Decoding\224 International Conference on Communications \(ICC\ 2008 Beijing, China, May 2008 3 Bro w n   Engineering a Beowulf-style Compute Cluster  http://www.phy.duke.edu/~rgb/Beowulf/beowulf_book beowulf_book> retrieved Oct 2009   S. C h erry 223Edhol m  s Law of B a ndwi d t h 224 IEEE Spectrum Jul. 2004  p ressed Data Form at Specification version 1.3 RFC 1951 May 1996  I Foster Designing and Building Parallel Programs  Addison-Wesley \(ISBN 9780201575941  L Franken Quality of Service Management: A ModelBased Approach PhD thesis, Centre for Telematics and Information Technology, 1996  R Fri e nd and R M onsour, \223IP Pay l oad C o m p ressi on Using LZS\224 RFC 2395 Dec. 1998  D. Huffm an, "A M e t hod for t h e C onst ruct i on of Minimum-Redundancy Codes Proceedings of the I.R.E  Sep. 1952  E. Jeannot 223Im p rovi ng M i ddl eware Perform ance wi t h  AdOC: An Adaptive Online Compression Library for Data Transfer\224 Proceedings of the 19 th IEEE International Parallel and Distributed Processing Symposium \(IPDPS 05 Apr. 2005  M Lehm ann LibLZF   http://oldhome.schmorp.de/marc/liblzf.html retrieved Aug 2009  G. Lu, \223Feedback C ont rol Appl i e d t o Dat a C o m p ressi on and Transmission\224 JPL Summer Student Report Aug 2009  M Luby 223LT C odes\224 Proceedings of the ACM Symposium on the Foundations of Computer Science FOCS Nov. 2002  D. M a i e r. "The C o m p l e xi t y of Som e Probl em s on Subsequences and Supersequences J. ACM 25: 322\226336 1978  M M a rchese QoS Over Heterogeneous Networks  Wiley, 2007   J. Zi v and A. Lem p el  223A Uni v ersal Al gori t h m for Sequential Data Compression\224 IEEE Transactions on Information Theory 23\(3\ay 1977 B IOGRAPHY  Philip Tsao is a member of the Communications Networks Group at JPL. He received a BEE from Georgia Tech in 1999 and an MSEE from Caltech in 2001 where he pursued research in collective robotics. From 2001 to 2005 he was a Systems Engineer with Raytheon Space and Airborne Systems where he supported radar software and mixed signal circuit design efforts Michael K. Cheng S\22202\226M\22203 received the B.S. degree from Carnegie Mellon University Pittsburgh, PA, the M.S. degree from The University of Texas at Austin, and the Ph.D. degree from University of California, San Diego in 1995, 1997 2004, respectively all in electrical and computer engineering. Since then, he has been with the Information Processing Group, Jet Propulsion Laboratory, Pasadena CA, working on design and effic ient implementation of modern error correcting codes George Lu is a senior at UC Berkeley majoring in mechanical engineering and minoring in electrical engineering. He became interested in electrical engineering after he took some programming and signals processing classes and saw how applicable they were. He has a wide variety of interests in engineering, some of which include robotics, controls, and design  Clayton Okino received a BS in Electrical Engineering at Oregon State University in 1989, a MS in Electrical Engineering at Santa Clara University in 1993, and a Ph.D. in Electrical and Computer Engineering from the University  


  8 of California, San Diego in 1998 After receiving his Ph.D Dr. Okino accepted a position as an assistant professor in Thayer School of Engineering at Dartmouth College, where he pursued research in communication and wireless networks, emphasizing on performance and security.  In 2001, Dr. Okino accepted a position as a Senior Member of the Technical Staff in the Digital Signal Processing group and in 2007 accepted a position as the group supervisor of the Advanced Signal Processing Projects group at Jet Propulsion Laboratory. Among his recent accomplishments Dr. Okino supports the iNET Communication Link Standards Working Group, the Netcentric Systems Test MENSA project, the NASA ISS to CEV communication adaptor project, and the DARPA F6 project 


INFORMATION TYPOLOGY BRIEF DESCRIPTION ETHICAL CONCERNS ECONOMIC-SOCIAL INIBITORS  ORGANIZATIONAL STRUCTURE  a\ Organization Structure \(Detailed biographic information of the President, data and links to the website of regional centers b\ List of National and Regional boards and their members compositions c\ List of Administrative Managers, Contact person Etc   Privacy   Competitors  OFFICIAL DOCUMENTS FINANCIAL DATA  a\ Official regulations for the management of financial resources b\ Balance sheet            INTERNAL DOCUMENTS  a\ Detailed internal reports of the organization expenses and investment during previous fiscal years b\ Audios of the meetings of the National Board committee c\ All internal communications concerning administrative, organization, logistic, fiscal decisions made by every National committee Etc   Security & Privacy   Competitors   SERVICES  a\ Radio b\ Audio Books online c\ Newspapers online d\ Subscriptions to mailing lists Etc   Security   Competitors   PARTNERSHIPS  a\ Link, contact person and document of protocol agreements with other organizations that support blind people b\ Event and activities organized in collaboration with National and International Partners            FORUMS & CONTACTS   a\ Online forums \(e.g. Professional Education, Equal opportunities for blind students b\ List of  e-mail addresses of President and Headquarters offices c\ List of e-mail addresses of  UICI boards \(e.g. board of equal opportunities, board of sports and entertainment d\ List of e-mail addresses of local offices     Security    Competitors   Table 1: Information typologies available on the UICI website and their related ethical concerns or economic/social inhibitors          Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


nnnn ndnd nYn dZnYn yzyz yy zay azay          yzyz zz a nnnn ndndY 2112 1221    11 The equations for Ya and Za can then be substituted back into \(8 line, I, as shown in \(12 6 3naI KKK t 3 2112 2112 2112 1221 ,, nI KK t nnnn ndnd nnnn ndndX yzyz yy yzyz zz a         12 where I = Intersection line Taaa ZYX ,,=aK t = Linear parameter varying over intersection line   Vector along the intersection line Note that Xa has been shown here for generality and should be replaced with zero or whatever value previously assigned.  Equation \(12 determinant form as shown in \(13 3 11 22 11 22 22 11 


11 nI KK t nn nn nd nd nd nd X yz yz y y z z a      13 The heading estimate  can then be calculated geometrically using the relative displacement of the lateral components of the intersection line in the navigation frame x = East and y = North 14        min,max min,max,atan2  xx yy II II?  \(14 4. THEORETICAL PERFORMANCE USING SYNTHESIZED FEATURES Simulation Procedure This section will describe a simulation that was developed to assess the theoretical heading accuracy possible from ALS data and a perfect synthesized feature.  The first step of the simulation was to define the scanner parameters that reflect those likely to be used in a flight test: 30 lines per second, 10,000 pulses per second, and a  30 deg field of view.  ALS angles, times, and ranges were then simulated using these parameters.  A ground feature was synthesized by replacing the simulated ranges at the correct ground positions with those calculated according to the desired feature parameters \(i.e. plane equations synthesis and plane intersection procedure described in this section is summarized in the block diagram shown in Figure 8  Figure 8: Feature Synthesis and Plane Intersection Block Diagram Table 1: Simulation Parameters Parameter Value Aircraft Altitude 350, m Points in Surface 1 127 Points in Surface 2 248 Feature Length 25.79, m Feature Width 51.12, m The synthesized feature for this paper was based on airborne measurements of the size, location, and roof slope parameters of the AEC hangar at the Ohio University Airport.  For an aircraft at ~350m altitude in the simulation the synthesized hangar would be illuminated by 127 points 


on surface 1 and 248 points on surface 2 contained within a feature footprint with dimensions of 25.79 m long x 51.12 m wide as summarized in Table 1.  The flight profile from a January 2005 flight test [7] was used to establish the aircraft position and orientation over the two second flight duration needed to extract the simulated feature.  For the data collection phase, the hangar building was over flown while collecting GPS, INS, and ALS data.  In the synthesis phase actual GPS and INS information were used, but the ALS data was replaced with simulated data so that the ALS measurement errors could be controlled.  The heading derived from the measured ALS flight data will be compared with the simulation in the next section Two existing simulators were used to generate the ALS scan angle and ranges measurements [9].  An angle simulator was used to produce an array of angles and times that correspond to the laser scanner parameters.  A range simulator was used to produce range estimates by calculating the geometric difference between the ALS height and the terrain crossing \(simulated using a Digital Terrain Elevation Data \(DTED terrain crossing was found by increasing the range until it intersected the terrain and then iterated until the height accuracy was within some threshold \(1 ?m in this case The simulated range accuracy was determined from the height accuracy using the geometric relationship described in \(15 7 cos dHdR =  \(15 where dR = Range accuracy dH = Height accuracy ALS scan angle The top subplot of Figure 9 illustrates the range accuracy threshold used by the simulator \(determined from the height error and \(15 can be seen in the bottom subplot of Figure 9 because of the range iteration technique.  A 1 ?m height accuracy \(iteration threshold sufficiently small \(i.e. approximating true range influence the standard deviation of the final simulated range measurement \(i.e. true range + sensor noise better represent the capabilities of the true ALS, 25 mm of Additive White Gaussian Noise \(AWGN range measurement as shown in the top subplot of Figure 10 following with the Gaussian distribution function shown in the bottom subplot.  The DTED accuracy was not important for this analysis since only the range measurements to the simulated feature were required for the orientation calculations.  The range to the DTED merely provides terrain measurements for height contrast in the feature extraction algorithms  Figure 9: Range Error Distribution from the Simulator   Figure 10: Range Error + Noise Distribution with 25 mm Noise The feature was synthesized by calculating its height using the planar equations for surface 1 and surface 2 of the AEC hangar roof and the illuminated ground pulse positions as shown in \(16 2 22 11   BH BH   


 Surface Surface V V 16 where V = Vertical height H = Horizontal position vector   B = Plane parameters \(Surface 1 or Surface 2 Pulses that fall within a predetermined space \(represented by white space in Figure 11 points.  The DTED terrain heights at these positions were replaced with feature heights from a feature database prior to range simulation. The synthesized feature is shown in Figure 12  Figure 11: Synthesized Feature Footprint   Figure 12: Synthesized Feature Extraction Simulation Results for Typical System Performance Once a feature has been simulated, the feature extraction and plane intersection algorithms described in the previous section are used to determine the feature orientation.  The result of the least squares plane fit is shown in Figure 13 Notice that a gap is present near the ridgeline in Figure 13 This is due to the exclusion of the actual ridge point Because the peak of the roof contains a ridge cap, inclusion of these points in surface 1 or surface 2 would distort the results.  By using the plane intersection method, these few points \(16 hundred other points would remain in the plane.  The height residuals after subtracting the plane fit are shown in Figure 14.  As expected, these height residuals reflect the same accuracy  25 mm 1 shown previously in Figure 10.  The plane residuals shown in Figure 14 can be used to measure the range accuracy directly from the data.  The next section will describe how to use this information to predict the heading accuracy 8  Figure 13: Data Point Segregation and Plane Fit   Figure 14: Plane Fit Residuals The plane parameters, 1B  and 2B  are then used to find the equation of the intersection line as is shown in Figure 15 The intersection line contains heading information for comparison with some reference data.  In this case, the reference was derived from the same range simulation without noise  Figure 15: Plane / Plane Intersection Line Multiple repetitions of the synthesized feature extraction and plane intersection algorithm were used to determine the repeatable accuracy of the technique over 5000 Monte Carlo runs as shown by the block diagram in Figure 8.  The resulting 1? heading accuracy is shown in Figure 16 for  25 mm 1? ALS range accuracy  Figure 16: Heading Accuracy for 25 mm Range Error The error performance is summarized in Table 2.  The bias term is statistically zero \(as would be expected from the zero mean AWGN added to the simulated range heading error is better than a tenth of a degree Table 2: Heading Error Summary Error Type Magnitude Heading Error Mean 4.0778 x 10-4, deg 1? Heading Standard Deviation 0.0395, deg 0.69, mrad 


1? Heading Standard Deviation 0.0395, deg 0.69, mrad Sensitivity Analysis for Varying System Performance In order to understand how the heading error varies with parameters such as laser ranging accuracy, position accuracy, and heading bias, a sensitivity analysis was conducted.  For computational efficiency the number of repetitions at each parameter setting was reduced to 1000 For each set of 1000 repetitions of the plane intersection algorithm, the one parameter was linearly increased and its affect on the heading accuracy was assessed.  The range accuracy will be considered first, whereby the simulated range noise was increase in 46 steps from 0.001m until 0.31m.  The heading error is computed by comparing the noisy simulated feature intersection with a noise-free feature intersection.  Figure 17 shows the increase in heading standard deviation as the range standard deviation increases while Figure 18 shows the rate of change of the standard deviation per unit of range noise.  Although not shown here the heading resolution bias is zero-mean up to 15 cm and increases to almost 0.6 deg for 30 cm of range noise 9  Figure 17: Heading Error Standard Deviation  Figure 18: Heading Error Standard Deviation Growth Rate per Unit of Range Noise The results shown in Figure 17 illustrate a fairly linear increase in heading error growth for small range errors \(less than 12 cm rate of error growth is constant \(~1.53 deg/m range error.  The results shown in the previous section can be directly estimated using these figures.  If a range noise standard deviation of 25 mm is multiplied by a 1.53 deg/m error growth rate, the predicted heading accuracy would be 0.668 mrad.  This predicted accuracy is approximately the same as the accuracy found from simulation summarized in Table 2 The next parameter to be considered was a constant heading bias.  For the algorithm presented in this paper, the heading bias must be consistent with the position error as would be the case for an IMU operating in a dead reckoning mode Consequently, a heading error was simulated from 0 to 2 degrees in the inertial measurements and then the ENU position was corrupted using the erroneous heading in the direction cosines matrix.  Figure 19 illustrates the residual heading resolution error bias after the simulated heading bias has been removed.  Figure 20 illustrates the standard deviation of the heading resolution error  Figure 19: Heading Resolution Error Bias as a Function of Injected Heading Bias   Figure 20: Heading Resolution Error Standard Deviation as a Function of Injected Heading Bias Up to 2 deg of injected heading bias, there is not discernable effect on the heading resolution accuracy in either the mean or the standard deviation.  This illustrates that the algorithm can detect a constant heading bias without sensitivity to its magnitude The final parameter to be considered was position noise The position noise was varied linearly from 0 to 10 cm in 48 steps.  The response of the error mean to position noise is shown in Figure 21 while the response of the error standard deviation to position noise is shown in Figure 22 10  Figure 21: Heading Resolution Error Bias as a Function of Injected Position Noise   Figure 22: Heading Resolution Error Standard 


Figure 22: Heading Resolution Error Standard Deviation as a Function of Injected Position Noise As the injected position noise increases, the heading error bias remained nearly constant at the mm-level as would be expected since the injected position noise was AWGN Similarly, the heading error standard deviation increases as the injected position noise increases.  As with the laser range noise, the position noise will also determine the heading determination standard deviation.  Inertial positions are generally low noise \(better than 1 cm source is not felt to be as significant of a contributor \(&lt; 0.04 deg As an aside, it is interesting to note that this error will increase slightly as the aircraft height increases.  This is thought to be due to the reduced number of laser pulses that comprise each plane in the intersection equation.  For example, if the aircraft height increases from 350m to 400 m, the pulse count decreases to 68 pulses in surface 1 and 141 pulses in surface 2.  The heading accuracy will decrease by approximately 0.01 degrees 5. FLIGHT TEST PROOF OF CONCEPT  Figure 23: Flight Path over AEC Hangar The flight test data was collected in January 2005 and provided ALS measurements containing several features at the Ohio University Airport including the AEC hangar [7 The flight path over the hangar is shown in Figure 23.  The ALS settings during this flight test were more optimal for this application than for mapping since the gaps between scan lines was large compared to the gap between pulses in a line.  In September 2005, a static GPS survey was conducted \(relative to the KUNI GPS Continuously Operating Reference Station the AEC hangar.  The GPS survey provided an absolute reference to compare with the ALS measurements.  The lateral measurement accuracy of this survey was thought to be on the order of  20 cm because of antenna placement uncertainty Planes were fit to the ALS data from each side of the hangar roof as described previously as shown in Figure 24.  The plane fits are shown in lighter colors \(cyan and magenta than the measured data \(blue and red  Figure 24: Two Planes fit to the Data  Figure 24 is only intended to provide an overview since the plane fit residuals contain more information as shown in Figure 25.  The plane fit residuals indicate 9 cm of height accuracy \(1 11  Figure 25: Plane Fit Residuals The resulting plane intersection line was overlaid on the ALS measurements as shown in Figure 26.  The line was a close visual match, but when comparing the calculated line with the surveyed line, slight errors can be observed  Figure 26: Line Formed by Plane Intersection A closer examination of the two lines in the horizontal plane reveals their differences in the heading as shown in Figure 27  Figure 27: Intersection Line Comparison As mentioned previously, the measured plane-fit residuals provide an indication of the heading accuracy that could be expected from the real data if the survey was perfect.  Since the flight test residuals were within the linear region of the empirical error curves shown in Figure 17, the expected heading accuracy should be predictable.  Since the measured range accuracy is 9 cm \(from the plane-ft residuals heading accuracy is expected to be 0.1377 deg \(at 1.53 


deg/m to 2.4 mrad.  This accuracy is better than the typical heading alignment accuracy of a commercial navigation grade INS The actual heading angle of the two vectors is summarized in Table 3 Table 3: Hangar Ridge Vector Comparison Heading Survey 60.9436, deg Intersection 61.8772, deg Difference 0.9336, deg 16.29, mrad The importance of this proof of concept was to demonstrate the technique and to show that the accuracy can be predicted.  In this case there are several sources of uncertainty that might explain part of the performance degradation The reference survey introduced lateral antenna placement errors of  20cm.  The antenna placement uncertainty is expected to be the dominant error source in the measured data presented here.  Over a 25 m baseline, a  20 cm placement error will become a 0.916 deg pointing error worst case prediction using flight test data Further effort is needed to make a more accurate ground feature survey, but the concept of heading determination from ALS plane intersections has been demonstrated 6. CONCLUSIONS This paper presented results that leverage the accuracy and high number of \(i.e., oversampled measurements along with a priori surveyed features to determine airborne platform heading.  Plane fitting has the affect of averaging the oversampled measurements and can deliver mrad-level heading observations.  The ALS-derived heading information shown in this paper could be used to periodically calibrate tactical grade IMU heading biases while in flight, to perform an ALS / IMU calibration prior to a terrain aided landing, or simply to stabilize the inertial heading measurements when GPS is unavailable.  A simulation was presented to show the theoretical performance of heading determination from plane intersections using typical ALS parameters.  With a perfect survey, the heading accuracy was better than 1 mrad.  Using the same simulation software, a theoretical sensitivity analysis was conducted to determine the effect of range noise, heading bias, and position noise on the heading accuracy.  The heading standard deviation was shown to be a function of the range measurement and position standard deviations.  Inertial positions are generally low noise, so this error source was not deemed to be as significant of a contributor as the range noise.  The heading error magnitude did not change the performance in a noticeable way. This theoretical performance assessment was then compared with 12 flight test data.  While the flight test performance was not as good as expected, the error sources were justified.  Even with large survey uncertainties, the heading measurement performance was still within 1 deg.  If a better survey was available, the flight test results were expected to be accurate to within a few mrad Operationally, there are many things than can be done to improve these results.  This paper discussed flight over a single building.  Accuracy can potentially be improved by flying over multiple buildings of larger sizes and at lower altitudes or other types of features.  Additionally, the heading error growth rates could be predicted by tracking the heading change while flying over a building or between two buildings.  With careful attention to calibration and measurement accuracy, these heading alignments can potentially be made at a higher accuracy than a navigation grade INS can align its heading REFERENCES 1] Schenk, T  Modeling and recovering systematic 


1] Schenk, T  Modeling and recovering systematic errors in airborne laser scanners  Proceedings of the OEEPE workshop on Airborne Laserscanning and Interferometric SAR for Detailed Digital Elevation Models, OEEPE Publication no. 40, 2001, pp. 40-48  2] Crombaghs, M.J.E., Brugelmann, R., and d Min, E.J  On the Adjustment of Overlapping Strips of Laseraltimeter Height Data  International Archives of Photogrammetry and Remote Sensing, 2000, 33 B3/1   3] Dickman, J., and Uijt de Haag, M  Aircraft Heading Measurement Potential from an Airborne Laser Scanner Using Edge Extraction  Proceedings of the IEEE Aerospace Conference, March 3-10, 2007  4] Kraus, K., Pfeifer, N  Determination of Terrain Models in Wooded Areas with Airborne Laser Scanner Data  ISPRS Journal of Photogrammetry and Remote Sensing, 1998, 53, pp. 193-203  5] Maas, H., Vosselman, G  Two Algorithms for Extracting Building Models from Raw Laser Altimetry Data  ISPRS Journal of Photogrammetry and Remote Sensing, 1999, 54, pp. 193-203  6] Venable, D., Campbell, J., and Uijt de Haag, M  Feature Extraction and Separation in Airborne Laser Scanner Terrain Integrity Monitors  Digital Avionics Systems Conference, 2005  7] Campbell, J. L., M. Uijt de Haag, van Graas, F  Terrain Referenced Precision Approach Guidance   Proceedings of the ION National Technical Meeting 2005, San Diego, CA, January 24-26, 2005, pp. 643653  8] http://www.geom.umn.edu/software/download/COPYI NG.html, accessed May 2007  9] V. Nguyen, et. al  A Comparison of Line Extraction Algorithms Using 2D Laser Rangefinder for Indoor Mobile Robotics  Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, Aug. 2-6, 2005, pp. 1929-1934  10] Vadlamani, A  Preliminary Design and Analysis of a LIDAR Based Obstacle Detection System   Proceedings of the 24th Digital Avionics Systems Conference  13 BIOGRAPHY Jeff Dickman graduated from Ohio University in 2008 with a Ph.D in Electrical Engineering His research emphasizes navigation system integration and sensor stabilization.  He has also been involved with GPS landing system research and antenna design and measurement.  He is presently working on vision-aided navigation systems.  He is a member of the IEEE ION, AIAA, Eta Kappa Nu, and Tau Beta Pi  Maarten Uijt de Haag is an Associate Professor of Electrical Engineering at Ohio University and a Principal Investigator with the Ohio University 


Investigator with the Ohio University Avionics Engineering Center.  He earned his Ph.D. from Ohio University and holds a B.S.E.E. and M.S.E.E from Delft University of Technology located in the Netherlands.  He has been involved with GPS landing systems  research, advanced signal processing techniques for GPS receivers, GPS/INS integrated systems, and terrain-referenced navigation systems.  The latter includes the development of terrain data base integrity monitors as an enabling technology for Synthetic Vision Systems and autonomous aircraft operation      pre></body></html 


           7 C ur re nt ly n ot w or ki ng 2  0 0 5  0 0 8 0 04 0 04 0 0 1 0 16  0 16             8 C ur re nt ly in e du ca tio n2   0 6 


6 7  0 01 0 1 9 0 08  0 03 0 6 8 0 0 7 0 3 2           9 C ur re nt ly w or ki ng 2  0 2 8  0 03 0 18  0 1 1 0 0 3 0 64  0 00 0 1 4 0 8 9   


        10 E du ca tio n ac hi ev ed 3  3 5 7 1 5 2  0 04 0 02 0 2 1 0 1 2 0 16  0 02 0 1 6 0 13  0 0 6         11 D is pe ns ab le in co m e   


  21 0 9 2 72 7  0 14  0 0 1 0 09  0 08  0 2 0 0 00 0 0 4 0 18  0 1 6 0 0 1        In te rn et u sa ge                     


  12 A ct iv e in te rn et u sa ge 1  0 0 2 0 9 6 0 2 1 0 25  0 11  0 12  0 10  0 0 4 0 05  0 0 8 0 0 5 0 0 1 0 12        13 H ou rs o nl in e h ou rs 


rs   2 6 5 3 0 3  0 04 0 12  0 1 1 0 0 3 0 40  0 0 7 0 0 7 0 4 7 0 5 3 0 07  0 1 1 0 07       14 W illi ng ne ss to p ay 1  1 8 3 0 6 3  0 03 0 10 


10  0 07  0 08  0 0 2 0 0 4 0 0 1 0 01  0 00 0 0 5 0 14  0 04 0 05      G am e sp ec ifi c va ria bl es                      15 T en 


ur e w ee ks   2 8 2 3 5 2 0 2 6 0 31  0 0 9 0 01 0 12  0 0 4 0 02 0 0 9 0 0 9 0 07  0 02 0 13  0 08  0 0 4    16 C ro ss o ve r on o ffl in e 4  0 1 5 


5 1 1 1 0 1 9 0 11  0 13  0 18  0 2 0 0 1 4 0 0 7 0 14  0 1 1 0 0 4 0 08  0 15  0 0 5 0 01 0 07    17 S at is fa ct io n1   18 7 5 1 3 16  0 18  0 00 


00 0 44  0 52  0 1 4 0 0 3 0 02 0 07  0 0 9 0 1 4 0 10  0 08  0 0 6 0 09  0 0 1 0 13   18 C om m itm en t1  0 6 2 0 8 3 0 3 1 0 13  0 37  0 39  0 0 7 


7 0 0 6 0 02 0 03  0 0 4 0 1 3 0 14  0 17  0 0 5 0 09  0 07  0 19  0 58  S ou rc e O w n ca lc ul at io n N ot e N  1 3 89 o bs er va tio ns S ig ni fic an ce le ve ls 


ls  p  0 05 S D  S ta nd ar d de vi at io n 1 5 po in t L ik er t s ca le ra ng in g fro m 2 to 2  2 du m m y va ria bl e 3 o rd in al v ar ia bl e ra ng in g fro m v oc at io na l e du ca 


tio n to P h D 4 n um be r o f c on ta ct s   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


