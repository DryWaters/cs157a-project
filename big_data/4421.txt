Feature Selection Algorithm Based on Association Rules Mining Method  Jianwen Xie, Jianhua Wu, Qingquan Qian Department of Computer Science of Zhuhai College Jinan University Zhuhai, China kenxie.4@163.com, tjhwu@jnu.edu.cn, qqq@home.swjtu.edu.cn   Abstract This paper presents a novel feature selection algorithm based on the technique of mining association rules The main idea of the proposed algorithm is to find the features that are closely correlative with the class attribute by association rules mining method. Experimental results on several real and artificial data sets demonstrate that the proposed feature selection algorithm is able to obtain a smaller 
and satisfactory feature subset when compared with other existing feature selection algorithms. It is a new feature selection algorithm with vast of application prospect and research value Keywords-feature selection; machine learning; Apriori algorithm; association rules I   I NTRODUCTION  Feature selection is a common technique used in data preprocessing for pattern recognition, machine learning and data mining. It performs to remove redundant and noisy features from high-dimensional data sets and select a subset of relevant features for building robust learning models [1 With a limited amount of training data, excessive amount of 
features may cause a significant slowdown in the learning process, and may increase the risk of the learned classifier to over-fit the training data because irrelevant or redundant features confuse learning algorithms [2  T hus i t  c a n  be s e e n  that selection of well feature subset can bring great benefits to real application of inductive learning Feature selection is a classical problem in the field of statistics, and also an important topic in machine learning which is mainly studied from the perspective of statistics and information processing, and generally refers the lowdimensional data sets and assumes that the features are independent on each other [3-5  In  th e f i el d of  m ach in e  
learning, feature selection has great significance in text categorization, data mining, bioinformatics, computer vision information retrieval, and time series prediction. Due to the development of information acquirement and storage, the dimension and amount of data stored in databases for some real-world application gets high increasingly. The existing feature selection algorithms are severely challenged, and we urgently need some feature selection algorithms that adapt to mass data and also have good accuracy and running efficiency At presents, feature selection has attracted high attention of scholars in the field of machine learning. The main reasons can be summarized as following two points: \(1\the 
performances of some learning algorithms are affected by irrelevant and redundant features. Some researches indicate with the number of irrelevant features increasing, the amount of training data increases exponentially [6-7  C o nseq ue nt l y  feature selection not only reduces computational complexity and improves classification accuracy, but also helps finding easier algorithmic models; \(2\ mass data processing problems with high-dimension features appear continuously The development of data mining brings an urgent demand on large-size data processing, such as information retrieval gene analysis, etc. It is an experiential axiom that highdimensional feature space is not suitable for machine 
learning. “Dimension disaster” or “combination explosion is fatal for some learning algorithms. Therefore, feature selection is required for dimension reduction in case with mass data According to the mode of combining the learning algorithm, the existing feature selection strategies can be mainly categorized into three groups: embedded methods filter methods and wrapper methods [8-10  W i th r e s p ec t t o  the structure of embedded method, feature selection algorithm as a component is embedded into the learning algorithm, such as some classification algorithms that are implemented by adding and eliminating features. The decision tree algorithm is representative among the 
embedded models, which selects the feature with the greatest potential classification ability in each node for dividing subspaces. Filter methods evaluate the goodness of the feature subset by using the intrinsic characteristic of the data They are relatively computationally cheap, because they do not involve the learning algorithm. However, they also take the risk of selecting subsets of features which may not match the chosen learning algorithm [9   T h e ty p i c a l f i l t e r m e th o d s  are ReliefF algorithm, chi-squared 2 ture selection information gain \(IG\ based feature selection, gain ratio \(GR based feature selection, symmetrical uncertainty \(SU\ based feature selection, etc. Wrappers methods use a search 
algorithm to search through the space of possible features and directly evaluate each subset by running a learning algorithm on the subset. As a result of wrapping the learning algorithm as evaluation tool, they generally outperform filter methods in terms of accuracy, but are computationally expensive and have a risk of over fitting to the learning algorithm [10   T h e fe at ur e sel e c t i o ns m e t h o d s usi ng ge ne t i c search or greedy hill climbing search are representative wrappers methods 
2009 Eigth IEEE/ACIS International Conference on Computer and Information Science 978-0-7695-3641-5/09 $25.00 © 2009 IEEE DOI 10.1109/ICIS.2009.103 357 
2009 Eigth IEEE/ACIS International Conference on Computer and Information Science 978-0-7695-3641-5/09 $25.00 © 2009 IEEE DOI 10.1109/ICIS.2009.103 357 


In this paper, we makes a great effort to apply association rules mining techniques to solve feature selection problems and attempt to produce a small size feature subset that is acceptable for classification tasks II  M INING ASSOCIATION RULES  A  Association analysis Association analysis is a methodology that is useful for discovering interesting relationships hidden in large data set It was initially applied to market basket data for finding relationships existing among the sales of the products which can help retailer identify new opportunities for cross-selling their products to customers. The development of association analysis can be traced back to AIS algorithm [11  p r op os e d  by R. Agrawal in 1993. AIS algorithm doesn’t utilize the property of the frequent itemsets, which result in unnecessarily generating and counting too many candidate itemsets. Subsequently, R. Agrawal and R. Srikant introduced the property of the frequent itemsets and proposed Apriori algorithm [12 w h i c h i s  a m a i n t e c hni q u e widely used in commerce at present. Under the influence of Apriori, many researchers were attracted into the field of research on mining association rules, and proposed many improved algorithms, e.g. Apriori-Hybrid algorithm \(R Srikant et al\ [1  f u zzy as s o ci ati o n ru l e alg o r ith m C  M   Kuok et al\ [1 F P Gro w t h a l go r i t h m  J   H a n e t a l   et c B  Related terms 1  Itemset and support count Let I  i 1  i 2  i d be the set of all items, and T  t 1  t 2  t N be the most of all instances.  Each instance t i contains a subset of items chosen from I In association analysis, a collection of zero or more items is termed an itemset. If an itemset contains k items, it is called a k itemset. An important property of an itemset is its support count, which refers to the number of instances that contain a particular itemset. Mathematically, the support count  X for an itemset X can be stated as follows    iii XtXttT    where the symbol | · | denotes the number of elements in a set   2  Support and  confidence An association rule is an implication expression of the form X Y where X and Y are disjoint itemsets. The strength of an association rule can be measured in terms of its support and confidence [15 Su p por t  d e t e r m i n e s ho w o f t e n  a r u l e i s  applicable to a given data set, while confidence determines how frequently items in Y appear in instance that contains X  The formal definitions of these metrics are support     XY sX Y N     confidence     XY cX Y X      3  Lift Lift is a correlation measure that can be used to augment the support-confidence framework for association rules, and it is given as follows. The occurrence of itemset A is independent of the occurrence of itemset B if P  B  P  A  P  B erwise, itemsets A and B are dependent and correlated as events. This definition can easily be extended to more than two itemsets. The lift between the occurrence of A  and B can be measured by computing     PA B lift A B PAPB   If the lift  A  B less than 1, then the occurrence of A is negatively correlated with the occurrence of B If the lift  A  B  is greater than 1, then A and B are positively correlated meaning that the occurrence of one implies the occurrence of the other. If the lift  A  B is equal to 1, then A and B are independent and there is no correlation between them [1   C  Apriori algorithm Apriori algorithm is the first association rules mining algorithm that pioneered the use of support-based pruning to systematically control the exponential growth of candidate itemsets. It divides the procedure of mining association rules into two steps The first step is to iteratively find out all frequent itemsets whose supports are not less than the user-defined threshold. The pseudocode for the frequent itemsets generation step of the Apriori algorithm is shown in Fig. 1 Let C k denote the set of candidate k itemsets and F k  denote the set of frequent k itemsets. The frequent itemsets generation algorithm has two important characteristics: \(1\ It is a level-wise algorithm; i.e., mapped to the lattice structure it traverses the itemset lattice one level at a time, from frequent 1-itemsets to the maximum size of frequent itemsets 2\It uses a generate-and-test strategy for finding frequent itemsets. At each iteration, new candidate itemsets are generated from the frequent itemsets found in the previous iteration. The support for each candidate is then counted and tested against the minimum support threshold   The second step is to construct association rules that satisfy the user-defined minimum confidence by using frequent itemsets. Suppose one of the frequent itemsets is F k  F k   i 1  i 2  i k association rules with this itemsets can be generated in the following way: the first rule is i 1  i 2  i k 1   i k by checking the confidence this rule can be determined as interesting or not. Then other rules are generated by deleting the last items in the antecedent and inserting it to the consequence, further the confidences of the new rules are checked to determine the interestingness of them. Those processes iterated until the antecedent becomes 
358 
358 


empty  T h e p s e udo cod e  o f  t h e r u l e s ge ner a t i o n st ep  i n  Apriori is shown in Fig. 2  Al g orith m  Frequent itemsets generation in Apriori algorith m Method  1 k 1 2 F k  i  i I  i  N  minsup Find all frequent 1itemsets 3 Repeat  4 k  k 1 5 C k candidates generated from F k 1  6 For each instance t T  do  7 C t subset C k  t Identify all candidates that belong to t  8 For each candidate itemset c C t  do  9  c   c 1;      //Increment support count 10 End for  11 End for  12 F k  c  c C k  c  N  minsup Extract the frequent k itemsets 13 Until  F k    14:   Result F k   Figure 1  Pseudocode of frequent itemsets generation step in Apriori  Al g orith m Rules generation in Apriori algorith m  Method  1 For each frequent k itemset f k  k 2 do  2 H 1  i  i f k 1-item consequents of the rule 3:      call ap-genrules f k  H 1  4 End for   Procedure ap-genrules f k  H m  1 k  f k size of frequent itemset 2 m  H m size of rule consequent 3 If k  m 1 then  4 H m 1 m+1-item consequents generated from H m  5 For each h m 1 H m 1  do  6 conf   f k   f k h m 1  7 If  conf minconf  then  8:                  output:  the rule f k h m 1  h m 1  9 Else  10:                delete h m 1 from H m 1  11 End if  12 End for  13:       call ap-genrules f k  H m 1  14 End if  Figure 2  Pseudocode of rules generation step in Apriori III  F EATURE SELECTION ALGORITHM BASED ON ASSOCIATION RULES  Association analysis is used for discovering the relationship of things by mining association rules in large size database or data warehouse. This paper proposes a new feature selection method that integrating the theory of association analysis in data mining. Its idea is to find the features that are closely correlative with the class attribute by mining strong association rules with its consequence as class attribute in the training data set This algorithm mainly includes three phases: generating association rules set, constructing feature set, and testing feature set. In the first stage, we use Apriori algorithm to generate all the association rules whose consequence is class attribute and lift is greater than 1. While applying Apriori the parameter support and confidence are determined by user The second stage is to pick rules circularly from the rules set by a certain strategy, as well as adding the attribute that appears in the antecedent of the picked rule into the feature set. The final feature set will be the result of feature selection The last stage is a testing procedure in which all selected features are evaluated by learning algorithms. In this paper we use C4.5 [17 cl as s i f i c a ti on  ac cu ra cy as  a m eas u r e t o  evaluate the goodness of the feature subset. The main steps of the proposed feature selection algorithm are described detailedly as follows Procedure of feature selection based on association rules Step 0: Discretize the numerical attributes in the training set D  Step 1: Generate an association-rule set R by employing Apriori algorithm to the training set D  Step 2: Clean up the feature set F  Step 3: Select all the rules with consequences as the class attribute from the rule set R then construct a new rule set R class using these rules Step 4: Calculate lift of each rule in R class and then obtain a new rule set R class by deleting the rules whose lifts are less than 1.0 Step 5: Sort R class on their length in ascending order first then on their confidence in descending order, and last on support in descending order Step 6: Check the loop termination condition: If satisfied output the feature subset and exit. Otherwise, execute the procedure: pop the first rule r out of R class and then add all attributes that appear in the antecedent of r to F  Step 7: Remove the instances covered by r from D  Step 8: Recalculate the confidence and support of the left rules according to the processed training set D  Step 9: Sort R class on confidence in descending order first then on support in descending order. Jump to Step 6 Fig.3 shows pseudocode for the feature selection algorithm based on association rules and its main related procedures. The function apriori \(line 3\ is to apply standard Apriori algorithm we mention above to generate the association rules. To ensure the success of rules generation, a procedure discretize \(line 2\ is needed to perform the act of making numerical attributes discrete before executing Apriori. The procedure sort1 \(line 9\ and sort2 \(line 18\are two multiple key sorting, the former is to sort rules on length in ascending order first, then on confidence in descending order, and last on support in descending order; the latter is to sort rules on confidence in descending order first, then on support in descending order. The function pop \(line 13 performs the act that getting the top rule out of the rule set Procedure extract_features_in_antecedent \(line 14\ executes returning a feature set that includes all attributes that appear in the antecedent of the rule in parameter list. Function filer_train_set \(line 16\ performs simple deletion work that canceling all instances covered by the transferred rule, and return the deleted instances set to main program. Procedure reset \(line 17\ is a key step that correcting the support and 
359 
359 


confidence of the remainder rules in rule set by recalculating them according to the new training set after executing procedure filer_train_set  Al g orith m  feature selection based on association rules Input  D training set minsup minimum support threshold minconf minimum confidence threshold C class attribute  cyclenum cycle number threshold Output  Result feature subset Method 1 result    2:     discretize D Discretize numerical attributes 3 R apriori D  minsup  minconf Apriori algorithm 4 For each rule r R  do  5 If lift r 1  consequence r  C  then  6:               delete r from R  7 End if  8 End for  9:   sort1 R on length in ascending order first, then on confidence in descending order, and last on support in descending order 10 For int i 1 i  cyclenum  i  do  11 If  R    then break  12 Else  13 r pop R  14 F extract_features_in_antecedent r  15 result  F  16 D delete filer_train_set r  D  17:             reset D delete  R  18:         sort2 R on confidence in descending order first then on support in descending order 19 End Else  20 End If  21 End For  22:   output result  Procedure reset D delete  R  23 N  NumOfTrainData  NumOfTrainData is the number of instances before filtering 24 For each instance s D delete  do  25 N  N 1 26 For each rule r  B  R  do  27 If  r cover s then 28  A B    A B  1 29  A    A  1 30 Else If As  then  A    A  1 31 End If  32 End If  33:            sup r    A  N  34:            conf r    r    A  35 End for  36 End for  Figure 3  Pseudocode of feature selection algorithm based on association rules  IV  E XPERIMENTS AND ANALYSIS  As we can see, the cycle number is a significant factor in feature selection algorithm based on association rules to control the loop when to stop, which directly affects the final size of the feature subset. Too great cycle number would generate a large size feature subset, while too few may result in that representative features are not enough to reflect the original feature set. Thus, an appropriate cycle number parameter is required. According to some experimentation we make a simple conclusion regarding the cycle number Generally, a considerable result can be obtained when the cycle number is defined as 3 to 6. In this paper, for the training sets whose feature number is less than 10, the cycle number is set as 3 in order to guarantee the reduction in the size of feature subset; for those whose feature number is greater than or equal to 10, the cycle number is defined as 6 to gain well integrated effect This section is to check the validity and advantage of the proposed feature selection algorithm by experiments and compare it with other algorithms. The algorithms to be compared with would be genetic search based feature selection, ReliefF algorithm, chi-squared feature selection information gain based feature selection, gain ratio based feature selection and symmetrical uncertainty based feature selection. In the experiments, we use two kinds of data sets artificial and real data sets. The 10 real data sets come from UCI machine learning repository [1 w h i c h i s a co ll ec t i on of databases that are used for the empirical analysis of machine learning algorithms. The selected data sets possess various feature dimensions ranging from ones to tens include different attribute types \(numerical attributes only nominal attributes only and mixed type\and some of them even have missing data. We used 2 types of artificial data sets. One is waveform-40 [1 con s t r u c te d by B r eim a n  ea ch  class of which is generated from a combination of 2 of 3 base” waves, and the latter added 19 attributes of which are all noise attributes with mean 0 and variance 1. The other is data sets generated by a preliminary artificial data generation program [19 c o de d by Is a b e l l e G u y o n f o r a l i n e a r 2 cl as s  classification problem and its detailed data generation rule can be found in the website of NIPS 2001 workshop on variable and feature selection. In this paper, we generated 2 such data sets and named them artif-1 and artif-2 We executed various feature selection algorithms on each data set and applied C4.5 algorithm for classification after feature selection. The record of the size of selected subsets and the C4.5 classification accuracy based on 10-fold cross validation are shown in Tab. I and Tab. II respectively.  The results of experiments indicates that feature selection algorithm based on association rules can be used to deal with feature selection problem, and it is able to reduce the number of selected features significantly and produces a little improvement in the classification accuracy. It even uses only about one-third of the features required by other feature selection methods to arrive at the similar classification accuracy. The classification accuracies obtained by each feature selection method considered are almost equal and close. But with regard to the number of features selected by each algorithm, it is obvious that effect of reduction achieved by the proposed method is superior to others. In a word compared with 6 methods on 13 various data sets, feature selection algorithm based on association rules obviously 
360 
360 


present its advantage in the great reduction of the number of subset and guarantee the classification accuracy acceptable which can offer an efficient preprocess for the data mining pattern recognition and machine learning on the data sets   TABLE I  C4.5 CLASSIFICATION ACCURACY OF VARIOUS FEATURE SELECTION ALGORITHMS  Data set  Raw Association Rules Genetic Search ReliefF Chi-square Information Gain Gain Ratio Symmetrical Uncertainty waveform-40 68 69.3333 73.6667 68.3333 70.6667 70.6667 70.3333 70.6667 artif-1 77 84 88.5 79.5 79.5 79.5 81 79.5 artif-2 75 75.3333 72 77 77.3333 77 77.3333 77 ionosphere 91.453 91.7379 85.4701 90.8832 91.1681 91.1681 90.8832 91.1681 vote 96.3218 95.1724 96.3218 96.3218 96.3218 96.3218 96.3218 96.3218 wine 93.8202 92.1348 94.382 93.8202 93.8202 93.8202 93.8202 93.8202 horse-clonic 81.2709 81.2709 86.6221 81.2709 81.6054 80.9365 80.9365 80.9365 zoo 92.0792 88 93.0693 92.0792 92.0792 92.0792 92.0792 92.0792 breast cancer 94.5637 95.279 95.7082 94.5637 94.5637 94.5637 94.5637 94.5637 monk-1 82.2581 95.9677 95.9677 95.9677 82.2581 82.2581 82.2581 82.258 monk-2 56.213 57.3964 56.213 56.213 56.213 56.213 56.213 56.213 monk-3 93.4426 93.4426 93.4426 93.4426 93.4426 93.4426 93.4426 93.4426 mushroom 100 99.1137 100 100 100 100 100 100 Average 84.7248 86.014 87.0280 86.1074 85.3056 85.2285 85.3219 85.2285  TABLE II  N UMBER OF FEATURES OF THE SUBSET GENERATED BY VARIOUS FEATURE SELECTION ALGORITHMS  Data set  Raw Association Rules Genetic Search ReliefF Chi-square Information Gain Gain Ratio Symmetrical Uncertainty waveform-40 40 8 19 32 19 19 19 19 artif-1 17 6 5 15 9 9 9 9 artif-2 35 5 16 27 9 9 9 9 ionosphere 34 4 13 33 33 33 33 33 vote 16 8 7 16 16 16 16 16 wine 13 4 5 13 13 13 13 13 horse-clonic 27 5 6 25 22 21 21 21 zoo 17 4 7 17 17 17 17 17 breast cancer 9 7 6 9 9 9 9 9 monk-1 7 3 3 3 7 7 7 7 monk-2 7 5 6 6 7 7 7 7 monk-3 7 3 2 3 7 7 7 7 mushroom 22 6 7 21 21 21 21 21 Average 19.31 5.2 7.85 16.92 14.54 14.46 14.46 14.46   V  C ONCLUSIONS AND FUTURE WORK  Reducing redundant or irrelevant features can improve classification performance in most of cases and decrease cost of classification. In this work, we propose a new feature selection algorithm based on association rules. We designed an adaptive feature selection strategy embedding Apriori algorithm for finding association rules, which can discover the features that are related to the class attribute according to the theory of association analysis. The experimental results indicate that the feature selection algorithm based on association rules can yield a significant reduction in the 
361 
361 


number of features required for classification algorithm and simultaneously keep classification accuracy acceptable. That is, there are potential advantages of using the techniques of mining association rules to implement feature selection This work shows a novel approach to dealing with feature selection. Though it presents a remarkable advantage in reducing the feature number, due to using Apriori algorithm directly to mining association rules, the time complexity of the algorithm is quite high. To reduce the computational complexity and improve its running efficiency, our future work will be focused on simplifying present way to generate association rules with consequence as class attribute. One solution worthy of considering is to filter out the rules whose consequence is non-class attribute inside the Apriori algorithm. In addition, to improve the classification accuracy, our further work will be a research on wrapper feature selection algorithm based on association rules, which directly use the classification algorithm to evaluate the feature subsets inside the feature selection algorithm A CKNOWLEDGEMENTS  The authors thank the anonymous reviewers for their valuable comments and suggestions that helped us for improving our work. This work is funded by the National Natural Science Foundation of China under Grant 50878188  R EFERENCES  1  K. Fukunaga. Introduction to Statistical Pattern Recognition Academic Press, San Deigo, California, 1990 2  L. Yu, H. Liu. Efficiently handling feature redundancy in highdimensional data, in: Proceedings of The Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD-03\, Washington, DC, August, 2003, pp. 685-690 3  Lewis P M. The characteristic selection problem in recognition system. IRE Transaction on Information Theory, 1962, 8, pp.171178 4  Kittler J. Feature set search algorithms. Pattern Recognition and Signal Processing. 1978, 41-60 5  Cover T M. The best two independent measurements are not the two best. IEEE Transactions on System, Man and Cybernetics, 1974, 4\(1 pp.116-117 6  Langley P. Selection of relevant features in machine learning Proceedings of the AAAI Fall Symposium on Relevance. Menlo Park, CA.:AAAI Press, 1994,140-144 7  Jain A k, Zongker D. Feature selection: evaluation, application, and small sample performance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1997, 19\(2\, pp.153-158 8  M. Dash and H. Liu. Feature Selection for Classification. Intelligent Data Analysis, 1997, Vol. 1, No. 3, pp.131-156 9  Zexuan Zhu, Yew-Soon Ong, Manoranjan Dash. Wrapper-filter feature selection algorithm using a memetic framework. IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics: a publication of the IEEE Systems, Man, and Cybernetics Society 2007; 37\(1\: 70-6   R. Kohavi and G. H. John. Wrapper for Feature Subset Selection Artificial Intelligence, vol. 97, no. 1-2, pp.273-324, 1997   R. Agrawal, T. Imielinski, and A. Swami. Mining association rules between sets of items in large databases. In Proceedings of the ACM SIGMOD International Conference on Management of Data, pp.207216, Washington D.C., May 1993   Agrawal R, Srikant R. Fast Algorithms for Mining Association Rules VLDB. Sep 12-15 1994, Chile, pp.487-499   C.M. Kuok, A. Fu, M.H. Wong. Mining Fuzzy Association Rules in Databases. ACM SIGMOD Record, Volume 27, Number 1, March 1998, pp.41-46   Jiawei Han, Jian Pei, Yiwen Yin, Mining frequent patterns without candidate generation, Proceedings of the 2000 ACM SIGMOD international conference on Management of data, p.1-12, May 15-18 2000, Dallas, Texas, United States   Jiawei Han, Micheline Kamber. Data Mining: Concepts and Techniques, Second Edition. Morgan Kaufmann, 2006   Pang-Ning Tan, Michael Steinbach, Vipin Kumar. Introduction to data mining. Addison Wesley Longman, 2006   Quinlan, J. R. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 1993   Asuncion, A. & Newman, D.J. \(2007\. UCI Machine Learning Repository [http://www.ics.uci.edu/~mlearn/MLRepository.html   Irvine, CA: University of California, School of Information and Computer Science   Isabelle Guyon. Preliminary artificial data generation program for a linear 2 class classification problem  [http://clopinet.com/isabelle Projects/NIPS2001/#datas     
362 
362 


Looking at j S k i  S k j j we can write Pr j S k i  S k j j\000j S i  S j j  2 024 j 024 016 j S i  S j j  2 024  024 2 001 e 000 j S i  S j j 016 2 6 024 2 We use the fact that Chernoff bounds also holds for negatively dependent random variables Since the last bound is the weakest of the three the lemma follows We want GP i;j to hold with probability 1 000 o 1 n 2  whenever items i and j both have support   From Lemma 6 we get that this holds if j S i  S j j  C\024 log n  for some constant C depending on 016  If s  i j   2 024Lf     025 024L then j S i  S j j 025 2 024L  Hence a suf\002cient condition for the similarity is s  i j   024L  3 It remains to understand what is the probability that given a good permutation the pair sampler will take a number of samples for a given pair in each chunk k that leads to a 1 006 016  approximation of s  i j   We denote the latter event by GBS i;j;k  and want to bound the quantity Pr[GBS i;j;k j GP i;j   For this purpose consider the random variable X i;j;k de\002ned as the number of times we sample the pair f i j g in chunk k  Assuming GP i;j we have that over the randomness in the pair sampling algorithm E  X i;j;k  016;L   f  j S 1 i j  j S 1 j j  034 j S i  S j j  2 024  Since the occurrences of f i j g are independently sampled we can apply a Chernoff bound to conclude X i;j;k 016;L  E  X i;j;k   This leads to the conclusion Lemma 7 X i;j;k 016;L   f  j S 1 i j  j S 1 j j  034 j S i  S j j  2 024 016 Suppose that X i;j;k is close to its expectation Then we can use it with 1 006 016  approximations of j S i j and j S j j  to compute a 1 006 O  016  approximation of s  i j   This follows by analysis of the concrete functions f of the measures in Figure 1 A suf\002cient condition on the similarity needed for a 1 006 016  approximation of X i;j;k can be inferred from lemma 7 If s  i j  025 4 024L=\034 then E  X i;j;k  025 s  i j  034  4 024 025 L  So it suf\002ces to enforce s  i j  025 4 024L=\034  4 In order to have O  mb  pairs produced by the pair sampling phase we will choose 034  4 M  The expected number of pair samples from T t is less than j T t j 2 034 f      using that f is decreasing For all measures we consider f     024 1   so j T t j 2 034 f     024 j T t j 2 M 024 j T t j  It remains to understand which is the probability that a pair of items each with support at least   is not sampled by SampleCount Let the random variable X k represent the total number of samples taken in chunk k  The probability that a f i j g is sampled in chunk k is X i;j;k X k  so the probability that it does not get sampled in any evennumbered chunk is Q k 2  024 even  1 000 X i;j;k X k  s  We have seen before that X i;j;k 016;L 025 s  i j  034  4 024  For what concerns X k using a Chernoff bound we can get X k 016;L  E  X k  024 mb=\024  using the linear upper bound on the number of samples So we can compute Y k 2  024 even  1 000 X i;j;k X k  s 024 022 1 000 s  i j  034 024 2 024\015 i;j mb 023 s\024 2 024 022 1 000 s  i j  034 4 mb 023 s\024 2 024 C exp 024 000 s  i j  034 s\024 8 mb 025 In order for this probability to be small enough  O 1 m 2   we need to bound the similarity to s  i j  025 8 mbL s\024\034 5 To choose the best value of 024 we balance constraints 3 and 5 getting 024L   mbL s\024\034  024  r mbM s 6 From which we can deduce s  i j   L  max  r mbM s  M   7 V D ATASET CHARACTERISTICS We have computed for a selection of the datasets hosted on the FIMI web page 1  the ratios between the number of occurrences of single items and pairs in the 002rst half of the transactions and the total number of occurrences of the same items or pairs The values of some of this ratios the most representative are plotted 002gure 3 on the x axis items or pairs are spread evenly after they have been sorted according to their associated ratio The y axis represents the value of the ratios We have taken into account only items and pairs whose support is over 20 occurrences in the whole dataset in order to avoid the noise that could be generated by very rare elements As we can see the number of occurrences and co-occurrences are not so far from what would be expected under a random permutation of the transactions The synthetic data set behaves exactly like we would expect under a random permutation with the ratio being very close to 1  2 for almost all items/pairs This means that even for real data sets where the order of transactions is not random the sampling probabilities used in the pair sampling are reasonably close to the ones that would be obtained under the random permutation assumption VI C ONCLUSIONS We presented the 002rst study concerning the problem of mining similar pairs from a stream of transactions that does rely on the similarity of items and not only on the frequency of pairs A thorough experimental study of carefully engineered versions of the presented algorithm remains to be carried out 1 http://fimi.cs.helsinki.fi 
127 
127 


Figure 3 Plots of the ratios j S 1 i j  j S i j and j S 1 i  S 1 j j  j S i  S j j  R EFERENCES  E Cohen M Datar S Fujiwara A Gionis P Indyk R Motwani J D Ullman and C Yang Finding interesting associations without support pruning IEEE Trans Knowl Data Eng  vol 13 no 1 pp 64–78 2001  Y.-K Lee W.-Y Kim Y D Cai and J Han Comine Ef\002cient mining of correlated patterns in Proc IEEE International Conference on Data Mining ICDM 2003  IEEE Computer Society 2003 pp 581–584  E Omiecinski Alternative interest measures for mining associations in databases IEEE Trans Knowl Data Eng  vol 15 no 1 pp 57–69 2003  J Han and M Kamber Data Mining Concepts and Techniques 2nd edition  Morgan Kaufmann 2006  R Agrawal and R Srikant Fast algorithms for mining association rules in large databases in Proc International Conference On Very Large Data Bases VLDB 1994  Morgan Kaufmann Publishers Inc Sep 1994 pp 487–499  J Han J Pei Y Yin and R Mao Mining frequent patterns without candidate generation A frequent-pattern tree approach Data Min Knowl Discov  vol 8 no 1 pp 53 87 2004  N Jiang and L Gruenwald Research issues in data stream association rule mining SIGMOD Record  vol 35 no 1 pp 14–19 2006  Y Zhu and D Shasha Statstream Statistical monitoring of thousands of data streams in real time Morgan Kaufmann 2002 pp 358–369  G Cormode and S Muthukrishnan What's hot and what's not tracking most frequent items dynamically ACM Trans Database Syst  vol 30 no 1 pp 249–278 2005  E D Demaine A L  opez-Ortiz and J I Munro Frequency estimation of internet packet streams with limited space in Proc 10th Annual European Symposium Algorithms ESA 2002  2002 pp 348–360  J X Yu Z Chong H Lu Z Zhang and A Zhou A false negative approach to mining frequent itemsets from high speed transactional data streams Inf Sci  vol 176 no 14 pp 1986–2015 2006  A Chakrabarti G Cormode and A McGregor Robust lower bounds for communication and stream computation in STOC  C Dwork Ed ACM 2008 pp 641–650  S Guha and A McGregor Stream order and order statistics Quantile estimation in random-order streams SIAM Journal on Computing  vol 38 no 5 pp 2044–2059  N Alon Y Matias and M Szegedy The space complexity of approximating the frequency moments J Comput Syst Sci  vol 58 no 1 pp 137–147 1999  J Misra and D Gries Finding repeated elements Sci Comput Program  vol 2 no 2 pp 143–152 1982  R M Karp S Shenker and C H Papadimitriou A simple algorithm for 002nding frequent elements in streams and bags ACM Trans Database Syst  vol 28 pp 51–55 2003  M Charikar K Chen and M Farach-Colton Finding frequent items in data streams Theor Comput Sci  vol 312 no 1 pp 3–15 2004  A Campagna and R Pagh Finding associations and computing similarity via biased pair sampling in Proc 9th IEEE International Conference on Data Mining ICDM 2009    Finding associations and computing similarity via biased pair sampling Invited for publication in Knowledge an Information Systems  2010  E Kushilevitz and N Nisan Communication complexity  New York Cambridge University Press 1997  J S Vitter Random sampling with a reservoir ACM Trans Math Softw  vol 11 no 1 pp 37–57 1985  D Dubhashi and D Ranjan Balls and bins a study in negative dependence Random Struct Algorithms  vol 13 no 2 pp 99–124 1998 
128 
128 


Application of Chaotic Particle Swarm Optimization Algorithm in Chinese Documents Classification 763 Dekun Tan Qualitative Simulation Based on Ranked Hyperreals 767 Shusaku Tsumoto Association Action Rules and Action Paths Triggered by Meta-actions 772 Angelina A. Tzacheva and Zbigniew W. Ras Research and Prediction on Nonlinear Network Flow of Mobile Short Message Based on Neural Network 777 Nianhong Wan, Jiyi Wang, and Xuerong Wang Pattern Matching with Flexible Wildcards and Recurring Characters 782 Haiping Wang, Fei Xie, Xuegang Hu, Peipei Li, and Xindong Wu Supplier Selection Based on Rough Sets and Analytic Hierarchy Process 787 Lei Wang, Jun Ye, and Tianrui Li The Covering Upper Approximation by Subcovering 791 Shiping Wang, William Zhu, and Peiyong Zhu Stochastic Synchronization of Non-identical Genetic Networks with Time Delay 794 Zhengxia Wang and Guodong Liu An Extensible Workflow Modeling Model Based on Ontology 798 Zhenwu Wang Interval Type-2 Fuzzy PI Controllers: Why They are More Robust 802 Dongrui Wu and Woei Wan Tan Improved K-Modes Clustering Method Based on Chi-square Statistics 808 Runxiu Wu Decision Rule Acquisition Algorithm Based on Association-Characteristic Information Granular Computing 812 JianFeng Xu, Lan Liu, GuangZuo Zheng, and Yao Zhang Constructing a Fast Algorithm for Multi-label Classification with Support Vector Data Description 817 Jianhua Xu Knowledge Operations in Neighborhood System 822 Xibei Yang and Tsau Young Lin An Evaluation Method Based on Combinatorial Judgement Matrix 826 Jun Ye and Lei Wang Generating Algorithm of Approximate Decision Rules and its Applications 830 Wang Yun and Wu-Zhi Qiang Parameter Selection of Support Vector Regression Based on Particle Swarm Optimization 834 Hu Zhang, Min Wang, and Xin-han Huang T-type Pseudo-BCI Algebras and T-type Pseudo-BCI Filters 839 Xiaohong Zhang, Yinfeng Lu, and Xiaoyan Mao A Vehicle License Plate Recognition Method Based on Neural Network 845 Xing-Wang Zhang, Xian-gui Liu, and Jia Zhao Author Index 849 
xiii 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





