Abstract 
227Several recent P2P streaming systems have adopted 
ouali@encs.concordia.ca Brigitte Kerherve Computer Science Department Universite du Quebec A Montreal Montreal Canada kerherve.brigitte@uqam.ca Brigitte Jaumard CIISE Department Concordia University Montreal Canada bjaumard@ciise.concordia.ca 
Toward Improving Scheduling Strategies in Pull-based Live P2P Streaming Systems Anis Ouali ECE Department Concordia University Montreal Canada a  
mesh overlays to disseminate content to participating peers because this topology appears to be more resilient to churns To cope with inferred problems such as data redundancy these systems opt for data-driven content retrieval mechanisms pull mechanisms Each node has a list of neighbors with whom it periodically exchanges buffer information and requests content fragments One of the drawbacks of such a mechanism is that it does not offer intelligent selection of sending neighbors based on their characteristics This is mainly because the most important criteria used to select nodes is the content availability This can result in some performance degradation for instance due to peers that are sending very small or big parts of the needed data Resiliency may then be weakened and overhead increased 
In this paper we propose to study how the integration of some end nodes characteristics can improve the performance of a typical pull mechanism with random scheduling We show that the improvement in performance is signi\036cant enough despite the fact that the room for improvement is bounded by the limitations of the pull mechanism Hence we believe that the awareness of end characteristics is an important block upon which we can build more ef\036cient content retrieval mechanisms The gain in performance can also be ampli\036ed by proposing an alternative to the pull mechanism such as a combined pull-push approach 
I I NTRODUCTION Peer-to-Peer P2P architecture has been considered as an 
attractive and scalable solution for video streaming It does not require Internet infrastructure changes and it helps eliminating bandwidth bottleneck at the content source server Nevertheless P2P systems especially for live video streaming face many challenging issues such as ef\336cient and optimized overlay construction 2 3 4 content retrie v al mechanisms 6 7 etc In P2P streaming systems the content retrieval mechanism allows an end-client to receive data from other nodes using the constructed overlay This mechanism plays a predominant role in the video streaming process and its ef\336ciency greatly in\337uences the global performance Existing research works 9 5 6 ha v e demonstrated that a multi-sender 
mechanism achieves better results than a single-sender one In such a context the data-driven approach mainly pull mechanism is currently a very simple and suitable approach as it allows the receiver to cope with two main challenges eliminating/reducing data redundancy and recovering from data loss The pull mechanism is heavily based on the content availability at peers what content is available from which neighbor Thus a receiving node has to locate the missing content and to request it from the appropriate nodes However data-driven approach adds complexity to the receiver side because it is responsible for scheduling the data sent by its parents that is to decide which neighbor should 
send which data In for e xample the authors use b uf fer maps to advertise available content so that a node is able to request a given content from its neighbors These maps are periodically sent by each node to its neighbors This results in overhead and extra load on peers In addition there is more delay because of the three required steps for data retrieval advertise request and send Gridmedia  illustrates the bene\336t of a combined push-pull mechanism on observed playback delay A client registers itself with some neighbors that will systematically send data to him push and the pull is only used initially or when there is missing data To be ef\336cient a content retrieval mechanism has to make 
the best use of the upload bandwidth of peers and to minimize the overhead This mechanism also has an important impact on minimizing delay and maximizing throughput or resiliency Actually the content retrieval mechanism should be considered as the way to achieve ef\336cient utilization of the overlay network Indeed the overlay network satis\336es some quality requirements but it does not impose how it is used Different strategies can be designed for that purpose For example when maximizing throughput a node may have parents with available upload bandwidth but if they don\325t have enough content to send the observed throughput will be lower than expected Moreover when a receiver node assigns the most part of 
978-1-4244-2309-5/09/$25.00 \2512009 IEEE 
the content to only one of its parents it will be sensitive to the departure or failure of this parent and then may experience signi\336cant quality degradation Hence the system has a weak resiliency In addition when a speci\336c node for example one with high upload bandwidth is experiencing high load for content forwarding probably there will be higher delays due to more overhead processing delays and possible packets loss and retransmissions Hence we can further reduce delay by balancing load on peers as much as possible 1 


Content Balancing Content Balancing forwardable 2  Indeed if a peer has signi\336cantly more or newer content than the other parents then any receiving node may try to get most of the content from that node This results in high load on this parent and then it is impossible to achieve optimization goals We claim that the performance of a P2P live streaming system can be enhanced through the introduction of intelligent strategies for neighbor selection where we consider content availability at candidate neighbors along with lower layer characteristics such as upload bandwidth capacity RTT with the receiving node etc An intelligent selection of neighbors can improve signi\336cantly the performance of a P2P live streaming system without resulting in high overhead This intelligent selection can take place at two stages the selection of neighbors either to construct an initial set of neighbors or to improve it during the session and the selection of a subset of neighbors to request packets from In the rest of this paper we will focus on the latter stage and we will differentiate between neighbors and sending neighbors with the latter set being a subset of the former one The work being presented in this paper is part of a larger study on the impact of end-nodes characteristics such as endto-end RTTs upload bandwidth capacity and on the impact of P2P live streaming parameters streaming rate number of neighbors etc on the performance of the system specially the playback delay and the resiliency For more details we refer to 11 Finally we want to emphasize that this paper makes two contributions The 336rst one is to show that we can improve the performance of a P2P streaming system by taking into consideration some characteristics of participating nodes that are easy and not costly to retrieve We show this through comparisons between some variations of the pull mechanism despite the fact it has many limitations as stated earlier in this section The second contribution is to introduce a new method for sending neighbors selection that we call  This new method performs better than typical pull mechanisms and it is not tied exclusively to a pull mechanism since it can be adapted to other potential mechanisms We position this work as the 336rst step toward more intelligent overlay construction and content retrieval mechanisms This paper is organized as follows Section II describes some related work In section III we give an overview of the different pull variations being compared Section IV describes the simulation environment and assumptions along with comments on the results we obtained Finally we draw our conclusions and describe future work in Section V Fig 1 Global Overview of Existing and Proposed Approaches for Requesting II R ELATED WORK Scheduling strategies for content retrieval have been studied in previous works  such as random scheduling and rarest 336rst scheduling These techniques focus on the characteristics of data namely its freshness and its frequency among neighbors Recent works 14 15 propose more elaborated scheduling techniques that are based on what content a receiving peer is in need for Indeed they put more focus on peers than on the content being retrieved In the authors propose a randomized distributed algorithm which leads to an optimal streaming rate in a fully connected mesh topology Each peer proceeds by identifying its neighbors that are the most deprived from data and randomly selects missing packets to forward to them In the authors propose a deterministic chunk scheduling strategy based on an adaptive queuing mechanism This strategy achieves near optimal bandwidth utilization The strategy differentiates between non forwardable To be compatible with load balancing on peers and criteria optimization we believe that the content retrieval mechanism must guarantee that the content is distributed almost equally between parents of the same node i.e the parents have almost the same content We will refer to this as content F and NF Pull signals are used by peers to explicitly request content Such chunks will be marked F while chunks exchanged among neighbors are NF In addition of their complexity the mentioned scheduling strategies have a weak point in common that is the lack of awareness of peers characteristics such as end-to-end propagation delays rtt upload bandwidth availability experienced playback delay etc In most of existing works the peers characteristics at the network level network delays upload bandwidth are typically used to select the set of neighbors only Application layer characteristics such as the playback delay and the ratio of delivered content are usually not exploited for neighbors selection or scheduling Our approach is complementary to the above work Indeed we propose to incorporate end node characteristics at both the application and the network level in the scheduling strategy as depicted in Figure 1 Thus we are able to choose the best subset of neighbors to request data from III P ULL V ARIATIONS In this section we describe the pull mechanisms we propose to compare One of the typical pull mechanisms used in P2P 


u u u u u u u  003 concept we propose and includes upload bandwidth awareness We argue that to be able to optimize the selection of sending neighbors and what packets they have to send the neighbors need to have great similarity in their buffers Indeed if a packet is present only at one node or at a small number of nodes there is no room for optimization of the node that must send it To be able to evaluate the content balancing of a neighbors set we introduce two concepts The 336rst one is the variance of a set Recall that each node knows what is the highest sequence number of the packets he has received Let 20 is 336xed to in the sorted initial set The subset to be selected is the one that minimizes the function V 001 001 001  002 002 n i h h h  1  max min min 1 Let k D 212 212  0  i i u u u u 5 1   i n i n i n where 3 The second variation takes into account the maximum upload bandwidth of neighbors Thus a node will not request from its neighbor a volume of data that is higher than its maximum sending capacity However a node may receive more requests than it is able to satisfy  IV S IMULATION R ESULTS In this section we 336rst give a brief description of the simulation conditions Then we detail and comment the results we obtained Due to space constraints we do not provide a sensitivity analysis of the CB method The parameter 001 001 001 1 1 1 n  i i i i v v i i Content Balancing Pure Random a Pure Random PR b Pure Random with Sending Node Bandwidth Awareness PRwBA c Content Balancing CB A Simulation 002 v u h u v V k v D 002 F h u  u u i F 002 002 variation each neighbor can be requested to send a packet However in the CB variation each node maintains a greater number of neighbors but only a dynamic subset of nodes can be requested for packets For example we decide to have 15 potential transferring nodes For the random variations the number of neighbors is equal to 15 For the content balancing variation the number of neighbors is 20 However at each request period a subset containing only 15 nodes is constructed and the packet can only be requested from nodes in that subset The initial subset of 20 nodes is sorted in descending order of 1 2 3 4 5 6 We make use of the discrete event based simulator built by Zhang  al and detailed in  Although this simulator has many useful functionalities already implemented it suffers from the lack of documentation The source code of the simulator is available at In the simulator the streaming data is divided into packets of size 1250 bytes not including headers It is assumed that nodes have DSL connections and that bandwidth bottleneck is located on the edge of the network i.e end-nodes access networks T able I sho ws ho w bandwidth capacities are distributed by default The 336rst variation is purely and simply a random selection where the needed packet is requested from a random neighbor that has it  When 15 16 17 18 19 20 systems is based on random scheduling where each node maintains a list of neighbors from which it randomly chooses a neighbor to which it sends a request for the needed stream packet We call it Pure Random mechanism When a node does not satisfy a packet request it does not try anymore to resend the packet Random scheduling performs quite well under certain circumstances 16 Its simple design mak es it a suitable choice for available commercial P2P streaming systems In this paper we will show how this strategy can be improved even if we only take in account some small information about end node characteristics such as the maximum upload bandwidth and the content of the buffer maps of neighbors Thus we will compare three variations of the random pull mechanism These variations have the following common process from the full set or a subset of neighbors for each needed packet they construct the list of neighbors having this packet in their buffers i.e the potential sending nodes Then they request the needed packet from a random node from that list These variations differ in how the set of potential transferring nodes is selected be a node in the session with n potential sending neighbors receives the buffer maps of its neighbors it is able to compute their variance as The second concept to be introduced is a sort of indicator D of how recent are the packets to be requested This can be even seen as a coarse estimation of the playback delay with the argument that requesting recent packets leads to low playback delays Let obtained from the received buffer maps To be able to make a fair comparison with the random mechanism we compare the variations based on the same number of potential transferring nodes In the  Thus the possible subsets that will serve for requests are is the neighbor at the position will have an impact on the frequency of the requested packets among neighbors and on how recent they are The degree of improvement we would expect is limited by the fact that the scheduling algorithm is the same for all variations i.e random scheduling Indeed there is no intelligent mapping of a packet to be requested to a speci\336c neighbor We recall that the main difference between the compared variations lies in the set of neighbors they operate on be the highest sequence number of the packets received by be the highest sequence number known to The third variation is based on the These two quantities are combined linearly with a factor to obtain the following function  Increasing or decreasing 


14   15   5   2 Average Packet Delay 1 Average Playback Delay 3 Average Hop Number 384 kbps   128 kbps    39    49    600 packets  20 s   16139   14311    14     7580   7456    2     2     The playback delay is an estimation of the video latency a user may experience It is computed using a sample time interval in the streaming buffer This sample interval of 336xed size must satisfy the condition that within it the ratio of delivery of packets is greater than a threshold 0.99 The playback delay will then be determined by the position of the sample interval in the buffer The average playback delay is computed over all participating peers Table III describes the results we obtained for the average playback delays We note that the PR and PRwBA lead approximately to the same playback delay However both have an increase of around 14 relatively to the playback delay with content balancing This is due to the fact that with the CB variation we are making a balance between the TABLE III A VERAGE P LAYBACK D ELAYS WITH 100 NODES AND 15 SENDING NEIGHBORS   CB    CB    CB    Av Hop Number   1.5 Mbps   3 Mbps   768 kbps    1 Mbps   15    300kbps    500ms    16259   8552   TABLE I B ANDWIDTH C APACITIES OF E ND N ODES   TABLE II S OME D EFAULT P ARAMETERS OF THE S IMULATOR   TABLE IV A VERAGE P ACKET D ELAYS WITH 100 NODES AND 15 SENDING NEIGHBORS   TABLE V A VERAGE H OP N UMBER WITH 100 NODES AND 15 SENDING NEIGHBORS   3.57   3.41    B Results Av of 10 executions   Av of 10 executions   Av of 10 executions   Nodes Fraction   Request Period   Request Window Size   Comparison with CB   Comparison with CB   Comparison with CB   F 4 PR   PR   PR   We keep the default values used in the simulator for the parameters of the pull mechanism such as the size of the request window the request period the streaming rate etc See Table II for some examples Network end-to-end latencies rtt between nodes are real-world values taken by default from a latency matrix computed within the Meridian project For each pull variation we executed the simulation ten times and then for each comparison criteria we took the average over the ten obtained results We assume that there is no churn rate because it is not relevant enough for the present study We conducted our simulations with 100 nodes In the random variations the number of neighbors is equal to 15 while in the CB variation the number of neighbors is 20 but only 15 nodes can be requested for packets at each request period See Section III We limit each execution to a duration of 120 seconds The results shown are the ones computed for the last 10 seconds this behavior is by default in the simulator In this section for each point upon which we base our comparison we give a description for more details see and we comment the obtained results PRwBA   PRwBA   Av Packet Delay ms   PRwBA   The packet delay is the time that a packet takes from its 336rst creation at the source to reach a particular destination The average packet delay is the average of delays of all packets received by all nodes Through the results detailed in Table IV we note that there is a signi\336cant improvement obtained by considering the maximum upload bandwidths of nodes when making requests to them Indeed PR leads to a packet delay that is 15 and 13 higher than the CB and the PRwBA Hence in CB and PRwBA we are reducing the number of unsuccessful requests Indeed a packet will need typically less requests and thus less delay to be successfully received It is important to mention that a better average packet delay does not lead necessarily to a better playback delay We observe this for example between PR and PRwBA where the difference in packet delay is very noticeable while the difference in playback delay is not signi\336cant One main reason for such result lies in the fact that average packet delay is calculated relatively to individual packets that are successfully received by nodes while playback delay is calculated relatively to the packets received successfully in a certain time window of the buffer of each node In other words the packet delay does not consider the contiguity of received packets while the playback delay does Other reasons are left for future investigations Similarly to the average packet delay the average hop number is the average number of hops observed by a packet when successfully received by nodes in the session Table V shows that Random variations specially the Pure Random have a higher average hop number than the CB method which can be seen as consistent with the results obtained for the average packet delay However the value of the increase is different For example the Pure Random variation has 15 higher average packet request of recent packets and the number of requests needed to successfully receive a packet Thus the proposed function Download Rate   Upload Rate   Streaming Rate   Av Playback Delay ms   3.5   see Section III manages to choose a set of neighbors that have some of the most recent packets and to increase the probability that these packets are present in more than one node which increases the probability of request success Thus packet arrival delay and playback delay are reduced 


NOSSDAV 22202 Proceedings of the 12th international workshop on Network and operating systems support for digital audio and video Proceedings of IEEE INFOCOM 2003 Proceedings of the 13th annual ACM int\222l conf on Multimedia Proceedings of the International Workshop on Network and Operating Systems Support for Digital Audio and Video SIGMETRICS 22203 Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems NOSSDAV 22204 Proceedings of the 14th international workshop on Network and operating systems support for digital audio and video Proceedings of IEEE/ACM CCGRID 2008 Eighth International Workshop on Global and Peer-to-Peer Computing ATEC 22205 Proceedings of the annual conference on USENIX Annual Technical Conference Proceedings of the 28th International Conference on Distributed Computing Systems ICDCS 2008 IEEE Journal on Selected Areas in Communications SIGCOMM Comput Commun Rev CB    CB    CB    99.86   94.42    May 2006  S Banerjee S Lee B Bhattacharjee and A Srini v asan 322Resilient multicast using overlays,\323 in  2003  M Castro P  Druschel A.-M K ermarrec A Nandi A Ro wstron and A Singh 322Splitstream high-bandwidth multicast in cooperative environments,\323 in  2003 pp 298\320313  D K ostic A Rodriguez J Albrecht and A V ahdat 322Bullet high bandwidth data dissemination using an overlay mesh,\323 in  2003 pp 282\320297  X Zhang J Liu B Li and Y  Y um 322CoolStreaming/DONet A data-driven overlay network for peer-to-peer live media streaming,\323 in  pp 2102\3202111  M Zhang J.-G Luo L Zhao and S.-Q Y ang 322 A peer to-peer netw ork for live media streaming using a push-pull approach,\323 in  2005 pp 287\320290  N Magharei and R Rejaie 322Understanding mesh-based peer to-peer streaming,\323 in  2008  D K osti 253 c R Braud C Killian E Vandekieft J W Anderson A C Snoeren and A Vahdat 322Maintaining high bandwidth under dynamic network conditions,\323 in  2008  Y  Guo C Liang and Y  Liu 322 Aqcs A dapti v e queue-based chunk scheduling for p2p live streaming.\323 in  vol 25 no 9 pp 1678\3201694 2007  http://media.cs.tsinghua.edu.cn  vol 35 no 4 pp 85\32096 2005 5.18    IEEE 2007 pp 1073\3201081 A v ailable http://dblp.unitrier de/db/conf infocom/infocom2007.html#MassoulieTGR07  M Zhang Q Zhang L Sun and S Y ang 322Understanding the po wer of pull-based streaming protocol Can we do better?\323 zhangm June 2008  B W ong A Sli vkins and E G Sirer  322Meridian a lightweight network location service without virtual coordinates,\323 Networking 0   2   12     Av of 10 executions   Av of 10 executions   Av of 10 executions   Proceedings of the 9th ACM SOSP Symposium INFOCOM 4 Average Delivery Ratio  New York NY USA ACM Press 2003 pp 102\320113  R Rejaie and S Staf ford 322 A frame w ork for architecting peer to-peer receiver-driven overlays,\323 in 10   99.79   99.92    0     5.09   5.78    94.41   92.64    2     Proceedings of the 9th ACM SOSP Symposium 002 Overhead includes all control packets exchanged by nodes such as buffer map packets and request packets It is computed in percentage of the used upload bandwidth of nodes in the session Table VII shows that the overhead in content balancing variation is higher than in random variations This is due to the fact that in Content Balancing each node maintains more neighbors than in the random variations The CB overhead is still considered to be low Proc of IEEE INFOCOM 5  2008  321\321 322T rade-of fs in peer delay minimization for video s treaming in p2p systems,\323 in 5 Overhead 6 Average Upload Bandwidth Utilization TABLE VI A VERAGE D ELIVERY R AT I O W I T H 100 NODES AND 15 SENDING NEIGHBORS   TABLE VII A VERAGE O VERHEAD WITH 100 NODES AND 15 SENDING NEIGHBORS   Av Delivery Ratio    Comparison with CB   Comparison with CB   Av Upload BW Utilization    Comparison with CB    New York NY USA ACM Press 2002 pp 177\320186  D A T ran K A Hua and T  T  Do 322Zigzag An ef 336cient peer to-peer scheme for media streaming,\323 in  New York NY USA ACM Press 2004 pp 42\32047  A Ouali B Jaumard and G Heb uterne 322Delay balancing vs delay minimization for live video streaming in p2p systems,\323 Technical Report PR   PR   PR   PRwBA   PRwBA   delay while the average hop number is only 5 higher This con\336rms that estimating delays with hop numbers as it is often done in P2P systems is not accurate PRwBA   live P2P streaming system The content balancing strategy we proposed leads to the best results within the pull mechanism We believe that this strategy has the potential of amplifying the performance gain within a different mechanism such as a pull-push combined approach We are currently investigating different alternatives and what is their impact on throughput resiliency and playback delay in P2P live streaming systems R EFERENCES  V  N P admanabhan H J W ang P  A Chou and K Sripanidkulchai 322Distributing streaming media content using cooperative networking,\323 in  Berkeley CA USA USENIX Association 2005 pp 14\32014  C Liang Y  Guo and Y  Liu 322Is random scheduling s uf 336cient in p2p video streaming?\323 in Average upload bandwidth utilization is measured relatively to the aggregate upload bandwidth provided by all peers of the system It includes both data and control packets forwarding The conclusion that can be drawn from Table VIII is that although CB has the lowest bandwidth utilization and the highest overhead it achieves better performance as shown in previous results than the random variations Thus CB strategy makes a more ef\336cient use of the available upload bandwidth of the system Actually it generates a lower number of packets requests V C ONCLUSION AND F UTURE W ORK In this paper we have demonstrated that being aware of end node characteristics in the selection of sending neighbors can improve signi\336cantly the performance of a pull-based TABLE VIII A VERAGE U PLOAD B ANDWIDTH U TILIZATION WITH 100 NODES AND 15 SENDING NEIGHBORS   Overhead    The delivery ratio is the ratio of successfully received streaming data to the streaming rate By successfully we mean that the streaming packet was received before its playback deadline Table VI shows that the average delivery ratio for the three variations remains almost the same This is due mainly to the fact that they operate on the same number of neighbors The difference lies mainly in the selection of neighbors  ser Lecture Notes in Computer Science A Das H K Pung F B.-S Lee and L W.-C Wong Eds vol 4982 Springer 2008 pp 433\320444 A v ailable http dblp.uni-trier.de/db/conf/networking/networking2008.html#GuoLL08  L Massouli A T wigg C Gkantsidis and P  Rodriguez 322Randomized decentralized broadcasting algorithms.\323 in 


 The citation semantic similarity S cise of documents A and B can be summarized in the following formula    6  1  1 1    2 1      M i B N A N L i S cise S   7   2  2 2 2 2  1 1 1 2    iN R rtiN N rciN N i R rti N rci N i R rti N rci N Max L i S       where M Min N A  N B d  N = Max N A  N B each citation cluster has only one label S i L could only be 0 or 1; However, if we allow a citation cluster to have multiple labels, then S i L could be a fraction. For example, if one cluster C a1 in A has labels L a L b L c  and another cluster in C b1 in B has labels L a L b L d  then S i L will be 2/3 R ij   i=1 M; j=1 N is defined as follows  8        rj R ri R Max rj R ri R Min ij R    Assuming  A B N N  for every citation cluster in B we try to find a corresponding citation cluster in A which has the highest similarity using Formula \(7\, and calculate the meta-ratio using Formula \(8\hen we use Formula \(6\et the overall similarity of citation semantics of documents A and B see Fig. 3  4. Experimental results and discussion We downloaded Pubmed Central \(PMC access articles from e ch os e t w el v e cate g ories  corresponding to topical j ournals as our original clusters as shown in Table 2. We evaluated our results according to these clusters  Table 2. The document categories  Category Number of Files Behav Brain Funct 129 BMC Blood Disord 29 BMC Cardiovasc Disord 175 BMC Endocr Disord 38 BMC Neurol 161 BMC Oral Health 73 BMC Plant Biol 201 Cough 31 AIDS Res Ther 70 BMC Biochem 173 BMC Cancer 123 BMC Infect Dis 96  Based on these document sets, we generated multiple document sets \(see Table 3\or training and testing by the random selection of papers from each category listed in the second column. This data set was used to develop our citation semantics approach of document clustering and also to obtain appropriate weights for Formulas \(5\d \(3\or \(4\The document sets involved in each combination were used as the ground truth for evaluating our clustering results. Note that the combinations in Table 3 are only from the first eight categories of Table 2. The remaining four categories were held back to test the robustness of our approach  Table 3. Training Corpora  ID Document Categories Pa 1 BMC Blood Disord, BMC Cardiovasc Disord 40 2 Behav Brain Funct, BMC Blood Disord 40 3 BMC Blood Disord, BMC Neurol Cough 58 4 Behav Brain Funct, BMC Blood Disord, BMC Oral Health 60 5 BMC Neurol, BMC Oral Health 234 6 BMC Blood Disord, BMC Neurol BMC Oral Health, Cough 78 7 Behav Brain Funct,  BMC Blood Disord, BMC Neurol, BMC Oral Health, Cough 98 8 Behav Brain Funct,  BMC Blood Disord, BMC Endocr Disord, BMC Oral Health, BMC Plant Biol, Cough 119 9 Behav Brain Funct,  BMC Blood Disord, BMC Cardiovasc Disord, BMC Endocr Disord, BMC Oral Health BMC Plant Biol, Cough 139 10 Behav Brain Funct,  BMC Blood Disord, BMC Cardiovasc Disord, BMC Endocr Disord, BMC Neurol, BMC Oral Health, BMC Plant Biol, Cough 158  We used F-Measur e h a r m onic m e a n o f  precision and recall call ecision call ecision Re Pr Re Pr 2    aluate our results The first step is to compute the weights in Formula 5\amely W 3 W 4 W 5  and W 6 We first clustered each corpus of training data shown in Table 3 by using just the Citonomy measure. In other words, we performed K-Medoids clustering using the similarity measure specified in Formulas \(3\d \(4\with W 1 0 and W 2 1. Before clustering using different combinations of strategies, we did clustering by using title only, citation clustering only, and co-citations only. The average F-Measures are 29.9%, 59.3%, and Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


 35.8%, respectively. Intuitively, we expect to find a higher F-Measure on combining the three approaches Also, since using citation clustering results in the highest individual F-Measure among these three, a higher weight on citation clustering is likely to give better results The average F-Measures in Fig. 4 show that when the weights W 3 W 5 W 6 are biased towards citation clustering \(note high values of W 5 e get the highest values of the F-Measure, which coincides with our expectation. In other words, when we assign more weight to the semantic similarity between reference clusterings, we get better results of document clustering. Assigining more weight to the similarity between titles or to number of co-citations does not improve document clustering.  For example, the FMeasures of 10:1:1, 1:1:10 and 10:1:10 are all less than that of 1:1:1; the F-Measure of 1:10:10 is better than that of 1:1:1, but less than that of 1:10:1      Fig. 4. Citonomy Clustering F-Measure  Among the weight ratios 1:5:1, 1:10:1, 1:15:1 1:20:1, 1:25:1, 1:30:1, the first two settings work the best as shown in the following tables. From Table 4 and Fig. 4 we can see the F-Measures of 1:15:1, 1:20:1 1:25:1, and 1:30:1 are all the same. At this point, the values 15, 20, 25, and 30 for W5 are so large that the similarities based on titles and co-citations have been rendered inconsequential. To summarize, the contribution of similarity of titles and similarity based co-citations is small, but significant    Table 4. Combined Clustering F-Measure   W 1 W 2  1:0 1:1 5:1 10:1 15:1 20:1 25:1 30:1 Corpus ID 1 92.5 92.5 90 97.5 97.5 100 100 100 2 97.5 97.5 97.5 100 100 100 100 100 3 44.1 75.6 77.2 79 80.5 80.5 53.9 53.9 4 91.6 91.6 95 93.3 93.4 93.4 93.4 95 5 84.2 86.7 87.1 91.5 93.3 92.9 92.9 93.8 6 81.7 82.9 82.8 87.8 86.3 85.1 85.1 85.1 7 62.5 80.7 83.9 68.4 69.8 65.1 65.7 66.4 8 72.8 72.8 72.9 85.9 84.1 84.1 84.5 84.7 9 72.9 72.9 73.5 72.3 83.7 83.9 84.7 84 10 58.7 60.7 60.7 59.5 71.1 71.2 67.2 67.2 Average 75.9 81.4 82.1 83.5 86.0 85.6 82.7 83.0 Standard deviation 17.0 11.2 11.2 13.3 10.3 11.5 15.6 15.7  The second step is to compute weights given by either Formula \(3\ \(4\amely W 1 and W 2 We use Formula \(3\ this paper. Table 5 shows the results of using W 3 W 5 W 6 1:10:1 From Table 4 we can see, when using vector space measure only \(first column\ the average F-Measure of clustering is 75.9% with a standard deviation of 17 while the results with different combinations of vector space and Citonomy measure are shown in the second column onwards. While it is hard to come up with rigorous statistically significant proof of superior performance, Table 4 shows that inclusion of the Citonomy measure consistently results in a higher Fmeasure of at least 5%, with an average increase of 7.6%. Choosing relative weights W 1 W 2 15:1 \(fifth column\ults in the highest average F-Measure of 86% with a standard deviation of 10.3, which represents more than a 10% improvement over using VSM alone We get the best results for W 1 W 2 10:1, 15:1, 20:1 using W 3 W 5 W 6 1:5:1 with average F-Measure 86.6 85.9%, and 86% respectively \(other ratios not shown for lack of space\here are two possible reasons to explain why the optimal ratio is biased towards VSM  W 1   W 2 he first reason is that the vector space measure includes more complete information than Citonomy measure. This can be seen by the clustering result using either one alone - 75.9% versus 63.8% for this training data set. Another reason is that W 3 W 5 W 6 1:5:1 or 1:10:1 implies a high value for the Citonomy measure. To compensate for this, we need to increase the contribution of the VSM measure by assigning higher values for W 1 10, 15, or 20   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


 Fig. 5 F-Measure of clustering using combination of vector space and Citonomy with W 3 W 5 W 6 1:15:1   It is clear that the more weight we assign to vector space measure, the closer the result is to that of using vector space measure only; the more weight we assign to the Citonomy measure, the closer the result is that of using the Citonomy measure only. Fig. 5 confirms this by plotting the average F-Measure of different W 1 W 2 ratios with W 3 W 5 W 6 1:15:1 To test the weights we get from the training data we generated another set of corpora with same categories shown in Table 3 but with different papers and sizes. Table 5 shows the result of clusterings on these corpora   Table 5. F-Measure \(%\f clusterings on test data of same categories  W 1 W 2  W 3 W 5 W 6  1:0 10:1 1:5:1 15:1 1:5:1 20:1 1:5:1 15:1 1:10:1 Corpus ID 1 82.6 91.7 91.7 95.8 93.9 2 90.9 100 100 100 100 3 44.7 81.4 82.5 82.4 82.4 4 93.1 91.7 91.7 90.4 91.7 5 83 84.5 91.8 92.3 91.8 6 59.2 61.1 61.7 61.7 61.9 7 66.5 68.5 69.5 64.9 67.8 8 73.7 74 73.5 73 73 9 58.3 72.6 72.5 72.1 72.5 10 67.5 66.7 65.2 58.3 63.5 Average 72.0 79.2 80.0 79.1 79.9 Standard deviation 15.6 12.7 13.3 15.1 13.9   Table 4 shows that the clusterings using Formula 3\prove by around 10% compared to using only VSM. Table 5 shows us the consistency of the behavior of this semantic clustering using Formula \(3 with the best weights. To test the robustness of the approach against, we generated a noisy set of corpora by mixing the last four categories in Table 2 with the first eight categories which were used in training data Table 6 shows these corpora   Table 6. Test corpora with noise  ID Document Categories involved Paper 1 AIDS Res Ther, BMC Cardiovasc Disord 51 2 AIDS Res Ther, BMC Oral Health 40 3 AIDS Res Ther, BMC Cancer, BMC Infect Dis 64 4 AIDS Res Ther, Behav Brain Funct BMC Biochem, BMC Cancer 84 5 AIDS Res Ther, BMC Cancer, BMC Cardiovasc Disord, BMC Oral Health Cough 103 6 Behav Brain Funct, BMC Biochem BMC Cancer, BMC Infect Dis, BMC Oral Health, Cough 130 7 Behav Brain Funct,  BMC Biochem BMC Blood Disord, BMC Cancer BMC Infect Dis, BMC Oral Health Cough 150 8 AIDS Res Ther, Behav Brain Funct BMC Blood Disord, BMC Cancer BMC Endocr Disord, BMC Oral Health, BMC Plant Biol, Cough 170 9 AIDS Res Ther, Behav Brain Funct BMC Blood Disord, BMC Cancer BMC Cardiovasc Disord, BMC Neurol, BMC Oral Health, BMC Plant Biol, Cough 204 10 Behav Brain Funct,  BMC Blood Disord, BMC Cancer, BMC Cardiovasc Disord, BMC Endocr Disord, BMC Infect Dis, BMC Neurol BMC Oral Health, BMC Plant Biol Cough 221   Table 7 shows the results of clusterings on these test data with different categories from training data The improvements \(compared with VSM baseline in the 1:0 column\ our trained model are still around 10% despite the addition of documents from additional categories to serve as noise    Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


   Table 7. F-Measure \(%\f clusterings on test data with noise  W 1 W 2  W 3 W 5 W 6   1:0 10:1 1:5:1 15:1 1:5:1 20:1 1:5:1 15:1 1:10:1 Corpus ID 1 98 100 100 100 100 2 87.4 90 92.5 95 92.5 3 39.4 76 76 77.3 76 4 81.7 86.8 88 88 86.8 5 69.8 69.8 88.2 89.3 87.1 6 82.4 82.5 84.3 84.2 85.1 7 69.5 78.4 77.8 78.9 79.2 8 74.5 76.9 75.2 75.2 76.4 9 78.3 84.7 86.3 85.2 83.6 10 58.3 63.8 62.3 55.7 59.2 Average 73.9 80.9 83.1 82.9 82.6 Standard deviation 16.3 10.3 10.6 12.3 11.0  5. Conclusion and future work  We have presented a new approach to document clustering in this paper that utilizes citation semantics as a similarity measure. Using experimental analysis we have demonstrated that the accuracy of clustering on incorporation of citation semantics is improved by 5-10  compared to using only the vector space measure An important consideration in machine learning applications is the cost benef it ratio. Ideally, the gain in performance should be worth the complexity of the approach. The non-optimized Citonomy pipeline used in this paper took about twice the time that the use of VSM alone takes. Though the use of Citonomy entails the use of several additional steps, these operate on a small subset of the data space, can be fully automated and several of the steps can be executed in parallel Therefore, running time can be potentially improved especially with removal of the combinatorial steps that were included for comparative validation. Further since this is an independent modular addition to the process of clustering, and essentially a pre-processing step for generating a pair-wise distance measure for documents with citations, it can be combined with essentially any clustering approach. Given that the resulting improvement is substantial and the extra cost is a linear addition that can potentially be minimized we feel the cost benefit ratio is high enough for this approach to be useful in practice While the degree of improvement may depend on the choice of documents to be clustered,  we speculate that the present report represents a lower bound and the contribution of Citonomy is likely to be higher in differentiating between subsets of closely related documents, in contrast to the coarse problem of journal-wise classification evaluated here. This is because Citonomy may be seen as an attempt to include semantics that stem from a consideration of spatial locality of terms, and from explicit links specificied by the authors of documents. Both these factors should lend themselves to a higher degree of discernment, as they go beyond considering only the independent occurrence of terms in a pure VSM approach. Further, the approach may be used as a distance measure not just for research papers, but essentially any documents that embed citations to other documents Future work may include the use of this approach on diverse types of document sets to establish its robustness. It may also be refined by using underlying domain ontologies for systematic recognition of matching terms, rather than performing only string matching as in the present work. Running time can also be improved by automating connections between the various steps and by judicious use of the values for the parameters that yield the best performance for the training sets   6. References  1. Berkhin, P Survey of clustering data mining techniques 2002 2 G. Salton, A.W., C.S. Yang A vector space model for automatic indexing Communications of the ACM, 1975 3 C.J. van Rijsbergen, S.E.R.a.M.F.P New models in probabilistic information retrieval in British Library Research and Development Report 1980 British Library: London 4 A. McCallum, K.N., L. H. Ungar Efficient clustering of high-dimensional data sets with application to reference matching in the sixth ACM SIGKDD international conference on Knowledge discovery and data mining 2000 5 F. Beil, M.E., X. Xu Frequent term-based text clustering in SIGKDD 02 2002. Edmonton Alberta, Canada 6. I. Yoo, X.H Biomedical ontology MeSH improves document clustering qualify on MEDLINE articles A comparison study in 19th IEEE Symposium on computer-based medical systems 2006 7 A. Hotho, A.M., and A. Staab Text clustering based on good aggregations Kunstliche Intelligenz KI\02 16 4\: p. 48-54 8 C. Lee Giles, K.D.B., S. Lawrence CiteSeer: An automatic citation indexing System in the third ACM conference on digital libraries 1998 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


 9. Geyer, C.J Practical Markov chain Monte Carlo  Statistical Science, 1992 7 4\: p. 473-511 10. Hansmann, U Parallel tempering algorithm for conformational studies of biological molecules  ArXiv:physics, 1997 1 29 11. S. Chib, E.G Markov chain Monte Carlo simulation methods in econometrics  Econometrics Theory, 1996 12 3\409-431 12 MeSH cited; Available from http://www.ncbi.nlm.nih.gov/sites/entrez?db=mesh  13 J. Bakus, M.F.H., and M. Kamel A SOM-based document clustering using phrases in the 9th international conference on neural information proceeding 2006 14. Garfield, E Where was this paper cited Current Contents, 1994 15. Cameron, R.D A universal citation database as a catalyst for reform in scholarly communication  First Monday, 1997 2 4 16. Garfield, E The concept of citation indexing: A unique and innovative tool for navigating the research literature Current Contents, 1994 17 M. Day, T.T., C. Sung, C. Lee, S. Wu, C. Ong, W Hsu A knowledge based approach to citation extraction in International conference on information reuse and integration, IEEE 2005 18. Takasu, A Bibliographic attribute extraction from erroneous references based on statistical model in the 2003 joint ACM/IEEE conference on Digital Libraries 2003 19 H. Han, L.G., H. Zha Two supervised  learning approaches for  name disambiguation in author citations in the 2004 joint ACM/IEEE conference on Digital Libraries 2004 20 H. Han, H.Z., L. Giles Name disambiguation in author citations using a K-way spectral clustering method in The 2005 joint ACM/IEEE conference on Digital Libraries 2005 21 S. Teufel, A.S., D. Tidhar Automatic classification of citation function in the 2006 Conference on Empirical Methods in Natural Language Processing 2006 22 P. Nakov, A.S., M. Hearst Citances: Citation sentences for semantic analysis of  bioscience text  in the SIGIR 04 workshop on Search and Discovery in Bioinformatics 2004 23 Kaufman, L. and P.J. Rousseeuw Finding Groups in Data in An Introduction to Cluster Analysis  1990, John Wiley & Sons: Brussels, Belgium 24 van Rijsbergen, C.J., S.E. Robertson, and M.F Porter New models in probabilistic information retrieval 1980, London: British Library 25 Markov Clustering Algoritm cited; Available from: http://micans.org/mcl 26. Levenshtein, V.I Binary codes capable of correcting deletions, insertions, and reversals  Soviet Physics Doklady 1966 10 p. 707 710 27 PubMed Central cited; Available from http://www.pubmedcentral.nih gov/about/ftp.html 28. Rijsbergen, C.J.v Information retrieval 2 ed 1979, London: University of Glasgow   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


Dr Mohammad Mojarradi is an expert in developing mixedsignallmixed-voltage electronic circuits for drive and control of actuators power supplies sensors and micro-machined electromechanical interface applications He manages the development of the electronic circuits for the thermal cycle resistant electronics task for Mars Science Laboratory at JPL and leads a research consortium of universities developing electronics for extreme environments He received his Ph.D from UCLA in 1986 has twenty-five patents eighty publications and is a senior member of IEEE Prior to joining JPL he was an Associate Professor at Washington State University and the Manager of the mixed-voltagelspecialty integrated circuit group at the Xerox Microelectronics Center El Segundo CA Patrick McCluskey is an Associate Professor of Mechanical Engineering at the University of Maryland College Park where he is associated with the CALCE Electronic Products and Systems Center He has published extensively in the area of packaging and reliability Of electronics and microsystems for high power and extreme temperature environments including two books and numerous book chapters He has also served as general or technical chairman for numerous conferences in these research areas Dr McCluskey is an associate editor of the IEEE Transactions on Components and Packaging Technologies He received his Ph.D in Materials Science and Engineeringfrom Lehigh University Benjamin J Blalock received his B.S degree in electrical engineering from The University of Tennessee Knoxville in 1991 and the M.S and Ph.D degrees also in electrical engineering from the Georgia Institute of Technology Atlanta in 1993 and 1996 respectively He is currently an Associate Professor in the Department Of Electrical Engineering and Computer Science at The University of Tennessee where he directs the Integrated Circuits and Systems Laboratory ICASL His research focus there includes analog integrated circuit design for extreme environments both wide temperature and radiation on CMOS and SiGe BiCMOS multi-gate transistors and circuits on SOI analog circuit techniques for sub 100-nm CMOS mixed-signallmixed-voltage circuit design for systems-on-a-chip and biomicroelectronics Dr Blalock has co-authored over 80 refereed papers He has also worked as an analog IC design consultant Dr Blalock is a senior member of the IEEE Raymond J Garbos is VP and Chief Engineer of Aura Instrumentation Inc He is responsible for the development of advanced avionics concepts architectures and technologies for aerospace applications He has over thirty five years of circuit logic and system architecture design experience He was an Engineering Fellow for Sanders Associates 1984-8 5 Lockheed Martin 1986 2000 and BAE Systems 2001-06 He was the RL V Avionic IPT lead for Lockheed Martin and has participated in many Advanced Space Avionics Studies supporting MSFC He received BSEEIMSEM degrees from Northeastern University in 196811971 and a MAT degree from Rivier College in 2001 Leora Peltz is a scientist at Boeing Phantom Works responsible for the development and application of sensors and advanced avionics and insertion   into flight applications She has experience in evaluation and modeling of circuits and materials in extreme environments and the real-time operation of distributed architectures Leora received a PhD degree from Case Western Reserve University in 2003 Dr Michael Alles is a Research Associate Professor in the Electrical Engineering Department and the Program Manager for Commercial Systems with Vanderbilt University's Institute for Space and Defense Electronics ISDE where he works in the area of radiation effects in microelectronics He spent 2 years as a Business Unit Director for Silvaco International 10 years with Ibis Technology Corporation in product development and program management and 1 year with Harris Semiconductor as a design engineer Dr Alles has a strong background in semiconductor technology including manufacturing and metrology computer-aided design tools for semiconductor fabrication processes devices and integrated circuit design and expertise in modeling and simulation of radiation effects in semiconductor devices and circuits Dr Alles has served on the SIA ITRS starting materials working group since 1999 serving as chairman of the SOI materials group for the 2001 revision of ITRS and has been a reviewer for Transactions on Nuclear Science several times He has over 40 technicalltrade publications and 2 patents Dr Alles received his Ph.D in Electrical Engineering 12192 MS in Electrical Engineering 8190 11 


and his B.E in Electrical Engineering with a Double Major in Physics 5187 allfrom Vanderbilt University Dr Alles present research focus is in the application of advanced and emerging semiconductor technologies in radiation environments R Wayne Johnson is a Samuel Ginn Distinguished Professor of Electrical Engineering at Auburn University and Director of the Laboratory for Electronics Assembly and Packaging LEAP At Auburn he has established teaching and research laboratories for advanced packaging and electronics manufacturing His research efforts are focused on the materials processing and reliability aspects of electronics manufacturing Current projects include lead free electronics assembly mixed leadfree and Sn/Pb electronics assembly wafer level packaging flip chip assembly assembly of ultra thin Si die 30,um and electronics packagingfor extreme environments 2300C to 4850C Wayne was the 1991 President of the International Society for Hybrid Microelectronics ISHM He received the 1993 John A Wagnon Jr Technical Achievement Award from ISHM was named a Fellow of the Society in 1994 and received the Daniel C Hughes Memorial Award in 1997 He is a Fellow of the Institute of Electrical and Electronics Engineers IEEE and a member of the Surface Mount Technology Association SMTA and IPC Association Connecting Electronics Industries He is currently a member of the IEEE Components Packaging and Manufacturing Technology CPMT Society Board of Governors and Vice President of Publications He is also Editor-in-Chief of the IEEE Transactions on Electronics Packaging Manufacturing He has published 58 journal papers 140 conference papers 6 book chapters and co-edited one book on electronics packaging and electronics manufacturing He has also presented a number of invited talks Wayne holds one U.S patent Wayne received the B.E and MSc degrees in 1979 and 1982 from Vanderbilt University Nashville TN and the Ph.D degree in 1987 from Auburn University Auburn AL all in electrical engineering 12 


11 Xiao Yang L Haizhon S Choi 2004 Protection and Guarantee for Video and Voice Traffic in IEEE 802.1 le Wireless LANs INFOCOM 2004 Twenty-third Annual Joint Conference of the IEEE Computer and Communication Societies Volume 3 Issue 7-11 21522162 12 W Spearman J Martin A Distributed Adaptive Algorithm for QoS in 802.1 le Wireless Networks Proceedings of the 2007 International Symposium on Performance Evaluation of Computer and Telecommunication Systems SPECTS'07 San Diego CA July 2007 pp 379-386 13 Lim L.W Malik R Tan P.Y Apichaichalermwongse C Ando K Harada Y Panasonic Singapore Labs A QoS Scheduler for IEEE 802.1l e WLANs Consumer Communications and Networking Conference 2004 pp 199-204 14 V Vleeschauwer J Janssen G Petit and F Poppe Quality bounds for packetized voice transport Alcatel Tech Rep 1st Quarter 2000 15 ITU Series H Audiovisual and Multimedia Systems Infrastructure of audiovisual services Coding of moving video H.264 03/2005 International Telecommunication Union 12 BIOGRAPHY cooperative signal received his B.S Engineering from respectively processing and sensor networks He M.S and Ph.D degree in Electrical UCLA in 1993 1995 and 2000 Will Spearman is a Master's Candidate at Clemson University's School of Computing His work focuses on QoS in 802.cle and wireless networks His background includes a B.S in Psychology with a minor focus in Computer Science He currently is employed at Network Appliance Inc Dr Jim Martin is an Assistant Professor in the School of Computing at Clemson University His research interests include broadband access autonomic computing Internet protocols and network performance analysis He has received funding from NASA the Department of Justice BMW IBM and Cisco Dr Martin received his Ph.D from North Carolina State University Prior to joining Clemson Dr Martin was a consultant for Gartner and prior to that a software engineer for IBM Jay Gao joined the Jet Propulsion Laboratory in 2001 and is currently a senior research staff in the Communications Networks Group in the Telecommunication Research and Architecture section His research is primarily focused on space-based wireless communications and networking with emphasis on applications for the Mars Network He is currently conducting research for developing quality-of-service QoS protocols for the envisioned Interplanetary Network IPN and study optimization and protocols for deep space Ka-band communications He also supports requirements definition and interface design activities for the Department of Defense's Transformational Communications MilSatcom project and system engineering effort for NASA's Exploration System and Mission Directorate ESMD supporting the Constellation Program for return of human to the Moon and Mars Other research interests include optical-based sensorweb discrete event simulation of distributed communication/sensor systems energy efficient routing and self-organization algorithm for 13 


  14  Figure 5:  Site B1 Terrain horizon ma sk with 1 degree azimuth spacing  Figure 6:  Site B1 Terrain horizon mask with 1 de gree azimuth spacing, in e quatorial coordinates 


  15  Figure 7: Lunar South Pole Solar Illumination Yearly Average  Figure 8:  Lunar South Pole DTE Visibility Yearly Average 


  16  Figure 9: Lunar North Pole Sola r Illumination Yearly Average  Figure 10:  Lunar North Pole D TE Visibility Yearly Average 


  17  Figure 11: Site A1 Elevation Topography  Figure 12: Site A1 Yearly Average Solar Illumination and DTE visibility, Medium Resolution 


  18   Figure 13:  Site LB Te rrain Horizon Mask  Figure 14:  Theory and Computed values of Average Yearly Solar Illumination 


  19  Figure 15:  Theory and Computed values of Average Yearly DTE Communication  Figure 16:  Heliostat Mirror Design to Eliminate Cable Wrap 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


