978-1-4673-8437-7/16/$31.00 ©2016 IEEE  Classification of Microarray Gene Expression Data using Associative Classification Alagukumar, S Assistant Professor, Department of Computer Applications Ayya Nadar Janaki Ammal College Sivakasi - 626 124, India alagukumarmca@gmail.com Lawrance, R Director, Department of Computer Applications Ayya Nadar Janaki Ammal College Sivakasi - 626 124, India lawrancer@yahoo.com   Abstract Data mining is one of the interdisciplinary fields on the research aspect. Recently, association rules have become an important concept for classification purposes, called Associative Classification. It is a data mining technique that combines association rule mining and classification technique to build classification models. The core object ive of this work is to classify the gene expression using Associat ive Classification algorithm. Our research work has been formulated based on association rule and classification mining. The proposed algorithm contains four phases called as Statistical Gene filtering, Discretization, Class Association Rules and prediction or class assignment. The gene filtering phase is to find the differentially expressed genes and select the significant genes in the specific gene expression. The discretization phase is to convert the continuous values into discrete values and substitute the gene values into gene intervals. The role of the Class Association Rule phase is to generate the set of class association rules using closed frequent itemset and generate the classifier model. The last phase predicts the class from trained model using scoring function. The experimental results carried out by using breast cancer gene expression data which are available on the NCBI online biological database. It has been tested with two class and multi-class datasets and compared with the classical classification algorithms such as Linear Discriminant Analysis SVM, and Decision Tree. The performance of the classifier model is evaluated using Leave-One-Out-Cross Validation method.  The result of this work is used to the drug designer for the pathway analysis and disease treatment decisions  Index Terms Microarray, Data Mining, Associative Classification I  INTRODUCTION Now-a-days, large amount of data are being collected from biological data. Analyzing and extracting knowledge from huge data is difficult task. Data mining techniques have been used to get the useful knowledge from the large amount of data. Data mining techniques are being applied to all the types of domains that have rich data, such as database technology, machine learning, statistics, mathematics, cybernetics, genetics, neural networks, information retrieval, marketing and etc. In this paper the proposed methodology focuses on Associative Classification technique which is a new technique in the data mining area for classifying the data in the field of Bioinformatics such as gene expression and gene sequence to analyze the data. Data Mining is the science of finding new interesting patterns and relationship in massive amount of data. The data mining is  cess of extracting meaningful new correlations patterns, and trends by extracting from large amounts of data stored in databases Data mining is also sometimes called Knowledge Discovery in Databases [1 h e m icroarra y dat a set can be form of an M x N matrix of expression values, where the column represents genes g 1 g 2 g 3  n and the row represents different experimental samples s 1 s 2 s 3  n Each element D i,j dep icts th e express i on le v e l of t h e g e n e g i in the sample s j  The matrix usually contains large amount of data [2, 11  T h e figure 1 presents the microarray ge ne expression data matrix [2   Fig. 1. Gene Expression Data  Associative Classification algorithm operates in three phases, the discretization, the rule generation and the classifier building. The structural of this paper in organized as follows Chapter 2 represents the background of research done in various discretization and associative classification algorithms. Chapter 3 discusses the methodology and techniques that are used in our research. Chapter 4 presents the results of associative classification on breast cancer d ata set. Chapter 5 discusses the conclusion and future enhancements about the work done in the research 


  II  REVIEW  OF  RELATED  WORKS In order to do the survey various algorithms have been studied. Finding the affected and over expressed genes for the diseases and then classifies the gene expression using association rules and gene intervals. For that the researcher must know the basic knowledge of statistical methods, discretization techniques, associative classification algorithms, and cross validation methods Nagata, K et al have analyzed the toxic genomic and toxicological data using classification association rules. Then they reported that the statistical t-test have often been applied in microarray data analysis. Then they selected only the genes that were up-regulated \(fold change > 2 and p < 0.05\ownregulated \(fold change < 0.5 and p < 0.05\ in the groups with increased or not-decreased groups, respectively [3   Discretization is also forms of transformation, the gene expression are replaced by number of intervals or concept labels This simplifies the original data and produces the mining more effective and the resulting patterns are easy to understand Garcia, S et al have made a survey on discretization techniques. They pointed out discretization is a vital preprocessing techniques used in knowledge discovery and data mining task. The main objective is to transform a set of continuous attributes into discrete values by associating categorical values to intervals [4, 18  T ABLE 1 C OMPARATIVE A NALYSIS OF DISCRETIZATION A LGORITHMS  Algorithms Supervised Unsupervised Splitting Merging Time Complexity Equal Width Interval Bin Discretization Unsupervised Splitting O\(n Equal Frequency Bin Discretization Unsupervised Splitting O\(n K-Means Clustering Based Discretization Unsupervised Splitting O\(ikn  Entropy \(MDLP\Based Discretization Supervised Splitting O\(n log\(n ID3 Based Discretization Supervised Splitting O\(n log\(n Chi-Merge Based Discretization Supervised Merging O\(n log\(n Class Attribute Contingency Coefficient Supervised Splitting O\(n log\(n  From the comparative analyses of discretization techniques have been performed and the results are shown in table 1 Alves,R et al have discussed frequent pattern methods for gene association analysis. Frequent pattern mining has been implemented successfully in various data such as business and scientific data for discovering knowledge patterns, and is becoming a hopeful strategy in microarray gene expression analysis. Though, with dense datasets such as telecommunications, microarrays, etc., where there are many lengthy frequent patterns, these methods scale poorly and sometimes are unfeasible. This problem is due to the high computational cost used by apriori algorithm Then they pointed out that the tree based methods such as FP-growth might find some difficulties when dealing with dense or high dimensional datasets [5  Zakaria, W et al have proposed a column enumeration based algorithm using high confidence association rules for up and down expressed genes. Then they explained that the generating all frequent itemsets in dense datasets requires large memory [6    Creighton,C et al have demonstrated an algorithm for powerfully mining association rules from gene expression data Then they explained that the transcript and protein level in the gene expression profiling can be valuable to study of genes biological networks and cellular information [7   Snousy, A. M. B et al have discussed the classification techniques among nine decision tree algorithms. They explained the problems in cancer gene expression profiles, is to determine genes or groups of genes that are highly expressed in cancer cells but not in normal cells. Then they pointed out the classification techniques are used with microarray datasets to form classifier models that improve the diagnost ic of different diseases [8  Nagata, K et al have applied the Classification Based on Association algorithm on toxicological data. They have been discretized gene expressions and relative liver weights based on  test conducted between a compounds treated grouping and its corresponding control group. They generated Class Association Rules using the Apriori-TFP algorithm. The classification between CBA and linear discriminant analysis is showed that CBA is superior to LDA in terms of both predictive performances and interpretability [3    Refaeilzadeh, P et al have discussed the various cross validation methods. The cross validation defined as a statistical method of calculating and comparing learning algorithms by splitting data into two segments, one used to learn or train a classifier model and the other used to validate the model [9  From the literature study, it has been concluded that the statistical methods are required for a proper selection of differentially expressed genes and significant features for classification. The frequent microarray data sets generated by Apriori algorithm requires large memory [10, 11, 12  T h e FP growth might find some difficulty to dealing with dense or high dimensional datasets. For that, the researcher uses Apriori-Close 13 alg o rithm to preve n t t h i s problem b y  g etti n g bigg er  on l y  frequent closed itemsets. The over fitting problem occurs when the classification perform well on the training dataset and poorly on the test data set. This can be affected the accuracy of the classification. For instance, while SVM produces high classification accuracy, resulting c lassifiers are hard to interpret in non linear, hence complicated to use in order to extract relevant biological knowledge from it. The researchers avoid this problem by using associative classification which combines the association rules mining and classification techniques [14   


  Finally, the Leave-One-Out-Cross Validation method has been used for validating the predictive performance of the classifier model and it provides unbiased performance estimation   Fig.2. Methodology III  METHODOLOGY The recent advent of new technologies in biology, such as DNA microarray, produces a large volume of data to the researcher. As the same time it is not easy to derive accurate and understandable knowledge from the data. Hence, computer programs can automatically and accurately classify the gene expression and finding affected genes in gene expression data become a necessity. The proposed methodology of overall diagram has been shown in the figure 2  A  Phase I - Gene Filter The human body cell typically expresses large number of genes. The modify in the expression patterns of few genes is sufficient to change the subsequent phenotype of the cells, for that reason the hypothesis testing is used to understand the genetic difference between the cells. Differential gene expression allows for the correlation between changes in physiological and gene expression differences It has been utilized to elucidate differences between tissues cells, developmental stages, normal and diseased cells, response to drug treatments, among others. Microarray technology allows to the analysis of the expression patterns of ten thousands of genes, and simultaneously [2 T h e det e ct ion  of dif f e r enti ally  expressed genes can help to clarify the differences between two conditions or samples and identify genes that are both statistically important and have biological significance knowledge. In this paper, the gene filtering is the process of selecting the differentially expressed genes and statistically significant in the gene expression data using a statistical independent t-test method. The independent t-statistic supplies a consistent estimate of differential expression according to the following formula \(1          2    2 2 1 2 2 1 2 m n Y m m i Y X n n i X pool   Where represents is sample means and  is an unbiased estimator for standard deviation. First calculate the sums of squares and Correction factor to subtract to give n times normal sample variance also called the sum of squared residuals, the associated likelihood under the null hypothesis is evaluated by reference to the t- distribution with   degree of freedom The p-value is used to determine if a number is significantly dissimilar from normal to cancer ce ll. It is the chance of attained a value that deviates from the mean as much as the value being tested with the null hypothesis.  In this paper, the independent test used to find the differential expressed gene and selects the informative genes. First, the sample means for both groups and the standard deviation are calcu lated. Then the t-statistics value will be calculated and the p-value calculated from t-distribution  genes and statistically significant genes are selected or biological significance based on probability with degrees of freedom  and p < 0.5. The last step of gene filtering removes the uninformative genes   B  Phase II - Discretization Discretization method generally contains four steps such as sorting the continuous values of the attribute or feature to be discretized, calculating a cut point or splitting points, splitting intervals of continuous value, and lastly stopping at some point Discretization techniques can be divided into supervised or unsupervised depending upon the datasets. Discretization methods can also be grouped in terms of top down approach or bottom up approach    


  In the top down approach, the intervals are divided, in the bottom up approach, the intervals are merged when discretization process. First step of associative classification is discretizing the continuous attributes. In this paper the process of discretization of continuous features in the gene expression using an ID3 algorithm. ID3 algorithm is one of the supervised discretization algorithms for making class information theory of candidate to select boundary points for discretization. The two metrics of ID3 algorithm are entropy and splitting information. Entropy measure is used in various applications or domains. When used in discretization technique, entropy is calculated in a supervised manner. The entropy value is calculated using the formula \(3      Where  be the entropy information  denotes the probabilities of information ID3 algorithm is inducing best or correct split point in decision trees. ID3 employs a greedy search to find possible split points within the existing range of continuous values using the following formula \(4\. In the equation \(4\, m is the number of classes in pj, left and pj, right are probabilities that an instances, go to class j, is on the left or right side of a potential split-point T            The split-point with the lowest entropy is preferred to split the range into two categorical values or intervals, and the binary split is sustained with each part until a stopping criterion is satisfied In our proposed methodology, the entropy values are calculated for each gene attributes in gene expression. Similarly, the split values are calculated for each gene attributes in the gene expression data. Consequently, the gene expression values are replaced with the unique interval or period containing it  C  Phase III  Class Association Rules Association Rule Mining is one of the vital roles in data mining method as well as computational biology. Association rule mining is one of the unsupervised data mining techniques where produces reasonable rules [10 In  th is pr o p o se d  methodology the process of discovered closed frequent rule items using Apriori Close [13 al g o ri th m   af ter dis c o v e r ed cl ose d  frequent itemset, the class association rules has been formed Association Rules Let I = {i 1 i 2 i 3   m be a set of m elements is gene items. A  Y I and X Y The left side of the rule is called as antecedent and right side of the rule is said to be consequent. A set of gene items I= {i 1 i 2   n and a set of transaction or samples T = {t 1  t 2 t 3   m a subset of I, S I is said to be a frequent itemset [1   Frequent Closed Itemsets The Closed itemset C is called as frequent if the support of C in D is at least minimum support. Frequent Closed itemset C I  itemsets has two properties. The first property is all sub-closed itemsets of a frequent closed itemset are frequent. The second property is, all sub-closed item sets of an in frequent closed itemset are also being infrequent Class Association Rule Let D be the dataset, a set of records d, let I be the set of non class items or attributes in D, and Y be the set of class labels in D. A record  represents  or simply  if d has all the non-class items of Likewise, a record   represents  or simply  if d has the class label A rule is an association of the form  for a rule   is called an antecedent of the rule and is called a consequent of the rule. In our proposed methodology, process of Class Association Rule are contain two steps \(1\o produce the entire set of rules that satisfy the user defined support and confidence constraints or threshold value and \(2\o build a classifier from these class association rules A-Close Algorithm The frequent closed itemset method is different from existing algorithms and based on the closed itemset lattice for finding frequent itemsets. A closed itemset is a frequent itemset, where maximal set of items common to set of objects. It constructs a set of candidate frequent closed itemsets which decides the frequent closed itemsets using the min-support threshold. In every step one pass over the database is needed to build the set of candidate frequent closed itemsets [13          The A-Close Algorithm first initializes the set of generator itemsets in frequent closed 1-candidate with the items present in the data mining. Each iteration contains of three phases. First, the closure property used to determine the candidate frequent closed itemsets and support. Next, the infrequent itemsets pruned from the frequent closed candidate itemsets. Finally, generate the valid frequent closed itemsets. The Associative Classification identifies the closed frequent itemsets for each class  by using gene intervals. Those itemsets generate the set of rules  per class    The antecedence of every rule is the conjunction of the gene expression and consequent is the membership of class Finally form the class association rules classifier model D  Phase IV - Prediction An association rule produces interesting relationships between gene expressions and classes or labels. The generated complete set of rules that satisfy the user specified minimum 


  support and minimum confidence constraints and to build a classifier from these class association rules. The last step in the life cycle of associative classification is prediction or class assignment techniques. In our proposed methodology, the process of prediction or class assignment for test data, the researcher used the score based prediction [14 L e t  be an unknown sample and let   be the association rules for class For class association rules calculates how many rules are satisfied in each rule and assigns to that class  whose rules are maximally satisfied using scoring function\(9   The class Class Association Rules evaluates for each rule by using the function      The phase IV reads the unknown discretized sample or test set and association rules for the class. The score based function evaluates the number of rules are s atisfied, even partially in each rule. Then the score values are computed for each rule. Finally the score value is assigned to the class of the maximally satisfy the test object. If no rules are applicable to the test object, the default class assigned to that test object E  Cross Validation Cross Validation is a statistical approach of evaluating predictive performances and comparing classification model by splitting data into two segments: one used to train a model and the other used to validate the model. In this paper leave one out cross validation \(LOOCV\ has been implemented to validate the classifier model [14       For a gene expression data dataset with N samples perform N Experiments, for each experiment use N-1 samples for training and the remaining samples are testing F  Classification Accuracy The effective classification model is evaluated with number of exact and wrong classifications in each possible value of the variable being classified in the form of confusion matrix      IV  RESULTS  AND  DISCUSSION All experiments are performed on a computer with a CPU processor clock rate of 2.8GHz and 2GB of main memory. The algorithm implemented in R statistical programming language with 3.1.1 version. Each experiment performed various times and the best of them is taken for the results.  Microarray gene expressions are used to assess the performance of the proposed method. The microarray breast cancer2 gene expression data has been collected from the standard database [14  T h e m icr o a r r ay  gene expression data description has been downloaded from the National Centre for Bioinformatics \(NCBI\ [15  Bre a st c a nc er  2  data set has been used for the analysis of algorithm. The original data set consists of 60 samples 32 cancer recurred patient and 28 disease free patient and 22575 gene expression conditions. The following table 2 depicts the sample microarray gene expression values T ABLE 2 G ENE E XPRESSION D ATA  Patient ID class EST AK026789 MLLT1  GSM22449 cancer 0.09 0.42 0.57  GSM22450 cancer 0.91 0.12 0.59  GSM22451 cancer 0.82 0.15 0.62  GSM22452 cancer 0.13 0.25 0.79  GSM22453 normal 0.42 0.27 0.53  GSM22454 cancer 0.49 0.1 0.28  GSM22455 cancer 0.38 0.02 0.5  GSM22456 cancer 0.56 0.27 0.53  GSM22457 cancer 0.48 0.24 0.77  GSM22458 normal 0.19 0.61 0.58  GSM22459 cancer 0.64 0.19 0.84  GSM22460 cancer 1.09 0.15 1.23  GSM22461 cancer 0.81 0.17 0.85  GSM22462 cancer 1 0.16 0.97  GSM22463 cancer 0.45 0.19 0.9  GSM22464 cancer 0.05 0.42 0.74  GSM22465 normal 0.13 0.35 0.41  GSM22466 normal 0.04 0.23 0.37         The independent t-test provides the standardized estimate of differential expression using the t-value and p-value. The p-value is used to determine if a numbe r is significantly different from normal genes. In proposed system, calculates these steps in an automated using implementing t-test statistical method First calculate the t-statistics then Calculate p value from t  value using t-distribution value. Finally, select differentially expressed and statistically significant genes on probability with degree of freedom and p < 0.05. There are 2701 genes are selected or filtered from 22575 genes. The selected significant genes and differentially expresse d genes with t-statistics values and p-values according to the p<0.05 criterion. In our proposed methodology, the entropy values are calculated for each gene attributes in gene expression. Similarly, the split values are calculated for each gene attributes in the gene expression data Consequently, the gene expression values are replaced with the unique interval containing it The following table 3 represents the discretized gene expression values. The following table 4 depicts the gene expression values are substituted by intervals. After the discretization the gene expression data are converted into transaction data and generate the class association rules 


  T ABLE 3 D ISCRETIZED G ENE E XPRESSION V ALUES  Patient class EST AK026789 MLLT1  GSM22449 cancer 14 18 11  GSM22450 cancer 1 1 13  GSM22451 cancer 1 1 15  GSM22452 cancer 13 13 20  GSM22453 normal 9 14 8  GSM22454 cancer 5 5 4  GSM22455 cancer 11 3 8  GSM22456 cancer 3 14 8  GSM22457 cancer 5 12 20  GSM22458 normal 19 19 12  GSM22459 cancer 3 1 20  GSM22460 cancer 1 9 22  GSM22461 cancer 1 1 20  GSM22462 cancer 1 1 20  GSM22463 cancer 8 1 20  GSM22464 cancer 18 18 18  GSM22465 normal 13 15 5  GSM22466 normal 17 11 5        T ABLE 4 G ENE E XPRESSION V ALUES S UBSTITUTED BY G ENE I NTERVALS  Patient class EST AK026789  GSM22449 cancer 0.11 -0.08 0.415 0.435   GSM22450 cancer Inf -0.785 Inf -0.04   GSM22451 cancer Inf -0.785 Inf -0.04   GSM22452 cancer 0.145 -0.110 0.245 0.260   GSM22453 normal 0.445 -0.415 0.260 0.295   GSM22454 cancer 0.510 -0.475 0.060 0.105   GSM22455 cancer 0.395 -0.345 0.025  0.000   GSM22456 cancer 0.760 -0.535 0.260 0.295   GSM22457 cancer 0.510 -0.475 0.235 0.245   GSM22458 normal 0.12  Inf 0.435   Inf   GSM22459 cancer 0.760 -0.535 Inf -0.04   GSM22460 cancer Inf -0.785 0.145 0.160   GSM22461 cancer Inf -0.785 Inf -0.04   GSM22462 cancer Inf -0.785 Inf -0.04   GSM22463 cancer 0.455 -0.445 Inf -0.04   GSM22464 cancer 0.045 0.120 0.415 0.435   GSM22465 normal Inf -0.785 0.295 0.390   GSM22466 normal 0.015  0.045 0.175 0.235    Each class generates the number of class association rules which are shows in figure 3  Fig. 3. Number of Class Association Rules Generated The Class-Association-Rules are generated based on closed frequent itemset with support of 50% and confidence of 100 The generated rules are used for classification. The antecedence of each rule is the consequent is the membership of class c. Then the class association rules classifier model is constructed which is represented in figure 4   Fig. 4. Class Association Rules Model  The last step of associative classification is prediction or class assignment techniques. The last phase of classification reads the unknown discretized sample or test set and association rules for the class. The score based function evaluates the how many rules are satisfied, even partially in ea ch rule. Then compute the score values for each rule. Finally the score value is assigned the class of the maximally satisfy to the test object. In our proposed methodology, the process of prediction or class assignment for test data, the researcher implemented the score based prediction For testing data, label removed breast cancer data and classification performance tested via leave one out cross validation method. The following table 5 presents the class prediction results using score based values. It has been analyzed that the use of gene intervals improves the classification accuracy as well as under or over expres sion and generates significant rules that are capable to find individual variation 


  For example, ABCC11 and IL1R2 genes presented in figure 4 these genes are under expression and all these genes are related to breast cancer [16, 17   T ABLE 5 C LASS PREDICTION USING SCORE BASED PREDICTION METHOD  Scores Predicted Class Real Class 0.16306184896678674  0.11787963311752411 cancer cancer 0.1224552572850139  0.0397661453730037 cancer cancer 0.44353396669274453  0.07036692343792059 cancer cancer 0.188646764482539  0.21119206413432903 normal cancer 0.1522962941752123  0.2154521600658755 normal normal 0.4197666787808435  0.30694558869809824 cancer cancer 0.48534506530073757  0.14346844132709374 cancer cancer 0.40650021365795297  0.13314684682620015 cancer cancer 0.3590791197979314  0.18677205116904233 cancer cancer 0.11816739414034069  0.40892631973628973 normal normal 0.48013078686644045  0.27622136998949687 cancer cancer 0.4138426396015541  0.14924947444406078 cancer cancer 0.34798041698293924  0.1273183791711423 cancer cancer 0.40543790078076125  0.04401089031827933 cancer cancer 0.20258661595296582  0.05767396607869007 cancer cancer 0.4627835656463016  0.4517425343821467 cancer cancer 0.1619104728401953  0.49982055846561274 normal normal 0.17006433387436767  0.13532194623044094 cancer normal    A  Comparative Analysis The proposed associative classification algorithm compared with bench mark classification algorithms viz., Support Vector Machine, Linear Discriminant Analysis, Decision Tree algorithms. The proposed algorithm obtained best result and reduce the time complexity to compare with classical rule generation and classification algorithm  T ABLE 6  E XPLORATION OF VARIOUS SETTINGS IN RULE GENERATION  Min sup  Min conf  Number of Rules Using AClose Total Time for AClose Second Number of Rules Using Apriori Total Time for Apriori Second 10 50 2636 8.66 41479 165.83 10 80 2633 8.78 41470 169.74 10 90 2632 8.83 41469 163.66 10 100 2632 8.88 41469 169.54 8 90 3086 10.34 45970 187.14 20 90 600 0.68 1250 0.87 30 90 326 0.17 982 0.86 40 90 212 0.10 780 0.96 50 90 126 0.07 370 0.86 50 50 126 0.09 370 0.89  The above table 6 shows comparative analysis proposed closed frequent itemset algorithm with apriori algorithm and shows that the of number of rules generation. The following figure 6 shows that comparative analysis of number of rules    Fig. 6. Comparative Analyses on Rules  The figure 7 depicts the time taken to generate number of rules    Fig. 7. Comparative analysis of time taken  From that comparative analysis the proposed classification algorithms provides better accuracy then all bench mark or classical classification algorithms. The following figure 8 shows that accuracy and error rates of Associative Classification on microarray gene expression data using gene intervals algorithm with traditional classification algorithm  0 10000 20000 30000 40000 50000 8 10 20 30 40 50 80 Number of class association rules Minimum Support Apriori Proposed Methodology 0 20 40 60 80 100 120 140 160 180 200 8 10 20 30 40 50 Time \(Sce Minimum Support proposed methodology Apriori 


    Fig. 8. Comparative Analysis Classification Algorithms V  CONCLUSION Associative Classification techniques are used to make better decision in critical situations. The proposed associative classification called as Classification of microarray gene expression data using associative classification and gene expression intervals used to clas sify the gene expression with gene intervals in affected gene expression. The experimental results are carried out by using the gene expression of breast cancer. The associative classification on gene expression data obtained the best prediction and accuracy of the classification result. The proposed algorithm was tested with two class and multi class data sets. The classification algorithm was compared with the classical classification algorithms such as Linear Discriminant Analysis, SVM, and Decision Tree. After the comparison of traditional classification algorithms, as per the view of possible error rates the Associative Classification algorithm is best for biological data. The results of this work are used to drug designer for cancer diseases. The proposed algorithm works on gene expression data. In future, it will be implemented on hadoop and big data mining for biological data VI  R EFERENCES  1   Morgan Kaufmann Publishers Elsevier 2002 2   Second Edition PicasetOy Helsinki, 2005 3  Nagata, K., Washio, T., Kawahara, Y. and Unami, A  prediction from toxicogenomic data based on class association rule  ELSEVIER journal Toxicology Reports, vol.41, no.10 pp. 1133-1142, 2014 4  Garcia, S., Luengo, J., Sáez, J. A., López, V. and Herrera, F survey of discretization techniques: Taxonomy and empirical  Knowledge and Data Engineering, IEEE Transactions vol. 25, no.4, pp.734-750, 2013 5  Alves,R., Rodriguez.B.D.S and Aguilar. R.J.S  analysis: a survey of frequent pattern mining from gene expression  Briefings in Bioinformatics 2009, vol.2, no.2, pp.210-224 6   Miner: Maximal Confident Association Rules Miner Algorithm for Up/Down Applied Mathematics and Information Sciences vol.8 no.2, pp.799-809, 2014 7    BMC Bioinformatics vol.19, no.1, pp.7986, 2003 8  Snousy, A. M. B., El-Deeb, H. M., Badran, K. and Al Khlil, I. A  based classification algorithms on cancer  Egyptian Informatics Journal vol.12 no.2 pp.73-82. 2011 9  Refaeilzadeh, P., Tang, L. and Liu, H   Encyclopedia of database systems, Springer US pp. 532-538 2009   R. Agrawal and R. Srikant, Fast Algorithms for Mining Association Rules Proceedings of the 20th Int. Conf. on Very Large Data Bases VLDB94\,475486, Santiago de Chile, Chile 1994   Alagukumar, S., and Lawrance R., "A Selective Analysis of Microarray Data Using Association Rule Mining Procedia Computer Science Vol.47, pp.3-12, 2015 doi:10.1016/j.procs.2015.03.177   Alagukumar  Cancer Data Analysis Using Frequent Pattern Mining and Gene  International Journal of Computer Applications ISSN 0975 8887, no.1, pp.9-14, June 2015   Pasquier, N., Bastide, Y., Taouil, R., & Lakhal, L Pruning closed itemset lattices for association rules  In BDA'1998 international conference on Advanced Databases pp. 177-196. 1998   Giugno R, Pulvirenti A, Cascione L, Pigola G, Ferro A MIDClass: Microarray Data Classification by Association Rules and Gene Expression Intervals. Tang H, ed. PLoS ONE 2013;8\(8\:e69873. doi:10.1371/journal.pone.0069873   http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE1379   Wang, Zuncai, et al. "The prognostic biomarkers HOXB13 IL17BR, and CHDH are regulated by estrogen in breast cancer Clinical Cancer Research 13.21 pp. 6327-6334, 2005    cancer progression and host polymorphisms in the chemokine system: role of the macrophage chemoattractant protein-1 \(mcp 2518 g allele Clinical Chemistry 51: 452 5.2005   Dash, Rajashree, Rajib Lochan Paramguru, and Rasmita Dash Comparative analysis of supervised and unsupervised discretization techniques." International Journal of Advances in Science and Technology 2.3 \(2011\: 29-37   0 10 20 30 40 50 60 70 80 90 100 LDA SVM Decision Tree CACGE Accuracy and Error rate Classification Algorithms Accuracy Error Rate 


on items contained in each item group When the number of pivots increases the entire database is split into a ner granularity and the number of partitions increase correspondingly Such a ne granularity leads to a reduction in distance computation among transactions On the other hand when the pivot number k continues growing the number of transactions mapped into one hash bucket signiŞcantly increases thereby leading to a large candidate-object set and high shufßing cost see Figs 3b and 3c Consequently the overall execution time is optimized when k is 60 for both algorithms see Fig 3a 6.2 Minimum Support Recall that minimum support plays an important role in mining frequent itemsets We increase minimum support thresholds from 0.0005 to 0.0025 percent with an increment of 0.0005 percent to evaluate the impact of minimum support on FiDoop-DP The other parameters are the same as those for the previous experiments Fig 4a shows that the execution times of FiDoop-DP and Pfp decrease when the minimum support is increasing Intuitively a small minimum support leads to an increasing number of frequent 1-itemsets and transactions which have to be scanned and transmitted Table 2 illustrates the size of frequent 1-itemsets stored in FList and the number of nal output records of the two parallel solutions under various minimum-support values Fig 4a reveals that regardless of the minimum-support value FiDoop-DP is superior to Pfp in terms of running time Two reasons make this performance trend expected First FiDoop-DP optimizes the partitioning process by placing transactions with a high similarity into one group rather than randomly and evenly grouping the transaction Fig 4b conŞrms that FiDoop-DPÕs shufßing cost is signiŞcantly lower than that of Pfp thanks to optimal data partitions offered by FiDoop-DP Second this grouping strategy in FiDoop-DP minimizes the number of transactions for each GList under the premise of data completeness which leads to reducing mining load for each Reducer The grouping strategy of FiDoop-DP introduces computing overhead including signature-matrix calculation and hashing each band into a bucket Nevertheless such small overhead is offset by the performance gains in the shufßing and reduce phases Fig 4a also shows that the performance improvement of FiDoop-DP over Pfp is widened when the minimum support increases This performance gap between FiDoop-DP and Pfp is reasonable because pushing minimum support up in FiDoop-DP lters out an increased number of frequent 1-itemsets which in turn shortens the transaction partitioning cost Small transactions simplify the correlation analysis among the transactions thus small transactions are less likely to have a large number of duplications in their partitions As a result the number of duplicated transactions to be transmitted among the partitions is signiŞcantly reduced which allows FiDoop-DP to deliver better performance than Pfp 6.3 Data Characteristic In this group of experiments we respectively evaluate the impact of dimensionality and data correlation on the performance of FiDoop-DP and Pfp by changing the parameters in the process of generating the datasets using the IBM Quest Market-Basket Synthetic Data Generator 6.3.1 Dimensionality The average transaction length directly determines the dimensions of a test data We conŞgure the average transaction length to 10 40 60 and 85 to generate T10I4D 130 blocks T40I10D 128 blocks T60I10D 135 blocks T85I10D 133 blocks datasets respectively In this experiment we measure the impacts of dimensions on the performance of FiDoop-DP and Pfp on the 8-node Hadoop cluster The experimental results plotted in Fig 5a clearly indicate that an increasing number of dimensions signiŞcantly raises the running times of FiDoop-DP and Pfp This is because increasing the number of dimensions increases the number of groups thus the amount of data transmission sharply goes up as seen in Fig 5b The performance improvements of FiDoop-DP over Pfp is diminishing when the dimensionality increases from 10 to 85 For example FiDoop-DP offers an improvement of 29.4 percent when the dimensionality is set to 10 the improvement drops to 5.2 percent when the number of dimensions becomes 85 In what follows we argue that FiDoop-DP is inherently losing the power of reducing the number of redundant transactions in high-dimensional data When a dataset has a low dimensionality FiDoop-DP tends to build partitions Fig 4 Impact of minimum support on FiDoop-DP and Pfp TABLE 2 The Size of FList and the Number of Final Output Records Under Various Minimum-Support Values minsupport 0.0005 0.001 0.0015 0.002 0.0025 FList 14.69k 11.6k 9.71k 6.89k 5.51k OutRecords 745 588 465 348 278 XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 109 


each of which has distinct characteristics compared with the other partitions Such distinct features among the partitions allow FiDoop-DP to efŞciently reduce the number of redundant transactions In contrast a dataset with high dimensionality has a long average transaction length therefore data partitions produced by FiDoop-DP have no distinct discrepancy Redundant transactions are likely to be formed for partitions that lack distinct characteristics Consequently the beneŞt offered by FiDoop-DP for highdimensional datasets becomes insigniŞcant 6.3.2 Data Correlation We set the correlation among transactions i.e corr to 0.15 0.25 0.35 0.45 0.55 0.65 and 0.75 to measure the impacts of data correlation on the performance of the two algorithms on the 8-node Hadoop cluster The Number of Pivots is set to 60 see also Section 6.1 The experimental results plotted in Fig 5c clearly indicate that FiDoop-DP is more sensitive to data correlation than Pfp This performance trend motivates us to investigate the correlation-related data partition strategy Pfp conducts default data partition based on equal-size item group without taking into account the characteristics of the datasets However FiDoop-DP judiciously groups items with high correlation into one group and clustering similar transactions together In this way the number of redundant transactions kept on multiple nodes is substantially reduced Consequently FiDoop-DP is conducive to cutting back both data transmission trafŞc and computing load As can be seen from Fig 5c there is an optimum balance point for data correlation degree to tune FiDoop-DP performance e.g 0.35 in Fig 5c If data correlation is too small Fidoop-DP will degenerate into random partition schema On the contrary it is difŞcult to divide items into relatively independent groups when data correlation is high meaning that an excessive number of duplicated transactions have to be transferred to multiple nodes Thus a high data correlation leads to redundant transactions formed for partitions thereby increasing network and computing loads 6.4 Speedup Now we are positioned to evaluate the speedup performance of FiDoop-DP and Pfp by increasing the number of data nodes in our Hadoop cluster from 4 to 24 The T40I10D 128 blocks dataset is applied to drive the speedup analysis of the these algorithms Fig 6 reveals the speedups of FiDoop-DP a nd Pfp as a function of the number of data nodes The experimental results illustrated in Fig 6a show that the speedups of FiDoop-DP and Pfp linearly scale up with the increasing number of data nodes Such a speedup trend can be attributed to the fact that increasing the number of data nodes under a xed input data size inevitably 1 reduces the amount of itemsets being handled by each node and 2 increases communication overhead among mappers and reducers Fig 6a shows that FiDoop-DP is better than Pfp in terms of the speedup efŞciency For instance the FiDoop-DP improves the speedup efŞciency of Pfp by up to 11.2 percent with an average of 6.1 percent This trend suggests FiDoopDP improves the speedup efŞciency of Pfp in large-scale The speedup efŞciencies drop when the Hadoop cluster scales up For example the speedup efŞciencies of FiDoopDP and Pfp on the 4-node cluster are 0.970 and 0.995 respectively These two speedup efŞciencies become 0.746 and 0.800 on the 24-node cluster Such a speedup-efŞciency trend is driven by the cost of shufßing intermediate results which sharply goes up when the number of data nodes scales up Although the overall computing capacity is improved by increasing the number of nodes the cost of synchronization and communication among data nodes tends to offset the gain in computing capacity For example the results plotted in Fig 6b conŞrm that the shufßing cost Fig 5 Impacts of data characteristics on FiDoop-DP and Pfp Fig 6 The speedup performance and shufßing cost of FiDoop-DP and Pfp 110 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


is linearly increasing when computing nodes are scaled from 4 to 24 Furthermore the shufßing cost of Pfp is larger than that of FiDoop-DP 6.5 Scalability In this group of experiments we evaluate the scalability of FiDoop-DP and Pfp when the size of input dataset dramatically grows Fig 7 shows the running times of the algorithms when we scale up the size of the T40I10D data series Figs 7a and 7b demonstrate the performance of FiDoop-DP processing various datasets on 8-node and 24-node clusters respectively Fig 7 clearly reveals that the overall execution times of FiDoop-DP and Pfp go up when the input data size is sharply enlarged The parallel mining process is slowed down by the excessive data amount that has to be scanned twice The increased dataset size leads to long scanning time Interestingly FiDoop-DP exhibits a better scalability than Pfp Recall that see also from Algorithm 1 the second MapReduce job compresses an initial transaction database into a signature matrix which is dealt by the subsequent process The compress ratio is high when the input data size is large thereby shortening the subsequent processing time Furthermore Fidoop-DP lowers the network trafŞc induced by the random grouping strategy in Pfp In summary the scalability of FiDoop-DP is higher than that of Pfp when it comes to parallel mining of an enormous amount of data 7R ELATED W ORK 7.1 Data Partitioning in MapReduce Partitioning in databases has been widely studied for both single system servers e.g and distributed storage systems e.g BigTable PNUTS[31 The existing approaches typically produce possible ranges or hash partitions which are then evaluated using heuristics and cost models These schemes offer limited support for OLTP workloads or query analysis in the context of the popular MapReduce programming model In this study we focus on the data partitioning issue in MapReduce High scalability is one of the most important design goals for MapReduce applications Unfortunately the partitioning techniques in existing MapReduce platforms e.g Hadoop are in their infancy leading to serious performance problems Recently a handful of data partitioning schemes have been proposed in the MapReduce platforms Xie et al  developed a data placement management mechanism for heterogeneous Hadoop clusters Their mechanism partitions data fragments to nodes in accordance to the nodes processing speed measured by computing ratios In addition Xie et al  designed a data redistribution algorithm in HDFS to address the data-skew issue imposed by dynamic data insertions and deletions CoHadoop is a H a d oop s lightweight extension which is designed to identify relateddataŞlesfollowedbyamodiŞeddataplacement policy to co-locate copies of those related les in the same server CoHadoop considers the relevance among les that is CoHadoop is an optimization of HaDoop for multiple les A key assumption of the MapReduce programming model is that mappers are completely independent of one another Vernica et al  broke such an assumption by introducing an asynchronous communication channel among mappers T his c hannel e nables the m appers to see global states managed in metadata Such situationaware mappers SAMs can enable MapReduce to exibly partition the inputs Apart from this adaptive sampling and partitioning were proposed to produce balanced partitions for the reducers by sampling mapper outputs and making use of obtained statistics Graph and hypergraph partitioning have been used to guide data partitioning in parallel computing Graph-based partitioning schemes capture data relationships For example Ke et al applied a graphic-execution-plan graph EPG to perform cost estimation and optimization by analyzing various properties of both data and computation Their estimation module coupled with the cost model estimate the runtime cost of each vertex in an EPG which represents the overall runtime cost a data partitioning plan is determined by a cost optimization module Liroz-Gistau et al proposed the MR-Part technique which partitions all input tuples producing the same intermediate key co-located in the same chunk Such a partitioning approach minimizes data transmission among mappers and reducers in the shufße phase The approach captures the relationships between input tuples and intermediate keys by monitoring the execution of representative workload Then based on these relationships their approach applies a min-cut k-way graph partitioning algorithm thereby partitioning and assigning the tuples to appropriate fragments by modeling the workload with a hyper graph In doing so subsequent MapReduce jobs take full advantage of data locality in the reduce phase Their partitioning strategy suffers from adverse initialization overhead Fig 7 The scalability of FiDoop-DP and Pfp when the size of input dataset increases XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 111 


7.2 Application-Aware Data Partitioning Various efŞcient data partitioning strategies have been proposed to improve the performance of parallel computing systems For example Kirsten et al  developed two general partitioning strategies for generating entity match tasks to avoid memory bottlenecks and load imbalances Taking into account the characteristics of input data Aridhi et al proposed a novel density-based data partitioning technique for approximate large-scale frequent subgraph mining to balance computational load among a collection of machines Kotoulas et al built a data distribution mechanism based on clustering in elastic regions Traditional term-based partitioning has limited scalability due to the existence of very skewed frequency distributions among terms Load-balanced distributed clustering across networks and local clustering are introduced to improve the chance that triples with a same key are collocated These selforganizing approaches need no data analysis or upfront parameter adjustments in a priori Lu et al studied k nearest neighbor join using MapReduce in which a data partitioning approach was designed to reduce both shufßing and computational costs In LuÕs study objects are divided into partitions using a Voronoi diagram with carefully selected pivots Then data partitions i.e Voronoi cells are clustered into groups only if distances between them are restricted by a speciŞc bound In this way their approach can answer the k-nearest-neighbour join queries by simply checking object pairs within each group FIM for data-intensive applications over computing clusters has received a growing attention efŞcient data partitioning strategies have been proposed to improve the performance of parallel FIM algorithms A MapReducebased Apriori algorithm is designed to incorporate a new dynamic partitioning and distributing data method to improve mining performance This method divides input data into relatively small splits to provide exibility for improved load-balance performance Moreover the master node doesnÕt distribute all the data once rather the rest data are distributed based on dynamically changing workload and computing capability weight of each node Similarly Jumbo adopted a dynamic partition assignment technology enabling each task to process more than one partition Thus these partitions can be dynamically reassigned to different tasks to improve the load balancing performance of Pfp Uthayopas et al  investigated I/O and execution scheduling strategies to balance data processing load thereby enhancing the utilization of a multi-core cluster system supporting association-rule mining In order to pick a winning strategy in terms of data-blocks assignment Uthayopas et al incorporated three basic placement policies namely the round robin range and random placement Their approach ignores data characteristics during the course of mining association rules 8F URTHER D ISCUSSIONS In this study we investigated the data partitioning issues in parallel FIM We focused on MapReduce-based parallel FPtree algorithms in particular we studied how to partition and distribute a large dataset across data nodes of a Hadoop cluster to reduce network and computing loads We argue that the general idea of FiDoop-DP proposed in this study can be extended to other FIM algorithms like Apriori running on Hadoop clusters Apriori-based parallel FIM algorithms can be classiŞed into two camps namely count distribution and data distribution  For the count distribution camp each node in a cluster calculates local support counts of all candidate itemsets Then the global support counts of the candidates are computed by exchanging the local support counts For the data distribution camp each node only keeps the support counts of a subset of all candidates Each node is responsible for delivering its local database partition to all the other processors to compute support counts In general the data distribution schemes have higher communication overhead than the count distribution ones whereas the data distribution schemes have lower synchronization overhead than its competitor Regardless of the count distribution or data distribution approaches the communication and synchronization cost induce adverse impacts on the performance of parallel mining algorithms The basic idea of Fidoop-DPÑgrouping highly relevant transactions into a partition allows the parallel algorithms to exploit correlations among transactions in database to cut communication and synchronization overhead among Hadoop nodes 9C ONCLUSIONS A ND F UTURE W ORK To mitigate high communication and reduce computing cost in MapReduce-based FIM algorithms we developed FiDoop-DP which exploits correlation among transactions to partition a large dataset across data nodes in a Hadoop cluster FiDoop-DP is able to 1 partition transactions with high similarity together and 2 group highly correlated frequent items into a list One of the salient features of FiDoopDP lies in its capability of lowering network trafŞc and computing load through reducing the number of redundant transactions which are transmitted among Hadoop nodes FiDoop-DP applies the Voronoi diagram-based data partitioning technique to accomplish data partition in which LSH is incorporated to offer an analysis of correlation among transactions At the heart of FiDoop-DP is the second MapReduce job which 1 partitions a large database to form a complete dataset for item groups and 2 conducts FP-Growth processing in parallel on local partitions to generate all frequent patterns Our experimental results reveal that FiDoop-DP signiŞcantly improves the FIM performance of the existing Pfp solution by up to 31 percent with an average of 18 percent We introduced in this study a similarity metric to facilitate data-aware partitioning As a future research direction we will apply this metric to investigate advanced loadbalancing strategies on a heterogeneous Hadoop cluster In one of our earlier studies see for details we addressed the data-placement issue in heterogeneous Hadoop clusters where data are placed across nodes in a way that each node has a balanced data processing load Our data placement scheme can balance the amount of data stored in heterogeneous nodes to achieve improved data-processing performance Such a scheme implemented at the level of Hadoop distributed le system HDFS is unaware of correlations among application data To further improve load balancing 112 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


mechanisms implemented in HDFS we plan to integrate FiDoop-DP with a data-placement mechanism in HDFS on heterogeneous clusters In addition to performance issues energy efŞciency of parallel FIM systems will be an intriguing research direction A CKNOWLEDGMENTS The work in this paper was in part supported by the National Natural Science Foundation of P.R China No.61272263 No.61572343 Xiao QinÕs work was supported by the U.S National Science Foundation under Grants CCF-0845257 CAREER The authors would also like to thank Mojen Lau for proof-reading R EFERENCES  M J Zaki Parallel and distribu ted associat ion mining A survey IEEE Concurrency  vol 7 no 4 pp 14Ğ25 Oct 1999  I Pramudiono and M Kitsuregawa  Fp-tax Tree structure based generalized association rule mining in Proc 9th ACM SIGMOD Workshop Res Issues Data Mining Knowl Discovery  2004 pp 60Ğ63  J De an a n d S Gh e ma wa t M ap re du ce  S i mp l i e d da ta pr o ce s si n g on large clusters ACM Commun  vol 51 no 1 pp 107Ğ113 2008  S Sakr A Liu and A G Fayoumi The family of mapred uce and large-scale data processing systems ACM Comput Surveys  vol 46 no 1 p 11 2013  M.-Y Lin P.-Y Lee and S.-C Hsueh Apriori-based frequent itemset mining algorithms on mapreduce in Proc 6th Int Conf Ubiquitous Inform Manag Commun  2012 pp 76:1Ğ76:8  X Li n  Mr a pr io ri  As so ci a ti o n ru le s a lg o ri th m ba se d on mapreduce in Proc IEEE 5th Int Conf Softw Eng Serv Sci  2014 pp 141Ğ144  L Zhou Z Zhong J Chang J Li J Huang and S Feng Balanced parallel FP-growth with mapreduce in Proc IEEE Youth Conf Inform Comput Telecommun  2010 pp 243Ğ246  S Hong Z Huaxuan C Shiping and H Chunyan The study of improved FP-growth algorithm in mapreduce in Proc 1st Int Workshop Cloud Comput Inform Security  2013 pp 250Ğ253  M Riondato  J A DeBrabant R Fonseca and E Upfal Parma A parallel randomized algorithm for approximate association rules mining in mapreduce in Proc 21st ACM Int Conf Informa Knowl Manag  2012 pp 85Ğ94  C Lam Hadoop in Action  Greenwich USA Manning Publications Co 2010  H Li Y Wang D Zhang M Zhang and E Y Chang PFP Parallel FP-growth for query recommendation in Proc ACM Conf Recommender Syst  2008 pp 107Ğ114  C Curino E Jones Y Zhang and S Madden Schism A workload-driven approach to database replication and partitioning Proc VLDB Endowment  vol 3 no 1-2 pp 48Ğ57 2010  P Uthayop as and N Benjamas Impact of i/o and execution scheduling strategies on large scale parallel data mining J Next Generation Inform Technol  vol 5 no 1 p 78 2014  I  P r a m u d i o n o a n d M  K i t s u r e g a w a  P a r a l l e l F P g r o w t h o n P C cluster in Proc.Adv.Knowl.DiscoveryDataMining  2003 pp 467Ğ473  Y Xun J Zhang and X Qin Fidoop Parallel mining of frequent itemsets using mapreduce IEEE Trans Syst Man Cybern Syst  vol 46 no 3 pp 313Ğ325 Mar 2016 doi 10.1109 TSMC.2015.2437327  S Owen R Anil T Dunning and E Friedman Mahout Action  Greenwich USA Manning 2011  D Borthakur  Hdfs architecture guide HADOOP APACHE PROJECT Available  http://hadoop.apache.org/common/docs current/hdfs design.pdf 2008  M Zaharia M Chowdhury M J Franklin  S Shenker and I Stoica Spark Cluster computing with working sets in Proc 2nd USENIX Conf Hot Topics Cloud Comput  2010 p 10  W Lu Y Shen S Chen and B C Ooi EfŞcient proces sing of k nearest neighbor joins using mapreduce Proc VLDB Endowment  vol 5 no 10 pp 1016Ğ1027 2012  T Kanung o D M Mount N S Netanya hu C D Piatko R Silverman and A Y Wu An efŞcient k-means clustering algorithm Analysis and implementation IEEE Trans Pattern Anal Mach Intell  vol 24 no 7 pp 881Ğ892 Jul 2002  A K Jain Data clustering 50 years beyond k-means Pattern Recog Lett  vol 31 no 8 pp 651Ğ666 2010  D Arthur and S Vassilvitskii  k-means  The advantages of careful seeding in Proc 18th Annu ACM-SIAM Symp Discr Algorithms  2007 pp 1027Ğ1035  J Leskovec A Rajaraman and J D Ullman Mining Massive Datasets  Cambridge U.K Cambridge Univ Press 2014  A Stupar  S Mich el and R Schen kel Rankred uceĞpr ocessin g k-nearest neighbor queries on top of mapreduce in Proc 8th Workshop Large-Scale Distrib Syst Informa Retrieval  2010 pp 13Ğ18  B Bahmani A Goel and R Shinde EfŞcient distributed locality sensitive hashing in Proc 21st ACM Int Conf Inform Knowl Manag  2012 pp 2174Ğ2178  R Panigrahy Entropy based nearest neighbor search in high dimensions in Proc 17th Annu ACM-SIAM Symp Discr Algorithm  2006 pp 1186Ğ1195  A Z Broder M Charikar  A M Frieze and M Mitzenma cher Min-wise independent permutations J Comput Syst Sci  vol 60 no 3 pp 630Ğ659 2000  L Cristofor ARtool Association rule mining algorit hms and tools 2006  S Agrawal V Narasayya  and B Yang Integrating vertical and horizontal partitioning into automated physical database design in Proc ACM SIGMOD Int Conf Manag Data  2004 pp 359Ğ370  F Chang J Dean S Ghema wat W Hsieh D Wallach  M  Burrows T Chandra A Fikes and R Gruber Bigtable A distributed structured data storage system in Proc 7th Symp Operating Syst Des Implementation  2006 pp 305Ğ314  B F Cooper R Ramakrishn an U Srivastava A Silberstein P Bohannon H.-A Jacobsen N Puz D Weaver and R Yerneni Pnuts Yahoo!Õs hosted data serving platform Proc VLDB Endowment  vol 1 no 2 pp 1277Ğ1288 2008  J Xie and X Qin The 19th heterogenei ty in computing workshop HCW 2010 in Proc IEEE Int Symp Parallel Distrib Process Workshops Phd Forum  Apr 2010 pp 1Ğ5  M Y Eltabakh Y Tian F  Ozcan R Gemulla A Krettek and J McPherson Cohadoop Flexible data placement and its exploitation in hadoop Proc VLDB Endowment  vol 4 no 9 pp 575 585 2011  R Vernica A Balmin K S Beyer and V Ercegovac Adaptive mapreduce using situation-aware mappers in Proc 15th Int Conf Extending Database Technol  2012 pp 420Ğ431  Q Ke V Prabhakar an Y Xie Y Yu J Wu and J Yang Optimizing data partitioning for data-parallel computing uS Patent App 13/325,049 Dec 13 2011  M Liroz-Gis tau R Akbarinia D Agrawal E Pacitti  and P Valduriez Data partitioning for minimizing transferred data in mapreduce in Proc 6th Int Conf Data Manag Cloud Grid P2P Syst  2013 pp 1Ğ12  T Kirsten L Kolb M Hartung A Gro H K  opcke and E Rahm Data partitioning for parallel entity matching Proc VLDB Endowment  vol 3 no 2 pp 1Ğ8 2010  S Kotoulas E Oren and F Van Harmelen Mind the data skew Distributed inferencing by speeddating in elastic regions in Proc 19th Int Conf World Wide Web  2010 pp 531Ğ540  L Li and M Zhang The strategy of mining associat ion rule based on cloud computing in Proc Int Conf Bus Comput Global Inform  2011 pp 475Ğ478  S Groot K Goda and M Kitsuregawa  Towards improv ed load balancing for data intensive distributed computing in Proc ACM Symp Appl Comput  2011 pp 139Ğ146  M Z Ashra D Taniar and K Smith ODAM An optimiz ed distributed association rule mining algorithm IEEE Distrib Syst Online  vol 5 no 3 p 1 Mar 2004 Yaling Xun is currently a doctoral student at Taiyuan University of Science and Technology She is currently a lecturer in the School of Computer Science and Technology Taiyuan University of Science and Technology Her research interests include data mining and parallel computing XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 113 


Jifu Zhang received the BS and MS degrees in computer science and technology from the Hefei University of Tchnology China and the PhD degree in pattern recognition and intelligence systems from the Beijing Institute of Technology in 1983 1989 and 2005 respectively He is currently a professor in the School of Computer Science and Technology TYUST His research interests include data mining parallel and distributed computing and artiŞcial intelligence Xiao Qin received the PhD degree in computer science from the University of Nebraska-Lincoln in 2004 He is currently a professor in the Department of Computer Science and Software Engineering Auburn University His research interests include parallel and distributed systems storage systems fault tolerance real-time systems and performance evaluation He received the U.S NSF Computing Processes and Artifacts Award and the NSF Computer System Research Award in 2007 and the NSF CAREER Award in 2009 He is a senior member of the IEEE Xujun Zhao received the MS degree in computer science and technology in 2005 from the Taiyuan University of Technology China He is currently working toward the PhD degree at Taiyuan University of Science and Technology His research interests include data mining and parallel computing  For more information on this or any other computing topic please visit our Digital Library at www.computer.org/publications/dlib 114 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


