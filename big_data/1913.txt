Discovering Representative Episodal Association Rules from Event Sequences Using Frequent Closed Episode Sets and Event Constraints Sherri K Harms, Jitender Deogun Jamil Saquer Tsegaye Tadesse Department of CSCE CS Department NDMC University of Nebraska SWMS University University of Nebraska Lincoln NE 68588-01 15 sharms{deogun} @cse.unl.edu jms48 1 f@smsu.edu tadesse@unlserve.unl.edu Lincoln, NE 68588-01 15 Springfield MO 65804 Abstract Discovering association rules from time-series data is an important data mining problem The number 
of potential rules grows quickly as the number of items in the antecedent grows It is therefore dificult for an expert to analyze the rules and identib the useful An approach for generating representative association rules for transactions that uses only a subset of the set of frequent itemsets called frequent closed itemsets was presented in 6 We employ formal concept analysis to develop the notion of frequent closed episodes The concept of representative association rules is formalized in the context of event sequences 
Applying con straints to target highly signi$cant rules further reduces the number of rules Our approach results in a sign@cant re duction of the number of rules generated while maintaining the minimum set of relevant association rules and retaining the abili9 to generate the entire set of association rules with respect to the given constraints We show how our method can be used to discover associations in a drought risk man agement decision support system and use multiple climatol ogy datasets related to automated weather stations minimizing the number of association rules 
discovered 2 Most of these approaches introduce additional measures for interestingness of a rule and prune the rules that do not sat isfy the additional measures as a post-processing step A set of representative association rules on the other hand is a minimal set of rules from which all association rules can be generated, during the actual processing step Usually the number of representative association rules is much smaller than the number of all association rules and no additional measures are needed for determining the representative as sociation 
rules 4 Recently Saquer and Deogun developed a different ap proach for generating representative association rules 6 Similarly Zaki 8 used frequent closed itemsets to gener ate non-redundant association rules in CHARM We use closure as the basis for generating frequent sets in the context of sequential data We then generate se quential association rules based on representative associa tion rule approaches while integrating constraints into our approach By combining these techniques, our method is well suited for time series problems that have groupings 
of events that occur close together in time but occur relatively infrequently over the entire dataset We apply this technique to the drought risk management problem 1 Introduction 2 Frequent Closed Episodes Discovering association rules is an important data mining problem The problem was first defined in the con text of the market basket data to identify customers buy ing habits  11 The problem of analyzing and identifying interesting rules becomes difficult as the number of rules increases In most applications the number of rules discov 
ered is usually large Two different approaches to handle this problem have been reported 1 identifying the associ ation rules that are of special importance to the user and 2 This research was supported in part by NSF Digital Government Grant No EIA-0091530 and NSF EPSCOR Grant No EPS-0091900 Our overall goal is to analyze event sequences, discover recurrent patterns of events, and generate sequential associ ation rules Our approach is based on the concept 
of repre sentative association rules combined with event constraints A sequential dataset is normalized and then discretized by forming subsequences using a sliding window 5 Using a sliding window of size 6 every normalized time stamp value xt is used to compute each of the new sequence values yt-6/2 to ylt+6/2 Thus the dataset has been divided into segments each of size 6 The discretized version of the 0-7695-1119-8/01 17.00 0 2001 IEEE 603 


time series is obtained by using some clustering algorithm and a suitable similarity measure Each cluster identifier is an event Qpe and the set of cluster labels is the class of events E The newly formed version of the time series is referred to as an event sequence An event sequence is a triple tB to S where tB is the beginning time to is the ending time and S is a finite time-ordered sequence of events p is the step size between each event d is the total number of steps in the time interval from time tg to time to and D  B  dp Each et is a member of a class of events E and t 5 tz+l for all i  B    D  lp A sequence of events S includes events from a single class of events E A window on an event sequence S is an event subse quence W  et7   etk where tB 5 t and tk 5 tD  1 5 The width of the window W is width\(W  tk  t The set of all windows W on S with width\(W  wzn is denoted as W\(S win The width of the window is pre specified An episode in an event sequence is a combination of events with a partially specified order The type of an episode is parallel if no order is specified, and serial if the events of the episode have a fixed order An episode is injec tive if no event type occurs more than once in the episode We extend the work of Mannila et al 5 to consider closed sets of episodes We use formal concept analysis as the basis for developing the notion of closed episode sets 6 Informally a concept is a pair of sets set of objects windows or episodes and set of features \(events common to all objects in the set That is s  etn 1 Q/3+IS eto+np   etr3+dp  et 1 where Definition 1 An episodal data mining context is defined as a triple W\(S win E R where W\(S win is a set of all windows of width win defined on the event sequence S and E is a set of episodes in the event sequence S and R C w x E Definition2 Let W,E,R be an episodal data mining context X C W and Y C E Define the mappings a p as follows p  2  2 Q  2E  2 p\(X  e E E I w e E R b w E X a\(Y  w E W 1 w e E R b'e E Y The mapping p\(X associates with X the set of episodes that are common to all the windows in X Similarly, the mapping a\(Y associates with Y the set of all windows containing all the episodes in Y Intuitively p\(X is the maximum set of episodes shared by all windows in X and a\(Y is the maximum set of windows possessing all the episodes in Y It is easy to see that in general for any set Y of episodes p\(a\(Y  Y A set of episodes Y that satisfies the condi tion p\(a\(Y  Y is called a closed set of episodes 6 The frequency of an episode is defined as the fraction of windows in which the episode occurs Given an event sequence S and a window width win the frequency of an episode P of a given type in S is I w E W\(S win  P occurs in w I fr\(P S win  I W\(S,win I Given a frequency threshold min P is frequent if fr\(P,S win 2 min-fr A frequent closed set of episodes FCE is a closed set of episodes that satisfy the minimum frequency threshold Closure of an episode set X C E denoted by closure\(X is the smallest closed episode set containing X and is equal to the intersection of all frequent episode sets containing X To generate frequent closed target episodes we develop an algorithm called Gen-FCE shown in Figure 1 Gen FCE is a combination of the Close-FCI algorithm the WINEPI frequent episode algorithms 5 and the Direct al gorithm Gen-FCE generates FCE with respect to a given set of Boolean target constraints B an event sequence S a window width win an episode type a minimum fre quency minyr and a window step size p The Gen-FCE algorithm requires one database pass during each iteration I Generate Candidate Frequent Closed Target Episodes of length I CFC1.a 2 k=l 3 while CFCk,a  8 do 4 Read the sequence S one window at a time let FCEk,a be the elements in CFCk?B with a new closure and with a frequency 2 minfr 5 6 k Generate Candidate Frequent Closed Target Episodes CFCk+l.Bfrom FCEk,t3 7 return U,=1 k 1 FCE,,p.closureandFCE,,B f requency Figure 1 Gen-FCE algorithm We incorporate constraints similar to the Direct algo rithm This approach is known to work well at low minimum supports and in large datasets 7 This approach requires an expensive cross-product operation so for dis junctive singleton constraints, the candidate generation al gorithm is used 5 3 Representative Episodal Association Rules We use the set of frequent closed episodes FCE pro duced from the Gen FCE algorithm to generate the repre sentative episodal association rules that cover the entire set of association rules The cover of a rule r  X  Y denoted by C\(r is the set of association rules that can be generated from r That 604 


is C\(r  X  Y  XUU  V 1 U,V Y Un V  8 and V  8 An important property of the cover operator stated in 4]is that if an association rule r has support s and confidence c then every rule r\222 E C\(r has support at least s and confidence at least c Using the cover operator a set of representative associ ation rules with minimum support s and minimum confi dence c RAR\(s c is defined as follows RAR\(s c  r E AR\(s,c I fir\222 E AR\(s,c r  r\222 and r E C\(r\222 That is a set of representative association rules is a least set of association rules that cover all the association rules and from which all association rules can be generated Clearly AR\(s c  U{C\(r I r E RAR\(s, c Gen-REAR shown in Figure 2, is a modification of the Generate-RAR 6 that generates REAR for a given set of frequent closed episodes FCE with respect to a minimum confidence c min-fr 0.05 0.10 0.15 0.20 0.25 1 FCE 2 while\(k  1 do 3 Generate REARk by adding each rule X  Z  X such that Z.support/X.support 2 c and X  Z\\X is not covered by a previously generated rule 5 return REAR k  the size of the longest frequent closed episode in 4 k Candidates Freq. Closed Iters time 224 Episodes SI 17284 3950 6 6932 4687 629 5 205 1704 229 4 10 807 102 4 1 567 58 3 1 Figure 2 Gen-REAR algorithm Using our technique on multiple time series while con straining.the episodes to a user-specified target set, we can find relationships that occur across the sequences 4 Empirical Results We are developing an advanced Geospatial Decision Support System GDSS to improve the quality and acces sibility of drought related data for drought risk management 3 Our objective is to integrate spatio-temporal knowledge discovery techniques into the GDSS using a combination of data mining techniques applied to geospatial time-series data by 1 finding relationships between user-specified tar get episodes and other climatic events and 2 predicting the target episodes The REAR approach will be used to meet the first objective In this paper we validate the effective ness of the REAR approach to find relationships between drought episodes at the automated weather station in Mead NE, and other climatic episodes, from 1989-1999 We com pare it to the WINEPI algorithm 5 We use data from nine sources including satellite vegetation data and precipitation and soil moisture data We experimented with several different window widths minimal frequency values minimal confidence values for both parallel and serial episodes When using constraints we specified droughts as our target episodes The experi ments were ran on a AMD Athlon 1.3GHz PC with 256 MB main memory, under the Windows 2000 operating system 4.1 Gen-FCE vs WINEPI Tables 1 and 2 represent performance statistics for find ing frequent closed episodes in the drought risk manage ment dataset for Mead NE with various frequency thresh olds for injective serial drought episodes with a 2 month window using the Gen FCE and WINEPI algorithms re spectively Table 1 min-fr 0.05 0.10 0.15 0.20 0.25 Gen-FCE serial episode performance Candidates Freq. Closed Iters time Episodes s 525 77 3 2 335 24 2 1 153 10 2 1 93 620 83 520 Gen-FCE performs extremely well when finding the drought episodes The number of frequent closed episodes decreases rapidly as the frequency threshold increases For the sample dataset at a frequency threshold of 0.10 and a window width of 2 months Gen-FCE produces 6 frequent drought serial episodes while WINEPI produces 1600 more  102\episodes Because we are working with a fraction of the possi ble number of episodes our algorithms are extremely ef ficient When finding all frequent drought episodes for the sample dataset using a window width of 5 months the running time was 1 second for Gen-FCE and 6 hours for WINEPI This illustrates the benefits of using closures and constraints when working with the infrequently occurring drought events As the window size increases so does the frequent episode generation time and the number of frequent 605 


episodes When using drought constraints, the increase is at a much slower pace than WINEPI For the sample dataset and a window width of 3 months Gen-FCE produces 53 frequent drought serial episodes while WINEPI produces 5779 more 3 1 16\episodes 4.2 Gen-REAR vs WINEPI Association Rules We next experimented with finding association rules in the drought risk management dataset for Mead NE with various confidence thresholds and window widths using the Gen-REAR and WINEPI AR algorithms for injective par allel and serial episodes The number of rules decreases rapidly as the confidence threshold increases and increases rapidly as the window width widens In all cases Gen REAR produces fewer rules than the WINEPI AR algo rithm Using the Gen-REAR approach all the rules can be generated if desired even though the meaning of the addi tional AR\222s is captured by the smaller set of REAR\222s Gen-REAR performs extremely well when finding drought episodal rules as shown in Table 3 The number of REAR\222s decreases rapidly as the confidence interval in creases For the sample dataset at a confidence threshold of 0.20 and a window width of 2 months Gen-REAR pro duces 24 drought parallel episodal rules while WINEPI AR produces 20892 more \(5038 rules. With the same param eters Gen-REAR produces 14 drought serial episodal rules while WINEPI AR produces 16257% more 2290 rules Table 3 Gen-REAR parallel and serial rules I I Parallel 1 Serial fl L Confidence Distinct Distinct threshold rules rules 0.20 24 14 0.25 24 12 0.30 19 9 0.35 13 7 0.40 10 6 0.45 8 5 Confidence threshold 10 A5 the window width widens Gen-REAR overwhelm ingly produces fewer rules than the WINEPI algorithm The number of REAR\222s increases as the window width For the sample dataset at a window width of 3 months Gen REAR produces 30 parallel drought episodal rules while WINEPI AR produces 53763 more 16159 rules With the same parameters Gen-REAR produces 8 serial drought episodal rules while WINEPI AR produces 24825% more 1994 The savings are obvious The Gen-REAR algorithm finds the drought REAR\222s for all reasonable window widths and confidence levels on the Mead NE drought risk man agement dataset in less than 30 seconds As the window widens the WINEPI AR algorithm quickly becomes com putationally infeasible to use for the drought risk manage ment problem 5 Conclusion This paper presents Gen-REAR a new approach for gen erating representative episodal association rules We also presented Gen-FCE a new approach used to generate the frequent closed episode sets that conform to user-specified constraints Our approach results in a large reduction in the input size for generating representative episodal associ ation rules for targeted episodes, while retaining the ability to generate the entire set of association rules We also stud ied the gain in efficiency of generating targeted representa tive episodal association rules as compared to the traditional WINEPI algorithm on a multiple time series drought risk management problem References  11 R. Agrawal T Imielinski and A Swami Mining association rules between sets of items in large databases In Proceedings of the ACM SIGMOD 1993 International Conference on Man agement of Data [SICMOD 931 pages 207-2 16, Washington D.C 1993 2 R. Bayardo, R. Agrawal and D Gunopupulos Constraint based rule mining in large, dense databases In Proceedings 3 S K Harms S Goddard S E Reichenbach W J Waltman and T Tadesse Data mining in a geospatial decision support system for drought risk management In Proceedings of the 2001 National Conference on Digital Government Research pages 9-16 Los Angelos, California USA May 2001 4 M Kryszkiewicz Fast discovery of representative association rules In Lecture Notes in Artijkial Intelligence volume 1424 pages 2 14-22 1 Proceedings of RSCTC 98 Springer-Verlag 1998 5 H Mannila H Toivonen and A I Verkamo Discovery of frequent episodes in event sequences Technical report De partment of Computer Science University of Helsinki Fin land 1997 Report C-1997-15 6 J Saquer and J S Deogun Using closed itemsets for discov ering representative association rules In Proceedings of the Twelfth International Symposium on Methodologies for Intel ligent Systems ISMIS 20001 Charlotte NC October 11-14 2000  R Srikant Q Vu and R Agrawal Mining association rules with item constraints In Proceedings qf the Third Intema tional Conference on Knowledge Discovery and Data Mining KDD97 pages 67-73 1997 8 M Zaki Generating non-redundant association rules In Proceedings of the Sixth International Conference on Knowl edge Discovery and Data Mining KDD2000 pages 3U3 Boston MA USA August 20-23 2000 of ICDE-99 1999 606 


and the information-value determining probability density is Each g E represents the mean value of a predicted conditional distribution there exists a filtered conditional density of the form For each such PYt 5 g  N 4twijt Zit Hi I wit Hit Pi1 i  1 1 9 24  2 E t-l and evaluating this expression at   c rlij*\(Z  27T  Pt I  The ALRT is Associate x{,t-l and zit  titlr zit2lT if for any g  21,22,~3,4 E Xtit--l qijt\(z 2 Figure a illustrates a family of three crossing tra jectories The tracks move generally from left to right at time increases the lines correspond to the z and y position components and the  symbols correspond to noise-corrupted observations Figure l\(b displays the filtered track sets corresponding to this simula tion with the ellipses representing the projections of the track sets onto position space The initial pre dicted track set Xol.-l includes the entire field of view and is not shown The three large elliptical regions correspond to the the filtered track sets after the first set of observations have been processed As time in creases the size of these track sets decreases rapidly for the first few observations corresponding to Tracks 1 arid 3 however there are multiple associations since tlhe tracks are fairly close and the track sets are still fairly large As more confidence is obtained in the as sociations these tracks become uniquely associated and the elliptical regions decrease rapidly in size and will asymptotically become point tracks At sample ten Tracks 1 and 2 nearly intersect and both tracks associate with the observations These multiple asso ciations persist for a few samples but as the tracks diverge the associations again become unique Track 2 is unambiguously associated and estimated as ev idenced by the radius of the track set converging to zero and the set-valued estimates asymptotically be come point-valued b??lit 5 References a D R Morrell and W C Stirling Set-Valued Fil tering and Smoothing IEEE Transactions on Sys tems Man and Cybernetics 21\(1 Jan uary/February 1991 3 I Levi The Enterprise of Knowledge MIT Press Cambridge Massachusetts 1980 208 Y Figure 1 Three Crossing Tracks a simulated tra jectories and observations; \(b filtered track sets l W C Stirling and D R Morrell Convex Bayes Decision Theory IEEE Transactions on Sys tmw Man and Cybernetics 21\(1 Jan uary/February 1991 311 


Figure 5 True and mean estimated backlog versus time,t   alternating between 1/9 and 00 223semi-genie\224 Figure 6 True and mean estimated backlog versus time t  I36  transients The normalized information throughput for this run was s  0558 Figure 6 shows a similar plot but where no genie in formation is provided The a priori estimEte of the arrival rate is taken to be the constant 6  163 A  14 We see that for this value of 6 the backlog is apparently over and under-estimated roughly equally The normalized through put that resulted from this run was 0388 or about 70 of that achieved with the semi-genie model Figure 7 shows the true and es2mated backlog pro cesses for the case when   1.86 A  65 Although the backlog is poorly estimated during the intervals of low arrival rates, the resulting normalized throughput obtained was 0415 which is somewhat better about 74 of semi genie than that achieved for the case of Figure 6 We ran simulations for the entire spectrum of values for 6 and generally found little sensitivity to the value selected We conclude that for the case considered some benefit may be possible by devising algorithms for tracking the gross arrival rate but this benefit is not major Recall the ex aggerated arrival rate extremes Given that a constant value is used it is not especially sensitive the value se lected We emphasize that these conclusions may or may not be extendable for all parameter settings and further analysis should be made for specific network designs 2231 0 Figure 7 Rue and mean estimated backlog versus time  1.86 V Conclusions In this paper we have developed implementable dynamic transmission control procedures for single-hop slotted ALOHA networks using frequency-hopped spread spec trum These control procedures are based on backlog es timates and a backlog estimation algorithm that operates in the presence of multiuser interference ambient noise and jamming was derived The backlog process is esti mated by sensing activity on the receiver-based code while the user is not transmitting half-duplex operation is as sumed 80 that minimal if any additional hardware is required Performance was derived by simulation and nu merical examples show that the feasible system operates at nearly the same level of performance as the 223genie\224 model These results demonstrate that considerable robustness is achievable VI References 1 2 3 4 5 L P Clare and A R K Sastry 223The effects of slot ting burstiness and jamming in frequency-hopped random access systems,\224 IEEE MILCOM\22289 Conf Rec Boston MA October 15-18 1989 pp 154-160 L P Clare J E Baker and A R K Sastry 223A performance comparison of control policies for slot ted ALOHA frequency-hopped multiple access sys tems,\222\222 IEEE MILCOM\222SO Conf Rec Monterey CA September 30  October 3 1990 pp 608-614 L P Clare and J E Baker 223The effects of jam ming on control policies for frequency-hopped slotted ALOHA,\224 Proc IEEE GLOBECOM\222SO San Diego CA December 2-5 1990 pp 1132-1138 B Hajek 223Recursive retransmission control  Ap plication to a frequency-hopped spread-spectrum sys tem,\224 Proc I$h Conf Infor Sciences and Systems Princeton University March 1982 pp 116-120 N Pronios 223On the stability of spread-spectrum networks, with decentralized recursive retransmission control under jamming,\224 Proc INFOCOM\222SO San Francisco CA, June 5-7 1990 pp 588-594 20.2.6 0401 


speci\002ed for each experiment and this value is then used to determine the number of itemsets rows or columns in a single block As itemsets are created each is added to an available block and new blocks are 223allocated\224 as needed If a new block is to be allocated and the number of blocks in the simulated memory also a parameter of the experiment is equal to the number of occupied blocks a victim block is chosen for replacement using one of a number of replacement strategies If the data in the victim block has been modi\002ed since last being written to disk it is 223written,\224 and a simulated data block write is counted If a data element is required from a block that has previously been written to disk e.g for a subset lookup that block is 223read\224 into the simulated memory and a data block read is counted When the counting algorithm is complete the total number of blocks written and read during the simulation is available for examination In addition to the number of data block reads and writes the simulation also counts the number of equal-cost CPU operations as described in Section 4 and the number of index block reads and writes This allows a comparison of the different algorithms based on the number of operations performed and the additional cost of index lookups The counting algorithms themselves are implemented in an I/O-ef\002cient manner using a block pinning and release strategy In all cases the join step is performed using a block nested loop 9 a l g o r i t h m w i t h each s e t o f b l o ck s i n the outer loop being pinned and released as necessary The number of blocks available for the inner and outer loops are both parameters of the experiment 5.2 Effects of Dataset Parameters The experiments were chosen to compare the effect of various distributions and relative 223dimensions\224 i.e number of rows compared to the number of distinct items or columns of the data For the RW approach subsets were identi\002ed in each row of the database and c ounters were accessed only as needed For the CW case the results of the conjoin were not kept For all experiments an LRU replacement strategy was used to determine victim blocks in memory Speci\002c parameters for each of the following 002gures are given at the end of this section in Figure 5 Figure 3 shows a comparison of the number of data block reads and writes for both the RW and CW approach as the level of minimum support is decreased leaving all other parameters 002xed Note that as the required level of minimum support is decreased the difference between the two approaches in the total number of block movements increases at an accelerating rate Figure 4 shows the results of an experiment intended to demonstrate the relative I/O cost of the RW and CW algorithms for datasets of 223equal height\224 i.e the number of  0 200,000 400,000 600,000 800,000 22 20 18 16 14 12 10 8 6 4 Minimum support RW CW Figure 3 Effect of decreasing support 100 1,000 10,000 100,000 0.10 0.20 0.40 0.60 0.80 1.00 1.20 1.40 1.60 Width ratio RW CW Figure 4 Effect of table width rows in the data held constant and varying 223width\224 i.e the number of columns The total number of blocks read or written is plotted in a logarithmic scale along the y axis including index blocks The x axis shows the relative ratio of the number of columns to the number of rows in the dataset For example a dataset with 1000 columns and 1000 rows would have a width ratio of 1 while a dataset with 100 columns and 1000 rows would have a width ratio of 0.1 That is the higher the width ratio the 223wider\224 the table is relative to its 223height\224 The number of rows i.e l as held constant at 500 while the number of columns i.e n  was varied from 50 to 1000 The other parameters of the dataset i.e B C max  B R max  were determined by the default values of the synthetic dataset generator As expected from the analysis of Section 4.3 the CW algorithm is far more ef\002cient than the RW algorithm for datasets with a relatively high width ratio Once the number of rows is suf\002ciently greater than the number of columns the advantage of the CW algorithm is no longer apparent Our experiments more results are presented in v e r ify that for datasets of the type we described accessing the database in a column-wise fashion leads to a reduction in the number of I/O operations required Again the key point 


Figure Rows  l  Items  n  M M D 1 M D 2 f I minsup 3 1000 1000 16 8 4 512 22-4 4 500 50-1000 32 10 8 512 0  2 l Figure 5 Experimental parameters per 002gure is that the counters which maintain the identity and frequency of occurrence for a particular set of items need not be repeatedly and expensively brought into the primary storage as is the case in the row-wise method 6 Discussion and Conclusions We discuss a few possible improvements not explored in our current work followed by a summary of our conclusions 017 We assumed that the RW algorithm strictly follows the strategy of identifying each subset present in a given row and then accesses the appropriate c ounter However an implementation may also read a number of blocks of counters and then 002nd all rows with an itemset to support that counter as per T h e o rde r ing o f the rows and the counters will likely have an effect on which of these two methods is more ef\002cient 017 An obvious improvement to the cost of the RW algorithm as presented is to perform the join for a pair of  k 000 1 itemsets or even a set of blocks thereof and then immediately perform the check of all its subsets Given the likelihood that many of the itemsets in ablockof L k 000 1 will share common items this could result in a signi\002cant savings since the cost of writing the join and then re-reading for the prune would be avoided probably without a signi\002cant increase in the number of counter blocks that need to be read 017 In the RW Apriori counting algorithm the primary purpose of the veri\002cation and pruning of all  k 000 1 subsets of any candidate before actually counting its support is to limit the number of times that a row or candidate need be brought in for support counting Since the CW algorithm already has all information necessary for counting at one pass it may be better to avoid the Apriori veri\002cation of subsets and simply to count the support of each candidate This is especially true if the candidates in a given block of itemsets all share a frequent common subset as the conjoin for that subset could be computed once and used directly in counting the support of candidates sharing that subset Also given the possibly larger processing cost of the CW algorithm when memory is unconstrained and the savings in I/O when memory is limited some combination of both the RW and CW algorithms may yield the best performance As organizations begin collecting their own electronic datasets and as growing sources of information become available through the Internet it is imperative that the data organization and access approaches be studied carefully to exploit ef\002cient and promising processing techniques In particular in this paper we have examined the I/O ef\002ciency considerations for association rules algorithms with respect to data organization We have shown that a column-wise strategy for mining tabular data may provide an improvement in the I/O ef\002ciency over a similar row-wise algorithm We used a simple analysis and experimental validation to support our approach This provides an indication that similar considerations may signi\002cantly bene\002t other high-cost computing problems associated with mining of datasets References 1 R  A gr a w al et al  T he Q u est D at a M i n i n g S yst e m  Technical report IBM Almaden Research Center 1996 http://www.almaden.ibm.com/cs/quest 2 R  A g r a w a l  T  I m i e lin s k i a n d A  S w a m i  M in in g A s s o c i ation Rules between Sets of Items in Large Databases In Proceedings of the 1993 ACM SIGMOD Int'l Conf on Management of Data  1993 3 R  A g r a w a l  H  M a n n illa  R  S rik a n t  H  T o i v o n e n  a n d A  I  Verkamo Fast Discovery of Association Rules In Advances in Knowledge Discovery and Data Mining  pages 307\226328 AAAI Press  MIT Press 1996 U.M Fayyad G PiatetskyShapiro P Smyth and R Uthurusamy editors 4 R  A gr a w al and R  S r i kant  F ast A l gor i t h m s f o r M i n i n g Association Rules In Proc 20th Int'l Conference on Very Large Databases  Santiago Chile 1994 5 R  J  B ayar do Ef 002ci e nt l y M i ni ng Long P a t t e r n s f r o m Databases In Proceedings of the 1998 ACM SIGMOD Conf on Management of Data  1998 6 V  C r e st ana D ec 1997 I n f o r m al cor r e spondence 7 B  D unkel a nd N  Sopar k ar  D at a O r g ani zat i o n a nd A ccess for Ef\002cient Data Mining Technical report The University of Michigan Ann Arbor 1999 8 B  D unkel  N  Sopar k ar  J  S zar o and R  U t hur usam y  Systems for KDD From concepts to practice Journal of Future Generation Computer Systems  October 1997 9 A  S i l b er schat z  H  K or t h  a nd S Sudar s han Database System Concepts  McGraw-Hill New York third edition 1997  M  J Zaki  S  P ar t h asar at hy  M  O gi har a  a nd W  Li  N e w Algorithms for Fast Discovery of Association Rules In 3rd Int'l Conf on Knowledge Discovery and Data Mining  pages 283\226286 Newport California Aug 1997 


local support count X.sup must be smaller than the local threshold s x D Following from the discus sion in Subsection 3.3 X.supq is bounded by the value min\(maxsupq X s x D  1 Hence an upper bound of X.sup can be computed by the sum x.supj  jEX.large-sites 2 min\(mazsupq\(X s x Dq  1 q=l q+?X.large-sites In FDM-LPP Si calls p-upper-bound to compute an upper bound for X.sup according to the above for mula This upper bound can be used to prune away X if it is smaller than the global support threshold 0 As discussed before both FDM-LUP and FDM LPP may have less candidate sets than FDM-LP How ever they require more storage and communication messages for the local support counts Their efficiency comparing with FDM-LP will depend largely on the data distribution 5 Performance Study of FDM An in-depth performance study has been performed to compare FDM with CD We have chosen to im plement the representative version of FDM FDM LP and compare it against CD Both algorithms are implemented on a distributed system by using PVM Parallel Virtual Machine 6 A series of three to six RS/6000 workstations running the AIX system are connected by a 10Mb LAN to perform the experi ment The databases in the experiment are composed of synthetic data In the experiment result the number of candidate sets found in FDM at each site is between 10  25 of that in CD The total message size in FDM is between 10  15 of that in CD The execution time of FDM is between 65  75 of that in CD The reduction in the number of candidate sets and message size in FDM is very significant The reduction in execution time is also substantial However it is not directly proportional to the reduction in candidate sets and message size This is mainly due to the overhead of running FDM and CD on PVM What we have ob served is that the overhead of PVM in FDM is very close to that in CD even though the amount of mes sage communication is significantly smaller in FDM From the results of our experiments it is also clear that the performance gain of FDM over CD will be higher in distributed systems in which the commu nication bandwidth is an important performance fac tor For example if the mining is being done on a distributed database over wide area or long haul net work The performance of FDM-LP against Apriori in a large database is also compared. In that case the response time of FDM-LP is only about 20 longer Interpretation transaction mean size mean size of maximal potentially large itemsets number of potentially large itemsets Number of items Clustering size Pool size Correlation level Multiplying factor Parameter ITI III ILI N sq Ps Mf Cr Value 10 4 2000 1000 5-6 50  70 0.5 1260  2400 Table 5 Parameter Table than 1/n of the response time of Apriori where n is the number of sites This is a very ideal speed-up In terms of total execution time FDM-LP is very close to Apriori The test bed that we use has six workstations Each one of them has its own local disk, and its partition is loaded on its local disk before the experiment starts The databases used in our experiment are synthetic data generated using the same techniques introduced in 2 lo The parameters used are similar to those in lo Table 5 is a list of the parameters and their values used in our synthetic databases Readers not familiar with these parameters can refer to 2  In the following we use the notation Tx.Iy.Dm to denote a database in which D  m in thousands IT1  x and 111  y T10.14.D200K s  3 4 5 6 Number of Nodes FDM CD Figure 1 Candidate Sets Reduction n  3 4 5 6 5.1 Candidate Sets and Message Size Re duction The sizes of the databases in our study range from 200K to 600K transactions and the minimumsupport threshold ranges from 3 to 3.75 Note that the number of candidate sets at each site are the same in CD and different in FDM In our experiment we witnessed a reduction of 75  90 of candidate sets on 39 


T10.14.D200K, n  3 T10.14.D200K, n  3 60  I S 8 3.00 3.25 3.510 3.75 YO  I YO Minimum support FDM kCD  gs 3.00 3.25 3.50 3.75 Minimum support FDM CD Figure 4 Message Size Reduction Figure 2 Candidate Sets Reduction average at each site when FDM-LP is compared with CD In Figure 1 the average number of candidate sets generated by FDM-LP and CD for a 200K transaction database are plotted against the number of partitions FDM-LP has a 75  90 reduction in the candidate sets The percentage of reduction increases when the number of partitions increases This shows that FDM becomes more effective when the system is scaled up In Figure 2 the same comparison between FDM-LP and CD is presented for the same database with three partitions on different thresholds In this case, FDM LP experienced a similar amount of reduction T10.14.D200K s  30/0 I 150 100 50 0 3 4 5 6 Number of Nodos FDM CB Figure 3 Message Size Reduction n  3 4 5 6 The reduction in candidate sets should have a pro portional impact on the reduction of messages in the comparison Moreover as discussed before the polling site technique guarantees that FDM only requires O\(n messages for each candidate set which is much smaller than the O\(n2 messages required in CD In our experiment FDM has about 90 reduction in the total message size in all cases when it is compared with CD In Figure 3 the total message size in FDM and CD for the same 200K database are plotted against the number of partitions In Figure 4 the same compari son on the same database of three partitions with dif ferent support thresholds are presented Both results confirm our analysis that FDM-LP is very effective in cutting down the number of messages required T10.14.D200K s  3 90 E3 28  U 70 cc Q 8 a c 50 c xs w  I 3 4 5 6 Number of Nodes FDM CD Figure 5 Execution Time n  3 4 5 6 T10.14.D200K n  3 3.00 3.25 3.50 3.75 Minimum Support E-FDM A-CD Figure 6 Execution Time 5.2 Execution Time Reduction We have also compared the execution time between FDM-LP and CD The execution time of FDM-LP and CD on a 200K database are plotted against the number of partitions in Figure 5 FDM-LP is about 40 


25  35 faster than CD in all cases In Figure 6 the comparison is plotted against different thresholds for the same database on three partitions Again FDM LP is shown to have similar amount of speed-up as in Figure 5 n  3 D  60011 s  2 I Apriori I FDM-LP response time sec I 1474 I 387 I total execution time sec I 844.7 I 842.9 I Table 6 Efficiency of FDM-LP We have also compared FDM-LP on three sites against Apriori with respect to a 600K transactions database in order to find out its efficiency in large database The result is shown in Table 6 The re sponse time of FDM-LP is only slightly 20 larger than 1/3 of that of Apriori In terms of the total ex ecution time FDM-LP is very close to Apriori For a large database FDM-LP may have a bigger portion of the database residing in the distributed memory than Apriori Therefore it will be much faster than running Apriori on the same database in a single ma chine This shows that FDM-LP on a scalable dis tributed system is an efficient and effective technique for mining association rules in large databases The performance study has demonstrated that FDM generates a much smaller set of candidate sets and requires a significantly smaller amount of mes sages when comparing with CD The improvement in execution time is also substantial even though the overhead incurred from PVM prevents FDM from achieving a speed-up proportional to the reduction in candidate sets and message size Even though we have only compared CD with FDM-LP there is enough evidence to show that FDM is more efficient than CD in a distributed environment In the follow ing sections we will discuss our future plan of imple menting the other versions of FDM 6 Discussions In this discussion we will first discuss the issue of possible extension of FDM for fast parallel mining of association rules Following that we will discuss two other related issues 1 the relationship between the effectiveness of FDM and the distribution of data and 2 support threshold relaxation for possible reduction of message overhead The CD and PDM algorithms are designed for share-nothing parallel environment. In particular CD has been implemented and tested on the IBM SP2 machine In designing algorithm for parallel mining of association rules not only the number and size of messages required should be minimized but also the number of synchronizations which is the number of rounds of message communication CD has a simple synchronization scheme It requires only one round of message communication in every iteration Besides the second iteration PDM also has the same synchro nization scheme as CD If FDM was used in the paral lel environment it has a shortcoming even though it requires much less message passings then CD it needs more synchronizations However FDM can be modi fied to overcome this problem In fact in each itera tion the candidate set reduction and global pruning techniques can be used to eliminate many candidates and then a broadcast can be used to exchange the local support counts of the remaining candidates This ap proach will generate less candidate sets than CD and has the same number of synchronization Therefore it will perform better than CD in all cases Performance studies has been carried out in a 32-nodes IBM SP2 to study several variations of this approach and the result is very promising Another interesting issue is the relationship be tween the performance of FDM and the distribution of the itemsets among the partitions From both The orem 1 and Example 1 it is clear that the number of candidate sets decreases dramatically if the distribu tion of itemsets is quite skewed among the partitions If most of the globally large itemsets were locally large at most of the sites the reduction of candidate sets in FDM would not have been as significant In the worst case if every globally large itemset is locally large at all the sites the candidate sets in FDM and CD will be the same Therefore data skewness may improve the performance of FDM in general Special partitioning technique can be used to increase the data skewness to optimize the performance of FDM Some further study is required to explore this issue The last issue that we want to discuss is the pos sible usage of the relaxation factor proposed in ll In FDM if a site sends not only those candidate sets which are locally large but also those that are almost locally large to the polling sites the polling sites may have local support counts from more sites to perform the global pruning of candidate sets For example if the support threshold is lo every site can send the candidate sets whose local support counts exceed 5 to their polling sites In this case for some candi date sets their polling sites may receive local sup port counts from more sites than the no relaxation case Hence the global pruning may be more effec tive However there is a trade-off between sending more candidate sets to the polling sites and the prun ing of candidate sets at the polling sites More study is necessary on the detailed relationship between the relaxation factor and the performance of the pruning 7 Conclusions In this paper we proposed and studied an efficient and effective distributed algorithm FDM for mining association rules Some interesting properties between 41 


locally and globally large itemsets are observed which leads to an effective technique for the reduction of can didate sets in the discovery of large itemsets Two powerful pruning techniques local and global prun ings are proposed Furthermore the optimization of the communications among the participating sites is performed in FDM using the polling sites Sev eral variations of FDM using different combination of pruning techniques are described A representative version FDM-LP is implemented and whose perfor mance is compared with the CD algorithm in a dis tributed system The result shows the high perfor mance of FDM at mining association rules Several issues related to the extensions of the method are also discussed The techniques of can didate set reduction and global pruning can be inte grated with CD to perform mining in a parallel envi ronment which will be better than CD when consider ing both message communication and synchronization Further improvement of the performance of the FDM algorithm using the skewness of data distribution and the relaxation of support thresholds is also discussed Recently there have been interesting studies on the mining of generalized association rules multiple level association rules quantitative association rules etc Extension of our method to the min ing of these kinds of rules in a distributed or parallel system are interesting issues for future research Also parallel and distributed data mining of other kinds of rules such as characteristic rules 7 classification rules, clustering 9 etc is an important direction for future studies For our performance studies an im plementation of the different versions of FDM on an IBM SP2 system with 32 nodes has been carried out and the result is very promising References l R Agrawal and J C Shafer Parallel mining of association rules Design implementation and experience In IBM Research Report 1996 2 R Agrawal and R Srikant Fast algorithms for mining association rules In Proc 1994 Int Conf Very Large Data Bases pages 487-499 Santiago Chile, September 1994 3 R Agrawal and R Srikant Mining sequential patterns In Proc 1995 Int Conf Data Engi neering pages 3-14 Taipei, Taiwan March 1995 4 D.W Cheung J Wan V Ng and C.Y Wong Maintenance of discovered association rules in large databases An incremental updating tech nique In Proc 1996 Int\222l Conf on Data Engi neering New Orleans, Louisiana Feb 1996 5 U M Fayyad 6 Piatetsky-Shapiro P Smyth and R Uthurusamy Advances zn Knowledge Dis covery and Data Mining AAAI/MIT Press 1996 6 A Geist A Beguelin J Dongarra W Jiang R Manchek and V Sunderam PVM Parallel Virtual Machine A Users\222 Guide and Tutorial for Networked Parallel Computing MIT Press 1994 7 J Han Y Cai and N Cercone Data driven discovery of quantitative rules in relational databases IEEE Trans Knowledge and Data En gineering 5:29-40 1993 Discovery of multiple-level association rules from large databases In Proc 1995 Int Conf Very Large Data Bases pages 420-431 Zurich Switzerland Sept 1995 8 J Han and Y Fu 9 R Ng and J Han Efficient and effective cluster ing method for spatial data mining In Proc 1994 Int Conf Very Large Data Bases pages 144-155 Santiago Chile, September 1994 lo J.S Park M.S Chen and P.S Yu An effec tive hash-based algorithm for mining association rules In Proc 1995 ACM-SIGMOD Int Conf Management of Data pages 175-186 San Jose CA May 1995 ll J.S Park M.S Chen, and P.S Yu Efficient par allel mining for association rules In Proc 4th Int Conf on Information and Knowledge Manage ment pages 31-36 Baltimore Maryland Nov 1995 12 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases In Proc 1995 Int Conf Very Large Data Bases pages 432-443 Zurich Switzerland Sept 1995 13 A Silberschatz M Stonebraker and J D U11 man Database research Achievements and op portunities into the 21st century In Report of an NSF Workshop on the Future of Database Sys tems Research May 1995 14 R Srikant and R Agrawal Mining general ized association rules In Proc 1995 Int Conf Very Large Data Bases pages 407-419 Zurich Switzerland Sept 1995 association rules in large relational tables In Proc 1996 ACM-SIGMOD Int Conf Manage ment of Data Montreal Canada June 1996 15 R Srikant and R Agrawal Mining quantitative 42 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


