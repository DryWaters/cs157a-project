Adaptive and Resource-Aware Mining of Frequent Sets S Orlando P Palmerini R Perego E Sil~estri Dipartimento di Informatica Universitl Ca Foscan Venezia Italy Istituto ISTI Consiglio Nazionale delle Ricerche \(CNR\Pisa Italy 3Dipartimento di Informatica Universitl di Pisa, Italy Abstract The performance of an algorithm that mines frequent sets from transactional databases may severely depend on the specijic features of the data being analyzed More ow some architectural characteristics of the computa tional platform used  e.g the available main memory  can dramatically change its runtime 
behavior In this paper we present DCI Direct Count  Intersect an efjicient al gorithm for discovering frequent sets from large databases Due to the multiple heuristics strategies adopted DCI can adapt its behavior not only to the features of the specijic computing platform but also to the features of the dataset being mined so that it results very effective in mining both short and long patternsfrom sparse and dense datasets Fi nally we also discuss the parallelization strategies adopted in the design of ParDCI a distributed and 
multi-threaded implementation of DCI 1 Introduction Association Rule Mining \(ARM one of the most popu lar topic in the KDD field regards the extractions of asso ciation rules from a database of transactions D Each rule has the form X  Y where X and Y are sets of items itemsets such that X f Y  0 A rule X  Y holds in 2 with a minimum confidence c and a minimum support s if at least the c of all the transactions containing X also contains 
Y and XU Y is present in at least the s of all the transactions of the database In this paper we are interested in the most computationally expensive phase of ARM i.e the Frequent Set Counting FSC one During this phase the set of all thefiefrequent itemsets is built An itemset of k items \(k-itemset is frequent if its support is greater than the fixed threshold s i.e the itemset occurs in at least minsup transactions minsup  s/lOO n where n is the number of transactions in D The computational complexity of the FSC problem de 
rives from the exponential size of its search space P\(M i.e the power set of M where M is the set of items con tained in the various transactions of D A way to prune P\(M is to restrict the search to itemsets whose subsets are all frequent The Apriori algorithm 51 exactly exploits this pruning technique and visits breadth-first P\(M for counting itemset supports At each iteration k Apriori gen erates ck a set of candidate k-itemsets and counts the occurrences of these candidates in the transactions The candidates in ck for which the the minimum support con 
straint holds are then inserted into Fkr i.e the set of fre quent k-itemsets and the next iteration is started Other algorithms adopt instead a depth-first visit of P\(M 8 21 In this case the goal is to discover long frequent itemsets first thus saving the work needed for discovering frequent itemsets included in long ones Unfortunately while it is simple to derive all the frequent itemsets from the maximal ones the same does not hold for their supports which re quire a further counting step In the last years several vari ations to the original Apriori algorithm as well as many parallel implementations have been 
proposed We can rec ognize two main methods for determining the supports of the various itemsets present in P\(M a counting-based approach 3 5 10 13 8 1 71 and an intersection-based one 15 9 161 The former one also adopted in Apriori exploits a horizontal dataset and counts how many times each candidate k-itemset occurs in every transaction The tatter method on the other hand, exploits a vertical dataset where a tidlist i.e a list of transaction identifiers lids is associated with items or itemsets and itemset supports are determined through tidlist intersections The support of a k-itemset 
c can thus he computed either by a k-way in tersection i.e by intersecting the k tidlists associated with the k items included in c or by a 2-way intersection i.e by intersecting the tidlists associated with a pair of frequent k  I whose union is equal to c Recently an other category of methods i.e the panern growth ones have been proposed l 11 141 FP-growth ll is the hest representant of this kind of algorithms It is not based on 0-7695-1754-4/02 17.00 Q 2002 IEEE 338 


candidate generation as Apriori hut builds in memory a compact representation of the dataset, where repeated pat terns are represented once along with the associated repeti tion counters FP-growth does not perform well on sparse datasets t171 so the same authors recently proposed a new pattern-growth algorithm H-mine 141 based on an inno vative hypeI-structure that allows the in-core dataset to be recursively projected by selecting those transactions that in clude a given pattern prefix In this paper we discuss DCI Direct Count Intersect a new algorithm to solve the FSC problem We also in troduce ParDCI a parallel version of DCI, which explic itly targets clusters of SMPs Several considerations con cerning the features of real datasets to he mined and the characteristics of modem hwlsw system have motivated the design of DCI On the one hand transactional databases may have different peculiarities in terms of the correlations among items so that they may result either dense or sparse Hence a desirable characteristic of a new algorithm should be the ability to adapt its behavior to these features DCI which supports this kind of adaptiveness thus constitutes an innovation in the arena of previously proposed FSC al gorithms which often outperformed others only for spe cific datasets On the other hand, modern hwkw systems need high locality for effectively exploiting memory hierar chies and achieving high performances. Large dynamic data structures with pointers may lack in locality due to unstruc tured memory references Other sources of performance limitations may he unpredictable branches DCI tries to take advantages of modem systems by using simple array data structures, accessed hy tight loops which exhibit high spa tial and temporal locality In particular DCI exploits such techniques for intersecting tidlists which are actually repre sented as hit-vectors that can he intersected very efficiently with primitive hitwise and instructions Another issue re gards VO operations which must he carefully optimized in order to allow DM algorithms to efficiently manage large databases Even if the disk-stored datasets to he mined may he very large DM algorithms usually access them se quentially with high spatial locality so that suitable out-of core techniques to access them can he adopted, also taking advantage of prefetching and caching features of modern 0 6 DCI adopts these out-of-core techniques to access large databases, prunes them as execution progresses, and starts using in-core strategies as soon as possible Once motivated the design requirements of DCI we can now detail how it works As Apriori at each iteration DCI generates the set of candidates ck determines their sup ports and produces the set Fk of the frequent k-itemsets However DCI adopts a hybrid approach to determine the support of the candidates During its first iterations DCI exploits a novel counting-based technique, accompanied by an effective pruning of the dataset, stored to disk in horizon tal form. During the following iterations DCI adopts a very efficient intersection-based technique DCI stans using this technique as soon as the pruned dataset fits into the main memory DCI deals with dataset peculiarities by dynamically choosing between distinct heuristic strategies For exam ple when a dataset is dense identical sections appearing in several hit-vectors are aggregated and clustered in order to reduce the number of intersections actually performed Conversely, when a dataset is sparse, the runs of zero hits in the hit-vectors to he intersected are promptly identified and skipped We will show how the sequential implementa tion of DCI significantly outperforms previously proposed algorithms In particular, under a number of different tests and independently of the dataset peculiarities DCI results to be faster than Apriori and FP-growth By comparing our experimental results with the published ones obtained on the same sparse dataset we deduced that DCI is also faster than H-mine 14 DCI performs very well on both synthetic and real-world datasets characterized hy different density features i.e datasets from which, due to the differ ent correlations among items, either short or long frequent patterns can he mined The rest of the paper is organized as follows Section 2 describes the DCI algorithm and discusses the various adaptive heuristics adopted while Section 3 sketches the solutions adopted to design ParDCI, the parallel version of DCI. In Section 4 we report our experimental results Fi nally in Section 5 we present some concluding remarks 2 The DCI algorithm During its initial counting-based phase DCI exploits an out-of-core horizontal database with variable length records DCI by exploiting effective pruning techniques inspired by DHP 13 trims the transaction database as ex ecution progresses In particular a pruned dataset Dk+l is written to disk at each iteration k and employed at the next iteration Let mk and nk be the number of items and transactions that are included in the pruned dataset 9 where mk  mk+l and nk 2 nk+1 Pruning the dataset may entail a reduction in U0 activity as the algorithm pro gresses, hut the main benefits come from the reduced com putation required for subset counting at each iteration k due to the reduced number and size of transactions As soon as the pruned dataset becomes small enough to fit into the main memory DCI adaptively changes its behavior builds a verrical layout database in-core, and starts adopt ing an intersection-based approach to determine frequent sets Note however that DCI continues to have a level wise behavior At each iteration DCI generates the can didate set ck by finding all the pairs of k  1 that are included in Fk-l and share a common k  2 339 


prefix Since Fk is lexicographically ordered the var ious pairs occur in close positions and candidate gener ation is performed with high spatial and temporal local ity Only during the DCI counting-phase ck is further pruned by checking whether also all the other subsets of a candidate are included in Fk Conversely during the intersection-based phase since our intersection method is able to quickly determine the support of a candidate item set we found much more profitable to avoid this further check While during its counting-based phase DCI has to maintain Ck in main memory to search candidates and in crement associated counters this is no longer needed dur ing the intersection-based phase As soon as a candidate k itemset is generated DCI determines its support on-the-fly by intersecting the corresponding tidlists This is an impor tant improvement over other Apriori-like algorithms which suffer from the possible huge memory requirements due to the explosion of the size of ck I DCI makes use of a large body of out-of-core techniques so that it is able to adapt its behavior also to machines with limited main memory Datasets are readwritten in blocks to take advantage of YO prefetching and system pipelin ing a The outputs of the algorithm e.g the various fre quent sets Fb are written to files that are map-ped into memory during the next iteration for candidate generation 2.1 Counting-based phase Since the counting-based approach is used only for few iterations, in the following we only sketch its main features Further details about DCI counting technique can be found in 12 where we proposed an effective algorithm for min ing shott patterns In the first iteration similarly to all FSC algorithms DCI exploits a vector of counters which are di rectly addressed through item identifiers Fork 2 2 instead of using complex data structures like hash-trees or prefix trees DCI uses a novel Direct Count technique that can be thought as a generalization of the technique used fork  1 Thetechniqueusesaprejirtable,PRELlXb[],ofsize 7 In parlicular each entry of PREF  is associated with a distinct orderedprefir of two items. For k  2 PREmk I can directly contain the counters associated with the var ious candidate 2-itemsets while for k  2 each entry of PREFIXk  contains the pointer to the contiguous sec tion of ordered candidates in ck sharing the same prefix To permit the various entries of PREmk  to be directly accessed we devised an order preserving minimal perfect hash function This prefix table is thus used to count the support of candidates in ck as follows For each transac tion t  Itl    t we select all the possible 2-prefixes of all k-subsets included in t We then exploit PwHxk I to find the sections of ck which must he visited in order to check set-inclusion of candidates in transaction t 2.2 Intersection-based phase Since the counting-based approach becomes less effi cient as k increases 15 DCI starts its intersection-based phase as soon as possible Unfortunately the intersection based method needs to maintain in memory the vertical representation of the pruned dataset So at iteration k k 2 2 DCI checks whether the pruned dataset Dk may fit into the main memory When the dataset becomes small enough its vertical in-core representation is built on the fly while the transactions are read and counted against ck The intersection-based method thus starts at the next iteration The vertical layout of the dataset is based on fixed length records tidlists stored as bit-vectors The whole verti cal dataset can thus be seen as a bidimensional hit-array V\222D  1 whose rows correspond to the hit-vectors associ ated with non pruned items. Therefore the amount of mem ory required to store VD   is mk x nk bits At each iteration of its intersection-based phase DCI computes Fk as follows For each candidate k-itemset c we and-intersect the k bit-vectors associated with the items included in c k-way intersection and count the 1\222s present in the resulting bit-vector If this number is greater or equal to minsup we insert c into Fk Consider that a hit-vector intersection can he carried out efficiently and with high spa tial locality by using primitive bitwise and instructions with word operands As previously stated this method does not require ck to be kept in memory we can compute the sup port of each candidate c on-the-fly as soon as it is generated The strategy above is in principle highly inefficient, he cause it always needs a k-way intersection to determine the support of each candidate c Nevertheless a caching pol icy could be exploited in order to save work and speed up our k-way intersection method To this end DCI uses a small \223cache\224 buffer to store all the k  2 intermediate in tersections that have been computed for the last candidate evaluated Since candidate itemsets are generated in lexico graphic order, with high probability two consecutive candi dates e.g c and d share a common prefix Suppose that c and c\222 share a prefix of length h 2 2 When we process c\222 we can avoid performing the first h  1 intersections since their result can be found in the cache To evaluate the effectiveness of our caching policy we counted the actual number of intersections carried out by DCI on two different datasets BMS a real-world sparse dataset and connect-4 a dense dataset the characteristics of these two datasets are reported in Table 1 We com pared this number with the best and the worst case The best case corresponds to the adoption of a 2-way intersec tion approach which is only possible if we can fully cache the tidlists associated with all the frequent k  1 in Fk-1 The worst case regards the adoption of a pure k way intersection method, i.e a method that does not exploit 340 


caching at all Figure l.\(a plots the results of this analy sis on the sparse dataset for support threshold s  0.06 while Figure 1 h regards the dense dataset mined with sup port threshold s  80 In both the cases the caching pol icy of DCI turns out to he very effective since the actual number of intersections performed results to be very close to the hest case Moreover DCI requires orders of magni tude less memory than a pure 2-way intersection approach thus better exploiting memory hierarchies Caut-BUs ww.m b Figure 1 Evaluation of DCI intersection caching policy We have to consider that while caching reduces the num her of tidlist intersections we also need to reduce intersec tion cost To this end further heuristics. differentiated w.r.1 sparse or dense datasets, are adopted by DCI In order to ap ply the right optimization, the vertical dataset is tested for checking its density as soon as it is built In particular, we compare the bit-vectors associated with the most frequent items i.e the vectors which likely need to be intersected several times since the associated items occur in many can didates If large sections of these hit-vectors turn out to be identical, we deduce that the items are highly correlated and that the dataset is dense. In this case we adopt a specific heuristics which exploits similarities between these vectors Otherwise the technique for sparse datasets is adopted In the following we illustrate the two heuristics in more detail Sparse datasets Sparse or moderately dense datasets originate hit-vectors containing long runs of 0's To speedup computation while we compute the intersection of the hit-vectors relative to the first two items c1 and c2 of a generic candidate itemset c  cl c2    ck E C we also identify and maintain information about the runs of 0's appearing in the resulting hit-vector stored in cache The further intersections that are needed to determine the sup port of c as well as intersections needed to process other k itemsets sharing the same 2-item prefix will skip these runs of O's so that only vector segments which may contain 1's are actually intersected Since information about the runs of 0's are computed once and the same information is reused many times this optimization results to be very effective Moreover, sparse and moderately dense datasets offer the possibility of further pruning vertical datasets as compu tation progresses The benefits of pruning regard the re duction in the length of the hit-vectors and thus in the cost of intersections Note that a transaction i.e a column of VD can be removed from the vertical dataset when it does not contain any of the itemsets included in 9 This check can simply he done hy or-ing the intersection hit-vectors computed for all the frequent k-itemsets However we ob served that dataset pruning is expensive, since vectors must he compacted at the level of single hits Hence DCI prunes the dataset only if turns out to he profitable i.e if we can obtain a large reduction in the vector length and the num her of vectors to he compacted is small with respect to the cardinality of ck b Figure 2 Evaluation of DCI optimization heuristics for sparse and dense datasets Dense datasets If the dataset turns out to he dense we expect to deal with a dataset characterized by strong corre lations among the most frequent items. This not only means that the hit-vectors associated with the mostfrequent items contain long runs of 1's hut also that they turn out to he very similar The heuristic technique adopted by DCI for dense dataset thus works as follows A we reorder the columns of the vertical dataset in order to move identical segments of the bit-vectors associated with the most fre quent items to the first consecutive positions B since each 341 


Dslsyt T25110010K TZSIZOolWK mk110.p8m10t 4Wk,30~p16mlt 120q8mlk I connect4 I Publicly available dM~e dsmaset wiul 130 items aod about 60K trmsactionr The maximal wansaction size is 45 I DeretipliO\224 Syntheticdamserwith IK items and IOKvanractions[ll Theave~g~sizeofmnraclionsirZS,andtheaveragesi2eofulemaximal tentirlly frequent itemsls is IO Synthetic dataset with IOK ilem and IWK Uansaclims Ill The average size of lmsaclions is 25 and ule average size of the maximal potentially frequent itemsets is 20 IOK iiemr and 4WK manractions The werage size of iraninctions is IO and ule average size of the maximal p~ienlidly hrqucnr itemsets is 8 Synthetic damel created with Ute IBM dalarer generator SI IK item and 4WK BansaELions The average size of transa~tionr is 30 and ule average si2 of Le maximal pxen~ially frequent itemsets is 16 Synthetic dalaret created with the IBM dalasl generator 151 With ais notation we identify a series of synuleue datael5 chanietenzed by IK iems The average msaelion ice is 20 and he weram size of maximal otentially frequent itemreis is 8 The numberof msactioni is varied for scaling maruremnis hblicly avnilable sparse dafarei also known as Gmrllr 497 i~m and 59K an~actioni conmining click-swam data from an e-commprcewebsire gazeile.com BMS candidate is likely to include several of these most frequent items we avoid repeatedly intersecting the identical seg ments of the corresponding vectors This technique may save a lot of work because 1 the intersection of identical vector segments is done once 2 the identical segments are usually very large, and 3 long candidate itemsets presum ably contains several of these most frequent items The plots reported in Figure 2 show the effectiveness of the heuristic optimizations discussed above in reducing the average number of hitwise and operations needed to in tersect a pair of bit-vectors In particular Figure 2.\(a re gards the sparse BMS dataset mined with support threshold s  0.06 while Figure 2.\(b regards the dense dataset connect-4 mined with support threshold s  80 In both cases we plotted the per-iteration cost of each bit-vector in tersection in terms of hitwise and operations when either our heuristic optimizations are adopted or not The two plots show that our optimizations for both sparse and dense datasets have the effect of reducing the intersection cost up to an order of magnitude Note that when no optimizations are employed the curves exactly plot the bit-vector length in words Finally from the plot reported in Figure 2.\(a we can also note the effect of the pruning technique used on sparse datasets Pruning has the effect of reducing the length of the hit-vectors as execution progresses On the other hand when datasets are dense the vertical dataset is not pruned so that the length of bit-vectors remains the same for all the DCI iterations 3 ParDCl In the following we describe the different paralleliza tion techniques exploited for the counting and intersection based phases of ParDCI the parallel version of DCI Since our target architecture is a cluster of SMP nodes in both phases we distinguish between inrra-node and inrer-node levels of parallelism At the inter-node level we used the message-passing paradigm through the MF\222I communica tion library while at the intra-node level we exploited multi threading through the Posix Thread library. A Count Disrri burion approach is adopted to parallelize the counting-based phase, while the intersection-based phase exploits a very ef fective Condidore Distriburion approach 4 The counting-based phase At the inter-node level the dataset is statically split in a number of partitions equal to the number of SMP nodes available The size of partitions depend on the relative powers of nodes At each iteration k an identical copy of ck is independently generated by each node Then each node p reads blocks of transactions from its own dataset partition vp,k performs subset count ing and writes pruned transactions to vp,k+l At the end of the iteration an all-reduce operation is performed to update the counters associated to all candidates of Ck and all the nodes produce an identical set Fk At the intra-node level each node uses a pool of threads each holding a private set of counters associated with candi dates They have the task of checking in parallel candidate itemsets against chunks of transactions read from Dp,k At the end of each iteration a global reduction of coun ters take place, and a copy of Fk is produced on each node The intersection-based phase During the intersection based phase a Candidate Distribution approach is adopted at both the inter- and intra-node levels This pardlelization schema makes the parallel nodes completely independent inter-node communications are no longer needed for all the following iterations of ParDCI 342 


  1 J 1   I Figure 3 Total execution times for DCI Apriori and FP-growth on various datasets as a function of the support threshold Let us first consider the inter-node level and suppose that the intersection-based phase is started at iteration  1 Therefore at iteration the various nodes build on-the-fly the bit-vectors representing their own in-core portions of the vertical dataset Before starting the intersection-based phase, the partial vertical datasets are broadcast to obtain a complete replication of the whole vertical dataset on each node The frequent set Fx i.e the set computed in the last counting-based iteration is then partitioned on the ba sis of itemset prefixes A disjoint partition Fp,x of Fx is thus assigned to each node p where U Fp,x  Fp It is worth remarking that this partitioning entails a Candidate Distribution schema for all the following iterations, accord ing to which each node p will be able to generate a unique CE k  k independently of all the other nodes where C,P Ckp  0 ifp  p andU,CE  Cb At the intra-node level a similar Candidate Distribution approach is employed but at a finer granularity by using dynamic scheduling to ensure load balancing 4 Experimental Results The DCI algorithm is currently available in two versions a MS-Windows one and a Linux one ParDCI which ex ploits the MPICH MPI and thepthread libraries, is currently available only for the Linux platform We used the MS Windows version of DCI to compare its performance with other FSC algorithms For test comparisons we used the FP-growth algorithm1 and the Christian Borgelt's imple mentation of Apriori2 For the sequential tests we used a Windows-"I workstation equipped with a Pentium II 350 MHz processor 256 MB of RAM memory and a SCSI 2 disk For testing ParDCl performance we employed a small cluster of three Pentium II 233MHz 2-way SMPs, for a total of six processors Each SMP is equipped with 256 MBytes of main memory and a SCSI disk. For the tests we used both synthetic and real datasets by varying the mini mum support threshold s The characteristics of the datasets used are reported in Table 1 DCI performances and comparisons Figure 3 reports the total execution times obtained running Apriori FP growth and our sequential DCI algorithm on some datasets described in Table 1 as a function of the support'Ihreshold s In all the tests conducted DCI outperforms FP-growth with speedups up to 8 Of course DCI also remarkably out performs Apriori in some cases for more than one order of magnitude For connect-4, the dense dataset, the curve of Apriori is not shown due to the relatively too long execu tion times. Note that accordingly to 17 on the real-world sparse dataset BMS \(also known as Gazelle Apriori turned out to be faster than FP-growth To overcome such had per formance results on sparse datasets, the same authors of FP growth recently proposed a new pattern-growth algorithm H-mine  By comparing our experimental results with We adrnowledge Pmf Jiawei Han for kindly providing us the latesf http://f"zzy.CS.uni-magdeburg.de/-borgelt fully optimized binary version of FP-growth 343 


the published execution times on the BMS dataset we de duced that DCI is also faster than H-mine. For s  0.06 we obtained an execution time of about 7 sec while H mine completes in about 40 sec on a faster machine The encouraging results obtained with DCI are due lo both the efficiency of the counting method exploited during early iterations and the effectiveness of the intersection based approach used when the pruned vertical dataset fits into the main memory For only a dataset namely T25IIODlOK FP-growth turns out to be slightly faster than DCI for s  0.1 The cause of this behavior is the size of C which in this specific case results much larger than the final size of F3 Hence DCI has to carry out a lot of useless work to determine the suppon of many candidate itemsets which will eventually result to be not frequent In this case FP-growth is faster than DCI since it does not require can 11102 a 01 e I umbr*il.-dr,"mml a bpd Mnepl>l an 2  i f  I81 I L  2-~-4 LP 1 2 i 8 P  LL--,/&L P-..I_-_ 8 YB-t    13*e  72 am2 m.1 OI 3.8 2 4T I 0 Figure 4 Total execution times of a DCI and b FP-growth on datasets in the series t20.p8mlk s  0.5 on a PC equipped with different RAM sizes as a function of the num ber of transactions ranging from 1OOK to 2M We also tested the scale-up behavior of DCI when both the size of the dataset and the size of RAM installed in the PC vary The datasets employed for these tests belong to the series t20$3mlk see Table I mined with support thresh old s  0.5 while the available RAM was changed from 64MB to 512MB by physically plugging additional mem ory into the PC main hoard Figure 4.\(a and 4.\(h plot several curves representing the execution times of DCI and FP-growth respectively as a function of the number of transactions contained in the dataset processed Each curve plotted refers to a series of tests conducted with the same F'C equipped with a different amount of memory As it can be seen from Figure 4.\(a DCI scales linearly also on ma chines with a few memory Due to its adaptiveness and the use of efficient out-of-core techniques it is able to modify its behavior in function of the features of the dataset mined and the computational resources available For example in the tests conducted with the largest dataset containing two millions of transactions the in-core intersection-based phase was started at the sixth iteration when only 64MB of RAM were available, and at the third iteration when the available memory was 512MB On'the other hand the re sults reponed in Figure 4.\(h show that FP-growth requires much more memory than DCI and is not able to adapt itself to memory availability For example in the tests conducted with 64MB of RAM FP-growth requires less than 30 sec onds to mine the dataset with 2OOk transactions but when we double the size of the dataset to 400k transactions FP growth execution time becomes 1303 seconds more than 40 times higher due to an heavy page swapping activity e in1 a I I lam   a   1  P   ____   L   8 111111 N F-...a b Figure 5 a Dense dataset connect-4 completion times of DCI and ParDCl vary ing the minimum support threshold b Speedup for sparse datasets 1000K 2000K and 3000K with s  1.5 Performance evaluation of ParDCI We evaluated ParDCl on both dense and sparse datasets First we com pared the performance of DCI and ParDCl on the dense dataset connect-4 for which we obtained very good speedups Figure 5.\(a\plots total execution times as func tions of the support thresholds s ParDCI-2 corresponds to the pure multithread version running on a single 2-way SMP while ParDCI-4 and ParDCI-6 also exploit inter node parallelism and run respectively on two and three 2-way SMPs For what regard sparse datasets we used 344 


the synthetic dataset series identified as t50-pp32mlk in Table 1 We varied the total number of transactions from looOk to 3000k In the following we will identify the vari ous synthetic datasets on the basis of their number of trans actions i.e IOOOk 2000k. and 3000k. Figure 54b plots the speedups obtained on the three synthetic datasets for a given support threshold s  1.5 as a function of the number of processors used. Consider that since our cluster is com posed of three 2-way SMPs we mapped tasks on processors always using the minimum number of SPMP nodes e.g when we used 4 processors we actually employed 2 SMP nodes This implies that experiments performed on either 1 or 2 processors actually have identical memory and disk resources available, whereas the execution on 4 processors benefit from a double amount of such resources Accord ing to the tests above ParDCl showed a speedup that in some cases is close to the optimal one Considering the re sults obtained with one or two processors, one can note that the slope of the speedup curve is relatively worse than its theoretical limit due to resource sharing and thread imple mentation overheads at the inter-node level Nevertheless when additional SMPs are employed the slope of the curve improves The strategies adopted for partitioning dataset and candidates on ow homogeneous cluster of SMPs suf ficed for balancing the workload. In our tests we observed a very limited imbalance The differences in the execution times of the first and last node to end execution were always below the 0.5 5 Conclusions DCI uses different approaches for extracting frequent patterns counting-based during the first iterations and intersection-based for the following ones Adaptiveness and resource awareness are the main innovative features of the algorithm On the basis of the characteristics of the dataset mined DCI chooses at run-time which optimiza tion to adopt for reducing the cost of mining. Dataset prun ing and effective out-of-core techniques are exploited dur ing the counting-based phase, while the intersection-based phase works in core, and is staned only when the pruned dataset can fit into the main memory As a result our algo rithm can manage efficiently also on machines with limited physical memory very large datasets from which due to the different correlations among items, either short or long frequent patterns can be mined The experimental evaluations demonstrated that DCI significantly outperforms Apriori and FP-growth on both synthetic and real-world datasets. In many cases the perfor mance improvements are impressive Moreover ParDCI the parallel version of DCI exhibits excellent scaleups and speedups on our homogeneous cluster of SMPs The va riety of datasets used and the large amount of tests con ducted permit us to state that the performances of DCI are not influenced by dataset characteristics and that our optimizations are very effective and general To share our efforts with the data mining community we made the DCI binary code available for research purposes at http:IIwww.miles.cnuce.cnr.it/-.palmerildatam/oCI References Ill R.C.Aganval.C.C.Aggwal,andV.V.V.Prasad ATreePmjection Algorithm for Generation of Frequent Itemsets JPDC 2MK Special Issue on High Performance Data Mining 2 R C. Aganual C C. Agganual and V.V.V Basad Oepth first gen eration of long patterns In Pmc ofrhe 61h ACM SlCKDD In Conf on Knowledge Discovery and Dolo Mining pages 108-1 18,2000 3 R Agawal H Mannila R Srikant H Toivonen and A Inkeri Verkamo Fast Discovery of Association Rules in Large Databases In Advances in Knowledge Discovery and Dam Mining pages 307-328 AAA1 Press 1996 4 R Agrawd and J C Shafer Parallel mining of association ruler IEEE TKDE 8:962-969,996 151 R Aerawal and R Srikant Fast Aleorithms for Minine Association  I I I Rules in Large Databases In Pmc ofrhc 201h VIDE Conf pages 487499 1994 161 R Baraglia D Laforenma S Orlando P Palmerini and R Perego Implementation issues in the design of U0 intensive data mining ap plications on dusters of workstations In Pmc offhe 3rd HPDM Workhop IPDPS-Zwo Cnncun Mexico pages 350-357 LNCS I80 Spinger-Verlag 2wO 7 Y Bastide R Tamil N Pwquier G Stumme and L Lakhal Min ing frequent patterns with counting inference ACM SICKDD Erplo rations Newslemr 2\(2 December 200 81 R J Bayardo Jr Efficiently Mining Long Patterns from Databases In Pm ofths ACM SIGMOD Inr Conf on Managamen ofDara pager 85-93 Seattle. Washington USA 1998 191 Brian Dunkel and Nandit Soparkar. Data organization and access for efficient data mining In Pmc ofrhe 151h lm Conf on Dam Engi neering pages 522-529 Sydney Ausualia 1999 IEEE Computer Society IO E H Ha G Karypis and Kumar V Scalable Parallel Data Mining for Association Rules IEEE TKDE 12\(3 MayNune 2000 I I J Ha I Pei. and Y Yin Mining Frequent Patterns without Candi dale Generation In Pmc ofrhe ACM SICMOD Inc Conf on Mon q.gpmen o/Do:a pages 1-12 Dallas. Texas USA 2WO I21 S Orlando P Palmerini and R Perego Enhancing the Apriori Al gorithm for Frequent Set Counting In Pmc oflhr jrd Inf Con on Dam Warehousing and Knowledge D;xove DaWaK 28331 INCS 2114.pages71-82.Munich.Germany.2001 I31 J S Park M.-S Chen and P S Yu An Effective Hash Based Al gorilhm far Mining Association Rules In Pmc ofrhr 1995 ACM SIGMODlnt Conf on Mnna~rmmtnfDnro pages 175-186 1995 1141 1 Pei I Ha H. Lu S Nishio and D Tang S amd Yang H-Mine Hyper-Structure Mining of Frequent Patterns in Large Databases In Pmc ofrhe 2001 IEEE ICDM CO San Jose CA USA 2001 IS A Savasere E. Omiecinski and S B Navathe An Efficient Algo rithm for Mining Association Rules in Large Databases In Pmc of the Zlfh VLDB Conf pages 432444 Zurich. Switzerland 1995 I61 M J Ui Scalable algorithms for association mining IEEE TKDE 12:372-390 MayNune 2000 171 Z Zheng R Kohavi and L Mason Real World Performance of Association Rule Algorithms In Pmc ofKDD-28331 201 345 


n I31 41 51 61 71 8 I 9 Graph diameter q5\(SCCn Average number of lateral links E Average number of MI local links MI\(loc Average number of MB local links MB\(Zoc I Graph size n  1 e n I 12 I 72 I 480 I 3,600 I 30,240 I 282,240 I 2,903,040 6 8 16 19 1.500 2.583 3.683 4.783 0.667 1.500 3.200 5.000 0.833 1.222 1.925 2.337 1.500 3.000 Average number of local links L Average distance d\(SCC 2.722 5.125 7.337 5.306 8.808 12.121 Table 1 Average distance of SCC graphs under minimal routing 300000 250000 g 200000 3 E L 150000   100000 50000 0 erage number of MB local links is concerned. Also observe that for 3 5 n 5 4 the greedy routing algorithm performs as well as the minimal routing algorithm Besides, our re sults indicate that the performance of these algorithms is quite similar for 5 5 n 5 9 which makes the less complex greedy routing algorithm particularly attractive Average costs of paths produced by the three routing al gorithms are summarized in Table 2 The random routing algorithm has a complexity of O\(n and performs reason ably well on the average Utilization of such an algorithm may however result in variations in the average cost of routes up to the worst-case values shown in Table 2  Minimal routing  andom rout. \(worst case     n 3 Minimal Greedy Random routing rout rout Theor Simul Worst-case 3.000 3.000 3.000 3.084 3.167 I1 I I I I I 4 5 I I I I I 5.306 5.305 5.500 5.514 5.694 8.808 8.812 9.261 9.264 9.775 Table 2 Average costs vs routing algorithms Figure 6 shows distribution curves comparing the three routing algorithms in the case of an SCC graph A point 01 NI in one of these curves indicates that the corre sponding routing algorithm will compute a route of cost DI to the identity for NI nodes in the SCC graph The aver age distribution for the random routing algorithm is shown but the results for that algorithm may actually vary from the minimal to the worst-case distributioncurves due to the non deterministic nature of the algorithm It is also interesting to observe that the greedy routing algorithm provides a dis tribution curve which is close to that of the minimal routing algorithm presenting however a smaller complexity 6 Considerations on wormhole routing 3 In this section we briefly describe how the algorithms presented in the paper cam be combined with wormhole routing 6 which is a popular switching technique used in parallel computers All three algorithms can be used with wormhole routing when implemented as source-based routing algorithms  111 In source-based routing tlhe source node selects the entire path before sending the packet Because the processing delay for the routing algorithm is incurred only at the source node it adds only once to the communication latency and can be viewed as part of the start-up latency Source-based routing however has two disadvantages 1 each packet must carry complete information about its path in the header which increases the packet length and 2 the path cannot be changed while the packet is being routed which precludes incorporating adaptivity into the routing algorithm Distributed routing eliminates the disadvantages of source-based routing by invoking the routing algorithm in each node to which the packet is forwarded ll Thus the decision on whether a packet should be delivered to the local processor or forwarded on an outgoing link is done 451 


locally by the routing circuit of a node Because the routing algorithm is invoked multiple times while a packet is being routed the routing decision must be taken as fast as pos sible From this viewpoint it is important that the routing algorithm can be easily and efficiently rendered in hardware which favors the random routing algorithm over the greedy and minimal routing algorithms Besides being the most complex algorithm discussed in this paper the minimal routing algorithm includes a feature which precludes its distributed implementation in associa tion with wormhole routing namely its backtracking mech anism Distributed versions of the random and greedy al gorithms, however, can be used in combination with worm hole routing A near-minimal distributed routing algorithm which supports wormhole routing can be obtained by re moving the backtracking mechanism from Alg 3 Such an algorithm is likely to have computational complexity and average cost that lie between those of the greedy and the minimal routing algorithm Due to its non-deterministic nature the random routing algorithm also seems to be a good candidate for SCC net works employing distributed adaptive routing  1 I Adap tivity is desirable for example if the routing algorithm must dynamically respond to network conditions such as conges tion and faults Some degree of adaptivity is also possible in the greedy and minimal routing algorithms which in some cases can decide between paths of equal cost 7 Conclusion This paper compared the average cost and the complex ity of three different routing algorithms for the SCC graph We divided routes into three components \(lateral links MI local links and MB local links and showed that only the number of MB local links may be affected by the routing algorithm being considered Exact expressions for the aver age number of lateral links and the average number of MI local links were presented Also an upper bound for the average number of MZ local links was derived considering a random routing algorithm As a result a tight upper bound on the average distance of the SCC graph was obtained Simulation results for a random a greedy and a minimal routing algorithm were presented and compared with theo retical values The complexity of the proposed algorithms is respectively O\(n O\(n2 and O\(n3 where n is the dimensionality of the SCC grap.h The results under mini mal routing produce exact numerical values for the average distance of SCC for 3 5 n 5 9 Results for the greedy algorithm match those of the min imal algorithm for 3 2 n 5 4 The greedy algorithm also performs close to minimality for 5 5 n 5 9 and is an in teresting choice due to its O\(n2 complexity The random routing algorithm has an O\(n complexity and performs fairly well on the average but may introduce additional MB local links in the route under worst-case conditions Finally we discussed how each of the routing algorithms can be used in association with the wormhole routing switch ing technique Directions for future research in this area in clude an evaluation of requirements for deadlock avoidance e.g number of virtual channels References l S B Akers,D. HarelandB Krishnamurthy,\223TheStarGraph An Attractive Altemative to the n-Cube,\224 Proc Int\222l Con Pal Proc 1987 pp 393-400 2 M M Azevedo N Bagherzadeh and S Latifi 223Broadcasting Algorithms for the Star-Connected Cycles Interconnection Network,\224 J Pal Dist Comp 25,209-222 1995 3 M M Azevedo N Bagherzadeh and S Latifi 223Embed ding Meshes in the Star-Connected Cycles Interconnection Network,\224 to appear in Math Mod. and Sci Comp 4 M M Azevedo N Bagherzadeh and S Latifi 223Fault Diameter of the Star-Connected Cycles Interconnection Net work,\224 Proc 28th Annual Hawaii Int\222l Con5 Sys Sci Vol 11 Jan. 3-6 1995 pp 469-478 SI W.-K Chen M F M Stallmann andE E Gehringer 223Hy percube Embedding Heuristics An Evaluation,\224 Int\222l J Pal Prog Vol 18 No 6 1989 pp 505-549 6 W J Dally and C I Seitz 223The Torus Routing Chip,\224 Dist Comp Vol 1 No 4 1986 pp 187-196 7 K Day and A Tripathi,\223A Comparative Study ofTopologica1 Properties of Hypercubes and Star Graphs,\224 IEEE Trans. Pal Dist Sys Vol 5 No 1 Jan. 1994 pp 31-38 8 D E Knuth The Art of Computer Programming Vol I Addison-Wesley 1968 pp 73 pp 176-177 9 S Latifi 223Parallel Dimension Permutations on Star Graph,\224 IFIP Trans A Comp Sei Tech 1993 A23 pp 191-201 lo S Latifi M M Azevedo and N Bagherzadeh 223The Star Connected Cycles A Fixed-Degree Interconnection Net work for Parallel Processing,\224 Proc Int\222l Con5 Pal Proc  1 11 L M Ni and P K McKinley 223A Survey of Wormhole Rout ing Techniques in Direct Routing Techniques,\224 Computer Feb 1993 pp 62-76  121 E P Preparata and J Vuillemin 223The Cube-Connected Cy cles A Versatile Network for Parallel Computation,\224 Comm ACM Vol 24 No 5 May 1981 pp 300-309  131 Y Saad and M H Schultz 223Topological Properties of Hy percubes,\224IEEE Trans Comp Vol 37 No 7 July 1988 pp 14 S Shoari and N Baghenadeh 223computation of the Fast Fourier Transform on the Star-Connected Cycle Network,\224 to appear in Comp  Elec. Engl 1996 15 P Vadapalli and P K Srimani 221\223ho Different Families of Fixed Degree Regular Cayley Networks,\224 Proc Int\222l Phoenix Con Comp Comm Mar 28-31,1995 pp 263-269 1993 Vol 1 pp 91-95 867-872 452 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


