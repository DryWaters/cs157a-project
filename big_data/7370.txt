SIBA: A Fast Frequent I tem Sets Mining Algorithm Based on Sampling and Improved Bat Algorithm  WEI Ying, HUANG Jian, ZHANG Zhongjie, KONG Jiangtao College of Mechatronics Engineering and Automation National University of Defense Technology Changsha, China, zjiezhang@hotmail.com  Abstract The adaptability of the classical topk frequent item sets mining algorithms is unsatisfied when being applied to some large datasets with both large scales of transactions and items. In this work, an algorithm called SIBA \(sampling, improved bat algorithm \(BA\roposed to solve this problem. SIBA applies BA and improves it by cloud model to search frequent item sets 
from a large number of items rapidly, and builds the sub-dataset in every iteration to reduce the scanning cost. Based on four open access datasets, it is compared with Apriori, FP-growth, and other heuristic algorithm such as PSO \(Particle swarm optimization\nd GA \(Genetic algorithm\. The experimental results show that when being applied to frequent item sets mining SIBA is faster than Apriori and FP-growth, and meanwhile, it is more robust than PSO and GA Keywords—data mining; association rule; bat algorithm  cloud model  sampling I   I NTRODUCTION  
Topk frequent item sets returning the k most frequent item sets from the transaction dataset, which is always a research hotspot in data mining. The classic algorithms in this realm are Apriori, FP-growth [1-3 n d th e i r i m prov e m e n ts [4 6  Despite of the wide application, these methods can’t perform well on those datasets with high number of transactions and items  w h ich m a ke so m e  trad ition al operation s in f e a s ib le  such as scanning all transactions even once, testing every candidate item set and so on. Ther efore, some researches have been done to face them Firstly, some heuristic algorithms [8-9 plied  w h ic h  simulates the nature system n e edn t test e v er y can d id ate item sets and give good solution in a little cost Mou r ad  applies quantum swarm evolutionary algorithm to this field 
 Ku o et al pr opos e a m i n i n g  m e t h od bas e d on parti c le swarm optimization algorithm and Alatas et al prese n t a  way to mine frequent item sets by rough particle swarm optimizatio All of  t h e m h a v e ac h i e v ed th e h i g h  performance. However, without sampling or other methods they can’t reduce the time cost by scanning transactions, and the large searching space reduces the global optimization performance of some classic heuristic algorithms Then, some sampling ways are applied [14-18  T o iv on en  proposes an algorithm, which builds a candidate of frequent item sets the probabilities according to the sample   Some progressive sampling way, which keeps increasing the sample size until the stop condition is satisfied, have been studied and proposed [18, 20-21  Ho w e v e r, despite th e y  
successfully reduce the cost to scan transactions, the problem brought by the large scale of items is not solved Therefore, a new method, SIBA \(Sampling, improved bat algorithm\, is proposed in this work, which has the following characteristics  SIBA applies BA \(bat algorith m\ to this field [22-26   which is a new heuristic algorithm  Cloud model is applied to improve BA  SIBA builds sub-dataset by sampling in every iteration to reduce the transactions scanning cost SIBA needs a fixed length of the frequent item set to code them more conveniently. So, it is a fast mining algorithm to mine topk frequent l item sets 
II  B ASIC C ONCEPTS  A  Bat Algorithm Bat algorithm simulates the behavior of micro-bats based on the following idealized rules [2   1\ Any bat  ies randomly with velocity 1   iid vv i V and pulse frequency min max  i freq freq freq at 1   iid pp i P  varying the rate of pulse emission min max  
i rate rate rate and the loudness min max  i LLL  2\ the food is closer, the i rate will be bigger and the i L  will be lower The rules of the updating of a bat are shown as follows [22  min max min    ii freq freq freq freq     i freq ii i VV PP  
 iii PPV   where 0,1 i is a random value according to a uniform distribution  P is the current global best solution among all the n bats, which is chosen through the fitness function  F i P  The pseudo code of BA can be summarized as follows  


Input: The fitness function F  Output: The best solution Method 1  Initialize the bat population  1 2  in i P and i V  2  Define pulse frequency i rate at i P  3  Initialize pulse rates i rate and the loudness i L  4  For 1 iter  1  Generate new solutions by adjusting frequency, and updating velocities and positions by Eq. \(1-3 2  If i rand rate  1  Select a solution among the best solutions 2  Generate a position around the selected best one 3  End if 4  Generate a new solution by flying randomly 5  If   i rand L F F i PP  1  Accept the new solutions 2  Increase i rate and reduce i L  6  End if 7  Rank the bats and find the current best  P  5  End for 6  Return  P  B  2 nd normal cloud model Let U be a universal set describe d by precise numbers, and C be the qualitative concept containing Ex  En and He related to U wherein Ex is the most representative sample of a concept En is the granularity scale of the concept and He is the uncertainty of En  If t h ere is a ra n d o m  n u m b er xU  which is a realization of the qualitative concept C and satis  es 2  xNExy where 2   yNEnHe The certainty degree of x on U is   2 2  2  xEx y xe     4 The distribution of x on U is a 2nd-order normal cloud which is denoted    CExEnHe and x is a cloud drop [2  Given Ex  En and He the 2 nd normal cloud generator denoted CG  Ex, En, He, n  pr odu ce n cloud drops whose pseudo code is as follow Input Ex  En  He and n  Output n cloud drops Method 1  For 1 in  1  Generate 2   i yNEnHe and 2   ii xNExy  2  Calculate the certain degree  i x  2  End for 3  Return n cloud drops C  Problem definition Let   Db tid A V f be a transaction dataset, where 12    tid t t is the universe of transactions 12    Aaa is the universe of items V is the value domain, which actually is 0,1}, and   fta is the function from tid A to V For aA and t tid   1 fta if and only if t contains a  For A and t tid  t s transaction vector under  denoted t is defined as 12            card fta fta fta t  i a  9 For t tid and A  t contains to the degree  denoted t if  1     card t     For A the support of denoted  Sup is defined as  1      card t tid t Sup card tid  10 For A let  b be the set of those item sets whose supports are higher than  Sup The top k frequent l item sets under Db is defined as        kl Top Db A b k card l 11  III  T HE SIBA ALGORITHM  A  The code of item sets For   Db tid A V f let 1 2 r A be the set of indexes of A s elements. Any l item set can be coded as 12   l ii i I  ir iA   12 Gven i i if 1 i i or  i icardA it will be modified by     i card A mod i card A or   1 i mod i card A where mod  a  b e remainder of a/b If i i is not an integer i i will be modified to i i   B  The improved BA based on cloud model Considering the large se arching space in topk frequent l  item sets mining, SIBA uses the 2 nd normal cloud model to describe the position of a bat, which simulates the disorder microscopic movements of a real bat [2 sho w n i n F i g 1   The movements of a real bat The movements of a real bat The movements of a bat in classic BA The movements of a bat in classic BA      Fig. 1  The movements of a real bat and a bat in classic BA For Db   iiii Batc En He Ep represents a bat, called bat cloud, where i Ep is the expected position ii En freq and 0.3 ii He freq For ji p Ep     ij j i i CpEnHe is a 2 nd normal cloud. The new position updated model are shown in Fig. 2 and the functions are explained in Eq. \(14-17\, where i D  is the set of the drops of i Batc and ji D d is actually an item set Dn i is the number of the drops of i Batc which is a random number according to uniform distribution. In this work 0,7 i Dn  gbest d is the historical global best drop of all the bat clouds F is the fitness function of i Batc  min max min    ii freq freq freq freq 14 0.3  iiiii D CG freq freq Dn Ep 15 


   i gbest i freq ii VV Epd 16  iii Ep Ep V 17         The drop in step 1 The drop in step 1                                                           The drop in step 2 The drop in step 2    The drop in step 3 The drop in step 3      The historical global best drop The historical global best drop  Fig. 2. The position updated model of a bat in SIBA C  The sub-dataset produc ed in every iteration Suppose the mining process of SIBA is in the iter th  iteration, for   Db tid A V f to reduce the scale of mining object, the subset of tid which is iter tid will be sampled to build   iter iter Db tid A V f Any bat cloud drop calculates its fitness through iter Db but Db The  iter card tid  is set by the user depending on the real problem. In this thesis   500 iter card tid  Despite the sampling size of every iteration is chosen, the sub-dataset are updated in every iteration so that the probability of losing the real frequent item sets is relatively low. However, on the other hand, the sampling in every iteration brings the unstable solutions. SIBA proposes a way to evaluate the stability of every solution and to choose gbest d  in every iteration For any item set I let  iter F I be the fitness of I on the iter th iteration. The variance of I is evaluated by  2 1 1      iter iter i i Var F F iter III 18 where   F I is the mean value from 1  F I to  iter F I  The lower  iter Var I means the higher stability of I  Let iter ibest d be the local best drop of i Batc on the iter th  iteration, and iter gbest d be the global best drop on the iter th  iteration 12     e iter iter iter gbest gbest gbest gbest Cd dd d is the set of the e best global drops chosen from the first iteration to the iter th  iteration. The gbest d on the iter th iteration satisfies    jj gbest ibest iter gbest iter gbest Cd Var Var ddd 19 In this work e 5 D  The whole pseudo of SIBA Based on the improved BA, the sampling in every iteration and the rough fitness, for Db the whole pseudo of SIBA is shown as follow Input Db  N bat  N iter  k and l  Output   kl Top Db  Method 1  For 1 mk  1  For 1 bat jN  1  Initialize j freq  j rate  j L and j Batc  2  Modify j Ep in j Batc  2  End for 3  Build 0 Db  4  Find  Ep For Ep  00    FF Ep Ep  5    gbest Cd Ep  6  For 1 iter iter N  1  Build iter Db  2  Update the variance of  gbest Cd s item sets 3  Choose the gbest d  4  For 1 bat jN  i  Update j freq and j D  ii  Modify the item sets of j D  iii  Update the fitness of s j D item sets iv  Update j V and j Ep  v  Modify j Ep  vi  If j rand rate  a  1   j gbest l Ep d  b  Modify j Ep  vii  End if viii  Choose iter jbest d from j D  ix  If     iter j jbest gbest rand L Sup Sup dd  a  Accept j Ep  b  Increase j rate and decrease j L  x  End if 5  End for 6  Update gbest Cd  7  End for 8  Add gbest d to   kl Top Db  2  End for IV  T HE E XPERIMENTS AND D ISCUSSIONS  Several datasets, shown in Table 1, were used to evaluate the performance of SIBA algorithm. They were downloaded from http  mi.ua.ac.be/data/. All of the experiments were conducted in the environment of Microsoft Windows XP using a compatible computer with Inter i7 3.4GHz and 3.49GB RAM and were coded by the Matlab T ABLE 1  T HE INFORMATION OF DATASETS  Dataset Average length Items Trans kosarak 8.1 41270 990002 mushroom 23 120 8124 connect 43 130 67557 retail 10.3 16470 88162 A  The experiment 1 In experiment 1, both Apriori and SIBA were used to mine the top-3 frequent 3 item sets of the datasets in table 1. In SIBA, the number of iteration was set to 1000, the population size of bat was set to 20 min freq  min rate and min L were all set to 0, and max freq  max rate and max L were all set to 1. The minimum support thresholds of the Apriori were shown in 


table 2. Apriori was used to mine every dataset once and SIBA was used to mine every dataset 10 times. During the experiment, the solutions given by Apriori, the best and worst solutions given by SIBA, and the error counts given by SIBA were recorded. The error count is the times in which the solutions given by SIBA didn’t equal the solutions given by Apriori. The results of experiment 1 are shown in table 2 Table 2 indicates that SIBA has high performance of accuracy, which can always get the accurate solutions from any dataset in table 1 in 10 times. Meanwhile, all the error rates of SIBA were less than or equal to 10%. For the high speed performance proved in experiment 2, such low error rates can be accepted to mining the large dataset B  The experiment 2 In experiment 2, Apriori, FP-growth and SIBA were respectively used to mine the top-3 frequent 3 item sets from the datasets in table 1. The speeds of them were compared. All algorithms were used to mine the datasets 10 times Parameters of SIBA were the same with experiment 1. The results of experiment 2 are shown in table 3 Table 3 indicates that SIBA is faster than Apriori and FPgrowth when it was used to mine ‘kosarak’, ‘connect’ and retail’. Especially, when the minimum support threshold was respective 6% on ‘kosarak 85% on ‘connect’ and 2% on retail’, it can save the time by one order of magnitude. The high speed performance of SIBA can be explained as follows 1\hen the length and dimension of a dataset is very large. Some costs brought by the traversal operation from Apriori and FP-growth such as scanning dataset, building FPtree and computing the combination of items, make the problem become NP-hard  T ABLE T HE RESULTS OF SIBARS AND A PRIORI  Dataset Min-sup Apriori SIBARS Best Worst Error Count kosarak 6 3,6,11 3,6,11 3,6,11 0 1,6,11 1,6,11 1,6,11 1,3,6 1,3,6 1,3,6 mushroom  30 34,85,86 34,85,86 34,85,86 1 34,85,90 34,85,90 85,86,90 85,86,90 85,86,90 34,86,90 connect 85 91,109,127 91,109,127 91,109,127 1 75,91,109 75,91,109 75,91,109 75,91,127 75,91,127 75,109,127 retail 2 39,41,48 39,41,48 39,41,48 1 38,39,48 38,39,48 38,39,48 32,39,48 32,39,48 38,41,48 T ABLE T HE TIME COST BY A PRIORI   F P GROWTH AND SIBARS Dataset Min-sup The Average Time Consumption \(s Apriori FP-growth SIBARS kosarak 8 159.78 106.18 8.15 7 189.2 137.29 7.78 6 272.22 201.98 7.86 mushroom  50 2.72 1.78 7.53 40 6.78 4.27 7.54 30 13.60 10.78 7.86 connect 95 67.74 49.25 8.24 90 134.02 103.19 7.71 85 220.73 164.86 8.45 retail 3 20.68 14.98 7.59 2.5 25.00 17.30 7.97 2 40.47 30.01 8.53 T ABLE 4  R ESULTS OF THE SPEED AND ACCURACY PERFORMANCES  TESTS ON SIBARS  PSO  GA AND THE CLASSIC BA  Database Error count The Average Time Consumption\(s SIBARS PSO GA The classic BA SIBARS PSO GA The classic BA kosarak 1 4 3 2 8.13 5.12 6.16 6.38 mushroom 0 2 2 1 7.59 6.01 6.03 6.48 connect 1 3 4 3 7.78 5.95 5.43 6.85 retail 2 4 5 2 8.04 5.99 6.43 6.56 


2\pling and the improved BA make SIBA not have to scan all the transactions and check all the candidate item sets which saves much time However, when the minimum support threshold is 50% or 40% and the object dataset is mushroom’, SIBA is slower than other algorithms, which is explained as follows 1\When the dataset is small such as ‘mushroom’, and the minimum support threshold is high, the problem of mining topk frequent item sets becomes easy and the advantage of SIBA on NP-hard problem, which is compared to Apriori and FP-growth, is gone 2\he fixed iteration steps and the fixed population size of SIBA make some unnecessary computations when the problem becomes easy So SIBA has better speed perf ormance when applied to the large dataset but not suited to the small one C  The experiment 3 In experiment 3, through mining the top-3 frequent 3 item sets from the datasets in table 1, the accuracy and speed performances of SIBA, PSO, GA and the classic BA were compared. Every algorithm was used to mine every dataset 10 times. The error counts and average consuming time of every algorithm were recorded. PSO, GA and the classic BA took  Sup to be the fitness function and built the sub-dataset with 500 transactions by sampling before mining In this experiment, the population size of every algorithm was set to 20, and the iteration number were set to 1000. For PSO, the inertia weight was set to 1, and both the cognitive and social parameters were set to 0.5. For GA, both the crossover and mutation probabilities were set to 0.5. For both the classic BA and SIBA min freq  min rate and min L were all set to 0, and max freq  max rate and max L were all set to 1. The results of experiment 4 are shown in table 4 Table 4 indicates that despite PSO, GA and the classic BA have higher speeds than SIBA However, their error rates are higher than SIBA. The analysis are given as follows 1\like any particle in PSO, gene in GA or bat in the classic BA, every bat in SIBA is a 2 nd normal cloud, and the computation of cloud generator, which generates the bat cloud drops, may consume more time 2\ore SIBA, the classic BA itself has already possessed better accuracy performance for many non-liner problems than many other heuristic algorithms  w h ic h i s a  good foundation 3\ The large number of items results in the large searching space, which probably reduces the search ability of PSO, GA and the classic BA 4\ Through the bat cloud and the stability assessment method, SIBA not only improves the global optimization performance but also avoids th e impact of changeable data and noisy data brought and increased by sampling V  C ONCLUSION   SIBA not only saves the time in the topk frequent item sets mining job, but also has a high accuracy. Firstly, the application of BA and the sampling in every iteration evidently pick up the speed of topk frequent item sets mining in large database. Then, the 2 nd normal cloud and solution stability assessment improve the accuracy of BA R EFERENCES  1  J.W. Han, M. Kamber, and J. Pei, “Data Mining Concepts and Techniques,” Third Edition, Beijing: China Machine Press, 2012, 157179 2  R Agrawal, T Imielinski T, and A Swami, “Mining association rules between sets of items in large databases,” Proceedings of 1993 ACM SIGMOD International Conference on Management of Data, 1993: 207216 3  J.W. Han, J. Pei, W.W. Yin, and R.Y. Mao, “Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach Data Mining and Knowledge Discovery, 2004, 8:53-87 4  A Salam, and H.K.M. Sikandar, “Mining topk frequent patterns without minimum support threshold,” Knowledge Information System, 2012 30:57–86 5  K.T. Chuang, J.L. Huang, and M.S. Chen, “Mining topk frequent patterns in the presence of the memory constraint,” VLDB, 2008 17:1321–1344 6  Z.H. Deng, “Fast mining Top-Rankk frequent patterns by using Nodelists,” Expert Systems with Applications, 2014, 41: 1763–1768 7  Z.M. Lu, C. Liu, S. Massinanke, C.X. Zhang, and L. Wang, “Clustering method based on data division and partition,” Journal of Central South University, 2014, 21: 213 222 8  K.G. Srinivasa, K.R. Venugopal, and L.M. Patnaik, “A self-adaptive migration model genetic algorithm for data mining applications Information Sciences, 2007, 177: 4295–4313 9  Z. Karimi, and H. Abolhassani, “A new method of mining data streams using harmony search,” Journal of Intelligent Inf. Sys., 2012, 39:491 511   O. Hasancebi, T. Teke, and O. Pekcan  A bat-inspired algorithm for structural optimization,” Computers and Structures, 2013, 128:77–90   Y. Mourad, “A Quantum Swarm Evolutionary Algorithm for mining association rules in large databases,” Journal of King Saud UniversityComputer and Information Sciences, 2011, 23:1-6   R.J. Kuo, C.M. Chao, and Y.T. Chiu, “Application of particle swarm optimization to association rule mining,” Applied Soft Computing, 2011 11:326-336   B. Alatas, and E. Akin. “Rough particle swarm optimization and its applications in data mining,” Soft Computation, 2008, 12:1205–1218   V.T. Chakaravarthy, V. Pandit, and Y. Sabharwa, “Analysts of sampling techniques for association rule mining,” Proc. of ICDT, 2009: 276–283   B. Chen, P. Haas, and P. Scheuermann, “A new two-phase sampling based algorithm for discovering association rules,” Proc. of KDD 2002:462–468   E. Cohen, N. Grossauga, and H. Kaplan, “Processing topk queries from samples,” Computer Network, 2008, 52\(14\:2605–2622   Y.R. Li, and R.P. Opalan, “Effective sampling for mining association rules,” Proceedings of AUS-AI, 2004: 391–401   A. Pietracaprina, and F. Vandin, “Mining topk frequent itemsets through progressive sampling,” Data Mining Knowl. Discovery, 2010 21:310-326   H. Toivonen, “Sampling large databases for association rules,” Proc. of VLDB, 1996: 134–145   G.H. John, and P. Langley, “Static versus dynamic sampling for data mining,”  Proceedings of KDD, 1996: 367–370   S. Parthasarathy, “Efficient progressive sampling for association rules Proceedings of ICDM, 2002: 354–361   YANG Xin-she, and GANDOMI A H, “Bat algorithm: a novel approach for global engineering optimization,” International Journal for Computer-Aided Engineering and Software, 2012, 29\(5\: 464-483   X.S. Yang, “A New Metaheuristic Bat-Inspired Algorithm,” Nature Inspired Cooperative Strategies for Optimization, 2012, 284:65-74 


  D.Y. Li, C.Y. Liu, and W.Y. Gan, “A New Cognitive Model: Cloud Model,” International Journal of Intelligent Systems, 2009, 24: 357–375   G.Y. Wang, “Generic normal cloud model,” Info. Sciences, 2014, 280 1–15   J.D. Alteringham, T. Macowat, and L. Hammond, “Bats: Biology and Behaviour,”  Oxford: Oxford Univesity Press, 1996  


Retail is a sparse dataset  consists of product sales data from retail stores Each transaction in the Retai dataset represents purchase information from one consumer at a time The details of the datasets are presented in gure 6 The programming language used to implement all the three algorithms is java and run in 3.3 GHz Intel processor 4 Gbyte memory and Windows 7 32bit OS Figs 7…9 show results of runtime experiments regarding the real and synthetic datasets shown in gure 6 In these gures we can observe that IHT-growth outperforms the others in all of the cases IHT-growth uses the proposed header tree structure to store the 1-frequent items instead of the older header table to minimize access times to search items As a result its advantages have a positive effect on reducing runtime in whole experiments Especially in the case of Retail dataset the difference of runtime between our algorithm and the others is much more than the other datasets  In all experiments FP-growth shows the worst performance Note that IHT-growth method can be used with any fptree mining to improve its ef“ciency We suggested here two methods to implement the new algorithm If we dont know the number of items in advance the First method is suggested In this experiments we used the second method By using the second method we can create a more ef“cient BSHTree because all the items with highest frequency will be appeared on the top of the BSHTree By using this method the run time can be improved by minimizing the searching time of items while sorting out the transactions VI CONCLUSION This study proposes an ef“cient transaction processing method during the transaction scanning time By applying the ef“cient binary search tree the mining time drastically reduced The new transaction sorting method is also improves the performance of mining The experimental results show that our algorithm outperforms the fp-growth and IFP-growth two well known and widely used algorithms Here we used a BSHTree after the rst scan to store the items with support count While using the BSHTree we used the actual names of items as keys This method can be applied to improve the mining process with any frequent itemset mining algorithm which is using a header table R EFERENCES  Jia wei H an Jian Pei and Y iwen Y in Mining Frequent P atterns without CandidateGeneration,SIGMOD 00 Proceedings of the 2000 ACM SIGMOD international conference on Management of data.Pages 1-12  Gw angb umPyun a Unil Y u n a  K eun Ho Ryu  E f cient frequent pattern mining based on Linear Pre“x tree Knowledge-Based Systems 55 2014 125…139  Y uh-JiuanTsay a T ain-Jung Hsu a Jing-Rung Y ub,FIUT  A n e w method for mining frequent itemsets,Information Sciences 179 2009 1724 1737  K e-Chung Lin I-En Liao  Zhi-Sheng Chen An impro v e d frequent pattern growth method for mining association rules Expert Systems with Applications 38 2011 5154…5161  F an-Chen Tseng An adapti v e approach to mining frequent itemsets ef“ciently Expert Systems with Applications 39 2012 13166…13172  R Agra w al T  Imielinski A.N Sw ami Mining association rules between sets of items in large databases in Proceedings of the ACMSIGMOD Conference on Management of Data pages 1993 pp 207…216  R Agra w al R Srikant F ast Algorithms for Mining Association Rules very Large Data Bases\(VLDB 1994 487499  Xiaobing Liu K u n Zhai W itold Pedrycz An impro v e d association rules mining method,Expert Systems with Applications 39 2012 13621374  Qiao yongwei,Y ang Hui Dong T ingjian,Research On QAR Data Mining Method Based On Improved Association Rule,Physics Procedia 24 2012 1514-1519  T  Hu S.Y  Sung H Xiong Q Fu Disco v ery of maximum length frequent itemsets Information Sciences 178 1 2008 69-87  G Lee U Y un K Ryu Sliding windo w based weighted maximal frequent pattern mining over data streams Expert Systems with Applications 41 2 2014 694-708  S.K T anbeer  C.F  Ahmed B.S Jeong Y  Lee Ef cient single-pass frequent pattern mining using a pre“x-tree Information Sciences 179 5 2008 559583  V S Tseng C.W  W u B.E Shie P S Y u  UP-Gro wth an ef cient algorithm for high utility itemset mining Knowledge Discovery and Data mining KDD 2010 253-262  T  W u  Y  Chen J han Re-e xamination of interestingness measures in pattern mining a uni“ed framework Data Mining and Knowledge Discovery DMKD 21 3 2010 371-397  J Han H Cheng D Xin X Y an Frequent pattern mining current status and future directions Data Mining and Knowledge Discovery DMKD 15 1 2007 55-86  Zaki Mohammed J Naren Ramakrishnan and Lizhuang Zhao Mining frequent boolean expressions application to gene expression and regulatory modeling International Journal of Knowledge Discovery in Bioinformatics IJKDB 1.3 2010 68-96 1084 2015 International Conference on Advances in Computing Communications and Informatics ICACCI 


375 6  R. Meo, G. Psaila, & S. Ceri 223A new SQL-like operator for mining association rules,\224 in Proceedings of the 22 nd International Conference on Very Large Data Bases Conference \(VLDB\2221996  Bombay, India, September 1996, pp. 122-133 7  H. C. Tjioe, & D. Taniar, \223Mining association rules in data warehouses,\224 International Journal of Data Warehousing and Mining vol. 1, no. 3, 2005, pp. 28-62 8  T. Imielinski, L. Khachiyan, & A. Abdulghani, \223Cubegrades generalizing association rules,\224 Data Mining and Knowledge Discovery vol. 6, issue 3, 2002, pp. 219-257 9  R. Ben Messaoud, R. S. Loudcher O. Boussaid, & R. Missaoui 223Enhanced mining of associati on rules from data cubes,\224 in Proceedings of the 9 th ACM International Workshop on Data Warehousing and OLAP \(DOLAP\2222006 Arlington, VA, 2006 pp. 11-18 10  R. Ben Messaoud, R. S. Loudcher O. Boussaid, & R. Missaoui 223OLEMAR: an online environment for mining association rules in multidimensional data,\224 Data Mining and Knowledge Discovery Technologies vol. 2, 2007, pp. 1-36 11  M. T. Fisun, G. V. Gorban, \223Research and implementation syntactic algorithms create OLAP-cubes,\224 Transactions of Kherson National University no. 2\(38\, 2010, pp. 110-117. \(in Ukrainian 12  M. T. Fisun, G. V. Gorban, \223Models and methods of construction of OLAP systems for object-ori ented databases,\224 Information Technology and Computer System s, no. 1 \(209\, 2014, pp. 41-45 in Russian 13  G. V. Gorban, \223Application of B*-trees for creating and calculating of OLAP-cubes using combinatorial algorithm,\224 Technological Audit of Production and Reserves no. 5/4 \(13 2013, pp. 10-12. \(in Ukrainian  


paying a price of a slightly worse ratio for subsets Taken together the price of having more subsets is preferred because subsets contain only items actually in the assembly while superset and overlap patterns also contain unrelated items The 002rst and second row of 002gure 8 correspond to the instance and the pattern-based approach for graded synchrony The third corresponds to the instance-based approach for binary synchrony Comparing the diagrams for unrelated patterns our graded method detects all injected patterns 050\002rst and second rows\051 while the binary method also produces unrelated pattern In 050Borgelt et al 2015\051 it is demonstrated that the instance-based approach yields slightly better results than the pattern approach However this approach does not consider the precision of synchrony Surprisingly using only the pattern-based approach with a graded notion of synchrony yields a better ratio for overlap and superset patterns 7 CONCLUSIONS In this paper we presented a method to detect frequent synchronous patterns in event sequences using a graded notion of synchrony for mining patterns in the presence of imprecise synchrony of events constituting occurrences and selective participation 050incomplete occurrences\051 Our method adapts methods presented in the literature to tackle selective participation using binary synchrony especially the instancebased approach which looks at instances of patterns to improve the detection by removing instances that are likely chance events checking the precision of synchrony of these instances We demonstrate in our experiments that using a graded notion of synchrony for support computation helps to simplify the detection of selective participation because a pattern-based approach yields better results or at least equally good results as an instance-based approach This is a considerable advantage since identifying the individual pattern instances is costly and thus it is desirable to avoid it ACKNOWLEDGMENTS The work presented in this paper was partially supported by the Spanish Ministry for Economy and Competitiveness 050MINECO Grant TIN2012-31372\051 and by the Principality of Asturias through the 2013-2017 Science Technology and Innovation Plan 050Programa Asturias CT1405206\051 and the European Union through FEDER funds REFERENCES Abeles M 0501982\051 Role of the cortical neuron Integrator or coincidence detector Israel Journal of Medical Sciences  18\0501\051:83\22692 Borgelt C 0502012\051 Frequent item set mining In Wiley Interdisciplinary Reviews 050WIREs\051 Data Mining and Knowledge Discovery  pages 437\226456 050 J Wiley  Sons Chichester United Kingdom 2 Borgelt C Braune C and Loewe K 0502015\051 Mining frequent parallel episodes with selective participation In Proc 16th World Congress of the International Fuzzy Systems Association 050IFSA\051 and 9th Conference of the European Society for Fuzzy Logic and Technology 050EUSFLAT\051 IFSA-EUSFLAT2015  Gijon Spain Atlantis Press Borgelt C and Picado-Muino D 0502013\051 Finding frequent synchronous events in parallel point processes In Proc 12th Int Symposium on Intelligent Data Analysis 050IDA 2013 London UK\051  pages 116\226126 Berlin/Heidelberg Germany Springer-Verlag Dudoit S and van der Laan M J 0502008\051 Multiple Testing Procedures with Application to Genomics  Springer New York USA Ezennaya-G 264 omez S and Borgelt C 0502015\051 Mining frequent synchronous patterns with a graded notion of synchrony In Proc 16th World Congress of the International Fuzzy Systems Association 050IFSA\051 and 9th Conference of the European Society for Fuzzy Logic and Technology 050EUSFLAT\051 IFSA-EUSFLAT2015  pages 1338\2261345 Gijon Spain Atlantis Press ISBN 050on-line\051 978-94-62520-77-6 Hebb D O 0501949\051 The Organization of Behavior  J Wiley  Sons New York NY USA Kernighan W and Ritchie D 0501978\051 The C Programming Language  Prentice Hall K 250 onig P Engel A K and Singer W 0501996\051 Integrator or coincidence detector the role of the cortical neuron revisited Trends in Neurosciences  19\0504\051:130\226137 Louis S Borgelt C and Gr 250 un S 0502010\051 Generation and selection of surrogate methods for correlation analysis In Gr 250 un S and Rotter S editors Analysis of Parallel Spike Trains  pages 359\226382 Springer-Verlag Berlin Germany Mannila H Toivonen H and Verkamo A 0501997\051 Discovery of frequent episodes in event sequences In Data Mining and Knowledge Discovery  pages 259\226 289 Springer New York NY USA 1\0503\051 Picado-Muino D and Borgelt C 0502014\051 Frequent itemset mining for sequential data Synchrony in neuronal spike trains Intelligent Data Analysis  18\0506\051:997\226 1012 Picado-Muino D Borgelt C Berger D Gerstein G L and Gr 250 un S 0502013\051 Finding neural assemblies with frequent item set mining Frontiers in Neuroinformatics  7 Picado-Muino D Castro-Le 264 on I and Borgelt C 0502012\051 Fuzzy frequent pattern mining in spike trains In Proc 11th Int Symposium on Intelligent Data Analysis 050IDA 2012 Helsinki Finland\051  pages 289\226300 Berlin/Heidelberg Germany Springer-Verlag 


Rossum G V 0501993\051 Python for unix/c programmers copyright 1993 guido van rossum 1 In Proc of the NLUUG najaarsconferentie Dutch UNIX users group  Torre E Picado-Muino D Denker M Borgelt C and Gr 250 un S 0502013\051 Statistical evaluation of synchronous spike patterns extracted by frequent item set mining Frontiers in Computational Neuroscience  7 Tsourakakis C Bonchi F Gionis A Gullo F and Tsiarli M 0502013\051 Denser than the densest subgraph Extracting optimal quasi-cliques with quality guarantees In Proc 19th ACM SIGMOD Int Conf on Knowledge Discovery and Data Mining 050KDD 2013 Chicago IL\051  pages 104\226112 New York NY USA ACM Press Zaki M J Parthasarathy S Ogihara M and Li W 0501997\051 New algorithms for fast discovery of association rules In Proc 3rd Int Conf on Knowledge Discovery and Data Mining 050KDD 1997 Newport Beach CA\051  pages 283\226296 Menlo Park CA USA AAAI Press 


