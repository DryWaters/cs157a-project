A Conceptual Infrastructure for Knowledge Discovery Process with Granularity  Ding Pan Center for Business Intelligence Research, School of Management Jinan University, Guangzhou, China tpanding@jnu.edu.cn   Abstract Knowledge discovery has received more and more attention from the business community for the last few years One of the most important and challenging problems in it is the definition of discovery process model, which are well understood, efficiency, and quality of outcome. A conceptual infrastructure for knowledge discovery process is proposed with business understanding, model selection and domain knowledge integration in evolving database environment. The 
corresponding layered architecture creates an automatable process and communication mechanism to incorporate known knowledge and data granules into discovery process, through ontology service facility. A higher order mining method embedded in the process is proposed, to achieve monitoring and identifying changes statically and tracing trends dynamically. Finally, implementation techniques of basic data structure and simplified mining algorithms are discussed Keywords-discovery process; conceptual infrastructure higher order mining I   I NTRODUCTION  The data mining \(DM\ and knowledge discovery in 
databases \(KDD\movements have been based on the fact that the true value is not in storing the data, but rather in our ability to extract useful reports and to find interesting trends and correlations. The set of DM processes used to extract and verify patterns in data is the core of the knowledge discovery process. These processes comprise many steps which involve data selection, data preprocessing, data transformation, DM, and interpretation and evaluation of patterns. During the last a few years, most research in DM focuses on the development of new algorithms or improvement in the speed or the accuracy of the existing ones [1  Rel a tiv e l y  littl e  h a s b e en p u b lish e d a b o u t  
theoretical frameworks of knowledge discovery In 1996, Fayyad et al. presented a process model that resulted from interactions between researchers and industrial data analysts [2 T h e  m ode l di d n o t a d d res s pa rt i c u l a r D M  techniques, but rather provided support for the complicated and highly iterative process of knowledge generation. Since then, several different process models have been developed These process models consist of multiple steps executed in a sequence, which often includes loops and iterations [3,4    The CRISP-DM \(CRoss Industry Standard Process for Data Mining\process model includes six steps: business understanding, data understanding, data preparation 
modeling, evaluation and deployment  I n  ge ner a l   t h e CRISP-DM is the most suitable for novice data miners working on industrial projects, due to the intuitive, industryapplications-focused description, and has been already acknowledged and relatively widely used in both research and industrial communities. One of the main limitations of the CRISP-DM life-cycle representation is that it is essentially sequential and linear Agrawal et al. showed an active data mining process where the mining algorithm was applied to each of the partitioned data set and rules were induced [5   S p i l i o p o u l o u  et al. investigated the utility of inducting rules from results of other mining routines [6 S e ve ra l  re se ar c h e s ha v e  
concentrated on fusion of domain knowledge with DM system [7  Wa ng et al  d i sc us se d so m e  r e ce nt  a c h i e ve m e nt s in domain-oriented data-driven data mining \(3DM\odel 8  H o w e v e r li ttl e at ten t i o n  h a s b e en p a i d t o es ta b lish m en t of a general infrastructure for supporting share of domain knowledge In this paper we attempt to explore knowledge discovery process model to make data mining for evolving database environments, and to incorporate the domain knowledge and data granules with the KDD process. Our conceptual infrastructure for a data mining system is built upon three essential components: Ontology, KDD process model and Knowledge engineering. It also provides a higher order 
mining method to identify and trace changes in data or patterns II  P ROCESS M ODEL AND I NFRASTRUCTURE  We present a model and an infrastructure for the KDD process, which is based on the concept of active, continuous mining and designed to integrate known knowledge and granules in order to support automatic discovery process A  Mining ontologies Ontology provides a framework for sharing conceptual models about known knowledge. A model that allows agents to deal with explicit, declaratively represented ontologies 
was described in the FIPA Ontology Service [9   Th e k n o w n  knowledge may be is shared and reused by utilizing ontology service It is necessary to set up conceptual frameworks into which data and datasets, databases and known knowledge might be classified. The families of available models need to be classified, and the relation of model-families to data-types needs to be made clear. We need abstract models of data and their relations, and of models and their relations. It is by knowing the ontological structure of the data available, and 
2010 International Conference on Artificial Intelligence and Computational Intelligence 978-0-7695-4225-6/10 $26.00 © 2010 IEEE DOI 10.1109/AICI.2010.260 99 
2010 International Conference on Artificial Intelligence and Computational Intelligence 978-0-7695-4225-6/10 $26.00 © 2010 IEEE DOI 10.1109/AICI.2010.260 99 


the models that may be applied to it, that we can reasonably make a choice of the model to use A necessary high-level structure to business and data exploration process is the trio of interrelated processes of business-understanding, data-understanding and the availability of a model-ontology. The data ontology includes specifically knowledge of data, data-types and data-source the model-ontology includes a clear understanding of the data-types associated with various classes of model To make use of data granules mechanism \(or groups classes, clusters of a universe\o support discovery process it need create various levels of data, as well as inherent data structure, by partitioning data attributes into intervals. The different levels of granularities form data granule ontologies B  Process model The model, called C-KDD \(Cohesive KDD\ model, is shown in Fig. 1. It consists of four stages: planning, session mining, merge mining, and post-processing During planning stage, the KDD process begins with business understanding, including business-aims and business-logic. Through interactive exploration and experimentation, discovery goals, business data, and subsequent processes are identified and the specification of discovery task schedule \(TS\ is generated. The ontological domain knowledge is used to eliminate irrelevant attributes update the vague prior business factors, inferring other abstract attributes, etc. Moreover, the set of valid data attributes, process steps, and algorithms are composed in suggested order based on the user’s desiderata, by a data mining ontology The session mining stage performs select-transferpremining and achieves partial data mining. It places emphasis on local and static rules induction, and executes induction on incremental data at regular intervals, e.g month. As the functions are already specified in the TS, they are periodically repeated on incremental data as per the frequency or trigger condition, and whose outcome forms a rule bin \(RB\. Ontological knowledge is used to assist in determining the selected features, parameters, etc The merge mining is initiated by mining queries or a trigger event. The query contents are listed in consultation with the TS; user can commit them, according to his requests. A trigger event is occurred as the causes of time or rule rising. It places emphasis on overall and dynamic rules discovery, in the interaction paradigm, the rules are merged and refined from several RBs. The parameters and constrains are supplemented with ontological knowledge The post-processing stage begins with matching discovered rules and known knowledge, filters useless ones then classified and ranked automatically interesting results according to interestingness. When a critical point threshold is reached, an alert will be triggered. Meanwhile, the user can review and confirm these findings. It would also integrate new interesting insights with the known knowledge, to perform knowledge evolution and presentation. Then, it forms a close-loop solution that helps to maintain the continuous knowledge discovery process  Figure 1  C-KDD Process Model When unable to satisfy intelligence application or rule review, the process flow goes back to the planning stage to re-explore data C  Higher order mining Higher order mining is a knowledge discovery process concerned with mining over previously induced patterns or models [1  I t s m e t hod s a n d  t e c hni q u e s ar e app l i e d i n t h e CKDD process model In planning stage, data attributes concerned with data mining are partitioned into intervals, called bags. Here, we consider only equi-width bags \(the interval size of each bag is the same\ For example Age 20 Age 21, and Age 22 form a bag \(20 000  Age  000 22\. When a quantitative attribute is partitioned into bags, the bags are mapped to consecutive integers, called BIDs; when attribute values are mapped to bags their value is replaced with the corresponding BID for that bag. For categorical attributes we also map the attribute values to a set of consecutive integers and use these BIDs in place of the categorical values. In the same way, these basis BID may be mapped to upper BIDs. So, a BID hierarchy defines a sequence of mappings from a set of low-level BIDs to higher, larger extent BIDs. It is useful to rule clustering and classification later stages In the session mining stage, Induction can be separated into two steps, the first involves the generation of rules upon primary data, with the other performing rule clustering and classifying. Rule clustering is the combination of adjacent attributes values, or adjacent bags of attribute values in rule For example, clustering Age 20\ and Age 21\ results in Session Mining Planning  Schedule Generation Data Exploration Business Understanding  Selection Induction Transform Merge Mining  Rule Extraction Mining Query  Postprocessing Human-Machine Processing Machine Processing  Rule Review Filter Alert  Evolution Deployment Grouping 
100 
100 


20 000  Age  000 21\. A clustered rule is an expression of the form X c 001 Y c  X c and Y c are items of the form Attribute value\ or  bag i  000  Attribute  000  bag i+l where bag i denotes the lower bound for values in the i th bag. Clustered rules will always have a support and confidence of at least that of the minimum threshold levels Fig. 2 shows a unifying framework that uses heuristics for generating the clustered association rules, which we now describe While the primary data is read, the minimum support is used along with the minimum confidence to generate a large number of association rules. Clustering rules engine forms clusters of adjacent association rules, supporting by the BID hierarchy, according to grouping criteria. These clustered association rules are then tested for their accuracy \(by the verifier\ against a sample of tuples from the primary data database. The accuracy is supplied to a heuristic optimizer that adjusts the minimum support threshold and/or the minimum confidence threshold, and restarts the mining process at the clustering rules engine. These heuristic adjustments continue until the verifier detects no significant improvement in the resulting clustered association rules, or the verifier determines that the budgeted time has expired Once the clustered association rules are discovered for a particular level of BID, we then go up to try upper BID Finally, the clustered rule structures are achieved Associative classification uses association rules to build a classifier. The association rules must have the consequent bound to the class label, hence are considered associative classification rules \(ACRs\ Once a set of ACRs are gathered, they are ranked, with the highest ranked ACRs used to create classifiers The framework of generating associative classification rules is similar to Fig. 2. Firstly, association rules must be pruned and ranked during the ACR generation process to reduce the volume of rules generated, and to allow for an informed selection procedure to build the classifiers from the most highly ranked rules. One procedure to evaluate ACRs is to rank them in the order of confidence, support, and precedence, respectively. Then, given a prioritized set of ACRs, a classifier is created based on the set of rules that cover the training set. The classifier is iteratively built where if a rule covers some set of examples in the training set, it is added and those pertinent examples are removed from further rule evaluations. Each rule, therefore, only covers a particular set of examples, and hence only rules that increase the accuracy of the classifier are included. The default class is determined as the rule which covers the largest number of examples In the merge mining stage, higher order mining focuses on frameworks that detect changes upon data that come from a non-stationary distribution, in pattern management suites and, obviously, in temporal mining methods. Not only the data change but their schema also changes as well We can detect changes in patterns to identify and quantify the differences between patterns drawn from different snapshots of the dataset, namely RBs. We also trace the trends in association rules or clusters from a sequence of RBs  Figure 2  Framework of Clustering Rules D  Knowledge engineering The processes of model interpretation, validation and evaluation have rightly been given places of importance in knowledge discovery through data exploration and mining and in particular in the process model, to ensure the quality of the outcome of KDD. The problems about meaningfulness, significance and importance are a crucial part of trained DM-model understanding. These problems about what we know from DM, and indeed what cannot be known, are essentially part of an as developing ‘Knowledge engineering’ for KDD Without doubt, a formal KDD theory of knowledge needs to be developed. A KDD epistemology will have close links with the data and model ontologies discussed above and also with representations and storage concerning the prior knowledge and communication For C-KDD process model, a hierarchy of processes operating on the constructs at each level of “Information Hierarchy” involving data, information, knowledge, and sometimes wisdom, may also be defined, and is illustrated in  C o rr e s po ndi ng t o t h e da t a l e v e l o f t h e hi er ar c h y   operations concerned with manipulations and storage of basic data resources we term “technical”. Information is derived by operations performed on data using the tools and techniques of DM. We refer to the processes at this level as contextual”. Then, derived and discovered information becomes knowledge only when it is subjected to processes which put the information in the context of the business problems or opportunities. Hence, the processes which deal with such “understanding” of information/knowledge are termed “tactical”. Finally, the processes, which put the integrated knowledge in the context of the whole enterprise and its aims, are referred as “strategic A parameter estimation method, used to explain the interestingness and evolutional regularity of the rule, has been discussed in [1   Association rule mining Primary data Min. Supp Min. Conf Association rules Higher order mining  Clustering rules engine Grouping criteria Verifier  Cluster analysis Test data Clustered association rules Heuristic Optimizer Generate clustered rule structure BID hierarchy 
101 
101 


III  L AYERED A RCHITECTURE  In implementing the model, the machine processing steps require developing autonomous components. Each component is an agent, which obtains the known knowledge through ontology service. The discussion of six components is as below 1. Data selection \(C sel to select data from data sources. It decides dynamically the target subset, specified in terms of the location and time span for heterogeneity 2. Data transformation \(C tra to manipulate data to satisfy C ind needs 3. Data induction \(C ind to execute mining algorithm to induce partial and raw rules from incremental data. It consists of a set of functions for each algorithm 4. Rule grouping \(C grp to execute clustering and classifying algorithm to group static and raw rules. It consists of a set of functions for clustering, classifying, verification and  optimization algorithm 5. Rule extraction \(C ext o achieve overall induction and refinement, only dealing with RBs. It is important to refine the premined results and perform higher order explanation, particularly those describing changes across the session 6. Rule filters \(C flt to verify mining results. The result that contradicts known knowledge may be considered being interesting, because they can occur due to new knowledge or un-codified simply general knowledge 7. Knowledge evolution \(C evl to incorporate newly discovered knowledge into the known knowledge. This derives the necessity to support the ontology evolution involved identification, mapping, and merging etc The motivation for the architecture is to synthesize the complete KDD process and to provide a continuous and convenient environment, as shown in Fig. 2 The top layer, the front-end user layer, composes the end user interface. It provides functionality of human machine processing, and implements the user interface for C-KDD model. The TSs, arranging each mining process and interrelated data, are stored in schedule pool \(MSP\and provide monitoring information through ontology services The Query Broker determines what TS should be contacted to meet a high-level user request. Rule Manager supports the review process for discovered results. Granularity Processor displays and controls the data granule transformation The middle layer, the autonomous component layer, is implemental in machine processing. The autonomous components and the RBs are located in this layer. The components populated the processes, are managed by corresponding to each TS entry. The RBs should be restrictively shared in order to preserve privacy Data mining in evolving domains presents several challenges from distributed, dynamic data sources. The storage management layer organizes the data sources by maintaining the ontology and metadata about the concept term, and physical attribute of each one. It implements ontology service-based information integration. Granularity Manager organizes the data into various granularities according information induction requirement. Resource Broker determines what data set should be contacted to meet a request specified in TS, by searching through ontologies for concept and metadata for physical location about data that match the request criteria All the three layers manipulate and share knowledge and metadata stored in repository segment. The segment maintains structural and semantic information about each data source, recording the relationship between attributes of the data sources with terms from business domain computing contextual information gleaned from these linkages and other source-related information. Ontology Agent provides mediating knowledge base and ontology services, to support access to their services in a unique way that is well integrated with the FIPA Agent mechanism [9    Figure 3  Architecture of C-KDD IV  I MPLEMENTATION  In order to experiment and evaluate the feasibility of the design, we constructed our architecture in a prototype. We concentrate on meta-model of RB and simplified mining algorithms invoking ontology services In the C-KDD model, the mining components are mainly three parts: data induction, rule grouping, and rule extraction which use the rule bin \(RB\ to preserve the former results Accordingly, design of data structure for RB is a significantly important activity in the implementation The meta-model of RB consists of four classes stored in metadata repository. A leading class identifies each RB class table. An attribute class defines the attributes’ properties of each RB. A method class stores its data operations and condition. A constraint class describes each event occurrence. They are used to provide continuous and active data mining. The primary attributes of the classes are as follows Leading \(ClassName, ParentName, Operation Attribute \(AttributesName, ClassName, MethodName AttributesType, AssoAttribute     Granularity Mana g e r Resource Broker Partitioner TS Generator User Manager  Query Broker Query Processor Front-end User Metadata Repository Storage Management DB1 DB2 DB n  Ontology Agent Knowledge Base  Autonomous Component C sel  C tra C ind C ext  C flt  C evl  Function Librar y  RB1  RBm Data Explorer MSP Rule Manager Component Manager Repository Repository Manager Granularity Processor C grp  
102 
102 


Method \(MethodName, ClassName, Parameters MethodType, Condition, Action Constraint \(ConstraintName, ClassName, MethodName Parameters, Event, Timing Table I is an example of the RB table for association rule whose attributes are defined by the above attribute class given a data table Source \(A, B, C, D\. The record counts of attribute for computing support and confidence of association rule are stored in this RB table, particularly ID of known knowledge is stored in field KK, according to year session. The details of rule extraction algorithm using ontology services \(OS\ are shown as below. It is shown to have linear scale-up TABLE I  RB TABLE EXAMPLE  Data RuleID Ant_count Rule_count Rule BID KK 2005 R1 Count\(B\ Count\(A, B\ B 001 A 91 R1_ID 2005 R2 Count\(C\ Count\(A, C\ C 001 A 81 R2_ID       2006 R1 Count\(B\ Count\(A, B\ B 001 A 92 R1_ID       RuleExtractionAR \(TSid,Session,minsup,minconf,OutRule Begin                //obtain possible interesting rules RB_table = OS_SelectRBTable \(TSid obtain RB table using OS Total_Rec = OS_CountRec \(TSid\;     //total of records For each No_Session in Session do Begin No_Rule = Count_Rule \(RB_table, No_Session count number of rules in the session For i = 1 to No_Rule do Begin Support \(i\B_table.Rule_count \(i\ Total_Rec Confidence \(i\B_table.Rule_count \(i RB_table.Ant_count \(i If Support\(i\>=minsup and Confidence\(i minconf  then If OS_IsInteresting \(TSid,RB_table.KK\(i then     //verify interesting using OS Output \(OutRule, Support\(i\Confidence\(i   output interesting rule End End End The change trend of rules \(second order rule\ay also be extracted from OutRule. The rule is a R = \(ruleClause statistics\ ruleClause has the form: A 001\027 B, the statistics may be the support, confidence, and accuracy. A higher order rule is defined as a temporal sequence of R. A sequence of statistics for the same rule R in each session forms a time series; therefore time series analysis can be operated on it We will not discuss here in details for tracing the trends in rules V  C ONCLUSION  Implementation of a continuous KDD process has been attracting more and more interests from various industries We have proposed a conceptual infrastructure for the KDD process model that endeavors to separate autonomous mining sub-processes from a full KDD process, and integrate known knowledge using ontology services. The architecture utilizes ontology service to provide an inherent mechanism for share known knowledge and data granules hierarchy, and constitute a knowledge discovery platform extending support for data independence and granularity transformation. We have also proposed a higher order mining method embedded in discovery process, to achieve monitoring and identifying changes statically and tracing trends dynamically. We have also discussed that data attributes concerned with data mining were partitioned into bags, and formed bags hierarchy to support clustering and classifying rules A CKNOWLEDGMENT  This work is supported by the National Natural Science Foundation of China under grant No. 70771044 and 70872020 R EFERENCES  1  X. Wu, P. S. Yu, and G. Piatetsky-Shapiro, “Data mining: how research meets practical development,” Knowledge and Information Systems, vol. 5, no.2, 2003, pp. 248-261 2  U. Fayyad, G. Piatesky-Shapiro and P. Smyth, Advances in Knowledge Discovery and Data Mining, California: AAAI Press 1996 3  CRISP-DM, CRoss Industry Standard Process for Data Mining http://www.crisp-dm.org, 2003 4  K. Cios and L. Kurgan, “Trends in data mining and knowledge discovery,” Advanced Techniques in Knowledge Discovery and Data Mining, London: Springer, 2005, pp. 1-26 5  R. Agrawal and G. Psaila, “Active data mining,” Proc. of the KDD’95, AAAI Press, 1995, pp. 3-8 6  M. Spiliopoulou and  J. Roddick, “Higher order mining,” Proc. of 2nd Data Mining Methods and Databases, WIT Press, 2000, pp. 309-320 7  P. Domingos, “Toward knowledge-rich data mining,” Data Mining and  Knowledge Discovery, vol.15, no.1, 2007, pp.21-28 8  G. Wang and Y. Wang, “3DM: domain-oriented data-driven data mining,” Fundamenta Informaticae, vol. 90, 2009, pp. 395-426 9  FIPA, FIPA Ontology Service Specification. FIPA XC00086D, 2001   J. F. Roddick, M. Spiliopoulou, D. Lister, and A. Ceglar, “Higher order mining,” SIGKDD, vol.10, no. 1, 2008, pp. 5-18   D. Pan, “An integrative framework for continuous knowledge discovery,” Journal of Convergence Information Technology, vol. 5 no. 3, 2010, pp. 46-53   D. Pan and Y. Pan, “The estimation of rule measure based on principle of information diffusion,” Proc. of the 5th ICMLC 2006 IEEE Press, 2006, pp. 1025-1029  
103 
103 


decreases number of valid candidate generation decreases For this reason, case with 5% MACS takes more time than case with 3% MACS and case with 10 MACS takes more time than case with 5 MACS Figure 7 illustrates accuracy results for our proposed algorithm based on minimum antecedent support The value of minimum antecedent support for each presented result is also indicated The figu re presents MAS has no lead in accuracy as it is not used as a parameter in selecting valid candidate an d rules Figure 8 illustrates accuracy results for our proposed algorithm based on maximum confidence The figure presents maximum confidence has lead in accuracy as it is used as parameter in selecting va lid rules As maximum confidence decreases accuracy increases and the number of discovered rules decreases. It is because less confidence indicates th at antecedent and consequent occurs rarely together  i n the dataset 7 Conclusion Irregular patterns represent wrong decision illegal practice and variability in decision In th is paper, we propose a level wise search algorithm tha t works based on action and non-action type data to find irregular association rule The proposed algorithm has been applied to a real world patient data set. We have shown significant accuracy in the  output of the proposed algorithm. Although we have used level-wise search for finding irregular patter ns each step of our algorithm is different from any ot her level-wise search algorithm Rules generation from desired item sets is also different from conventiona l association mining algorithms 8. References 1 R Agrawal T P L H O L V N L D Q G   Swami Mining Association Rules between Sets of Items in Very Large Databases in Proceedings of the 1993 ACM SIGMOD international conference on Management of data Washington, D.C., 1993, pp. 207-216 2 R Agrawal and R Srikant Fast Algorithms for Mining Association Rules in Large Databases in Proceedings of the 20th International Conference on  Very Large Data Bases  San Francisco CA USA 1994, pp. 487 499 3 S Brin, R Motwani, J D. Ullman, and Shalom Tsur Dynamic Itemset Counting and Implication Rules for Market Basket Data in Proceedings of the 1997 ACM SIGMOD international conference on Management of data Tucson, Arizona, United States 1997, pp. 255-264 4 H Mannila H Toivonen and A I Verkamo Efficient Algorithms for Discovering Association Rules in AAAI Workshop on Knowledge Discovery in Databases 1994, pp. 181-192 5 J S Park M S Chen and P S Yu An Effctive Hash based Algorithm for mining association rules in Prof ACM SIGMOD Conf Management of Data  New York, NY, USA, 1995, pp. 175 186 6 A Savasere E Omiecinski and S B Navathe An Efficient Algorithm for Mining Association Rules in Large Databases in Proceedings of the 21th International Conference on Very Large Data Bases  1995, pp. 432 444 7 B Liu W Hsu and Y Ma Mining Association Rules with Multiple Minimum Supports in SIGKDD Explorations 1999, pp. 337--341 8 H Yun D Ha B Hwang and K H Ryu Mining association rules on significant rare data using relat ive support Journal of Systems and Software archive  vol. 67, no. 3, pp. 181 191, 2003 9 M Hahsler A Model-Based Frequency Constraint for Mining Associations from Transaction Data Data Mining and Knowledge Discovery  vol 13 no 2, pp. 137 166, 2006 10 L Zhou and S Yau Association rule and quantitative association rule mining among infrequent  items in International Conference on Knowledge Discovery and Data Mining  San Jose California 2007, pp. 156-167 11 R. U Kiran and P. K Reddy, "An improved multiple minimum support based approach to mine rare association rules in The IEEE Symposium on Computational Intelligence and Data Mining  Nashville, TN, USA, 2009, pp. 340-347 12 S Brin R Motwani and C Silverstein Beyond Market Baskets Generalizing Association Rules to Correlations in In The Proceedings of SIGMOD  AZ,USA, 1997, pp. 265-276 13 X Wu, C Zhang, and S Zhang, "Efficient Mining of Both Positive and Negative Association Rules ACM Transactions on Information Systems  vol 22 no 3 p. 381  405, 2004 14 M L Antonie and O R. Zaïane, "Mining positive and negative association rules an approach for confined rules in Proceedings of the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases Pisa, Italy, 2004, pp. 27 38 15 P A Ortega C J Figueroa and G A Ruz A Medical Claim Fraud/Abuse Detection System based on Data Mining A Case Study in Chile in DMIN  2006, pp. 224-231 16 W S Yanga and S Y Hwangb A Process-Mining Framework for the Detection of Healthcare Fraud and Abuse Expert Systems with Applications  vol 31 no. 1, p. 56  68, July 2006 68 


 T ABLE 26  C LASSIFICATION  OF T RANSLATION C OUNT  Translation  Count Class Population Percentage one 58,990 76.34 two 14,581 18.87 three or more 3698 4.79 15\  User Interface There are 97,302 OSS Projects \(72.32 of the recorded projects that list the available user interface for the project.  There are 60 distinct values which is then classified into 4 classes as shown in Table 26  T ABLE 26  U SER I NTERFACE CLASSIFICATION FOR OSS  P ROJECT  User Interface Class Population Percentage  Desktop-based 58,136 46.91 Web-based 31,679 25.57 Text-based 21,228 17.13 Other 12,872 10.39 Where  Desktop-based: Win32 \(MS Windows\, X Window System \(X11\, Gnome, GTK+, etc  Text-based: Non-interactive \(Daemon Console/Terminal, etc The classification of the count of User Interface in each OSS Project is shown in Table 27 T ABLE 27  C LASSIFICATION OF U SER I NTERFACE C OUNT  User Interface Count Class Population Percentage one 77,983 80.15 two 14,132 14.52 three or more 5187 5.33 V  R ESULT AND D ISCUSSION  The process of Datamining 2-Itemset Association Rule is using Weka Datamining tool.  The class parameters were the combination of two parameters with the Consequent was always set to parameter Download There are 24 possible combinations of other parameters as the Antecedent and Download as the Consequent The result that have Download Thousands or more as Consequent with Confident  50 and Support  10% are interesting values Table 28 shows all the result of Datamining 2-Itemset Association Rule the has Consequent  Download - Thousands of more  T ABLE 28  R ESULT WITH D OWNLOAD  T HOUSANDS OF MORE AS C ONSEQUENT  Antecedent Analysis Parameter Class Pop. Sup. Conf Audience Common Users 123909 14.91% 51.76  Development Status 5-Production Stable 83422 17.54% 60.12  Antecedent Analysis Parameter Class Pop. Sup. Conf Operating System  Linux-like 129912 19.88% 52.34  Operating System  Windows 129912 18.51% 55.19  Review Count three or more 7822 28.97% 94.10  Review Count two 7822 14.02% 81.32  Review Count one 7822 36.93% 71.07  Size MB 70343 15.46% 55.71  Total Thumb eleven or more 15117 12.17% 98.87  Total Thumb two to three 15117 18.83 75.26  Total Thumb single 15117 29.34% 64.03  Total Thumb four to ten 15117 15.28 90.73  Translation English 73412 30.92% 51.62  Translation European 73412 21.54% 63.97  Note: Pop. - Population, Sup. - Support, Conf. - Confidence Table 28 shows that the success factors of OSS Project with the goal to get the number of download in thousands or more are 1  Project should target for common users as audience 2  Project source code should already in 5 – Production Stable development status 3  Project should work on either Linux-like or Windows operating system 4  Project should be reviewed and thumb-reviewed by at least one users 5  Project has filename in zip format with size in MB in magnitude 6  Project should have either English or European language translation Most of the above results are reasonable except for the file size.  Stating Common users as audience will reach wider segment of users.  Users are also more interested in projects that are in reasonable level of maturity so they will select OSS Project which is already in 5 – Production / Stable development status.  The selected Operating System in Windows or Linux-like are also reasonable since both are the most popular desktop-based Operating System.  The need for reviews and thumbs by other users seems to have good influence for another users to download and try the OSS The file size in MB magnitude appears with possible explanation that users are more interested in larger scale OSS Project than medium or small one.  Lastly, the English or European language translation is important since many of OSS Projects are originated from this region \(Englishspeaking countries or other European language-speaking countries\. It is also interesting to note that some freedom is still available for project initiator to decide i.e. topic programming language and description of his/her project without affecting the number of download Some caution should be considered relating to these rules The subject being researched is small to medium OSS Projects 


 from sourceforge.net that may not reflect the whole population of OSS Projects that are small, medium and large scale.  These result should also verified using OSS Project data from other portal i.e. launchpad.net, google code, etc. to verify their validity VI  C ONCLUSION  We present the Datamining 2-Itemset Association Rule of 134,549 OSS Projects crawled from sourceforge.net portal This covers about 84% of the total of 160,141 OSS Projects registered at the portal in the month of January 2010.  There are more than 27 parameter being recorded into MySQL database i.e. audience, audience count, database environment database environment count, developer count, development status, development status count, number of download filename and file size, license, license count, operating system, operating system count, programming language programming language count, review count, topic, topic count, translation, translation count, user interface, and user interface count The result of this datamining process are 6 success factors that may be applied by project initiators and developers in order to increase the probability of success of their projects The details of the guidelines is shown in Section V. Future work of this research include expanding the experiment to cover other portal i.e. launchpad.net, google code and freshmeat.net ACKNOWLEDGMENT  The authors would like to thank Maranatha Christian University \(http://www.maranatha.edu\ that provides the funding for this research and Department of Computer Science and Electronics at Gadjah Mada University http://mkom.ugm.ac.id\ which provides guideline and technical assistance for the research R EFERENCES  1  R. Agrawal., R. Srikant, “Fast Algorithm for Mining Association Rule Proceeding of 20th International Conference Very Large Database 1994, pp 1 - 32 2  A. Capiluppi, J.F. Ramil, “Studying the Evolution of Open Source Systems at Different Levels of Granularity: Two Case Studies Proceeding of the 7th International Workshop of Principles of Software Evolution, 2004, 113 - 118 3  S. Christley, G. Madey, “Analysis of Activity in the Open Source Software Development Community”, Proceeding of the 40th IEEE Annual Hawaii International Conference on System Sciences, 2007 166b 4  T.T. Dinh-Trong, J.M. Bieman, “The FreeBSD Project: A Replication Case Study of Open Source Development”, IEEE Transaction on Software Engineering Vol. 31 No. 6,  June 2005, 481 – 494 5  V.K. Gurbani, A. Garvert, J.D. Herbsleb, “A Case Study of Open Source Tools and Practices in Commercial Setting”, Proceeding of the fifth Workshop on Open Source Software Engineering, 2006, 1 - 6 6  J.E. Istiyanto, A.W.R. Emanuel, “Success Factors of Open Source Software Projects using Datamining Technique”, Proceeding of Information Technology and Communication International Seminar ITIS\, June 2009 7  P.L. Li, J. Herbsleb, M. Shaw, “ Finding Predictors of Field Defects for Open Source Software Systems in Commonly Available Data Sources a Case Study of OpenBSD”, Proceeding of 11th IEEE International Software Metrics Symposium, 2005,  32 8  G. von Krogh, S. Spaeth, S. Haefliger, “Knowledge Reuse in Open Source Software: An Exploratory Study of 15 Open Source Projects Proceeding of 38th Hawaii International Conference on System Sciences, 2005, 198b 9  A. Mockus, R.T. Fielding, J. Herbsleb, “Two Case Studies of Open Source Software Development: Apache and Mozilla”, ACM Transaction on Software Engineering and Methodology Vol. II No. 3 Juli 2002, 309 – 346 10   A. Mockus, R.T. Fielding, J. Herbsleb, “A Case Study of Open Source Software Development: The Apache Server”,  ACM ICSE, 2000, 263 272 11   E.S. Raymond, “The Cathedral and the Bazaar”, version 3, Thyrsus Enterprises \(http://www.tuxedo.org/~esr/\, 2000 12   Sourceforge.net web portal at http://www.sourceforge.net 13   S. Spaeth, M. Stuermer, “Sampling in Open Source Development: The Case for Using the Debian GNU/Linux Distribution”, Proceedings of the 40th IEEE Hawaii International Conference on System Sciences 2007, 166a 14   Weka website at  http://www.cs.waikato.ac.nz/ml/weka 


Application of Chaotic Particle Swarm Optimization Algorithm in Chinese Documents Classification 763 Dekun Tan Qualitative Simulation Based on Ranked Hyperreals 767 Shusaku Tsumoto Association Action Rules and Action Paths Triggered by Meta-actions 772 Angelina A. Tzacheva and Zbigniew W. Ras Research and Prediction on Nonlinear Network Flow of Mobile Short Message Based on Neural Network 777 Nianhong Wan, Jiyi Wang, and Xuerong Wang Pattern Matching with Flexible Wildcards and Recurring Characters 782 Haiping Wang, Fei Xie, Xuegang Hu, Peipei Li, and Xindong Wu Supplier Selection Based on Rough Sets and Analytic Hierarchy Process 787 Lei Wang, Jun Ye, and Tianrui Li The Covering Upper Approximation by Subcovering 791 Shiping Wang, William Zhu, and Peiyong Zhu Stochastic Synchronization of Non-identical Genetic Networks with Time Delay 794 Zhengxia Wang and Guodong Liu An Extensible Workflow Modeling Model Based on Ontology 798 Zhenwu Wang Interval Type-2 Fuzzy PI Controllers: Why They are More Robust 802 Dongrui Wu and Woei Wan Tan Improved K-Modes Clustering Method Based on Chi-square Statistics 808 Runxiu Wu Decision Rule Acquisition Algorithm Based on Association-Characteristic Information Granular Computing 812 JianFeng Xu, Lan Liu, GuangZuo Zheng, and Yao Zhang Constructing a Fast Algorithm for Multi-label Classification with Support Vector Data Description 817 Jianhua Xu Knowledge Operations in Neighborhood System 822 Xibei Yang and Tsau Young Lin An Evaluation Method Based on Combinatorial Judgement Matrix 826 Jun Ye and Lei Wang Generating Algorithm of Approximate Decision Rules and its Applications 830 Wang Yun and Wu-Zhi Qiang Parameter Selection of Support Vector Regression Based on Particle Swarm Optimization 834 Hu Zhang, Min Wang, and Xin-han Huang T-type Pseudo-BCI Algebras and T-type Pseudo-BCI Filters 839 Xiaohong Zhang, Yinfeng Lu, and Xiaoyan Mao A Vehicle License Plate Recognition Method Based on Neural Network 845 Xing-Wang Zhang, Xian-gui Liu, and Jia Zhao Author Index 849 
xiii 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





