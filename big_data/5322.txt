For some computer vision tasks such as location recognition on mobile devices or Structure from Motion SfM computation from Internet photo collections one wants to reduce a large set of images to a compact representative 
Michal Havlena Wilfried Hartmann Konrad Schindler Institute of Geodesy and Photogrammetry ETH Z urich Wolfgang-Pauli-Strasse 15 8093 Z urich Switzerland 
michal.havlena,wilfried.hartmann,schindler geod.baug.ethz.ch 
Abstract 
Optimal Reduction of Large Image Databases for Location Recognition 
  
subset sometimes called keyframes or skeletal set We examine the problem of selecting a minimum set of such keyframes from the point of view of discrete optimization as the search for a minimum connected dominating set CDS of the graph of pairwise connections between the database images Even the simple minimum dominating set DS problem is known to be NP-hard and the constraint that the dominating set should be connected makes it even harder We show how the minimum DS can nevertheless be solved to global optimality ef“ciently in practice by formulating it 
as an integer linear program ILP Furthermore we show how to upgrade the solution to a connected dominating set with a second ILP if necessary although the complete method is no longer globally optimal We also compare the proposed method to a previous greedy heuristic Experiments with several image sets show that the greedy solution already performs remarkably well and that the optimal solution achieves roughly 5 smaller keyframe sets which perform equally well in location recognition and SfM tasks 
1 Introduction 
Location recognition from an image taken by the user is a useful application of computer vision even more so now that practically all mobile phones and tablets are equipped with cameras The state-of-the-art methods to build such a service 10 rely on t h e  big d ata paradigm lar ge precomputed image databases which reside on computer clusters to which user images must be submitted via Internet or cellular networks to query for the location With increasing computational power and storage capacity of the mobile devices it becomes possible to run such an application even 
locally without network access For the described mobile application of location recognition the compactness of the pre-computed database is crucial but the need for a compact yet meaningful subset of big image data is not restricted to this particular use case Mobile robotic SLAM 6 a nd s horto r w ide-bas eline s equential visual odometry methods 22 29 oft en rel y on a set of keyframes to robustify the computation and/or to overcome the drift of the estimated camera poses over time by performing loop-closing Compactness of the keyframe set is important for the ef“ciency of loop-closing which 
must share limited processing power with the actual perframe SLAM computation Note that the image selection procedure is even more complicated in the SLAM case because of the on-line nature of the computation The contribution of this paper is twofold First we explore optimal methods for reducing a large redundant input image set by selecting the most relevant representatives via solving the same graph problem as 14 S econdl y  we compare the selections made by the proposed near-optimal selection algorithm with the output of the existing subopti 
mal greedy algorithm used by 14 and s ho w t ha t t he proposed method gives more compact image subsets while the performance of the location r ecognition task remains unchanged We also show that the difference between the sizes of the subsets obtained using suboptimal and optimal methods is rather small while the proposed approach is arguably more principled our results vindicate the widely used greedy heuristic but have a slight edge for applications with strict storage constraints The bulk of visual location recognition techniques relies on image retrieval based on visual words 27 F u r  
ther advances using either a more compact data representation or achi e vi ng hi gher ef  ci enc y t hanks t o v o cabulary trees 23 allo w f or lo cation recognition from cityscale databases 26 R ecently  m ethods ha v e appeared which offer precise positioning of a given query image 15 25 Thes e m ethods pro vide the camera pos e w r t a precomputed 3D point cloud inst ead of just an approximate localization Once the point cloud is geo-referenced a precise location on the Earth can be obtained 19 
2013 IEEE International Conference on Computer Vision Workshops 978-0-7695-5161-6/13 $31.00 © 2013 IEEE DOI 10.1109/ICCVW.2013.93 676 
2013 IEEE International Conference on Computer Vision Workshops 978-1-4799-3022-7/13 $31.00 © 2013 IEEE DOI 10.1109/ICCVW.2013.93 676 
2013 IEEE International Conference on Computer Vision Workshops 978-1-4799-3022-7/13 $31.00 © 2013 IEEE DOI 10.1109/ICCVW.2013.93 676 


    
 GIST 24 Al t hough t h i s approach was successfully used to speed-up Structure from Motion SfM computation 17 8 we belie v e that methods bas ed on visual words are more suitable for the location recognition task On the other extreme one could estimate similarity through the number of keypoint matches or even through the relative pose however such approaches in most cases defy the purpose as matching and pose estimation are computationally too expensive We use a technique based on visual words similar to the one of 14 Not e ho we v er  our approach i s generi c and applies to any distance/similarity matrix We show that thanks to modern integer linear program ILP solvers it is possible to solve this graph problem optimally even for rather large image sets and that there is a noticeable gap between the sizes of the optimal and greedy solutions It is worth mentioning that also the widely used large-scale SfM pipeline based on skeletal image sets 28 greedily s o lv es the same graph problem so our results apply to city-scale SfM too Let us assume for the moment that a connected graph is available whose vertices are the input images and in which an edge exists between two vertices if and only if the two corresponding images share a signi“cant part of the view“eld  are co-located Then to make a compact selection covering all views present in the image set one needs to select a minimum subset of vertices in such a way that all the vertices in the graph are either in the subset or connected to it by an edge meaning that every database image is colocated with at least one keyframe and the subgraph induced by the selected subset is connected In graph theory a subset that ful“lls is known as the DS of the graph see Figure 1 while is called the CDS Searching a CDS avoids database fragmentation but makes the problem considerably harder In some applications such as nding a skeletal set for SfM connectedness is mandatory while in others it might be dropped or only encouraged rather than enforced Following 11 t he mi ni mum C D S probl em i s de ned as follows Given a graph  nd a minimum size subset of vertices  such that the subgraph induced by is connected and forms a dominating set in In a graph with a dominating set  each vertex is either in the dominating set or adjacent to some vertex in the dominating set We say that is covered by in the latter case The problem of nding the minimum CDS is known to be hard 9 By de“nition to nd the optimal solution to an hard problem one has to evaluate an exponential number of candidates in the worst case Nevertheless a large class of hard problems can be formulated as integer linear programs ILPs and for many instances of such ILPs modern solvers can nd the optimum quickly The general ILP formulation reads minimize subject to 1 The high ef“ciency of ILP solvers is achieved thanks to the following strategy obtain a lower bound of the objective function in low polynomial time by solving the relaxed LP problem where the integer constraint is dropped and use the lower bound in branch-and-bound type methods to quickly rule out large portions of the search space 
A 
2.1 Minimum CDS formulated as ILP 
2 Method 
V E S\002V S S D\002V 003V 003D\004\005 003D 003E 006 007 007 003 Z 
e.g i.e i ii i minimum dominating set ii minimum connected dominating set NP NP NP i ii 
Figure 1 Unconnected dominating set denoted by black vertices We use location recognition as a benchmark for reducing big image databases to representative subsets Our aim here is to decrease the size of the keyframe set by revisiting the problem of selecting relevant representatives from a large image collection To our best knowledge no systematic attempts have been made so far to select such image subsets optimally by formally minimizing the size of the subset that has the required properties Note however in other contexts the formulation of the minimum dominating set problem as an integer linear program has been known for several years 16 Most techniques for reducing an image set rely on the same idea namely to rst compute pairwise distances within the database and then prune it such that for every image there is a suf“ciently near representative in the keyframe set or equivalently a representative for each cluster of images The differences lie mostly in the proxy used to estimate the distance One possibility is to employ global image descriptors 
T j j j j 
c x x b 
002 002 002 
G  G V V V V V V V l x u x 
677 
677 
677 


3 5 
D 
nearest neighbor strategy a cover number of guarantees that at least useful neighbors are available for localization respectively pose estimation To formulate the search for the minimum dominating set as an ILP we construct an 
002 
G 
Figure 2 Four components of the subgraph of induced by denoted by color ellipses are connected by adding two yellow vertices into  These two vertices form the solution of Program 3 for a matrix the union of the sets and for the three gray edges of the corresponding maximum spanning tree of the constructed weighted graph contains four 1-vertex and one 2-vertex bridges see text Although this can in the worst case still lead to an exponential number of candidates the actual convergence times are quite short for our ILPs as will be shown in Section 3.4 We believe that unfortunately it is not possible to formulate the search for the minimum CDS as an ILP because of the comprehensiveness of the connectivity constraint On the contrary the ILP formulation of nding a possibly disconnected minimum DS is straightforward 16 W e t hus resort to the same strategy also used by the greedy formulation see Section 2.2 namely rst nd the minimum dominating set 
D S  C C 
k Q Q 
G n n n G V i V j  x j x j n m m i j m y j y j m k k k i j m 
 If the subgraph induced by is connected is at the same time the minimum connected dominating set of  Otherwise we need to include additional vertices in  While one can also select these connecting vertices in a one-shot optimization the total CDS is no longer guaranteed to be the globally most compact one To prevent excessive fragmentation of the found minimum dominating set  and thereby increase the chance that the subgraph induced by is already connected we require even the selected vertices to be covered so the minimum possible size of a component is 2 and not 1 The connectivity of can be further improved by requiring each vertex to be covered by more than one vertex We call the number of covering vertices of a keyframe set its CN 1 This changes the minimum component size to 3 or even more and often directly yields a minimum CDS at the cost of obtaining larger keyframe sets see Section 3 1 Note the relation to the matrix A where is the number of vertices of and A ij 1 with the remaining elements of A being zero By zeroing the diagonal of A  we require the selected vertices to be covered also Our program then reads minimize 1 T x subject to A x 2 The desired cover number can be set through parameter vector b  The solution of the program is a Boolean vector x of length  indicating the membership of vertices in the set  The minimized objective val ue is the number of vertices in the DS If the subgraph induced by consists of multiple say disconnected components then for every such component there exists another one to which it can be connected by adding at most two additional vertices thanks to the properties of the dominating set 11 W e e xamine all p airs of components to nd vertices which would form such 1vertex or 2-vertex bridges and then try to nd the smallest subset of vertices that need to be added to to obtain a connected graph First to simplify the task the  pairs of components which are going to be connected are pre-selected For each pair of components and  the set of feasible 1-vertex bridges and the set of feasible 2-vertex bridges are found Second a weighted graph is constructed whose vertices are the individual components and whose edges are the potential connections weighted according to numbers of feasible bridges the weight of a given edge is either the size of if is empty or the size of plus the size of the largest in the entire graph otherwise By selecting the  pairs forming the maximum spanning tree of this graph the pairs of components which can be connected using a 1-vertex bridge are preferred The secondary criteria then is the number of feasible 1-vertex or 2-vertex bridges because this increases the chance that some of these bridges could potentially connect other component pairs too Then a second ILP is constructed minimize c T y subject to D y 3 where D is a  matrix is the size of the union of sets and of the pre-selected component pairs and c is a vector of length which is 1 for 1-vertex bridges and 2 for 2-vertex bridges The mini mized objective function then represents the number of added vertices The entries of D are D ij 1 if the th component pair would be connected by the th bridge and zero otherw ise The constraints ensure that all  components pairs will be connected 
cover number 
D D D S\010D D D D  011\012  003E 006 b 0 007 007 1 003 Z D D D  1 C ij C 002 ij C 002 ij C ij C ij C 002  1 006 1 0 007 007 1 003 Z  1  C C 002  1 
678 
678 
678 


Algorithm 1 Input Output 
G c c m m G  G V V c V c V V c V V G G G G 
with a single component a CDS see Figure 2 A suboptimal greedy solution to the minimum CDS problem is given in 11 s e e A l gori t h m 1  B ei ng greedy  the algorithm inherently has polynomial time complexity Moreover its approximation ratio is in the worst case  with the maximum vertex degree in the graph The greedy algorithm selects one vertex at a time picking the one that covers the largest number of uncovered vertices see Figure 3\(a Note that this strategy is suboptimal Figure 3\(b If the desired cover number is greater than one the algorithm can be extended by introducing fractional coverage Each not fully covered vertex contributes as many times to the value of as it needs to be further covered Thereby the selection prefers vertices whose neighbors received less coverage from the previously selected vertices In our experience this works better than counting all not yet fully covered vertices equally as proposed originally Once the greedy selection has found a DS it is connected to a CDS with another greedy search procedure For all component pairs one checks whether a connection by a 1vertex bridge or 2-vertex bridge is possible Then one iteratively selects the bridge which connects the highest number of components For a graph consisting of components at most iterations are needed to obtain the nal  This can be rather slo w for highly fragmented graphs because graph components need to be updated after each iteration Approximate min CDS computation 11 Unweighted undirected graph Vertices of the min CDS of  I Label all vertices white II Set and repeat until no white vertices are left 1 For all black vertices set  2 For all gray and white vertices set number of white neighbors of  3 Set  4 Label black and add it into  5 Label all neighbors of gray III Set and connect components of the subgraph induced by  by adding at most 2 vertices per component into  IV Return  Until now we have assumed that the graph representing the co-located images is available In fact constructing such a graph properly  by matching all image pairs and verifying pairwise geometries is rather costly We use a standard image indexing technique instead to obtain a proxy more ef“ciently 27  Imag es are resamp led to 3 M p i x r esolution and a visual vocabulary of 200,000 visual words trained on images of city landmarks is used to quantize SURF 2 features detected in the databas e i mages  The entries of the similarity matrix are dot products of the images vectors and the edges of graph can be obtained by thresholding the similarity matrix Special care must be taken to ensure there is a solution if the graph is not connected no CDS exists Moreover already the DS is infeasible if has small isolated components whose vertex count is lower than the desired cover number To bypass this issue we pre-process the graph and recursively remove uncoverable vertices until the problem is solvable Note this step can already signi“cantly decimate the database size if a high similarity threshold is used Three landmark image sets each consisting of roughly 4,000 images were downloaded from Flickr 32 as results for queries di Trevi duomo milano and old town square prague The variability of the image sets increases as DITREVI represents a single wall DUOMO represents a single building while OLDTOWN represents a larger area with multiple buildings Only images which were available in at least 0.75Mpix resolution were used 
a b Figure 3 a Illustration of the computation of Algorithm 1 Step II Two black vertices have been added to 
003 003 003 
D 
ln 3  1     0   arg max  
3 Experiments 
so far the gray vertices are already covered Note that three more vertices need to be added to construct the dominating set whereas the optimal solution b consist of three vertices in total resulting in a subgraph of 
2.2 Greedy Algorithm 2.3 Constructing the Graph 
V V V V V V 
i.e i.e tf-idf 
013 013  D V E S 003V D  003V D S D D S S 
679 
679 
679 


123 0 100 200 300 400 0 075 0 075 0 075 0 1 0 1 0 10 15 0 15 0 15 123 0 100 200 300 400 0 075 0 075 0 075 0 1 0 1 0 10 15 0 15 0 15 123 0 100 200 300 400 0 075 0 075 0 075 0 1 0 1 0 10 15 0 15 0 15 
G G 
Connected dominating sets were computed for the three datasets independently using both the ILP solver and the greedy algorithm The corresponding graphs were obtained using the method described in Section 2.3 for three different thresholds of the image similarity namely 0.075 0.1 and 0.15 Cover numbers parameter vector in ILP of 1 2 and 3 were tested The sizes of the returned minimumDSandCDSof are displayed in Figure 5 It can be seen that more images are required for the most complex scene OLDTOWN than for the other two datasets As both methods rst search for the dominating set it is also interesting to see how many vertices had to be added to connect the DS into a CDS Con in Figure 5 We observe that as expected the fragmentation of the dominating set CN 5 smaller than those of the greedy method That difference slightly decreases for the respective CDS solutions Apparently the redundancy in the greedy solution means that it is closer to a CDS and fewer vertices have to be added 
a b c Figure 4 Sample images from the three datasets downloaded from Flickr 32 a DITREVI b  DUOM O c OLDT O W N To evaluate location recogn ition using the selected subsets of images 300 images from each image set were randomly set aside as query images and the rest of the images form the datasets of sizes 3,110 3,104 and 3,255 images respectively see Figure 4 Two application scenarios were explored with the heldout query images First lo cation recognition based on image retrieval and subsequent geomet ric veri“cation using the selected keyframe sets And second pose estimation of the query images w.r.t 3D models reconstructed from the keyframe sets a CN b CN c Figure 5 Sizes of the connected dominating sets obtained by the greedy algorithm and by solving the ILP with different thresholds for image similarity and different cover numbers Datasets a DITREVI b DUOMO and c OLDTOWN is smaller when the similarity threshold is lower and hence the graph is denser and also when a higher cover number is chosen In some cases the optimal DS was already connected in which case it is also the globally optimal CDS In the remaining cases and always for the greedy solutions no optimality guarantee can be given We also observe that the greedy DS often needs fewer additional vertices to turn it into a CDS the DS returned by ILP are 
014 
Number of images Greedy DS Greedy Con ILP DS ILP Con Number of images Greedy DS Greedy Con ILP DS ILP Con Number of images Greedy DS Greedy Con ILP DS ILP Con 
3.1 CDS Computation 
                           
b 
680 
680 
680 


3.2 Image Retrieval 3.3 Pose Estimation 
tf-idf tf-idf i.e tf-idf e.g i.e 
Image Retrieval correctly localized images of 300 Greedy ILP Thr  CN 123 12 3 0.075 140 151 160 137 144 156 0.100 153 165 160 156 156 163 0.150 171 169 169 169 174 172 Greedy ILP Thr  CN 123 12 3 0.075 35 62 57 25 59 69 0.100 68 81 83 67 78 79 0.150 75 86 91 78 87 90 Greedy ILP Thr  CN 123 12 3 0.075 82 102 113 81 94 107 0.100 119 111 123 112 121 123 0.150 121 123 131 124 124 135 Table 1 Evaluation of location recognition using image retrieval with geometric veri“cation with different thresholds for image similarity and different cover numbers Following the same procedure as for the dataset images we extract SURF 2 features from the query images resampled to 3Mpix resolution quantize them with a pretrained visual vocabulary and represent images by vectors 27 Our d atabas e i s formed by the union of the minimum CDS of all three im age collections DITREVI DUOMO and OLDTOWN Similarity between each query image and all database images is computed as the scalar product of their scores The ve database images having the highest score are passed to geometric veri“cation with a homography 13 The ori gi nal S UR F des cri pt ors are matched with approximate NN-search 30 and the homography is then computed with RANSAC 7  with an erro r threshold of 4 pixels Among those candidates for which the t has at least 100 inliers and an inlier ratio better than 10 the one with the highest inlier ratio is selected The localization is considered correct if the selected candidate is from the same collection   the same landmark Results summarized in Table 1 show that over all cover numbers and image similarity thresholds the numbers of correctly localized images are roughly the same for the keyframe sets obtained with the greedy algorithm and those from the ILP solution although the latter are more compact The numbers of incorrectly localized images were around 8 for DITREVI and around 11 for DUOMO and OLDTOWN Regarding storage the footprint of the database is 70kB per image for the zipped vectors plus 500kB per image to store the image itself for ef“cient on-the-”y extraction of SURF features The size of the visual vocabulary independent of the dataset size is 108MB Pose Estimation correctly localized images of 300 Greedy ILP Thr  CN 123 12 3 0.075 188 206 199 176 210 218 0.100 213 226 230 221 224 230 0.150 222 229 230 235 230 234 Greedy ILP Thr  CN 123 12 3 0.075 66 73 80 62 63 78 0.100 74 90 84 75 86 82 0.150 85 97 93 94 94 92 Greedy ILP Thr  CN 123 12 3 0.075 65 84 85 63 64 76 0.100 85 122 151 85 n/a 148 0.150 134 154 152 133 153 134 Table 2 Evaluation of location recognition using pose estimation w.r.t the 3D point cloud of the largest partial model with different thresholds for image similarity and different cover numbers VisualSFM 31 w as us ed to create 3D models of the selected keyframe sets The sizes of the largest reconstructed partial models are small for low similarity thresholds because with such strict thresholds the image collections disintegrate int o many small unconnected fragments We note however that with one exception OLDTOWN image similarity 0.1 cover number 2 all 3D models are visually correct albeit incomplete„they capture only the most prominent part of th e scene Dataset DUOMO seems to be the most challenging one because several separate clusters of images exist  the frontal view the building interior and the view from the roof Consequently only the largest cluster„the frontal view„is reconstructed The reconstruction of OLDTOWN becomes quite complete when a suf“ciently high threshold for image similarity is used We used VisualSFM resume feature to test whether it is possible to connect a given query image to the constructed landmark models The numbers of correctly localized images summarized in Table 2 show that the results are again roughly the same for the keyframe sets obtained using the greedy algorithm and those from the ILP solution In these experiments not a single image was wrongly localized   connected to the wrong landmark model One can also see that for the easy DITREVI dataset the results of localization using pose estimation are better than those obtained from localization using image retrieval On the other hand localization using image retrieval works better than via pose estimation for the most complex dataset OLDTOWN because of the dif“culties of building 3D models of complicated scenes from reduced overly sparse image sets 
014 014 
DITREVI DUOMO OLDTOWN DITREVI DUOMO OLDTOWN 
681 
681 
681 


DUBROVNIK 
                     
1MB per image The size of the reconstructed 3D model is dependent on the scene type and the spatial distribution of cameras for our datasets it was approximately 30kB per reconstructed image Finally the performance and scalability of the proposed selection method was tested u sing the publicly available DUBROVNIK dataset 18 w hich compris e s 6,044 databas e and 800 query images with known ground truth poses w.r.t the 3D point cloud constructed from the database images As only SIFT 20 d es cri pt ors are pro vi ded and not t h e o ri ginal images we trained a 200,000 word vocabulary from the database image descriptors The sizes of the obtained minimum DS and CDS are shown in Figure 6 Notice that the graph is similar to the one of the OLDTOWN dataset but the overall portion of selected images is nearly twice as large due to the even more complex layout of DUBROVNIK and lower thresholds for image similarity were used because for higher thresholds the removal of uncoverable vertices during pre-processing already eliminated the majority of the images Run times are reported for the minimum DS search of our multi-threaded MATLAB imp lementation of the greedy algorithm and MOSEK 21 ILP s olv er  T imes were measured on a single Intel\(R Core\(TM i7-3930K workstation with Linux 64bit operating system see Table 3 We point out that the run times of the greedy algorithm and the ILP solver are similar but there is a much higher variance in the run times of the ILP solver Also in general the greedy algorithm gets slower with higher cover number whereas the time needed to solve the ILP stays the same or even decreases 2 2 This need not be the case for other problems when applying the proposed method to very sparse graphs we observed much longer run times of the ILP solver Greedy Thr  CN 123 0.050 0.075 0.100 ILP Thr  CN 123 0.050 0.075 0.100 Greedy ILP Thr  CN 123 12 3 0.050 679 724 729 669 705 731 0.075 719 737 734 717 736 732 0.100 644 687 680 650 658 677 Table 4 Evaluation of location recognition using pose estimation w.r.t the 3D point cloud of the respective submodel with different thresholds for image similarity and different cover numbers For each of the obtained subsets of images a submodel was created by removing from the 3D model the unselected cameras as well as 3D points which were left with less than two projections Then average SIFT descriptors of the 3D points in the submodel were computed using the remaining projections and tentative 3D-to-2D matches between the submodel and each of the 800 query images were created with approximate NN-search The poses of all query images with more than 20 tentative matches were estimated using P4Pfk P3P with unknown focal length 3 i n a RANSAC loop with reprojection error threshold of 2 pixels and the maximum number of iterations limited to 200 We use the same methodology as 18 and report t he number of query images which attained at least twelve inliers supporting the recovered pose see Table 4 The numbers of registered images are again roughly the same for the keyframe sets obtained using the greedy algorithm and those from the ILP solution and also similar to the results presented in 18 Notice t hat lar ger s ubs ets d o not al w a ys yield better registration performance Regarding localization accuracy the median camera position error is around with quartiles and  independent of both the threshold for image similarity and the cover number an exception being the smallest subset image similarity 0.05 cover number 1 where localization is less accurate Similar to 18 there are around 100 cameras which have position errors larger than  
i ii 
123 0 500 1000 1500 0 05 0 05 0 05 0 075 0 075 0 075 0 1 0 1 0 1 
         
Figure 6 Sizes of the connected dominating sets obtained by the greedy algorithm and by solving the ILP with different thresholds for image similarity and different cover numbers for dataset DUBROVNIK Regarding storage the most salient SIFT features extracted by VisualSFM occupy Table 3 Run times in seconds of the search for the minimum DS of DUBROVNIK multi-threaded MATLAB implementations of the greedy algorithm and MOSEK  I L P s ol v e r  Pose Estimation registered images of 800 
Number of images Greedy DS Greedy Con ILP DS ILP Con 
3.4 DUBROVNIK Dataset 
CN 
7 43 s 11 71 s 13 98 s 10 07 s 13 10 s 11 87 s 4 79 s 5 21 s 3 84 s 48 24 s 4 20 s 8 63 s 5 96 s 15 30 s 0 83 s 0 26 s 0 21 s 0 25 s 0 9m 0 25 m 4 5m 20 m 
014 
682 
682 
682 


1 S  A gar w a l  N S n a v el y  I  S i mon S  S e i t z  a nd R  S zel i s ki  Building Rome in a day In  2009  H Bay  A E ss T  T uyt el aars and L  V an Gool  S peeded-up robust features SURF  110\(3\:346…359 2008 3 M  B uj nak Z  K u k e l o v a  a nd T  Paj dl a Ne w e f  ci ent sol ution to the absolute pose problem for camera with unknown focal length and radial distortion In  2011 4 D  Ch e n e t a l City scale l an d mark i d e n ti“catio n o n m o b ile devices In  2011  O  C hum J P h i l b i n  and A Z i sserman Near dupl i cat e i mage detection min-Hash and tf-idf weighting In  2008 6 A  D a v i s on I  R e i d  N  M ol t on and O S t asse MonoS L A M Real-time single camera SLAM  29\(6\:1052…1067 2007  M  F i s chl er and R  B ol l e s Random sampl e consensus A paradigm for model tting with applications to image analysis and automated cartography  24\(6\:381 395 1981  J  M F r ahm et al  B ui l d i n g R ome on a c l oudl ess day  In  2010  M  G are y and D Johnson  W.H Freeman 1979  Googl e Googl e G oggl es Use p i c t u res t o search t h e w eb   2011  S  Guha and S  Khul l e r  Appr oxi mat i o n a l gor i t h ms f o r c onnected dominating sets  20\(4\:374…387 1998  R Hammoud et al  O v erhead-based i m age and vi deo geolocalization framework In  2013  R  Hart l e y a nd A Z i sserman  Cambridge University Press second edition 2003  M Ha vlena A T o rii and T  Pajdla E f  cient structure from motion by graph optimization In  2010  A Irschara C Z ach J M F rahm and H Bischof F rom structure-from-motion point clouds to fast location recognition In  2009  F  K uhn and R  W at t e nhof er  C onst ant t i m e d i s t r i b ut ed dominating set approximation  17\(4\:303…310 2005  X L i  C  W u C  Z ach S  L azebni k and J M  F r ahm Modeling and recognition of landmark image collections using iconic scene graphs In  2008 18 Y  Li N Sn a v ely  an d D  H u tten l o c h e r  Lo catio n reco g n itio n using prioritized feature matching In  2010  Y  L i  N  S na v e l y  D  H ut t e nl ocher  and P  F ua W or l dwi de pose estimation using 3D point clouds In  2012  D L o we Distincti v e i mage features from scale-invariant keypoints  60\(2\:91…110 2004  Mosek ApS  MOS E K hi gh per f o r m ance opt i m i zat i o n s of t ware   2012 22 D Nist  er O Naroditsky and J Bergen Visual odometry In  2004 23 D Nist  er and H Stew enius Scalable recognition with a vocab ulary tree In  2006  A Ol i v a and A T o rral b a Model i n g t he shape of t h e scene A holistic representation of the spatial envelope  42\(3\:145…175 2001  T  S a ttler  B L eibe and L  K obbelt F a st image-based localization using direct 2D-to-3D matching In  2011  G S chi ndl er  M  B ro wn and R  S zel i s ki  C i t y scal e l ocat i o n recognition In  2007  J 031 Sivic and A Zisserman Video Google Ef“cient visual search of videos In  2006 28 N Sn a v e ly  S Se itz  a n d R Sz e lisk i Sk e le ta l g ra p h s fo r ef“cient structure from motion In  2008  A T o ri i  M Ha vl ena and T  Paj d l a  O mni d i r ect i onal i mage stabilization for vi sual object recognition  91\(2\:157 174 2011  A V edal di and B  F ul k e rson VL F eat  A n open and portable library of computer vision algorithms   2008  C  W u  V i s ual S F M  A vi sual st ruct ure from m ot i o n s yst em   2013  Y a hoo F l i c kr  O nl i n e phot o m anagement and phot o s har i ng application   2005 
ICCV CVIU ACCV CVPR BMVC PAMI Comm ACM ECCV Computers and Intractability A Guide to the Theory of NP-Completeness Algorithmica Perception Beyond the Visible Spectrum PBVS Multiple View Geometry in Computer Vision ECCV CVPR Distributed Computing ECCV ECCV ECCV IJCV CVPR CVPR IJCV ICCV CVPR Toward Category-Level Object Recognition CLOR CVPR IJCV 
http://www.google.com/mobile/goggles http://mosek.com http://www.vlfeat.org http://ccwu.me/vsfm http://www.flickr.com 
4 Conclusions Acknowledgements References 
We have shown that the search for the minimum dominating set of a graph„the problem which needs to be solved when selecting a compact yet meaningful subset of a large image collection„can be formulated as an integer linear program and solved ef“ciently by current ILP solvers When comparing the sizes of keyframe sets obtained by the proposed method with those obtained with the state-of-theart greedy algorithm we found a gap of about 5 This result empirically vindicates the greedy method which performs surprisingly close to optimal and much better than what could be expected from the theoretical guarantees Apparently the structure of visual databases is favorable for greedy keyframe selection Still the optimal solution delivers consistently smaller keyframe sets which could be signi“cant for application scenarios with strict memory limitations As an application example we have shown that there is no perceptible difference between the greedy and optimal keyframe sets in location recogn ition problems both for location recognition based on imag e retrieval with geometric veri“cation and for pose estimation w.r.t a pre-computed 3D point cloud The authors would like to thank Vojt\031 ech Franc for discussions about the ILP formulation of the CDS problem and all Flickr users whose images were used in the project 
683 
683 
683 


Copyright © 2009 Boeing. All rights reserved  Architecture Server-1 Server-2 DB2 SURVDB XML Shredder WebSphere Message Broker Ext.4 H Ext.3 G Ext.2 F Ext.1 E C WebSphere MQ TCP/IP Live ASDI Stream IBM Cognos Server-3 IBM SPSS Modeler SPSS Collaboration Deployment Services 


Copyright © 2009 Boeing. All rights reserved  Database Modeling Schemas for correlated ASDI messages translated into equivalent relational schemas  Database tables generated based on classes created from schema definitions  Nine main, eleven supporting tables  Each main table contains FLIGHT_KEY 


Copyright © 2009 Boeing. All rights reserved  Database Modeling 


Copyright © 2009 Boeing. All rights reserved  Correlation Process To archive received ASDI data  Track messages must be correlated with flight plan messages FLIGHT_KEY assigned Uncorrelated data tagged Approx 30 minutes to correlate one day of data 


Copyright © 2009 Boeing. All rights reserved  Historical Data Processing To load correlated data  Uncompress, unmarshall  Create a list of files containing the correlated data  Write data to warehouse 


Copyright © 2009 Boeing. All rights reserved  Live Data Processing Processed using IBM MQ IBM Message Broker and a technique called XML Shredding Message Broker Compute Nodes  Uncompress Node  Extract correlated messages  Shred Node adds to DB Stored Procedure “shreds XML docs and adds to tables 


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a “key, value” list using an XSTL  Queries made against this list of “key, value” pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


