Probability-based incremental association rule discovery algorithm  Ratchadaporn  Amornchewin Faculty of Information Technology King Mongkut’s Institute of Technology Ladkrabang Bangkok, 10520 Thailand ramornchewin@yahoo.com   Worapoj  Kreesuradej   Faculty of Information Technology King Mongkut’s Institute of Technology Ladkrabang Bangkok, 10520 Thailand worapoj@it.kmitl.ac.th   Abstract  In dynamic databases, new transactions are appended as time advances. This may introduce new association rules and some existing association rules would become invalid. Thus, the maintenance of association rules for dynamic databases is an important problem. In this paper, probability-based incremental association rule discovery algorithm is proposed to deal with this problem. The proposed algorithm uses the principle of Bernoulli trials to find expected frequent itemsets. This can reduce a number of times to scan an original database. This paper also proposes a new updating and pruning algorithm that guarantee to find all frequent itemsets of an updated database efficiently. The simulation results show that the proposed algorithm has a good performance   1. Introduction  Data mining is one of the processes of Knowledge Discovery in Database \(KDD\at is used for extracting information or pattern from large database One major area of data mining is association rule mining [1 t h a t d i sc o v e r s hi d d e n kno w l e d ge i n  database. The association rule mining problem is to find out all the rules in the form of   X => Y, where X and Y 002 I are sets of items, called itemsets. The association rule discovery algorithm is usually decomposed into 2 major steps. The first step is find out all large itemsets that have  support value exceed a minimum support threshold and the second steps is find out all the association rules that  have  value exceed a minimum confidence threshold However, a database is dynamic when new transactions are inserted into the database. This may introduce new association rules and some existing association rules would become invalid. As a brute force approach, apriori algorithm may be applied to mining a whole dynamic database when the database has been changed. However, this approach is very costly even if small amount of new transactions is inserted into a database. Thus, the association rule mining for a dynamic database is an important problem. Several research works [6, 7, 8, 9 h a v e  proposed several incremental algorithms to deal with this problem. Review of related works will be introduced in section 2 In this paper, a new incremental algorithm, called probability-based incremental association rule discovery, is introduced. The goal of this work is to solve the updating problem of association rules after a number of new records have been added to a database Based on probabilistic approach, our incremental algorithm predicts infrequent itemsets that have capable of being frequent itemsets after a number of new records have been added to a database. That infrequent itemsets is called expected frequent itemsets. Our algorithm can reduce a number of times to scan an original database. As a result, the algorithm has execution time faster than that of previous methods  2. Previous work  An influential algorithm for association rule mining is Apriori [2 p rio r i co m p u t es f r eq u e n t  ite m s et s i n a  large database through several iterations based on a prior knowledge.  Each iteration has 2 steps which are a joining step and a pruning step. For a frequent itemset, its support must be higher than a userspecified minimum support threshold. The association rule can be discoverd based on frequent itemsets  For dynamic databases, several incremental updating techniques have been developed for mining association rules. One of the previous works for incremental association rule mining is FUP algorithm that was presented by Cheung et al [3 T h e m a j o r idea  of FUP is re-using frequent itemsets of previous 
International Symposium on Computer Science and its Applications 978-0-7695-3428-2/08 $25.00 © 2008 IEEE DOI 10.1109/CSA.2008.39 212 
International Symposium on Computer Science and its Applications 978-0-7695-3428-2/08 $25.00 © 2008 IEEE DOI 10.1109/CSA.2008.39 212 


mining to update with frequent itemsets of an incremental database based on the concepts of Apriori Furthermore, negative border approach is presented by Toivon h om a s et  al [6] an d Fe ld m a n et al   T h e  n e g a ti v e border approach is  an i n cre m e n tal  mining algorithm based on FUP. The border itemset is not a frequent itemset but all its proper subsets are frequent itemsets. The approach need to keep a large number of border itemsets in order to reduce scanning times of an original database To reduce memory space, Hong d Amornchewin et al op os e a n e w approach T h e approach maintains both frequent itemsets and expected frequent itemsets An expected frequent itemset is not a frequent itemset but is expected to become a frequent itemset when a new database is added to an original database. In order to guarantee that all frequent itemsets can be found when a new database is added to an original database, the approach can only allow very small size of an incremental database to insert into an original database Similarly, the proposed method in this paper also keeps not only frequent itemsets but also expected frequent itemsets from an original database. Unlike the previous works, this paper proposes a new technique for predicting expected frequent itemsets. Here, the principle of Bernoulli trials is used to predict the expected frequent itemsets The expected frequent itemsets obtained from the proposed technique has lesser members than the border itemsets and the expected frequent itemsets obtained from the previous technique. This work also proposes a new updating algorithm that guarantee to find all frequent itemsets of a dynamic database efficiently  3. Probability-based Incremental Association Rule Discovery Algorithm  When a dynamic database is inserted new transactions, not only some existing association rules may be invalidated but also some new association rules may be discovered. This is the case because frequent itemsets can be changed after inserting new transactions into a dynamic database. Therefore, an association rule discovery algorithm for a dynamic database has to maintain frequent itemsets when new transactions are inserted into the dynamic database The task of an association rule discovery algorithm for a dynamic database can be divided into three tasks The first task is to update support count of existing frequent itemsets. The second task is to prune existing frequent itemsets that have support count below a minimum support threshold after updating the database. The third task is to discover new frequent itemsets that have support count equal or above a minimum support threshold after updating the database In this section, we describe our algorithm into 2 subsections. Firstly, probability-based expected frequent itemsets is presented. Secondly, updating frequent and expected frequent itemsets is introduced  3.1  Probability-Based Expected Frequent Itemsets  For our algorithm, an original database, which is a database before being inserted new transactions, is firstly mined to find all frequent itemsets that satisfy a minimum support count, denoted k original Furthermore, the proposed algorithm also predicts and keeps expected frequent itemsets that may become frequent itemsets if new transactions are inserted into the original database Our assumption for the new algorithm is that the statistics of new transactions slightly change from original transactions and the maximum number of new transactions that be allowed to insert into an original database is available.   According to the first assumption the statistics of old transactions, obtained from previous mining, can be utilized for approximating that of new transactions. Therefore, the new algorithm uses support count of itemsets obtained from previous mining to approximate the probability of infrequent itemsets in an original database that may be capable of being frequent itemsets when new transactions are inserted into the original database Here, the process of inserting m transactions into an original database of n transaction can be considered as m+n\ernoulli trials, which are \(m+n\equence of identical trials. Each itemset has its probability of appearing in a transaction, denoted by p, i.e., the probability of success. According to the principle of Bernoulli trials, the probability of the number of an itemset to appearing in \(n+m\ transactions, denoted by P\(x\, can be found by the following equation x m n x  p  p x m n  x  P    003 003 000 000 000 000 000 000 000 000   1  where p is the probability of  an itemset appearing in a transaction, m is a number of new transactions, and n is a number of transactions of an original database Thus, if k is a minimum support count after inserting new transactions into an original database, the probability of an itemset to be a frequent intemset in an updated database can be obtained as the following equations   item item k x P k x P    004 1 1 Here, an  expected frequent itemset is an itemset that is not a frequent itemset but has its probability to be a frequent intemset greater than Prob pl  Prob pl  is a threshold constant specified by users.  Prob pl indicates the minimum confidence level that a  promising 
213 
213 


frequent itemset will be a frequent itemset after inserting new transaction into an original database  3.2. Updating frequent and expected frequent itemsets  When new transactions are added to an original database, an old frequent k-itemset could become an infrequent k-itemset and an old expected frequent kitemset could become a frequent k-itemset. This introduces new association rules and some existing association rules would become invalid. To deal with this problem, all k-itemsets must be updated when new transactions are added to an original database. The notation used in this section is given in Table1  Table 1 The notation for Updating frequent and expected frequent itemsets algorithm DB Original database db Incremental Database UP Updated database k Number of  itemset 005  Minimum support 006  Minimum Expected Frequent C k Candidate k-itemset F k  Frequent  k-itemset EF k  Expected Frequent k-itemset  Here, a new updating frequent and expected frequent itemsets algorithm shown in Figure 1 is proposed in this paper. The algorithm consists of three phases. The first phase is updating 1- frequent and expected frequent itemsets, i.e. line1-3. The second phase is repeatedly updating the other frequent and expected frequent itemsets by using only an incremental database, i.e. line 6-11. The third phase is scanning an original database, i.e. line 13-22 The First phase is updating 1- frequent and expected frequent itemsets. According to our propose, the 1candidate itemsets of an updated database, i.e UP C 1  can be found by combining the 1-candidate itemsets of an original database, i.e DB C 1 with the 1-candidate itemsets of an incremental database, i.e db C 1 Then, the support count of UP C 1 can be updated by scanning only an incremental database. Then, the result of this phase is consist of  1-frequent and expected 1-itemsets of an updated database The second phase has 2 major steps which are a generating k incremental candidate itemsets step and an updating support count of k- incremental frequent and k- incremental expected frequent itemsets  step for k greater than or equal to 2. For k 2, the 2- incremental candidate itemsets are easily obtained by joining UP F 1  with UP F 1 For k 2, the algorithm is firstly find k  candidate itemsets of an incremental database, i.e db k C  by joining db k F 1 with db k F 1 Similar to Apriori algorithm the k candidate itemsets of an incremental database can be the updated frequent itemsets, i.e UP k F only if the subsets of the k candidate itemsets of an incremental database must be in the k 1 \- updated frequent. Thus, the k incremental candidate itemsets will keep only the k- candidate itemsets of an incremental database whose subsets of the k candidate itemsets are in the k 1\ - updated frequent intemsets This can prune the k candidate itemsets of an incremental database that can’t be the k updated frequent intemsets  Algorithm1 : Updating  frequent and expected frequent itemsets Algorithm          scanDB _ Temp clear  do end  k k  UP  X c and scanDB _ Temp X X EF EF  UP  X c and scanDB _ Temp X X F F  do end  db  X c DB  X c UP  X c  do scanDB _ Temp X all for  scanDB _ Temp X all for Database Original Scan  do  m k  and scanDB _ Temp  while  k  if end  do end  k k  scanDB _ Temp of itemset imum max the is m    scanDB _ Temp  m return  itemset k Update  Itemset Candidate Generate  do k  F  k for  else  k k  itemset Update  k if  k  EF  F  Output count their and EF  F  C     k  db  DB  Input UP UP k UP k UP k UP k UP k UP k k k k UP k UP k UP k DB DB DB DB UP UP 23 22 1 21 20 19 18 17 16 15 14 2 13 12 11 1 10 9 8 7 2 6 5 1 4 1 3 1 2 1 1 1 1 1   005  007 006 010 011  005 004 010 011    010 010 007 012 013       012 013       006 006 005   Figure 1  Updating frequent and expected frequent itmesets Algorithm When any k itemsets are not in the union set of the original k frequent and the original k expected frequent itemsets, but is in the k- incremental candidate itemsets, their support counts need to be specially updated. This is the case because their support counts obtained from an original database are not available Here, their support counts in an original database are assumed to be equal to the sum of 1 DB 006 and their support counts from an incremental database. If any k itemsets have support counts below updated min support count, i.e 005 UP the k itemsets can’t be the k updated frequent itemsets. On the other hand, if any k itemsets have support counts above or equal to an 
214 
214 


0 1000 2000 3000 4000 Average of execution time \(sec 3 4 Minimum support threshold T10I4D10 FUP Borders Probability-Based updated min support count, the k itemsets are likely to be the k updated frequent itemsets. Thus, the k itemsets, which have support counts above or equal to an updated min support count, are set aside for finding their true support counts from an original database At the third phase, an original database is scanned to find true support counts for the k itemsets that are likely to be the k updated frequent itemsets. The support counts of the likely k updated frequent itemsets are found and updated by scanning an original database. Then, all k updated frequent itemsets and k updated expected frequent itemsets are found  4. Experiments  To evaluate the performance of probability-based incremental association rules discovery algorithm, the algorithm is implemented and tested on a PC with a 2.8 GHz Pentium 4 processor, and 1 GB main memory The experiments are conducted on a synthetic dataset called T10I4D10K. The technique for generating the dataset is proposed by Agrawal and et  T h e synthetic dataset comprises 110,000 transactions over 100 unique items, each transaction has 10 items on average and the maximal size itemset is 4 Firstly, the proposed algorithm with Prob pl 0.06 used to find association rules from an original database of 10,000 transactions. Then, the same sizes of incremental databases, i.e. 10% of the original database, are added to the original database for 100 trials.  For comparison purpose, FUP and Borders algorithm are also used to find association rules from the same original database and the same incremental databases. Figure 2 and Table 2 show the average of execution time for FUP, Borders and our approach. The results also show that the proposed algorithm has much better running time than that of  FUP  Table 2 Average of Execution time   Average of Execution time for 100 trials Min_sup FUP Borders Probability-Based 3% 3195.6939 2660.9297 2354.24533 4% 2274.4477 2087.8636 1931.19147                       Figure 2  The execution time of  FUP, Borders and the proposed algorithm 5. Conclusion   We have proposed  probability-based incremental association rule discovery algorithm. Assuming that the two thresholds, minimum support and confidence, do not change, the algorithm can guarantee to discover all frequent itemsets. From the experiment, our algorithm has better running time than that of FUP and Borders algorithm. In the future, further researches and experiments on the proposed algorithm will be presented  6. References  1  R. Ag r a wa l T  I m ie lin s k i  a nd A S w a m i M inin g association rules between sets of items  in large databases”, In Proceeding of the ACM SIGMOD Int'l Conf. on Management of Data  \(A CM SIGMOD '93\, Washington USA,May 1993, pp. 207-216 2 R. A g ra wa l a nd R. Sri k a n t  F a s t a l g o r ith m s  f o r m inin g  association rules”, In Proceedings of 20 th Intl Conf. on Very Large Databases \(VLDB'94\, pages487-499, Santiago, Chile September 1994, pp. 478 -499 3 D  C h e ung J   H a n, V. Ng a nd C  Y   W ong   M a i nt e n a n ce o f  discovered association rules in large databases: An incremental updating technique”, In 12th IEEE International Conference on Data Engineering, February 1996, pp. 106-114 4 D W  Ch eu n g  S  D  L e e B  Kao   A  Gen e r a l in c r e m en tal  technique for maintaining discovered association rules” , In Proceedings of the 5 th Intl. Conf. on Database Systems for Advanced Applications \(DASFAA'97\, Melbourne, Australia April 1997, pp. 185-194 5  H   T o i v one n S am plin g La r g e  Da t a bas e s f o r A s s o c i a tion  Rules”, Proceeding of  the 22th International conference on Very Large Data Bases,  September 1996,pp. 134-145 6 S. T h o m a s S. Bod a g a l a K  A l s a bti a nd S. Ra n k a   A n  efficient algorithm for  the incremental updation of association rules in large databases” , In Proceedings of the 3rd Intl. Conf on Knowledge Discovery and Data Mining \(KDD'97\, New Port Beach, California, August 1997, pp. 263-266 7 C C Ch an g Y C  L i an d J S   L e e  A n  e f f i ci en t al go ri t h m  f o r  incremental mining of association rules”, Proceedings of the 15th international workshop on research issues in data engineering: stream data mining and applications \(RIDESDMA’05\ IEEE, 2005  8  R   Fe ld m a n, Y  A u m a nn, a n d O  L i ps ht a t  B or d e r s A n  efficient algorithm for association generation in dynamic databases”,Journal,Intelligent Information System, 1990, pp. 6173 9  T  P   Ho n g C Y W a n g  an d Y H Tao   A  n e w in cr e m en t a l  data mining algorithm using pre-large itemsets”, Journal Intelligent Data Analysis, Vol. 5, No.2, pp. 111-129, 2001 10  R  A m o r nc hew i n a nd W   K r e e s u r a de j   I n c r e m e nt a l  association rule mining using promising frequent itemset algorithm”, In Proceeding 6th International Conference on Information, Communications and Signal Processing,  Dec. 1013 2007, pp.1-5  
215 
215 


introduced to adjust the slope of the line controlling the dependence of the steering on the speed The original intention with hacked controller was to extract the main variables into a vector and then use an evolutionary strategy to evolve the values of those However in order to meet the competition deadline there was insuf\002cient time for this and instead the values of the variables were optimised by hand The 002rst approach was to observe the behaviour of the car as it was being driven by the hacked controller and then to make adjustments to the variables to correct problems that were observed Due to the non-linear interactions of the variables and the excessive time spent watching the car in visual mode this approach proved to be ineffective Best results were obtained by running the simulation in results only mode which allows evaluations to be done much faster than real-time and therefore many more settings of the variables can be tried The approach taken was a kind of direct policy optimisation where variables were initially set based on intuition or taken from the SimpleSoloController Trials were made with each one adjusted by around 10 or 20 of its current value The 002tness of each setting was noted and I looked for patterns in values of variables or combinations of variables that worked well In total around 50 trials were made The variables adjusted in this way together with their 002nal values in parentheses were steeringFac\(0.35 trackFac\(0.38 breakSpeedLimit\(105 trackPositionLimit 0.15 On three trials on E-Track 3 this hacked controller scored 7815 7776 and 7811 giving an average of 7801 a signi\002cant improvement over the original controller In summary hacked controller represents a somewhat hastily designed effort to use the available sensor data to improve on the supplied SimpleSoloController The supplied controller was extended in the ways described above and a kind of manually operated evolutionary process was used to tune the parameters This was found to me more effective than tuning the parameters by trying to directly observe their effects on speci\002c aspects of the driving behaviour It would be interesting to try evolving these values using an evolution strategy to see whether the manually chosen parameters can be signi\002cantly improved on they probably can C Matt Simmerson NEAT controller The idea behind this controller was to evolve a neural network that controlled a racing car around many tracks equally well based on a set of input data provided by the racing environment The added complication for my controller was to evolve the network topology given no domain knowledge a priori 1 De\002ning the controllers The controller was trained using the NEAT algorithm that e v olv ed populations of neural networks and was created using the NEAT4j software an implementation of the NEAT algorithm The NEAT4j implementation allows for the initial selection of a sub set of 3 inputs of the available 29 sources of input data The 3 outputs controlled the power in the range the gear change in the range and the steering in the range  The throttle and brak e actions were Boolean so the power output node was used to apply throttle for 277 0.6 or apply the brake for 241 0.4 For the middle values then the neither the throttle or brake would be applied and hence the controller would coast The gear change would attempt to change up if the output node was 277 0.5 and change down otherwise The gears selected were limited to R N 1 2 3 4 5 As all the output nodes used a sigmoid acti v ation function the actual values created as the controller actions were scaled from to the appropriate range The subset of inputs from the table de\002ned above available for this controller were 1 Current speed 2 Angle to track axis 3 The 19 track sensors 4 Track position with respect to left and right edges 5 Current gear selection 6 The 4 wheel spin sensors 7 Current RPM All the inputs were scaled to be in the range or 0,1 depending on the sign of the input This prevents large input values completely swamping other smaller input signals The initial controllers were very simple with 3 randomly selected input nodes connected to one of the output nodes such that all output nodes were connected to exactly one input node but an input node could be connected to more than one output node 2 Controller evolution The NEAT algorithm in essence uses a genetic algorithm to create a neural network topology from a given genome Each genome consists of a set of node genes that describe an individual neuron and connection genes which describe a nodes connections The population size was just 100 as this was a reasonable compromise between evolution and the time it took for each epoch The mutation and crossover operators were those de\002ned by the NEAT algorithm The parent selector function was a tournament round where the allowed parents were pitted against each other with the winner taking the spoils i.e the 002ttest Recurrency was allowed in this experiment 3 Training the controllers The car was trained on just one track G3 which was not one selected for the initial valuation This track was selected as it had some varying turns i.e left right curve and also straights of varying lengths into the various corners Ideally for the sake of generalisation I would have liked to train the cars over several tracks however the current version of the TORCS environment prevented this 4 Controller evaluation The car was tested on the track for a maximum of 4000 steps equivalent to around 80 seconds real time If the car sustained more than 100 points of damage out of a maximum of 10000 the evaluation for that car was aborted The overall 002tness of the car was calculated thus F c  2 003 Dr  do  speedmax  C 1 1 Dr was the distance raced value reported by TORCS engine and could be both positive and negative 


2 d was the value reported by the TORCS engine and has a maximum value of 10000 3 o is a measure of how much the car stayed on the track This was necessary to prevent the car using the barriers as a guide with no damage penalty Until this variable was added it prevented any really successful controllers The edge of the track was represented by 1 left and 1 right The outside value was calculated as 0 if the car was in these limits and Abs\(track position 1 for values outside these edges 4 Max speed was calculated throughout the cars trial based on the speeds reported by the TORCS engine This was to try and reward fast cars early on that crashed as the name of the game is speed 5 C was used to ensure the 002tness value was always positive and was set to 10000 Negative values were created when cars went the wrong way round the track or had high-speed crashes near the start resulting in large damage Whilst the 4000 time steps used for evaluation represented 80 seconds real time the TORCS engine allowed a non-GUI version which was evaluated in around 3-4 seconds With the population was set to 100 each evolutionary epoch lasted 400 seconds The entry for Neat4J was selected from the winning phenotype from the 170th generation i.e nearly 19 hours on my dual core laptop This had a good level of performance over a number of different track types e.g oval twisty etc D Diego Perez and Yago Saez Rule-based controller The idea of this controller is to evolve a set or rules that drives a vehicle using sensors as input data The usage of sensors to obtain autonomous driving has been addressed by numerous researchers 9 10 11 as well as the use of evolutionary algorithms in this 002eld 13 14 1 Input data effectors and rules The input data is discretized from the values of four sensors  angle  discretized to where 0 means the smaller angles trackPos  with a discretization performed in a range where 0 means centered on the track and 1 the car near the edges speedX  in a range where 0 means lo wer speed than higher v alues and track  where only three of these sensors have been used front and inmediate sensors on right and left and discretized to a unique range where 0 means that a track edge has been detected beyond 20 meters 1 when the track edge is up to 20 meters and 2 in case no track edge is detected A key part of the design is the usage of symmetry for the 002rst two sensors This concept works using the absolute value of the sensor to match to the proper discretized value The objective of this approach is to avoid duplication of efforts by reducing the search space The effectors of the controller have been designed as follows throttle and brake  where both pedals have been codi\002ed in a common output to avoid non-sense values as full gas in both pedals simultaneously Hence a unique value is applied and both gas pressures are extracted from it steer  codi\002ed as a real number from 1 to 1 and discretized with a precision of 0,1 gear  which changing process consists on increasing the current gear when the rpm value is higher than 6000 and decreasing it if it is below 3000 The discretization and codi\002cation applied over the input data and effectors allows us to create a set of 120 rules where conditional part is composed by the sensors and the actions are formed by acceleration braking and steering These rules compose the base individual 2 Application of an evolutionary rule system Traditional random initialization of individuals used in evolutionary techniques do not work properly in this 002eld because it is almost impossible to obtain a con\002guration that drives the vehicle correctly by chance This is the reason of getting a base individual before evolve it to obtain better results The algorithm used to get this base individual is a generation of a subset of rules that allows the vehicle to end a lap minimizing the angle of the car with the track axis Each one of this rules is created by testing how each allowed combination of acceleration and steering behaves when the condition of the rule is triggered Once we get the base rule set the evolving individual is extracted from it taking all its rules Therefore the individual is composed by a set of rules each one of them formed by condition and effectors that need to be evolved to obtain the controller To evolve this individual the algorithm executes evolutionary steps until a stopping criteria is reached The evaluation of the individual is performed recording lap time and damage suffered setting the 002tness using a linear combination of both values with weights of 0.4 and 0.6 respectively in order to avoid over\002tting to the training circuit In this system we can not decide when a rule is better than another because the behaviour of the individual depends on the whole set of rules used Because of this selection operator has been designed as a random pick-up from the rules pool taking two of them to apply uniform crossover Finally mutation operator is performed over the new rule applying an addition of 006 1 unit to the left part and 006 0  3 to the effectors of the rule obeying limits and codi\002cation precision The next step in this algorithm consists of searching for a rule from the individual where its conditional part is most similar to the new rule This rule is extracted from the pool and the new one substitutes it The new set of rules is then evaluated and its 002tness is compared with the one calculated before inserting that new rule Only if the new rule set is worse the substituted one is retrieved and the new rule is eliminated Results have proved this algorithm to be effective reducing lap times of the base individual in few generations keeping the car damage almost nonexistent The usage of symmetry however brought a side effect that was not expected the car drives in a smoothly zig-zag trajectory centered on the circuit This is because a small steering value can center the vehicle on the track but not necessarily drive it parallel to the track axis Nevertheless the controller 


TABLE III R ESULTS OF THE FIRST STAGE OF THE EVALUATION PROCESS  T HE REPORTED ARE THE MEDIAN OVER 10 RUNS    Entry  Ruudskogen  Street-1  Speedway     Kinnaird-Heether et al  6716.7  3692.9  14406.9    Lucas  4134.2  5502.8  12664.5    Simmerson  5934.0  6477.8  12523.3    Perez et al  3786.9  2984.8  317.3    Tan et al  3443.5  2998-5  10648.2     C Sample Controller  4465.1  4928.8  7464.5    Java Sample Controller  5593.8  2963.2  5689.9   behaved in a reasonable way only with the exception of some speci\002c circuits the oval ones These circuits similar to Nascar tracks have banked curves which make the zig-zag movement completely uncontrollable E Chin Hiong Tan and Kay Chen Tan The entry submitted by Chin Hiong Tan and Kay Chen Tan was developed in a three-step process First the sensory information was aggregated and preprocessed second a parametrized controller based on simple rules was designed 002nally the parameters of controller were optimized using evolution strategies The resulting controller drives in the direction where the range\002nder sensors indicate the largest free distance with a speed dependent on that distance IV R ESULTS The entries were scored through a two stages process which involved three tracks available in TORCS the Ruudskogen the Street-1 and the D-Speedway The 002rst warm up stage was aimed at eliminating particularly bad performing controllers Each controller raced alone in each of the three tracks and its performance was measured as the distance covered in 10000 game tics approximately 200 seconds of actual game time For each of the three selected tracks we run each controller ten times The performance has been computed as the median the 50th percentile over the ten runs to avoid any issue about skewness Table III compares the performance of the 002ve controllers submitted to the one of the two sample programmed controllers provided by the organizers The results show that the controller submitted by Leonard Kinnaird-Heether and Robert Reynolds outperforms the other controllers in all the tracks but the Street-1 track As can be noted the performances of the controllers are highly different among the three tracks but they generally compare well to the performances of the sample controllers provided by the organizers In particular the entries submitted respectively by Leonard Kinnaird-Heether et al by Simon Lucas and by Matt Simmerson the 002rst three controllers reported in Table III performs consistently better than the sample controllers almost in all the three tracks As all the 002ve submitted controllers performed well on the 002rst stage none of them was eliminated from the second stage in which the controllers competed together in each of the three tracks In this stage the task consisted of completing three laps and each controller was scored based on its arrival order TABLE IV R ESULTS OF THE SECOND STAGE OF THE EVALUATION PROCESS  T HE SCORES REPORTED ARE THE MEDIAN OVER 10 RUNS    Entry  Ruudskogen  Street-1  Speedway  Total     Simmerson  10  10  6  26    Kinnaird-Heether et al  4  8  10  22    Lucas  6  6  8  20    Tan et al  5  5  5  15    Perez et al  5.5  4.5  5  14   using the same point system used in F1 10 points to the 002rst controller that completed the three laps 8 points to the second one 6 to the third one 5 to the fourth and 4 to the 002fth one Ten runs for each track were performed using as start grid a random permutation of the competitors in order to test the reliability of the controllers performance Then the score of a controller on one track was computed as the median of the scores obtained during the ten runs The 002nal score for each controller was 002nally computed as the sum of the points collected on each track Table IV shows the 002nal scoreboard Matt Simmerson won the competition with 26 points followed by Leonard Kinnaird-Heether et al with 22 points by Simon Lucas with 20 points by Tan Chin Hiong with 15 points and 002nally by Diego P 264 erez 14 points This results suggest that although the controllers submitted by Kinnaird-Heether and Reynolds is fast the one submitted by Simmerson is more reliable especially in the presence of other controllers Finally it is worthwhile to underline that the second stage of the evaluation process suggested that all the submitted controllers have poor overtaking and obstacle-avoidance capabilities whereas these features are very important to succeed in a racing competition Additional results and a video with the highlights of the competition are available on the webpage of the competition The reason Simmerson's controller won over KinnairdHeether and Reynold's was probably that the latter had been optimized for racing on tracks with smooth curves in the presence of other cars Simmerson's controller had been trained on the 223G3\224 track that included sharp turns like Ruudskogen but on its own Both of these controllers were optimized with stochastic algorithms and it stands to reason that such approaches outperform the hand-tuning used by Lucas V T HE FUTURE OF THE CAR RACING COMPETITION While this competition differed greatly from the competitions organized during 2007 in that a more sophisticated racing game was used there was also a great deal of continuity Not only in that some of the participants of the 2007 competitions also participated in the current competition but also in the similarity of rules and arrangements The organizers believe that this continuity is very important for the competition to be successful We need a high participation level to ensure that a broad spectrum of approaches are represented and regular repetitions of the competition to ensure that the participants have time 


to perfect their approaches Our aim is to ensure further continuity through holding a series of future competitions using gradual re\002nements of the rules and software used in the current competition The following improvement will be made to the software in time for the CIG competition 017 The installation process will be streamlined 017 Reliability will be improved 017 Support for multi-car and multi-track training will be added making it easier to apply co-evolution and incremental evolution 017 More sample controllers and trainers e.g temporal difference learning trainers will be supplied An amusing illustration of the need to improve reliability is that in an early version of the software it was possible to achieve the 002tness value of driving a whole lap simply by slowly driving up to and passing the start line the car starts 100 meters before that line then turning and passing the line again This 003aw is inherent in TORCS presumably because its developers never thought of anyone doing something so bizarre Evolutionary algorithms however are good at coming up with bizarre solutions and Matt Simmerson's algorithm quickly evolved a controller that exploited this bug A patch for this bug is now part of the software package A reviewer of the paper summarizing the previous car racing competitions pointed out that in its current form the competition is not only about learning algorithms It is certainly possible to hand-code a non-learning controller that outperforms the best CI-based controllers Indeed the best controllers that come with the TORCS game developed by the TORCS developers are non-learning and by far outperform all the controllers submitted to this competition so far though they often access information state information that is not directly available through the competition API Of course we hope that future editions of this competition will see CI-based contributions that perform better than the best hand-coded ones and there are no reasons why this should not happen Still it would be interesting to run a version of the competition that compared only the quality of the learning algorithm One way could be to de\002ne a standard e.g neural network-based controller architecture and then provide an interface for a learning algorithm to set the parameters for this controller optimally given a certain numbers of laps around an unknown track The participants would then submit an algorithm rather than a controller to be run and evaluated by the organizers of the competition Another interesting version of the competition would be one where the controllers where presented with a richer but more primitive state description in particular visual data This could come in the form of the full rendered 3D view through the controlled car's windscreen or a part of it Such a state description would ultimately give the controllers more information and thus allow for better driving but would also require more complex controllers Given the various interesting variations on the car racing concept that are possible our plan is to organize editions of the car racing competition in conjunction with several international conferences and at each conference hold both the competition in its original form and some variation on the concept like the ones suggested above VI C ONCLUSION We have described the organization rules and software of the car racing competition in the form it was organized in conjunction with IEEE WCCI 2008 Four out of 002ve participating teams described the architecture and training of their controllers We have also reported the scoring procedure and results of the competition and plans for future competitions We hope that this paper in addition to serving as a record of the competition will provide organizers of similar competitions with inspiration and insights and that the descriptions of the controllers will be useful for researchers working on learning vehicle control in general and for participants in future car racing competitions in particular R EFERENCES  J T ogelius S M Lucas H Duc Thang J M Garibaldi T Nakashima C H Tan I Elhanany S Berant P Hingston R M MacCallum T Haferlach A Gowrisankar and P Burrow 223The 2007 ieee cec simulated car racing competition,\224 Genetic Programming and Evolvable Machines  2008 A v ailable http://dx.doi.org/10.1007/s10710-008-9063-0  223The open raci ng car simulator  224 Online A v ailable http://torcs.sourceforge.net  223Softw are manual of the car racing competition 224 WCCI-2008 A v ailable http://cig.dei.polimi.it/wpcontent/uploads/2008/04/manual  v03.pdf  R G Re ynolds and M Z Ali 223Computing with the social f abric The evolution of social intelligence within a cultural framework,\224 IEEE Computational Intelligence Magazine  vol 3 no 1 pp 18\22630 2008  R G Re ynolds M Z Ali and T  Jayyouzi 223Mining the social f abric of archaic urban centers with cultural algorithms,\224 Computer  vol 41 no 1 pp 64\22672 2008  K O Stanle y  223Ef 002cient e v olution of neural netw orks through complexi\002cation,\224 Ph.D dissertation Department of Computer Sciences University of Texas Austin TX 2004  M Simmerson 223Neat4j homepage 224 2006 Online A v ailable http://neat4j.sourceforge.net  S Baluja and R Caruana 223Remo ving the genetics from the standard genetic algorithm,\224 in Proceedings of the international conference on machine learning ICML  1995  R Sukthankar  S Baluja and J Hancock 223Proto yping intelligent vehicle modules,\224 in Proceedings of the International Conference on Robotics and Automation ICRA  1997  J T ogelius and S M Lucas 223Ev olving controllers for simulated car racing,\224 in Proceedings of the Congress on Evolutionary Computation  2005  227\227 223Ev olving rob ust and specialized car racing skill s 224 in Proceedings of the IEEE Congress on Evolutionary Computation  2006  J Bernard J Gruening and K Hof fmeister  223Ev aluat ion of v ehicle/driver performance using genetic algorithms,\224 Society of Automotive Engineers  1998  D Floreano T  Kato D Marocco and E Sauser  223Coe v olution of active vision and feature selection,\224 Biological Cybernetics  vol 90 pp 218\226228 2004  J T ogelius and S M Lucas 223 Arms races and car races 224 in Proceedings of Parallel Problem Solving from Nature  Springer 2006  223The car racing competition homepage 224 WCCI-2008 Online Available http://cig.dei.polimi.it/?page  id=5 


Table 3 Compressed sizes and number of extracted itemsets f or the itemset selection algorithms Candidate Itemsets S ET P ACK S ET P ACK G REEDY K RIMP Dataset min-sup  sets c  T  c  T  c  T b    sets c  T  c  T  c  T b    sets  bits  sets anneal 175 8837 20777 89.9 103 20781 89.9 69 31196 53 breast 1 9920 5175 63.7 42 5172 63.9 49 4613 30 courses 55 5030 64835 84.9 268 64937 85.1 262 73287 93 mammals 700 7169 65091 83.4 427 65622 84.1 382 124737 125 mushroom 1000 123277 313428 70.9 636 262942 59.5 1225 474240 140 nursery 50 25777 314081 93.0 276 314295 93.1 218 265064 225 pageblocks 1 63599 11961 78.3 92 11967 78.3 95 10911 53 tic–tac–toe 7 34019 23118 92.0 620 23616 94.0 277 28957 159 large candidate family for mushroom  For comparison we use the same candidates for K RIMP  We also compare to S ET P ACK G REEDY  which required 1–12 minutes 7 minutes typically with an exception of 2 1 2 hours for mushroom  Comparing the results of this experiment Table 3 with the results of G REEDY P ACK in the previous experiment we see that the selection process is more strict now even fewer itemsets are regarded as interesting enough Large candidate collections are strongly reduced in number up to three orders of magnitude On the other hand the compression ratios are still very good The reason that G REEDY P ACK produces smaller compression ratios is because it is allowe d to consider any itemset Further the fact alone that even with this very strict selection the compression ratios are generally well below 90 show that these few sets are indeed of high importance to describing the major interactions in the data If we compare the number of selected sets to K RIMP  we see that our method returns in the same order as many itemsets These descriptions require far less bits than tho se found by K RIMP  As such ours are a better approximation of the Kolmogorov complexity of the data Between S ET P ACK and S ET P ACK G REEDY the outcomes are very much alike this goes for both the obtained compression as well as the number of returned itemsets However the greedy search of S ET P ACK G REEDY allows for much shorter running times 8 Discussion The experimentation on our methods validates the quality of the returned models The models correctly detect dependencies in the data while ignoring independencies Only a small number of itemsets is returned which are shown to provide strong compression of the data By the MDL principle we then know these describes all important regularities in the data distribution in detail ef\002ciently and witho ut redundancy This claim is further supported by the high classi\002cation accuracies our models achieve The G REEDY P ACK algorithm generally uses more itemsets and obtains better packing ratios than S ET P ACK  While G REEDY P ACK is allowed to use any itemset S ET P ACK may only use frequent itemsets This suggests that we may able to achieve better ratios if we use different candidates  for example low-entropy sets 16  The running times of the experiments reported in this work range from seconds to hours and depend mainly on the number of attributes and rows of the datasets The exhaustive version S ET P ACK may be slow on very large candidate sets however the greedy version S ET P ACK G REEDY can even handle such families well Considering that our curren t implementation is rather na¨\021ve and the fact that both methods are easily parallelized both G REEDY P ACK and S ET P ACK G REEDY are suited for the analysis of large databases The main outcomes of our models are the itemsets that identify the encoding paths However the decision trees from which these sets are extracted can also be regarded as interesting as these provide an easily interpretable view o n the major interactions in the data Further just consideri ng the attributes used in such a tree as an itemset also allows for simple inspection of the main associations In this work we employ the MDL criterion to identify the optimal model Alternatively one could consider using either BIC or AIC both of which can easily be applied to judge between our decision tree-based models 9 Conclusions In this paper we presented two methods that 002nd compact sets of high quality itemsets Both methods employ compression to select the group of patterns that describe all interactions in the data best That is the data is considere d symmetric and thus both the 0s and 1s are taken into account in these descriptions Experimentation with our methods 
596 
596 


showed that high quality models are returned Their compact size typically tens to thousands of itemsets allow fo r easy further analysis of the found interactions References 1 C  C  A g g a r w a l a n d P  S  Y u  A n e w f r a m e w o r k f o r itemset generation In Proceedings of the ACM SIGACTSIGMOD-SIGART symposium on Principles of Database Systems PODS  pages 18–24 ACM Press 1998 2 R  A g r a w a l  H  M a n n i l a  R  S r i k a n t  H  T o i v o n e n  a n d A  I  Verkamo Fast discovery of association rules In Advances in Knowledge Discovery and Data Mining  pages 307–328 AAAI 1996 3 S  B r i n  R  M o t w a n i  a n d C  S i l v e r s t e i n  B e y o n d m a r k e t baskets Generalizing association rules to correlations In ACM SIGMOD International Conference on Management of Data  pages 265–276 ACM Press 1997 4 S  B r i n  R  M o t w a n i  J  D  U l l m a n  a n d S  T s u r  D y n a m i c itemset counting and implication rules for market basket data In ACM SIGMOD International Conference on Management of Data  pages 255–264 1997 5 B  B r i n g m a n n a n d A  Z i m m e r m a n n  T h e c h o s e n f e w  O n identifying valuable patterns In IEEE International Conference on Data Mining ICDM  pages 63–72 2007 6 T  C a l d e r s a n d B  G o e t h a l s  M i n i n g a l l n o n d e r i v a b l e f r e quent itemsets In Proceedings of the 6th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases  pages 74–85 2002 7 V  C h a n d o l a a n d V  K u m a r  S u m m a r i z a t i o n c o m p r e s s i n g data into an informative representation In Proceedings of the IEEE Conference on Data Mining  pages 98–105 2005 8 F  C o e n e n  T h e L U C S K D D d i s c r e t i s e d  n o r m a l i s e d A R M and CARM data library 2003 9 G  F  C o o p e r a n d E  H e r s k o v i t s  A B a y e s i a n m e t h o d f o r the induction of probabilistic networks from data Machine Learning  9:309–347 1992 10 T  C o v e r a n d J  T h o m a s  Elements of Information Theory 2nd ed John Wiley and Sons 2006 11 W  D u M o u c h e l a n d D  P r e g i b o n  E m p i r i c a l b a y e s s c r e e n i n g for multi-item associations In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 67–76 2001 12 C  F a l o u t s o s a n d V  M e g a l o o i k o n o m o u  O n d a t a m i n i n g  compression and kolmogorov complexity In Data Mining and Knowledge Discovery  volume 15 pages 3–20 Springer 2007 13 P  D  G r  u n w a l d  The Minimum Description Length Principle  MIT Press 2007 14 J  H a n  H  C h e n g  D  X i n  a n d X  Y a n  F r e q u e n t p a t t e r n mining Current status and future directions In Data Mining and Knowledge Discovery  volume 15 Springer 2007 15 J  H a n a n d J  P e i  M i n i n g f r e q u e n t p a t t e r n s b y p a t t e r n growth methodology and implications SIGKDD Explorations Newsletter  2\(2\:14–20 2000 16 H  H e i k i n h e i m o  E  H i n k k a n e n  H  M a n n i l a  T  M i e l i k  a i nen and J K Sepp¨anen Finding low-entropy sets and trees from binary data In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 350–359 2007 17 S  J a r o s z e w i c z a n d T  S c h e f f e r  F a s t d i s c o v e r y o f u n e x p ected patterns in data relative to a bayesian network In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 118–127 2005 18 S  J a r o s z e w i c z a n d D  A  S i m o v i c i  I n t e r e s t i n g n e s s o f frequent itemsets using bayesian networks as background knowledge In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 178–186 2004 19 A  J  K n o b b e a n d E  K  Y  H o  M a x i m a l l y i n f o r m a t i v e k itemsets and their ef\002cient discovery In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 237–244 2006 20 A  J  K n o b b e a n d E  K  Y  H o  P a t t e r n t e a m s  I n Proceedings of the 10th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases  pages 577–584 2006 21 P  K o n t k a n e n a n d P  M y l l y m  a k i  A l i n e a r t i m e a l g o r i t h m for computing the multinomial stochastic complexity Information Processing Letters  103\(6\:227–233 2007 22 M  v a n L e e u w e n  J  V r e e k e n  a n d A  S i e b e s  C o m p r e s s i o n picks the item sets that matter In Proceedings of the 10th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases  pages 585–592 2006 23 M  L i a n d P  V i t  a n y i  An Introduction to Kolmogorov Complexity and its Applications  Springer-Verlag 1993 24 R  M e o  T h e o r y o f d e p e n d e n c e v a l u e s  ACM Trans Database Syst  25\(3\:380–406 2000 25 A  J  M i t c h e l l J o n e s  G  A m o r i  W  B o g d a n o w i c z  B Krystufek P J H Reijnders F Spitzenberger M Stubb e J B M Thissen V Vohralik and J Zima The Atlas of European Mammals  Academic Press 1999 26 K  V  S  M u r t h y  On growing better decision trees from data  PhD thesis Johns Hopkins Univ Baltimore 1996 27 S  N i j s s e n a n d  E Fromont Mining optimal decision trees from itemset lattices In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 530–539 2007 28 N  P a s q u i e r  Y  B a s t i d e  R  T a o u i l  a n d L  L a k h a l  D i s c o vering frequent closed itemsets for association rules Lecture Notes in Computer Science  1540:398–416 1999 29 J  R i s s a n e n  F i s h e r i n f o r m a t i o n a n d s t o c h a s t i c c o m p l e xity IEEE Transactions on Information Theory  42\(1\:40–47 1996 30 A  S i e b e s  J  V r e e k e n  a n d M  v a n L e e u w e n  I t e m s e t s t h a t compress In Proceedings of the SIAM Conference on Data Mining  pages 393–404 2006 31 N  T a t t i  M a x i m u m e n t r o p y b a s e d s i g n i 002 c a n c e o f i t e m s e t s Knowledge and Information Systems KAIS  2008 Accepted for publication 32 N  T a t t i a n d H  H e i k i n h e i m o  D e c o m p o s a b l e f a m i l i e s o f itemsets In Proceedings of the 12th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases  2008 33 J  V r e e k e n  M  v a n L e e u w e n  a n d A  S i e b e s  C h a r a c t e r i s i ng the difference In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 765–774 2007 34 J  V r e e k e n  M  v a n L e e u w e n  a n d A  S i e b e s  P r e s e r v i n g privacy through data generation In Proceedings of the IEEE Conference on Data Mining  pages 685–690 2007 
597 
597 


