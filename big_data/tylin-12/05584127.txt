FACISME: Fuzzy Associative Classifi cation using Iterative Scaling and Maximum Entropy  Ashish Mangalampalli Member, IEEE and Vikram Pudi Member, IEEE    Abstract. All associative classifiers developed till now are crisp  in nature, and thus use sharp  partitioning to transform numerical attributes to binary ones like \223Income = [100K and abov  On th e ot h er h a n d  th e novel  fuzzy  associative  classification algorithm called FACISME, which we propose in this paper, uses fuzzy logic to convert numerical attributes to fuzzy  attributes like \223Income = High\224, thus maintaining the integrity of information conveyed by such numerical attributes Moreover, FACISME is based on maximum  entropy and uses iterative scaling, both of which lend a very strong theoretical foundation to the algorithm. Entropy is one of the best measures of information, and maximum-entropy-based algorithms do not  assume independence of parameters in the classification process Thus, FACISME provides very  good  accuracy and can work with all types of datasets irrespective of size and type of attributes \226 numerical or binary\ and domains  I.   INTRODUCTION Recent studies in classification have proposed ways to exploit the paradigm of association rule mining for the problem of classification. A new classification approach called associative classification has gained a lot of popularity of late, because of its accuracy, which can be attributed to its ability to mine huge  amounts of data in order to build a classifier. It integrates association rule mining and classification by using association rule mining \(ARM algorithms, such as Apriori T h es e m e t h ods m i n e  high quality association rules and build classifiers based on them Associative classifiers have several advantages  200  Introduces uncertainty especially at the boundaries of sharp partitions  leading to loss of information    200  Frequent itemsets capture all the dominant relationships  between items in a dataset 200  Efficient itemset mining algorithms exist 200  As these classifiers deal only with statistically significant associations the classification framework is robust  However, there are two major problems with the associative classification as it stands right now  200  Most existing studies in the associative classification paradigm either do not provide any theoretical justification behind their approaches, or assume independence between some parameters in the domain  3    Ashish Mangalampalli is with the Centre for Data Engineering International Institute of Information Technology\(IIIT\, Hyderabad-500032 India. \(phone: +91-40-40148873; e-mail: ashish_m@research.iiit.ac.in Vikram Pudi is with the Centre for Data Engineering, International Institute of Information Technology \(IIIT\, Hyderabad-500032, India. \(email: vikram@iiit.ac.in 200  Crisp associative classification is not suitable, both theoretically and practically, for datasets and domains which make heavy use of numerical  attributes or have a mix of numerical and binary attributes  Thus, we have come up with a new fuzzy associative classification algorithm called FACISME. It uses maximum entropy and iterative scaling to build a theoretically sound classifier which is meant for accurate and efficient performance on any kind of datasets  irrespective of size and type of attributes 226 numerical or binary The maximum entropy principle is well-accepted in the statistics community. It states that given a collection of known facts about a probability distribution, choose a model for this distribution that is consistent with all the facts but otherwise is as uniform as possible. Hence, the chosen model for FACISME does not assume any independence between its parameters that is not reflected in the given facts In this paper, we use the Generalized Iterative Scaling \(GIS algorithm o com p u t e t h e m a xi m u m ent ropy  m odel   Crisp associative classification algorithms can mine only binary attributes, and require that any numerical attributes be converted to binary attributes. Until now the general method for this conversion process is to use sharp partitions, and try to fit the values of the numerical attribute in these sharp partitions. Assuming we have three such sharp partitions for the attribute Income, namely up to 20K, 20K-100K, 100K and above. Income = 50K would fit in the second partition but so would Income = 99K. Thus  using  sharp partitions  200  Moreover small changes in the election of intervals may lead to very different results which can be  misleading    200  The intervals also do not generally have clear semantics associated   A better way to solve this problem is to have attribute values belong to such partitions or sets with some membership value in the interval [0, 1 in stead o f b e lo n g i n g  totally or not at all belonging to a particular partition or range, and to have transactions with a given attribute represented to a certain extent \(in the range t h is  way crisp binary attributes are replaced by fuzzy ones  Thus, we need to use fuzzy methods, by which quantitative values for numerical attributes are converted to fuzzy binary values   s t ead of cri s p rang es  T h e us e of f u zz y 978-1-4244-8126-2/10/$26.00 ©2010 IEEE 


methods depends on the nature of dataset because in a few cases \(like datasets with all binary attributes\zzy methods may not be required Generation of fuzzy frequent itemsets using an appropriate fuzzy ARM algorithm, and then building a fuzzy associative classifier using these itemsets is not a straightforward process. First, we need to convert the crisp dataset, containing crisp binary and numerical attributes, into a fuzzy dataset, containing crisp binary and fuzzy binary attributes. This involves substantial pre-processing using appropriate techniques, like the one described in [7 8  Conversion of numerical attributes to fuzzy binary attributes is a part of such pre-processing. Second, crisp ARM and associative classification algorithms calculate the frequency of an itemset just by looking for its presence or absence in a transaction of the dataset, but fuzzy ARM and associative classification algorithms need to take into account the fuzzy membership of an itemset in a particular transaction of the dataset, in addition to its presence or absence. Thus, given a dataset, using FACISME to build a fuzzy classifier based on this dataset is a very complex and involved process, far removed in many major as well as minor aspects from the process of building a crisp classifier, based on the same dataset, using a conventional associative classification algorithm Our main contributions in this work are as follows  200  We have developed FACISME, which integrates maximum-entropy-based associative classification with fuzzy logic   200  Because of the use of maximum entropy, FACISME has a very strong theoretical foundation, and does not  assume independence of parameters in the classification process, thus providing very  good  accuracy Moreover maximum entropy models are interesting because of their ability to combine many different kinds of information  200  And, this accuracy is easily extensible over any kind of datasets irrespective of size and type of attributes \226 numerical or binary\nd domains, through the use of fuzzy logic, by creating a fuzzy associative classifier  Section 2 describes maximum entropy models, and section 3 describes the fuzzy pre-processing technique and fuzzy parameters used in FACISME. The actual FACISME algorithm is explained in detail in section 4. In section 5 we give an overview of the related work. And, in section 6 we describe our experimental results, before concluding in section 7  II.   MAXIMUM ENTROPY MODELS Let K  k 1  k 2 205 k n be the set of all items that can appear in a transaction Conditional maximum entropy models \(the term \223conditional\224 is generally dropped in normal usage\ the form        where X  x 1  x 2 205 x 2 n 1   is an input vector representing the set of all possible transactions with each transaction containing one or more items from K And y is an output class such that y   Y  y 1  y 2 205 y l 226 the set of all l classes f i are the indicator functions or feature values that are true if a particular property of x  y is true, and i is a weight for the indicator f i Let S  s 1  s 2 205 s S be the union of the fuzzy frequent itemsets extracted from each class y The fuzzy training dataset E derived for the original training dataset D a set of transactions, each of which is labeled with one of the l classes. This transformation of the original crisp dataset to its fuzzy version is done through the fuzzy pre-processing described in section 3. Maximum entropy models have several valuable properties. The most important is constraint satisfaction. For a given f i we can count how many times f i was observed in the data  observed  i  j  f i  x j  y j  2   For a model P  with parameters  we can see how many times the model predicts that f i would be expected, so  expected  i  j  y  P   y  x j  f i  x j  y  3   The itemsets in S are parameters to model each class such that each fuzzy itemset s together with expected  i   y  in class y forms a constraint that needs to be satisfied by the statistical model for that particular class. Thus, for each class y we have a set of constraints C  y  s q  s q   S The probability distribution that we build for class y must satisfy these constraints However, there could be multiple distributions satisfying these constraints. But, we use the maximum entropy principle [9 select th e d i strib u tio n  w i th t h e h i g h e s t  entropy. Maximum entropy models have the property that expected  i  observed  i  o r all co n s trai n t s i An additional property is that, of models in the form of eq. 1, the maximum entropy model maximizes the probability of the training data. Yet another property is that maximum entropy models are as close as possible to the uniform distribution subject to constraint satisfaction  III. FUZZY PRE-PROCESSING AND FUZZY MEASURES In this section, we describe the fuzzy pre-processing methodology and fuzzy measures that are used for the actual fuzzy ARM and fuzzy associative classification processes The assumption made in mining association rules and in building associative classifier s is that attributes are categorical. But, that is rarely the case, as many attributes are quantitative. And, to model such a scenario, we use discrete ranges e  g up to 25, 25-60, 60 and above\d try to fit the values of the numerical attribute Age in these ranges. For example, Age = 35, would fit in the range 25-60 but so would Age = 59. Thus, using ranges introduces 


uncertainty, especially at the boundaries of ranges, leading to loss of information The alternative is to use fuzzy partitions such as Young Middle-aged and Old, and then ascertain the fuzzy membership 265 range f each cris p num erical v a l u e i n  these fuzzy partitions. Thus, Age = 35 may have 265 0.6 for the fuzzy partition Middle-aged 265 0.3 for Young 265 0.1 for Old. And Age = 59 may have 265 0.3 for Middle-aged 265  0.1 for Young 265 0.3 for Old. Thus, by using fuzzy partitions, we preserve the information encapsulated in the numerical attribute, and are al so able to convert it to a categorical attribute, albeit a fuzzy one. Therefore, many fuzzy sets can be defined on the domain of each quantitative attribute, and the original dataset is transformed into an extended one with attribute values having fuzzy memberships in the interval [0, 1   Each membership function 265 can be constructed manually by an expert in that domain. This is an expert-driven approach \(fig. 1\fortunately, most real-life datasets are very huge \(in the order of thousands and sometimes even millions\nd can contain many quantitative attributes. Thus it is humanly impossible for an expert to create fuzzy partitions for each attribute and then convert each crisp numeric value to a fuzzy value using these fuzzy partitions    Fig. 1. Fuzzy partitions \(piecewise linear\ using an expertdriven approach  The alternative is to automate the creation of the fuzzy partitions, and to do this fuzzy clustering can be used. We use a data-driven pre-processing approach which automates the creation of fuzzy partitions for numerical attributes [10  and the subsequent conversion of a crisp dataset D to a fuzzy dataset E This approach requires very minimal manual intervention even for very huge datasets. Moreover, with appropriate value of fuzziness parameter m in eq. 1 \(such as m  2\, we can get fuzzy partitions which are very close to linear, as illustrated in fig. 2    Fig. 2. Fuzzy partition \(Gaussian-like\ created using a data-driven approach  The fuzzy sets in most real-life datasets are Gaussian like as opposed to triangular or trapezoidal\due to the varied and heterogeneous nature of the datasets. And, our preprocessing technique is able to generate such Gaussian-like fuzzy datasets from any real-life dataset. Numerical data present in most real-life datase ts translate into Gaussian-like fuzzy sets, where in a particular data point can belong to two or more fuzzy sets simultaneously. And, this simultaneous  membership of any data point in more than two fuzzy sets can affect the quality and accuracy of the fuzzy association rules, and the fuzzy classifier generated using these data points and fuzzy sets  A.   Pre-processing Methodology This pre-processing approach consists of two phases  200  Generation of fuzzy partitions for numerical attributes  200  Conversion of a crisp dataset into a fuzzy dataset using a standard way of fuzzy data representation  As part of pre-processing, we have used fuzzy c-means FCM\stering [11 in o r d er to create f u zz y p artitio n s  from the dataset, such that every data point belongs to every cluster to a certain degree 265 in the range [0, 1  Th e  algorithm tries to minimize the objective function  265       where m is any real number such that 1  m   265 ij is the degree of membership of x i in the cluster j  x i is the i th  d dimensional measured data c j is the d dimension center of the cluster, and ||*|| is any norm expressing the similarity between any measured data and the center. The fuzziness parameter m is an arbitrary real number m 1 In the first phase, we apply one-dimensional FCM clustering on each of the numeric attributes to obtain the corresponding fuzzy partitions, with each value of any numeric attribute being uniquely identified by its membership function 265 in these fuzzy partitions. One needs to select appropriate value of k number of one-dimensional clusters\d then label the resulting clusters according to the nature of the attribute In the second phase, if an attribute is quantitative, then the pre-processing methodology converts each crisp record in the dataset D to multiple fuzzy records based on the number of fuzzy partitions defined for the attribute. Doing so has the potential of generating fuzzy records in a combinatorial explosive manner. To deal with this problem, we have fixed a lower threshold for the membership function 265 0.3 most of the times\ to keep the number of fuzzy records generated under control. If an attribute is a binary attribute, then we output each record appended with a membership function 265  1, indicating this value has full membership in the binary attribute. The final fuzzy dataset E is used to extract fuzzy frequent itemsets and to build the fuzzy associative classifier as described in section 4 


B. Fuzzy Association Rules, Fuzzy Associative Classification, and Fuzzy Measures  During the fuzzy ARM and fuzzy associative classification process, a number of fuzzy partitions are defined on each quantitative attribute, as a result of which the original dataset is transformed into an extended one with attribute values in the interval [0, 1 In o r d e r to p r o cess th i s  extended \(fuzzy\ dataset, we need new measures \(analogous to crisp support and confidence\which are in terms of tnorms  1 3 T h e g e n e rat i o n of  fu zz y ass o ci at i o n  rules is directly impacted by the fuzzy measures we use Equations 5 and 6 respectively define a t-norm and the cardinality of a fuzzy set in a finite universe D     Fuzzy sets A and B in D are mapped as D  0 i t h  A  x  and B\(x eing the degrees to which attributes A and B are present in a transaction x respectively. Thus, using fuzzy partitions A and B and a t-norm, we can define fuzzy support eq. 7\d confidence \(eq. 8   T h e m o re g e n e ral l y  used t-norms are listed in Table I, with T M t-norm being the most popular one. Even we use the same t-norm in our algorithm  A  T  B  x  T  A  x  B  x   5    A   x  D   A  x    6   supp A B   x  D   A  T  B   x  X   7   conf A B   x  D   A  T  B   x   x  D   A  x   8    Table I t-norms in fuzzy sets  t-norm T M  x  y in x  y   T P  x  y  xy T W  x  y ax x  y  1, 0   IV. FACISME AND FUZZY ASSOCIATIVE CLASSIFICATION In this section we describe how FACISME is used to build a fuzzy associative classifier, and how this classifier is used for actual classification. Before the classifier training can ensue, we need to extract the fuzzy frequent itemsets from fuzzy dataset E This can be done using any popular fuzzy ARM algorithms, most of which are fuzzy adaptations of Apriori. But, in this case, we use the fuzzy ARM algorithm that we have developed in [8  W e ha ve c o me  u p with a new fuzzy ARM algorithm meant for fast and efficient performance to generate fuzzy frequent itemsets and fuzzy association rules In FACISME, the training phase involves finding the set of constraints i  e  S frequent itemsets extracted using fuzzy ARM\d computing values for all the classes. These  values indicate the weights of the fuzzy frequent itemsets in each class y The computed values for each class are stored, and are used in the actual classification phase. We use the classical GIS algorithm  o deal w i t h  m a x i m u m entropy models. In each iteration of GIS, a step is taken in a direction that increases the likelihood of the training data with the step size being not too large or not too small. The likelihood of the training data increases at each iteration and eventually converges to the global optimum. GIS converges such that, for each f i  expected  i  observed  i  h en ev e r  they are not equal, we can move them closer. To avoid very small probability and likelihood values, GIS is generally used in its logarithmic form. In this form, we add log  observed  i  expected  i  t o  i   but along with a slowing factor f eq. 9  equal to the largest total value of  f  max  j  y  i  f i  x j  y  9         i  i    11   Next, GIS computes an update \(eq. 10\ter which i is added to i eq. 11\The algorithm stops when there is no significant change in the i  values. This solution is globally optimal as it has been proved that the search space of i s over which we are searching for the final solution is concave leading to the conclusion that every locally optimal solution is globally optimal   A. Working of FACISME Equations 12, 13, and 14 are actively used by FACISME in determining the final i values \(in each iteration\ in conjunction with equations 9, 10, and 11  expected  i  f i  x j  y j  e s  j  y   z  12    z  y e s  j  y    13   s  j  y  i 327 f i  x j  y j   14   The resultant pseudo-code for a single iteration of FACISME is shown in fig. 3, and described as follows. First it initializes all i s to 1 and expected values to 0 \(fig. 3, lines 1-3\. Then, iterating over all possible transactions that can be derived by using items present in E and for each class y we calculate the s  j  y  i ng i n to accou n t  i values from the previous iteration \(lines 4-10\he presence of each frequent constraint\mset i   S in a particular transaction x j is indicated by f i  x j  y sed on the s  j  y  d z values calculated for each class y, we calculate the expected  i  values \(lines 11-16\e current iteration is the first iteration, fig, 1, line 16 is evoked to calculate expected  i Or else, equation 7 \(line 14\ used to calculate expected  i  Last, for each frequent \(constraint\mset i   S we calculate i and i lines 17-19\This i is used in the next iteration. We continue iterating over this procedure until expected i  observed i  o r each  i  i  e till convergence is achieved   


1\st iteration 2 expected 0 F   3 i 1; where i 1 to S  4\or each training instance j  all possible transactions using items in E 5\ not first iteration 6\   for each output class y  7 s  j  y   8\         for each i such that f i  x j  y  0 9 s  j  y  i 327 f i  x j  y  10 z  y e s  j  y   11\or each output class y  12\or each i such that f i  x j  y  0 13\     if not first iteration 14 expected  i  f i  x j  y  e s  j  y   z  15\     else if first iteration 16 expected  i 1 X  17 i  18 i log \(observed i   ex p e ct ed  i  f  19 i  i   Fig. 3. Pseudo-code for one iteration of FACISME  B.  Actual Classification using FACISME Before a given crisp transaction x containing crisp items can be classified using a classifier trained by FACISME, we need to run the same pre-pro cessing steps \(described in section 3\hat we used on the original training dataset before training. We use the fuzzy partitions obtained from preprocessing to transform the crisp attributes present in x to fuzzy attributes. This transformation process leads to the generation of one or more fuzzy transactions based on the number of fuzzy partitions generated for each crisp numerical attribute. The fuzzy transactions are represented by set X  x 1   x 2  205 x r   Our objective is to find the class which best classifies the set X For a fuzzy transaction x   X we first extract all the frequent itemsets  S that are subsets of x These itemsets are the features of x Then, we compute the entropy equations 15 and 16\or each class y   entropy  i log p i 327 p i    15   entropy  i  e i 327 265 327 fuzzy _ support  16   Eq. 15 is the standard equation for entropy. This equation has been transformed into eq. 16, in order to handle entropy in the fuzzy associative classification context. \(-log p i eq 15 is equivalent to e i in eq. 16 \(recall that i is in logarithmic form\.  And p i in eq. 15 is equivalent to 265 327 fuzzy _ support  Because FACISME involves fuzzy logic, the fuzzy membership value 265 of each fuzzy transaction has to be taken into consideration. Likewise, fuzzy support \(as described in section 3.B\ each frequent itemset constraint\so needs to be used To calculate the entropy, first we find the fuzzy membership 265 of the current fuzzy transaction using a suitable t-norm \(Table \205\ this case, we use the T M tnorm. Next, for every i th itemset S which is present in the current fuzzy transaction, we extract its fuzzy support calculated during fuzzy ARM process\ and i value calculated during training phase of FACISME\sing these three values, we calculate the entropy for each fuzzy transaction x   X and determine the best class for each x  based on normalized values of entropy. The overall best class for the set X is determined by the normalized values of the number of times each class y is best in the fuzzy transactions  X We also maintain the total entropy for each class y so that based on the normalized total entropy we can select the best class for the whole set X if more than one class turns out to be best in the same number of fuzzy transactions  X    C.  Space and Time Complexities of FACISME For GIS, the set of all possible transactions X is stored as a sparse matrix of all non-zero indicator functions for each instance j and output y GIS requires X the s of size S as well as the expected and observed arrays, which are also of size S Thus, GIS requires space O  X  S and since X  must be at least as large as S this would be O  X  Regarding time complexity, every time Equation 5 and 6 are used \(fig. 1, lines 17-19\pected i  is recalcu lated   This step takes O  X and to execute it for each i would take O  X 327 S If the algorithm requires m iterations for the distribution to converge, the time complexity of GIS can be given as O  m 327 X 327 S    D Non-closed Itemsets and FACISME The maximum entropy model as applied to fuzzy associative classification fails in some cases when a frequent itemset, present in the constraint set S is \223not clos  An itemset is not closed if and only if it has the same frequency as one of its supersets i  e  expected  s u  0,  and  s v   S s.t s u   s v   expected  s u  expected  s v  Thus, a major disadvantage with the maximum entropy model is that there is no solution when the system of constraints has non-closed itemsets in it. Hence, in cases when the system of constraints have non-closed constraints the exact solution does not exist, and the model parameters will not converge under the GIS algorithm. This is elaborated in [16   Hence, a modified  form of the maximum entropy model is used which can accommodate non-closed itemsets. Let S be the set of closed constraints in S The non-closed constraints in S are only used to determine whether expected values are 0 or not. Thus, only those constraints S are used for the actual calculation and final classifier building as their expected values are > 0. Thus, multiple iterations of GIS are run using S until convergence is achieved  V. RELATED WORK CBA w a s o n e of t h e f i rs t as s o ciati v e clas s i fiers an d  focuses on focusing on mining a special subset of association rules, called class association rules \(CARs CMAR [18 co n s id ers m u ltip le ru le s at t h e ti m e o f  classification using weighted 2 CPAR [19 tak e s a to tall y  


different approach to associative classification in that it does not directly use the rules generated by ARM, but only uses the frequent itemsets and their respective counts to build a classifier using a FOIL-like technique called PRM. Even before the advent of associative classifiers, many nonassociative classifiers were proposed, prominent of which is C4.5, which is a widely known decision tree classifi  and Ri  The principle of maximum entropy has been widely used for a variety of natural language tasks [22  A C M E  1 6  use s  it for binary data classification with mined frequent itemsets as constraints, discusses the failure of maximum entropy model to accommodate closed constraints, and proposes solutions to this problem. In fact, closed itemsets were first defined by Zaki in Thabtah in prov i d es a v e ry det a i l e d su rv e y of cu rre n t  associative classification algorithms. The author has compared most of the well-known algorithms, like CBA CMAR, and CPAR among others, highlighting their pros and cons, and describing briefly how exactly they work. He lays more emphasis on pruning techniques, rule ranking, and predication methods used in various classifiers, and also provides valuable insights into the future directions which should be undertaken in associative classification In [6 ller m eier p r o v id es a d etailed o v erv i e w o f fu zz y  methods as applied to data-mining. De Cock et  al 1 a nd Chen et  al  s c us s i n det a i l abou t  fu zz y i m p l i cat ors a n d  t-norms for discovering fuzzy association rules and also talk in depth about new measures of rule quality which would help the overall fuzzy ARM process. In fact, Dubois et. al  13 m a ke a  ve r y d e t a i l e d a n a l ysi s o f t no r m s a n d implicators with respect to fuzzy partitions, and provide a firm theoretical basis for their conclusions. Verlinde et. al   des c ri be v e ry bri e f l y des c ri be a preproces s i ng  technique, to obtain fuzzy attributes from numerical attributes, using FCM [11  FR a f u zz y e x te n s ion of R I P P E R bas e learn e r round robin scheme\nd the recently introduced R3 learner FURIA [25 ex te n d s th e w e ll k n o w n  RIP P E R alg o rith m  a n d  learns fuzzy rules instead of conventional rules and unordered rule sets instead of rule lists. The structural learning algorithm in vague environment \(SLAVE algorithm makes use of genetic algorithms to learn a fuzzy classifi    VI. PERFORMANCE STUDY In this section, we describe the performance study model that has been used for testing FACISME using three mostwidely-used disparate UCI Machine Learning \(ML\ datasets namely iris, breast, and pima. These three datasets are disparate from each other on various facets \(like number of attributes, type of attributes \226 numerical or binary, size of dataset, and type of dataset \226 dense or sparse\his makes any experimental analysis performed simultaneously on all the three datasets reliable and veracious, and such that its covers a wide range of aspects on which testing of classification accuracy and performance can be done The performance of FACISME has been compared with that of other state-of-the art classifiers, and the results of the same are detailed in section 6.A. Specifically, the classifiers used for comparisons are  200  Classification based on Predictive Association Rules CPAR 200  Classification based on Multiple Rules \(CMAR\8  200  Classification based on Associations \(CBA  200  C4.5 [22  200  Ri  200  Fuzzy Round-Robin Ripper \(FR3  200  Fuzzy Unordered Rule Induction Algorithm FURIA  200  Structural Learning Algorithm in a Vague Environment \(SLAVE    For the experiments on FACISME, we downloaded the raw datasets from the UCI ML website. These datasets then underwent the fuzzy pre-processing, mentioned in section 3 before being used to generate fuzzy frequent itemsets. To generate the fuzzy frequent itemsets that are used by FACISME, we have used the fuzzy ARM algorithm described in T h e m i n i m u m su pport u s ed f o r t h e f u zz y ARM process in order to build the fuzzy classifier, for all the three datasets, was 0.2 All the experiments were performed on an AMD 2600 Sempron PC with 512 MB main memory. All the approaches have been implemented by their authors with the parameters that they have used for testing. For CPAR, the best five rules are used for prediction. In all the experiments of FACISME and other associative classification algorithms\uracy is measured using the 10-fold cross validation. We implemented FACISME using Java on Windows XP. The results of the remaining algorithms i  e  excepting FACISME\aken from  2 4 25   A. Experimental Results As mentioned above, three standard UCL-ML datasets have been used to illustrate the efficacy of FACISME in terms of accuracy and simplicity of use. Figures 4, 5, and 6 illustrate the experimental results to test accuracy obtained for various classifiers on each of the three UCI-ML datasets Fig. 4 shows the accuracy obtained by each classifier on the iris dataset. FACISME performs the best in terms of accuracy as compared to the other datasets. From fig. 5 we get to know the accuracies for all the classifiers on the breast breast cancer\et. And, FACISME performs nearly as well as the most accurate classifier for this dataset. Finally fig. 6 depicts the results of the experimental analysis done on pima dataset. Even in this case, FACISME performs the best as compared to the other classifiers. Thus, the basic inference from this experimental analysis is that FACISME consistently performs very well on the basis of accuracy, and is even the best in two cases Each of the classifiers \(excepting FACISME\used in this experimental analysis are crisp in nature, and thus use sharp partitioning to convert numerical attributes to binary attributes. Normally 5-6 sharp partitions are used, but these can extend up to 10 or more depending on the nature of the 


numerical attribute e  g the attribute Age which is generally in the range 0-100, can be divided into six sharp partitions namely 0-15, 16-30, 31-45, 46-60, 61-75, 76 and above. In case of FACISME, fuzzy sets are used instead of sharp partitions, in order to convert numerical attributes to fuzzy partitions. The attribute Age can be divided into three fuzzy partitions, namely Young, Medium-aged, and Old, with each value of age belonging to each of the three fuzzy partitions with some membership value 265 Thus, the number of sharp partitions that need to be used to handle a numerical attribute is far greater than the number of fuzzy partitions required to do the same. Moreover, this leads to better understanding by the user as there are few well-defined fuzzy partitions with linguistic names/meanings like Young and Old, as opposed to sharp partitions which are less intuitive and to which a user cannot relate to immediately For the current analysis we have used two fuzzy partitions for all numerical attributes throughout all the datasets. As described in section 5.C, the time complexity of FACISME is quite high, because of which the time required to train a fuzzy classifier using FACISME is also high. Hence, we have used two fuzzy partitions so as to limit the training time. But, even with just two fuzzy partitions we have achieved quite high accuracies, as described above. More importantly, even with this, we achieved similar \(sometimes better results\s other state-of-the art classifiers used in this experimental analysis. The reason for these high accuracies is that FACISME is based on the theoretically sound and well-established maximum entropy framework  VII. CONCLUSIONS In this paper we have proposed a new classifier based on the paradigms of association rules mining and fuzzy logic  Though recent classifiers i nvolving association rules have shown their techniques to be accurate, their approaches to build a classifier either manifest unobserved statistical relationships in the dataset, or have no statistical basis at all FACISME uses the well-known maximum entropy principle and iterative scaling to build a statistical and robust theoretical model for classification Fuzzy Logic gives FACISME the capability to deal accurately and efficiently  with any type of datasets and domains, and in any kind of environments, which is not necessarily true for traditional crisp classifiers. Thus, using maximum entropy and fuzzy logic, FACISME provides very  good  accuracy and can work with all types of data irrespective of size and type of attributes \226 numerical or binary  REFERENCES 1  Agrawal, R. and Srikant, R.: Fast algorithms for mining association rules. In Proceedings of the 20 th International Conference on Very Large Data Bases, pp. 487-499. Morgan Kaufmann, Santiago, Chile 1994 2  Clark, P. and Niblett, T.: Bayesian network classifiers. Machine Learning, 29, 131-163 \(1997 3  Meretakis, D. and Wuthrich, B.: Extending naive-bayes classifiers using long itemsets. In Proceedings of International Conference on Knowledge Discovery and Data Mining, pp. 165-174. ACM, New York, NY \(1999 4  Zadeh, L.A.: Fuzzy sets. Inf. Control, 8, 338\226358 \(1965 5  Chen G., Yan P., Kerre E.E.: Computationally Efficient Mining for Fuzzy Implication-Based Association Rules in Quantitative Databases. International Journal of General Systems, 33, 163-182 2004 6  H\374llermeier, E.: Fuzzy methods in machine learning and data mining Status and prospects. Fuzzy Sets and Systems. 156, 387-406 \(2005 7  Mangalampalli, A., Pudi, V.: Fuzzy Logic-based Pre-processing for Fuzzy Association Rule Mining. Technical Report IIIT/TR/2008/127 International Institute of Information Technology \(2008 8  Mangalampalli, A., Pudi, V.: Fuzzy Association Rule Mining Algorithm for Fast and Efficient Performance on Very Large Datasets In Proceedings of IEEE International Conference on Fuzzy Systems pp. 1163-1168. IEEE Computational Intelligence Society, Piscataway NJ \(2009 9  Good, I.: Maximum entropy for hypothesis formulation, especially for multidimensional contingency tables. Annals of Mathematical Statistics, 34, 911-934 \(1963   Verlinde, H., De Cock, M., Boute, R.: Fuzzy Versus Quantitative Association Rules: A Fair Data-Driven Comparison. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics 36, 679-683 \(2006   Dunn, J.C.: A Fuzzy Relative of the ISODATA Process and its Use in Detecting Compact, Well Separated Clusters. J. Cyber., 3, 32-57 1974   De Cock, M., Cornelis, C., and Kerre, E.E.: Elicitation of fuzzy association rules from positive and negative examples. Fuzzy Sets and Systems, 149, 73\22685 \(2005   Dubois, D., H\374llermeier, E., Prade, H.: A systematic approach to the assessment of fuzzy association rules. Data Min. Knowl. Discov., 13 167-192 \(2006   Darroch, J.N. and Ratcliff, D.: Generalized iterative scaling for loglinear models. The Annals of Mathematical Statistics, 43, 1470\2261480 1972   Zaki, M.J.: Generating non-redundant association rules. In Proceedings of the 6 th International Conference on Knowledge Discovery and Data Mining, pp. 34-43. ACM, New York, NY \(2000   Thonangi, R. and Pudi, V.: ACME: An Associative Classifier Based on Maximum Entropy Principle. In Proceedings of the 16 th  International Conference Algorithmic Learning Theory, pp. 122-134 Springer, Singapore \(2005   Liu, B., Hsu, W., and Ma, Y:. Integrating classification and association rule mining. In Proceedings of the 4 th International Conference on Knowledge Discovery and Data Mining, pp. 80-86 AAAI Press, New York, NY \(1998   Li, W., Han, J., and Pei, J.: CMAR: Accurate and efficient classification based on multiple class-association rules. In Proceedings of IEEE International Conference on Data Mining, pp. 369-376. IEEE Computer Society, San Jose, CA \(2001   Yin, X. and Han, J.: CPAR: Classification based on Predictive Association Rules.  In Proceedings of the 3 rd SIAM International Conference on Data Mining, pp. 331-335. SIAM, San Francisco, CA 2003   Beeferman, D., Berger, A., and Lafferty, J.: Statistical models for text segmentation. Machine Learning. 34, \(1-3\, 177-210 \(1999   Thabtah, F.A.: A review of associative classification mining Knowledge Engineering Review, 22, 37-65 \(2007   Quinlan, J.R.: C4.5: Programs for Machine Learning. Morgan Kaufmann, San Francisco \(1993   Cohen, W.: Fast Effective rule induction. In Proceedings of the 12 th  International Conference on Machine Learning, pp. 115-123. Morgan Kaufmann, Tahoe City, CA \(1995   H\374hn, J. C., and H\374llermeier, E.: FR3: A Fuzzy Rule Learner for Inducing Reliable Classifiers. IEEE Transactions on Fuzzy Systems 17, 138-149 \(2009   H\374hn, J. C., and H\374llermeier, E  FURIA: an algorithm for unordered fuzzy rule induction. Data Mining Knowledge Discovery, 19, 293-319 2009   Gonzalez, A. and Perez, R.: Slave: A genetic learning system based on an iterative approach. IEEE Transactions on Fuzzy Systems, 7, 176191 \(1999   Gonzalez, A. and Perez, R.: Selection of relevant features in a fuzzy genetic learning algorithm. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics, 31, 417\226425 \(2001  


  Fig. 4. Experimental results on iris dataset     Fig. 5. Experimental results on breast \(breast cancer\ataset     Fig. 6. Experimental results on pima dataset  93 93.5 94 94.5 95 95.5 FACISME CPAR CMAR CBA C4.5 Ripper FURIA FR3 SLAVE 94 94.5 95 95.5 96 96.5 FACISME CPAR CMAR CBA C4.5 Ripper 71 72 73 74 75 76 FACISME CPAR CMAR CBA C4.5 Ripper FURIA SLAVE 


Application of Chaotic Particle Swarm Optimization Algorithm in Chinese Documents Classification 763 Dekun Tan Qualitative Simulation Based on Ranked Hyperreals 767 Shusaku Tsumoto Association Action Rules and Action Paths Triggered by Meta-actions 772 Angelina A. Tzacheva and Zbigniew W. Ras Research and Prediction on Nonlinear Network Flow of Mobile Short Message Based on Neural Network 777 Nianhong Wan, Jiyi Wang, and Xuerong Wang Pattern Matching with Flexible Wildcards and Recurring Characters 782 Haiping Wang, Fei Xie, Xuegang Hu, Peipei Li, and Xindong Wu Supplier Selection Based on Rough Sets and Analytic Hierarchy Process 787 Lei Wang, Jun Ye, and Tianrui Li The Covering Upper Approximation by Subcovering 791 Shiping Wang, William Zhu, and Peiyong Zhu Stochastic Synchronization of Non-identical Genetic Networks with Time Delay 794 Zhengxia Wang and Guodong Liu An Extensible Workflow Modeling Model Based on Ontology 798 Zhenwu Wang Interval Type-2 Fuzzy PI Controllers: Why They are More Robust 802 Dongrui Wu and Woei Wan Tan Improved K-Modes Clustering Method Based on Chi-square Statistics 808 Runxiu Wu Decision Rule Acquisition Algorithm Based on Association-Characteristic Information Granular Computing 812 JianFeng Xu, Lan Liu, GuangZuo Zheng, and Yao Zhang Constructing a Fast Algorithm for Multi-label Classification with Support Vector Data Description 817 Jianhua Xu Knowledge Operations in Neighborhood System 822 Xibei Yang and Tsau Young Lin An Evaluation Method Based on Combinatorial Judgement Matrix 826 Jun Ye and Lei Wang Generating Algorithm of Approximate Decision Rules and its Applications 830 Wang Yun and Wu-Zhi Qiang Parameter Selection of Support Vector Regression Based on Particle Swarm Optimization 834 Hu Zhang, Min Wang, and Xin-han Huang T-type Pseudo-BCI Algebras and T-type Pseudo-BCI Filters 839 Xiaohong Zhang, Yinfeng Lu, and Xiaoyan Mao A Vehicle License Plate Recognition Method Based on Neural Network 845 Xing-Wang Zhang, Xian-gui Liu, and Jia Zhao Author Index 849 
xiii 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





