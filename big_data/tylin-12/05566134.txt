Hypothesis Testing Based K nowledge Discovery in Distributed Multiple Data Sources Shilei Bai,  Hui Ren,  Wei Jiang,  Yujian Jiang School of Information Engineering, Communication University of China Beijing, P. R. China E-mail: bsl@cuc.edu.cn  Abstract 227In the past several years, most data mining researchers focus on data mining from single data source. Nowadays, data mining from multiple data sources is a new problem in Web environment and is also an efficient technique for solving knowledge discovery in distributed databases. A new method for mining multi-data sources is presented in this paper. By sharing 
knowledge patterns discovered in other similar data sources hypothesis testing is employed for verifying whether the patterns are also suitable for local data source or not. So that can improve the efficiency of KDD greatly. Finally the effectiveness of this method is analyzed and experimental result is given. This method can be extended as an efficient data mining algorithm in case of apriori hypothesizes are provided. And it can be also used for incremental data mining Keywords- multiple data sources, hypothesis testing, knowledge sharing, knowledge discovery I   I 
NTRODUCTION  With the availability of inexpensive storage and the progress in data capture technology, many organizations haves created ultra-large databases, and this trend is expected to grow. As an important way to obtain knowledge from mass data, data mining \(also called knowledge discovery in databases, KDD\s been brought into focus. Increasing number of scholars have engaged in this area since 1990s, and achieved considerable fruits. However, the traditional research of KDD mostly aimed at centralized data source. With the rapid development of network technology, it is possible to realize the remote accessing of data and large-scale information 
sharing. Large amount of distributed and multiple data source information systems appeared. In practice, KDD systems need more and more deal with distributed multiple data source and incremental data [1,2,3  In  th e en v i r o n m en t o f d i str i b u t e d  multiple data sources, KDD faces some new problems as follows i\ Usually, there are many similar data sources in distributed database system. Take a big chain-store enterprise for example, affiliated agencies in different regions usually build local database, which is similar to each other. Does the knowledge discovered in one database equally applicable to the others ii\ In distributed data source environment, different data 
sources may be used by many users. Different users often have customizing requirements about the knowledge discovered in database, such as support, confidence, etc. If one  user had run data mining algorithm and got some rules, does these rules meet another user\222s demand Above problems will be discussed in this paper, and an effective solution is given II  R ELATED W ORK  In order to discover knowledge in multiple data sources or distributed databases, the following methods are often used A 
 Centralized Mining To collect data from various data sources, and establish a data warehouse or data mart, then run the data mining algorithms in local synthesized database [4  T hi s m e t ho d c a n  realize exhaustive mining through full search in all data records, but its cost is too high. Because KDD faces mass data the spatio-temporal complexity of mining algorithm is usually very high and is sensitive to the amount of records. At the same time, the cost of data communication, data conversion, data pre-processing, etc. used to establish centralized database are also huge. In addition, information security, data 
confidentiality and other issues may also be involved in Therefore, centralized mining is not an idea solution B  Partition Method To break the data mining task of multiple data sources into three subtasks: i\ classify the multiple data sources; ii\ mining each database separately; iii\ynthesis mining knowledge from similar databases [6,7 Me an w h ile, m u ltiag en t tech n o l o g y  h a s  also been brought into distributed KDD systems. The system sends data mining agent to each site for data mining, then sends back the mining results to local for synthesis [8,9,10 A ll t h e  data records have to be processed through these methods, the 
complexity can not be significantly reduced, and the synthesis of pattern is also lack of effective algorithm at present [11   C  Mining on Sample Space The research on sample detection method for data mining has been proposed since several years ago. Tovienven[1 proposed a random sampling approach to mining association rules. Firstly, mining the sample space, then check the results of the remaining data. This algorithm has higher efficiency 978-1-4244-5143-2/10/$26.00 \2512010 IEEE 


However, the method is applicable to the mining of single homogeneous database. Jong[1 a n d o t her re se arc h e r s  have  proposed a variable precision association rule mining method Firstly, obtain necessary knowledge from some samples by sampling techniques, and then use this information to mine the entire database in a heuristic way. The methods mentioned above are all faced to the single data sets which can not be used for distributed situation. Reference [14 p r op os ed  a di s t ri bu t e d  variable precision association rule mining algorithms SMDM Firstly, randomly sample a certain capacity of data at each site mine the sample data, and then re-use meta-learning strategies to obtain the final results. This method has higher efficiency but the accuracy of results decreases sharply. Meanwhile, it is considerable difficult for meta-learning and verification III  K NOWLEDGE D ISCOVERY M ETHOD B ASED ON H YPOTHESIS T ESTING  In fact, the multiple data sources of the same enterprise or the same field are so similar that the implicated knowledge in these data may be shared to some extent. As a result, KDD system could improve the efficiency of knowledge discovery by knowledge-sharing mechanisms. We can use the knowledge discovered from database D to guide the knowledge discovery process in a similar database D’. We just need to test the knowledge and determine the applicability in D’, then reduce the repetitive work. It is more effective than traditional data mining. If testing all the related records in database is avoidable, the efficiency of knowledge discovery can be further improved Based on this idea, knowledge from another similar data sources can be seen as a hypothesis in the local database, and the records in local database are used to evaluate the validity of the knowledge. So we can transform the knowledge judging problem into hypothesis testing problem. It is similar to product acceptance: rules \(hypothesis\ can be regarded as acceptance criteria, each record in local database looks as a product. The products consistent with the criteria are qualified otherwise are rejected. If the rejection rate is too high, it means that the hypothesis does not conform to the database, which can not commendably describe the data. So it should be refused. If the rejection rate is low, the rule is adopted. In order to efficiently determine the hypothesis, we can sample from the database reasonably to reduce the capacity of subsample. Then we just need test the subsample space to verify the hypothesis The traditional methods of sampling inspection mainly include three modes: unitary, duplex and sequential. Though the requirement of sub-sample capacity for unitary mode is larger than the other two, the process is simple. Since the testing process discussed in this paper is not complicated and not very sensitive to subsample capacity, unitary mode is a feasible method Unitary sampling inspection means to sample only once then makes a decision to accept or reject according to the observations results. It can be divided into two types, quality based inspection and the amount based inspection. The problem discussed in this paper is to check whether the data record is consistent with a given rule, so quality based inspection is adopted In the quality based inspection, two criteria of rejection rate is usually set p 0 and p 1 and p 0 p 1  p 0 is called acceptable quality level \(AQL\d p 1 is called limit quality level LQL\When the rejection rate p p 0 receive this batch of product; When p p 1 refuse this batch of products. Suppose the subsample capacity is n, and k stands for the number of inferior product in subsample. We need to choose suitable integers n and c to make the error rate as small as possible while making the following judgment: "When k c this batch of product is eligible; When k  c this batch of product is ineligible." None sampling inspection methods can guarantee the conclusion of testing to be entirely correct. But we can preset a upper limit probability of getting wrong conclusion, and determine n and c by the pre-set limit. Then we can assure that the conclusion gotten from the sampling inspection to be correct in a high probability. In mathematical statistics, there are two kinds of mistake[15  W e  cal l r ef u s in g a c o r r e c t  hypothesis mistakenly” as the first mistake, and call “accept a wrong hypothesis mistakenly” as the second mistake. Suppose the upper limit probability that we make the first mistake is  and the upper limit probability that we make the second mistake is this is a hypothesis testing problem. For a hypothesis H 0  p 1Cr  Cr is a pre-set lower limit of reliability. Now we  test H 0 and the requirements are as follows when p 1Cr the probability of committing  the first mistake does not exceed when p 1Cr the probability of committing the second mistake does not exceed  For the population whose rejection rate is p the number of population is N the number of reject is M We randomly select n products, suppose D is the number of reject. Then when N is a finite D obeys hypergeometric h  n, M  N    M NM knk PD kNnp N n Np Nq knk N n            1 When N is infinite D obeys binomial distribution, since the amount of data which KDD dealt with is very large, binomial distribution can be used to approximate. In this case, the probability of that the reject in the sub-sample is not more than c is as follows 0 1 1  1  1 1  c knk k cnc p n Lpnc p p k n nxxdx c       2 The requirements of the sampling plan can be expressed as follow 


 0 1  1  L pnc p p L pnc p p      3 Equation \(2\owed that L is a monotonic decreasing function of p then n and c which satisfies \(3\an be obtained by solving the following equations  0 1 1  Lp nc Lp nc     4 The solution of function \(4\ is complex. During the practical application n and c are solved through a special mathematics table, which is more convenient for computer processing The above sampling method depends entirely on the parameters n and c it is also known as n  c sampling plan And the function L  p  n  c is called operating characteristic function of n  c sampling plan, as shown in Figure 1        Figure1. Sketch map of operating characteristic function Take association rules as example, assume that we have obtained a rule from the database D', R y x x x n   2 1  now we verify whether it is equally effective in database D Suppose the data set which contains items x 1 x 2  x n is S  and the data records don’t comply with the rule in S is S R  Suppose  R p SS  then the confidence of the association rule R is equal to 1p Take S as the population, randomly sample n data records from S as subsample to test rule R Suppose the number of records which don’t complied with R in subsample is k It is reasonable to accept R with the smaller k  and it is reasonable to refuse R with the bigger k Suppose the lower limit confidence of accepting R is Cr then if 1 p Cr   accept R; if 1 p Cr  refuse R. The most convenient approach is to suppose 01 1 p pCr   1    then determine the integers n and c according to the above equation 4\ and make sure the sampling plan. But unfortunately this kind of sampling plan does not exist. Because take 01 1 p pCr  into equation \(4\en we get 1    It is unable to meet the accuracy requirements 1   As can be seen from figure 1, when n and c are unchanged if 1   then  10 p p is rather big. While p 0 is closer to p 1  then the curve in the diagram between p 0 and p 1 may be steeper. Which indicated the larger subsample n highefficiency test can not be achieved In practice we can pre-set Cr 0 and Cr and Cr 0  Cr  Cr is defined as the lower limit confidence of rule R. Suppose the confidence of rule R is C R If C R Cr 0 it means that R has higher reliability and is acceptable; if C R  Cr it indicates that the credibility of R is low and should be denied. Suppose AQL=1Cr 0 LQL=1Cr  Cr 0 Cr 1    n, c\ is determined by \(4\ for testing rule R. If the testing result is accepting R, then the probability of C R  Cr is smaller than  re-testing is not needed; if the result is rejecting R, the probability of C R  Cr 0 is smaller than but it is still possible that the value of C R  is between Cr 0 and Cr Thus R may still be an accepted rule. In this case, whether completely testing in the whole population space is needed, is decided according to our actual necessaries. For the high confidence rules, this approach only needs a small amount of samples to test; completely testing is needed only in case of very lower confidence rules Because the testing approach in this paper is carried out through data comparison by computer, it is not very sensitive to the subsample capacity. So Cr 0 can be made adequately close to Cr thereby reducing the risk that mistakenly refuse such rules whose C R is between Cr 0 and Cr For the data of random distribution, the inspection process does not require a specific sampling plan and data pre-processing. It only need compare such related data records with the rule one by one. If it is found the rule can be accepted or rejected, then stopped IV  E XPERIMENTAL R ESULTS AND A NALYSIS  In order to demonstrate the correctness of the algorithm, we have done several experiments with association rules. The experiment data set is generated by Assocgen T h e numb e r of transactions in database D and D is 10,000 respectively The association rules are generated by algorithm mentioned in 16  an d  th e alg o r i t h m is im p l em en ted w ith C   Set  min_sup 2%, and min_conf 60%, we get 19 association rules from database D Then we verify these 19 rules in database D  Set  0.05. While hypothesis testing, we set Cr 0 0.85 Cr 0.7, and get the sampling plan n, c  506, 122\ The testing result shows that: in eleven rules whose confidence are more than 70%, there are only two rules are refused; and the other eight rules whose confidence are less than 70% are all refused. In this testing, there are only 506 records to be checked. It means the method proposed in this paper has higher efficiency Because the hypothesis is made by the requirement of users, the parameters Cr 0 and Cr are set by the user. Then different users may set the confidence of rules flexibly in need of their tasks.  As for the support of rules, hypothesis testing methods can also be used to determine whether it is satisfied the user’s requirements \(the specific approach shall not be repeated here\. Thus, the hypothesis testing method can solve the second problem proposed in section I This method is also applicable to other types knowledge discovery tasks such as sequential patterns, classification rules etc. It only needs to pre-process the data correspondingly with the knowledge type. Furthermore, the method can also be used to verify conjectural knowledge. Hypothesis derived from background knowledge or prior knowledge, or given by human experts, can also be verified by this method p   0 1 L  p 0  p 1  1 1 


What’s more, the method we proposed is valid on the multiple data sources which are similar, but not always effective to those multiple databases of different quality V  C ONCLUSIONS AND F UTURE W ORK  Data mining from multiple data sources is a new problem in Web environment and is also an efficient technique for solving knowledge discovery in distributed databases. A new method of knowledge discovery from multiple data sources is proposed in this paper. It shares knowledge found in other similar data source to determine the validity of knowledge in the local database through sampling test. The experimental results show that the method is effective and can significantly improve the efficiency of knowledge discovery The hypothesis testing based knowledge discovery method proposed in this paper is also an effective solution to incremental data mining. In the real world, new data records added into an application database \(such as a sales database\, to some extent, is generally similar to original data. Under this circumstance, it is necessary to determine the impact of new added samples, and to make sure how to re-arrange subsample size and the sampling strategy. Specific research findings will be given in future A CKNOWLEDGMENT  This work was partially supported by “211” Project No.21103040122\nd “382” Talents Scheme \(No G2009382309\mmunication University of China R EFERENCES   1  H. Kargupta, B. Park, D. Hershberger, and E. Johnson, “Collective data mining: A new perspective toward distributed data mining,” Advances in distributed and parallel knowledge discovery, chapter 5, pp.131–178 AAAI/MIT Press, 2000 2  E. Cesario and D.Talia, “Distributed data mining models as services on the grid,” IEEE International Conference on Data Mining Workshops 2008, pp.486–495 3  P. Kadel and H. Choi, “Incremental algorithm for distributed data mining,” The Sixth IEEE International Conference on Computer and Information Technology, 2006, pp.72–75 4  G. Goulbourne, F. Coenen and P. Leng, “Algorithms for computing association rules using a partial-support tree,” Knowledge-Based Systems, vol.13, 2000,  pp.141–149 5  Y. Chen and L. Qu, “The research of universal data mining model system based on logistics data warehouse and application,” International Conference on Management Science and Engineering 2007, pp.280–285 6  S. Zhang, X. Wu and C. Zhang, “Multi-database Mining,” IEEE Computational Intelligence Bulletin, vol.2, no.1, 2003,  pp.5–13 7  N. Zhong and Y. Yao. “Interestingness, peculiarity, and multi-database mining,” IEEE International Conference on Data Mining 2001,  pp.1–8 8  J. Rajan and V. Saravanan, “A framework of an automated data mining system using autonomous intelligent agents,” International Conference on Computer Science and Information Technology 2008, pp.700-704 9  L. Cao, V. Gorodetsky and P. A. Mitkas, “Agent mining: the synergy of agents and data mining,” Intelligent Systems, IEEE, vol.24, Issue 3,  May-June 2009, pp.64–72   Danish Khan, “CAKE – classifying, associating & knowledge discovery an approach for distributed data mining \(DDM\ing parallel data mining agents \(PADMAs\,” IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology 2008, pp.596 601   X. Wu and S. Zhang, “Synthesizing high-frequency rules from different data sources,”  IEEE Transactions on Knowledge and Data Engineering vol. 15, No. 2, March/April 2003,  pp.353–367   H. Toivonen, “Sampling large databases for association rules International Conference on Very Large Data Bases 1996, pp.134–145   J. S. Park, P. S. Yu and M. Chen, “Mining association rules with adjustable accuracy,” International Conference on Information and Knowledge Management 1997, pp.151–160   C. Wang and H. Huang, “Distributed Mining Adjustable Accuracy Association Rules Using Sampling,” Journal of Computer Research and Development, China, vol.37, no.9, 2000, pp.1101–1106   Wang Fubao, Probability Theory and Mathematical Statistics \(third edition\, Shanghai: Tongji University Press, 1998, pp.308–319   R Agrawal and R. Srikant, “Fast algorithms for mining association rules,”  International Conference on Very Large Data Bases 1994, pp 487–499  


sampling probability is known which means that 015 i;j will be constant for any given f i j g  we can still use the sample counts to reliably estimate similarity Details For a transaction T t we can visualize the pairs in T t 002 T t as a 2-dimensional table with rows and columns sorted by support where we are interested in the pairs below the diagonal index i  j  Since f is non-increasing the sampling probabilities are decreasing in each row and column This means that for any k  0  in time O  j T t j  we can determine what interval in each row of the table is to be sampled with probability 2 000 k  To produce the part of the sample for one such interval we describe a method for producing a random sample of S  f 1      036 g  for a given integer 036  where each number is sampled with the same probability p  Since p\036 may be much smaller than 036  we want the time to depend on the number of samples rather than on 036  This can be achieved using a simple recursive procedure similar to the one used in ef\002cient implementations of reservoir sampling With probability 1 000 p  036 we return an empty sample Otherwise we choose one random element x from S  and recursively take a sample of the set S nf x g with sampling probability p  The set S can be maintained in an array where sampled numbers are marked In case more than half of the numbers are marked we construct a new array containing only unmarked numbers the amortized cost of this is constant per marking To select a random unmarked number we sample until one is found which takes expected O 1 time because no more than half of the numbers are marked In summary for each sampling probability 2 000 k we can compute the corresponding part of the sample in expected time O  j T i j  z k   where z k is the number of samples This is done for k  1  2      2 log nm   Sampling probabilities smaller than  nm  000 2 are ignored since the probability that any such pair would be sampled in any transaction is less than 1 m  That is with high probability ignoring such pairs does not in\003uence the sample To state our result let 2 000 N denote the set of negative integer powers of 2 Lemma 4 Let  f  N 002 N  2 000 N be non-increasing in both parameters Given a transaction T t and support counts j S i j for its items in expected time O  j T t j log nm   z  we can produce a random sample of z 2-subsets of T t such that 017 f i j g is sampled with probability  f  j S i j  j S j j  if  f  j S i j  j S j j    nm  000 2  and otherwise with probability 0  and 017 the samples are independent 016 For all similarity measures in 002gure 1 and any feasible value of 034  the minimum support requirement will ensure that the expected number of samples in a transaction is at most j T t j  This means that for each transaction T t  the time spent is O  j T t j log nm  with high probability B SampleCount This phase sees the stream of pairs generated by the pair sampling and has to 002lter out as many low similarity pairs as possible while successfully identifying high similarity pairs By the properties of pair sampling this is essentially the task of identifying frequent pairs in the stream of samples We aim for space usage that is smaller than that of standard algorithms for frequent item mining in a data stream In order to accomplish this we use a modi\002cation of an algorithm by Demaine et al Their algorithm 002nds frequent items in a randomly permuted stream of items and so does not directly apply to our setting where only the transactions are assumed to come in random order Demaine et al are able to sample random elements by simply taking the 002rst elements from the stream This would not work in our setting where all these elements might be pairs coming from the same transaction Reservoir sampling Instead we use a reservoir sampling method W e sk etch the mechanism here and we refer to the original paper for a complete description Suppose we have a sequence of d items and we want to sample a random subset of the sequence We 002rst of all put in the sample the 002rst s elements that we see For each subsequent element in position t  s  we will put it in the sample with probability s=t  When a new element has to be included in the sample another one that is already part of the sample has to be evicted Each element of the set of samples will be chosen as the victim with probability 1 s  This technique ensures we will end up with a set of samples that is a true random sample of size s  SampleCount We consider the stream of pairs divided into 024 chunks The pair sampling generates these chunks such that each chunk corresponds to some set of transactions i.e all the pairs sampled from each transaction end up in the same chunk We run reservoir sampling on every other chunk to produce a truly random sample of size s 2  We then proceed to count the occurrences of the elements of the sample in the next chunk Assume in the following that we number chunks by  024   such that reservoir sampling is done on evennumbered chunks indexed by  024 even   When doing the above whenever we see a pair f i j g whose count must be updated we weigh the sample by the factor 015 i;j that got lost during the pair sampling phase so as to consider an expected number of samples exactly proportional to s  i j   At the end of a counting chunk we estimate the similarities of all pairs sampled and keep the s 2 largest similarities seen so far At the end of the stream the similarity estimates found are returned to supersede the previous estimates Pseudocode for the SampleCount algorithm is shown in 002gure 1 
125 
125 


Algorithm 1 Pseudocode for the S AMPLE C OUNT phase 1 procedure S AMPLE C OUNT  P s size   P is a stream of pairs each of which has associated a similarity value The length of P is known 2 S out   3 while There are elements in P do 4 S 0   5 S   6 t  0 7 S  the 002rst s 2 elements in P 8 while  t  size 2 000 s 2 do 9 i  the next element in P 10 Choose uniformly at random a number r 2 0   11 if r 024 s  s  2 t  2 then 12 Choose uniformly at random a victim from S and substitute it with i 13 end if 14 t  t  1 15 end while 16 initialize  S 0  S   S 0 is an associative array indexed on the distinct items present in S  initializing it means putting all its entries to 0 17 while  t  size  do 18 i  the next element in P 19 if i 2 S then 20 S 0  i   S 0  i   015 i 21 end if 22 t  t  1 23 end while 24 Choose the s topmost distinct items between S 0 out and S 0  and assign them to S 0 out 25 end while 26 Return S 0 out 27 end procedure IV A NALYSIS Let S i denote the set of transactions containing the element i  This means that S i  S j is the set of transactions containing the pair f i j g  Let S 1 i denote the set of transactions containing i in the current pre\002x of the stream Similarly S k i will denote the set of transactions containing i in C k  the chunk k of the suf\002x of the stream up to the point in which a new current pre\002x changes the counts of items occurrences So S k i  S i  C k De\002nition 5 Given x y 2 R we say that x  016 L  approximates y  written x 016;L  y  if and only if x 025 L implies x 2 1 000 016  y  1  016  y   016 The notation extends in the natural way to approximate inequalities In what follows we will use  016 L  approximations where L  C log mn  for a suitably large constant C depending on the accuracy 016 in Theorem 2 The task is to analyze the accuracy of the new approximation computed when the current pre\002x changes We introduce two random events G OOD P ERMUTATION GP and G OOD B ISAM S AM PLE GBS and bound the probability that they do not happen A permutation of the transactions is called good for f i j g  denoted GP i;j  if and only if the following conditions hold for the current pre\002x 1 j S 1 i j 016;L  j S i j  2 and j S 1 j j 016;L  j S j j  2  2 8 k j S k i  S k j j 016;L  j S i  S j j  2 k  Essentially goodness means that the frequencies of individual items are close in the 002rst and second half of the current pre\002x and the frequency of the pair is evenly spread over the chunks in the second part of the current pre\002x Lemma 6 Given 016 2 0 022 R  we have Pr[GP i;j  025 1 000 6 001 e 000j S i j 016 2 6 Proof An interesting property of the random variables j S 1 i j and j S k i  S k j j is that they are negatively dependent First of all we bound the probability that j S 1 i j is far from j S i  2 j  Using Chernoff bounds we can write Pr j S 1 i j 000 j S i j  2 j 024 016 j S i j   024 2 001 e 000 j S i j 016 2 6 1 
126 
126 


Looking at j S k i  S k j j we can write Pr j S k i  S k j j\000j S i  S j j  2 024 j 024 016 j S i  S j j  2 024  024 2 001 e 000 j S i  S j j 016 2 6 024 2 We use the fact that Chernoff bounds also holds for negatively dependent random variables Since the last bound is the weakest of the three the lemma follows We want GP i;j to hold with probability 1 000 o 1 n 2  whenever items i and j both have support   From Lemma 6 we get that this holds if j S i  S j j  C\024 log n  for some constant C depending on 016  If s  i j   2 024Lf     025 024L then j S i  S j j 025 2 024L  Hence a suf\002cient condition for the similarity is s  i j   024L  3 It remains to understand what is the probability that given a good permutation the pair sampler will take a number of samples for a given pair in each chunk k that leads to a 1 006 016  approximation of s  i j   We denote the latter event by GBS i;j;k  and want to bound the quantity Pr[GBS i;j;k j GP i;j   For this purpose consider the random variable X i;j;k de\002ned as the number of times we sample the pair f i j g in chunk k  Assuming GP i;j we have that over the randomness in the pair sampling algorithm E  X i;j;k  016;L   f  j S 1 i j  j S 1 j j  034 j S i  S j j  2 024  Since the occurrences of f i j g are independently sampled we can apply a Chernoff bound to conclude X i;j;k 016;L  E  X i;j;k   This leads to the conclusion Lemma 7 X i;j;k 016;L   f  j S 1 i j  j S 1 j j  034 j S i  S j j  2 024 016 Suppose that X i;j;k is close to its expectation Then we can use it with 1 006 016  approximations of j S i j and j S j j  to compute a 1 006 O  016  approximation of s  i j   This follows by analysis of the concrete functions f of the measures in Figure 1 A suf\002cient condition on the similarity needed for a 1 006 016  approximation of X i;j;k can be inferred from lemma 7 If s  i j  025 4 024L=\034 then E  X i;j;k  025 s  i j  034  4 024 025 L  So it suf\002ces to enforce s  i j  025 4 024L=\034  4 In order to have O  mb  pairs produced by the pair sampling phase we will choose 034  4 M  The expected number of pair samples from T t is less than j T t j 2 034 f      using that f is decreasing For all measures we consider f     024 1   so j T t j 2 034 f     024 j T t j 2 M 024 j T t j  It remains to understand which is the probability that a pair of items each with support at least   is not sampled by SampleCount Let the random variable X k represent the total number of samples taken in chunk k  The probability that a f i j g is sampled in chunk k is X i;j;k X k  so the probability that it does not get sampled in any evennumbered chunk is Q k 2  024 even  1 000 X i;j;k X k  s  We have seen before that X i;j;k 016;L 025 s  i j  034  4 024  For what concerns X k using a Chernoff bound we can get X k 016;L  E  X k  024 mb=\024  using the linear upper bound on the number of samples So we can compute Y k 2  024 even  1 000 X i;j;k X k  s 024 022 1 000 s  i j  034 024 2 024\015 i;j mb 023 s\024 2 024 022 1 000 s  i j  034 4 mb 023 s\024 2 024 C exp 024 000 s  i j  034 s\024 8 mb 025 In order for this probability to be small enough  O 1 m 2   we need to bound the similarity to s  i j  025 8 mbL s\024\034 5 To choose the best value of 024 we balance constraints 3 and 5 getting 024L   mbL s\024\034  024  r mbM s 6 From which we can deduce s  i j   L  max  r mbM s  M   7 V D ATASET CHARACTERISTICS We have computed for a selection of the datasets hosted on the FIMI web page 1  the ratios between the number of occurrences of single items and pairs in the 002rst half of the transactions and the total number of occurrences of the same items or pairs The values of some of this ratios the most representative are plotted 002gure 3 on the x axis items or pairs are spread evenly after they have been sorted according to their associated ratio The y axis represents the value of the ratios We have taken into account only items and pairs whose support is over 20 occurrences in the whole dataset in order to avoid the noise that could be generated by very rare elements As we can see the number of occurrences and co-occurrences are not so far from what would be expected under a random permutation of the transactions The synthetic data set behaves exactly like we would expect under a random permutation with the ratio being very close to 1  2 for almost all items/pairs This means that even for real data sets where the order of transactions is not random the sampling probabilities used in the pair sampling are reasonably close to the ones that would be obtained under the random permutation assumption VI C ONCLUSIONS We presented the 002rst study concerning the problem of mining similar pairs from a stream of transactions that does rely on the similarity of items and not only on the frequency of pairs A thorough experimental study of carefully engineered versions of the presented algorithm remains to be carried out 1 http://fimi.cs.helsinki.fi 
127 
127 


Figure 3 Plots of the ratios j S 1 i j  j S i j and j S 1 i  S 1 j j  j S i  S j j  R EFERENCES  E Cohen M Datar S Fujiwara A Gionis P Indyk R Motwani J D Ullman and C Yang Finding interesting associations without support pruning IEEE Trans Knowl Data Eng  vol 13 no 1 pp 64–78 2001  Y.-K Lee W.-Y Kim Y D Cai and J Han Comine Ef\002cient mining of correlated patterns in Proc IEEE International Conference on Data Mining ICDM 2003  IEEE Computer Society 2003 pp 581–584  E Omiecinski Alternative interest measures for mining associations in databases IEEE Trans Knowl Data Eng  vol 15 no 1 pp 57–69 2003  J Han and M Kamber Data Mining Concepts and Techniques 2nd edition  Morgan Kaufmann 2006  R Agrawal and R Srikant Fast algorithms for mining association rules in large databases in Proc International Conference On Very Large Data Bases VLDB 1994  Morgan Kaufmann Publishers Inc Sep 1994 pp 487–499  J Han J Pei Y Yin and R Mao Mining frequent patterns without candidate generation A frequent-pattern tree approach Data Min Knowl Discov  vol 8 no 1 pp 53 87 2004  N Jiang and L Gruenwald Research issues in data stream association rule mining SIGMOD Record  vol 35 no 1 pp 14–19 2006  Y Zhu and D Shasha Statstream Statistical monitoring of thousands of data streams in real time Morgan Kaufmann 2002 pp 358–369  G Cormode and S Muthukrishnan What's hot and what's not tracking most frequent items dynamically ACM Trans Database Syst  vol 30 no 1 pp 249–278 2005  E D Demaine A L  opez-Ortiz and J I Munro Frequency estimation of internet packet streams with limited space in Proc 10th Annual European Symposium Algorithms ESA 2002  2002 pp 348–360  J X Yu Z Chong H Lu Z Zhang and A Zhou A false negative approach to mining frequent itemsets from high speed transactional data streams Inf Sci  vol 176 no 14 pp 1986–2015 2006  A Chakrabarti G Cormode and A McGregor Robust lower bounds for communication and stream computation in STOC  C Dwork Ed ACM 2008 pp 641–650  S Guha and A McGregor Stream order and order statistics Quantile estimation in random-order streams SIAM Journal on Computing  vol 38 no 5 pp 2044–2059  N Alon Y Matias and M Szegedy The space complexity of approximating the frequency moments J Comput Syst Sci  vol 58 no 1 pp 137–147 1999  J Misra and D Gries Finding repeated elements Sci Comput Program  vol 2 no 2 pp 143–152 1982  R M Karp S Shenker and C H Papadimitriou A simple algorithm for 002nding frequent elements in streams and bags ACM Trans Database Syst  vol 28 pp 51–55 2003  M Charikar K Chen and M Farach-Colton Finding frequent items in data streams Theor Comput Sci  vol 312 no 1 pp 3–15 2004  A Campagna and R Pagh Finding associations and computing similarity via biased pair sampling in Proc 9th IEEE International Conference on Data Mining ICDM 2009    Finding associations and computing similarity via biased pair sampling Invited for publication in Knowledge an Information Systems  2010  E Kushilevitz and N Nisan Communication complexity  New York Cambridge University Press 1997  J S Vitter Random sampling with a reservoir ACM Trans Math Softw  vol 11 no 1 pp 37–57 1985  D Dubhashi and D Ranjan Balls and bins a study in negative dependence Random Struct Algorithms  vol 13 no 2 pp 99–124 1998 
128 
128 


Application of Chaotic Particle Swarm Optimization Algorithm in Chinese Documents Classification 763 Dekun Tan Qualitative Simulation Based on Ranked Hyperreals 767 Shusaku Tsumoto Association Action Rules and Action Paths Triggered by Meta-actions 772 Angelina A. Tzacheva and Zbigniew W. Ras Research and Prediction on Nonlinear Network Flow of Mobile Short Message Based on Neural Network 777 Nianhong Wan, Jiyi Wang, and Xuerong Wang Pattern Matching with Flexible Wildcards and Recurring Characters 782 Haiping Wang, Fei Xie, Xuegang Hu, Peipei Li, and Xindong Wu Supplier Selection Based on Rough Sets and Analytic Hierarchy Process 787 Lei Wang, Jun Ye, and Tianrui Li The Covering Upper Approximation by Subcovering 791 Shiping Wang, William Zhu, and Peiyong Zhu Stochastic Synchronization of Non-identical Genetic Networks with Time Delay 794 Zhengxia Wang and Guodong Liu An Extensible Workflow Modeling Model Based on Ontology 798 Zhenwu Wang Interval Type-2 Fuzzy PI Controllers: Why They are More Robust 802 Dongrui Wu and Woei Wan Tan Improved K-Modes Clustering Method Based on Chi-square Statistics 808 Runxiu Wu Decision Rule Acquisition Algorithm Based on Association-Characteristic Information Granular Computing 812 JianFeng Xu, Lan Liu, GuangZuo Zheng, and Yao Zhang Constructing a Fast Algorithm for Multi-label Classification with Support Vector Data Description 817 Jianhua Xu Knowledge Operations in Neighborhood System 822 Xibei Yang and Tsau Young Lin An Evaluation Method Based on Combinatorial Judgement Matrix 826 Jun Ye and Lei Wang Generating Algorithm of Approximate Decision Rules and its Applications 830 Wang Yun and Wu-Zhi Qiang Parameter Selection of Support Vector Regression Based on Particle Swarm Optimization 834 Hu Zhang, Min Wang, and Xin-han Huang T-type Pseudo-BCI Algebras and T-type Pseudo-BCI Filters 839 Xiaohong Zhang, Yinfeng Lu, and Xiaoyan Mao A Vehicle License Plate Recognition Method Based on Neural Network 845 Xing-Wang Zhang, Xian-gui Liu, and Jia Zhao Author Index 849 
xiii 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





