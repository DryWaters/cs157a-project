Mining Generalized Actionable Rules Using Concept Hierarchies Li-Shiang Tsay 1 Seunghyun Im 2 1 North Carolina A&T State Univ School of Technology, USA 2 Univ. of Pittsburgh at Johnstown, Dept. of Comp. Science, USA ltsay@ncat.edu lishiangtsay@yahoo.com, sim@pitt.edu Abstract A series of mining actionable rule methods have been proposed from various aspects, but the existing models do not incorporate the concept of hierarchy/taxonomy into the mining process and restrict the terms used to build actionable rules to atomic concepts.  In order to resolve this problem, an integrated framework for extracting multiple-level actionable rules with ontology support is proposed so more generalized knowledge from data can be extracted.  This type of generalized rules will contain not only the attribute values contained in data, but also some concepts encoded in a given taxonomy Obtaining generalized actionable rules are a necessity since they provide a more general view of the domain The proposed framework is based on a breadth-first top-downward model to be developed by extending the existing single-level actionable rule discovery methods This framework can improve the quality of the extracted actionable rules in terms of their interestingness and understandability 1 Introduction Discovering actionable patterns in a data set has been a focused topic in recent data mining research [6  1323 T h i s ty p e of  r e s e a r c h  m o v e s  f u r t h e r  th a n  the classical data mining methods 1   3   1 4   26 by n o t only summarizing the data but also comparing the context of segmentations. Given a historical database where each record is described by a set of values, an actionable rule is an expression 001 X 000 001 Y 001 X and 001 Y are a set of attributes with value variation. The intuitive meaning of such a rule \(what changes within attribute X\, is needed to re-classify some objects from a lower profitability class to a higher one \(in attribute Y\. An example of such a rule might be that 94% of special need students will learn better and improve his/her grade by one letter grade if their teacher’s office hours are increased to 6 hours. In this case, 94% is called the confidence of the rule.  The left support of the rule 001 X 000 001 Y is the percentage of the records that contain the initial values on both X and Y and its right support is the percentage of records in X and Y after changing the values of both.   The problem of mining actionable rules is to find all rules that satisfy userspecified minimum left support, right support, and confidence thresholds Over the years many techniques have been proposed for discovering actionable rules from large collections, such as a domain-dependent way [13 a post-process analyzing rules manner [6  9 10   1619  23 a n d a do m a i n i n d e pe n de n t w a y   2 02 1    Unfortunately, those approaches do not incorporate the concept of hierarchy/taxonomy into the mining process and restrict the terms used to build actionable patterns to a single concept level.  The first issue is that the rules discovered at the atomic concepts level may not provide informative knowledge of the dataset due to specificity.  The second issue is that the rules at the atomic concepts level may not have minimum supports and thus some significant actionable rules may not be discovered.  To increase the chance of finding significant actionable rules at primitive concept level the minimum left and right support thresholds must be reduced substantially. By doing this, it may generate too many rules.  To reduce the number of valid rules one may place higher confidence in the pattern extraction procedure.  Pursuing high confidence usually results in a complex model and overly detailed rules In practice, the end users often prefer a simpler and more understandable model with slightly less accuracy Therefore, there is a need to extract actionable rules using the information of pre-defined taxonomy over the attribute values The concept hierarchies as background knowledge have long been successfully used to help generate a simpler and more meaningful result in information retrieval, association rule mining, etc 2   15  In  many applications, the taxonomy information is either stored implicitly in the database or provided by domain experts [4 d can b e u s ed  to  g e n e rali ze p r im i t i v e level concepts to higher level ones. More initiative and informative regulations can be found by mining data at higher abstract concepts. By exploring multiple-level concepts, rules can be generated across different levels 
2009 Fifth International Joint Conference on INC, IMS and IDC 978-0-7695-3769-6/09 $26.00 © 2009 IEEE DOI 10.1109/NCM.2009.410 2004 
2009 Fifth International Joint Conference on INC, IMS and IDC 978-0-7695-3769-6/09 $26.00 © 2009 IEEE DOI 10.1109/NCM.2009.410 2016 


of concept hierarchy to derive more interesting concise, simple, and understandable rules The most intuitive approach to mine generalized actionable rules is to apply the existing single-level actionable rule mining methods to exam attribute value-pairs at multiple levels of abstraction under the same minimum left support, right support and confidence thresholds.  This approach is simple but it may lead to some undesired results. Since the size of granules at each level of the hierarchy is different, the higher level concepts tend to have higher support and the lower level concepts tend to have lower support One is likely to find many strong rules at higher level concepts and those rules have high potential to echo to prior knowledge and expectations. This side effect exists in most of the generalized association rule methods 2  5  T o  av o i d t h i s d r aw b ack  w e u s e th e same approach as in [5 a d i f f eren t s e t o f  t h res h o l d s  will be used for different levels of abstraction In this study, we assume that domain experts will provide a concept hierarchy for each nominal attribute and the Rosetta-type algorithm will be utilized to automatically discretize for continuous attributes. An integrated framework for extracting generalized actionable rules with ontology support is proposed so more generalized knowledge from data can be extracted.  This type of generalized rules will contain not only the attribute values in data, but also some concepts encoded in a given taxonomy. Obtaining such rules is a necessity since they provide a more general view of the domain and the discovered rules are also simpler and more understandable than classical actionable rules. The proposed framework is based on a breadth-first top-downward model to be developed by extending the existing single-level actionable rule discovery method StrategyGeneratorIII 22 It  i s a  two-part process: identifying a set of frequent-closedfactor-sets within the different level concepts at different thresholds and generating reclassification rules from these factors. The anti-monotonic property is applied, in order to avoid construction of meaningless factor-sets, by considering if a factor is not frequent then its descendants will not be frequent as well This paper is organized as follows. In section 2, the concept of hierarchy is introduced.  An actionable rule and a non-redundant actionable rule are reviewed in sections 3 and 4, respectively.  The generalized actionable rules and the new algorithm are proposed in section 5. This study is concluded in section 6 along with future work 2 Concept Hierarchy Let V  v 1  v 2  v m be a set of possible values for attribute A Let H be a concept hierarchy for the attribute A which organizes relationships of values in a tree structure, shown in Figure 1.  In such tree, each level represents a different degree of abstraction. In [2   authors define the concept hierarchy as a partial order set and represent the special-general relationship between concepts as a partial order relation. Given two concepts v and w in a partial order relation P i.e v  w  003 P and v 004 w where v is a descendant of w and w is an ancestor of v For example, the relationship of concepts middleAge and Adult can be described as middleAge 004 Adult where the concept of Adult is a more general than the concept of middleAge or that middleAge is a more specific concept than Adult The value sits at root node representing the most general concept and the leaf node representing the primitive concept. Note that only the leaf values of concept hierarchy are represented in the original database and the attribute name is represented in the root node Intuitively, an attribute can be extended to have taxonomy by moving up from bottom to top. By grouping the similar primitive values into a general concept, the new-formed concept will be represented as a parent node of those primitive values.  The domain of the new-formed concept will be identified as the union of their children’s domain.  For example, the domain of Adult can be formed by adding the record ID that contains its children’ values Young  MiddleAge and Senior  3 Actionable Rules Reclassification Rule Mining \(RRM\[21-23  an d  Action Rule Mining \(ARM\ [7-8   1620  bot h a i m t o  build an actionable model that allows users to perform actions directly.  The structure of reclassification rules is similar to action rules; however, it differs in the following ways.  For RRM, the target of discovery is not pre-determined, while for ARM there is one and only one predetermined target.  RRM aims to discover Fi g ure 1. Conce p t hierarch y for a g e  MiddleA g e  age Children Youn g 18      19  30 Adult Senior 
2005 
2017 


all the patterns Z from a given data set, while ARM discovers only one specific subset of Z In addition attributes are not required to be classified as stable flexible, or decision attributes in reclassification rule mining.  RRM is more general than ARM.  It is important that the space of all actionable patterns is known.  If practical implementation of discovered actionable rules is too expensive to achieve, an alternative way is needed for searching the space of all patterns to obtain the same reclassification effect.    In this paper, we will focus on reclassification rule type of actionable rules 3.1 Information System An information system [12 i s  u s ed f o r rep r es en t i n g  knowledge and is defined as a pair S  U  A where U is a nonempty, finite set of objects, and A is a nonempty, finite set of attributes, i.e a  U 005 V a is a function for any a 003 A where V a is called the domain of a Elements of U are called objects. In this paper for the purpose of clarity, objects are interpreted as customers 3.2 Factor-Set  Any set of factors is called a factor-set F It is partitioned into constant factors F S and flexible factors F F Let us assume that v 1  v 2 003 V a Then, the expressions a  v 1 000 v 2 are elements of F S 006 F F  If v 1 007 v 2 then a  v 1 000 v 2 refers to a flexible factor and it denotes the fact that the value of attribute a can be changed from v 1 to v 2 for a number of objects in S  Similarly, the term a, v 1 005 v 2  o eans that the attribute value a  o  v 1 can be changed to a  o  v 2 for the object o If v 1  v 2  a  v 1 000 v 2 ers to a constant factor and its value does not change.  A factor-set is defined as any conjunction of factors where each factor is associated with a different attribute, for example F  a 1  v 1 000 v 2  a 2  v 1 000 v 2  a n  v 1 000 v 2  o 1  o 2   o m where 010 i 004 n  010 j 004 n  i 000 j  005   a i 000 a j  f o r m objects   3.3 Frequent Factor-Set  A factor-set 011 which supports the user specified minimum left support threshold s 1 012 0 and minimum right support threshold s 2 012 0 is said to be frequent The collection of all frequent factor-sets supporting these two thresholds is denoted by 000  s 1  s 2  000 011 1  011 2  011 n 000  supL 011  denotes left support and supR 011  denotes right support of 011 A frequent factor-set 011 in S is defined as a term 011  a 1  013 1 005 014 1  015  a 2  013 2 005 014 2  015  015  a p  013 p 005 014 p    where  010 i 004 p  010 j 004 p  i 000 j  005   a i 000 a j   supL  011  012 s 1  and supR  011  012 s 2  The Left Support defines the domain of a factorset which identifies objects in U on which the factorset can be applied.  The larger its value, the more interesting the factor-set will be for a user. The left hand side of the factor-set 011  is defined as P L   013 1  013 2  013 p   The domain Dom S  P L s a set of objects in S that exactly match P L  The left support supL of the factor-set 011 is defined as supL 011 rd[Dom S P L  C ar d[U   The Right Support shows how well the factor-set is supported by objects in S The higher its value, the stronger its effect will be.  The right hand side of the factor-set 011 is defined as P R  014 1  014 2  014 p   The domain Dom S  P R is a set of objects in S that exactly match P R The right support supR of the factor-set 011 is defined as supR  011  Card  Dom S  P R  Card  U  3.4 Reclassification Rule The set of all candidate rules is constructed from frequent factor-sets.  The number of factors in a factorset will be called the length of the factor-set.  A factorset of the length k is referred to as a k factor-set Each rule is extracted through k factor-set having at least two flexible factors f F when k 012 2.  A candidate rule whose confidence is greater than the user specified minimum confidence c is called a strong rule and it belongs to 0005  s 1  s 2  c  A reclassification rule 000U in S is defined as a statement 000U  000 000 000<\000\017 000\003 where 000  000 003 000  000 007 000  and 000 016 000  017  The factor-set 000;\000\003 is called the action premise antecedent\ and 000 is called the effect consequent\f the reclassification rule.  Let us assume that 000  a 1  013 1 005 014 1  015  a 2  013 2 005 014 2  015  015  a p  013 p 005 014 p  n d  000  b 1  020 1 005 021 1  015  b 2  
2006 
2018 


020 2 005 021 2  015  015  b q  020 q 005 021 q    I f w e s a y t h a t  objects o 1  o 2 003 U support the reclassification rule 000U  in S if and only if   010 i 004 p  a i  o 1  013 i  015  a i  o 2  014 i     010 j 004 q  b j  o 1  020 i  015  b j  o 2  021 i   Statistical significance of a rule 000U is expressed in terms of left support and right support denoted as supL  000U nd supR  000U respectively.  The left support measure defines the range of the rule, i.e., the proportion of objects which are applicable. The right support measure defines the feasibility of the rule, i.e the proportion of objects which are role models for others.  Because supports indicate the frequency of the factor-set occurring in the rule, then supL  000U d supR  000U are basically the same as the supports for the factor-set 011 i.e supL  000U  supL 011  and supR  000U   supR 011  The strength of a reclassification rule 000U is characterized by confidence, which is denoted by conf 000U Statistical significance of the antecedent of a rule 000U is called antecedent-right-support and antecedent-left-support which are denoted as a-supL  000U  and a-supR  000U respectively a-supL  000U is defined as the number of objects in S which have property 013 i for i  1  2  p  a-supR  000U is defined as the number of objects in S that have property 014 j for j  1  2  q  The confidence of a reclassification rule 000U in S is defined as conf  000U  supL  000U   supR  000U    a-supL  000U   asupR  000U   4 Non-redundant Reclassification rules  A set of non-redundant reclassification rules is based on the concept of closed frequent factor-set which are sufficient to drastically reduce the rule set without information loss [20   By a closed factor-set 000 in S we mean a term 000  a 1  013 1 005 014 1  015  a 2  013 2 005\014 2  015  015  a p  013 p 005\014 p  iff none of its supersets 000 satisfies supL  000  supL  000  supR  000  supR  000   It means that 000 is not closed if at least one of its immediate supersets has both supports the same as 000  By a frequent closed factor-set 022 we mean that  022  a 1  013 1 005 014 1  015  a 2  013 2 005\014 2  015  015  a p  013 p 005\014 p  is a frequent closed factor-set iff 022 023 000 supL 022  012 s 1 and supR 022  012 s 2  A closed factor-set 022 which supports the user specified minimum left support threshold s 1 012 0 and minimum right support threshold s 2 012 0 is said to be a frequent and closed factor-set. supL 022  denotes left support and supR 022  denotes right support of 022 If 022  011  supL 022 supL 011 d supR 022 supR 011 The collection of all such factor-sets supporting these two thresholds is denoted by 000 000F 000\003  s 1  s 2  000 022 1  022 2  022 n 000  Let us first formally define what we mean by a redundant reclassification rule.  Assume 000U i denotes the rule X  013 005 014  i 000  Y  020 005 021  i   000U 1 is more general than rule 000U 2 if 000U 2 is valid 000U 1 and 000U 2 have the same left/right support and confidence X  013 005 014  1 023  X  013 005 014  2 and Y  020 005 021  1 023  Y  020 005 021  2 and it is denoted as 000U 1 004 000\003\000U 000\025 000\021 000\003\000\003\000\003 Let 0005  000\003 000U 1  000\003\000U 2   000\003\000\017 000\003\000U n be a set of rules that have the same left and right support counts and confidence, i.e supL  000U 1  supL  000U 2   supL  000U n  supR  000U 1  supR  000U 2  supR  000U n  and conf  000U 1  conf  000U 2  conf  000U n  000U m is a redundant rule if 000U n is valid and 000 000U n 004 000\003\000U m 000\017\000\003\000\003\000\003 000  Y  020 005 021  n  Y  020 005 021  m 000\003 and 000\003  X  013 005 014  n 000  X  013 005 014  m but not  X  013 005 014  m 000  X  013 005 014  n or 000  X  013 005 014  n  X  013 005 014  m and Y  020 005 021  n 000  Y  020 005 021  m  Therefore, the non-redundant rules are those that have the shortest antecedent and the longest consequent. A non-redundant reclassification rule 0001\0005 is defined as 0001\0005  X  013 005 014  000  Y  020 005 021   is  not redundant iff none of the existed rules 0005   X  013  005 014   000  Y  020  005 021    with supL  0001\0005  supL  0005   supR  0001\0005  supR  0005   conf  0001\0005  conf  0005   satisfies X  013  005 014   023  X  013 005 014  and  Y  020 005 021  023  Y  020  005 021   As mentioned in the introduction, some shortcomings in the existing methods for actionable rule mining are due to not incorporating the concept of hierarchy/taxonomy into mining procedure 
2007 
2019 


 GAR H  S  s l1  s l 2   c l  Input: H   a hierarchy concept for every attribute S  U  A  V an information system, where U are objects A are attributes, and V are their values s l1   threshold for a minimum Right Support at level l s l2   threshold for a minimum Left Support at level l c l   threshold for a minimum Confidence at level l  Step 1 Find the frequent generalized closed factor-sets: the sets of factors that have minimum left and right supports at highest level Step 1.1 Find the domain of each concept at level l and its support count above the predefined s l1 or s l2  AV  Get all distinct attribute-concept-value pairs for all attribute-concept-value pair  a,v  do begin HD  A, CV, L, D\:=Get domain of each concept value CV – concept value, L – Level value, 1, 2, 3,…., D – domain While L 007 1 do For each concept value If Card\(HD  min \(s l1 s l2    qualified attribute-value pairs  a, v  distinctValueQueue  006  a,v   end if end for end while end for Step 1.2 Find the frequent generalized closed factor-set Step 1.2.1 1-factor-sets N:= Get all elements at level  1 from distinctValueQueue M:= Get all elements at level  1 from distinctValueQueue while subLevelDistinctValueQueue changed do  for each attribute-value pair  a n v n  do For each attribute-value pair  a m v m  do If  a n  a m  AtomicFactor  a l  v n 005 v m   If  supL  AtomicFactor  012 s l1 supR  AtomicFactor  012 s l2  atomicFactorQueue 006  AtomicFactor  subLevelDistinctValueQueue 006 subLevelDistinctValue  end if end if end for N:= Get all elements from subLevelDistinctValueQueue M:= Get all elements from subLevelDistinctValueQueue end for end while  Step 1.2.2 2-factor-sets F:= Get all atomic factors  from atomicFactorQueue every factor denotes as f=\(a, v 005 v n:= F.size         // the total number of primitive factors in F for  i:=0; i++; i>n  do for  j  i+1  j++; j>n  do if  f i a 007 f j a  newCandidate  f i 015 f j  if \( supL  newCandidate  s 11  supR  newCandidate  s 12  add this new factor-set into the frequent factor-set queue and mark its length with 2 frequentFactorQueue 006  newCandidate    end if end if end for end for Step 1.2.3 Find the frequent k factor-sets, where k 2.  This step is iteratively generating new k factor-sets using the frequent \(k1\ctor-sets found in the previous iteration until there are no new frequent factor-sets generated P:=Get all frequent  k 1  factor-sets from frequentFactorQueue n:= the total number of \(k-1\-factor-sets  in P For  i:=0; i++; i>n  do If the first \(k-2\ factors are identical in f i and f j newCandidate  f i 015 f j  If \( supL  newCandidate  s 1 supR  newCandidate  s 2  If all subsets of newCandidate 023 frequentFactorQueue add this new factor-set into the frequent factor-set queue and mark its length with k frequentFactorQueue 006  newCandidate  end if end if end if end for Step 1.2.4 F max get all frequent factor-sets f with max Length from frequentFactorQueue maxLength:= the length of a factor-set in F max CompactFactorQueue  006 F k for  i   maxLength-1  i- -, i  1  do F i get all frequent factor-sets f’ with length i from frequentFactorQueue for each f’ in F i do if f  supL > max  010 f.supL in F i+1  if f  supR > max  010 f.supR in F i+1   CompactFactorQueue  006 f  end if end if end for end for Step 2. Generate strong Rules having the confidence above c  For each frequent factor-set f in frequentFactorQueue do n := f.length //the length of the frequent factor-set EQ for elementQueque EQ:= Parse the factor-set into factors and store each factor with level of 999 for the length of  the premise i from 1 to n-1 get next available premise for all factors in EQ Consequence  f- premise.factor   end for ruleCandidate  premise 000 Consequence  if \(conf\(ruleCandidate  012 c add rule the output and place a positive mark results 006 ruleCandidate mark EQ.level with level i //place a Positive mark end if end for end for print all Reclassification rules from result Figure 2. Pseudo-code of algorithm GAR 
2008 
2020 


5. Generalized Reclassification Rules Authors in [15  n tro d u ce h i erarch y  tax o n o m y  into association rule mining to identify more interesting rules and their results show promise. In this paper, we also studied generalization in mining, but used a different mining paradigm, actionable rule mining.  We developed a similar approach, called Generalized Actionable Rules \(GAR\, in order to find a concise set of reclassification rules.  The algorithm GAR is a breadth-first approach to mining a set of all frequent generalized closed factor-sets within concept hierarchies from top to lower level abstraction and then constructs reclassification rules straightforward by partitioning the valid factor-sets into a IF-THEN representation A generalized closed factor-set 000 G in S is defined as 000 G  a 1  013 1 005 014 1  015  a 2  013 2 005\014 2  015  015  a p  013 p 005\014 p  iff 000 G 023 000 and none of its factors is the ancestor factor of others A frequent generalized closed factor-set 022 G is defined as 022 G  a 1  013 1 005 014 1  015  a 2  013 2 005\014 2  015  015  a p  013 p 005\014 p  is a frequent generalized closed factor-set at level l iff 022 G 023 000 G  supL 022 G  012 s l1 and supR 022 G  012 s l2 where s l1 and s l2 are the user specified minimum left support  and minimum right support thresholds at level l respectively If a factor occurs rarely, its descendants will be even less frequent. For finding generalized actionable rules, different minimum left support right support, and confidence thresholds can be specified at different levels.   By doing this, if users want to find actionable rules at relatively lower levels of abstraction, the minimum left and right supports can be set relatively low without compromise, generating many valid uninteresting rules at higher or intermediate levels A generalized reclassification rule 000U G in S is defined as a statement 000U G  000 000 000<\000\017\000\003\000\017 where 000  000<\000\017\000\003 003\022 G  000 007 000<\000\017  000 016 000<\000\017\000\003  017 and no factor-set in 000 is an ancestor of any factor-set in 000 in the concept hierarchy Algorithm GAR consists of two main steps: \(1 generate all frequent generalized closed factor-sets, \(2 generate strong generalized reclassification rules Below is the short description of the algorithm and the pseudo-code of GAR is presented in Figure 2 5.1 Generate all Frequent Generalized Closed Factor-Sets  This step computes the frequent factor-sets in the data set through several iterations and the breadth-first top-downward approach is utilized. In each iteration we generate new candidates from frequent generalized closed factor-sets found in the previous iteration; the left support and right support for each candidate is then computed and tested against the user-specified thresholds for that level.  It examines the highest top level of abstraction to generate frequent closed factorsets and then progresses deeper into their frequent descendants at lower concept levels. The lower level concepts will be evaluated only when its ancestor has large left and right supports at the corresponding upper level.  End users have the power to gradually decrease the minimum left and right support thresholds at lower levels of abstraction in order to discover rules at different levels 5.2 Generate Generalized Rules  A reclassification rule is extracted by partitioning the factor-set W into two non-empty subsets X and Y and represented as X 000 Y where X, Y 023 W  X, Y 024 F S and X  W Y A level-wise approach is utilized for generating rules, where each level corresponds to the number of factors belonging to the rule premise The rule-premise is extended one factor at a time.  It starts with one factor on the rule-premise, such as X If the rule confidence is above the predetermined threshold, it is called a strong rule and marked with a positive mark  The principle of minimum description length is also adopted to identify the general rules.  Therefore, in the next iteration, it generates new candidates from only unmarked rules found in the previous iteration by moving one of the consequence-type factors to the rule premise, now X is a 2-factor-set.  Repeat the previous step until there is only one flexible factor on the rule consequent, such as Y By doing this, the resulting set of rules is comprised of those with the shortest length of premise, not all the lengths 
2009 
2021 


6.   Conclusion This paper addressed the problem of discovering generalized actionable rules in a dataset.   We investigated the properties of Reclassification Rules presented the notions of generalized closed factor-sets generalized reclassification rules, and introduced algorithm GAR This new algorithm is based on the concept of frequent generalized closed factor-sets to generate a very concise set of rules. The proposed framework provides more generalized knowledge from data than previous existing methods by incorporating concept hierarchy into mining procedure.  In addition the quality of the extracted rules in terms of their interestingness and understandability is improved Therefore, it would be easier for the user to comprehend the rule result in a timely manner.   In the future, we intend to apply this approach to a large variety of databases and application domains 7. References 1 A grawal R   Imiel i n s ki  T an d  S w ami, A M i n i n g  association rules between sets of items in large databases. In Proceedings of the ACM SIGMOD International Conference on the Management of Data. P. Buneman and S. Jajodia, Eds ACM Press, Washington DC, 1993, pp. 207–216   B e ne dit t o  M  E M  D  a nd Ba r r o s L  N d Us ing  Concept Hierarchies in Knowledge Discovery. A.L.C Bazzan and S. Labidi \(Eds.\: SBIA 2004, LNAI 3171, pp 255–265, 2004. Springer-Verlag Berlin Heidelberg 2004   G r z y m a l a B us se   J  A ne w v e r s i on of the r u le induc t i on system LERS. In: Fundamenta Informaticae, 31\(1\, 1997, pp 27-39  Ha n  J  Ca i  Y   and Ce r c one  N  Da t a D ri v e n D i s c ov e r y  of Quantitative Rules in Relational Databases. In: IEEE Trans. Knowledge and Data Eng., vol. 5, pp. 29-40, 1993   Ha n J   a nd Fu Y   D i s c ov e r y  of  m u lt ipl e l e v e l  association rules from large databases. In Proc. of the 21st Int'l Conference on Very Large Databases Zurich Switzerland, September 1995  H e  Z  Xu, X  a nd De ng  S  M i ning  Cl us te r D e f i ning  Actionable Rules. In: Proceedings of NDBC’04, 2 004   H e Z X u X  D e ng S a nd M a R  M i ning A c t i on Rules From Scratch. Expert Systems with Applications. Vol 29, No. 3, 691--699 \(2005   I m S and R a 001\036 Z.W.: Action Rule Extraction from a Decision Table: ARED.  In: 17th International Symposium on Methodologies for Intelligent Systems \(ISMIS\, pp 160—168. Springer,  Toronto, Canada \(2008 9 i n g  C  X Che n T  Y a ng Q a nd Che n   J  M i ni ng  Optimal Actions for Intelligent CRM. In: 2002 IEEE International Conference on Data Mining, pp.767--770 IEEE Computer Society, Maebashi City, Japan \(2002 9 i u B   H s u  W   a nd M a  Y    I d e n ti f y ing  Nona c t i ona ble  Association Rules. In: Proceedings of KDD 2001, pp. 329-334. San Francisco, CA, USA \(2001  a s quie r N    B a st i d e Y T a oui l R a nd L a k h a l   L    Discovering Frequent Closed Itemsets for Association Rules In: the 7th international conference on database theory ICDT’99\ pp 398--416, Jerusalem, Israel \(1999 11 P a wlak Z   In f o r m at io n S y st em s   Th eo ret i ca l Foundations. Information Systems Journal. Vol. 6, pp. 205-218 \(1981  i a t e s k y S ha pi ro G   a nd M a t h eu s C   J    T h e  interestingness of deviations. In: Proceedings of AAA Workshop on Knowledge Discovery in Database, pp. 25--36 AAAI Press, Menlo Park, CA \(1994  n la n J   R  C4.5: program for machine learning  Morgan Kaufmann, 1992  m a k r i s hns n  S  a nd R a k e s h  A    M i ni ng G e ne ra l i z e d  Association Rules. In: Proceedings of ICDM’03, pp 685688. IEEE Computer Society, Florida, USA \(2003  a 001\036 Z.W. and Tsay, L.-S.: Discovering Extended Action-Rules \(System DEAR\. In: Proceedings of the IIS'2003 Symposium, Advances in Soft Computing, pp. 293300. Springer, Zakopane, Poland \(2003  001\036 Z.W. and Wieczorkowska, A.: Action Rules: How to Increase Profit of a Company. In: Principles of Data Mining and Knowledge Discovery, Proceedings of PKDD'00, LNAI, pp. 587-592. Springer, Lyon, France 2000  T s a y L  S a nd Ra 001\036 Z.W.: Action Rules Discovery System DEAR2, Method and Experiments. Journal of Experimental and Theoretical Artificial Intelligence, Vol. 17 No. 1-2, pp. 119-128. Taylor and Francis \(2005  s a y L  S a nd Ra 001\036 Z.W.: E-Action Rules. In: Lin T.Y., Xie, Y., Wasilewska, A., and Liau, C.-J. \(eds Foundations of Data Mining, Studies in Computational Intelligence, pp. 261--272. Springer, Berlin / Heidelberg 2007  s ay   L  S a nd Ra 001\036 Z.W.: Discovering the Concise Set of Actionable Patterns. In: 17th International Symposium on Methodologies for Intelligent Systems \(ISMIS\ LNAI, Vol 4994, pp. 169--178. Springer \(2008 20  Tsay  L  S   R as  Z  W   an d Im, S   Re class i ficat io n  Rules. In: IEEE/ICDM Workshop on Foundations of Data 
2010 
2022 


Mining \(FDM 2008\, pp. 619--627. IEEE Computer Society Pisa, Italy \(2008  T s a y L  S   a nd I m   S    M i ni ng NonRe dunda nt Reclassification Rules.  In: Proceedings of the Twenty Second International Conference on Industrial, Engineering Other Applications of Applied Intelligent Systems IEA/AIE'09\, Tainan, Taiwan, June 24-27, 2009, 806-815  W ong R  C  W a nd Fu A   W  C    I S M   It e m  Se le c t ion for Marketing with Cross-selling Considerations. In the Eighth Pacific-Asia Conference on Knowledge Discovery and Data Mining \(PAKDD  pp. 431--440. Sydney, Australia 2004  ng Q Yi n  J   L i n  C  X   a nd C h e n  T     Postprocessing Decision Trees to Extract Actionable Knowledge. In Proceedings of ICDM’03, pp 685-688 IEEE Computer Society, Florida, USA \(2003  Z h a n g   H  Zha o Y  C a o, L  a nd Zha n g   C    C o m b i n e d  Association Rule Mining. In: Advances in Knowledge Discovery and Data Mining, Proceedings of the PAKDD Conference, Lecture Notes in Computer Science, 5012, pp 1069-1074. Springer, Antwerp, Belgium \(2008  n g  T  Ra m a k r i s hna n, R a nd L i v n y  M   B I R C H  An efficient data clustering method for very large databases In: Proceedings of ACM SIGMOD Conference, Montreal Canada, pp. 103–114 
2011 
2023 


