Incrementally Mining Hi gh Utility Itemsets in Dynamic Databases   Chun-Wei Lin Department of Computer Science and Information Engineering National Cheng Kung University Taiwan, R.O.C jerry@jerry.idv.tw Tzung-Pei Hong Department of Computer Science and Information Engineering National University of Kaohsiung Taiwan, R.O.C Department of Computer Science and Engineering National Sun Yat-sen University Taiwan, R.O.C tphong@nuk.edu.tw  Guo-Cheng Lan, Hsin-Yi Chen Hung-Yu Kao Department of Computer Science and Information Engineering National Cheng Kung University Taiwan, R.O.C rrfoheiay@gmail.com p78981215@mail.ncku.edu.tw hykao@mail.ncku.edu.tw   Abstract  Utility mining is proposed to consider additional measures, such as profits or costs according to user preference. In the past, a two-phase mining algorithm was proposed for fast disc overing high utility itemsets from databases. In this paper, an incremental mining algorithm to efficiently update high utility itemsets is proposed for record insertion. Experimental results 
also show that the proposed algorithm executes faster than the two-phase batch mining algorithm  1. Introduction  Utility mining [3, 10 a y  b e th o u g h t o f as a n  expansion of frequent-itemset mining [1-2, 5  w i t h so l d  quantities and item profits considered. Liu et al  designed the two-phase algorithm o r eff i cie n tl y  extracting all high utility itemsets based on the downward-closure property. In the above approaches the database was assumed static and the mining processes were performed in a batch way In real-world applications, transactions in a database do not usually remain a stable condition Some transactions may be inserted into an original database due to new purchases from customers Cheung et al then proposed the Fast-UPdate \(FUP algorithm [4 so lv e t h e li m itat i o n s o f asso cia tio n rule mining in an incremental approach 
In this paper, an incremental mining algorithm to update the discovered high utility itemsets is thus proposed. It is based on Liu et al s two-phase mining approach [7 d th e FUP  co n cep t [4 t o p a rtitio n  itemsets into four parts according to whether they are high transaction-weighted utilization itemsets in the original database and in the new transactions. Each part is then executed by its own procedure. Experimental results show that the proposed algorithm has a better performance than the two-phase batch mining algorithm  2. Review of related works  In this section, the FUP algorithm and the concept of mining high utility itemsets are briefly reviewed  2.1. The FUP algorithm  In real-world applications, transactions in a database do not usually remain a stable condition Considering an original database and some new transactions, the following four cases \(illustrated in 
Figure 1\may occur  
Original database 
New transactions 
Large Itemsets Small Itemsets Large Itemsets Small Itemsets Case 1 Case 2 Case 3 Case 4  Figure 1. Four cases when new transactions are inserted into existing database 
2010 IEEE International Conference on Granular Computing 978-0-7695-4161-7/10 $26.00 © 2010 IEEE DOI 10.1109/GrC.2010.151 303 


Since itemsets in Case 1 are frequent \(large\n both the original database and the new transactions, they will still be frequent \(large\fter the weighted average of the counts. Similarly, itemsets in Case 4 will still be non-frequent \(small\e new transactions are inserted. Thus Cases 1 and 4 will not affect the final frequent \(large\ itemsets Case 2 may remove existing frequent \(large\ itemsets, and Case 3 may add new frequent \(large\ itemsets  2.2. Mining high utility itemsets  In the past, several mining algorithms were proposed for efficiently discovering high utility itemsets i u  et al then designed a two-phase algorithm for efficiently discovering all high utility itemsets [7 n s i s ts o f t w o p h ases to g e n e rate a n d test high utility itemsets in a level-wise way. The transaction utility was first used as the effective upper bound of each candidate itemset in the transaction such that the “transaction-weighted downward closure property” could be kept in the search space to decrease the number of candidate itemsets. An additional database scan was then performed to find out the real utility values of the remaining candidates and identifies the high utility itemsets  3. The proposed increme ntal algorithm for record insertion  Considering there is an example to show the original database in Table 1. It consists of 10 transactions and 6 items, denoted A to F   Table 1. An original database in the example  TID A B C D E F 1 3 2 0 3 0 0 2 2 0 0 4 2 0 3 3 0 5 0 0 3 4 1 0 3 0 1 2 5 1 0 0 3 2 0 6 1 2 0 4 0 0 7 2 3 2 0 1 1 8 0 0 0 0 0 2 9 0 0 3 3 0 0 10 3 0 0 4 0 0  Assume the minimum high utility threshold is set at 35%, and also assume the user-defined profit values for the items are given in a utility table shown in Table 2 The two-phase algorithm is first performed to find high transaction-weighted utilization itemsets and their actual utility values before new transactions come. The results are then shown in Table 3  Table 2. The utility table  Item Profit A 3 B 150 C 1 D 50 E 100 F 20  Table 3. The high transaction-weighted utilization itemsets and their actual utility values  Itemset High transactionweighted utility Actual utility value A 2728 48 B 1540 1050 D 2083 1050 E 1483 600 AB 1540 1068 AD 1930 930 AE 1483 618  The details of the proposed incremental mining algorithm are described below  The proposed incremental algorithm   INPUT An original database D the discovered high transaction-weighted utilization itemsets with their actual utility values from the original database, a minimum high utility count muc D total transaction utility 000 002 D T k k tu minimum high utility threshold 003  in the original database, and a set of d new transactions OUTPUT A set of high utility itemsets HU U or the updated database U  D d 004  STEP 1 Calculate the utility value u jk  of each item i j  in each new transaction t k as u jk  q jk  p j  where q jk  is the quantity of i j  in t k and p j is the profit of p j Accumulate the utility values all items  in each transaction t k as the transaction utility tu k That is 1  m kjk j tu u   000  STEP 2 Calculate the minimum high utility count muc d  000  n k k tu 1 minimum high utility threshold 003 the new transactions  STEP 3 Set k 1, where k records the number of items in the itemsets s currently being processed  
304 


STEP 4 Generate the candidate k itemsets and calculate their transaction-weighted utilities twu d  s rom the new transactions as the summation of the utilities of the new transactions which include i j That is     000 002  k j t i k d tu s twu  STEP 5 Check whether the twu d  s each k itemset from the new transactions is larger than or equal to the minimum high utility count muc d in the new transactions. If s satisfied the condition, put it in the set of high transaction-weighted utilization k itemsets for the new transactions d k HTWU  STEP 6 For each k itemset s in the set of high transaction-weighted utilization k itemsets  D k HTWU rom the original database, if it also appears in the set of high transactionweighted utilization k itemsets d k HTWU  in the new transactions, do the following substeps Case 1  Substep 6-1 Set the updated transactionweighted utility of itemset s as twu U  s  twu D  s  twu d  s  where twu D  s is the high transactionweighted utility of itemset s in the original database D k HTWU d twu d  s e high transaction-weighted utility of itemset s in the new transactions   d k HTWU  respectively Substep 6-2 Put s in the set of updated high transaction-weighted utilization k itemsets U k HTWU  STEP 7 For each k itemset s in the set of high transaction-weighted utilization k itemsets  D k HTWU rom the original database, if it does not appear in the set of high transaction-weighted utilization k itemsets  d k HTWU the new transactions, do the following substeps Case 2  Substep 7-1 Set the updated transactionweighted utility of itemset s as twu U  s  twu D  s  twu d  s  Substep 7-2 Check whether the updated  twu U  s larger than or equal to the updated minimum high utility count muc D  muc d  If s satisfied the above condition, put it in the set of updated high transaction-weighted utilization k itemsets U k HTWU  Otherwise, remove s from the set of high transaction-weighted utilization k itemsets  D k HTWU the original database STEP 8 For each k itemset s if it does not appear in the set of high transaction-weighted utilization k itemsets D k HTWU rom the original database, but appears in the set of high transaction-weighted utilization k itemsets   d k HTWU in the new transactions, do the following substeps  Case 3  Substep 8-1 Rescan the original database to determine the transaction-weighted utility  twu D  s f itemset s   Substep 8-2 Set the updated transactionweighted utility of itemset s as  twu U  s  twu D  s  twu d  s  Substep 8-3 Check whether the updated transaction-weighted utility twu U  s er than or equal to the updated minimum high utility count muc D  muc d f s satisfied the above condition, put it in the set of updated high transaction-weighted utilization k itemsets U k HTWU  STEP 9  Generate the candidate k 1\msets from the set of high transaction-weighted utilization k itemsets U k HTWU the updated database STEP 10 Set k  k 1 STEP 11 Repeat STEPs 4 to 10 until no new candidate itemsets are generated STEP 12 For each high transaction-weighted utilization itemset s in U HTWU of the updated database, if it appears in the set of high transaction-weighted utilization itemsets in D HU of the original database do the following substeps Substep 12-1  Calculate the actual utility value of each itemset s for the new transactions as 000¦\000 005\002  kj t ss i jk d u s au    where u jk is the utility value of each item i j  in each new transaction t k  Substep 12-2  Set the updated actual utility value of itemset s in the updated database as       s au s au s au d D U    Substep 12-3 Check whether the actual utility value of itemset s is larger than or 
305 


equal to the updated minimum high utility count muc D  muc d it satisfies the above condition, put it in the set of updated high utility itemsets HU U  STEP 13 For each high transaction-weighted utilization itemset s in U HTWU of the updated database, if it does not appear in the set of high transaction-weighted utilization itemsets in D HU of the original database do the following substeps Substep 13-1 Calculate the actual utility value of each itemset s for the new transactions as 000¦\000 005\002  kj t ss i jk d u s au    Substep 13-2  Rescan the original database to determine the actual utility value au D  s  Substep 13-3  Set the updated actual utility value of itemset s in the updated database as       s au s au s au d D U     Substep 13-4 Check whether the actual utility value of itemset s is larger than or equal to the updated minimum high utility count muc D  muc d it satisfies the above condition, put it in the set of updated high utility itemsets HU U   After STEP 13, the high utility itemsets for the updated database can then be updated  4. An example  In this section, the new transactions to be inserted are shown in Table 4  Table 4. Five new transactions in the example  TID A B C D E F 11 3 0 4 8 0 0 12 3 0 3 7 0 0 13 3 0 2 6 0 0 14 1 0 0 0 1 1 15 4 3 0 0 0 0  The original database and already discovered high transaction-weighted utilization itemsets with their actual utility values were shown in Table 3. Assume the minimum high utility threshold is also set at 35 The proposed incremental algorithm for record insertion proceeds as follows STEP 1 & 2 & 3 The utility value of each item occurring in each new transaction in Table 4 is calculated. The minimum high utility count in the new transactions is then calculated as \(1671*0.35\, which is 584.85. The variable k is first set to 1 STEP 4 & 5 In this example A 1671 C 1086 and D 1086} are then put in the set of high transaction-weighted utilization 1-itemsets in the new transactions STEP 6 In this example A 4399} and D 3169 are then put in the set of high transaction-weighted utilization 1-itemsets for the updated database STEP 7 In this example, only B 2002} is put in the set of high transaction-weighted utilization 1itemsets for the updated database STEP 8 & 9 & 10 In this example, only C 2037 is then put in the set of high transaction-weighted utilization 1-itemsets for the updated database. After STEP 8, the high transaction-weighted utilization 1itemsets for the updated database are A 4399  B 2002 C 2037} and D 3169}. The candidate 2itemsets are then generated from the above results STEP 11 The STEPs 4 to 10 are then repeated until no candidate itemsets are generated. In this example, the results are A 4399 B 2002  C 2037 D 3169 AB 2002 AC 1884}, and  AD 3016 STEP 12 In this example, two itemsets D 2100 and AD 2007} are considered as the high utility itemsets for the updated database STEP 13 In this example, no high utility itemsets are generated in this STEP  After STEP 13, the high utility itemsets for the updated database are D 2100} and AD 2007  5. Experimental results  A series of experiments was conducted to evaluate the performance of the two-phase algorithm \(TP-HUI in a batch way and our proposed incremental algorithm for record insertion \(FUP-HUI\The public IBM data generator was used in our experiments al u a te  the performance. The first 200,000 transactions were then extracted [8 in itiall y  m i n e th e h i g h tran sactio n weighted utilization itemsets with their actual utility values. The next 2,000 transactions were then generated sequentially used each time as new inserted transactions. Figure 2 then showed the execution time required by the two-phase algorithm \(TP-HUI\d the proposed incremental algorithm \(FUP-HUI\or processing each 2,000 new transactions on the T10I4N4KD200K dataset at 0.2% minimum high utility threshold  
306 


 Figure 2. The comparison of the execution time for different numbers of inserted transactions   It can be observed from Figure 2 that the proposed FUP-HUI ran faster than the TP-HUI in a batch way Experiments were then also made to evaluate the efficiency of the proposed FUP-HUI in different minimum high utility threshold values and shown in Figure 3 for the execution time. The minimum high utility thresholds were then set from 1.0% to 0.2 decrease 0.2% each time   Figure 3. The comparison of the execution time at different minimum utility thresholds   It can easily be observed from Figure 3 that the execution time by the proposed FUP-HUI was much less than that by the TP-HUI in a batch way for handling record insertion at different minimum high utility thresholds  6. Conclusion  In this paper, an efficient incremental algorithm to update the discovered high utility itemsets for record insertion is proposed. When new transactions are inserted into the original database, the proposed maintenance incremental algorithm then partitions itemsets into four parts according to they are high transaction-weighted utilization itemsets in the original database or not and in the new transactions. Each part is then processed to update the discovered high utility itemsets in its own way. Experimental results show that the performance of the proposed algorithm executes faster than the two-phase algorithm in a batch way  7. References  1 R A g ra wa l a nd R Srik a n t   F a s t a lg o rithm s f o r m ining  association rules in large databases The 20th International Conference on Very Large Data Bases pp. 487-499, 1994  2 R A g ra w a l, T  Im i e linsk i, a n d A  Sw a m i M ini n g  association rules between sets of items in large databases International Conference on Management of Data pp. 207216, 1993  3 R. Cha n Q. Ya ng a nd Y. D She n  M i n ing hig h  utility  itemsets The 3rd IEEE International Conference on Data Mining pp. 19-26, 2003  4 D  W  C h e ung H  J i a w e i V. T  N g a n d C  Y  W ong   Maintenance of discovered association rules in large databases: An incremental updating technique The 12th International Conference on Data Engineering pp. 106-114 1996  5 J Han J  P e i Y Yin  a n d R  M a o M in i n g f r eq u e n t  patterns without candidate generation: A frequent-pattern tree approach Data Mining and Knowledge Discovery vol. 8 pp. 53-87, 2004  6 T   P H ong  C  W  L i n a n d Y  L  W u  I nc r e m e nta lly  f a s t  updated frequent pattern trees Expert Systems with Applications vol. 34, pp. 2424-2435, 2008  7 Y  L i u, W  k  L i a o a nd A  C h o u d h a r y  A t w opha s e  algorithm for fast discovery of high utility itemsets Lecture Notes in Computer Science vol. 3518, pp. 689-695, 2005  8 I B M Q u e s t D a ta Mini ng P r o j e c t  199 6, Q u e s t s y nthe tic  data generation code. Available http://www.almaden.ibm.com/cs/quest/syndata.html   N L  S a rd a and N  V  S r i n i v as  A n ad ap t i v e al go ri t h m for incremental mining of association rules The 9th International Workshop on Database and Expert Systems Applications p. 240, 1998  10 H  Y a o H  J  H a m ilton, a n d C  J  B u tz   A f ounda tio na l approach to mining itemset utilities from databases The 4th SIAM International Conference on Data Mining pp. 211225, 2004 
307 


Algorithm 1 Pseudocode for the S AMPLE C OUNT phase 1 procedure S AMPLE C OUNT  P s size   P is a stream of pairs each of which has associated a similarity value The length of P is known 2 S out   3 while There are elements in P do 4 S 0   5 S   6 t  0 7 S  the 002rst s 2 elements in P 8 while  t  size 2 000 s 2 do 9 i  the next element in P 10 Choose uniformly at random a number r 2 0   11 if r 024 s  s  2 t  2 then 12 Choose uniformly at random a victim from S and substitute it with i 13 end if 14 t  t  1 15 end while 16 initialize  S 0  S   S 0 is an associative array indexed on the distinct items present in S  initializing it means putting all its entries to 0 17 while  t  size  do 18 i  the next element in P 19 if i 2 S then 20 S 0  i   S 0  i   015 i 21 end if 22 t  t  1 23 end while 24 Choose the s topmost distinct items between S 0 out and S 0  and assign them to S 0 out 25 end while 26 Return S 0 out 27 end procedure IV A NALYSIS Let S i denote the set of transactions containing the element i  This means that S i  S j is the set of transactions containing the pair f i j g  Let S 1 i denote the set of transactions containing i in the current pre\002x of the stream Similarly S k i will denote the set of transactions containing i in C k  the chunk k of the suf\002x of the stream up to the point in which a new current pre\002x changes the counts of items occurrences So S k i  S i  C k De\002nition 5 Given x y 2 R we say that x  016 L  approximates y  written x 016;L  y  if and only if x 025 L implies x 2 1 000 016  y  1  016  y   016 The notation extends in the natural way to approximate inequalities In what follows we will use  016 L  approximations where L  C log mn  for a suitably large constant C depending on the accuracy 016 in Theorem 2 The task is to analyze the accuracy of the new approximation computed when the current pre\002x changes We introduce two random events G OOD P ERMUTATION GP and G OOD B ISAM S AM PLE GBS and bound the probability that they do not happen A permutation of the transactions is called good for f i j g  denoted GP i;j  if and only if the following conditions hold for the current pre\002x 1 j S 1 i j 016;L  j S i j  2 and j S 1 j j 016;L  j S j j  2  2 8 k j S k i  S k j j 016;L  j S i  S j j  2 k  Essentially goodness means that the frequencies of individual items are close in the 002rst and second half of the current pre\002x and the frequency of the pair is evenly spread over the chunks in the second part of the current pre\002x Lemma 6 Given 016 2 0 022 R  we have Pr[GP i;j  025 1 000 6 001 e 000j S i j 016 2 6 Proof An interesting property of the random variables j S 1 i j and j S k i  S k j j is that they are negatively dependent First of all we bound the probability that j S 1 i j is far from j S i  2 j  Using Chernoff bounds we can write Pr j S 1 i j 000 j S i j  2 j 024 016 j S i j   024 2 001 e 000 j S i j 016 2 6 1 
126 
126 


Looking at j S k i  S k j j we can write Pr j S k i  S k j j\000j S i  S j j  2 024 j 024 016 j S i  S j j  2 024  024 2 001 e 000 j S i  S j j 016 2 6 024 2 We use the fact that Chernoff bounds also holds for negatively dependent random variables Since the last bound is the weakest of the three the lemma follows We want GP i;j to hold with probability 1 000 o 1 n 2  whenever items i and j both have support   From Lemma 6 we get that this holds if j S i  S j j  C\024 log n  for some constant C depending on 016  If s  i j   2 024Lf     025 024L then j S i  S j j 025 2 024L  Hence a suf\002cient condition for the similarity is s  i j   024L  3 It remains to understand what is the probability that given a good permutation the pair sampler will take a number of samples for a given pair in each chunk k that leads to a 1 006 016  approximation of s  i j   We denote the latter event by GBS i;j;k  and want to bound the quantity Pr[GBS i;j;k j GP i;j   For this purpose consider the random variable X i;j;k de\002ned as the number of times we sample the pair f i j g in chunk k  Assuming GP i;j we have that over the randomness in the pair sampling algorithm E  X i;j;k  016;L   f  j S 1 i j  j S 1 j j  034 j S i  S j j  2 024  Since the occurrences of f i j g are independently sampled we can apply a Chernoff bound to conclude X i;j;k 016;L  E  X i;j;k   This leads to the conclusion Lemma 7 X i;j;k 016;L   f  j S 1 i j  j S 1 j j  034 j S i  S j j  2 024 016 Suppose that X i;j;k is close to its expectation Then we can use it with 1 006 016  approximations of j S i j and j S j j  to compute a 1 006 O  016  approximation of s  i j   This follows by analysis of the concrete functions f of the measures in Figure 1 A suf\002cient condition on the similarity needed for a 1 006 016  approximation of X i;j;k can be inferred from lemma 7 If s  i j  025 4 024L=\034 then E  X i;j;k  025 s  i j  034  4 024 025 L  So it suf\002ces to enforce s  i j  025 4 024L=\034  4 In order to have O  mb  pairs produced by the pair sampling phase we will choose 034  4 M  The expected number of pair samples from T t is less than j T t j 2 034 f      using that f is decreasing For all measures we consider f     024 1   so j T t j 2 034 f     024 j T t j 2 M 024 j T t j  It remains to understand which is the probability that a pair of items each with support at least   is not sampled by SampleCount Let the random variable X k represent the total number of samples taken in chunk k  The probability that a f i j g is sampled in chunk k is X i;j;k X k  so the probability that it does not get sampled in any evennumbered chunk is Q k 2  024 even  1 000 X i;j;k X k  s  We have seen before that X i;j;k 016;L 025 s  i j  034  4 024  For what concerns X k using a Chernoff bound we can get X k 016;L  E  X k  024 mb=\024  using the linear upper bound on the number of samples So we can compute Y k 2  024 even  1 000 X i;j;k X k  s 024 022 1 000 s  i j  034 024 2 024\015 i;j mb 023 s\024 2 024 022 1 000 s  i j  034 4 mb 023 s\024 2 024 C exp 024 000 s  i j  034 s\024 8 mb 025 In order for this probability to be small enough  O 1 m 2   we need to bound the similarity to s  i j  025 8 mbL s\024\034 5 To choose the best value of 024 we balance constraints 3 and 5 getting 024L   mbL s\024\034  024  r mbM s 6 From which we can deduce s  i j   L  max  r mbM s  M   7 V D ATASET CHARACTERISTICS We have computed for a selection of the datasets hosted on the FIMI web page 1  the ratios between the number of occurrences of single items and pairs in the 002rst half of the transactions and the total number of occurrences of the same items or pairs The values of some of this ratios the most representative are plotted 002gure 3 on the x axis items or pairs are spread evenly after they have been sorted according to their associated ratio The y axis represents the value of the ratios We have taken into account only items and pairs whose support is over 20 occurrences in the whole dataset in order to avoid the noise that could be generated by very rare elements As we can see the number of occurrences and co-occurrences are not so far from what would be expected under a random permutation of the transactions The synthetic data set behaves exactly like we would expect under a random permutation with the ratio being very close to 1  2 for almost all items/pairs This means that even for real data sets where the order of transactions is not random the sampling probabilities used in the pair sampling are reasonably close to the ones that would be obtained under the random permutation assumption VI C ONCLUSIONS We presented the 002rst study concerning the problem of mining similar pairs from a stream of transactions that does rely on the similarity of items and not only on the frequency of pairs A thorough experimental study of carefully engineered versions of the presented algorithm remains to be carried out 1 http://fimi.cs.helsinki.fi 
127 
127 


Figure 3 Plots of the ratios j S 1 i j  j S i j and j S 1 i  S 1 j j  j S i  S j j  R EFERENCES  E Cohen M Datar S Fujiwara A Gionis P Indyk R Motwani J D Ullman and C Yang Finding interesting associations without support pruning IEEE Trans Knowl Data Eng  vol 13 no 1 pp 64–78 2001  Y.-K Lee W.-Y Kim Y D Cai and J Han Comine Ef\002cient mining of correlated patterns in Proc IEEE International Conference on Data Mining ICDM 2003  IEEE Computer Society 2003 pp 581–584  E Omiecinski Alternative interest measures for mining associations in databases IEEE Trans Knowl Data Eng  vol 15 no 1 pp 57–69 2003  J Han and M Kamber Data Mining Concepts and Techniques 2nd edition  Morgan Kaufmann 2006  R Agrawal and R Srikant Fast algorithms for mining association rules in large databases in Proc International Conference On Very Large Data Bases VLDB 1994  Morgan Kaufmann Publishers Inc Sep 1994 pp 487–499  J Han J Pei Y Yin and R Mao Mining frequent patterns without candidate generation A frequent-pattern tree approach Data Min Knowl Discov  vol 8 no 1 pp 53 87 2004  N Jiang and L Gruenwald Research issues in data stream association rule mining SIGMOD Record  vol 35 no 1 pp 14–19 2006  Y Zhu and D Shasha Statstream Statistical monitoring of thousands of data streams in real time Morgan Kaufmann 2002 pp 358–369  G Cormode and S Muthukrishnan What's hot and what's not tracking most frequent items dynamically ACM Trans Database Syst  vol 30 no 1 pp 249–278 2005  E D Demaine A L  opez-Ortiz and J I Munro Frequency estimation of internet packet streams with limited space in Proc 10th Annual European Symposium Algorithms ESA 2002  2002 pp 348–360  J X Yu Z Chong H Lu Z Zhang and A Zhou A false negative approach to mining frequent itemsets from high speed transactional data streams Inf Sci  vol 176 no 14 pp 1986–2015 2006  A Chakrabarti G Cormode and A McGregor Robust lower bounds for communication and stream computation in STOC  C Dwork Ed ACM 2008 pp 641–650  S Guha and A McGregor Stream order and order statistics Quantile estimation in random-order streams SIAM Journal on Computing  vol 38 no 5 pp 2044–2059  N Alon Y Matias and M Szegedy The space complexity of approximating the frequency moments J Comput Syst Sci  vol 58 no 1 pp 137–147 1999  J Misra and D Gries Finding repeated elements Sci Comput Program  vol 2 no 2 pp 143–152 1982  R M Karp S Shenker and C H Papadimitriou A simple algorithm for 002nding frequent elements in streams and bags ACM Trans Database Syst  vol 28 pp 51–55 2003  M Charikar K Chen and M Farach-Colton Finding frequent items in data streams Theor Comput Sci  vol 312 no 1 pp 3–15 2004  A Campagna and R Pagh Finding associations and computing similarity via biased pair sampling in Proc 9th IEEE International Conference on Data Mining ICDM 2009    Finding associations and computing similarity via biased pair sampling Invited for publication in Knowledge an Information Systems  2010  E Kushilevitz and N Nisan Communication complexity  New York Cambridge University Press 1997  J S Vitter Random sampling with a reservoir ACM Trans Math Softw  vol 11 no 1 pp 37–57 1985  D Dubhashi and D Ranjan Balls and bins a study in negative dependence Random Struct Algorithms  vol 13 no 2 pp 99–124 1998 
128 
128 


Application of Chaotic Particle Swarm Optimization Algorithm in Chinese Documents Classification 763 Dekun Tan Qualitative Simulation Based on Ranked Hyperreals 767 Shusaku Tsumoto Association Action Rules and Action Paths Triggered by Meta-actions 772 Angelina A. Tzacheva and Zbigniew W. Ras Research and Prediction on Nonlinear Network Flow of Mobile Short Message Based on Neural Network 777 Nianhong Wan, Jiyi Wang, and Xuerong Wang Pattern Matching with Flexible Wildcards and Recurring Characters 782 Haiping Wang, Fei Xie, Xuegang Hu, Peipei Li, and Xindong Wu Supplier Selection Based on Rough Sets and Analytic Hierarchy Process 787 Lei Wang, Jun Ye, and Tianrui Li The Covering Upper Approximation by Subcovering 791 Shiping Wang, William Zhu, and Peiyong Zhu Stochastic Synchronization of Non-identical Genetic Networks with Time Delay 794 Zhengxia Wang and Guodong Liu An Extensible Workflow Modeling Model Based on Ontology 798 Zhenwu Wang Interval Type-2 Fuzzy PI Controllers: Why They are More Robust 802 Dongrui Wu and Woei Wan Tan Improved K-Modes Clustering Method Based on Chi-square Statistics 808 Runxiu Wu Decision Rule Acquisition Algorithm Based on Association-Characteristic Information Granular Computing 812 JianFeng Xu, Lan Liu, GuangZuo Zheng, and Yao Zhang Constructing a Fast Algorithm for Multi-label Classification with Support Vector Data Description 817 Jianhua Xu Knowledge Operations in Neighborhood System 822 Xibei Yang and Tsau Young Lin An Evaluation Method Based on Combinatorial Judgement Matrix 826 Jun Ye and Lei Wang Generating Algorithm of Approximate Decision Rules and its Applications 830 Wang Yun and Wu-Zhi Qiang Parameter Selection of Support Vector Regression Based on Particle Swarm Optimization 834 Hu Zhang, Min Wang, and Xin-han Huang T-type Pseudo-BCI Algebras and T-type Pseudo-BCI Filters 839 Xiaohong Zhang, Yinfeng Lu, and Xiaoyan Mao A Vehicle License Plate Recognition Method Based on Neural Network 845 Xing-Wang Zhang, Xian-gui Liu, and Jia Zhao Author Index 849 
xiii 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





