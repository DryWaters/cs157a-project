Nonfuzzy Classification using Rules annotated with Weight of Evidence from Statistical Data Raja K Department of Information Technology MIT, Anna University Chennai, India raja_koth@yahoo.co.in Raghavendran N, Vasudha V India Development Center Oracle Corporation Bangalore, India raghavan291@gmail.com, vasu18@gmail.com Rashmi Magdalene J Software Private Limited D.E.Shaw & Co Hyderabad, India rashmi.magdalene@gmail.com Abstract To design and develop a nonfuzzy classification paradigm from a statistical data set. The event association 
patterns of different orders are detected which provides a probabilistic inference mechanism to achieve flexible classification and prediction. To detect significant event associations, residual analysis in statistics is used. Patterns are detected and rules are generated based on the deviations of the observed patterns from a default model. The discriminative power of each rule generated is described using Weight of Evidence \(WOE\statistic. Classification decisions are made using WOE based estimation of the relative likelihoods of each possible labeling. Estimates are calculated by using the set of rules triggered by matching input values. Experimental results are discussed towards the end of the paper Keywords-Maximum marginal entropy; Event generation 
Residual analysis; Pattern discovery; Nonfuzzy I I NTRODUCTION One of the basic tasks of data analysis is to automatically uncover the qualitative and quantitative patterns in a data set When a large database is given, discovering the inherent patterns and regularities becomes a challenging task especially when no domain knowledge is available or when the domain knowledge is too w Because of the size of the dataset, it is almost impossible for a decision maker to manually abstract from it the useful patterns. Hence, it is desirable for automatic pattern discovery tools to support various types of decision-making tasks Consider a set D of M observations or samples obtained 
from a database. Each observation is described by N attributes. Thus, for any distinct attribute, there is a domain of possible values which can be numeric, boolean, and/or symbolic. Here, all the possible values are assumed to be discrete. The purpose of pattern discovery is to find the relations among the attributes and/or among their values Those events whose occurrences are significantly different from those based on a “random” model are extracted. Thus the co-occurrence of events detected may reflect the nature of the data set and provide useful information for future inferences and reasoning To design a pattern discovery system, some specific 
requirements must be addressed The discovery process and the representation of the patterns should be transparent and easily interpretable To account for probabilistic uncertainty in the data the description of patterns should have a probabilistic indication of statistical relevance As real data is often noisy, the discovery system must be able to filter random noise from useful information To alleviate an often-vexatious data-preprocessing step, the discovery process should not be sensitive to monotonic, linear scaling of the data 
In order to meet these objectives, past methods have often sacrificed interpretability. A pattern discovery method is presented that aims at meeting the above challenges under a single framework, while remaining fairly transparent. The discovery method is based on the statistically guided construction of events in the sample space A Theoretical background The fundamental concepts and definitions are presented follows Consider a data set D containing M samples. Every sample is described in terms of N attributes, each of which can assume values in a corresponding discrete finite alphabet 1 Feature 
A feature, X, is a random variable which, in general, may be a nominal, ordinal, or numerical attribute or characteristic of an object or process 2 Event An event is the elemental construct in the present formulation of pattern discovery. An event is just a subspace of the continuous sample space  3 Primary Event A primary event of a random variable X i is a realization of X i which takes on a value ‘a’ from D i   
i a  4 Compound Event A compound event associated with the variable set X s is a set of primary events. A ‘k compound event is made up of k primary events of k distinctive variables. Every sample in the data set is an N compound event 
Third International Conference on Emerging Trends in Engineering and Technology 978-0-7695-4246-1/10 $26.00 © 2010 IEEE DOI 10.1109/ICETET.2010.114 27 
Third International Conference on Emerging Trends in Engineering and Technology 978-0-7695-4246-1/10 $26.00 © 2010 IEEE DOI 10.1109/ICETET.2010.114 27 
Third International Conference on Emerging Trends in Engineering and Technology 978-0-7695-4246-1/10 $26.00 © 2010 IEEE DOI 10.1109/ICETET.2010.114 27 
Third International Conference on Emerging Trends in Engineering and Technology 978-0-7695-4246-1/10 $26.00 © 2010 IEEE DOI 10.1109/ICETET.2010.114 27 


5 Observed Occurrence The observed occurrence is the actual occurrence of the compound event X s in data set D 6 Expected Occurrence The expected occurrence of a compound event X s in data set D is its expected total under the assumption that the variables in X s are mutually independent. The expected occurrence of X s is denoted as E x  7 Pattern If a compound event X sj passes the statistical significance test T then X sj is a significant pattern, or simply, a pattern, of order |s II D ISCRETIZATION BY P ARTITIONING In order to function in a continuous domain the pattern discovery algorithm has been adapted to use discretized data The approach for event construction is to partition the continuous sample space into nonoverlapping sections Figure 1 Observed data events and MME partitioning The following are the reasons for partitioning The continuous sample must be discretized because the pattern discovery cannot work on continuous data values Each event can be described probabilistically i.e. the probability of each event can be estimated A Maximum marginal entropy partitioning The marginal maximum entropy criterion is derived from Information Theory and prescribes a method of partitioning the sample space The su bspace of interest is segm ented in a way, which approximately maximizes the overall entropy H, of the partition. If the subspace is partitioned into J events, then the entropy of the partition is expressed as  J  H =  P j log P j  2  j=1 The approximate nature of the method is due to the fact that partitioning is not performed in the full dimensionality but rather marginally along each dimension 1\Maximum Entropy \(MME To approximately maximize the entropy of a d-dimensional partition maximize the entropy of each one-dimensional marginal Thus discretizatio n is perfor med via independent analysis of the density of feature values along each dimension using marginal maximum entropy partitioning in which the values observed in each input feature is divided among Q quantization bins, such that each bin contains the same number of values as shown in Fig. 1 B Choice of Q value The choice of partition size Q is governed by the tradeoff between maximizing statistical significance of the resulting events while minimizing computational complexity 5 Too low value of Q will cause coarse partition and hence exact specific and true patterns cannot be identified Too high value of Q will cause sparse number of samples in each partition and hence the confidence of the rules identified cannot be asserted Q is chosen such that each d-dimensional event has at least the minimum requisite number of data points for statistical testing C Algorithm For a subspace with as the threshold sample size and vj as the volume of the event in a d-dimensional partition, the simple recursive partitioning with MME criterion is Choose a partition size Q o where Q o 2 Call Partition Q o  Procedure Partition\(S,Q Partition S into Q d events according to the MME criterion For each event E j j=1….Q d If n j   Estimate probability density as p j n j N vj Else Choose a partition size Q j Call Partition\(E j Q j  End if End For End Partition III P ATTERN DISCOVERY An event in a dataset is said to be a statistically significant pattern if there is significant deviation difference between its expected and observed occurrence [6  A positive pattern is one where the observed occurrence is much more than the expected occurrence A negative pattern is one where the observed occurrence is much less than the expected occurrence 
28 
28 
28 
28 


This deviation is measured in terms of residuals Simple residual Standard residual Adjusted residual For an event X i e x indicates the number of occurrences of x l m expected from the observation of a training set of known size under an assumed model of uniform random chance o x is the observed number of occurrences of x l m in a training data set A Simple Residual Simple Residual is just the absolute difference between the expected and observed occurrence s x o x e x 3 But it does not show the actual deviation, as it is not scalable. It depends on the size of the input dataset B Standard residual Standard residual is defined as the ratio of the simple residual and the square root of the expected occurrence. The standardized residual provides a normalization of the simple residual z x o x e x  e x 4 But it does not have a unit standard deviation C Adjusted residual Adjusted residual is defined is the ratio of the standard deviation and its variance  d x z x  v x 5 The benefit of the adjusted residual is that it scales the space of the standardized residual into that of a normal deviate with unit standard deviation. Using the resulting N\(0 1\ we can easily calculate observed deviation from expectati It is important to note that the expectation value expected value cannot fall to zero under the assumed model used here This is because it is equivalent to a linear scale of the number of available training examples, as it is produced by dividing the available training examples among the number of quanta The only way a zero value could therefore be produced is by having no observed values for some feature. If this were to occur \(i.e. if all the values for a given column in the training data were missing\adjusted residual would be undefined however in this pathological case there is no discovery system that would be able to proceed. It suffices to proceed under the assumption that there will be a non-zero number of examples available for every input feature D Pattern discovery by residual analysis To select significant events the adjusted residual is used as it provides an N \(0, 1\distributed z statistic \(i.e., a statistic drawn from a normal distribution with zero mean and unit standard deviation\The value of the adjusted residual defines the relative significance of the associated event. The pattern discovery algorithm determines an event to be significant if the adjusted residual is greater than a threshold value [1 E Logic A compound event is said to be a pattern if the absolute value of the adjusted residual is greater than the threshold value. If the value is positive it is a positive pattern else it is a negative pattern. The testing can be formalized as follows H 0 x is random  H 1 x is significant The decision rule has the following form d x threshold 1 F Selecting the threshold value The threshold value determines the statistical significance the compound event has to pass to be considered as a pattern. So the selection of the threshold H is very crucial. It depends on the confidence sup For a confidence level of 95 percent, H is chosen as 1.96 Figure 2.  Region of significance and threshold determination G Elimination and validation of test cases As a precaution against recording spurious patterns when is low, the pattern discovery classifier implemented will not consider as patterns any events for which the expected number of occurrences is less than a cut off is desired in order to prevent the discovery of high-order patterns caused by the occurrence of only one or two instances of a high-order event \(instances that were possibly generated by chance\. The existence of such patterns may diminish classifier performance, especially when the total number of features, M, is quite large. The use of a spurious pattern would then corrupt the evaluation of any remaining features in the induction of the correct class label value So a cut off is decided. Only when expected occurrence of an event is greater than or equal to the cut off 
29 
29 
29 
29 


the compound event is eligible for residual analysis. Only when e x  The test is valid This has two purposes Elimination of spurious candidate patterns early in the pattern discovery phase Avoiding exhaustive search of all combinations of primary events Usually the value for is taken as any integer greater than 3. For extremely stringent cases a large cut of value can be assumed H Algorithm for pattern discovery by residual analysis Procedure: PatternDiscovery Input: Data Set Output: {Pattern, Label Begin Procedure Let Order=2 While Order<=N, Do For all compound events xi of order, Do Calculate R, adjusted residual If |R|>Threshold Output xi as a rule End if End for Let Order = Order + 1 End while End Procedure I Output of pattern discovery The algorithm takes as input the given dataset and outputs a set of patterns by calculating the residual analysis and comparing with the threshold It outputs a set of patterns with statistical significance If the pattern contains a label it becomes a rule IV W EIGHT OF EVIDENCE In order to measure the discriminative power of a pattern weight of evidence” statistic or WOE is Definition: Let \(Y=y k t the label portion of a given pattern x l m the remaining portion \(consisting of the input feature values\is referred to as x  l The mutual information between these two components can be calculated using 6 A WOE in favor of or against a particular labeling y k Y can be calculated as  7 or 8 WOE thereby provides a measure of how discriminative a pattern x  l is in relation to a label y k and gives us a measure of the relative probability of the co-occurrence of x  l and y k i.e., the “odds” of labeling correctly The domain of WOE values is 1    h ere 1 indicates those patterns \(x  l that never occur in the training data with the specific class label y k 1 indicates patterns which only occur with the specific class label y k These ±1 valued WOE patterns are the most descriptive relationships found in the training data set as any non-infinite WOE indicates a pattern for which conflicting labels have been observed V N ON F UZZY C LASSIFICATION WOE-based support for each y k possible class label evaluated in turn by considering the highest order pattern with the greatest adjusted residual from the set of all patterns occurring in an input data vector to be classified, and accumulating the WOE of this pattern in support of the associated label. All features of the input data vector matching this pattern are then excluded from further consideration for this y k and the next-highest order occurring pattern is consid This continu e s until no  patterns match the remaining input data vector, or all the features in the input data vector are excluded. This independent” method of selecting patterns attempts to accumulate their WOE in way, which estimates the accumulation of the probabilities of independent random variables. Once this is repeated to consider each y k the label with the highest accrued WOE is assumed as the highest likelihood match and this class label is assigned to the input feature vector. The classification is done using independent rule firing A Independent Rule Firing The algorithm proceeds through the following steps 1 Produce a list L of all input variables 2 Repeat steps 3-9 for each classification label 3 Set search order o to be N, the number of inputs to the system 4 Place all rules of order o whose precedents exist in the list L into a list of matches, M. If this search fails, repeat after setting o = o 1 5 If o = 1 has been reached, and no matches have been found, then stop 6 Find the rule R max in M which has the highest adjusted residual 7 Add the WOE of R max to the WOE support of the label 8 Remove from L all the variables matching the precedents of rule R max 9 If L is empty, then stop 10 The class label with the highest accrued WOE is assigned to the input feature vector 
30 
30 
30 
30 


The occurrence of a pattern is assumed to consume the information associated with the primary events describing the pattern. For this reason, each input feature can support at most one rule firing, in order to maintain the assumption of statistical independence of the input featu 16 VI E XPERIMENTAL R ESULTS The pattern discovery and classification modules were successfully implemented. A large sample dataset was used as input and the rules were got as output. The modules were successfully tested on various datasets and results were obtained TABLE I I NPUT DATA SET A training dataset D is used as input to the algorithm 10 D consists of six input features with five attributes Red”, “Blue”, “Green”, “Hue”, “Saturation” and a label “Texture 1500 record sets were considered A Parameters The three important parameters, which had to be decided on, are Q number of quantization bins H threshold level for statistical significance  valid significance test cut-off 1 Q value The value of Q was a tradeoff between the classification precision and the computational complexity Taking all the parameters into consideration, the value of Q was fixed as 5 2 H value The threshold level was chosen to be a standard value of 1.96 which corresponds to 95 percent confidence level which ensures a reasonable assertion to make a strong suggestion in decision support system  3 value The valid significance test cut off was decided based on the size of the database. To eliminate the spurious patterns early in pattern discovery phase, and to avoid exhaustive search of the data set, an optimum value of was chosen as 10 B Quantization The numeric data of all the 5 input features were discretized using maximum marginal entropy partitioning  Each sample space was divided into quantization bins along each margin The value of Q was chosen as 5 The input data was distributed over the 5 quantized bins in such a way that the entropy was maximized The number of sample data in each bin was equal 1 Quantized bins TABLE II Q UANTIZED BINS C Events generated After quantization the input sample space is partitioned into events each of which can be determined probabilistically Primary events are the basic building blocks A :  [a  a3 a4 a 5 B :  [b  b3 b4] [b 5 C :  [c  c3] [c 4] [c5  D  d3] [d 4] [d5 E :  [e1 e2 e3 e 4 e5 The primary events are connected to form higher order events Second order [a1 a2, b 2 Third order [a1, a2, a b1, b2 And so on D Rules generated 303 rules were generated. Each rule mapped an event to a label value and the following are some of the rules generated IF \(Green 0.0...3.55556\\(Hue 0.0...8.22222\THEN Window IF \(Blue 0.0...7.33333\and \(Green 3.55556...16.8889 THEN Cement IF \(Blue 0.0...7.33333\ \(Hue 8.33333...23.6667 THEN Cement IF \(Red 0.0...5.44444\\(Blue 7.33333...20.0 Foliage 
31 
31 
31 
31 


IF \(Red 0.0...5.44444\and \(Green 3.55556...16.8889 THEN Foliage No of rules generated.. 303 E Weight of Evidence Weight of Evidence for each rule is calculated IF \(Green 0.0...3.55556\\(Hue 0.0...8.22222\THEN Window WOE : 1.5279335293281375 IF \(Blue 0.0...7.33333\and \(Green 3.55556...16.8889 THEN Cement WOE : 7.067363981745421 IF \(Blue 0.0...7.33333\ \(Hue 8.33333...23.6667 THEN Cement WOE : Infinity IF \(Red 0.0...5.44444\\(Blue 7.33333...20.0 Foliage WOE : 5.495014176104029 IF \(Red 0.0...5.44444\and \(Green 3.55556...16.8889 THEN Foliage WOE : 4.35485123066857 F Classification Same training data was used as input to the classification algorithm. Of the 1500 input sets, 949 were correctly labeled 26.3333,35.2222,25.6667,35.2222,-2.04915,Brickface Miss : LABEL  :  Window 13.1111,17.8889,21.5556,21.5556,2.69011,Cement Hit : LABEL  :  Cement 0,1.33333,0,1.33333,-2.0944,Foliage Hit : LABEL  :  Foliage 6.55556,6.44444,11.5556,11.5556,2.09315,Cement Hit : LABEL  :  Cement 0.777778,3,0,3,-1.82221,Grass Miss : LABEL  :  Foliage 0.444444,6.44444,1.44444,6.44444,-2.26954,Foliage Miss : LABEL  :  Path 115.111,142.222,121.333,142.222,-2.33375,Path Hit : LABEL  :  Path 51.8889,72.4444,49.6667,72.4444,-1.99159,Brickface Hit : LABEL  :  Brickface 7.44444,7.11111,14.6667,14.6667,2.06945,Cement Hit : LABEL  :  Cement 41,57.2222,39.5556,57.2222,-2.01072,Brickface Hit : LABEL  :  Brickface No of hits : 949 C ONCLUSION Hence a framework for nonfuzzy classification is proposed and proved experimentally. This model identifies statistically significant event associations inherent in a given data set, using residual analysis to test whether or not a pattern candidate is significantly deviated from a default loglinear model. The generated rules annotated with weight of evidence are fired independently to classify the input feature vector In scenarios where labeling requires further statistical augmentation, a deterministic means of confidence could be derived. The confidence measure can be further combined with the label to supplement the decision analogous to multiple votes. The proposed non-fuzzy model leveraging such weighted classifiers increases the accuracy of decisionmaking R EFERENCES  M Hol s heimer and A. Siebes  D ata Mining: The Search for Knowledge in Databases,” Technical Report CS-R9406, CWI Amsterdam,1994  R. Ag rawal S Gliosl i. T. Im ielinski   B. Iy er and A Sw ami, An interval classifier for database mining applications. In VLDB-92, pp 560-573: 2002  D.K  Y. Chiu A.K C Wo ng, and B Cheung I nformation Discov ery through Hierarchical Maximum Entropy Discretization and Synthesis,” Knowledge Discovery in Databases, G. PiatetskyShapiroand W.J. Frawley, eds. AAAI Press/The MIT Press, 1991  D.V. Gokha le, “On Joint a nd C ond itional Entropies,” Entropy, vol 1 no. 2, pp. 21-24, 1999  J Han Y  Cai, and N. Cercone, Data-dr iven discove ry of quantitative rules in relational databases, IEEE Trans. on Knowledge and Data Engineering 5\(1\40. 1993  A.K C. Wo ng and Y Wa ng, “Patte rn Discove ry: A Data Drive n  Approach to Decision Support,” IEEE Trans. Systems, Man, and Cybernetics C, vol. 33, no. 1, pp. 114-124, Feb. 2003  A. Hamilton-W right and D.W. Stashuk Com paring Pattern  Discovery and Back-Propagation Classifiers,” Proc. Int’l Joint Conf Neural Networks \(IJCNN  S.J Haber m an, “T he Analy s is of Residuals in Cross-Classif ied Tables,” Biometrics, vol. 29, no. 1, pp. 205-220, Mar. 1973  Estimation of Normal parameters http://www.weibull.com/LifeDataWeb/estimation_of_the_parameters htm#probability_plotting  Datasets h ttp://w ww cs.waikato.ac.nz ml/weka  Wang Y. and A. K C Wo ng Fr om association to classific ation Inference using weight of evidence. IEEE Transactions on Knowledge & Data Engineering, 15\(3\764–767, May-June 2003  Duda, R O P. E. Hart and D. G Stork. Pattern Classifi cation John Wiley & Sons, 2nd edition, 2001.ISBN 0-471-05669-3  Becker P. W Recognition of Patterns Using the Fr equencies of Occurrence of Binary Words. SpringerVerlag, 2nd edition, 1968  Chan K. C C. and D. K. Y Wo ng Andre w K. C Chiu. L e arning sequential patterns from probabilistic inductive prediction. IEEE Transactions Systems, Man, Cybernetics, 24\(10\532–1547, October 1994  Levitin, G Evaluating corre ct classificatio n probability for we ighted voting classifiers with plurality voting. European Journal of Operations Research, 141:596–607, 2002  Guro v, S I Reliabi lity estimation of classification algor ithms Introduction to the problem point frequency estimates. Computation Mathematics and Modeling, 15\(4\5–376, 2004  To m Chau M arginal Maxim u m Ent ropy Partiti oni ng Yields Asymptotically Consistent Probability Density Functions,” IEEE transactions On Pattern Analysis And Machine Intelligence, Vol.23 No.4, April 2001 
32 
32 
32 
32 


7 ForestCoverType  data is shown in Figure 6 Appendix as an example The tool takes this data as input and produces the hierarchical clusters of data by setting different clustering parameters With the help of this tool user can see  the generated hierarchy of the clusters along with the data present in each cluster. For the experiment we sele cted the row wise clustering option and the algorithm used f or this type of clustering was the Euclidean algorithm. The reason for this choice is that Euclidean distance is the m ost commonly used type of distance measure in cluster analysis It uses raw data instead of standardized data to compute the distances Using the complete linkage method the hierarchal clusters are produced, it can be seen from the Figu re 2 that 3 distinct clusters are produced where each one of them is having its specific hierarchy B Automatic Schema generation In this step the cluster results file which was ex ported using the HCE tool becomes the input of the automat ic schema builder. We developed a prototype to automat e the process of schema generation The prototype takes t he clustered data and generates the schema of any part icular type such as star snowflake or galaxy/constellatio n The prototype has been developed using the C sharp C  programming language in Microsoft Visual Studio.net 2005 Once the hierarchical clustering has been done, the  next step is to extract this hierarchical information in the form of cluster information tables which are fed into the d eveloped prototype called Automatic Schema Generator Figure  3 depicts the main interface of the Automatic Schema Generator having the clustered relationship data C luster names are the abbreviations which are formed using the first letters of each cover type for example clus ter name for C1 is PCD, which means cover types of Ponderosa pine Cottonwood and Douglas-fir PCD All other names are abbreviated by using the first alphabetical letters  After reading the cluster relationship file the bu tton of Create Schema as shown in the following diagram displays the schema selection window. Figure 4 show s the schema selection window of automatic schema builder  Using this selector window cluster name and type o f schema can be selected to build database in the dat abase server. User can select from star, snowflake and ga laxy or constellation options of schema type using the radi o button The “Generate” button on the bottom of schema selec tor window performs the following major functions 1\Generates automatic database in the database serv er 2\Creates dimension and fact tables for the schema type selected 3\Manage relationships among the tables 4\Upload data in the fact and dimension tables from the clustered file After performing the above mentioned functions the  control is passed to the schema visualization windo w which gives a glimpse of the schema created automatically  as shown in Figure 5  Fig 2: Hierarchical clusters generated by HCE tool Fig 3 User interface of automatic schema builder Fig 4 Schema Selector window 41 


Fig 5 Schema visualization window V CONCLUSION AND FUTURE WORK In this paper we reviewed the literature regarding  the integration of OLAP with data mining and automatic generation of OLAP schema. Literature review reveal ed the fact that none of the previous works targeted at th e automatic schema generation from the mined data set  Furthermore the works in the past pose a number of limitations. The major limitation of the previous w ork was the absence of a model that can produce the three b asic types of OLAP schema star, snowflake and galaxy Based on these observations we presented the model for th e integration of data mining and OLAP along with the automatic generation of OLAP schema. We have develop ed a prototype for the automatic schema generation Th is developed prototype takes mined data and produces t he schema of user’s choice Finally we implemented the  proposed model and evaluated the results with the h elp of experiment It is evident from the result that the prototype system overcomes the manual schema design and implementation requirement in the data warehousing environment We are working on the enhancement of t he proposed model for automatic schema generation One  possible way of enhancement is the use of other dat a mining techniques along with OLAP for the schema generation. Further more, we are exploring how OLAP can be further extended and enhanced to meet the new challenges and to make it more effective efficient  and intelligent OLAP REFERENCES 1  S   C h a u d h u r i  a n d  U   D a y a l   A n  o v e r v i e w  o f  d a t a  warehousing and OLAP technology ACM SIGMOD Record  Vol 26 1997\, pp 6574 2  A   C u z z o c r e a   D   S a c c a  a n d  P   S e r a f i n o   A  h i e r a rchy driven compression technique for advanced OLAP visualizati on of multidimensional data cubes in Proc of 8th Int’l Conf on Data Warehousing and Knowledge Discovery \(DaWak Springer Verlag 2006\, pp. 106-119 3  S   M a n s m a n n  a n d  M   S c h o l l   E x p l o r i n g  O L A P  a g g r e gates with hierarchical visualization techniques, in Proc. of ACM Symposium on Applied Computing 2007\, pp. 1067-1073 4  F a y y a d  U  M   P i a t e s k y S h a p i n o  G    S m y t h  P   a n d  U thurusany R From datamining to knowledge discovery: An overvie w,”in Proc. of Advances in data mining and knowledge discovery MIT Press, pp. 134 5   S   G o i l  a n d  A   C h o u d h a r y    H i g h  p e r f o r m a n c e  O L AP and data mining on parallel computers Data Mining and Knowledge Discovery vol. 1, no. 4, pp. 391-417, Dec. 1997 6   S   A s g h a r   D   A l a h a k o o n  a n d  A   H s u    E n h a n c i n g  OLAP functionality using selforganizing neural networks  Neural, Parallel and Scientific Computations vol. 12, no. 1, pp. 1-20, March 2004 7   R   B   M e s s a o u d   O   B o u s s a i d  a n d  S   R a b a s e d a    A new OLAP aggregation based on the AHC technique,” in Proc. of the 7th ACM Int’l Workshop on Data Warehousing and OLAP DOLAP ACM New York, 2004, pp. 65-72 8   J   H a n    T o w a r d s  o n l i n e  a n a l y t i c a l  m i n i n g  i n  l arge databases ACM SIGMOD Record vol. 27, no. 1, pp. 97-107, March 1998 9   V   M a r k l   F   R a m a s a k  a n d  R   B a y e r    I m p r o v i n g  OLAP performance by multidimensional hierarchical clustering in Proc of the 1999 Int’l Symposium on Database Engineering and Applica tions IDEAS 1999, p. 165 10   V   M a r k l  a n d  R   B a y e r    P r o c e s s i n g  r e l a t i o n a l  OLAP queries with UB-trees and multidimensional hierarchical clusteri ng in Proc of the Int’l. Workshop on Design and Management of Dat a Warehouses DMDW\, 2000, pp. 1-10 11   N   K a r a y a n n i d i s   T   S e l l i s  a n d  Y   K o u v a r a s    CUBE file A file structure for hierarchically clustered OLAP cubes  Advances in Database Technology LNCS Springer Verlag Berlin-Heidelberg pp. 621-638, 2004 12   D   T h e o d o r a t o s  a n d  A   T s o i s    H e u r i s t i c  o p t i m ization of OLAP queries in multidimensionally hierarchically cluste red databases,” in Proc of the 4th ACM Int’l Workshop on Data Warehou sing and OLAP DOLAP\, ACM New York, 2001, pp. 48-55 13   K   H a n n   C   S a p i a  a n d  M   B a l a s c h k a    A u t o m a t i cally generating OLAP schemata from conceptual graphical models,” in  Proc. of the 3rd ACM Int’l Workshop on Data Warehousing and OLAP  DOLAP ACM New York, 2000, pp. 9-16 14   V   P e r a l t a   A   M a r o t t a  a n d  R   R u g g i a    T o w a r d s the automation of data warehouse design Technical Report TR-03-09 InCo Universidad de la República, Montevideo, Uruguay, J une 2003 15   N   T r y f o n a   F   B u s b o r g  a n d  J   G   B   C h r i s t i a n sen StarER A conceptual model for data warehouse design,” Proc of the 2nd ACM Int’l. Workshop on Dataarehousing and OLAP \(DOLAP ACM New York, 1999, pp. 3-8 16   Y   S o n g   e t   a l     S A M S T A R   A n  a u t o m a t i c  t o o l for generating star schemas from an entity-relationship diagram in Proc of the 27th Int’l. Conf. on Conceptual Modeling LNCS 2008, pp. 522-523 17   S   R   M a d d i  a n d  V   K h a n    C o m p a r a t i v e  a n a l y s i s of on-line analytical processing tools,” University essay from IT-uni versitetet I Göteborg, Sweden 2007 18   H   Z h u    O n l i n e  a n a l y t i c a l  m i n i n g  o f  a s s o c i a t ion rules Master Thesis, Simon Fraser University, 1998, pp. 1-117 19   J   F o n g   H   K   W o n g  a n d  A   F o n g    O n l i n e  a n a l ytical mining Webpages tick sequences J. of Data Warehousing vol. 5, no. 4, pp. 5967, 2000 20   S   D z e r o s k i   D   H r i s t o v s k i  a n d  B   P e t e r l i n    Using data mining and OLAP to discover patterns in a database of patients  with Y chromosome deletions,” in Proc. AMIA Symp 2000, pp. 215–219 21  F   D e h n e   T   E a v i s  a n d  A   R a u C h a p l i n    C o a r s e grained parallel online analytical processing OLAP for data mining in Proc of the Int’l Conf. on Computational Science ICCS\, 2001, 589-598 22   J   H a n   S   H   S   C h e e  a n d  J   Y   C h i a n g    I s s u es for on-line analytical mining of data warehouses,” in Proc. of the SIGMOND Workshop on Research Issues on Data Mining and Knowledge Discov ery DMKD\, Seattle, 1998, pp. 2:1-2:5 23   J   A   B l a c k a r d   D   J   D e a n  a n d  C   W   A n d e r s o n  Forest cover type The UCI KDD Archive http://kdd.ics.uci.edu   I r v i n e CA University of California Department of Information  and Computer Science \(1998 24   J   S e o   e t   a l     I n t e r a c t i v e  c o l o r  m o s a i c  a n d dendrogram displays for signal/noise optimization in microarray data analys is,” in Proc. of the Intl. Conf. on Multimedia and Expo-Volume 3 2003, pp. 461-464 42 


9 Appendix Fig 6: Forest Cover Types of the U.S. \(Source. USGS National Atlas of US Summary of Forest Cover Type Data Type Multivariate Abstract The forest cover type for 30 x 30 meter cells obtai ned from US Forest Service \(USFS\ Region 2 Resource Information System RIS\ data Data Characteristics The actual forest cover type for a given observatio n \(30 x 30 meter cell\ was determined from US Fores t Service \(USFS\ Region 2 Resource Information System RIS data Independe nt variables were derived from data originally obta ined from US Geological Survey \(USGS\ and USFS data. Data is in raw form \(not scaled\ and contains binary \(0 or 1 columns of data for qualitative independent variables \(wilderness areas and soil types Summary Statistics Number of instances observations 581012 Number of Attributes 54 Attribute breakdown 12 measures, but 54 columns of data \(10 quantitativ e variables, 4 binary wilderness areas and 40 binary soil type variables Missing Attribute Values None 43 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





