Associative classification over Data Streams   Zhen-Hui Song Department of Computer ShiJiaZhuang Vocational Technology Institute ShiJiaZhuang China E-mail: songzhenhui_sjz@163.com Yi Li Department of electrical and electronic engineering ShiJiaZhuang Vocational Technology Institute ShiJiaZhuang China E-mail: liyi_sjz@126.com   Abstract 227Based on association rules, Associative classification AC\as shown great promise over many other classification techniques on static dataset. However, the increasing prominence of data streams arising in a wide range of advanced application has posed a new challenge for it. This paper describes and 
evaluates AC-DS, a new associative classification algorithm for data streams which is based on the estimation mechanism of the Lossy Counting \(LC\ and landmark window model. We apply AC-DS to mining several datasets obtained from the UCI Machine Learning Repository and the result show that the algorithm is effective and efficient Keywords--data streams; associative classification; frequent itemsets I   I NTRODUCTION  Data-steam mining is a technique which can find valuable information or knowledge from a great deal of primitive data Unlike mining static databases, mining data streams poses many new challenges [1 First, each data element should be examined at most once 
It is unrealistic to keep the entire stream in the main memory Second, each data element in data streams should be processed as fast as possible. Third, the memory usage for mining data streams should be bounded even though new data elements are continuously generated. Finally, the results generated by the online algorithms should be instantly available when user requested A  Data Stream Let I={i 1 i 2 205,i m  be a set of literals, called items. An itemset is a subset of I An itemset consisting of m items is called a m-itemset Let us assume that the items in an itemset 
are in lexicographical order.  A transaction is a tuple tid itemset where tid is the ID of the transaction A transaction data stream DS={B 1 B 2 205 , B N 205 be an infinite sequence of blocks, where each block is associated with a block identifier n, and N is the identifier of the latest block B N  Each block Bi consists of a set of transactions, that is Bi=[T 1 T 2 205, T k  where k>0. Hence, the current length of the 
data stream is defined as CL=|B 1 B 2 205|B N   The frequency of an itemset X denoted as f req\(X is the number of transactions in B that support X. The support of X is defined as fre\(X\/N where N is the total number of transactions received X is a Frequent Itemset \(FI\ in B if sup\(X  minSupport where minSupport \(0<=minSupport<=1  is a user defined 
minimum support threshold Many algorithms have been proposed for mining data stream. According to the processing model on the data stream we can classify the research work into three fields: landmark windows, sliding windows and damped windows. Manku and Motwani  pr opo sed  a si ngl e-p a ss al gor i t hm  Lo ssy Co unt i ng to mining frequent itemsets, and the algorithm is based on a well known Apriori-property. Yu et al  pr opo sed a algorithm, FDPM, which is derived from the Chernoff bound to approximate a set of FIs over a landmark window. Li et al.[4  propose a single-pass algorithm, DSM-FI, to mine all frequent itemsets over the entire history of data streams. Pedro Domingos and Geoff Hulten e s c r i be d a nd e va l ua t e d a n  algorithm, VFDT, to predict the labels of the records it 
received B  Associative Classification Let D be the dataset. Let I be the set of all items in D and C be the set of class labels. We say that a data case d  contains X 
002 I, a subset of items, if X 002 di. A class association rule \(CAR\ an implication of the form  where X 002 I and   Bing Liu et al fi r s t  pr opose d  t he A C a ppro a c h  na m e d classification based on association algorithm \(CBA\ for building a classifier based on the set of discovered class association rules. The difference between rule discovery in AC and conventional frequent itemset mining is that the former 
task may carry out multiple frequent itemset mining processes for mining rules of different classes simultaneously. Data mining in associative classification \(AC\ framework usually consists of two steps 1  Generating all the class association rules \(CARs which has the form of iset c, where iset is an itemset and c is a class 2  Building a classifier based on the generated CARs Generally, a subset of the association rules was selected to form a classifier and AC approaches are based on the confidence measure to select rules [7   978-1-4244-7941-2/10/$26.00 \2512010 IEEE 


Input: DS---a data stream in which each record has N items S window the window size, S window Bi minSupport--- Suppor t threshold Output: M---a classifier with a lot of association rules whose confidence are great than 50% and support value great than minSupport Method Initial the rule memory  M 000ž\000\003 Do Read in a data block Bi={T1, T2, …, Tk m=0; A m  000ž\000\003\000\003\000\003\000\003\000\003\000\003\000\003\000\003\000\003 Clear the set of candidate itemsets A A m+1 Gen\(Bi, A m Generate n candidate frequent itemsets  Itemset 1 Itemset 2 Itemset n  each Itemset i in A has 1 items While A 001.\000  For  i=1 to n S=Supp Itemset i Calculate the support of Itemset i  If  S>= minSupport  then M 001\025 M Itemset i Put the itemset into memroy   Endif Endfor m=m+1 A m+1 Gen\(Bi, A m Generate the \(m+1\ generation A Endwhile M=Rank\(M\                   //Rank rules by their confidence values M=Decay\(M\                  //Decay the rules in memory While Most of the algorithms shown above were used for finding frequent itemsets. Some of them were used to classify the data steams with a decision tree. In this paper, we present an approach to mine class association rules and then to make a classifier. Classifying a data stream with an association classifier is a newly explored area of research, which may be viewed as a further extension to the earlier work on mining frequent itemsets over data stream II  ASSOCIATIVE CLASSIFICATION ON DATA STREAMS  A  Problem Definitions A data stream is a massive unbounded sequence of data elements continuously generated at a rapid rate. Due to the unique characteristics of streaming data, most one-pass algorithms have to sacrifice the correctness of their analysis results by allowing some errors. Hence, the True support of an intemset X denoted by Tsup\(X is the number of transactions seen so far in which that itemsets occurs as a subset. The estimated support of the itemset X, denoted as Esup\(X is the estimated support of X stored in the summary data structure constructed by the one-scan approaches, where Esup\(X\<=Tsup\(X An itemset X is called a frequent itemset if Tsup\(X\=Minsupport*CL Hence, given a user-defined minimum support threshold minSupport \(0<= minSupport <=1 and a data stream DS, our goal is to develop a single-pass algorithm to classify the streaming data in the landmark windows model using as little main memory as possible Figure 1  The proposed associative classification algorithm B  Associative Classification Algorithm for Data Stream We can see the completed algorithm from Fig.1. Our algorithm accepts two user-specified parameters: a support threshold minSupport and a window size Swindow=| Bi |. Let N denote the current length of the stream, i.e., the number of records seen so far. Every time received a record, our algorithm can forecast its class label based on the association rules extracted from the records before. Each of the rules has a estimated support Esup\(X\, whose value is great than minSupport For a given data block B i the first pass of the algorithm counts item occurrences to determine the frequent 1-itemsets As long as the set of 1-itemsets was not empty, the algorithm subsequently carry out the next pass to find frequent \(m+1\itemset. When the algorithm obtains all of the frequent itemsets it will calculate the confidence of the rules and sort them in the memory. Then, if there is a request of classification, the classifier will predict the class label of a record. At the bound of block B i the memory rules will be pruned and the rules with low support value will be deleted C  Funtions and Data Structure 1  Data struture M The data structure M is a set of entries of the form \(itemset fclass\(1\, fclass\(2\,…, fclass\(i\, t\, where itemset is a subset of conditional attributes, fclass\(i\ is an interger representing the approximate frequency of class attributes i, and t is the number of the data blocks in which the itemset appeared firstly Initially, M is empty. Whenever a new association rule \(itemset class\(i\\ arrives, we examine M to see whether an entry mj already exists or not. If exists, we update the entry by incrementing its corresponding frequency fclass\(i\ by one Otherwise, we create a new entry of the form \(set, fclass\(1 fclass\(2\,…, fclass\(i\ t\.  The parameter t is the number of the data block and fclass\(x\s the frequency of class x f class\(x 000\014 0 \(class\(x 001 class\(i f class\(x 000\014 1 \(class\(x\ =  class\(i\                          \(1 2  Function Gen\(B x A m  The Gen\(Bx, Am\ function takes as argument Am, the set of all frequent m-itemsets. It returns a superset of the set of all m+1\itemsets. The function works as follows [8  a  Step 1, we join A m with A m  Insert into A m+1  Select p.item 1 p.item 2 p.item m q.item m  From A m  p A m  q  Where p.item 1 q.item 1 p.item m-1 q.item m-1  p.item m q.item m  b  Step 2, we delete all itemsets a 001\031 A m+1 such that some m-subset of a is not in A m  Forall  itemsets 000D\001\031 A m+1 do Forall   m-subsets s of a do If   NOT \(s 001\031 A m en Delete a from A m+1  3  Function Supp\(Itemset x  The function Supp\(Itemsetx\as used to calculate the support value of Itemsetx. The value of the function was calculated like this Supp\(Itemset x MAX\(f class\(1 f class\(2 f class\(i  2  Where N is the number of current data block 


4  Function Rank\(M The function Rank\(M was used to sort the rules in M with their confidence values. The confidence value of a entry in M was calculated like this Confidence\(mi\\(f class\(1 f class\(2 f class\(i  f class\(1 f class\(2 f class\(i  3 5  Function Decay\(M The function Decay \(M was used to delete some entries at the boundary of a data block. If the expression below is true the entry will be deleted from the memory M. N is the number of current data block  MAX\(f class\(1 f class\(2 f class\(i minSupport*\(t-1\ *CL  MIN S UPPORT N*CL   4 III  EXPERIMENTS In this section, we provided a formal model of associative classification and further examine how such algorithm can be applied in data stream mining. We have conducted an experiment on a 3.0 GHz Pentium PC with 512MB of memory running with Microsoft Windows XP to measure the performance of the proposed approach. The datasets used in the experiment are obtained from the UCI Maching Learing Repository   In order to compare our algorithm with other classification algorithms and make the evaluation more credible and reliable we choose some large datasets from UCI. The Entropy method was used in the progress of discretization of continuous attributes  A  Effects of Parameters We investigated the effects of the different block sizes on the effectiveness and efficiency of the algorithm and found that the accuracy of the algorithm was not affected obviously by it But the number of the rules found and the time cost were affected by it largely  Figure 2  The accuracy of the classification algorithm and the number of the rules found As can be seen in Fig.2, when the initial data block size was set to 1000, 2000, 3000, 4000 and 6000, and minimum support threshold was set to 10%, we got the similar forecast results.  However, the numbers of the found rules are very different. As the block size getting larger, the number of the found rules became smaller This is because that, although we delete some entries at the boundary of a data block, there still are some un-frequent rules in the memory and they will be deleted in the later decay step The large the data block size is, the less the number of unfrequent rules exists   Figure 3  The run time of the classification algorithm We further investigated the effects of different block sizes on the time cost by the algorithm and found that with the same dataset, as the increasing of the block size, the total run time becomes more and more long. With the different dataset, the run time was decided mainly by the number of attributes and the number of the items of the dataset.  We can see this from Fig.3, the numbers of the attributes of the four datasets Nursery, Krkopt, Poke and Adult, are 8, 6, 10 and 14. And the numbers of the items of them are 27, 40, 85 and 127. Although the support thresholds and the block sizes of them are the same the run time of them are so different and it increased obviously with the increasing of the item and attribute  Figure 4  The effects of different support thresholds When the block size was set to 2000, we tested the performance of the algorithm with the different support thresholds. As it can be seen from Fig.4, with the decreasing of the support threshold value, the more accurate classification result we can get. But, at the same time, more classification rules will appear in our classifier, it means more run time B  Comparision with different algorithms We implemented the CBA algorithm [8 an d A C D S algorithm for comparison. In the implementation of the CBA algorithm, the minimum confidence was set to 50%. In the implementation of the AC-DS algorithm, the confidence threshold was set to 50% too, and the data block size was set to 


1000. The support thresholds of the two algorithms were shown in the Table.1 Column 1: It lists the names of the 6 datasets Column 2: It lists the number of attributes in the dataset Column 3: It lists the number of classes in the dataset Column 4: It shows the support threshold used in the algorithm Column 5: It shows the classification accuracy of the algorithm CBA Column 6: It shows the number of the records which had to been read in the memory by CBA Column 7: It shows the classification accuracy of the algorithm AC-DS Column 8: It shows the number of records in a data block which was read in the memory by AC-DS We test 6 datasets come from UCI Machine Learning Repository. As can be seen from table.1, the mean accuracy of the two algorithms is similar, but the memory used by CBA is obviously greater than AC-DS. Since CBA is a classification algorithm for static dataset and AC-DS is a classification algorithm for mining data streams, the more total time cost by AC-DS than CBA should be think acceptable as long as the two accuracy rates of them are similar and the memory cost by AC-DS is in a special limit TABLE I   C OMPARISION WITH DIFFERENT ALGORITHM  Dataset #attr#class#supp CBA AC-DS accu#mem#accu#mem Adult 14 2 10 83.7% 48842 82.1% 1000 Adult 14 2 5 83.9% 48842 81.9% 1000 Adult 14 2 1 85.3% 48842 83.7% 1000 Digit 16 10 10 78.7% 10992 75.6% 1000 Digit 16 10 5 86.0% 10992 83.4% 1000 Digit 16 10 1 87.2% 10992 86.0% 1000 Letter 16 26 10 59.2% 20000 62.8% 1000 Letter 16 26 5 69.0% 20000 67.4% 1000 Letter 16 26 1 7 0.1% 20000 6 8.6% 1000 Mushroom 22 2 20 94.0% 8124 91.4% 1000 Mushroom 22 2 10 96.4% 8124 93.2% 1000 Mushroom 22 2 2 96.6% 8124 94.6% 1000 N urser y  8 5 10 89.9% 12960 90.4% 1000 N urser y  8 5 5 91.5% 12960 89.5% 1000 N urser y  8 5 1 96.1% 12960 93.8% 1000 Krkopt 6 18 10 89.2% 28056 87.4% 1000 Krkopt 6 18 5 94.2% 28056 91.8% 1000 Krkopt 6 18 1 97.9% 28056 97.1% 1000  IV  DISCUSSIONS AND CONCLUSION  Since the transactions were not processed one by one, we always try to read in the available main memory as many transactions as possible. So, we always select a large data block size to process. When the support threshold is fixed, the more the data block size is, the less combinatorial explosion of itemsets takes place One important fact we should notice is that our algorithm was designed to deal with dataset in which the all data was generated by a single concept. If the concept function is not a stationary one, in other words, a concept drift takes place in it our algorithm will not output an accurate result This paper described an associative classification approach based on association rules for mining data streams. Empirical studies show its effectiveness in taking advantage of massive numbers of examples. AC-DS’s application to a high-speed stream is under way R EFERENCES   1  R. Agrawal, T. Imielinski and A. Swami. “Mining association rules between sets of items in large databases”. In Proc. of the ACM SIGMOD Conference on Management of Data, Washington, D.C, May 1993 2  Gurmeet Singh Manku and Rajeev Movtwani, “Approximate Frequency Counts over Data Streams”. Proceedings of the 28 th VLDB conference Hong Kong, China, 2002 3  Yu J, Chong Z, Lu H et al. “False positive or false negative: ming frequent itemsets from high speed transactional data streams. In Nascimento et al.\(eds\ Proceedings of the thirtieth international conference on very large data bases”, Toronto, Canada, September 3August 31, 2004, pp 204-215 4  Li H, Lee S, Shan M. “An efficient algorithm for mining frequent itemsets over the entire history of data streams”. Proceedings of the first international workshop on konwledge discovery in data streams, Pisa Italy, 2004 5  Pedro Domingos and Geoff Hulten. “Mining high-speed data streams”,In Proceedings of the sixth ACM SIGKDD international conference on knowledg discovery and data ming, pape 71-80, Boston MA, 2000. ACM Press 6  B. Liu, W. Hsu, and Y. Ma. “Integrating classification and association rule mining”. In KDD 98, New York, NY, Aug.1998 7  B. Liu, Y. Ma, and C.-K. Wong, “Improving an association rule based classifier,” in Proc.4th Eur. Conf. Principles Practice Knowledge Discovery Databases\(PKDD-2000\,2000 8  R. Agrawal and R.Srikant, “Fast algorithms for mining association rules”. In Proc. 20th Int. Conf. Very Large Data Bases\(VLDB\ 1994 pp.1-12 9  D. J. Newman, S. Hettich, C. Blake, and C. Merz, “UCI Repository of Machine Learning Databases”. Berleley, CA: Dept. Information Comput. Sci., University of California,1998   R. Kohavi, D. Sommerfield, and J. Dougherty, “MLC++: A machine learning library in C++,” in Proc.6th Int. Conf. Tools Artificial Intelligence, New Orleans, LA, 1994, pp.740–743  


        


3462                                                                                                             


A Model of Cluster-Based Rough Set and its Application 562 Yan Xu A New Kind of the Generalized R-implication on Interval-set 568 Zhan-ao Xue, Hao-cui Du, Hao-zhe Yin, and Yun-hua Xiao A Nonlinear Multiregression Model Based on the Choquet Integral with a Quadratic Core 574 Nian Yan, Zhengxin Chen, Yong Shi, and Zhenyuan Wang Predicting Protein Secondary Structure Based on Compound Pyramid Model 580 Bingru Yang, Lijun Wang, Wu Qu, and Yun Zhai The Application of ARIMA Model in Chinese Mobile User Prediction 586 Xu Ye Efficient Parallel Algorithm for Nonlinear Dimensionality Reduction on GPU 592 Tsung Tai Yeh, Tseng-Yi Chen, Yen-Chiu Chen, and Wei-Kuan Shih A Clustering Method Based on the Most Similar Relation Diagram of Datasets 598 Yan Yu, Yu Shan Bai, Wei Hong Xu, and Nan Li Knowledge Reduction Algorithm Based on Relative Conditional Partition Granularity 604 Jingling Yuan, Hongfu Du, and Luo Zhong Learning Multiple Latent Variables with Self-Organizing Maps 609 Lili Zhang and Erzsébet Merényi Implicative Pseudo-BCK Algebras and Implicative Pseudo-Filters of Pseudo-BCK Algebras 615 Xiaohong Zhang and Hongjuan Gong From Tinnitus Data to Action Rules and Tinnitus Treatment 620 Xin Zhang, Zbigniew W. Ras, Pawel J. Jastreboff, and Pamela L. Thompson A Novel Two Sub-swarms Exchange Particle Swarm Optimization Based on Multi-phases 626 Jia Zhao, Li Lü, Hui Sun, and Xing-wang Zhang Assessment of Emergency Response Policy Based on Markov Process 630 Yafei Zhou, Mao Liu, and Liming Hu Probabilistic Soft Sets 635 Ping Zhu and Qiaoyan Wen A Double-Window-Based Classification Algorithm for Concept Drifting Data Streams 639 Qun Zhu, Xuegang Hu, Yuhong Zhang, Peipei Li, and Xindong Wu Application of Covering Rough Sets to Linguistic Dynamic Systems 645 William Zhu Short Papers Solving Course Timetabling Problem Using Interrelated Approach 651 Aftab Ahmed and Li Zhoujun Application of Cascading Rough Set-Based Classifiers on Authorship Attribution 656 Oguz Aslanturk, Ebru A. Sezer, Hayri Sever, and Vijay Raghavan Triadic Concept Analysis of Data with Fuzzy Attributes 661 Radim Belohlavek and Petr Osicka Extension of Rough Set Theory Based on Strict Similarity Relation 666 Meng Chen and Xin Xia A Unified Paradigm for the Accuracy of Classification Based on Granular Computing 669 Yongbing Chen, Shuang Liu, and Ping Ye Matrix Theory for Tolerance Granular Computing 673 Zehua Chen and T.Y. Lin 
xi 


Analysis to the Contributions from Feature Attributes in Nonlinear Classification Based on the Choquet Integral 677 Jing Chu, Zhenyuan Wang, and Yong Shi A New Criterion of Delay-Range-Dependent Stability for Systems with Time-Varying Delay 683 Songjian Dan and Haixia Wu An Enhanced P4P-Based Pastry Routing Algorithm for P2P Network 687 Zhengwei Guo, Lin Min, Shuai Yang, and Huaipo Yang Attack Detection by Rough Set Theory in Recommendation System 692 Famei He, Xuren Wang, and Baoxu Liu Rough Sets in Partially Ordered Sets 696 Kai Li, William Zhu, and Jianguo Tang An Algorithm for Attribute Reduction Based on Distance of Partitions 700 Min Li, Shaobo Deng, Feng Shengzhong, and Jian-Ping Fan Knowledge Reduction of Incomplete Information Systems with Negative Decision Rules 704 Tong-Jun Li and Wei-Zhi Wu Using Feature Component for Spatial Data Modeling of GIS 708 Wei Li, Shuai Gong, and Xiaohui Wang Mining Algorithm of Maximal Frequent Itemsets Based on Position Lattice 712 Yuan Li, Jun Li, Ning An, and Chong Han Rough Lattices 716 Zuhua Liao, Lian Wu, and Miaohan Hu The Classification Study of Texture Image Based on the Rough Set Theory 720 Huang Lin, Ji-yi Wang, and Shuang Liu Type-2 Fuzzy Logic Controllers Optimization Using Genetic Algoritms and Particle Swarm Optimization 724 Ricardo Martinez, Antonio Rodriguez, Oscar Castillo, and Luis T. Aguilar Interval Type-2 Fuzzy Logic Applications in Image Processing and Pattern Recognition 728 Patricia Melin The Close-Degree of Concept Lattice and Attribute Reduction Algorithm Based on It 732 Huili Meng and Jiucheng Xu Isomorphism Between Information Tables for Tolerance and Classical Rough Sets 735 Jun Meng, Tsauyoung Li, Zehua Chen, Xiukun Wang, and Peng Wang Assignment Reduct and its Calculation Method in Inconsistent Incomplete Decision System 739 Zuqiang Meng, Zhongzhi Shi, and Hua Qin Generalized Relation Based Knowledge Discovery in Interval-valued Information Systems 743 Duoqian Miao, Wei Yang, and Nan Zhang Classification and Discrimination Based on RS-PLS-DA 749 Bin Nie, Jianqiang Du, Riyue Yu, Hongning Liu, Guoliang Xu, and Zhuo Wang Research of Support Vector Regression Algorithm Based on Granularity 752 Lv Qing, Han Xiaoming, Xie Gang, Yan Gaowei, and Xie Jun A Similarity Measuring Method Between Image Regions Based on Granular Computing 755 Jinling Shi and Genyuan Du An Improved Apriori Algorithm 759 Yongge Shi and Yiqun Zhou 
xii 


Application of Chaotic Particle Swarm Optimization Algorithm in Chinese Documents Classification 763 Dekun Tan Qualitative Simulation Based on Ranked Hyperreals 767 Shusaku Tsumoto Association Action Rules and Action Paths Triggered by Meta-actions 772 Angelina A. Tzacheva and Zbigniew W. Ras Research and Prediction on Nonlinear Network Flow of Mobile Short Message Based on Neural Network 777 Nianhong Wan, Jiyi Wang, and Xuerong Wang Pattern Matching with Flexible Wildcards and Recurring Characters 782 Haiping Wang, Fei Xie, Xuegang Hu, Peipei Li, and Xindong Wu Supplier Selection Based on Rough Sets and Analytic Hierarchy Process 787 Lei Wang, Jun Ye, and Tianrui Li The Covering Upper Approximation by Subcovering 791 Shiping Wang, William Zhu, and Peiyong Zhu Stochastic Synchronization of Non-identical Genetic Networks with Time Delay 794 Zhengxia Wang and Guodong Liu An Extensible Workflow Modeling Model Based on Ontology 798 Zhenwu Wang Interval Type-2 Fuzzy PI Controllers: Why They are More Robust 802 Dongrui Wu and Woei Wan Tan Improved K-Modes Clustering Method Based on Chi-square Statistics 808 Runxiu Wu Decision Rule Acquisition Algorithm Based on Association-Characteristic Information Granular Computing 812 JianFeng Xu, Lan Liu, GuangZuo Zheng, and Yao Zhang Constructing a Fast Algorithm for Multi-label Classification with Support Vector Data Description 817 Jianhua Xu Knowledge Operations in Neighborhood System 822 Xibei Yang and Tsau Young Lin An Evaluation Method Based on Combinatorial Judgement Matrix 826 Jun Ye and Lei Wang Generating Algorithm of Approximate Decision Rules and its Applications 830 Wang Yun and Wu-Zhi Qiang Parameter Selection of Support Vector Regression Based on Particle Swarm Optimization 834 Hu Zhang, Min Wang, and Xin-han Huang T-type Pseudo-BCI Algebras and T-type Pseudo-BCI Filters 839 Xiaohong Zhang, Yinfeng Lu, and Xiaoyan Mao A Vehicle License Plate Recognition Method Based on Neural Network 845 Xing-Wang Zhang, Xian-gui Liu, and Jia Zhao Author Index 849 
xiii 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





