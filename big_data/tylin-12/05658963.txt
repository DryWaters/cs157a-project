Quantitative Association Rules Mining Method Based on Trapezium Cloud Model Wang Zhao-hong 
Department of Computer Science and Technology  Weifang University Weifang, China wangzhhwfxy@163.com   Abstract 227The quantitative association rules mining method is difficult for their values are too large. The usual means is dividing quantitative Data to discrete conception. The trapezium Cloud model combines ambiguity and randomness organically to fit the real world objectively, divide quantitative Data with trapezium Cloud model to create concepts, the concept cluster within one class, and separated with each other. So the 
quantitative Data can be transforms to Boolean data well, the Boolean data can be mined by the mature Boolean association rules mining method Keywords- Quantitative Associa tion Rules; Trapezium-Cloud Model; Conception Division  Frequent Item Sets I  I NTRODUCTION  Association rule mining refers mainly to get the knowledge such as "customers bought tea 
 also purchased the coffee," which meet the minimum support and the minimum confidence. At present association rules can be divided into two types: Boolean association rules and quantitative 
association rules, and most of the research are focused on the Boolean association rules research First, give the formal description of association rules mining: Suppose I = \(i 1 i 2 i m s a collection of m different items, T = \(t 1 t 2 t n saction database, t j is a group of items of I set, t j I. Each transaction with a unique identifier T ID linked. If X is a subset of I with X t j we say that a transaction contains X. An association rule is a "X 
Y implication, in which X I, Y I, and X Y  Definition 1: If the ratio of transaction T contains X Y is Sup, the association rules X Y in T has a support degree Sup    100 n y support\(x Sup 
327   
1  Support \(X Y\ stands for the number which support X Y transactions, n stands for the total number of transactions 
Definition 2: If the ratio transaction T contains X also contains Y is Conf, then the confidence level of association rules X Y in T is Conf     100 support\(x y support\(x Conf 
327   
2 Association rules are called the strong association rules if they meet the minimum support and minimum confidence [1  2   Definition 3: Boolean association rules are got from the transaction database whose items values are 0 or 1 Mining process is as follows 
1\entify the transaction database T in all the non-empty sets which meet user-defined minimum support degree, if the item sets meet the minimum support degree which are called frequent item sets 2\ Using frequent item sets generate the required association rules. For each frequent item sets A, identify all non-empty subset A a, if  conf min support\(a support\(A 
002 then generated association rules a A-a   The above methods only consider whether the transaction contains the item, regardless of the number of item, for 
example, they only consider whether a particular transaction contains tea, regardless of a pound of tea or five pounds of tea a lot of useful information are lost[3    In applications, quantitative attribute Data quantity gathered in a certain range may contain very useful knowledge Upper and lower limits in the expression is too rigid, for example: for <age, 18, 38>, 16-17 years old, which certainly are not divided into this interval, but if we use "young concept to describe the age of people, 16-17 years old, which can be classified into the concept or not depending on the Individuals, or they belong to the concept partly. The method mentioned in this paper can identify association rules from the quantitative Data. Here, we first look at the mathematical 
theory II  C LOUD M ODEL  Cloud model is a qualitative and quantitative conversion model; it combines ambiguity and randomness organically 978-1-4244-6977-2/10/$26.00 \2512010 IEEE 
development projects of Shandong Province \(Grant Nos 2008GG30001030\atural Science Fund Project of Weifan g  University 
Grant Nos. 2008Z08  
This work was supported by the Scientific and technological 


          000\003\000\003\000\003\000\003\000\003\000\003    A A A 2 x Ex1 2 13 1 1 2En1 Ex1<=x<=Ex2 2 x Ex2 2 2232 2En2 MEC x MEC x  1 MEC x e e Ex En x Ex E xxExEn Set U is a mathematical domain U={x}, T is the language value associated with the U 002\035 T x\ stable tendency random number which expressed the elements x subordination of T concept, subordination’s distribution in the domain is known as the clou   Cloud mathematical expected curve is its subordination curves from the view of fuzzy set theory. However thickness" of the curve is uneven, waist is the most scattered the top and bottom of the curve are convergent, cloud's "thick reflects the subordination degree randomness, near to or away from the concept center have smaller subordination randomness, while concept center have the largest subordination randomness, which is consistent with people's subjective feelings The digital features of the normal cloud characterized by three values with the expectation Ex, entropy En, excess entropy He\the expected value Ex: the center value of the concept domain, is the most representative qualitative value of the concept, it should be 100% belongs to the concept; entropy En: is a qualitative measure of the concept’s ambiguity reflecting the accepted range values of the concept domain hyper entropy He: can be described as entropy En of entropy reflecting the degree of dispersion of the cloud droplets. The normal cloud is the most important cloud model, because various branches of the social and natural sciences have proved the normal distribution’s universality. The equation of normal cloud curve  e 2En 2 Ex  002\035 2   MEC A  3En in its corresponding cloud model all can be ignored, because it has been proved that approximately 99.74% elements of model fall into the range of Ex  3En by the mathematical characteristics of normal distribution. Extending the normal cloud model to get trapezoidal cloud model, trapezoidal cloud model can be expressed by six values, they are the expected number: Ex 1 and Ex 2 entropy: En1 and En 2 hyper entropy: He 1 and He 2   Figure 1  The digital features of Trapezoidal cloud model Trapezoidal cloud curve equations are determined by the expectation and the Entropy are following 000\003 000\003\000\003\000\003\000\003\000\003\000\003\000\003 4 000\003 Clearly, the left and the right half-cloud expectation curve is a normal curve It can complete the qualitative and quantitative transform more accurately, if there is a range belongs to the concept totally, then it can be expressed by the upper edge of Trapezoid, if only one value belongs to the concept totally then the upper edge of Trapezoid degenerate to a point trapezoidal cloud model also degenerated into the normal      3  Expectation curve is a normal curve, for a qualitative knowledge, the elements outside the Ex 5 000\003 g\(x\: data distribution function f j x\ cloud-based expectations function c j coefficients m: the number of superimposed cloud 0010 user-defined maximum error From the concept of clouds: in the domain the element’s subordination to the concept has statistical and random properties. In addition, the high-frequency elements contributions to the concept are higher than the low-frequency elements. That is the reason to use probability density function of data distribution to get the concept set, so the concept division algorithms can be done According to the definition of cloud transform, the quantitative attribute’s domain dividing into m concepts can evolve to a problem to get answers from the formula  006 000\003      0  1  cloud model. He1 and He2 can have different values, and thus the concept of the border on behalf of different fuzzy situation when the He1 and He2 all degenerate to 0, trapezoidal cloud model expressed a concept with accurate border subordination when one of the He1 or He2 degenerate to 0, which expressed a concept with one r accurate border subordination and one vague border subordination, so trapezoidal cloud model has a better generality III  D IVIDING Q UANTITATIVE D ATA TO C ONCEPTS U SING T RAPEZOIDAL C LOUD M ODEL  A  Cloud Transform 000\003 Any function can be decomposed into cloud-based superposition with allowed error range, which is Cloud Transform. The equation is      1 x f x g j m j j c 000 007    000  x f x g j j m j c 000\003\000\003\000\003\000\003\000\003 


000    m j i j j j j j j i j He He En En Ex Ex f x g c 1  2  1  2  1  2  1    007 I.e. to get Ex1 j Ex2 j En1 j En2 j He1 j He2 j and c j for each cloud concept, the quantitative attribute domain is divided into a number of concepts by using cloud model, the data in each concept aggregate, and the data between different concepts separate  8  B  Concept division  algorithm Cloud transform recover the data distribution concepts from a large number of property values, the conversion is from quantitative Data to qualitative concept, is a clustering problem essentially. Local peak of the histogram is that the data aggregation part, taking it as a concept center is reasonable, the higher the peak, indicating more data convergence there, deal with it with priority. The concept division algorithm is  Algorithm 1: Concept division algorithm Input: the domain of quantitative attributes that need the concept division, the overall error threshold 0010 and the peak height error threshold 0010 y, the length error 0010 x between trapezoidal top edge and the minimum value Output: m concepts and the corresponding digital features of attribute i  1\ount the each possible values x of attribute i and get the actual data distribution function g \(x 2\j=0 3\louds 010 002 g’\(x\=g\(x 4\hile max\(g’\(x 0010  5\ Ex j Find_Ex\(g’\(x 6\ Ex1 j search1\(g’\(x 0010 y  0010 x  7\ Ex2 j search2\(g’\(x 0010 y  0010 x  8\ En1 j Find_En\(c j Ex1 j  0010  9\ En2 j Find_En\(c j Ex2 j  0010  10\ gj\(x j Cloud\(Ex 1 En1 j Ex2 j En2 j  11\ g’\(x\g’\(x j x 12\ j=j+1 13\dwhile 14\or j=0 to m-1 do 15\louds\(Ex1 j Ex2 j En1 j En2 j He1 j He2 j  Calculate_He\(g j x\’\(x\loud\(Ex1 j Ex2 j En1 j En2 j  16\dfor  In Step 1, using statistical methods to get the actual data distribution function g\(x\; Step 2, 3 does variable initialization; Step 4 the division of the process is ended, if the error limit less than a given error Step 5 Search for the peak value of c j of property in the data distribution function g\(x and its corresponding value x is defined as the cloud model center \(expectation\; Step 6, 7 search approximate horizon line near the peak \(within the error limit threshold 0010 y the width is greater than the minimum width of the threshold value 0010 x  where were identified as uniformly distributed, the two endpoints of the line are recorded as the trapezoidal top edge endpoints Ex1 j Ex2 j otherwise get the trapezoidal top edge endpoints are equal to the peak point value Ex1 j Ex2 j Ex j  trapezoidal cloud degenerated into the normal cloud Trapezoidal cloud height coeffi cient is the function value of the Ex1 j or Ex2 j The step 8, 9 calculate cloud model entropy En1 j and En2 j to fit g \(x\or the half-liter cloud with Ex1 j  half-falling cloud with Ex2 j to get En1 j searching left area of the cloud model with Ex1 j to get En2 j searching right area of the cloud model with Ex2 j the entropy value increase from 0 step from the smaller value until the threshold 0010 is greater than the difference between the half-normal cloud value and distribution histogram value; the step 10 calculate distribution function of the corresponding Trapezoidal; step 11 use the original data minus the known distribution function of trapezoidal cloud model data distribution to get the new data distribution function g'\(x\peat step 4 to 12 until the peak value is less than the error threshold. Step 14, 15, 16 determine half hyper entropy of all cloud model with the residuals of distribution histogram IV  M INING A SSOCIATION R ULES F ROM C ONCEPTS M APPED F ROM Q UANTITATIVE D ATA  Dividing quantitative attributes into several concepts using concept division algorithm, mapping the attributes data to the corresponding concept, sometimes, one attribute value may be mapped to different concepts, then mapping it to the concept that the attribute data has the largest subordination in the Boolean association rules I={i 1 i 2 002  002 i m the original meaning of i i is to buy not to buy tea, through the concept division algorithm a quantitative attribute value is extended to buy a lot of tea, buy some tea, buy little tea. Of course, every transaction belong to one of these situations only, problems can be solved by the Boolean association rules mining, using the famous Apriori frequent item set algorithm and the non-frequent item sets of FP-tree algorithm can carry out association rules mining[9 0   V  E XPERIMENTAL A NALYSIS  Take two typical supermarket retail food rice and millet as example, the majority data scattered between 0.5-5kg Extract 20000 transaction records randomly as experiment data, use concept division algorithm with error threshold 0010 0 5, and the peak height error threshold 0010 y 0.03, the length threshold error between trapezoidal top edge 0010 x 0.03. Then every transaction record data are divided into buy a lot of rice 


millet\, buy some rice \(millet\, buy little rice \(millet\, the number of the concepts can be more or less according to the threshold values Using the famous Apriori algorithm to mine the database with the concepts mapped from the quantitative data MinSup=30%,  MinConf=80%. We get the following results a lot of rice 000 a little of millet >,<a little of bean some rice 000 some millet >,<some bean to improve its accuracy further, reduce error thresholds, and lower the minimum support for association rules and, but this will take more time, the minimum support degree and the minimum confidence degree shouldn’t too small, so do the error thresholds, if so, the association rules does not make sense, the minimum support degree and the minimum confidence degree are given by the experts in the field VI  C ONCLUSION  In this paper, we propose a new method to mining association rules from quantitative data based on the trapezoidal cloud model, which first take the original data distribution in the database into account, and then use the trapezoidal cloud model which combines ambiguity randomness and uncertainty organically, and transforms the quantitative concept and qualitative data each other, in the conversion take account of the basic characteristics of human behavior fully, because of the presence of randomness, the same quantity data may belongs to different concept. for the people’s age falls into the interval \(18,35\, we all think that he was young, but the 16-17 year-olds should be assigned to the concept “young” or not, that different people may have different views, even the same individuals may have different views at different times. concept division algorithm Based the trapezoidal cloud model take the above situation into account not all the data have the above characteristics, only the elements on the conceptual border have this randomness, and the randomness can not affect the whole concept, the elements fall into the interval \(Ex1, Ex2\ fall into its own concept, which are their certainty. Therefore, this method can simulate the phenomena of human society better. The quantitative data are change into Boolean data in a reasonable manner, use sophisticated Boolean association rule mining algorithm, and we can achieve quantitative association rules. Using the quantitative data mining method can excavate more knowledge from database to support decision-making better R EFERENCES 000\003 1  J. Han, and M. Kamber, Data Mining Concepts and Techniques, 2nd ed Morgan Kaufmann Publishers: 2006, pp.98-112 9  2  Mehmed Kantardzic, Datamining Concepts, Models, Methods and Algorithms, Beijing: Tsinghua University Press, 2009 9  3  Lu Qiu-gen, Fuzzy clustering algorithm and implementation, Computer Knowledge and Technology, 2008, vol.3\(27\ ,pp.1987-1990 9  4  Liao Zhian, Data Fuzzy processing, Journal of Ezhou University, 2005 vol.12\(6\,  pp.49-51 5  Li De-yi and Liu Chang-yu, Discussion of the universal nature on the normal cloud model. China Engineering Science, 2004,vol. 6,  pp. 28-33 6  Fan Zhoutian, Fuzzy Matrix Theory and Applications, Beijing:Science Press, 2006 7  LuJianjiang, Zhang Yanlin, Song Zilin, Fuzzy Association Rules and Application, Beijing:Science Press, 2008 8  Wang Hu, Mao Wenting, telecom customer behavior study based on cloud model and association rules, Journal of Wuhan University of Technology \(Information and Management Engineering\ 2009, vol.31 5\, pp.77-79 9  9  Yan Yuejin, Li Zhoujun,Chen Huowang, Efficient maximal frequent itemsets Mining Based on FP-Tree, Journal of Software, 2005 vol.16\(2\, pp.215-222   J Han, J Pei, Y Yin, Mining Frequent Patterns without Candidate Generation A Frequent Pattren Tree Approach, data Mining and knowledge discovery, 2004, vol. 8,  pp.53-87 9  


 Figure 2  Dependency network  Figure 3  Example of strong coupling The data mining package used also allows the rules to be visually presented. Figure 2 displays a dependency network in which each node represents a file. Each edge represents a pair wise association rule 
58 


Figure 3 displays an example of a zoom over a strong coupling between several forms of the system and its main form B  Data Validation In order to build more confidence on what was being mined, a sample of the generated rules was validated by the programmers. They used the visual representation of the rules and confirmed the consistency of the main associations presented by the dependency networks  The number of original transaction records gathered throughout eight years of operation of the version control system, was 256,804. However, after a careful data analysis, we identified that of the 256,804 original records 153,288 accounted for marks on the files \(labels\nd 69,592 were directory changes. In the end, only 33,924 changes in 4.096 files were taken into account V  R ESULTS  As seen in tables 2 and 3, our results reached averages higher than the averages presented in Zimmerman et al [5   These results suggest that association rules obtained in our industrial environment are better than those obtained in the published OSS domains. In addition, low standard deviation shown in Table 5 indicates that the values do not spread too much around the averages. Our next step was to test this evidence using inferential statistics A  Analysis and interpretation The analysis started by calculating the harmonic mean for the data presented in Zimmerman et al for eight OSS projects [5  T h is d ata is s h o w n in T a bl e 2   T h ey s e rv e as  a baseline of comparison for the results we obtained in the industrial environment, shown in Table 3 For the statistical testing, we established an apriority significance level 001 0.05. The Kolmogorov-Smirnov Test ensured that the sample was normally distributed. As seen in Table 4, we found p-values of 0.996 and 0.982 for the Navigation and Error Prevention F-measure 000  respectively.  As the p-value is the lowest possible significance with which it is possible to reject the null hypothesis, and they are larger than 0.05, we cannot reject the hypothesis that the data is normally distributed TABLE II  R ESULTS FOR COARSE GRANULARITY IN 9 AND ITS FMEASURE R=R ECALL   P    P RECISION  Suppor t Confidence Project R 000 P 000 F 000 measure R 000 P 000 F 000 measure ECLIPSE 0.17 0.26 0.21 0.03 0.48 0.06 GCC 0.44 0.42 0.43 0.29 0.82 0.43 GIMP 0.27 0.26 0.26 0.08 0.74 0.14 JBOSS 0.25 0.37 0.30 0.05 0.44 0.09 JEDIT 0.25 0.22 0.23 0.01 0.44 0.02 KOFFICE 0.24 0.26 0.25 0.04 0.61 0.08 POSTGRES 0.23 0.24 0.23 0.05 0.59 0.09 PYNTHON 0.24 0.36 0.29 0.03 0.67 0.06 Average 0.26 0.30 0.28 0.07 0.60 0.12 Navigation Prevention 0.1 0.9 13  In order to assure that we could use the T-test for independent samples, we had to test the equality of variances for the independent samples. The Levene Test, shown in Tables 6, produced p-values of 0.544 and 0.908, above 0.05 allowing us to accept the equality of variances for the samples TABLE III  R ESULTS FOR COARSE GRANULARITY IN AN INDUSTRIAL ENVIRONMENT R    R ECALL   P    P RECISION  Support Confidence Project, Description R 000 P 000 F 000 measure R 000 P 000 F 000 measure CadAlmox, Warehouse System Utilities 0.83 0.56 0.67 0.41 0.87 0.56 Cadastros, Requests System 0.79 0.61 0.69 0.30 0.94 0.45 Cliente, Customer System 0.87 0.73 0.79 0.56 0.98 0.71 Contabil, Accounting System 0.65 0.53 0.58 0.37 0.85 0.52 Fiscal, Tax System 0.89 0.62 0.73 0.43 0.93 0.59 FrameWork, Corporate Framework 0.59 0.71 0.64 0.35 0.97 0.51 GUF, Cosmetic System 0.73 0.61 0.66 0.51 0.93 0.66 Legado, Sales System 0.91 0.54 0.68 0.43 0.90 0.58 Logistica, Logistics System 0.67 0.71 0.69 0.42 0.96 0.58 SECV, Administration System 0.74 0.51 0.60 0.40 0.87 0.55 SisAlmox, Warehouse System 0.90 0.48 0.63 0.57 0.86 0.69 SisAP, Survey System 0.86 0.50 0.63 0.29 0.91 0.44 SisComo, Lending System 0.57 0.46 0.51 0.39 0.88 0.54 SisCusteio, Budget System 0.68 0.51 0.58 0.54 0.96 0.69 SisManutencao, Portal Configuration System 0.67 0.50 0.57 0.55 0.80 0.65 SisMkt, Marketing System 0.89 0.67 0.76 0.60 0.92 0.73 SisPortal , Web Portal 0.83 0.68 0.75 0.65 0.92 0.76 Web Services 0.65 0.56 0.60 0.50 0.87 0.64 Average 0.76 0.58 0.65 0.46 0.91 0.60 Navigation Prevention 13 0.1 0.9  Finally, we applied the T-Test and, in both cases, we obtained the p-value of 0.000, significantly lower than 0.05 rejecting our null hypotheses 000 1\(F-measure 000  000 2\(Fmeasure 000 We can, therefore, infer that better effectiveness of file modification prediction was achieved in our industrial projects than in the OSS projects Zimmermann et al [5 analyzed. The threats to validity to this conclusion are analyzed in the next section TABLE IV  K OLMOGOROV S MIRNOV T EST  18 18 6533 6028 7.444E-02 9.380E-02 096 110 096 110 076 102 409 466 996 982 N Mean Std. Deviation Normal Parameters a,b Absolute Positive Negative Most Extreme Differences Kolmogorov-Smirnov Z Asymp. Sig. \(2-tailed Navigation Prevention Test distribution is Normal a Calculated from data b  TABLE V  G ROUP S TATISTICS G ROUP 1    C LOSED S OURCE   G ROUP 2    O PEN S OURCE  18 6533 7.444E-02 1.755E-02 8 2750 6.969E-02 2.464E-02 18 6028 9.380E-02 2.211E-02 8 1213 1293 4.573E-02 Projects 1 2 1 2 Navigation Prevention N Mean Std Deviation Std. Error Mean 
59 


TABLE VI  L EVENE AND T  T ESTS FOR N AVIGATION AND P REVENTION   379 544 12.182 24 000 3783 3 106E-02 3142 4424 12.507 14.376 000 3783 3 025E-02 3136 4431 014 908 10.751 24 000 4815 4 479E-02 3891 5740 9.481 10.421 000 4815 5 079E-02 3690 5941 Equal variance assumed Equal variance not assumed Equal variance assumed Equal variance not assumed Navigatio n Preventio n F Sig Levene's Test for E quality of Variances t df Sig 2-tailed Mean Difference Std. Error Difference Lower Upper 95% Confidence Interval of the Difference t-test for Equality of Means B  Threats to Validity In spite of having achieved statistical significance in the study, one must consider the following threats to the validity of our study   We cannot conclude that all closed-source projects will yield similar results as the one we have obtained, process maturity can play a large role on the prediction effectiveness   Other software characteristics such as size and complexity may affect the association results. We have not test for those variables   The projects that we analyzed were stable and had a few number of transactions per day \(Table 1\ Less stable or more dynamic projects may yield different results   Like in t h e  or der i ng o f  cha nge s m a y i n fl ue nc e  the results for Navigation and Error Prevention VI  C ONCLUSIONS AND FUTURE WORK  This article shows that software module modification association rules derived from our stable industrial environment were more effective than those derived from the OSS projects reported in the literature. As association rules are already being successfully applied for impact analysis in this context [5  o u r  resu l t s i n d i c a tes th at th ey c a n  als o  b e  effectively applied in industrial environments such as the one we studied  We believe that this finding may stimulate further studies in the industry and, hopefully, motivate them to invest on association mining to support software maintenance and evolution activities  It is important to observe that in our analysis systems with little evolutive maintenance, such as the Marketing and the Customer Management System, yield rules with precision and recall better than the average. This was expected as more stable systems should yield better association rules Also, independent of the number of transaction per day systems with small training sets, such as the Accounting and the Web Services Systems, yield rules with precision and recall worse than the average. This was also expected, as this number training examples helps to strengthen the rules As mentioned before, we adapted the software engineering experimental process described in Wohlin et al  o so ft w a r e  r e po si t o r i e s m i ni ng e x p e r i m e nt s. We be l i e v e that the studies, applications, and tools for software repository mining can benefit from this type of approach Rigorous experimental description facilitates replication of studies and the executing of systematic reviews and other types of secondary analysis in the field [18  C o n s id e r in g th e  number of studies being done in the area, we believe that experimental formalism combined with approaches like standardization of data exchange formats among software repositories [1  can  con t ri b u t e t o f a cili tat e th e ex ecu t i o n  and replication of studies in the area As future work, we will start to look at fine granularity associations, analyzing classes and methods as opposed to files. We will also analyze relationships between coupling and the associations mined from version control systems We are currently finishing a Microsoft Visual Studio plug-in to use the mined rules to aid programmers on the fly It will work as a monitor that verifies saved files and warns programmers about unverified associated files. This will facilitate the programmers’ impact analysis effort and mitigate errors during program modification. After its completion, the plug-in will be experimentally evaluated in carefully controlled case studies. Lastly, we will use our Data Warehouse [6 o t e st s e ve ra l  o t her so ft w a r e m i ni ng  techniques over the data we have already gathered and preprocessed A CKNOWLEDGMENT  This study could only be developed due to the full support of CIAL–Brazil and its staff. This work is partially 
60 


supported by CNPq – Brazil’s National Research and Development Council R EFERENCES  1  Mendonça, M. G. and Sunderhaft, Nancy L \(1999\. Mining Software Engineering Data: A Survey. A DACS State-of-the-Art Report Available at http://www.dacs.dtic.mil/techs/datamining/datamining.pdf 2  Mithun Acharya, Tao Xie, Jian Pei, and Jun Xu \(2007\. Mining API Patterns as Partial Orders from Source Code: From Usage Scenarios to Specifications. In Proceedings of the 6th joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering \(ESEC/FSE 2007\ Dubrovnik, Croatia, pp. 25-34 3  Zimmermann, T.; Kim, S.; Whitehead ,E. J. and Zeller, A \(2007 Predicting Faults from Cached History. In Proc. of the 29th International Conference on Software Engineering \(ICSE 2007 pages 489–498 4  Lucas Layman, Nachiappan Nagappan, Sam Guckenheimer, Jeff Beehler, Andrew Begel \(2008\. Mining software effort data preliminary analysis of visual studio team system data. In Proceedings of the International Working Conference on Mining Software Repositories, pages 43-46 5  Zimmermann, T., Zeller, A., Weissgerber, P., and Diehl, S \(2005 Mining Version Histories to Guide Software Changes. IEEE Transactions on Software Engineering, 31, 429-445 6  Colaço, Methanias, Mendonça, M. G., Rodrigues, Francisco \(2009 Data Warehousing in an Industrial Software Development Environment.  Paper submitted to 33th IEEE Software Engineering Workshop 7  T. T. Ying \(2003\. Predicting source code changes by mining revision history. Master’s thesis, University of British Columbia, Canada 8  T. Zimmermann, S. Diehl, and A. Zeller \(2003\. How history justifies system architecture \(or not\. In Proc. International Workshop on Principles of Software Evolution, Helsinki, Finland, pages 73–83 9  H. Gall, M. Jazayeri, and J. Krajewski \(2003\. CVS release history data for detecting logical couplings. In Proc. International Workshop on Principles of Software Evolution, Helsinki, Finland, pages 13–23   Robert Michael Dondero, Jr \(2008\. Predicting Software Change Coupling. PhD dissertation, Drexel University, USA   Michail \(2000\. Data mining library reuse patterns using generalized association rules. In International Conference on Software Engineering, pages 167–176   Wohlin, P. Runeson, M. Host, M. C. Ohlsson, B. Regnell, and A Wesslén \(2000\. Experimentation in software engineering: an introduction. Kluwer Academic Publishers, ISBN: 0-7923-8682-5   R. van Solingen and E. Berghout \(1999\. The Goal/Question/Metric Method: A practical guide for quality improvement of software development. McGraw-Hill   C. J. V. Rijsbergen \(1979\ Information Retrieval, 2nd edition Butterworths, London   Zimmermann T. and Weibgerber, P \(2004\ Preprocessing CVS data for fine-grained analysis. In Proc. International Workshop on Mining Software Repositories \(MSR04\, Edinburgh   K. Fogel and M. O’Neill \(2002\. cvs2cl.pl: CVS-log-message-toChangeLog conversion script. Available at http://www.redbean.com/cvs2cl   R. Agrawal and R. Srikant \(1994\ Fast algorithms for mining association rules. In Proceedings of the 20th Very Large Data Bases Conference \(VLDB\, pages 487–499. Morgan Kaufmann   T. Dyba, B. Kitchenham, and M. Jorgensen \(2005\. Evidence-based software engineering for practitioners. Software, IEEE, 22\(1\65   Sunghun Kim, Thomas Zimmermann, Miryung Kim, Ahmed E Hassan, Audris Mockus, Tudor Girba, Martin Pinzger, E. James Whitehead Jr., Andreas Zeller \(2006\. TA-RE: An Exchange Language for Mining Software Repositories. In Proceedings of the Third International Workshop on Mining Software Repositories Shanghai, China, pp. 22-25   Michael Grossman and Robert Katz \(1986\. A new approach to means of two positive numbers. International Journal of Mathematical Education in Science and Technology, Volume 17 Number 2, pages 205 – 208 
61 


Application of Chaotic Particle Swarm Optimization Algorithm in Chinese Documents Classification 763 Dekun Tan Qualitative Simulation Based on Ranked Hyperreals 767 Shusaku Tsumoto Association Action Rules and Action Paths Triggered by Meta-actions 772 Angelina A. Tzacheva and Zbigniew W. Ras Research and Prediction on Nonlinear Network Flow of Mobile Short Message Based on Neural Network 777 Nianhong Wan, Jiyi Wang, and Xuerong Wang Pattern Matching with Flexible Wildcards and Recurring Characters 782 Haiping Wang, Fei Xie, Xuegang Hu, Peipei Li, and Xindong Wu Supplier Selection Based on Rough Sets and Analytic Hierarchy Process 787 Lei Wang, Jun Ye, and Tianrui Li The Covering Upper Approximation by Subcovering 791 Shiping Wang, William Zhu, and Peiyong Zhu Stochastic Synchronization of Non-identical Genetic Networks with Time Delay 794 Zhengxia Wang and Guodong Liu An Extensible Workflow Modeling Model Based on Ontology 798 Zhenwu Wang Interval Type-2 Fuzzy PI Controllers: Why They are More Robust 802 Dongrui Wu and Woei Wan Tan Improved K-Modes Clustering Method Based on Chi-square Statistics 808 Runxiu Wu Decision Rule Acquisition Algorithm Based on Association-Characteristic Information Granular Computing 812 JianFeng Xu, Lan Liu, GuangZuo Zheng, and Yao Zhang Constructing a Fast Algorithm for Multi-label Classification with Support Vector Data Description 817 Jianhua Xu Knowledge Operations in Neighborhood System 822 Xibei Yang and Tsau Young Lin An Evaluation Method Based on Combinatorial Judgement Matrix 826 Jun Ye and Lei Wang Generating Algorithm of Approximate Decision Rules and its Applications 830 Wang Yun and Wu-Zhi Qiang Parameter Selection of Support Vector Regression Based on Particle Swarm Optimization 834 Hu Zhang, Min Wang, and Xin-han Huang T-type Pseudo-BCI Algebras and T-type Pseudo-BCI Filters 839 Xiaohong Zhang, Yinfeng Lu, and Xiaoyan Mao A Vehicle License Plate Recognition Method Based on Neural Network 845 Xing-Wang Zhang, Xian-gui Liu, and Jia Zhao Author Index 849 
xiii 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





