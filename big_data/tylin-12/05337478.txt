Meaningful Inner Link Objects for Automatic Text Categorization   Jau-Ji Shen*, Jia-Chiuan Wu Department of Management Information Sy stems, National Chung Hsing University 250, Kuo Kuang Rd., Taichung 402, Taiwan jjshen@dragon.nchu.edu.tw  Abstract  This paper presents a novel approach for automatic text categorization. The mainstream of the research on rule-based classifier regards document as a container of term, and generates rules by using the term distribution in documents. General speaking, there must be existed some kind of semantic relevance between term and paragraph in a document. We call it Meaningful Inner Link Objects-MILO which must be 
varied with different semantics of a document itself While this paper concentrates on using these MILOs that associate with semantic relevance for text categorization, hence we focus on two problems  1 finding the best MILOs which associate with semantic relevance; and \(2\ using these specific MILOs to build a classifier for text categorization. From the experiment results, our proposed classification approach base on MILO has a better accuracy while other state of the art technique without considering the relevance between term and paragraph  1. Introduction  Text categorization is the task of classifying 
unlabelled documents into one or several predefined categories. It’s gradually becoming a subject and receives much concern from re searchers in recent years Because the continuing popularity of Internet technology, user can readily access enormous digital documents. Hence to efficient organize and retrieve these digital documents have become a very important subject for discussion Currently, Text categorization is a discipline at the crossroads of information retrieval and machine learning that is mentioned in [1   T h e p r o cess o f tex t  categorization can be simply divided into 
preprocessing and classifier construction. As shown on Fig.1, there are techniques in preprocessing, such as tf·rf, information gain, chi-square, etc. And these techniques are arranged in T h es e techn i qu es as s i g n  appropriate weights to the terms. And the terms in higher weights will be selected as indices of a document, and other terms in lower weights will be filtered out. The classifier construction such as Bayes classifier [3 r o b a b ilistic classi f i er b y ap p l y i n g  bayes theory for classification that bases on the hypothesis of independence between terms relationship. K-nearest neighbor classifi s an 
example-based classifier which categories an unlabelled document according to the distance information with K nearest examples in the classifier In addition, decision tree classifier [5 ciatio n ru le  classifi all of th e m ach iev e v e r y s i gn i f icant  performance in text categorization These techniques mentioned above do not consider the paragraph factor, hence to investigate the relationship about paragraphs further that needs to normalize irregularity paragraphs at first, and then builds a classifier which considers the paragraph factor. Next section will illustrate the way to build a classifier which considers the paragraph factor 
The remainder of this paper is organized as follows Section 2 introduces our new concept and illustrates the way to build a classifier. Section 3 provides experiment results and Section 4 draws a conclusion  
 Figure 1. Concept diagram of text classifier construction  2. Building a MILO classifier  2.1. Basic concepts  This section will introduce a new concept that is presented in this paper. Firstly, a document is written 
2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing 978-0-7695-3762-7/09 $26.00 © 2009 IEEE DOI 10.1109/IIH-MSP.2009.21 266 
2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing 978-0-7695-3762-7/09 $26.00 © 2009 IEEE DOI 10.1109/IIH-MSP.2009.21 266 


base on the thought of author. And a thought is fully presented in a document which should be precisely structured and be logically. For example, the head part of a document will introduce the document’s topic, the body part will mainly present the details about this topic, and the tail part will draw conclusions. The semantics of a document will generate certain term structures which are formed across paragraphs for some specific meaning. On the other words, the linkage of terms across different paragraphs in a certain order denotes the author’s specific meaning The term’s linkage across different paragraphs is called as Meaningful Inner Link Objects \(MILO\.2 illustrates the concept of MILO            Figure 2. Concept of MILO  As shown on the dotted lines of Fig.2, MILO is a directional linking relationship which is composed by terms across paragraphs. Such as [A1 001 B2 001 A3 i s a  MILO with length 3 which is composed from a term of P 1 to a term of P 2 and then ending at a term of P 3 respectively  Similarly, [A1 001 A3 i s a M I LO wi t h  length 2 which is composed from a term of P 1 to a term of P 3 A1 is a len g t h 1 MIL O in P 1 We use the collection of extracted MILO s from all documents in each category to build a classifier  2.2. The process of building MILO classifier  This section will illustrate the way to use MILO for text categorization. Fig.3 shows the classifier building process. First of all, the selected dataset is divided into training set and testing set in a certain proportion. The classifier is trained by training set and evaluated by testing set  Figure 3. Construction forMILO classifier  2.2.1. Preprocessing of training set According to patterns of MILO which is illustrated in Fig.2, they are denoted by the term’s directional linkage across the paragraphs in each document, hence each document will be normalized into three paragraphs first. Scott Piao [7 p r o v id es th is p a rag r ap h n o r m alizatio n to o l o n  the website of The National Centre for Text Mining NaCTeM\ After normalizations, three paragraphs are stored into three databases respectively. For noisy reduction, a stop word list [8 p lied to  rem o v e  general stop words. In order to define each term’s relevance degree in different paragraphs, the chi-square function is used to calculate the weight of each term’s relevance degree from the three databases respectively Eq.1 is the chi-square formula defined base on the term occurrences as follow  d c  b a  d b  c a   c  b d  a   N  C  t  i       2 2 002 1 a  occurrences of term t i in category C b  occurrences of other terms in category C c  occurrences of term t i in other categories d  occurrences of other terms in other categories N  a+b+c+d, total occurrences of all terms in all categories Finally, each normalized paragraph of every document in each category is denoted by top T chisquare value terms per category in each database. On the other words, all selected terms are t 1  t 2  000  t T  which are used to denote a category  2.2.2. MILO mining The selected T terms t 1 t 2  000  t T  with relative importance in each document’s normalized paragraphs will be used to find the MILOs as shown Fig.2. Since MILOs are concealed in the document, we present three algorithms for MILO mining  For any document d in training set, let’s define the following notations first C the category of document d  P 1 the first normalized paragraph of d denoted by  o 11  o 12  000  o 1 001 where o 1i  i 1,2 000  001 is the number of occurrences of term t i 003  t 1 t 2  000  
267 
267 


t T On the other words P 1 contains 001 different  terms which belong to t 1 t 2  000 t T  P 2  P 3 the second and third normalized paragraph of d denoted by o 21  o 22  000  o 2 002\025 d o 31  o 32  000  o 3 002\026 respectively with the similar meaning as P 1  CM  l f length l MILOs in category C OCM  l et of number of occurrence with respect to each MILO in CM  l  Algorithm 1 Extract MILO with length 1 from a document d in C  Input  P 1  P 2 and P 3 of d  Output CM 1\d OCM 1 Methods  1  for each o 1i in vector P 1 1 004 i 004 001  2   OCM 1 0018 CM 1 005 o 1i  3   CM 1 0018 CM 1 005  t 1i  4  for each o 2j in vector P 2 1 004 j 004 002\025  5   OCM 1 0018 OCM 1 005 o 2j  6   CM 1 0018 CM 1 005  t 2j  7  for each o 3k in vector P 3 1 004 k 004 002\026  8   OCM 1 0018 OCM 1 005 o 3k  9   CM 1 0018 CM 1 005  t 3k  Because the MILO extracted by Algorithm 1 has length 1, hence occurrence of each extracted MILO is equal to the occurrence of this MILO’s term Algorithm 2 Extract MILO with length 2 from a document d in C  Input P 1  P 2 and P 3 of d  Output CM 2\d OCM 2 Methods 1  for each o 1i in vector P 1 1 004 i 004 001  2  for each o 2j in vector P 2 1 004 j 004 002\025  3   OCM 2 0018 OCM 2 005 min o 1i  o 2j  4   CM 2 0018 CM 2 005  t 1i 001 t 2j  5  for each o 2j in vector P 2 1 004 j 004 002\025  6  for each o 3k in vector P 3 1 004 k 004 002\026  7   OCM 2 0018 OCM 2 005 min o 2j  o 3k  8   CM 2 0018 CM 2 005  t 2j 001 t 3k  9  for each o 1i in vector P 1 1 004 i 004 001  10  for each o 3k in vector P 3 1 004 k 004 002\026  11   OCM 2 0018 OCM 2 005 min o 1i  o 3k  12   CM 2 0018 CM 2 005  t 1i 001 t 3k  Algorithm 2 extracts MILOs with length 2, such that the linkage of MILOs are composed as P 1 to P 2  P 1 to  P 3   and P 2 to P 3 respectively as shown in Fig.2 Algorithm 3 Extract MILO with length 3 from a document d in C  Input P 1  P 2 and P 3 of d  Output CM 3\d OCM 3 Methods 1  for each o 1i in vector P 1 1 004 i 004 001  2  for each o 2j in vector P 2 1 004 j 004 002\025  3  for each o 3k in vector P 3 1 004 k 004 002\026  4   OCM 3 0018 OCM 3 005 min o 1i  o 2j  o 3k  5   CM 3 0018 CM 3 005  t 1i 001 t 2j 001 t 3k  Algorithm 3 extracts all the MILOs across three normalized paragraphs in an ordered sequence from P 1 to P 2 and to P 3 finally The previous designed algorithms will extract all MILOs with lengths 1, 2 and 3 from each document of a category C By three extracted MILOs, a classifier of training set is built. On the other words, for a category C the collection of MILOs CM 1 CM 2\d CM 3 are now built as a classifier  2.3. Prediction with MILOs  When a MILO classifier predicts an unlabelled document’s category, unlabelled document should be normalized into 3 paragraphs at first, then all MILOs of this document are extracted by Algoritms 1, 2 and 3  In order to define the MILO’s weighting in categorization, the following Algorithm Confidence is designed Algorithm Confidence Input D a set of MILOs from an unlabelled document UCM  CM 1 005 CM 2 005 CM 3\d  OCM  OCM 1 005 OCM 2 005 OCM 3 Output the confidence value of D with respect to certain category C  Methods 1  S C 0018  001 S C a set of C s MILOs which match patterns in D  2  weight 0018 0 /* weight of the length of MILO 3  Confvalue C 0018 0 /* confidence summation 4  for each MILO 002\035 003 UCM  5  for each MILO 0011 in D  6  if 002\035  0011  7   S C 0018 S C 005 002\035  8  for each MILO 002\034 003 S C  9  if 002\034 length 1 10   weight  0018  w 1  11  if  002\034 length 2 12   weight  0018  w 2  13  if 002\034 length 3 14   weight  0018  w 3  15  Confvalue C  0018 Confvalue C  002\034  conf C weight  16   conf C is defined as Eq.\(2 According to any given category’s MILO collection, steps \(4\\(7\ pattern’s matching between the unlabelled document and the given category C If any MILO is matched, that means not only terms matching but also the direction of MILO is matched. The matched MILOs will be stored into S C  Steps \(8\\(15\fidences of all matched MILOs in S C The following Eq.2 is the formula to calculate confidence of a MILO   categories  all  in   MILO  of  occurences  total C category  in   MILO  of  occurences  total conf C 006 006  2 For the weights w 1  w 2 and w 3 if a matched MILO has the longer length, the more important it is \(i.e w 3  w 2  w 1  Algorithm Confidence calculates the 
268 
268 


confidence value to estimate the similarity between an unlabelled document and a given category C   The following steps show how all algorithms are operated as a classifier to classify a given unlabelled document 1  For each category C in training set Algorithm 1  2  and 3 are applied on each document of C to extract all MILOs of C  2  The MILO’s classifier is composed by all MILO’s set of all categories in training set 3  For any given unlabelled document D  Algorithm 1  2 and 3 are applied first to find all MILOs of D  and then Algorithm Confidence is performed to calculate the confidence value of D to each category in training set 4  Suppose the confidence value of D to a category X  have the largest value then we let D belong to X   3. Experiments results  In this paper, The benchmark in this experiment is 20 Newsgroups T h e v e rs i o n of t h i s ben c hm ark i n  this experiment is “bydate” version which is already divided into 60 percent training set and 40 percent testing set. And accordin e ev al u a tio n measures, micro F1 and macro F1 are used for performance evaluation. The experiments are conducted on five subsets of 20 Newsgroups as shown on Table 1 respectively. Table 2 displays the best experiment results which selects 700 terms\(i.e T 700 in highest chi-square value from all categories in each database. We compare our results with the research of Pu et al n 2007 T h e ex peri m e n t resu l t s of o u r  method have better classification accuracy than the best results of Pu et al’s method  4 Conclusion   Our new approach also provides the following two benefits: \(1\e MILOs are human readable and allows manual maintenance if necessary. \(2\The MILOs in the classifier could be incrementally updated. When the new documents are presented for retraining Algorithm 1  2 and 3 can extract MILOs from these new documents and then importing these new MILOs into the classifier directly without reconsidering the past documents. Hence, the cost of retraining is lower than SVM  association rules 000 etc. With these advantages of the MILO-based approach for text classification, there are still areas for further improvement. We plan to increase the efficiency of algorithms for reducing the training time and testing time, or to find a way which measures the importance of the MILO to build the classifier with higher quality  Table 1. The partition of 20 newsgroup according to subject matter  comp.graphics comp.os.ms-windows.misc comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey sci.crypt sci.electronics sci.med sci.space misc.forsale talk.politics.misc talk.politics.guns talk.politics.mideast talk.religion.misc alt.atheism soc.religion.christian  Table 2. The results of MILO categorization T 700 w 1 0.33 w 2 0.66 w 3 1  Data sets Measures Purpose method Pu et al’s method 5 comp ma-F1 0.685 0.671 mi-F1 0.690 0.670 4 rec 002  ma-F1 0.911 0.905 mi-F1 0.911 0.904 4 sci 002  ma-F1 0.856 0.848 mi-F1 0.856 0.843 3 politics 002  ma-F1 0.813 0.798 mi-F1 0.824 0.802 3 religion 002  ma-F1 0.715 0.752 mi-F1 0.732 0.773 Average ma-F1 0.796 0.795 mi-F1 0.803 0.798  References  1 F Se ba stia ni  M a c h ine L e a r ning in A u tom a te d T e x t  Categorization ACM Comput. Surv vol. 34, no. 1, pp. 147, 2002 2 L M a n  T Ch e w Li m  S  J i a n et al Supervised and Traditional Term Weighting Methods for Automatic Text Categorization IEEE Trans. Pattern Anal. Mach. Intell vol 31, no. 4, pp. 721-735, 2009 3 D  D  L e w i s   N a i ve  B ay e s   a t For t y  T h e I nde pe nde nc e  Assumption in Information Retrieval Proc  European  Conf  Machine Learning pp. 4-15, 1998 4 Y  Y a ng a n d X. L i u   A R e e x a m i na tion of T e x t  Categorization Methods Proc Int’l Conf. Machine Learning pp. 412-420, 1997 5 J. R Qu i n lan   C4.5 1993  M  L  A n t o n i e an d O R  Z a i a n e  T ext Do cu m e n t  Categorization by Term Association Proc  IEEE  int’l  Conf  Data Mining 2002  S   P i ao  S e n t e n ce an d  P a ragrap h Breaker In  http://text0.mib.man.ac.uk:8080/scottpiao/sent_detector 8 C Fo x  A sto p  list f o r G e n e ral T e x t  ACM  SIGIR Forum  vol. 24, no. 1-2, pp. 19-21, 1989 9 K  L a ng T h e 20 N e w s g r oups  D a ta Se t, I n  http://people.csail.mit.edu/jrennie/20Newsgroups 10 W   Pu, N  L i u, S Y a n e t a l    L oc a l  W o rd Ba g Mode l for Text Categorization Proc  IEEE  int’l  Conf  Data Mining  2007 
269 
269 


T ABLE II N EW P REDICTED HIV-1\226 HUMAN PROTEIN INTERACTIONS Pr edicted Interactions Con\002dence of Prediction  re v  ACTG1 tat  ACTG1 gag p6  PRKCA  vif  PRKCA  75 gag capsid  MAPK1 gag capsid  PRKCA vpr  PRKCA en v gp120  A CTB  env gp160  A CTB gag matrix  PRKCB1  rev  PRKCB1 77.8 gag matrix  PRKCQ  rev  PRKCQ env gp41  MAPK1  pol pr otease  MAPK1 vpr  MAPK3 80 nef  PRKCD  pol protease  PRKCE en v gp160  CASP3  env gp160  CD4 85.7 en v gp160  MAPK3 gag nucleocapsid  PRKCA 87.5  N Lin B Wu R Jansen M Gerstein and H Zhao 223Information assessment on predicting protein-protein interactions,\224 BMC Bioinformatics  vol 5 no 154 2004  Y  Y amanishi J P  V ert and M Kanehisa 223Protein netw ork inference from multiple genomic data A supervised approach,\224 Bioinformatics  vol 20 no Suppl 1 pp i363\226i370 2004  L Zhang S W ong O King and F  Roth 223Predicting co-comple x ed protein pairs using genomic and proteomic data integration,\224 BMC Bioinformatics  vol 5 no 38 April 2004  A Ben-Hur and W  S Noble 223K ernel methods for predicting proteinprotein interactions,\224 Bioinformatics  vol 21 no Suppl 1 pp i38\22646 2005  Y  Qi J Klein-seetharaman and Z Bar joseph 223 A mixture of feature experts approach for protein-protein interaction prediction,\224 BMC Bioinformatics  vol 8 no Suppl 10 2007  R Agra w al T  Imieli 264 nski and A Swami 223Mining association rules between sets of items in large databases,\224 in Proceedings of the 1993 ACM SIGMOD international conference on Management of data SIGMOD'93  New York NY USA ACM 1993 pp 207\226216  R Agra w al and R Srikant 223F ast algorithms for mining association rules in large databases,\224 in Proc 20th International Conference on Very Large Data Bases  San Francisco CA USA Morgan Kaufmann Publishers Inc 1994 pp 487\226499  S Bandyopadh yay  U Maulik L B Holder  and D J Cook Advanced Methods for Knowledge Discovery from Complex Data Advanced Information and Knowledge Processing  Springer-Verlag London 2005  J Hipp U G 250 untzer and G Nakhaeizadeh 223Algorithms for association rule mining 226 a general survey and comparison,\224 SIGKDD Explorations  vol 2 no 1 pp 58\22664 July 2000  B Goethals 223Ef 002cient frequent pattern mining 224 Ph.D dissertation University of Limburg Belgium 2002  G I W ebb 223Ef 002cient search for association rules 224 in Proc Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  The Association for Computing Machinery 2000 pp 99\226107  O T astan Y  Qi J Carbonell and J Klein-Seetharaman 223Prediction of interactions between HIV-1 and human proteins by information integration,\224 in Proceedings of the Paci\002c Symposium on Biocomputing  2009 pp 516\226527  W  Fu B E Sanders-Beer  K S Katz D R Maglott K D Pruitt and R G Ptak 223Human immunode\002ciency virus type 1 human protein interaction database at NCBI,\224 Nucleic Acids Research Database Issue  vol 37 pp D417\226D422 2009 


each of the k-candidate set is generated by intersection of the list \(of transaction ids\f all its \(k-1\ubsets. The kcandidate set along with the list is pushed into the data set C k List This is followed by pruning of all those k-candidate sets that do not have support at least equal to minimum support. Thus large k-itemsets are obtained gen_candidates_with_list \( C k-1 C k New  For all c 005 C k New do begin For all possible \(k-1\ubsets s of c do Insert { itemset,list } into C k List  where itemset = c.itemset AND list 006 000 013i 013g\013@\0135 i list s 005 C k-1  End End 5. Performance Study To confirm the time efficient approach of the algorithm, we implemented the proposed algorithm and the apriori algorithm on a 512 MB RAM Memory and 1.67 GHz Intel processor. We carried out a substantial performance evaluation and compared it with results obtained for apriori algorithm Table 3 compares performance of the proposed algorithm with the apriori algorithm for different minimum supports against different number of transactions. As minimum support decreases, the performance of the proposed algorithm increases and after a certain point, it performs better than the apriori algorithm. The relative performance also increases with increase in number of transactions. These results are also demonstrated in Figure 1. Total number of data items and average number of items per transaction are 30 and 15 respectively Table 4 compares the number of items in MEDIUM and LOW corresponding to the data values shown in Table 3 The execution time is shown to compare the increase in performance with increase in number of candidate sets in LOW Figure 2 compares the number of elements in MEDIUM and LOW against different number of initial data items for a given database and minimum support. For this experiment number of transactions in the database is 2K and minimum support is 20%.The proposed algorithm performs better with increase in the number of candidate sets present in LOW Figure 2 shows that the number of elements in MEDIUM and LOW increases with increase in number of data items Thus, the proposed algorithm performs better for large number of data items in the database 5.1 Result Analysis The performance of the proposed algorithm is directly proportional to the number of passes over the algorithm number of elements in LOW and data items in the database During initial passes, size of L k C k and 007 004 k is more than the database. Hence, initial passes are time consuming. But during the latter passes, these data sets become significantly smaller than the database. Larger minimum support generates less number of candidate sets and involves lesser passes over the algorithm. Thus, Figure 1 shows a dip in performance of the proposed algorithm for larger minimum support values. But, it performs better than the apriori algorithm as minimum support decreases  Table 3: Execution time in seconds   Table 4: Execution time v/s size of MEDIUM and LOW  6. Conclusion and future directions The proposed algorithm presents a probabilistic approach to apriori algorithm. The proposed algorithm creates exactly the same number of rules as the apriori algorithm but in lesser time. The results confirm that the discovery of association rules in larger databases is faster in the proposed algorithm than the apriori algorithm Integration of probabilistic approach to data mining in distributed databases shall produce amazing results [9 Hence, the effectiveness of the proposed algorithm needs to be checked on distributed databases. This algorithm was implemented on a synthetic database. Its application on real 
407 


world databases presents a future direction for further research  Figure 1: Execution time v/s Minimum Support  References 1 R. A g r a w a l a nd R. Srik a n t F a s t A l g o rithm s  f o r Mining  Association Rules,” In Proc. of the 20th VLDB Conference  Santiago, pp. 487-499, Chile, 1994 2 R. A g ra w a l T  Im i e linsk i, a nd A. Swami. “Mining Association Rules between Sets of Items in Large Databases.” In Proceedings of ACM SIGMOD pages 207-216, May 1993 3 R. A g ra wa l, R. Srik a n t M in i n g se que ntia l pa t t e r ns In proc. of the 11th International Conference on Data Engineering \(ICDE'95  pages 3-14, March 1995 4 M Blu m R.W  Flo y d   V   P r att, R.L  Riv e st an d R E. T a rjan  Time bounds for selection,” J. Comput. Syst. Sci. 7\(1973\, pp 448-461  Total number of data items Number of elements in MEDIUM Number of elements in LOW 10 139 0 20 3238 127 30 21899 1923 Figure 2: Size of MEDIUM and LOW v/s number of data items 5 Fe r e nc B odo n  A f a s t  A P R I O R I i m plem e n ta tion I n pr oc o f  IEEE ICDM Workshop on Frequent Itemset Mining Implementations \(FIMI'03 Melbourne, Florida, USA, 2003 6 Feren c Bo d o n   A T r i e b as ed A P RIORI Im p l e m en tatio n f o r  Mining Frequent Item Sequences.” In ACM SIGKDD Workshop on Open Source Data Mining Workshop \(OSDM’05 pages 56-65 Chicago, IL,USA, 2005  M  C h en  J Han an d  P  S   Yu  Dat a M i n i n g  A n o v ervi e w  from a Database Perspective IEEE Transactions on Knowledge and Data Engineering vol. 8, no. 6, pp. 866-883, Dec. 1996 8 A r on Culotta A ndre w Mc Ca llum Jona tha n Be tz  I nte g ra tin g probabilistic extraction models a nd data mining to discover relations and patterns in text,” In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics p.296-303, June 04-09, 2006, New York  M M S u fy an Be g P aral l e l an d Di st ri b u t ed Di sco v er y o f  Association Rules In Artifical Intelligence Application Book  Fadzilah Siraj, Eds\ersity Utara Malaysia 10 X ita o Fa n k os Fe ls v  l y i Ste phe n A  Siv o  M onte C a r l o   SAS® for Monte Carlo studies: a guide for quantitative researchers 11 U  Fa y y e d  G  P i a t e t s k y Sha p iro P. Sm y t h a nd R. U t h u ra s a my  eds.\. “Advances in Knowledge Discovery and Data Mining AAAI Press / The MIT Press, 1996 1 W  J F r aw l e y  G  P i at et sk y S h ap i r o an d C M a t h eu s   Knowledge “Discovery In Databases: An Overview. In Knowledge Discovery In Databases eds. G. Piatetsky-Shapiro, and W. J Frawley, AAAI Press/MIT Press, Cambridge, MA., 1991, pp. 1-30 13 V  K u m a r  a nd M. J o s h i T utor ia l o n H i g h P e r f or m a nc e D a t a  Mining,” In proc. of International Conference on High Performance Computing \(HiPC-98 Dec. 1998  a v i d L  O l s o n a n d D e s h e n g Wu   D eci s i on  m a k i ng  with uncertainity and data mining.” In X. Li, S. Wang and Z.Y Dong\(Eds Lecture notes in Artificial Intelligence pp. 1-9 Berlin: Springer\(2005 15  P  W r ig ht. K now le dg e D i s c ov e r y I n D a ta ba s e s  T ools a n d  Techniques ACM  Crossroads Winter 1998   
408 


 7. Reference    Fetzer C,Hagstedt, K,Felb er P  Autom atic Deteciton  and Masking of Non-Atomic Exception Handling International Conference On Dependable Systems and Networks, \(DSN2003\10-116    Y e n S J, Lee Y S. “Mining Interesting  Associatio n  R u l es  and Sequential Patterns”. International Journal of Fuzzy Systems, 2004-6 \(4   Alasf f ar A  H, Deogun J S. “Concept-b a sed Retr iev a l with Minimal Term Sets”. Foundations of Intelligent Systems: 11th Int’l Symposium, Springer, Poland, 2004 114- 122   Qiu Y ong gang,Frei H P  Concept B a sed Quer y   SIGIR’03,2003:16 0-169   Saltom G  W ong A, Y a ng C  S. “A V ector Sp ace Model for Automation Indexing”. Communications of the ACM 2005, 18\(5\-620   Agrawal R, Srikant R. “Fast Algorithm f or Mining Association Rules in Large Databases.” Proceedings of the 20th International Conference on Very Large DataBases Santiago , Chile , 2004   Park J S. “Using A Hash-Based Method with Transaction Trimming forMining Association Rules.” IEEE Transactions on Knowledge and Data Engineering, 2007   Savasere A, Omiecinski E Navathe S  An Ef ficient Algorithm for Mining Association Rules in Large Databases.” Proceedings of the 21st International Conference on Very large Database, Switzerland, 2002  


              


   


                        





