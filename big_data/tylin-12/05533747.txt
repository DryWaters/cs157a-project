An Improved Algorithm for Frequent Patterns Mining Problem   Thanh-Trung Nguyen Department of Computer Science University of Information Technology Vietnam National University HCM City HCM City, Vietnam e-mail: nguyen_thanh_trung_key@yahoo.com.vn   Abstract Frequent pattern mining is a basic problem in data mining and knowledge discovery. The discovered patterns can be used as the input for analyzing association rules, mining sequential patterns recognizing clusters, and so on. However, discovering fre quent patterns in large scale datasets is an extremely time consuming task Most research in the area of a ssociation rule discovery has focused on the method of efficient frequent pattern discovery, e.g. Park, Chen & Yu \(1995 Omiecinski & Navathe \(1995\Yin \(2000 Han, Lu, Nishio, Tang & Yang \(2001\ll associations that satisfy constraints on support and confidence, once frequent patterns have been identified generating the association rules is trivial. In the last decade various algorithms have been proposed on this problem, e.g maximal pattern mining ñ Grahne & Zhu \(2003 pattern mining ñ Pei, Han & Mao \(2000\Grahne & Zhu 2003\ the most interesting frequent patterns ñ Fu Kwong & Tang \(2000\ Han, Wang, Lu & Tzvetkov \(2002 2005\e, Iwahashi & Yamana \(2004\However, some challenges are still existed and need to be overcome In this paper, a mathematical space will be introduced with some new related concepts and propositions to design a new algorithm for solving frequent patterns mining problem. It is hoped that such an improved algorithm will be simple to implement and more efficient Keywords- association rule; data mining; frequent patterns mining; improved algorithm I  S PACE OF B OOLEAN C HAINS  Let B = {0,1}, B m is the space of m-tuple Boolean chains, whose elements are s = \(s 1 s 2 s m  i   B, i 1,..,m A Definitions A Boolean chain a = \(a 1 a 2 a m said to cover another Boolean chain b = \(b 1 b 2 b m or b is covered  by a, if for each i  1,..,m} the condition b i 1 implies that a i 1. For instance, \(1,1,1,0\overs \(0,1,1,0 Let S be a set of n Boolean chains. If there are k chains in S covering a chain u = \(u 1 u 2 u m u is called a form with a frequency of k in S and [u pattern of S. For instance, if S  0,1,1,1 0,0,1,0\\(0,1,0,1  then u = \(0,1,1,0\m with a frequency 2 in S and [\(0,1,1,0 S  maximal pattern if and only if the frequency k is the maximal number of Boolean chains in S. In the above instance, [\(0,1,1,0 maximal pattern in S The set P of all maximal pattern s in S whose forms are not covered by any form of other maximal pattern, is called a representative set and each element of P is called a representative pattern of S Based on this definition, the following proposition is trivial Proposition 1: Let S be a set of m-tuple Boolean chains and P be the representative set of S, then any couple of two elements in P are not coincided By listing all of n element s of the set S of m-tuple Boolean chains in a Boolean n  m matrix, each element [u  t forms a  maximal rectangle with maximal height of k in S set For instance, if S  0,1,1,1\ \(1,1,1,0 0,0,1,1  then representative set of S consists of five representative patterns: P 1,1,1,0  0,1,1,0 0 0,1 0  0,1 1 1  0,0 1 1  All of five elements of S are listed in a form of Boolean 5  4 matrix, as follows A 5x4 matrix and 0,1,1,0  1 1 1 0  1 11 0 0 1 1 1  0 11 1 1 1 1 0  1 11 0 0 1 1 1  0 11 1 0 0 1 1  0 0 1 1 Figure 1 A 5x4 matrix for [\(0,1,1,0 Figure 1 shows a maximal rectangle with boldface 1s and a maximal height of 4 corresp onding to the pattern 0,1,1,0 a ximal rectangles form ed by elements of P are 1,1,1,0    0 0 1 0  111 0  1 1 1 0 0 1 1 1  0 1 1 1 111 0  1 1 1 0 0 1 1 1  0 1 1 1 0 0 1 1  0 0 1 1 Figure 2 Matrices for 1,1,1,0 0,0,1,0 2010 International Symposium on Comput er, Communication, Control and Automation 978-1-4244-5567-6/10/$26.00 ©2010 IEEE                                                    3CA 2010 


0,1,1,1    0 0 1 1  1 1 1 0  1 1 1 0 0 111 0 1 11 1 1 1 0  1 1 1 0 0 111 0 1 11 0 0 1 1  0 0 11 Figure 3 Matrices for 1,1,1,0 0,0,1,1 A noteworthy case of the above instance: [\(1,1,0,0 is a maximal pattern, but it is not a representative pattern of S because its form is covered by \(1,1,1,0\ pattern 1,1,1,0 B Binary Relations Let a = \(a 1 a 2 a m  b 1 b 2 b m tuple Boolean chains, then a = b if and only if a i b i for any i = 1,..,m. Otherwise, it is denoted by a  b Given patterns and [v; q in S, [u; p is contained   denoted by u; p    if and only if u v and p  q. To negate, the operator ! is use    v; q The minimum chain of a, b Ö denoted by a  b, is a chain z = \(z 1 z 2 z m   b where z k  min\(a k b k m The minimum pattern of [u and v q is a pattern   denoted by [u; p    defined as f o llows: [u; p   v; q] = [w; r] here w = u  v and r = p + q II A N I MPROVED A LGORITHM  A Theorem 1: for Adding a New Element Given S be a set of mtuple Boolean chains and P representative set of S   P and z  S, let u; p   t; p+1 v q     one of the following cases must be satisfied i. [u; p  t and u; p   d; q t = d ii. [u; p   and u; p   d t  d iii. [u; p   and u; p   d  Proof From Proposition 1 obviously u  v. The theorem is proved if th e following claim is true: Let a\d , and t = d b\ d , and t  d c\nd u  d only one of the above statements is correct By the method of induction on the number m of entries of chain, in the first step, we show that the claim is correct if u and v differ at only one k th entry Without loss of generality, we assume that u k 0 and v k 1. The following cases must be true Case 1: z k 0; Then min\(u k z k min\(v k z k  hence t = u  z = \(u 1 u 2 0, .. , u m   z 1 z 2 0 z m  x 1 x 2 0, .. , x m  i min\(u i z i  ik d = v  z = \(v 1 v 2 1, .. , v m   z 1 z 2 0, .. , z m  y 1 y 2 0, .. , y m  i min\(v i z i 1,..,m, ik From the assumption u i v i when i  k thus x i y i so t d. Hence, if u = t then u = d and \(a\orrect. On the other hand, if u  t then u  d, therefore \(c\correct Case 2: z k 1; We have min\(u k z k min\(v k z k  1 and t = u  z = \(u 1 u 2 0, .. , u m   z 1 z 2 1, .. , z m  x 1 x 2 0, .. , x m  i min\(u i z i 1,..,m, ik d = v  z = \(v 1 v 2 1, .. , v m   z 1 z 2 1, .. , z m  y 1 y 2 1, .. , y m  i min\(v i z i 1,..,m, ik So, t  d. If u = t then u  d, thus the statement \(b correct In summary, the above claim is true for any u and v of S that differ only at one entry By induction in the second step, it is assumed that the claim is true if u and v diffe r at r entries, and only one of b\or \(c Without loss of generality, we assume that the first r entries of u and v are different and they differ at \(r+1 entries. Applying the same method in the first step where r 1 to this instance, it is obtained  True statements when u  v and their first r entries are different True statements when u  v and their first r+1 entries are different True statements when combining the two possibilities a a a b\\(b a c c a\\(b b b b c\\(c c a c b\\(c c c c Figure 4 Cases in comparision Therefore, if u and v are different at r+1 entries, only one of the \(a\ \(b\ \(c\ents is correct. The above claim is true, and Theorem 1 is proved B Algorithm: for Finding a New Representative Set Let S be a set that consi sts of n m-tuple Boolean chains, and P the representative set of S. If a m-tuple Boolean chain z is added to S, the following algorithm is used to determine the new representative set of S 012 z  ALGORITHM NewRepresentative P , z Finding new representative set for S when one chain is added to S Input: P is a representative set of S, z: a chain added to S Output: The new rep resentative set P of S 012 z 


1. M  M: set of new elements of P 2. flag1 = 0 3. flag2 = 0 4. for each x  P do 5 q = x  z; 1 6 if q  0 // q is not one chain with all elements 0 7  if x  q then P = P  x   8   q then flag1 = 1 9  for each y  M do 10.    if y  q then 11.     M = M  y   12.     break for 13.    endif 14.    if m  y then 15.     flag2 = 1 16.     break for 17.    endif 18  endfor 19 else 20  flag2 = 1 21 endif 22 if flag2 = 0 then M = M 012   q   23 flag2 = 0 24. endfor 25. if flag1 = 0 then P = P 012   z; 1   26. P = P 012 M 27. return P C Theorem 2: for Finding Representative Sets Let S be a set of n mtuple Boolean chains. The representative set of S is determined by applying NewRepresentative algorithm to each of n elements of S in turn Proof This theorem is proved by induction on the number n of elements of S Firstly, when applying the above algorithm to the set S of only one element, this elemen t is added into P and then P with that only element is the representative set of S Thus, Theorem 2 is proved in the case of n = 1 Next, assume that S consists of n elements, the above algorithm is applied to S, and it is obtained a representative set P 0 of p patterns. Each element of P 0 allows to form a maximal rectangle from S. When adding a new m-tuple Boolean chain z to S, it is necessary to prove that the algorithm allows to find a new representative set P of S 012  z Indeed, with z, some new rectangle forms will be formed along with the existing rectangle forms. But some of these new rectangle forms wh ich are covered by other rectangle forms, the so-called èredundantê rectangles, need to be removed to gain P The fifth statement in the NewRepresentative algorithm shows that the operator  is applied to z and p elements of P 0 to produce p new elements belonging to P. This means z scansê all elements in the set P 0 to find out new rectangle forms when adding z into S Consequently, three groups of 2p+1 elements in total are created from the sets P 0 P, and z To remove redundant rectangles, we have to check whether each element of P 0 is contained by elements of P or not, and elements of P contai n other one, in which z is an element in P Let x be an element of P 0 and consider the form x  z there are two instances: if the form of z covers the one of x then x is a new form; or if the form of x covers the one of z then z is a new form. Anyway, the frequency of the new form is always one unit greater than frequency of the original According to Theo rem 1, with x  P 0 if some pattern w contains x then w must be a new element which belongs to P, and that new element is q = x   whether x is contained by elem ents belonging to P, we do that whether x is contai ned by q or not. If x contained by q it must be removed from the representative \(line 7 In summary, first the algorithm checks whether elements belonging to P 0 is contained by elements belonging to P. Then, the algorithm checks whether elements of P contain one other \(from line 9 to line 18 and whether [z  is contained by elements belonging to P or not \(line 8 Finally, the above NewRepresentative algorithm can be used to find new representa tive set when adding new elements to S III A  C ASE S TUDY  A Association Rule Discovery Advanced technologies have enabl ed us to collect large amounts of data on a continuous or periodic basis in many fields. On one hand, these d ata present the potential for us to discover useful information and knowledge that we could not find before. On the other hand, we are limited in our ability to manually process large amounts of data to discover useful information and knowledge. This limitation demands automatic tools for data mining to mine useful information and knowledge from large amounts of data. Data mining has become an active area of research and development Association rules Ö a data mi ning methodology that is usually used to discover fr equently co-occurring data items, for example, items that are commonly purchased together by customers at grocery stores. Association rule discovery problem were int roduced in 1993 by Agrawal  n, this proble m has received m u ch attention Today the exploitation such ru les is still one of the most popular way for exploiting pa ttern in order to conduct knowledge discovery and data mining Exactly, association rule discovery is the process discovering sets of value of a ttribute appearing frequently in data objects. From the frequent patterns, association rules can be created in or der to reflect the ability of appearing simultaneously the value of attribute in the set of objects. To bring out the meaning, an association rule of X Y reflects the occurrence of the set X conduces to the 


appearance of the set Y. In other words, an association rule indicates an affinity between X antecedent Y  consequent An association rule is accompanied by frequency-based statistics that describe that relationship The two statistics that were used initially to describe these relationships were support and confidence 13 The apocryphal example is a chain of convenience stores that performs a analysis and discovers that disposable diapers and beer are frequ ently bought together  unexpected knowledge is potentially valuable as it provides insight into the purchasing behavior of both disposable diaper and beer purchasing customers for the convenience store chain. So that we can find association rules help identify trends in sales, customer psychology etc. to make strategic layout item, business, marketing, and so on In short, the association rul e discovery can be divided into two parts  Finding all frequent patterns that satisfy minsupport   Finding all association rules that satisfy minimum confidence  Most research in the area of association rule discovery has focused on the sub-pr oblem of efficient frequent pattern discovery \(for exampl e, Han, Pei, & Yin, 2000 Park, Chen, & Yu, 1995; Pei, Han, Lu, Nishio, Tang Yang, 2001; Savasere, Omiecinski Navathe, 1995 When seeking all associations that satisfy constraints on support and confidence, once frequent patterns have been identified, generating the association rules is trivial B The Methods of Finding Frequent Patterns  Candidate Generation Methods: for instance Apriori method proposed by Agrawal in piece of  7 and algori thms rely on Apriori such as AprioriTID, Apriori-Hybrid, DIC, DHP PHP    Without Candidate Generation Methods: for example, Zakiês method relies on IT-tree and intersection part of Tidset s in order to reckon support  Hanês one relies on FP tree in order to exploit frequent patterns, [8   5   or  methods Lcm, DCI, ... presented in [8    Paralleled Methods: instan ces in [11]Ö[12 C Issues Need to be Solved  Working with the varying database is the biggest challenge. Especially, it need not scan again the whole database whenever having need of adding a new element  A number of algorithms are effective, but their basis of mathematics and way of installation are complex  The limit of computer memory. Hence, combining how to store the data mining context most effectively with costing the memory least and how to store frequent patterns is also not a small challenge  Ability of dividing data into several parts for paralleled processing is also concerned Using the NewRepresentative algorithm is making good the above problems D Data Mining Context Let O be a limited non-empty set of invoices, and I a limited non-empty set of goods Let R be a binary relation between O and I so that: for o  O and i  I, \(o, i  R if and only if the invoice o includ es the i-th goods. Then R is a subset of the product set O  I, and the trio \(O, I, R describes a data mining context For example, we have a con text which is illustrated in Table I, consisted of five invoices with the codes o j j 1,..,5 and four kinds of goods i k k = 1,..,4. The corresponding binary relation R is described in Table II which can be represented by a 5  4 Boolean matrix TABLE I I NVOICE D ETAILS  Invoice code Goods code o1 i1 o1 i2 o1 i3 o2 i2 o2 i3 o2 i4 o3 i2 o3 i3 o3 i4 o4 i1 o4 i2 o4 i3 o5 i3 o5 i4 TABLE II B OOLEAN M ATRIX OF D ATA M INING C ONTEXT  i1 i2 i3 i4 o1 1 1 1 0 o2 0 1 1 1 o3 0 1 1 1 o4 1 1 1 0 o5 0 0 1 1 E Applying the NewRepresentative Algorithm The tenor of the algorithm is to find out new rectangle forms created by applying the operator  to corresponding line of data of current step and rectangle forms found in prior steps. Howeve r, in the first step, gro uping the data mining context in order to gain the first evident rectangle forms and their corresponding height is a need. In per step if the rectangle form created newly covers an existent rectangle form then removing the old rectangle form is necessary. Besides, it also needs to remove the newly created rectangle form if they mutually cover In summary, after per step, the set P whose elements differ mutually is created. This means to achieve various maximal rectangle forms in the database, as from the first line of data to the line of data corresponding to the current step 


Based on the Boolean matrix that represents a data mining context in Table II with a minsupp of 40%, after grouping, three evident fo rms or Boolean chains with corresponding frequencies are listed in Table III TABLE III F ORMS AND F REQUENCIES OF THE C ONTEXT  Line Form  Frequency 1. 1110 2 2. 0111 2 3. 0011 1 Let P be the set of maximal rectangle forms of that Boolean matrix. It can be deter mined by following steps Step 1: Consider the 1 st line, put the fo rm \(1,1,1,0 and its frequency into P, we obtain P  1,1,1,0    Step 2: Consider the 2 nd line with a p attern [\(0,1,1,1  In or der to find a new se t P, we have t o determ i ne a minimum pattern of the existing old element [\(1,1,1,0  and this new pattern. If the existing old pattern is contained by the new element pattern, the old element must be removed But [\(1,1,1,0  0,1,1,1  0 1,1 0  therefore [\(1,1,1,0 and 0,1 1 1 are not c ontained by [\(0,1,1,0 P  1,1,1,0 1 ent of P 0,1,1,0 2  0,1,1,1  3  this new pattern is not contained by [\(0,1,1,0  Step 3: Consider line 3, find minimum patterns between [\(0,0,1,1 ee existing old elem ents of P we have 1,1,1,0   0,0,1,1 0,0,1,0 0,1,1,0   0,0,1,1 0,0,1,0 0,1,1,1   0,0,1,1 0,0,1,1 The pattern [\(0,0,1,1 u st be rem oved because it is  contained by [\(0,0,1,1 Sim ilarly, the first pattern 0,0,1,0 oved bec ause this is contained by 0,0,1,0 S o we obtained  P  1,1,1,0 1 0,1,1,0 2 0,1,1,1 3 0,0,1,0 4 0,0,1,1 5 Now, the elements of P are the maximal rectangle forms, representing the Boolean matrix expressing data mining context. The frequent patterns satisfy minsupp 40% \(2/5\ted    i2, i3, i4  2/5  i3, i4  3/5  i1, i2, i3  2/5  i2, i3  4/5   IV C ONCLUSIONS AND F UTURE W ORK  This research proposed the improved algorithm for mining frequent patterns. It ensures a number of the following requests  To solve adding data into the data mining context in which scanning again the database is unnecessary  To install easy and th e low complexity \(n2 2m  where n is number of invoices and m is number of goods. In reality, m is not varying and thus 2 2m is considered a constant  The representative set P created deputies mainly for the data mining context. So, sometimes, to save the capacity of computer memory, it needs only to store the P set instead of the whole context  To surmount the limit of computer memory not enough for storing the enormous data mining context. Because the algorithm allows classifying the context into several parts for processing one by one  Applying simply paralleled strategy to the algorithm A number of problems need studying further  Expanding the algorithm for the circumstance of altering data  Improving the rate of the algorithm  Investigating for applying the algorithm to the real works R EFERENCES   Charu C. Aggarwal, Yan Li Jianyong Wang, Jing Wang Frequent Pattern Mining with Uncertain Data ACM KDD Conference 2009  H. Cheng, X. Yan, J Han, and P. S. Yu Direct Discriminative Pattern Mining for Effective Classification ICDE'08 \(Proc. of 2008 Int. Conf. on Data Engineering  Jiawei H a n and Micheline K a m be r Data Mining: Concepts and Techniques \(2nd edition Morgan Kaufmann Publishers, 2006  Jean-Marc Ada mo Data Mining for Association Rules and Sequential Patterns Springer-Verlag NewYork, Inc., 2006  Quang Tran Minh, Shigeru Oy anagi Katsuhiro YAMAZAK I An Explorative Approach to Mining the k-Most Frequent Patterns  2006  Sun, L and Zhang, X 2004 Efficient frequent pattern mining on web logs in JX Yu et al. \(ed.\Advanced Web Technologies and Applications: Sixth Asia-Pacific Web Conference, APWeb 2004  Nong Ye The Handbook of Data Mining Lawrence Erlbaum Associates Publishers, Mahwah New Jersey, 2003 8 Han, J., Pei J., a nd Yin, Y Mining frequent patterns without candidate generation In proc. of ACM SIGMOD Conference on Management of Data pp. 1-12, 2000  R. Cooley, B. Moshaber, J. Sriva stava Data Preparation for Mining World Wide Web Browsing Patterns Knowledge and Information Systems, 1\(1\ 1999  M  B e r r y  G  L i n of f  Data Mining Techniques - For Marketing Sales and Customer Support John Wiley & Sons, 1997  R. Agrawal and J. C. Shafer Parallel mining of association rules  IEEE Trans. OnKnowledge And Data Engineering, 8:962Ö969 1996  M oham m ed Jav eed Zaki M its unor i Ogihar a Sr inivasan  Parthasarathy, and Wei Li Parallel data mining for association rules on shared-memory multiprocessors Technical Report TR618, 1996  R. Agrawal R. S r ikant Fast Algorithms for Mining Association Rules Proc. of the 20th VLDB Conference, 1994  Br ian F J. M a nly Multivariate Statistical Methods Chapman Hall, 1986  


   developed by Garc\355a and Herrera Table III shows the adjusted p values of those statistical tests. The statistical results also show the effectiveness of the proposed candidate rule addition and genetic lateral tuning Table IV shows the average number of rules in the obtained classifiers. From Ta ble IV, we can see that each classifier has a very small number of rules. There are three interesting observations. One is that the number of rules was increased when we compared Pa reto with EC and ECW. This means that the necessary rules were successfully added to the candidate rule set and selected by genetic rule selection Another interesting observation is that the number of rules was reduced after genetic lateral tuning. This is a positive side effect of genetic lateral tuning. The other interesting observation is that the number of rules decreased as the imbalance rate became larger. We need further investigation in a future study  TABLE  IV N UMBER OF F UZZY R ULES IN THE O BTAINED C LASSIFIERS    0.0 Pareto EC ECW Pareto* EC* ECW bal0 8.3 14.8 25 5 6.5 12.3 24.5 bal2 8.0 14.8 27 5 4.6 12.5 25.8 glass1 6.0 10.3 11.2 5.1 8.8 9.8 pima 7.7 12.5 15 4 7.1 11.4 13.9 glass0 3.0 2.8 2.8 2.9 2.6 2.4 newt2 5.3 6.8 6 6 4.6 5.2 5.4 haberman 7.3 8.4 8.1 6.6 7.5 7.2 newt1 3.9 4.4 4 5 3.4 4.2 4.5 newt0 3.8 3.8 3 8 2.4 2.1 2.2 glass5 3.6 4.6 5.3 2.7 4.5 5.0 glass2 3.6 3.5 3.5 2.7 2.9 2.9 glass3 3.2 4.1 5.1 3.1 3.8 4.8 glass4 3.7 4.6 4.3 2.7 3.6 3.0   TABLE  V T EST D ATA A CCURACY OF THE O BTAINED C LASSIFIERS    0.0 Pareto EC ECW Pareto* EC* ECW bal0 88.35 91.51 89.22 92.26 92.68 89.79 bal2 91.58 92.43 91.07 94.07 93.03 91.68 glass1 63.97 68.35 67.65 72.19 76.62 74.45 pima 73.99 72.14 73.51 74.19 73.36 72.28 glass0 62.85 68.10 68.04 70.75 73.40 72.53 newt2 94.77 92.48 92.98 97.09 96.44 95.57 haberman 55.48 57 30 56.75 59.76 60.20 60.37 newt1 95.28 95 56 94.86 95.19 97.04 96.67 newt0 96.14 93.36 95.03 96.31 96.03 96.59 glass5 93.53 93 05 93.14 92.05 91.10 93.69 glass2 49.62 49 15 48.90 49.40 50.44 50.56 glass3 80.71 81.26 83.59 81.76 87.00 82.32 glass4 69.46 82 17 78.99 72.17 88.32 91.58 Average 78.13 79.76 79.52 80.55 82.74 82.16 Ave. Rank 4.54 4.31 4.62 3.00 2.23 2.31 Table V shows the average test data accuracy over 30 runs for each data set and the averag e test data accuracy over all the data sets as Table II. From Table V, it seems that there is a less effect of candidate rule addition comparing the training data accuracy in Table II. On the other hand, we can see that genetic lateral tuning can improve the generalization ability for the test data. We performed the statistical tests in the same manner as the training data accuracy. The Friedman\222s test rejected the null hypothesis. Thus, we performed Nemenyi\222s test, Holm\222s test, Shaffer\222s test, and Bergmann-Hommel procedure. Table VI shows the adjusted p values of those statistical tests. There are clear statistically significant differences between Pareto and all the approaches with genetic lateral tuning. We can also see that the combinational use of candidate rule addition and genetic lateral tuning can improve the generalization ability on the test data very well e.g., see the p values of Pareto vs. ECW* and Pareto vs EC B  Comparison among Several the Dominance Margins We examined the effects of the dominance margin. Figs. 5 and 6 show the overall training data accuracy and test data accuracy over all the data sets. While a larger value improved the training data accuracy thanks to a larger number of candidate rules, the test da ta accuracy was not improved by the larger value. Comparing the results in the same we can see a clear improvement by genetic lateral tuning for both training and test data accuracy  Pareto EC ECW Pareto EC ECW  0.00 0.01 0.05 0.10 80 84 88 92 Accuracy  Fig. 5.  Average training accuracy over all the data sets Pareto EC ECW Pareto EC ECW  0.00 0.01 0.05 0.10 76 79 82 85 Accuracy  Fig. 6.  Average test accuracy over all the data sets 


    Figs. 7 and 8 show the average rank for the training data accuracy and the test data accuracy. We can observe almost all of the same effects of candidate rule addition and lateral tuning as in the case of 0.0  Pareto EC ECW Pareto EC ECW  0.00 0.01 0.05 0.10 0 2 4 6 Average rank  Fig. 7.  Average rank for the training data accuracy over all the data sets A smaller rank is better Pareto EC ECW Pareto EC ECW  0.00 0.01 0.05 0.10 0 2 4 6 Average rank  Fig. 8.  Average rank for the test data accuracy over all the data sets A smaller rank is better  C  Comparison between Low and High Imbalanced Data We divided the data sets used in our experiments into two subsets: Low imbalance data sets \(IR < 3.0\gh imbalanced data sets \(IR > 3.0\gs. 9 and 10 show the average rank for the test data accuracy over the low imbalanced data sets and the high imbalance data sets respectively. We can see that the positive effect of genetic lateral tuning became weaker as became larger for the low imbalance data sets. On the other hand, the approaches with candidate rule addition and ge netic lateral tuning \(i.e., EC and ECW*\kept good ranking in almost all cases. This observation may indicate that the proposed extensions are effective especially for the high imbalanced data sets V   C ONCLUSION  In this paper, we proposed two extensions of our genetic fuzzy rule selection for designi ng more accurate classifiers One extension is to add compatible rules with misclassified patterns into candidate rules for genetic fuzzy rule selection The other extension is to tune membership functions by genetic lateral tuning after genetic fuzzy rule selection Experimental results showed that the candidate rule addition can improve the training data accuracy. This is because the possibility that the misclassified training patterns are classified by additional rules becomes high. Experimental results also showed that the genetic lateral tuning can improve the test data accuracy as well as the training data accuracy, since fuzzy membersh ip functions are properly adjusted according to the pattern distributions. The combination of the proposed two extensions would be the best choice As a future study, we will examine the effects of the proposed extensions using a large number of data sets TABLE  VI A DJUSTED P VALUES OBTAINED BY N EMENYI 222 S T EST   H OLM 222 S T EST   S HAFFER 222 S T EST  AND B ERGMANN H OMMEL P ROCEDURE FOR THE T EST D ATA A CCURACY OBTAINED IN OUR C OMPUTATIONAL E XPERIMENTS  i Hypothesis Unadjusted p  p Neme  p Holm  p Shaf  p Berg  1 Pareto vs. ECW 3.1 x 10 10 4.8 x 10 9 4.8 x 10 9 4.8 x 10 9 4.9 x 10 9  2 Pareto vs. EC 1.6 x 10 7 2.4 x 10 6 2.2 x 10 6 1.6 x 10 6 1.6 x 10 6  3 EC vs. ECW 1.6 x 10 4 0.0024 0.0021 0.0016 0.0016 4 Pareto vs. ECW 8.0 x 10 4 0.0119 0.0095 0.0080 0.0056 5 Pareto* vs. ECW 0.0012 0.0173 0 0127 0.0116 0.0081 6 Pareto vs. Pareto 0.0024 0.0355 0 0237 0.0237 0.0142 7 ECW vs. ECW 0 0033 0.0500 0.0300 0.0237 0.0142 8 EC vs. EC 0.0064 0.0963 0.0514 0.0449 0.0385 9 Pareto vs. EC 0 0119 0.1781 0.0831 0.0831 0.0475 10 Pareto* vs. EC 0 0277 0.4156 0.1662 0.1662 0.0831 11 ECW vs. EC 0.0592 0.8876 0.2959 0.2367 0.1183 12 EC* vs. ECW 0 2945 1.0000 1.0000 1.0000 1.0000 13 EC vs. ECW 0.4017 1.0000 1.0000 1.0000 1.0000 14 EC vs. Pareto 0 6002 1.0000 1.0000 1.0000 1.0000 15 ECW vs. Pareto 0 7532 1.0000 1.0000 1.0000 1.0000 


   including data sets with more than two classes. We also have to compare the proposed method with other learning algorithms. There are still a lot of things we have to improve in the proposed extensions, es pecially the search space and genetic operations in the lateral tuning. The discussion on the tradeoff relation among accuracy, complexity, and interpretability is also another important future research issue  Pareto EC ECW Pareto EC ECW  0.00 0.01 0.05 0.10 0 2 4 6 Average rank  Fig. 9.  Average rank for the test data accuracy over low imbalanced data sets A smaller rank is better Pareto EC ECW Pareto EC ECW  0.00 0.01 0.05 0.10 0 2 4 6 Average rank  Fig. 10.  Average rank for the test da ta accuracy over high imbalanced data sets. A smaller rank is better R EFERENCES  1  R. J. Bayardo Jr. and R. Agrawal 223Mining the most interesting rules,\224 Proc. of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ACM Press, New York, NY USA, pp. 145-154, 1999 2  B. Liu, W. Hsu, and Y. Ma, \223Integ rating classification and association rule mining,\224 Proc. of 4th International Conference on Knowledge Discovery and Data Mining New York, AAAI Press, August 27-31 1998, pp. 80-86 3  X. Yin and J. Han, \223CPAR: Cl assification based on predictive association rules,\224 Proc. of SIAM International Conference on Data Mining, San Francisco CA, May 2003, pp. 331-335 4  H. Ishibuchi, K. Nozaki, N. Yamamoto, and H. Tanaka, \223Selecting fuzzy if-then rules for classi fication problems using genetic algorithms,\224 IEEE Trans. on Fuzzy Systems vol. 3, no. 3, pp. 260-270 1995 5  H. Ishibuchi, T. Murata, and I B. Turksen, \223Single-objective and two-objective genetic algorithms for selecting linguistic rules for pattern classification problems,\224 Fuzzy Sets and Systems vol. 89, no. 2 pp. 135-150, 1997 6  H. Ishibuchi, T. Nakashima, and T Murata, \223Three-objective genetics based machine learning for linguistic rule extraction,\224 Information Sciences vol. 136, no. 1-4, pp. 109-133, 2001 7  H. Ishibuchi and T. Yamamoto, \223F uzzy rule selection by multi objective genetic local search algorith ms and rule evaluation measures in data mining,\224 Fuzzy Sets and Systems vol. 141, no. 1, pp. 59-88 2004 8  H. Ishibuchi, I. Kuwajima, and Y Nojima, \223Prescreening of candidate rules using association rule mining a nd Pareto-optimality in genetic rule selection,\224 Proc. of 11th International Conference on Knowledge Based and Intelligent Informa tion and Engineering Systems pp 509-516, 2007 9  I. Kuwajima, Y. Nojima, and H. Ishibuchi, \223Obtaining accurate classifiers with Pareto-optimal and near Pareto-optimal rules,\224 Proc. of 11th International Symposium on Artificial Life and Robotics pp 195-198, Beppu, Japan, January 31-February 2, 2008 10  H. Ishibuchi and T. Nakashima, \223E ffect of rule weights in fuzzy rule-based classification systems,\224 IEEE Trans. Fuzzy Systems vol. 9 no. 4, pp. 506-515, August 2001 11  H. Ishibuchi and T. Yamamoto, \223Rule weight specification in fuzzy rule-based classification systems,\224 IEEE Trans. on Fuzzy Systems vol 13, no. 4, pp 428-435, August 2005 12  J. M. Alonso and L. Magdalena, \223A n interpretability-guided modeling process for learning comprehensible fuzzy rule-based classifiers,\224 Proc of 9th International Conference on Intelligent Systems Design and Applications pp. 432-437, 2009 13  K. Deb, A. Pratap, S. Agarwal, a nd T. Meyarivan, \223A fast and elitist multiobjective genetic algorithm: NSGA-II,\224 IEEE Trans. on Evolutionary Computation vol. 6, no. 2, pp. 182-197, April 2002 14  R. Alcal\341, J. Alcal\341-Fdez, and F Herrera, \223A proposal for the genetic lateral tuning of linguistic fuzzy syst ems and its interaction with rule selection,\224 IEEE Trans. on Fuzzy Systems vol. 15, no. 4, pp. 616-635 2007 15  J. Alcal\341-Fdez, R. Alcal\341, M. J. Gacto, and F. Herrera, \223Learning the membership function contexts for mining fuzzy association rules by using genetic algorithms,\224 Fuzzy Sets and Systems vol. 160, pp 905-921, 2009 16  M. Kaya and R. Alhaji, \223Utilizi ng genetic algorithms to optimize membership functions for fuzzy weighted association rules mining,\224 Applied Intelligence vol. 24, no. 1, pp. 7-15, February 2006 17  W. Wang and S. M. Bridges, \223G enetic algorithm optimization of membership functions for mining fuzzy association rules,\224 Proc. of the 7th International Conference on Fuzzy Theory & Technology  pp.131-134, Atlantic City, NJ February 27-March 3, 2000 18  J. Dem\232ar, \223Statistical comparisons of classifiers over multiple data sets,\224 Journal of Machine Learning Research vol. 7, pp. 1-30, 2006 19  M. Friedman, \223The use of ranks to avoid the assumption of normality implicit in the analysis of variance,\224 Journal of the American Statistical Association vol. 32, pp. 675-701, 1937 20  P. B. Nemenyi Distribution-free Multiple Comparisons PhD thesis Princeton University, 1963 21  S. Holm, \223A simple sequentially re jective multiple test procedure,\224 Scandinavian Journal of Statistics vol. 6, pp. 65-70, 1979 22  J. P. Shaffer, \223Modified seque ntially rejective multiple test procedures,\224 Journal of the American Statistical Association vol. 81 pp. 826-831, 1986 23  G. Bergmann and G. Hommel, \223Impr ovements of general multiple test procedures for redundant systems of hypotheses,\224 In Bauer, Hommel and Sonnemann, eds  Multiple Hypotheses Testing Springer, Berlin, pp 100-115, 1988 24  S. Garc\355a and F. Herrera, \223An exte nsion on \221Statistical comparisons of classifiers over multiple data sets\222 for all pairwise comparisons,\224 Journal of Machine Learning Research vol. 9, pp. 2677-2694 December 2008 The source code is available at http sci2s.ugr.es/keel/multipleTest.zip   


Application of Chaotic Particle Swarm Optimization Algorithm in Chinese Documents Classification 763 Dekun Tan Qualitative Simulation Based on Ranked Hyperreals 767 Shusaku Tsumoto Association Action Rules and Action Paths Triggered by Meta-actions 772 Angelina A. Tzacheva and Zbigniew W. Ras Research and Prediction on Nonlinear Network Flow of Mobile Short Message Based on Neural Network 777 Nianhong Wan, Jiyi Wang, and Xuerong Wang Pattern Matching with Flexible Wildcards and Recurring Characters 782 Haiping Wang, Fei Xie, Xuegang Hu, Peipei Li, and Xindong Wu Supplier Selection Based on Rough Sets and Analytic Hierarchy Process 787 Lei Wang, Jun Ye, and Tianrui Li The Covering Upper Approximation by Subcovering 791 Shiping Wang, William Zhu, and Peiyong Zhu Stochastic Synchronization of Non-identical Genetic Networks with Time Delay 794 Zhengxia Wang and Guodong Liu An Extensible Workflow Modeling Model Based on Ontology 798 Zhenwu Wang Interval Type-2 Fuzzy PI Controllers: Why They are More Robust 802 Dongrui Wu and Woei Wan Tan Improved K-Modes Clustering Method Based on Chi-square Statistics 808 Runxiu Wu Decision Rule Acquisition Algorithm Based on Association-Characteristic Information Granular Computing 812 JianFeng Xu, Lan Liu, GuangZuo Zheng, and Yao Zhang Constructing a Fast Algorithm for Multi-label Classification with Support Vector Data Description 817 Jianhua Xu Knowledge Operations in Neighborhood System 822 Xibei Yang and Tsau Young Lin An Evaluation Method Based on Combinatorial Judgement Matrix 826 Jun Ye and Lei Wang Generating Algorithm of Approximate Decision Rules and its Applications 830 Wang Yun and Wu-Zhi Qiang Parameter Selection of Support Vector Regression Based on Particle Swarm Optimization 834 Hu Zhang, Min Wang, and Xin-han Huang T-type Pseudo-BCI Algebras and T-type Pseudo-BCI Filters 839 Xiaohong Zhang, Yinfeng Lu, and Xiaoyan Mao A Vehicle License Plate Recognition Method Based on Neural Network 845 Xing-Wang Zhang, Xian-gui Liu, and Jia Zhao Author Index 849 
xiii 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





