A Novel Fuzzy Positive and Negative Association Rules Algorithm HU Kai China ship Development & Design Center Wuhan, China iamhukai@gmail.com   Abstract According to the existing mining algorithm of fuzzy association rules, a novel fuzzy positive and negative association rules algorithm will be proposed in this paper. We focus on the membership function of fuzzy set and minimum support parameters of positive and negative association rules and adopt a method that selects parameters automatically which is based on the k-means clustering. Besides, multi-level fuzzy support and correlation coefficient are chosen to restrain the quantity and quality of rules generated by the algorithm Finally the validity and accuracy of the algorithm are proved by an experiment Keywords- data mining; fuzzy association rules; membership function; multi-level fuzzy support; correlation coefficient I  INTRODUCTION Association rules mining, which aims to discover relevant valuable knowledge describing interrelation between data items from huge numbers of data, is a extremely important research subject of data mining domain.  Since it was first proposed in 1993 by Rakesh Agrawa s o ciation ru les e x traction  h a s recei v e d widespread concern and in-depth research both in algorithms and efficiency 2-3 For numeric databases quantitative association rules mining  is a significant branch. Since the fuzzy association rules mining technology was put forward combined with fuzzy set theory 6 this field has got more and more attention. However, the selection of membership functions, a key issue of the fuzzy association rules mining technology, gets little research. In most cases, the methods developed by experts are adopted by making use of a priori knowledge, but there is a certain blindness, which will cause an influence on the accuracy and efficiency of the final rules extraction. In addition, most studies focus on the extraction of positive association rules such as X 004\010 Y no matter in the traditional Boolean association rules mining or quantitative association rules mining and many mature algorithms come into being Negative association rules in the form of X 004\010 Y, ¬X 004\010 Y or X 004\010 Y have been received widespread concern  but such issues as the parameter selection of support degree etc call for further study. Negative association rules stand for a negative correlation knowledge of things and they also plays a very important role compared with the positive association rules According to current research status, a new fuzzy positive and negative association rules mining algorithm is proposed, which determines the membership functions based on k-means clustering method. And its advantage lies in discovering the cluster center of data set guidelessly in the absence of sufficient prior knowledge, thereby identifying the membership functions reasonably and avoiding wrong mining results caused by false parameter selection of the membership functions. Meanwhile, it adopts the multiple fuzzy supports in the fuzzy association rules for the first time and introduces the correlation coefficient criterion, controlling the quality of rules effectively and ensuring the accuracy and efficiency of algorithms The remainder of this paper is organized as follows: An introduction of positive and negative association rules and multiple support degree theories are given in Section 2. In Section 3, the fuzzy association rules and the selection of membership functions are introduced. In Section 4, the new fuzzy positive and negative association rules mining algorithm is presented. After that, Section 5 introduces an experimental simulation of the proposed algorithm. Finally some conclusions are drawn in Section 6 II  MULTI-SUPPORT  POSITIVE  AND  NEGATIVE  ASSOCIATION  RULES Suppose that I={i 1 i 2 i m is a set of binary attributes and T={t 1 t 2 t n is a set of n data records, where ti 002 I namely, each record in T can be viewed as a subset of I. The association rule \(AR\he logical implication of such form as X 004\010 Y, where X 003 I, Y 003 I, and X 001 Y 002N When the support degree and the confidence degree of the rule are greater than a given minimum support and minimum confidence threshold separately, then it is considered as a correct one. And its support degree is supp\(X 004\010 Y\|T X 004 Y T|, while the confidence degree is conf\(X 004\010 Y 000\014\000 T X 004 Y T X where |T X represents the count of transactions containing X in T. When a considered thing does not happen, that is to say, ¬X shows that itemset X does not occur, bringing about such negative rules as X 004\010 Y, ¬X 004\010 Y and ¬X 004\010 Y. The support degree and confidence degree can be counted by the corresponding positive item sets s\(¬X\=1-s\(X s\(X 004 Y\\(X\\(X 004 Y s\(¬X 004 Y\\(Y\\(X 004 Y s\(¬X 004 Y\s\(X\-s\(Y\\(X 004 Y c\(X 004\010 X 004\010 Y 
2010 Ninth International Symposium on Distributed Computing and Applications to Business, Engineering and Science Unrecognized Copyright Information DOI 10.1109/DCABES.2010.163 623 


c\(¬X 004\010 Y s\(X 1 Y    004  X s Y s  c\(¬X 004\010 Y   1       1 X s Y X s Y s X s  004    1-c\(¬X 004\010 Y In fact, negative association rules exist not only in infrequent itemSets \(inFS\ut also exists in the frequent itemSets \(FS\ negative rules need to be mined in inFS while in FS, both positive and negative rules need to be mined. In theory, inFS can be regarded as a complement of FS, but, in practice use, it is often very large, generating plenty of negative rules, which are not all meaningful. The size of inFS can be restraint by 2-level Supports  and multiple level minimum supports\(MLMS  in which the later establishes different minimum supports for those candidate itemsets which have different lengths ms\(1 000 ms\(2 000  000 ms\(n 000 ms>0, where ms represents the threshold of inFrequent itemSets and ms\( k\tands for the minimum support of frequent K-itemsets. If s\(X 000 ms\(k then X is a frequent itemset. But if s\(X\ms\(k\ and s\(X\ms, then X is an inFrequent itemset. This model MLMS can control the number of the frequent and inFrequent itemSets by establishing different m\(k The correlation between the preceding paragraph X and consequent paragraph Y of association rules can be measured as follows        Y s X s Y X s corr y x 004  If corr x,y 1, then X and Y are positively correlated, that is, the occurrence of X will promote the occurrence of Y. But if corr x,y 1, then X and Y are independent, that is, the occurrence of X and Y has nothing to do with each other. If corr x,y 1, then X and Y are negatively correlated, that is, the occurrence of X conflicts with the occurrence of Y. The correlation between negative itemsets can be counted by corresponding positive itemsets When corrx,y>1   Y X s 004      Y s X s  so           Y s X s Y s Y X s Y s   004   while       Y X s Y X s Y s 004   004         1         X s Y s X s Y s Y s X s Y s       hence       Y X s Y s X s 004     thus 1          004   Y s X s Y X s corr y x  And in the same way, we can infer that corr x,¬y 1 corr x,¬y 1 When corr x,y 1, Certainly, the greater corr x,y the stronger the positive correlation between X and Y. While when corr x,y 1, the closer corr x,y comes to zero, the stronger the negative correlation between X and Y III  FUZZY  ASSOCIATION  RULES  When datasets containing continuous numeric attributes emerge during association rules mining, the method discretizing data is usually adopted. As to a specified continuous numeric attribute Q, it will be divided into p intervals, and then the new generated attributes Q1, Q2 Qp are used to replace the orig inal attribute Q. However, a simple division of continuous attributes into several intervals may lead to so-called "hard boundary" issues, such as the record "Those who are 31 years old age 31 spend 6 hours online per day” does not support the rule Ag 004\010 Average daily online duration 5 However in fact there is little age difference between 31 and 30, in other words, such fuzzy association rules as "young adults are more likely to spend a long time online" seem more correct and more instructive 11 For the continuous numeric dataset T=\(t 1 t 2 t n and I=\(i 1 i 2 i m e set containing all the attributes of T each continuous attribute i k has a corresponding fuzzy set     2 1 l i i i i k k k k f f f F  where j i k f represents the j-th fuzzy set. For example, the continuous attribute "income" has three fuzzy sets: high, middle and low, so it can be expressed as "F Income = {high, middle, low The form 000 000 000 000 000 B Y A X   is a fuzzy association rule which means that when attribute X is A, then it can inferred that attribute Y is B. Where X 003 I and Y 003 I are the sets of attributes, A and B are corresponding with the fuzzy sets of attributes which belong to X and Y respectively. The standards for valid fuzzy rules are the same as traditional Boolean rules. Namely, fuzzy support and fuzzy confidence of the fuzzy rules should be greater than the given thresholds of minimum fuzzy support and minimum fuzzy confidence respectively. However, the calculation of fuzzy support and fuzzy confidence is different from that of traditional Boolean rules. In the mining of traditional Boolean association rules and quantitative association rules the transaction of a record either appears or not, and the degree of transaction support a property can be counted by the times which the attribute appears in all the transactions While in the mining of fuzzy association rules, fuzzy support can be counted by the membership of data items to each attribute, and described by the form of probability The support of 000 000 A X  is calculated as follow        T x t S j i a X x T t A X j j i 000 005 006 006 000 000  007  Where t i  x j   represents the value of the j-th attribute in the i-th record    j i a x t j 007 stands for such fuzzy support of t i  x j  that attribute x j is equal to a j and 005 006     j i a X x x t j j 007  represents the result which is multiplied by the fuzzy support of each attribute x j Here, T-mode T P x,y\ is adopted instead of other T-modes, such as T M x,y\=min\(x,y T W x,y\=max\(x+y-1,0 etc for the reason that operation 002I  not only considers the membership of all attributes, but also the convenient calculation of the algorithm The fuzzy confidence of 000 000 000 000 000 B Y A X   is calculated as follows 
624 


000 005 000 005 006 006 006 006 000 000 000 000 000²\000 000 000 000¢\000                j i c X x T t k i c Z z T t A X C Z B Y A X z t z t S S C k j i k k i 007 007  Where 000 000 C Z  is the frequent fuzzy item set Z X 003  Y=Z-X C A 003 B=C-A How to determine a Membership function \(MF\f the sample data is a key issue of fuzzy association rules. So the determination of the parameters of MF is particularly important. This paper adopts the method based on k-means clustering, which identifies the cluster center of data sets by self-learning, and hence, determines the parameters of MF As to the data set T, first of all, cluster p continuous numeric attributes that need discretization into specified q clusters by k-means, and generate a p * q cluster center matrix C, where the j-th center point of the i-th attribute can be represented by C i,j Then, combine with MF of triangle to find out the fuzzy MF of each attribute. Attribute i k has q cluster centers described as      2  1  q i i i k k k C C C the fuzzy set MF of which is shown as  Figure 1 1 C i k q C i k  2  C i k  1   Fig.1  MF of i k determined by the clustering center The i k attribute’s fuzzy set     2 1 q i i i i k k k k f f f F  where 1 2 1 1  2 1 2 2 1 2 1  0 k k k kk kk k k k ki i k iiki ii i i ik iC C i f CiC CC CC Ci 000 010 000 000  010 000  000 000  000  1 1  2  2 1 1  2 3 2 2 3 2 3 3 2 3 1   0 or k kk kk kk k kk k kk kk kk i k iki ii ii i k iki i ii ii ikki C i CiC CCCC C i CiC f CC CC CiiC 000 010\010 000  000 000 000 010  000  000 000 000  000 000  1 1 1  1,1  0  1 k k k kk kk k k k iq k iq q k iiqkiq iq iq iq iq kiq Ci C i f CiC CC C C iC     000 010 000 000  010 000  000 000 011 000  IV  FUZZY  POSITIVE  AND  NEGATIVE  ASSOCIATION  RULES  ALGORITHM This paper proposes a new mining fuzzy positive and negative association rules algorithm, which can be divided into two steps. Firstly, adopt k-means clustering algorithm to process continuous numeric attributes of data sets, and determine the MF of each attribute by cluster center points Secondly, convert original data set T into fuzzy data set T according to the generated MFs. This step can be viewed as a pre-processing stage. The algorithm is shown as follows Input: original data set -T, the number of clusters -K Call k-means to cluster data set T, and get k cluster centers {C 1 C 2 C k  For each attribute A i do Determine M i   the MF of attribute A i by C 1 C 2 C k  End for For each record t i 015 T Make the record t i fuzzy by M i End for Output: fuzzy data set T  MS_FPNAR is adopted in the second step. Multi-level Fuzzy support is used to generate Frequent Fuzzy itemSet and infrequent Fuzzy itemSet in the fuzzy data set T Then fuzzy positive and negative association rules, which are greater than the minimum fuzzy confidence, are mined in frequent fuzzy itemsets and fuzzy negative association rules are mined in infrequent fuzzy itemsets combined with correlation coefficient  corr x,y constraints The algorithms for generating frequent fuzzy itemsets and infrequent fuzzy itemsets are shown as follows Input: fuzzy data set T’, Multi-level minimum Fuzzy Support mfs\(k\, Infrequent fuzzy itemset threshold mfs C 1 a fuzzy itemset which has a support 011 mfs FFS 1 a fuzzy itemset which has a support 011 mfs\(1\ in C 1  inFFS 1 C 1 FFS 1  For \(k=2;C k-1 NULL;k C k apriori_gen\(C k-1 mfs For each c 006 C k If c contains multiple fuzzy sets in the same attribute then delete c from C k  C t subset\(C k t Calculate the support of each c 006 C t  End for C k a fuzzy itemset which has a support 011 ms in C t  FFS k a fuzzy k itemset which has a support 011 mfs\(k\ in C k  inFFS k C k FFS k  add FFS k into FFS, and add inFFS k into inFFS End for Output: frequent fuzzy itemset FFS, infrequent fuzzy itemset inFFS 
625 


The algorithm which is mining fuzzy positive and negative association rules in FFS and inFFS as follows Input: frequent fuzzy sets - FFS, infrequent fuzzy sets inFFS, minimum fuzzy confidence - mfc, minimum correlation information number- min_corr For each fuzzy set 000 000 C Z  in FFS Calculate the correlation coefficient of 000 000 000 000 B Y A X   K When X 004 Y=Z, A 004 B=C and X 012 Y 013  A 012 B 013  If the correlation coefficient of 000 000 000 000 B Y A X   K min_coor If the fuzzy confidence     000 000 000 000 000 B Y A X c 011 mfc then add 000 000 C Z  into FPAR If the fuzzy confidence     000 000  000 000 000  B Y A X c 011 mfc then add 000 000 C Z  into FNAR If the correlation coefficient of 000 000 000 000 B Y A X   K 1/min_corr If the fuzzy confidence     000 000  000 000 000 B Y A X c 011 mfc then add 000 000 C Z  into FNAR If the fuzzy confidence     000 000 000 000 000  B Y A X c 011 mfc then add 000 000 C Z  into FNAR End for For each fuzzy set 000 000 C Z  in inFFS Calculate the correlation coefficient of 000 000 000 000 B Y A X   K When X 004 Y=Z, A 004 B=C and X 012 Y 013  A 012 B 013  If the correlation coefficient of 000 000 000 000 B Y A X   K min_coor If the fuzzy confidence     000 000  000 000 000  B Y A X c 011 mfc then add 000 000 C Z  into FNAR If the correlation coefficient of 000 000 000 000 B Y A X   K 1/min_corr If the fuzzy confidence     000 000  000 000 000 B Y A X c 011 mfc then add 000 000 C Z  into FNAR If the fuzzy confidence     000 000 000 000 000  B Y A X c 011 mfc then add 000 000 C Z  into FNAR End for Output 031 fuzzy positive association rules -PAR, fuzzy negative association rules -NAR V  EXPERIMENTAL  RESULTS A standard data set consisting of 150 samples, which belongs to three different kinds of rocks separately and contains data with four oxide components, is used in the experiment. Moreover, each sample has a six-dimensional attribute, where the first one represents the sample number and the second to the fifth one represents the content of SiO 2 Al 2 O 3 MgO, Fe 2 O 3 respectively, while the sixth stands for the rock class number which the sample belongs to. Besides, the second to the fifth dimension attribute are continuous numeric variables. The boundary points of which are decided by the domain experts based on the prior knowledge before the modeling of association rules mining However, due to the diversity, variability and complexity of the geological phenomena and conditions, there is plenty of uncertainty and imprecision. The inaccuracy in choosing boundaries may directly affect the quality of extracting rules According to the proposed algorithm, the second to fifth dimension of the original data set are firstly clustered with k-means, and discretized to two-dimensional fuzzy attribute high, low}. As a result, Two cluster center points are generated center1={50.0566,33.6981,15.6038,2.9057 center2={63.0103,28.8660,49.5876,16.9588 The membership functions of the content of SiO2 Al2O3, MgO, Fe2O3 are shown respectively as figure 2 to figure 5. According to these MFs, a new fuzzy sets T’ will be generated if the value of each attribute of every sample in the original data set T is changed into a form of a fuzzy set 63.0103 50.0566 1 SiO 2  33.6981 28.8660 1 Al 2 O 3   Fig.2 the MF of SiO2  Fig.3 the MF of Al2O3 49.5876 15.6038 1 MgO 16.9588 2.9057 1 Fe 2 O 3   Fig.4 the MF of MgO Fig.5 the MF of Fe2O3 In the following, a comparison will be done to T’, the generated fuzzy set, with traditional fuzzy association rules algorithm and   MS_FPNAR algorithm proposed in the paper respectively, the results of which are shown as figure 1 to figure 10 and table 6 to table 7, where the multi-level fuzzy support vector is expressed by mfs\(k\mfs\(1\mfs\(2\mfs\(3\mfs\(4\mfs\(5\mfs d t h e  minimum fuzzy confidence is expressed as mfc. The minimum correlation coefficient is stated as min_corr. The number of the fuzzy positive association rule is expressed in PAR. The number of the negative association rules is expressed in two different variables: NAR1 and NAR2. The difference between the variables is the expressions. NAR1 is expressed as 000 000  000 000 000 B Y A X   and 000 000 000 000 000  B Y A X    while NAR2 is expressed as 000 000  000 000 000  B Y A X    Rules_num is stated as the total number of the positive and negative association rules 
626 


Mine the fuzzy association rules by the traditional fuzzy association rules algorithm at first. Then set mfc = 0.6 and min_corr = 1.Use the uniform minimum mfs and set mfs equal to 0.3, 0.2, 0.16, 0.12, 0.10 and 0.08 respectively. It can be seen that, when mfs gets a larger value, as shown in Table 1, the number of the rules extracted from the frequent itemsets is limited. Take for example, while mfs\(k\ = 0.3 small amounts of frequent fuzzy two itemsets, three itemsets and four itemsets are generated. Some meaningful rules especially some longer rules fail to be mined for their higher supports. However, in order to mine more long rules turning more four itemsets and five itemsets into the frequent fuzzy itemsets to get smaller fuzzy supports, which turns out the other way. Take Table 6 for example, when mfs = 0.08, moderate frequent fuzzy five itemsets are generated, but amounts of frequent fuzzy two itemsets and three itemsets are extracted and the number of mined fuzzy rules which may contain lots of redundant and meaningless rules is multiple. The relation between the value of the mfs in traditional fuzzy association algorithm and the rule number of fuzzy association is shown in figure 6. Thus the uniform mfs used in the traditional algorithm has a great effect on the accuracy of the final rule mining if its value is either too big or small. How to find a more moderate fuzzy support threshold is a common problem To solve this problem effectively, a method based on multiple minimum fuzzy supports is adopted. Suppose mfs k\, 0.2, 0.16, 0.12, 0.1, 0 s s h o w n  i n t a bl e 7 each layer has the same rules as the corresponding layer from table 2 to table 5 when k=2,3,4,5 respectively. In this way, not only the large number of meaningless rules is avoided, but some meaningful rules also cannot be removed However, it doesn’t take into account the negative association rules generated in the infrequent itemsets. By comparison of table 7 and table 8, certain infrequent itemsets inFFS\, in which some meaningful negative rules could be generated, can be produced by setting the threshold of the infrequent fuzzy itemsets. But it does not mean that the lower the threshold, the better. Comparing table 8 with table 9, we can see that plenty of fuzzy negative rules, most of which are meaningless and redundant, will be generated when inFFS is the complement of FFS. It is clearly inadvisable TABLE I  FPNAR WHEN MFS  K 0.3 mfs\(k\=[0.3,0.3,0.3,0.3,0.3,0.3 f c  0 6 m i n _ c o r r  1  FFS inFFS PAR NAR1 NAR2 Rules_num k=2 14 0 25 0 25 50 k=3 9 0 43 0 46 89 k=4 1 0 9 0 10 19 k=5 0 0 0 0 0 0 Total 24 0 77 0 81 158 TABLE II  FPNAR WHEN MFS  K 0.2 mfs\(k\=[0.2,0.2,0.2,0.2,0.2,0.2 f c  0 6 m i n _ c o r r  1  FFS inFFS PAR NAR1 NAR2 Rules_num k=2 25 0 38 2 40 80 k=3 22 0 98 0 114 212 k=4 9 0 93 0 114 207 k=5 1 0 26 0 30 56 Total 57 0 255 2 298 555 TABLE III  FPNAR WHEN MFS  K 0.16 mfs\(k\=[0.16,0.16,0.16,0.16,0.16,0.16  m f c 0 6 m i n _ cor r  1   FFS inFFS PAR NAR1 NAR2 Rules_num k=2 26 0 38 4 40 82 k=3 26 0 106 0 127 233 k=4 11 0 104 0 135 239 k=5 2 0 40 0 55 95 Total 65 0 288 4 357 649 TABLE IV  FPNAR WHEN MFS  K 0.12 mfs\(k\=[0.12,0.12,0.12,0.12,0.12,0.12  m f c 0 6 m i n _ cor r  1   FFS inFFS PAR NAR1 NAR2 Rules_num k=2 30 0 38 20 40 98 k=3 29 0 112 7 133 252 k=4 15 0 122 4 167 293 k=5 2 0 40 0 55 95 Total 76 0 312 31 395 738 TABLE V  FPNAR WHEN MFS  K 0.1 mfs\(k\=[0.1,0.1,0.1,0.1,0.1,0.1 m f c  0 6 m i n _ c o r r  1  FFS inFFS PAR NAR1 NAR2 Rules_num k=2 34 0 38 32 40 110 k=3 36 0 121 27 154 302 k=4 18 0 138 10 194 342 k=5 4 0 60 4 97 161 Total 92 0 357 73 485 915 TABLE VI  FPNAR WHEN MFS  K 0.08 mfs\(k\=[0.08,0.08,0.08,0.08,0.08,0.08  m f c 0 6 m i n _ cor r  1   FFS inFFS PAR NAR1 NAR2 Rules_num k=2 36 0 38 38 40 116 k=3 40 0 126 32 164 322 k=4 19 0 142 13 206 361 k=5 4 0 60 4 97 161 Total 99 0 366 87 507 960 TABLE VII  FPNAR WHEN USE MULTI LEVEL FUZZY SUPPORT  mfs\(k\=[0.3,0.2,0.16,0.12,0.1,0  m f c  0  6  m in _co rr 1   FFS inFFS PAR NAR1 NAR2 Rules_num k=2 25 0 38 2 40 80 k=3 26 0 106 0 127 233 k=4 15 0 122 4 167 293 k=5 4 0 60 4 97 161 Total 70 0 326 10 431 767 TABLE VIII  FPNAR WHEN USE MS_FPNAR mfs\(k\=[0.3,0.2,0.16,0.12,0.1,0  m fc 0 6 m in _c or r 1   FFS inFFS PAR NAR1 NAR2 Rules_num k=2 25 11 38 38 40 110 k=3 26 14 106 32 164 302 k=4 15 4 122 13 206 342 k=5 4 0 60 4 97 161 Total 70 29 326 97 507 930 TABLE IX  FPNAR WHEN MFS 0 mfs\(k\=[0.3,0.2,0.16,0.12,0.1  m fc 0  6  m in _cor r 1   FFS inFFS PAR NAR1 NAR2 Rules_num k=2 25 23 38 78 40 156 k=3 26 77 106 378 235 719 k=4 15 92 122 819 564 1505 k=5 4 39 60 702 486 1248 Total 70 231 326 1977 1325 3628 
627 


 Fuzzy rules Fuzzy rules mfs Fuzzy rules Fuzzy rules mfs  Fig.6  mfs and Fuzzy rules in Traditional algorithms Table 10 and Figure 7 reflect the relationship between the minimum correlation coefficient \(min_corr\d fuzzy positive and negative association rules \(FPNAR\where the former is used to remove those frequent fuzzy itemsets of weak correlation. Table 10 presents the number of fuzzy positive and negative association rules, which are generated with min_corr varying from 1 to 2.8 under such preconditions as mfs = [0.3, 0.2, 0.16, 0.12, 0.1, 0.08 d  m f c  0.6.As can be seen from Figure 7, with the min_corr increasing, the number of fuzzy association rules attained shows an obvious decreasing trend, as the minimum correlation coefficient can eliminate the rules which are generated by frequent itemsets of weak correlation, leaving more meaningful rules. So, the minimum correlation coefficient can effectively remove those meaningless or less meaningful rules to ensure that the final mined fuzzy rules are meaningful TABLE X  RELATIONSHIP BETWEEN MIN _ CORR AND FPNAR mfs\(k\=[0.3,0.2,0.16,0.12,0.1,0  m fc 0 6  min_corr= 1.0 1.3 1.5 1.7 2.0 2.3 2.5 2.8 PAR 326 292 249 192 133 106 72 44 NAR 604 522 435 322 193 130 80 48 TotalRules 930 814 684 514 326 236 152 92  Fig.7  relationship between min_corr and FPNAR According to the above analysis, the algorithm is adopted to mine rules from the rock sample data set. If correlation coefficient constraints min_corr = 2, and multiple minimum fuzzy support mfs \(k\3, 0.2, 0.16, 0.12, 0.1, 0 t h en  326 fuzzy rules, including 133 fuzzy positive rules and 193 fuzzy negative rules, are generated after mining. Compared with the traditional fuzzy association rules mining algorithm, this algorithm solves the problem about the selection of fuzzy support, and generates more accurate and effective fuzzy positive and negative association rules VI  CONCLUSION There are broad development space and prospects for the mining fuzzy positive and negative association rules however, there are still many drawbacks in the selection of the membership function and minimum support, and the accuracy of selected parameters directly affects the result of the final mining rules. This paper proposes a novel fuzzy positive and negative association rules algorithm for the first time, and utilize k-means clustering method to determine the membership function, avoiding uncertainty problems which may be brought about in current existed related algorithms that need to identify the membership function subjectively Meanwhile, the multi-level fuzzy support and the correlation coefficient criterion are introduced based on fuzzy positive and negative association rules. In the end, the proposed algorithm is proved to be more effective and accurate in comparison with the traditional algorithm through an experiment REFERENCES 000>\000\024\000  R. Agrawal, T. Imielinski and A. Swami, “Mining association rules between sets of items in large database”, Proceeding of the 1993 ACM SIGMOD International Conference on Management of Data New York: ACM Press, 1993, pp.207-216 000>\000\025\000  R. Agrawal, R. Srikant, “Fast algorithms for mining association rules in large database”, Proceedings of the 1994 International Conference on VLDB, San Francisco: Morgan Kaufmann Publishers, 1994 pp.487-499 000>\000\026\000  J. Han, J. Pei, Y. Yin et al Mining Frequent Patterns without Candidate Generation: a Frequent-Pattern Tree Approach”, Data Mining and Knowledge Discovery, Vol. 8, 2004, pp. 53-87 000>\000\027\000  R. Srikant, R. Agrawal, “Mining Quantitative Association Rules in Large Relational Tables”, In Proc. 1996 ACM–SIGMOD Int. Conf Management of Data, Montreal, Canada: ACM Press, 1996, pp. 1-12 000>\000\030\000  B. Lent, A. Swami, J. “Widow. Clustering association rules”, In Proc 1997 Int. Conf. Data Engineering, Birmingham, England: ACM Press, Apr. 1997, pp. 220-231 000>\000\031\000  T.P. Hong, J.B. Chen, “Processing individual fuzzy attributes for fuzzy rule induction”, Fuzzy Sets Syst, 2000, pp. 127-140 000>\000\032\000  S. Brin, R. Motwani, C. Silverstein, “Beyond Market: Generalizing Association Rules to Correlations”, Processing of the ACM SIGMOD Conference, Tucson, AZ: ACM Press, 1997, pp. 265-276 000>\000\033\000  X. Wu, C. Zhang, S. Zhang, “Efficient Mining of Both Positive and Negative Association Rules”, ACM Trans. On Information Systems Vol.22, 2004, , pp. 381-405 000>\000\034\000  X.J. Dong, S.J. Wang, H.T. Song, “Approach for Mining Positive Negative Association Rules Based on 2-level Support”, Computer Engineering, Vol.31, 2005, pp. 16-18 000>\000\024\000\023\000  X.J. Dong, Z.D. Niu, X.L. Shi et al Mining Both Positive and Negative Association Rules from Frequent and Infrequent Itemsets ADMA 2007. Harbin, China: Spring-Verlag, 2007, pp. 122-133 000\003 
628 


world databases presents a future direction for further research  Figure 1: Execution time v/s Minimum Support  References 1 R. A g r a w a l a nd R. Srik a n t F a s t A l g o rithm s  f o r Mining  Association Rules,” In Proc. of the 20th VLDB Conference  Santiago, pp. 487-499, Chile, 1994 2 R. A g ra w a l T  Im i e linsk i, a nd A. Swami. “Mining Association Rules between Sets of Items in Large Databases.” In Proceedings of ACM SIGMOD pages 207-216, May 1993 3 R. A g ra wa l, R. Srik a n t M in i n g se que ntia l pa t t e r ns In proc. of the 11th International Conference on Data Engineering \(ICDE'95  pages 3-14, March 1995 4 M Blu m R.W  Flo y d   V   P r att, R.L  Riv e st an d R E. T a rjan  Time bounds for selection,” J. Comput. Syst. Sci. 7\(1973\, pp 448-461  Total number of data items Number of elements in MEDIUM Number of elements in LOW 10 139 0 20 3238 127 30 21899 1923 Figure 2: Size of MEDIUM and LOW v/s number of data items 5 Fe r e nc B odo n  A f a s t  A P R I O R I i m plem e n ta tion I n pr oc o f  IEEE ICDM Workshop on Frequent Itemset Mining Implementations \(FIMI'03 Melbourne, Florida, USA, 2003 6 Feren c Bo d o n   A T r i e b as ed A P RIORI Im p l e m en tatio n f o r  Mining Frequent Item Sequences.” In ACM SIGKDD Workshop on Open Source Data Mining Workshop \(OSDM’05 pages 56-65 Chicago, IL,USA, 2005  M  C h en  J Han an d  P  S   Yu  Dat a M i n i n g  A n o v ervi e w  from a Database Perspective IEEE Transactions on Knowledge and Data Engineering vol. 8, no. 6, pp. 866-883, Dec. 1996 8 A r on Culotta A ndre w Mc Ca llum Jona tha n Be tz  I nte g ra tin g probabilistic extraction models a nd data mining to discover relations and patterns in text,” In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics p.296-303, June 04-09, 2006, New York  M M S u fy an Be g P aral l e l an d Di st ri b u t ed Di sco v er y o f  Association Rules In Artifical Intelligence Application Book  Fadzilah Siraj, Eds\ersity Utara Malaysia 10 X ita o Fa n k os Fe ls v  l y i Ste phe n A  Siv o  M onte C a r l o   SAS® for Monte Carlo studies: a guide for quantitative researchers 11 U  Fa y y e d  G  P i a t e t s k y Sha p iro P. Sm y t h a nd R. U t h u ra s a my  eds.\. “Advances in Knowledge Discovery and Data Mining AAAI Press / The MIT Press, 1996 1 W  J F r aw l e y  G  P i at et sk y S h ap i r o an d C M a t h eu s   Knowledge “Discovery In Databases: An Overview. In Knowledge Discovery In Databases eds. G. Piatetsky-Shapiro, and W. J Frawley, AAAI Press/MIT Press, Cambridge, MA., 1991, pp. 1-30 13 V  K u m a r  a nd M. J o s h i T utor ia l o n H i g h P e r f or m a nc e D a t a  Mining,” In proc. of International Conference on High Performance Computing \(HiPC-98 Dec. 1998  a v i d L  O l s o n a n d D e s h e n g Wu   D eci s i on  m a k i ng  with uncertainity and data mining.” In X. Li, S. Wang and Z.Y Dong\(Eds Lecture notes in Artificial Intelligence pp. 1-9 Berlin: Springer\(2005 15  P  W r ig ht. K now le dg e D i s c ov e r y I n D a ta ba s e s  T ools a n d  Techniques ACM  Crossroads Winter 1998   
408 


 7. Reference    Fetzer C,Hagstedt, K,Felb er P  Autom atic Deteciton  and Masking of Non-Atomic Exception Handling International Conference On Dependable Systems and Networks, \(DSN2003\10-116    Y e n S J, Lee Y S. “Mining Interesting  Associatio n  R u l es  and Sequential Patterns”. International Journal of Fuzzy Systems, 2004-6 \(4   Alasf f ar A  H, Deogun J S. “Concept-b a sed Retr iev a l with Minimal Term Sets”. Foundations of Intelligent Systems: 11th Int’l Symposium, Springer, Poland, 2004 114- 122   Qiu Y ong gang,Frei H P  Concept B a sed Quer y   SIGIR’03,2003:16 0-169   Saltom G  W ong A, Y a ng C  S. “A V ector Sp ace Model for Automation Indexing”. Communications of the ACM 2005, 18\(5\-620   Agrawal R, Srikant R. “Fast Algorithm f or Mining Association Rules in Large Databases.” Proceedings of the 20th International Conference on Very Large DataBases Santiago , Chile , 2004   Park J S. “Using A Hash-Based Method with Transaction Trimming forMining Association Rules.” IEEE Transactions on Knowledge and Data Engineering, 2007   Savasere A, Omiecinski E Navathe S  An Ef ficient Algorithm for Mining Association Rules in Large Databases.” Proceedings of the 21st International Conference on Very large Database, Switzerland, 2002  


              


   


                        





