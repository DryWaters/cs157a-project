Research on Leakage State Classification of Pipelines Based on Wavelet Packet Analysis and Support Vector Machines Na Liu 1 Lixin Zhang 1 Yanyan Zhao 2 1 Department of Automation, Beijing Institute of Petrochemical Technology, Beijing, 102617 P.R. China E-mail:liuna@bipt.edu.cn zhanglixin@bipt.edu.cn 2 Department of Computer Application, Beijing University of Chemical Technology, Beijing 100029, P.R. China E-mail:zhaoyanyan110@163.com Abstract Aimed at the problem during the pipeline leakage state detecting process, that the datum which the sensor directly surveys are quite big and the characteristic are not strong, this paper brings forward one pipeline leakage state classification method, which combines the wavelet packet analysis and support vector machines. Through using the wavelet packet to the original data, carrying on the frequency band decomposing and the energy analysis, obtains the characteristic that can most reflect the classified essence. State sorter constituted by the support vector machines, only needs a few training samples, can make signal frequency band energy as the eigenvector to recognize and classify. The experiment datum shows that, this method effectively realizes the classified recognition of leakage state 1. Introduction Pipeline transport run through each domain in industrial productio n, and its decision effect is more prominent in the national ec onomy. The often occur of pipeline leakage already becomes the key barrier of the enterprise run and the eco nomic benefit, so earnest studying the pipeline leakage online monitor technology has importan t social and economic benefit to guarantee equipment safety operation In the leakage state monitor process, the signal get through the sensor are measure data. Therefore the leakage state recognition faces questions as follows One is how can do effective feature extraction to the measure data; the other is desi gning an effective sorter to realize the pipeline leakage state recognition. After these two questions settled may input all eigenvectors coming from the test data to the sorter, to realize the pipeline leakage state recognition This paper utilizes the wavelet packet analysis method to extract eigenvectors in freque ncy field of the pipeline leakage state, and adopts support vector machines \(SVM\which is a kind of machine learning method, to solve the analysis question of leakage state under the limited samples 2. Eigenvectors selection of wavelet packet analysis 2.1 Wavelet packet analysis theory The traditional signal analysis method like Fourier transformation is one kind of overall transformation, is unable to indicate the time frequency nature of the signal, and does not suit the non-stead y signal analysis Along with the wavelet theo ry development, by way of the new signal processing method, the wavelet analysis has widely applied in fault diagnosis and other flied Although wavelet transformation satisfies this kind of non-steady signal analysis because of its good time frequency localization and the multi-distinguish characteristic, it does not decompose the high frequency part agai The wa ve let packet transformation makes up this shortage, it can provide one careful analysis method for the signal. It carries the frequency band through mu lti-level partition, does further decompose to high frequency part which multi-distinguish analysis does not subdivision, and averagely decomposes the original function frequency range into entries of the equal size frequency segment. If sampling frequency is known, the analyze layers are enough , then may make frequency range of each frequency segment to be small, thus carries on a j 2 
2008 International Conference on Intelligent Computation Technology and Automation 978-0-7695-3357-5/08 $25.00 © 2008 IEEE DOI 10.1109/ICICTA.2008.468 235 


more careful analysis to the si gnal, so that provide a more effective method for the signal characteristic extraction The wavelet packet arithmetic of the signal sequence  describes as: Assume together clutches filter satisfies   t x   n h     kl l n h k n h   2   2  1    2   n h 2 Make then the recurrence formula of the wavelet pack et decomposition is  1   1    k h k g k     2    2    2    2   1 2 2 k t D k g t D k t D k h t D k n n k n n        3     0 t x t D  In the formula  is coefficient of the wavelet packet decomposition   t D n n is the layer of the wavelet packet decomposition  1  0  n 2.2 Eigenvector selection The wavelet packet signal decomposition is incorporating random signal includes the sine signal into the corresponding frequency band.These decomposition frequency band signal all has certain energy, may use signal energy in each frequency band as eigenvector to token pipeline leakage state The wavelet packet frequency band analysis technology is the same as the Fourier frequency chart analysis technology, the theory basis also is Parseval energy integral ecause t h e e n e r gy of the signal in time field  x f dx x f f 2 2 2     4 The  wavelet transformation is   x f        R k k k k kj dx x f j x j W C    2  2 2  2 2 012 5 In type \(5   x 012  is the base wavelet. According to type 5\well as th e Parseval energy integral equality, has   2 2      j kj R C dx x f 6 May know by type \(6\ wavelet transformation coefficient  has the energy dimension, may use as the energy characteristic extraction in the pipeline leakage state. Supposes restructuring signal of which after the wavelet pack et decomposition the kj C kj S k layer and the j entries frequency band, corresponding signal energy is then has kj E 2 1 2        N m jm kj kj x dt t S E 7 In type \(7 expresses data length  expresses the wavelet packet deco mposition level N k p j  2  1  0  expresses the serial number of the decomposing frequency band   expresses the disperse poin ts breadth cost of the restructuring signal Signal total energy equals to the sum of the various frequency bands energy, namely 1 2   k p jm x kj S k E  8    p j kj k E E 0 After doing energy restruct uring of the character gene in type \(8\ult eigenvector by wavelet packet decomposition extraction k kp k k E E E E T      1 0  9 3. Support vector machines and arithmetic Support vector machines is bringing forward by Vapnik et al. acts according to the frame risk minimize principle of the statistics theory, arithmetic first aims at two value classification qu estion. In the industrial equipment fault diagnosis domain, the simple two value classification cannot satisfy the actual needs Therefore, in two value classification foundation establishes multi-classification model of the support vector machines 3.1 Support vector machines principle Support vector machines translate the conformation of the most superior hyperpl ane into seeking the most superior answer with hypo-two optimization, its math form is      l i i C w w w 1   2 1      015 10 s.t i i i b x w y      1     l i  1  0  i   l i  1  The actual classified question often is non-linear.The basic thought which th e support vector machines deal with this question is mapping the sample space to the higher dimension character space, and seeks the most superior hyperplane in character space this hyperplane actually is corresponding th e non-linear hyperplane of the original sample space. Through 
236 


possessing special nature nuclear function, support vector machines ably avoid straight dealing with problems in high dimension space, so the complexity of the computer is basically not increase h rough seeking its antithesis question, comes down to a quadratic function extremum problem   j i j i j l j i i l i i x x K y y W  2 1   1  1            11 s.t C i    0  l i  1  0 1    i l i i y  Solves the various coefficients in above formula educes the classified distinguish function is        1 b x x K y sign x f i i l i i      12 Therefore, may distinguish the sample x respective category, according to the positive or negative of the classified function The commonly used nuclear function includes Polynomial function  is the polynomial coefficient Radial group function  2  1    1         d y x y x K d d            2 2 2 exp     y x y x K   is the extent of the nuclear function; Sigmoid nuclear function  and respectively be scale and attenuation parameter    tanh    c y x b y x K    b c The support vector machin es solution process needs to solve a hypo-two programming problem, when sample point number are bigge r, directly solving this hypo-two programming problem needs quite large calculate quantity an d storage capacity, that will cause arithmetic invalidation erefore many scholars bring forward the special arithmetic to deal with the large issue, for example, the choosing block arithmetic the decomposition arith metic and SMO \(sequential minimal optimization\hmetic. Hereinto, SMO training arithmetic is a simple and fast arithmetic, it will change large-scale h ypo-two programming question into a series of the optimization sub-question only containing two variables, and will be able to solve directly by applying pa rsing method, and causes computing process celerity and nicety The SMO arithmetic is as follows: assigns precision   makes  selects optimization variable parses this optimization problem about these two variable, so gets the optimization solution  according to the above renews 0 0    0  k k k 2 1      1 2 1 1   k k    to Among precision 1  k   if satisfies the engine off rule  0 1    l i i i y   C j    0 l i  1                     1  0    1  0    1      1 C x C x x b x x K y y j j j j j j l i j i i i j     13 Then takes appro ximate solution Otherwise makes 1    k   1   k k renews  to 1 duplicates above step  k  3.2 Multi-classified question In order to solve the fault diagnosis and so on the multiple value classified questi on, needs to establish the multi-classified suppo rt vector machines.With the mathematical language describing as, according to training regulations which assigns   l l l Y X y x y x T         1 1    Among them    1   M Y y R X x i n i     l i  1   seeks a decision-making function Thus it can be seen, solves the kinds of multi-classification questions, materially is to find a rule, which divides the points in the y R x x f n      n R into M part. At present classification arithmetic mainly has one to one, one to more and etc this article applies one to one classification arithmetic One to one classification arithmetic is based on the classification method of two kinds of problems.The concrete arithmetic is, to all   1          M j i j i j i j i     Extracts all i y  and the j y  sample points from the training regulations. Com poses training regulations based on these sample points, uses the support vector machines that solve the two kinds classification problems to get real-value function and determines the interpolator of that belong to the kind or the j i T    x g j i  X x  i j kind[5           others j x g i x f j i j i  0  14 4. Pipeline leakage state example analysis The experiment selects one pi peline that its diameter is 50mm, wall thickness is 2 5mm, the cr ack and the vent is the artificial simulation. When the pressure is 0.5MPa, flux is 3.525L/min, under such working condition, separately carries on the pipeline equipment state experiment in the normal, the crack and the vent 3 kinds. Hereinto, support vecto r machine adopts SMO arithmetic, and combines the work collections selection policy in the SVMlight arithmetic. The type is 
237 


C-SVC, the nuclear function is the radial direction group function, to the multi-classified questions, adopt one to one classified arithmetic Figure 1 respectively contains the original signal and the restructuring sketch map of various frequency bands after the wavelet packet decomposing under 3 conditions, Table 1 re presents frequency range of every nodes in the 3 rd layer after the wavelet packet decompounds Utilizing wavelet packet energy method analyzed above extracts signals eigenvector. In this example the signal sampling points are 2048, decomposing to the 3rd layer dispersion points are 256, therefore n=256.Taking various freq uency bands energy as element to construct sup port vector machines input vector Takes 12 groups training datum \(see Table 2\re into 4 groups are the normal state, 4 groups are the vent state, 4 groups are crack state. Taking 8 groups test datu m to carry through the test \(see Table 3\output of the support vector machines is when the pipeline is under the vent state when the pipeline is under the crack state when the pipeline is under the normal state      37 31 30 E E E T  Y 1  Y 2  Y 3  Y From table 2, may see in the different frequency range the energy distributing rate is different. Under the normal state signal frequency mainly distributes in E31, under the crack state si gnal frequency mainly distributes in E32, under the vent state signal frequency then mainly distributes in E36. The experiment indicates that, the wavelet packet energy analysis can reflect obviously the running status of the pipeline, the different fault type is corresponding the different frequency band ener gy distribution May know from the test results \(see Table 4  Table 4. Result of SVMs in testing Serial number 1 2 3 4 5 6 7 8 State 2 2 2 1 1 3 3 3 Test result 2 2 2 1 1 3 3 3 signal eigenvector extracts with the wavelet packet energy method, take this as the support vector machines input vector is feasible, also the classified effect is very ideal. The test results in Table 4 indicate the accuracy is 100 5. Conclusions Aimed at the existing prob lems in the pipeline leakage state detection, this paper proposes a classification method which combines the wavelet packet analysis and support vector machines. The research results indicate that using wavelet packet to original data to do frequency decomposition and energy analysis, can get the most essential character that reflects classification, and the state sorter classification which comp osed by support vector machines is quite effective. It offers an effective approach for pipeline leakage fault diagnosis, and has a broad application foreground Reference  ShingDong Feng, BaoMin g, QuLiangSheng. Wavelet  envelope analysis in rolling bearing diagnosis ap  Chinese mechanical engineering 2002, 11\(12\, 1382-1385  LiangPing BaiLei, LongXin Feng, FanLiLi The steamer  rotor vibration fault diagnosis based on the wavelet packet analysis and th  Control theory and application 2007, 24\(6\,981-985  Nello Cristianini, John Shawe  Tay lor  LiGuoZheng  WangMeng, ZengHuaJun translate. Support vector machines introductory re Beijing Electronics industry publishing house 2004  HeXueWen ZhaoHaiMing Support v ector m achin es and  its application in mechanical fa  South central university journal 2005,36\(1  DengNaiYang, TianYingJie New method in data mining  support vector machin  Beijing  Scientific publishing house 2004 a\ vent signal                         \(b\ crack signal                     \(c\ normal signal Figure 1. Vibration signal in the three states and wavelet packet decomposition 
238 


Table 1. Frequency extension of every nodes in the third layer after wavelet analysis Signal d30 d31 d32 d33 d34 d35 d36 d37 Frequency/kHz 0-2.5 2.5-5 5 -7.5 7 5-10 10-12.5 12.5-15 15-17.5 17.5-20 Table 2. Learning samples of SVMs E30 E31 E32 E33 E34 E35 E36 E37 2 0.014853 0.125484 0.524027 0.214979 0.076956 0.196449 4.257876 0.745861 2 0.087546 0.185978 4.01497 0.041123 0.508625 0.193704 0.955934 1.649599 2 0.102268 0.007125 2.402897 0.623195 0.627574 0.471595 1.915745 0.788635 2 0.030333 0.1485 0.772661 0.753196 0.045032 0.056044 0.481392 0.826673 3 0.000416 2.18E-05 0.000287 0.000758 0.000122 4.45E-06 9.27E-06 1.65E-05 3 0.000203 0.000118 4.49E-05 0.000682 3.47E-05 1.34E-06 6.25E-04 1.76E-05 3 7.48E-05 9.57E-06 5.06E-06 2.79E-05 3.06E-06 7.62E-07 5.57E-08 7.44E-06 3 1.87E-07 5.61E-06 2.29E-05 1.80E-04 7.50E-06 7.96E-07 2.22E-05 4.03E-06 1 2.07E-03 2.05E-01 9.47E-01 1.25E+00 4.16E-02 4.76E-01 1.94E+00 7.35E-01 1 1.10E-03 6.63E-02 2.24E+00 3.64E01 3.19E-01 2.89E+00 6.51E+00 4.47E-01 1 4.52E-01 7.10E-03 1.55E-02 1.92E-02 4.19E-02 4.40E-02 5.82E-01 1.42E+00 1 5.53E-03 6.50E-04 1.51E-02 3.49E-03 3.66E-03 4.39E-03 9.56E-02 1.71E-02 Table 3. Testing samples of SVMs E30 E31 E32 E33 E34 E35 E36 E37 2 1.094746 0.112768 0.89526 0.645575 0.247348 0.611803 1.993438 1.23933 2 0.08897 0.593063 0.459816 0.105917 0.152654 1.198789 0.969644 2.165458 2 0.067893 0.032029 1.394216 0.024262 1.128428 0.039826 5.015734 2.278476 1 0.005048 0.403039 0.204116 0.008172 0.013817 0.010279 0.669582 0.168224 1 0.070766 0.201433 0.050859 0.009735 0.339984 0.089394 0.110285 0.172407 3 0.000385 1.37E-05 3.15E-05 1.92E-06 8.77E-06 4.04E-06 1.29E-04 4.59E-05 3 1.69E-07 3.02E-07 2.08E-06 2.84E-07 3.10E-07 3.03E-07 5.94E-06 1.52E-06 3 3.71E-08 1.17E-07 3.48E-06 5.85E-07 2.04E-06 1.83E-07 2.81E-06 1.20E-05 
239 


  6 only access to NEWS merged Level 2 data. Illustrates the redundant and non-reusabl e processing of manual data access  We leveraged Service Oriented Architecture \(SOA technologies to avoid the redundant downloading and processing of the voluminous data sets by promoting serverside processing and client-sid e data access. Our services enable scientists to customize the conditional subsetting of voluminous NEWS Level 2 data and the production of Level 3 data using a customizable Level 3 Quantization data reduction technique. An implicit benefit of these services is the transparent data search and access mechanism that previously was done manually in a separate step. The services include a temporal and spatial query that searches for the merged Level 2 data that matches the user\222s given space-time constraints Enabling Distributed Exploratory Computing We understand that scientists routinely utilize their own trusted code and/or perform analysis in their own familiar working environments of choice. Rather than developing another web portal or new data access tool that forces the scientists to depart from their working environments, we developed service-oriented components that can be integrated into their own code environments \(figure 4 below\This facilitates the \223exploratory computing\224 that scientists are familiar with where they can remain fully in their analysis environment and serendipitously explore the data  Figure 4. Overview of th e distributed system for accessing and manipulating the merged NEWS data Multiple Web Service client s enable users to perform analysis directly within their familiar computing environments  By taking the proven Web Services-based approach, we ensure interoperability across different platforms and strengthen the interconnectedness and reuse of the Earth science data. This will not only promote actual use of service-oriented architectures, but also facilitate the streamlining of transparent access and manipulation of Earth science data from within scientists\222 own tools. For example, users can within th eir own Matlab analysis code call a function to get global-scale monthly averages of data in a given time period of water vapor. This seamlessly performs a remote query for all merged NEWS data on the server matching the space-time bounding box, and then averages by calendar month all water vapor data. The results are seamlessly downloaded to the user\222s machine from within the Matlab environment. The Matlab function call then returns with the data files that can immediately be used for analysis right within Matlab Data Storage Conventions The voluminous instrument and merged NEWS Level 2 data files that are stored on the server can potentially increase complexity in file access. Typically this can be addressed by indexing the files with a crawler that populates a searchable database. Earth sc ience data systems have also tried creating in-house crawlers that populate scalable database such as the open source MySQL and PostgresSQL Others have also tried using the open source Lucene search engine for fully i  Regardless of data files being index and cataloged additional methods can be applied to increase file access performance. We store data files following a consistent file and path-naming scheme where data files are located under directories for instrument names, year, month, and day From this approach, one can generate the full file path location of the data given just the data\222s type and timestamp. When reading the f iles for processing, no search is then required to find the corresponding set matching a temporal range 6  E NDPOINT S ERVER D EVELOPMENT  A set of proven SOA technologies have coalesced into the Web Services Protocol Stack. Built upon common web standards, Web Services can ensure interoperability in a distributed environment. Additionally, the proliferation of open source implementations adhering to these standards ensure that we do not get lock ed-in with a specific vendor\222s proprietary implementation. We utilized this set of proven protocols to create our distributed architecture for Level 2 and Level 3 data processing We also leveraged the proven Simple Object Access Protocol \(SOAP\ng platform-independent XML messages over a distributed network environment Using SOAP enables platform-independent Remote Procedure Calls \(RPC ws us to create interoperable remote method calls to the backend data processing of subsetting, clustering, and summarization. One major benefit of SOAP is its ability to work over various transport 


  7 protocols, though http/https is the most common and versatile With SOAP enabling the calling of remote methods, we also create Web Services Description Language \(WSDL\XML documents that define each of the exposed interfaces WSDL provides a programmatic way to define the public interface names, arguments, and re turned information that is independent of the underlying platform, programming language, and therefore native data types Collectively, this set of Web Services protocols enables us to develop an architecture that promotes reuse at the service level rather than code level. SOAs facilitate the exposing of legacy processes as reusable services. We first developed an averaging service of the merge NEWS Level 2 data Averages are easily understandable from the science community and are an immediately useful capability to provide. Then leveraging the same infrastructural developments, a subsetting service created, followed by an offline Level 3Q quantitative summary service We took a layered architecture approach where each layer is responsible for a well-defined role. Figure 5 below shows an example for the averaging service that takes as input the set of merged A-Train Level 2 data and produces globalscale averages of a specific time range  Figure 5. The layered architecture of the Web Service clients and Web Service endpoints  Web Service Endpoints To maximize interoperability between all of our multiplatform clients, we want to utilize more standards compliant implementations of Web Services. Sun\222s Java API for XML Web Services \(JAX-WS\has a reference implementation that supports JAX-WS 2.0/2.1 \(JSR 224 WS-I Basic Profile 1.1, WS-I Attachments Profile 1.0, WSI Simple SOAP Binding Profile 1.0, WS-Addressing 1.0 Core, SOAP Binding, and WSDL Bindi  With JAX-WS, we were able to quickly develop Web Services endpoints utilizing Java annotations that generate much of the boilerplate code. By marking Java code with metadata annotations, JAX-WS was able to generate both client and server code that performs much of the SOAP functionality. JAX-WS also includes JAXB that performs the datatype marshalling/unmarshalling to and from SOAP XML representations Processing Layer Naturally there may alread y exist tested and proven processing code that scientists want to make available as a service. We require the ability to wrap into a Web Service executables that were developed in IDL, Matlab, Python native C/C++, and Fortran. By executing this layer as a forked process, the Web Services layer is shielded from process and memory issues related to the processing code Unstable processing code should not bring down the Web Services as well We developed Java-based abstractions that exposed the generic interaction to IDL, Matlab, Python, and C. By defining a domain neutral interface to each of these platforms from Java, we were ab le to quickly integrate any domain-specific processing code into the Java environment 223Business Logic\224 Model Layer To reduce the complexity of interacting with the processing layer, we developed a \223business logic\224 model layer that defines an interface to a model representation of the processing. This allows us age of the model interface without having to know specifics of interacting with the lower-level process execution mechanism. An example of this benefit would be replacing a current IDL processing engine with a Matlab varian t, where the model interface would not need to be modified Invocation Sandboxes To avoid conflicts arising from near simultaneous service calls, it became clear that each service request must be given its own process environment and directory space within which to work.  Every new invocation of a Web Service creates a sandbox environment on the server that serves to house the working environment for that endpoint invocation.  Each sandbox is given a unique name that corresponds to the specific service being requested, the unique date/time timestamp, and a process id. This ensures that each sandbox is unique and avoids the possibility of process collision. The invocations sandboxes are automatically removed when a service is done. Any data that needs persistence are moved out of the sandbox to more permanent directory locations Long Processing Times Unlike commonplace Web Services such as those for getting stock quotes, the processing services presented here 


  8 often have long run times. Depending on the temporal and spatial range of data requested, processing wall-clock times may range from minutes to weeks. The notion of a service call changes from a quick \223getting\224 of a data product, to an 223ordering\224 of a product. The normal synchronous aspect of the services must then be changed to an asynchronous model Job Management Additionally with long-running services, there will inevitably be overlapping processing times on the server Therefore given the finite computing resources available overloading the computational resources with simultaneous service requests should be avoided. We require the ability to define the appropriate level of computational resources to be utilized and want any remaining service requests to be queued until sufficient computing resources are available This then maximizes processo r utilization of each service request Distributed resource management \(DRM\stems can be leveraged to manage the distribution of workload to available compute resources Each Web Service request would map to a job request on the processing server. DRMs can monitor the current state of all resources and assign the jobs to the best-suited resources Rather than directly interfacing with one specific DRM implementation, we leveraged the Open Grid Forum\222s Distributed Resource Management Application \(DRMAA pronounced \223drama\224\API for job submission, monitoring control, and retrieval of resu lts Thi s l a nguage and vendor agnostic API frees us from language specific implementations, as well as vendor specific implementations of job management, and allows us to focus on the abstract representation of resource management This API is supported by several different vendors of job scheduling implementations, including Sun Grid Engine SGE and Torque.  Though bot h Torque and SGE fi t  our requirement of an open source scheduler, Torque is still based on an older PBS implementation and does not offer the scalability and Java bindings that SGE does Asynchronous Services By default, a Web Service call is a synchronous call where a request is sent from the client to the server and blocks until a response is available to be sent back to the client \(figure 6\ut for long running processing routines, it is impractical to hold a network connection and block until the service has completed. A scientist on the client side may not be able to wait for the response after a completed longrunning job. We want to enable complete network disconnects where the client may be potentially shut down Additionally, service calls where seamless data access is intrinsic would require the gene rated data to be downloaded as well before the service call unblocks and completes. The client user would have to wait for both processing and downloading times There have been extensions to existing web services-based standards to augment with asynchronous capabilities. WSNotification, a standards-base d approach to enable eventdriven capabilities to Web Se rvices using the publishsubscribe pattern. OASIS\222 Web Services Notification WSN\ses this notification pattern to allow subscribing to a Web Service\222s event information and be notified of such information. OGC\222s Web Pr ocessing Service \(WPS spatially referenced data have been augmented with  OGC\222s W e b Notification Service \(WNS\so provides messaged-based notifications between services. By using client-side modules, each web service client can receive notifications without resorting to polling. However, all of these approaches require a richer set of clients that may only be available in Web Service-rich language platforms such as Java. Our goal requires that our asynchronous solution work in more simpler and less rich platforms such as C and IDL What we desire is a simpler approach that minimizes clientside requirements of running any form of a messaging server. Preferably, the solution should be simple enough to work with clients that have a minimal capable of doing HTTP GET, such as a basic REST service call  Figure 6. Sequence diagram showing potentially long blocking calls for long processing times from Web Services Asynchronous Services using Web Services A more desirable model of behavior would be to use Asynchronous Web Services \(figure 7\he one synchronous blocking call is par titioned into smaller atomic Web Service calls. The Apache Axis2 project and Sun\222s JAX-WS Reference Implementation provide a set of asynchronous atomic calls that allow a service client to submit a service, check if it is done, and then get the results 


  9 when it is done. However, even this model does not fit the desired model of behavior  Figure 7. Sequence diagram showing the push and pull methods from Asynchronous Web Services  Asynchronous Web Services can use either a Synchronous or Asynchronous MEP \(Message Exchange Protocol transmitting and receiving protoc ol messages.  There are two methods supported for both synchronous and asynchronous MEP types:  polling and callback. Both support non-blocking client side behavior The polling method represents the \223pull\224 model of processing where the client determines when the response is received.  The callback method has the client passing a callback handler to the Web Service Endpoint. This callback is essentially an endpoint on the client side running in a separate thread that waits for the server to respond.  The callback method represents the \223push\224 model of processing where the server determin es the notification For polling-based asynchronous MEP, we find that the client still needs to be active while it polls. That is, a service submission would normally return a response object that is used to check if the job is done. That same response object is also used later to retrieve the results when available. For long duration processing times, the client must still maintain the same response object. Though the response object may in principle be marshalled out for longer persistence implementation-specific network connection reliance be prove to be impractical. The preferred approach would be to move the role of asynchronous waiting from the Web Services request/response objects down further to the job scheduler The remaining option would be to use normal synchronous Web Service calls, but cleanly se parate the different atomic operations into individual synchronous Web Service calls These individual calls consists of submitting a job canceling a job, getting the status of the job \(running cancelled, etc\ting the progress of the job \(console output, etc\and finally getting the results of the job \(figure 8\ Each synchronous call is then delegated to an underlying job manager for the actual service job management. Collectively, the client is provided with an asynchronous service model For particularly long running service jobs, we find that users also want to see progress of the processing activity. In addition to getting the status of the job state \(such as queued, running, done, cancelled\so provide the capability to see progressive real-time output form the processing job. Our service endpoint manages a console buffer of a processing job\222s STDOUT/STDERR. When a Web Service call to get the progress of a particular job is invoked, the current buffer contents of the progress is returned and then flushed from the server-side buffer. This enables clients to build GUI applications on top of this asynchronous service and see the real-time console output of the server-side processing on their client GUI window Additionally, the console refresh rate, determined by the client, can be adjusted to an appropriate rate for that client\222s usage. Other approaches are possible including publishing the progress to a custom Atom feed. But here, the console content is only handled and retrieved when the client needs it  Figure 8. Sequence diagram showing atomic synchronous calls that implement job management capabilities of a service  Though callbacks \(registerable handlers\ally the preferred paradigm over polling, this client example demonstrates the utility of keeping it simple. We require maintaining a set of lightweight Web Service clients across multiple platforms. Not all platforms and Web Service APIs support asynchronous Web Services. For true callbacks 


  10 implementations would also require the clients to run Web Service endpoints of their own This asynchronous service model that is composed of synchronous atomic Web Service calls can also be used at varying levels of complexity as desired by the client user For the novice, the service \223submit-isDone-get\224 sequence can be encapsulated into one simple function call. For the intermediate user, the same aggregated function call can also provide real-time notifications \(Observer design pattern\job currently running on the server. For the advance user network disconnects and client shutdown after a service job has been submitted can be achieved. The unique job id that is returned upon service submission can be stored for persistence enabling users to submit a product request from laptop, shutdown their laptop, and restartup at later time to retrieve the generated products Using AXIS2 and JAX-WS for the various client-side implementations introduces an extra dependency for users of some clients and therefore is less preferable. We also want to maximize ease-of-use for the user where we lessen the burden of installation, set up, and library dependencies Whenever possible, we used the \223vanilla\224 SOAP implementation that is intr insically available for each platform to keep the client footprints small Another intended use of our services is to be orchestratable by Web Services-enabled workflow engines. The callback approach to asynchronous Web Services is currently not as interoperable as standard synchronous Web Services However, current Web Service workflow engines can be set up to operate with standard Web Services that have the atomic actions exposed as multiple synchronous Web Service calls that poll for when a job request is done before continuing Data Delivery Once processing has completed, the service response is returned to the user. In th e Earth science domain, these results typically are generated data products that may be large in size. Given that the SOAP approach to Web Service uses XML as the underlying content being transferred transferring binary data in the SOAP message would not be efficient for large binary data sizes. Large binary file data support in XML currently still exhibits technical and performance issues. Approaches ex ist to encode binary data in various encoding schemes such as MIME and base64 More recently, there have b een new W3C recommendations for handling large binary data transfers such as XML-binary Optimized Packaging \(XOP Transmission Optimization Mechanism \(MTOM and Resource Representation SOAP Header Bl  However these new extensions may not be supported or compatible with all the client platforms that we need to support \(such as Matlab, IDL, and Python We settled on an approach that is compatible across the major client platforms. Rather then forcing data to be encoded in any scheme in SOAP, we simply allow the binary data to be transferred efficiently in standard http/https. When a Web Service job has completed, the generated product data files are placed in a unique URLaccessible location, and these UR Ls are sent back in the service response. The clients are then responsible for downloading the product files from the URLs A side benefit of this approach is observed when orchestrating our services fro m workflows. The URL results from one workflow operator are passed onto the next operator, which then is responsible for pulling the data from the given URLs. This method allows each operator to control the rate of accessing th e previous operator\222s data results OPeNDAP Another mechanism we support for data access is Data Access Protocol  \(DAP\ore specifically, we leverage the  for requesting and transporting data that is generated by the services. OPeNDAP also enables remote subsetting of data using constraint expressions, and the translation of data from one format to another. HDF5 data has been recently shown to work well over OPeNDAP using the Hy  The OPeNDAP protocol in recent years has become more widely used and accepted in the Earth Science community The Earth System Grid \(ESG on Environment and Water \(CREW\two large Earth science data centers using OPeNDAP for data access Date Time Handling For time handling, leveraging the Java GregorianCalendar simplifies handling timezones, time ranges, as well as correct leap years.  The bus iness logic model layer fully leverages the GregorianCalendar model to allow us to support manipulating multiple time granules, from seconds to years, and of any duration for each time granule. There is also a related XMLGregorianCalendar that we leverage for representation of W3C XML Schema 1.0 date/time datatypes across the Web Services For averaging, given a particular start and end time in GregorianCalendar format, along with an integer time duration amount, the model is able to determine the correct time ranges \(i.e. number of averaged files\d partitions the work accordingly for the av eraging engine \(in this case the engine is IDL\anCalendar individual time elements are retrieved and passed on to the engine for 


  11 processing, and the result is a list of files that propagates up to the server side layer For subsetting, latitude and longitude values are handled at the model layer, and translated \(when necessary correct coordinate values for the underlying engine.  Also to reduce the number of parameters a user would need to provide, we support user inputs of date/time string using the ISO 8601 format, the International Standard for the representation of dates and tim For exam pl e t h e input string \2232008-10-31T12:00:00Z\224, would be converted into a GregorianCalendar inst ance representing that exact date/time for the model layer Using the combination of GregorianCalendar and the ISO 8601 standard allows us to easily handle timezone-aware and timezone-na\357ve inputs. Internally to our \223business logic\224 model layer, all date/times are timezone-aware and set to the UTC timezone to match most of the instrument data conventions. But if the user provides a timezone-na\357ve date/time, that is with no timezone specified, then we promote it to UTC standard time. We also support user input in any timezone here we leverage the GregorianCalendar to convert any timezone to the UTC internal representation This simplifies science studies where users simply provide the local time at the area of interest to query data for  7  C LIENT I MPLEMENTATION FOR A NALYSIS E NVIRONMENTS  We implemented client-side modules to adapt to the major working environments favored by most scientists: \(1 4\thon, \(5\/C++, and \(6 Fortran90. Unlike other approaches that force the scientists to leave their familiar working environment to access data our services tool set brings the data access and manipulation back into their working envi ronments. Whenever possible we also aimed to develop the ability to automatically download and construct the native data objects in each respectively environment. This eliminates the need for the end user to worry about data file downloads, local file management, and loading them into in-memory data arrays for manipulation. A consistent experience is given to the user, both across the different tools and across the different platforms, with common interfaces and usage conventions This form of seamless integration directly facilitates the transparent access and manipula tion of heterogeneous data as called for by NASA\222s ACCESS NRA goals Java The Java client was designed to be an importable jar library from any user Java application. Since the Web Service endpoint server was already written with Sun\222s JAX-WS Reference Implementation, we also chose the same for the Java client implementation. This maximizes interoperability since both client and server utilize the same library. The client contains high-level methods for calling the Web Service and automatically downloading the custom-created files, allowing the entire process of service querying and downloading data to be contained in a single method call Lower level methods are also exposed in the service allowing the user more fine-grain control over the data flow and interface with the Web Services Matlab Mathwork\222s Matlab is a popular working environment used by scientists to perform science analysis. The 2008 release of Matlab has built-in support fo r Web Services with autocode-generation from WSDL URLs. It leverages the Java integration that Mathworks has already worked into Matlab We leveraged this built-in capability to develop Matlab modules that access the same server-side Web Services for Level 2 and Level 3 data access and manipulation Our Matlab service client consists of a number of .m files file extension for Matlab custom code\and requires no Java package dependencies beyond the JVM native to the Matlab environment.  Built-in SOAP functions help to create, send, and parse the SOAP message, which is used to communicate with our remotely hosted Web Services For automatic data file downloads, classes and methods standard with Java version 1.5 \(standard with Matlab 7.6 were used to access and download the files via http.  The resulting client allows all of the Web Services and downloading functionality to be transparent to a user Matlab supports the construction and handling of full Java data objects and the invocation of Java class methods directly form within Matlab. We made use of built-in functions that served as the bridge between a Matlab script and a SOAP service call \(createSoapMessage.m callSoapService.m, and parseSoapResponse.m\. These built-in SOAP functions in Matlab constrained us to passing a narrow range of Matlab data types due an incomplete set of Matlab datatype to W3C SOAP datatypes. Particularly the date-time and arrays of strings must be manually handled as more primitive types Two methods of datetime passing were settled upon.  One relies simply upon passing a tuple representation of datetime\227a number for the year, and others for the month, day hour, minute, and second.  While such a method worked well \(converting simple numerical data types\t was seen to be cumbersome and made for far less readable code to have to use six parameters to specify a single datetime.  We turned then using short char acter strings to represent datetimes, following the ISO 8601 standard.  Data type conversion of character strings between Matlab and XML is similarly easy as numerical types, and made for very concise and readable function calls 


  12 Passing arrays of strings required a retreat from any kind of array-like data structure.  Instead, a single, long string was created from an array of strings in Matlab, separated by a distinct delimiter \(a \223,\224 in this case\s single long string is passed through the service interface to the endpoint server, which then parses the string back into a list format before continuing on with the rest of the call sequence IDL ITT\222s IDL is another powerful visualization and analysis tool popular with the Earth science analysis community  Unl i k e M a t l a b, IDL \(as of versi on 7.0 have any built-in Web Services support. IDL can be made to speak the Web Services languages via an IDL-Java bridge delegating the Web Services capability to a linked Java library.  This does allow calling the Web Services, but there were some issues encountered along the way First, the IDL-Java bridge connectivity required some setup and handling by the end user. Environment variables and jar classpaths must be properly configured. While this is easily resolve with an installer, there was a strong desire to minimize the IDL client foot print to where there are no dependencies. We wanted to provide an IDL client code that could be dropped into a directory somewhere and should \223just work\224 Second, there are some known issues working with objects in IDL. We encountered memory errors during execution which appeared to be a memory leak in the IDL-Java bridge there was previously a known memory leak that had been patched\also ex tra overhead when interfacing between IDL and Java where data is converted from Java objects to IDL types Our current effort is focused on building a \223poor man\222s\224 SOAP as part of our IDL client that will allow us to directly call and interface with the Web Services, without having to go through Java.  We plan to utilize IDL\222s built-in simple http support to send manually constructed SOAP messages Though this approach forgoes the robustness of the JAXWS implementation, it will however provide a pure IDL client to our end users with no external dependencies Python The Python scripting environment has become a popular working environment for fast prototyping and exploratory science processing. Among all of the clients here, the Python Web Services client is the most trivial. We leveraged the suds package, a lightweight SOAP client for consuming Web Services in Pyt Though ot her open-source Python Web Service packages exist \(such as ZSI\we have found suds to be the most easy to use and more dynamic in nature. Suds does not require class code generation and can read WSDLs at runtime to dynamically construct a proxy object with an interface representing the WSDL C/C We want any C/C++ client to be able to interact with the server-side Web Services of Level 2 and Level 3 data generation. By using the using the popular open-source gSOAP Toolkit for SOAP Web Services package  client-side modules can interact with the data generation services developed on the server-side. gSOAP also includes facilities to autogenerate C/C++ RPC code from our published WSDL definition files of the Web Service on the server. We have also found that gSOAP has a good selfcontained XML bindings facility Fortran Fortran90 modules can be made capable of remotely accessing and the Level 2 and Level 3 data. Though Fortran has no built-in libraries to perform Web Services, we leveraged our C/C++ Web Service API via gSOAP to do the work. Fortran can call an 223externed\224 C Web Service API and pass back the relevant data into the Fortran environment. This would enable Fortran to fully delegate the Web Services operations to the C/C++ implementation 8  NEWS  L EVEL 2  P RODUCTS  With the availability of the software infrastructure supporting server-side processing, and seamless client-side data query and access, downstream data products can now be generated from the source merged NEWS Level 2 data Averaged One of the most common wa ys to summarize the large amount of data is to calculate the averages of data within a given temporal and spatial boundary. For example, it is very useful for scientists to make daily, weekly, monthly averages of some parameters in a regular latitude-longitudepressure grid, make a global map of the average, and analyze any global patterns and trends. In order to facilitate the needs, we developed an averaging Web Service to generate averaged data products that a user can customize The input arguments for the averaging Web Service are the time range, time granule, and a list of parameter names to produce averaged products with. The time range specifies the start and end time to access the NEWS data from. The time granule specifies the aver aging time period. The list of parameter names specifies the choice of parameters that are requested to make averaged products Subsetted Subsetting a data set is a fundamental way to access specific data from a large collection of data. We developed a usercustomizable subsetting Web Service that supports three general subsetting conditions 1  Spatial condition \(latitude, longitude, vertical range 


  13 2  Temporal condition \(e.g. from 2002-05 to 2002-07 3  Parameter selection \(e.g. te mperature and atmospheric water vapor only The combination of these three conditions allows a user to subset data in time, location, and parameter space 9  NEWS  L EVEL 3  P RODUCTS  Many of these quantities in NEWS L2 product interact through fundamental physical processes \(e.g. temperature affects cloudiness, and also the converse\ Consequently the observations should be treated as statistically separate variables, though traditiona l methods of summarizing satellite data do just that. We applied statistical clustering methods to a multiple-parameter set of observations from the A-Train instruments over the multi-year record. The resulting Level 3 quantitative summaries are made accessible through our serv ice-oriented tool Level 3Q Level 3Q data sets are statistical summaries of underlying Level 2 data. Like traditional Level 3 products they are 223gridded\224 in the sense that they provide a summary of Level 2 data belonging to space-time grid cells. These cells are typically defined as one or fi ve degree spatial regions over a time period of one or eight days, or one calendar month Unlike traditional Level 3 products, the Level 3Q \(L3Q grid cell summaries provide nonparametric multivariate estimates of the joint probability distributions of multiple geophysical parameters. Distribution estimates are derived from the underlying Level 2 data using informationtheoretic principles that balance the quality of the estimate against the amount of data reduction achieved  Figure 9. Raw and summarized data for one grid cell Raw data belonging to that grid cell can be listed in a data table with one row for each of N data points and one column for each variable \(here, two alternate representation: a scatter plot. In both cases each data point has weight 1. On the right are two representations of the compressed summary. The data table has K<<N rows and two extra columns showing cluster count and distortion. Counts are shown in the corresponding scatter plot by the bar heights  Data reduction replaces a larg e number of individual data points with a smaller number of representative data points and associated weights and quality measures. Figure 9 illustrates the basic concept. The idea is to treat a set of coincident measurement of different geophysical parameters for the same footprint as a multivariate vector, and collect all such vectors belonging to a given spatial-temporal grid cell as a set of points in high-dimensional data space. These data are partitioned into disjoi nt groups, called clusters, and we report the following statistic s for each: i\the centroid which is the representative, ii\he number or proportion of original data points assigned to it, and iii\the average squared distance between member data points and the centroid. This latter quantity is also called the cluster distortion The method that assigns data points to clusters is an adaptation of a signal-processing algorithm called Entropyconstrained Vector Quantizati EC VQ i s si m i l a r t o  the well-known K-means clusteri  Kmeans finds an assignment of raw data points to K clusters that minimize distortion. ECVQ finds an assignment that minimizes a quantity based partly on distortion, but also on the entropy of the probability distribution defined by the clustering. Entropy is a measure of information-theoretic complexity, and it is also well known that greater complexity is required to achieve lower distorti  ECVQ was originally proposed as a way of estimating this trade-off. The algorithm may find fewer than K groups as it attempts to balance the competing goals of fidelity to the original data and parsimony of representation.  This produces the smallest, or more properly, the least complex output data set that achieves a given level of fidelity to the original data. Our version of ECVQ is adapted in a number of ways for use as a massive data set reduction tool. These are described in detail in  and 26  W e  have al so previously employed our version of ECVQ to produce monthly summaries of Atmospheric Infrared Sounder data  The algorithm\222s output is best thought of as an estimate of the multivariate distribution of the data in a given space-time grid box. The original data have a distribution that puts probability  N on each multivariate data point, where N is the number of data points. ECVQ coarsens this distribution by collecting similar points into clusters, representing them by cluster centroids, and assigning probabilities N k k where N k is the number of point assigned to the k th cluster. In addition we also report the within-cluster mean squared error distortion\, which is a measure of the quality of the cluster representative as a stand-in for the original data assigned to it 1 N to cluster  


  14 10  A NALYSIS R ESULTS  AIRS, AMSR-E, MODIS and CLOUDSAT data have been merged into a dataset by the NEWS effort, and a framework of Web Services for averaging, subsetting and statistical analysis have been developed. Collectively it facilitates the data access and analysis of hydr ological processes. Here we present an example usage of instrument intercomparison Comparing Data Products Prior to Merging A necessary step in creating a formal merged data product is intercomparison of component data sets.  This ensures that the mutual random and systematic differences between the two data sets are quantified.  This approach does not provide information about absolute bias, which can be obtained only from comparisons with unbiased standard data sets.  For example, wate r vapor and temperature biases are typically constrained through comparisons with in situ observations as from radiosonde.  Such comparisons are usually the responsibility of the data provides, so the analyses described below assume some knowledge of satellite measurement biases An example of comparing component data sets is presented here with a single atmospheric state variable, in this case observed by AIRS \(Atmos pheric Infrared Sounder AMSR-E \(Advanced Microwave Scanning Radiometer for EOS\For this example five variables \(AIRS Total Water column, AMSR-E total water column, AIRS cloud fraction AIRS total water error estimate, AMSR-E liquid water path stemming from two different instruments \(AIRS and AMSR-E\pared and correlated The Atmospheric InfraRed Sounder \(AIRS\he Advanced Microwave Scanning Radiometer \(AMSR-E\are two instruments aboard the AQUA spacecraft. AMSR-E estimates water vapor over water surfaces and AIRS estimates water vapor over ocean and land. A map of the daily average of terrestrial water vapor column is shown in figure 10. This figure maps th e AIRS estimate of average total \(column\er vapor in mm during March 2003 at a spatial resolution of 1 degree in latitude and longitude  Figure 10. Map of averaged AIRS Total column water vapor for 2003-03   Figure 11. Scatter plot of monthly AIRS and AMSR-E column water vapor. AIRS and AMSR-E water vapor agree very well on the co incident locations  A similar a subset of AMSR-E water vapor over the ocean was prepared with our services and merged with the AIRS dataset at the same spatial and temporal resolutions. A scatter plot of the values estimated with AMSR-E is compared to the collocated va lue of AIRS in figure 11 Figure 11 also shows a red line to mark the location where all points should fall if the AIRS and AMSR-E estimates were the same. The figure shows a tendency by AMSR-E to estimate higher total water vapor than AIRS. However there are locations where AIRS does show higher values. A map of the averaged differences \(AIRS-AMSR-E\15 selected monthly means between the years 2003--2006 is shown in figure 12 This map highlights locations where each instrument tends to overe stimate compared with the other. Blue tones identify regions where AIRS estimates are larger than AMSR-E and shades of brown locate the regions where the opposite is true 


  15  Figure 12. Map of average differences over 15 months between AIRS and AMSR-R water vapor  Figure 12 highlights regions th at are characterized by different hydrological regimes. AIRS overestimates coincide with regions where cold western boundary currents cause frequent cold marine stratocumuli. AMSR-E tends to estimate higher total water vapor in regions characterized by warm sea surface temperatures and frequent convective activity. This result is consistent with previous comparisons    Figure 13. AIRS Total Cloud Fraction sum for 2003-03 Sum over all pressure levels AIRS is an IR measurement that cannot estimate water vapor in regions overcast with optically thick clouds. This property introduces a bias that depends on the cloud fraction. Figure 13 shows an estimate of the cloud fraction using a surrogate for cloud fraction over several AIRS pressure levels. It adds up the cloud fraction at the different levels \(because there may be overlaps, the sum over all pressure levels can be larger than one between the areas with large cloud fraction sums and large AMSR-E overestimates with respect to AIRS and vice versa, areas with the smallest cloud fraction sums coincide with the areas with large AIRS water vapor estimates A proxy for the "thickness" of the clouds in the overcast regions is the liquid water content of such clouds. The advantage of this proxy over others is that it also conveys information about the physical and hydrological characteristics of the scenes co rrelated with the differences Figure 14 shows a PDF of the differences as a function of AIRS water vapor and AMSR-E cloud liquid water path. A black contour line marks the change of sign in the differences. It shows that AIRS estimates higher total water vapor at low liquid water paths with a characteristic quasilinear increase between 5--20 mm. AMSR-E estimates larger total water vapor at 1 x 1 degree regions where the liquid water path is high. The pattern of the differences raises questions about why does AMSR-E estimates differ so quickly from AIRS at low water vapor contents and low liquid water paths  Figure 14. AIRS-AMSR-E differences \(in mm function of AIRS total water and AMSR-E cloud liquid water for the month 2003-03  


  16  Figure 15. AIRS-AMSRE differences as a function of AIRS error estimate over one day  AIRS has an error estimate of the total water vapor value that it calculates. The diffe rences between AIRS and AMSR-E are shown as a function of this estimate in figure 15 and very little correlation is found 11  R ELEVANT W ORK  Merged A-Train Level 2 Data A merged product that preserves the relationship of observed atmospheric water properties facilitates the hydrological studies by enabling scientists to get directly at the model data without worrying about the logistics of finding, collecting, and coordinating the measured quantities from different instruments. Previously there did not exist a capability to discover and access data from the A-Train\222s multiple instruments as merged multi-parameter data sets Enabling Orchestratable Service Workflows Our distributed service-oriented approach of loosely coupled services also enable s a higher level of reusability and orchestration with other services. Increasing numbers of workflow engines are already supporting Web Services as components/operators, which can then be orchestrated together into higher-level meta/virtual services SciFlo, a Scientific Dataflow Execution Environment, is a workflow engine that already supports assembling reusable SOAP Services, native execu tables, local command-line scripts, and codes into a distributed computing flow \(a graph of operators\8 SciFlo can u tilize o u r g en eric SOAP services as part of a larger coordinated data flow The Taverna Workbench is a free software tool for designing and executing workflows. Like SciFlo, it can orchestrate SOAP-based Web Services as components within a workflow. Taverna provides a visual editor to construct and edit the sequence of services in the workflow We have found that Taverna can dynamically introspect a given WSDL and construct the workflow component interface representing it Giovanni Giovanni, an acronym for the Goddard Earth Sciences Data and Information Services Cent er, or GES DISC, Interactive Online Visualization and Analys is Infrastructure, is a webbased tool to help visualize Earth science data  It  provides a simple and intuitive way to visualize, analyze and access vast amounts of Eart h science remote sensing data without having to download the data. Similar to the services developed here, it addresses the difficulties of traditional data acquisition and analysis methods by moving the complexity to the server-side Giovanni provides multiple in terface instances based on instrument and measurement ty pes. For example, the \223ATrain Along CloudSat Track Inst ance\224 can provide plots of vertical profiles of clouds, temperature, humidity, cloud and aerosol classification across the multiple instruments of the A-Train A distinction between Givanni\222s A-Train data and the data set in this paper is that we are using a formal merged product of the A-Train. We leverage the NEWS effort that is based on error- and resolution-weighted mean of the input data sets, with associated uncertainty estimates. This provides a formal model of the collective A-Train observations rather than the collection of the individual instrument measurements Each of Giovanni\222s multiple interface instances provides a very simple and easy to use web interface. However, we recognized that sometimes scientists want more than the simple interfaces. Some scien tists may want to process Level 3 products using their own trusted code, or may want to perform variations of their own plots. With Giovanni, the individual scientist wanting more custom advanced capabilities must depend on the Giovanni development team Giovanni is based on the web portal paradigm where users visit a web page and use web tools to find and visualize data. Similar to Giovanni, our client APIs also make data acquisition more seamless. However, our services are based on the different paradigm were the power and flexibility of data analysis and processing are shifted back into the scientists own familiar computing environments. We realize that scientists generally want to perform \223exploratory computing\224 where they can sere ndipitously analyze the data using their own familiar and trusted code 


  17 Giovanni 2 was inherently synchronous where processing was bounded to a single http session. Long service running times still require the user to hold the same http session Similar to our asynchronous Web Service we discussed, the upcoming Giovanni 3 will be supporting asynchronous sessions. They will be using a RSS feed to monitor the service request. Version 3 will also be based on a servicesoriented architecture, wher e Giovanni services can be offered as a standard SOAP Web Services. This is similar to our approach, as well as SciFlo\222s services 12  C ONCLUSIONS  To achieve the science research goal of investigating longterm and global-scale trends in climate, water and energy cycle, and weather variability, we enhanced and improved on existing algorithms to work with distributed and heterogeneous data and information systems infrastructure By developing a service-oriented architecture for discovering, accessing, and mani pulating of NEWS merged A-Train data sets, we can strengthen the interconnectedness and reusability of these services across broader range of Earth science investigations The merged NEWS Level 2 data is a formal model containing the voluminous data from the AIRS, AMSR-E MLS, MODIS, and CloudSat instruments. Previously scientists wanting to perform long-term and global-scale studies encompassing simultaneous measured quantities would quickly face a data acce ss hurdle of first finding the data, then manually downloading them, and finally merging the data into a cohesive model\227before starting their analysis. Additionally the voluminous nature of the data particularly because of the MODIS data\each scientist potentially downloading the same data resulting in redundancy of reprocessing on the client sides. Our paradigm pushes more of the commonly repeated processing onto the server side. Moreover, this avoids repeated downloading of the same data among the science users. We can deliver customi zed averaged, subsetted, and summarized data of the merged A-Train observations to the scientists for them to immediately begin their analysis work We recognized that scientists also often want to perform 223exploratory computing\224 where they can freely explore the aspects of the data and run serendipitous exploration in their own familiar environment. We developed client-side distributed APIs in popular analysis environments such as Matlab, IDL, and Python. Our APIs hide the complexity of Web Services and allow the service capabilities to be embedded in the scientists own computing environments By purposely avoiding the \223web portal\224 paradigm and providing the suite of platform specific APIs in each of these language platforms, we enable the scientists to remain within their own familiar environments to select, process and download the data seamlessly into their environment for their own further analysis. Alternative methods involving web portals force the scientists to leave the environment and manually interact with the web portal to search and download the data We can examine not only long-term changes in amplitude of a single variable but also those among multiple variables Our L3Q clustering method was specifically designed to preserve information about the covariability of multiple observations, such as those from the A-Train.  Weather and climate variability is characterized by changes among atmospheric observables, but those changes have been limited by a lack of observations and analytical techniques We are not aware of any multi-parameter analyses to date The full potential of the A-Train climate record will not be realized until the multi-parameter climatology is understood. The work presented is one method of approaching this difficult problem Our service tool addresses several objectives of the NASA Earth science data community including 1\mprove interoperability to facilitate the transparent access and manipulation of heterogeneous and distributed data by science users, 2\ransition and deploy existing Earth science research analysis tools and software using a 223Service Oriented Architecture\224 \(SOA\ to enhance their reuse potential for other science domains and improve overall awareness and access of these tools by a broad community, 3\ increase users\222 ability to customize their discovery, access, deliv ery, manipulation, and preferred format of data and information 12  F UTURE W ORK  On-demand Level 3T Summaries from Level 3Q We plan to develop services for creating custom summaries of the L3Q data into more refined Level 3T summaries L3T\create their own custom Level 3 products on demand from L3Q. The custom Level 3 products are the transformation of L3Q data based on user-specific objectives such as regression and correlation analyses. The cust om production will generate not only the transformed data but also the statistical estimation of the accuracy of the summarized data based on the distribution of L3Q and the quality of L3Q Delegating the Temporal-Spatial Data Querying Currently, our processing layer utilizes existing and legacy processing code that was developed in IDL, Matlab, and C++. Though the original intent was to be able to adapt existing code and wrap as a service, this meant maintaining its original form of accessing the source data for processing Small modifications were made to enable these codes to quickly access the data based on file path and file naming schemes. However, we want to decouple the file accessibility and processing roles 


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


