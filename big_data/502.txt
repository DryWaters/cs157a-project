An Efficient Algorithm for Mining Association Rules for Large Itemsets in Large Centralized Databases Allan K.Y. Wong S.L Wu and L Feng Department of Computing Hong Kong Polytechnic University Hunghom, Kowloon Hong Kong P.R. China, Email csalwong@comp.polyu.edu.hk ABSTRACT The proposed algorithm 
is derived from the conventional Apriori approach with features added to improve data mining performance These features are embedded in the encoding and decoding mechanisms It has been confirmed by the preliminary test results that these features can indeed support effective and efficient mining of association rules in large centralized databases The goal of the encoding mechanism is to reduce the 
1/0 time for finding large itemsets, and to economize memory usage in a predictable manner The decoding mechanism contributes to speed up the process of identifying different items in a transaction The performance of three different decoding methods will be compared to demonstrate the potential gain delivered by any ingeniously devised decoding approach Keywords Apriori algorithm pe$ormance bottleneck encoding decoding association 
rules 1 INTRODUCTION In this paper an efficient algorithm for mining association rules   in large centralized databases is proposed The new algorithm is derived from the conventional Apriori approach 3,4 by adding the encoding and decoding mechanisms The encoding mechanism reduces the U0 time in tallying the large itemsets and naturally economizes memory usage in a 
predictable manner The decoding mechanism helps speed up the process of identifying specific items in a transaction In order to demonstrate the impact by any ingeniously devised decoding approach on data mining performance, three different decoding methods will be empirically compared The data used in the comparison were test results collected from different experiments performed on the same platform for sequential computation This platform consists of a Sun Ultra machine, and a 
centralized database generated by the public IBM package described in 5 The new algorithm can be represented conceptually by the eight steps as follows 1 LI=[frequent-I itemsets   Read the whole database to identify Level 1's large itemsets L1 and encode every transaction by the rule Vt=Z2  X=TRUE 2 
For\(k=2 Lk-l!={};K  Loop until no more large itemset  3 Ck=apriori_gen\(Lk-l Generate itemsets CK from Lk-1 for level k 4 For\(m=O rn<no-transactions-in-encoded-database m  5 decode-transaction-mfbr_level-k encoded-database Work on V value 6 count-itemsetsJor-Ck 
transaction-m For finding Lk s large itemsets 7 Lk=[itemsetsfor-Ck 2Minsup Identify large itemsets insup for Level k+Lk 8 Find-association-rulesfor-large-itemsets Lk  From large itemsets at all levels  encode \(whole-database 0-7803-5731-0/99/$10.00 01999 IEEE lII 905 


The highlighted portions in step 1 and step 5 are the encoding and decoding mechanisms respectively The decoding approach adopted in the latter mechanism is normally dictated by the philosophy implemented in the former The proposed new algorithm can have many variants which are characterized by the particular decoding methods incorporated If the highlighted portions in step 1 and step 5 are deleted, then the new algorithm is reverted back to the conventional Apriori The operational difference between the two algorithms is that in the new one the database is read only once for encoding purpose as shown in Figure 1 No reading of the database is required again for the rest of the data mining life cycle In the traditional Apriori however the whole database must be read once again when a deeper level denoted by Lk+l is mined for large itemsets The encoding process encode whole-database transforms a large centralized database into a miniaturized and manageable form. It the transformation process it produces the encoded value V,=X2 for every transaction in the database, where X marks the physical position of the item in the transaction The basic logic for the decoding mechanism the highlighted code segment\in step 5 is  if V zZx  V 2 transaction9s-x-item  TRUE 99 I Very large 1 database Read database b only once for the encoding  Proposed  f Trac The database is read for mining large itemsets at every level fi 3 tiona I algorithm Qpriov Figure 1 The main difference between the two algorithms is in the U0 An itemset is large because its chance of X Y and XuY are large itemsets and occurrence is greater than or equal to S b support_count_of-XuY/support-count-of_X minimum support count or Minsup An 2 minimum confidence C The symbol 3 association rule X+Y holds provided that a abstracts the relationship R between X and Y Ill 906 


2 THE NEW ALGORITHM b c divided into three stages d The development of the proposed algorithm is a evaluate the I/O performance of the conventional Apriori approach devise the encoding method devise efficient decoding methods for the encoding measure above and test the overall data mining efficacy of the algorithm when the encoding and decoding methods are combined 2.1 YO Performance Evaluation experiments in which the conventional Apriori algorithm was executed without the code The aim is prove that an appropriate encoding segment for computation. This deletion of the method is necessary for improving the I/O code segment is illustrated by the absence of performance of the traditional Apriori program statements after step 4 in the following approach The evaluation was performed with pseudo-program 1 2 For\(k=2 Lk-]!={};K 3 4 For\(m O m no-transactions-in-encoded-database m    7 L~=[frequent-l itemsets  Read whole database to identify Level 1's large itemsets L1 encode \(whole-database Ck  ap rio ri_gen Lk-1  and encode every transaction by the rule V,=D  x=TRUE Generate itemsets CK from Lk-1 for level k   The computation code segment is deleted from the traditional Apriori  The results from many evaluation experiments bottleneck of the traditional approach is in the with different database sizes indicate that the I/O operation Therefore the bit-encoding conventional Apriori approach consistently method is proposed for inclusion in the new spends more than 70  of its runtime on I/O algorithm to reduce the number of U0 accesses Obviously the performance accesses 2.2 The Bit-Encoding Method two objectives a to reduce the number of YO accesses in data mining, and b to speedup the Encoding here means reorganizing and decoding process Generally speaking such a transforming a large database into a data encoding process is an essential part of any miniaturized and manageable structure to fulfill typical knowledge discovery approach 6 2O 2 22 23 X 1 2 4 8 Encoded 4ariable V,=C2 Table 1 Every item in a transaction is represented by its 2 value X is a bit's physical position IJI 907 


The are two mandatory requirements for the position in the transaction The whole bit-encoding method: \(a the database should be transaction is then represented by its unique read only once within the whole life cycle of encoded value The bit-encoding concept that data mining, and \(b\memory utilization should transforms a large database into its miniaturized be maximized In this encoding method every and manageable form is exemplified by Table item in a transaction is represented by a 2 1 where the encoded value for the value where X marks the item's physical transaction is equal to 2l 22  23  14 600 d 500 g400 Q f300 3 e200 n r 0 I E 100 0 1 I I I  Proposed algorithm Other algorithm 4 bytes zh Other algorithm 2 bytes for 1 item for 1 item 10 32 64 128 Average no of items per transaction Figure 2 Efficient predictable memory utilization by the proposed encoding method first The bit-encoding method also maximizes memory utilization in a predictable manner This is achieved for the following reasons a memory usage is economized by encoding items in bits instead of bytes and b bit representation is linear On the contrary it is necessary in the conventional Apriori to read and decode items that are encoded in a predefined number of bytes For verifying the 3 THREE DECODING METHODS The decoding method can seriously affect the performance of the data mining process For demonstration of this point the test results from claim that memory usage is linear and economical, many experiments were performed with different database sizes generated by the public D3M package Figure 2 demonstrates the trend pinpointed by the results from these experiments For the presented case the database had 100,000 transactions D100K constructed out of 1000 N1000 possible items three different methods will be compared The three methods are as follows a Basic decoding This algorithm of the following logic is executed repeatedly until III  908 


i which initialized to the encoded value V is reduced 0 if i 2 2x then i=i-2 Yh-item-in-transaction  TRUE In this approach the encoded value of 14 in Table 1 would represent the TRUE states for those items in the 2 2x=2  and 2x=3 positions of the first transaction. During the decoding operation the encoded transaction is scanned bit by bit starting from the most significant bit b Binary decoding This approach is based on the basic decoding approach except that the first step is to find the most significant TRUE position for X7 by binary search The X=X-I else X=X-1 I U 3.1 Test Results I The three aforementioned decoding methods were evaluated in different experiments performed on a Sun Ultra machine running on Unix It was found that the logarithmic aim is to slash the scanning time of the basic decoding approach by half c Logarithmic decoding In this approach which saves decoding time by eliminating the entire scanning process the following logic is executed repeatedly until i initialized to the encoded value V is reduced to 0 i  i  2 integer\(log2W  9 integer\(logz\(i  TRUE The integer\(Zogz\(i operation converts Zogz\(i into an integer by truncating whatever after the decimal point For example it yields the integer values of 3,2 and 1 from the encoded value of Vt  14 successively approach is consistently the most efficient among the three The databases used in these experiments were generated by the same IBM package 4 Each test case involved a specific combination of database size and number of items Time\(sec Figure 3 Comparison of four algorithms T10.14.D100K.N200 m 909 


Figure 3 demonstrates the difference in performance among the four algorithms namely the conventional Apriori 4 bytes is used in this case for encoding a data item and three variants of the new algorithm Each variant is backed either by the basic, the binary or the logarithmic decoding method The database for producing the test data for the comparison was generated by the public IBM package with the following statistics a Total number of transactions in the database D is 1 00,000 100K 4 CONCLUSION The aim of in this project is to improve the performance of the conventional Apriori algorithm that mines association rules The approach to attain the desired improvement is to create a more efficient new algorithm out of the conventional one by adding the encoding and decoding mechanisms to the latter. In order to demonstrate the importance of efficient decoding to high data mining performance three methods namely basic binary and logarithmic were evaluated These three decoding methods were devised with respect to 5 REFERENCES I P Adriaans and D Zantinge Datu Mining Addison Wesley 1996 2 S.P Jong 223Using a Hash-Based Method with Transaction Trimming for Mining Association Rules\224 ZEEE Transactions on Knowledge and Data Engineering Vol 9 No 5 September/October 1997 pp 8 13-825 3 R Agrawal and J.C Shafer 223Parallel Mining of Association Rules\224 ZEEE Transactions on Knowledge and Data Engineering Vol 8 No 6 December 1996 pp 962-969 b Average number of items per transaction T is 10 c Total number of items in the database N for forming transactions is 200 d For large itemsets the number of transaction patterns is 1000 the average number items in a transaction \(I is 4 The test programs for collecting the data for Figure 3 were written in C In the data mining experiments large itemsets were tallied up to the 8\222h level Lk=8 the bit-encoding approach that maximizes memory utilization in a predictable manner The findings from different experiments have confirmed that the logarithmic decoding method is the most efficient among the three. It can speed up the data mining process significantly as demonstrated in the performance comparison. The imminent future work is to investigate how the logarithmic approach can be applied effectively and efficiently to large-scale distributed data mining particularly in the Internet environment  R Agrawal T Imielinski and A Swami 223Mining Association Rules Between Sets of Items in Large Databases\224 Proceedings of International Conference on Management of Data SZGMOD 93 May 1993 pp 207-216 5 IBM Almaden Research Center 223Synthetic Data Generation Code for Association and Sequential Patterns\224 http://www.almaden.ibm comialmaden/projects.html 1998 6 223From Data Mining to Knowledge Discovery An Overview\224 in Advances in Knowledge Discovery and Data Mining U.M Fayyad G Piatetsky-Shapiro P.Smyth, and R Uthurusamy Ed AAAI/MIT Press 1996 III 910 


1232.119s 90,146,85 FHUT 185.933 s 20,142,05 HUTMFI PEP 103.492 s 21.582 s 9,150,058 1,331,158 2.904 s Mushroom at 1 support Scaled 1 Ox vertically With Reordering Figure 8 FH+HM 103.323 s 9,150,030 Figures 8 and 9 show the effects of each component of the MAFIA algorithm on the mushroom dataset at 1 minimum support The number of transactions was increased by repeating all transactions in the database by a certain scaling factor We call this form of scaling vertical scaling In this case mushroom was scaled ten times vertically Note that vertical scaling will not change the search space and will only affect the time taken for counting the support of itemsets The components of the algorithm are represented in a cube format where the running times and number of lattice nodes visited during the MAFIA search for all possible combinations are shown The top of the cube shows the time for a simple traversal where the full search space is explored while the bottom of the cube corresponds to all three pruning methods being used Two separate cubes with and without dynamic reordering\rather than one giant cube are presented for readability Note that all pruning components yield some savings in running time but that certain components are more effective than others In particular HUTMFI and FHUT yield very similar results since they use the same type of superset pruning but with different methods of implementation The efficient MFI lookups that HUTMFI uses to check for frequency explain why HUTMFI outperforms FHUT see Section 3 It is also FH+PEP HM+PEP 16.925 s 8.943 s 1,134,863 535,813 Mushroom at 1 support Scaled lox vertically Without Reordering Figure 9 interesting to see that adding FHUT when HUTMFI is already performed yields very little savings i.e from HM to HM+FH or from HM+PEP to ALL the running times do not significantly change HUTMFI checks for the frequency of a node\222s HUT by looking for a frequent superset in the MFI while FHUT will explore the leftmost branch of the subtree rooted at that node Apparently, there are very few cases where a superset of a node\222s HUT is not in the MFI but the HUT is frequent PEP has the biggest effect of the three pruning methods All of the running time of the algorithm occurs at the lower levels of the tree where the border between frequent and infrequent itemsets exists and since PEP is most likely to trim out large sections at the lower levels this pruning yields the greatest results  Dynamically reordering the tail also has dramatic savings cf Figure 8 with Figure 9 It is interesting to note that without PEP dynamic reordering runs nearly an order of magnitude faster than the static ordering while with PEP it is 223only\224 3-5 times faster Since both PEP and reordering remove elements from a node\222s tail it is not surprising that they overlap in their efficacy 5.2 Comparison With Depthproject We tested the algorithm on 223real\224 datasets containing long patterns that have been used in earlier work  1,7 449 


These datasets are publicly available from the UCI Machine Learning Repository http://www.ics.uci.edu/-mlearn/MLRepository html At the lowest supports tested the longest patterns in these databases have over 20 items, making any algorithm that examines all possible subsets of these patterns or a significant portion thereof infeasible This makes the task of finding the patterns computationally intensive despite the small size of the databases For some of the experiments the databases were scaled vertically by Time Comparison on Connect4.data 1000 MAFIA  Depthproject 100 h  U 10  I 1 I 0.1 90 80 70 60 50 40 30 20 10 Min Support  Figure 10 I IlllW wJrlllJ"llsull UII c.lless.ulla 100 MAFIA DP I 60 55 50 45 40 35 30 25 20 Min Support  Figure 12 concatenating copies of the database together This only affects the time counting takes since the bitmaps compressed or not are longer and the search space examined remains constant Figures 10  12 illustrate the results of comparing MAFIA to our implementation of the Depthproject method the state-of-the-art method for finding maximal patterns I The x-axis is the user-specified minimum support, while the y-axis uses a logarithmic scale to show the running time Time Comparison on Mushroom.data 10 MAFIA  Depthproject I 1 h Y cn  i I o 1 0.01 10 8 6 4 2 0 Min Support  Figure 11 scaleup OT r;ness.aata 45 40 35 MAFIA DP 30 rr 25 20 E 15 10 5 0 0 5 10 15 20 25 Scaleup factor Figure 13 Table 1  Reduction Factor of Nodes Considered Due to PEP Pruning 450 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


