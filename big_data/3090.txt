On Incorporating Subjective Interestingness Into the Mining Process Sigal Sahar gales@post.tau.ac.il Tel-Aviv University Abstract Subjective inter~stingness is at the heart of the successful discovery of association rules To deter mine what is subjectively interesting users\222 domain knowledge must be applied 7 introduced an ap proach that requires uery little domain knowledge and interaction to eliminate the majority uf the rdes that are subjectively not interesting In this paper we investigate how this approach cun 
Ire in corporated into the mining process the Irenefits und disadvantages of doing so and ezamine the results of its application to real databases 1 Introduction 4 defined Knowledge Discovery in Databases KDD as 223   the non-trivial process of identi fying valid, novel potentially useful and ultimately understandable patterns in data.\224 Interesting ness distinguishes the 223valid, novel potentially use ful and ultimately understandable\224 patterns from those that are not Since the problem of interest ingness is ultimately subjective, subjective criteria that explicitly employ users\222 domain knowledge are 
needed to completely resolve this problem Incor porating subjective interestingness into the niining process has obvious advantages less patterns will be mined and those are more likely to be inter esting to the users However a drawback of this tactic is that the results may be tailored to match only the prespecified subjective criteria exploring slightly different interestingness criteria could re quire reexecuting the entire mining process To reduce the number of mined patterns to make their post-processing easier we need to incorporate into the mining geneml subjective interestingness criteria that 1 
are very easy to define and 2 apply to the interests of a wide audience base In this work we tackle this task by incorporating the method of 7 into the mining process to create an algorithm that is simple general and effective \(see Section 3 We also examine the results and reper cussions both benefits and disadvantages of the application of this method on the same databases used in 171 Note that as in 171 our goal is to reduce rather than eliminate the number 
of not-interesting rules mined Related are works such as 1,6,9 51 that incorpo rate subjective interestingness iuto the mining pro cess but rely on the availability of doniain experts to express all the needed information in a predefined grammar See 81 for a detailed review 2 Definitions and Preliminaries Let A be a set of attributes over the booleau domain V he a set of transactions over A and i itemset denote an itemset of size i For A 
B C 11 4 n B  0 the association rule A  B is defined 2 to have support s and confidence c if 8 of 2 contain A U B and c of 2 that contain 4 also contain B We refer to A a3 the assumption of the rule and to B as its consequent 2]k algorithm outputs all the association rules that have at least predefined support and 
confidence thresholds In this work we build on the AprioriTid algorithm Z Let R be the list of association rules mined over A Let U b E A and p  U  b The family of p in R is defined 171 as familyn\(p  r E Rjr  A  B a E A b E B U p p is then defined as the ancestor rule of the family of p 
in R We now define the binuclear family of a 2 itemset u,b as biFn\({a:b Dsf furnilyn\(u f b U famiryn\(b a u,b is referred to as the an cestor itemset and is said to span biFn\({a,b A binuclear-family is also be defined to be spanned by an ancestor rule biFn\(p  hFn\({a b Note that biFn\(u  b  biFn\(b  a Def Dsf 0-7695-1754402 17.00 Q 2002 IEEE 681 


3 The Algorithm 1 T  ancestor itemsets classified for deletion 2 Zr new-attributes\(T,Z 3 Li  large 1-itemsets over VI CI database 4 C2  apriori-gen\(l1 6 delete-subjectively-not-interestin~\(C I\\Z  1 Ulr    5 detennine-support\(C*,c2,C1 The goals of our algorithm are 1 Simplicity keep the process simple by asking users only a few, easy classification questions 2 Generality the results of the mining process with the integrated subjective interestingness will be general enough to be applicable to a wide user base for further interestingness post-processing 3 Effectiveness integrating this type of sub jective interestingness into the mining process will result in mining of much fewer association rules\222 q uses domain knowledge of the kind 223not interested in Jamilyn\(p for interestingness post processing The families of a  b and b  U cannot be distinguished during the mining process when only itemsets are recognized To integrate user pref erences of the type used in 7 into the mining we have to use generalized preferences of the type 223not interested in biFn\(p We can decrease the num her of mined patterns by eliminating one or more of the frequent itemsets discovered during the min ing The challenge in deleting a frequent iternset I  u,;b is in ensuring that even though I is removed; rules of the type 4  D and D  A  a bi E 4 and a bi D 1 that would have been mined had I not been deleted will still be mined An example for a rule in Equation 1 is tomatoes,cucumbers  sugar if I,t cucumbers tomatoes is deleted 3.1 Selecting  Classifying Ancestor Itemsets We only ask users a one-dimensioned classi fication question is the ancestor iternset inter esting or not-interesting to the user that is is the binuclear-family spanned by the ancestor item set interesting/not-interesting If a user classi fies an ancestor itemset as not-interesting it is marked for elimination used in Section 3.2 0th erwise no action is taken on that ancestor iteni set We select ancestor itemsets for user classi fication to be the 1-3 inost frequent 2-itemsets\222 in order of their frequency These limited selec tion and classification processes naturally guaran 221For example integrating the first classification resulted on average io the mining of fever than 80 of the rules that would have been mined otherwise see Section 4 for details tLarger iiernsets itemsets with more than two attributes are considered loo complicated for the naive clas~ification that we preserve while the biFn\({o b  fomrivn\(o t bl U fomdyn\(b a the biFomily of a,b,c would be the union of the 223families\224 of a  bc ob t e b  oc bc t a ac  b c 7 ob and possibly aim of a t 0 I 4 c b i c b t a e t a and e  b with an even larger list far larger ancestor itemsets Figure 1 The Algorithm Part I tee our three goals classifying an ancestor item set is equivalent to classifying two ancestor rules as interesting/not-interesting biFn\({u b  U is easy to classify because it consists simply of fumilyn\(a  b U familyn\(b  a and if U b E X then X  Y Y  XgU The other reason the clas sification is simple is that users only need to clas sify the 1-3 most frequent 2-itemsets These item sets are very likely to describe relationships known even to naive users, making them easy to classify Since these rnost frequent P-itemsets describe re lationships that naive users are probably familiar with a wide audience base is likely to classify them similarly yielding generality Finally since the top three most frequent 2-itemsets are likely to have very large binuclear-families likely to be classified as not-interesting known the limit on the nuniber of ancestor itemsets also yields effectivenesst 3.2 The Mining Process The domain knowledge available to us is a list of pairs I c where I is an ancestor iternset and c is its classification interesting/not-interesting For I classified as not-interesting we cannot simply limit the search space of large itemsets by deleting the iteniset a,b see Equation l but we can alter the search space Figure 1 outlines the first part of our algorithm In line 1 we initialize I\224 to be the set of ancestor iternsets classified by the user for deletion Line 2 is described in Section 3.2.1 lines 3 5 in Section 3.2.2 and line G in Section 3.2.3 The secoxid part of the algorithm consisting of iterations for k 2 3 and the rule generation is performed as in 121 We use the notations Lt Ck and ek defined in 121 3.2.1 Creating New Attributes For any iternset X  Q   xn let vx be a new attribute where the value of vx on any transac In 221I prior to each classification users can rely on the RSCL defined in i as an indicator of the potential impact of the next CLaSsifiCation they make In our case when interest ingness is incorporated in one step during the mining process this kind of indication is not possible 682 


tion T will be TRUE if and only if the value of every xi E x is TRUE in 7 For each element in Y  us:6i where ui b E A we create a new attribute vy defined for Y  a 6 and for n  3 fur intersections as in YZ below We now define AI D A UZr where Zr  uv and DI to be the extension of 2 spanned by Ax We examine the creation of new attributes for two examples for YI  6}~{c>d Ir  c{a,b V{e,d For YZ  a:b U three new attributes are created Ir   ya a,b,e The need for the third attribute will become clear in Section 3.2.3 3.2.2 Starting the Modified Mining Process In line 3 of Figure 1 we initialiae L1 the set of large I-itemsets from AI the set of candidate large Litemsets Since the ancestor iternsets in Y are the most frequent 2-itemsets alniost all mining pro cesses will discover all of IT as-large I-itemsets During tlie same pass on DE C1 is constructed apriuri-gen on line 4 and the eliniination of the candidate large 2-iteinsets that do not have at least the predefined support threshold on line 5 are ex ecuted as in 2 but over 271 and AI 3.2.3 Deleting Ancestor Iternsets For TI  b where there is only one ancestor itemset to be deleted  U b we delete the fol lowing itemsets 1 u:b the itemset classified as not iuteresting 2 U ab since it is equivalent to u:u;6  e and 3 6,21.b Note that if e is a large iteinset all three of the above 2-itemsets are large and will actually be deleted from Cz This observation holds for all the iternsets below as well For YZ  el el where el  U 6 e2  c d one of two cases will occur Case 1 u,b n c:d  0 then Yz  vob:uCd and wedelete 1 u:b 2 U:v.b 3 b:wnb 4 c:d 5 c:vcd and \(6 d;ued using the same reasoning as for Yl  Case 2 01  u:b e2  u;d U  6  d U  c theu IT  Uab;U,d:V,bd and we delete the 14 2-itenISetS 1-6 U;b U,d U;V,b b;vab U>'Gad d:%d 7-14 U;Vabd d:'Gabd Uab:'Uabd uad;vabd d:Vob 6:t~ad U.b,'tiod where itenisets 7-14 are equivalent to bd Note that deletions 7-14 are not syninietric as ubd 6 YZ We examine this case in Y3 below Case 2 clarifies the need for Dabd Without it rules such as U b d  e will not be inined when users classify U 6 and U d as not-interesting Note that we do not need to eliminate any item sets in the third iteration when the 3-itemsets are constructed Itemsets such as U z,~.b that we would want deleted are automatically deleted dur ing the pruning stage of candidate itemsets since at least one of its 2-iternsets for example U vab is not a member of CZ as it has been deleted in one of the 14 deletions above Note that for T3  a~b}~{u,d  vGb d tJabdr Ubd and 21 Z-iteniset deletions are required the 14 deletions of Case 2 and 15 b;d 16-17 b,Wbd d:Vbd equivalent tU ubd aid 18-21 a;.bd b;z vadlvbd and tJabd Wad whicli are equivalent to Jabd 4 Experiments With Real Data We ran our algorithm on the same DBs used in 7 1 The Grocery DB describes 67,470 shoppiug baskets using 1,757 attributes of an online Israeli grocery store sparsest DB 2 The WWW DB describes the accesses of tlie 2;336 heaviest users to 15 site categories densest DB 3 The Adult DB is based on tlie 45,222 entries with no missing values from the 4dult dataset 3 discretisized as in 7 into 171 boolean attributes We mined each DB with zero one two and three ancestor itemsets classified for deletion Nut surprisingly the ancestor itenisets corresponded to the ancestor rules used in 7 allowing us to readily compare the results 4.1 Reduction in Number of Rules Mined Figure 2 depicts the re sults of mining the U'WU the Grocery and the Adult DBs with 0-3 ancestor iteni sets classified for deletion o>s DIII mz&---mm and two mining thresholds each Each histogram depicts the nuinher of rules mined with a specific threshold and each bar represents the num ber of rules mined when  3 ancestor itenisets were used in the mining The reduction in tlie nun ber of rules mined as result of eliminating the first few an cestor itenisets is more sub A  stantial than that of elinii    Figure 2 nating latter ancestor item sets an average of more than 23 due to the dele tion of the first ancestor itemset just under 18 683 


and 12 for the second and third ancestor item sets Tliis is parallel to the elimination of larger number of rules by the first few ancestor rules in 7 as compared to the elimination of latter ancestor rules The explanation to both events is the same captured by RSCL 71 and is the reason we limit the number of ancestor itemsets to a niaxiinuui of 3 Overall the reduction in the number of mined rules due to the deletion of the three ancestor iteinsets was approximately 47 for the WWW DB 36 and 40 for 3.5 and 6 mining thresholds for the Grocery DB and 45 and 47 for the Adult DB 4.2 Resource Consumption 1 Mining is normally dominated by the time it takes to calculate the support of the candidate iteni set5and Figure 3 depicts the size of Ck for 0-3 ancestor itemsets deleted for the Adult DB 4ft_er the fourth iteration the size of Ck decreases more rapidly when more ancestor iternsets are deleted, requiring less iterations to coniplete the mining when more ancestor itemsets are used This behavior was manifested after the secorid iteration in the WWW DB See Section 5 for conclusions Cutnulative iteration run  w times in seconds on a Conipaq ProLiant DL580 4 700MHz Xeon CPUs with 1GB R.4M running Solaris 7 and Per1 5.005-03 are depicted in Figure _n 4 Note the small nuniber of iterations for the sparse Grocery DB compared to the large number of iterations for the dense WWW DB Note that the relatively short mining times for the WWW DB combined with the large nuni ber of mined rules from this dense DB resulted in better ouernll runtimes when incorpo I rating the first ancestor item set into the mining Tlie mod ified AprioriTid runtime with 3.5 thresholds was 115 seconds with 0 or 1 ancestor itemsets The total runtime including ap-genrules was 140secs when no ancestor itein sets were used and 130secs wheii one ancestor iteni set was used with 29910 vs 22932 rules When mining with 6 thresholds AprioriTid took 51 sec onds with 0 or 1 ancestor itemsets 65 seconds total I i Figure 3 I i 1  f 1 _un  5 d __,__ I Figure 4 for 0 ancestor itemsets and 54 seconds total with 1 ancestor iteinset with 9206 vs 6972 rules mined 5 Conclusions and Future Work To fully address the problem of interestingness subjective interestingness criteria must be used In this work we investigated liow to incorporate 7 into a simple general arid effective rnining process The reduction in the number of rules outputted by the altered mining process is similar to the post processing elimination in 171 Tlie domain knowl edge we incorporate can only be used to alter the search space by flattening it resulting in niore item sets discovered in the first few iterations making those iterations execution time longer The execu tion time of later iterations is reduced and the last one or two iterations are often eliminated How ever in general the execution time of this new rnin ing process that outputs significantly fewer rules is longer than that of mining the entire set of associa tion rules Our conclusion is that there is not always a benefit to incorporating subjective interestingness into the mining process The output of the new niiri ing process is significantly reduced, almost by a half inaking post-processing easier but post-processing can often achieve similar results with shorter run time Future work in this area includes investigating and formalizing types of doniain knowledge that can be used to decrease the size of the search space References I G Adornavicius and A Tuzhilin Discovery of actionable patterns in datahases The action hierarchy approach In SIGKDD pages 111-114 1997 121 R Agrawal hl Heikki R Srikant H Toivonen and A 1 Verkamo Advances in Knowledge Discovery and Data Mining chapter 12 pages 307-328 1996 3 C Blake and C Men UCI reposi tory of machine learning databases 1998 http://wwa.ics.uci.edu/5rnlearn/ml~epasito~y.html 4 U M Fayyad G Piatetsky-Shapiro and P. Smyth Ad uances in Knowledge Discovery and Doto Mining chap 51 R Kg L Lakshmanan I Han and A Pang Exploratory mining and pruning optimizations of constrained associ ation rules In SIGMOD pages 13-24 1998 GI B Pdmanabhan and A Tuzhilin Small is beautiful Discovering the minimal set of unexpected patterns In SIGKDD pages 54-63 2000 7 S Sahar Interestingness via what is not interesting In SIGKDD pages 332-336 1999 ter 1 pages 1-34 1996 181 S Sahar Interestingness preprocessing In ICDM pages 489-496 2001 9 R Srikant Q Vu and R Agrawal Mining association rules with item constraints SIGKDD pages 67-73 1997 684 


Data Source and systems In order to test the algorithm with the real world application we selected 10000 transaction records as our testing data set In our example data, six attributes i.e income, education level, sex age marital status and relationship with others in family are taken into account Execution time The execution time in this experiment is carried out to show the performance of the algorithm FSETM on the DELL Optiplex GXlMDSK500 In order to test the algorithm we use the 0.2 0.25 0.3 0.35 0.4 0.45 as the minimum supports We choose these values because they can help to generate a reasonable number of large itemsets Comparing the algorithm SETM proposed in l we see that algorithm SETM and algorithm FSETM are very stable. These result are presented in Table 5 in which Table a shows the execution time of the algorithm FSETM b the execution time of the algorithm SETM b Table 5 Execution time 5 Conclusion We designed a preliminary version of querying fuzzy association rules FSETM for effective data mining in relational databases The major contribution of this paper is that it developed the algorithm SETM into mining fuzzy association rules applications In the future we will attempt to use the algorithm in financial data analysis, such as stock movement prediction 0-7803-707&3/0U$lO C IEEE Acknowledgement This work is supported by the Hong Kong Polytechnic University Central Research Grant G-V762 Reference M Houtsma and A Swami 223Set-oriented Mining for Association Rules in Relational Databases,\224 In proceeding of the IIIh International Conference on Data Engineering 1995 pp 25-33 J.M Medina 0 Pons and M.A Vila GEFRED 223A Generalized Model for Fuzzy Relational Databases,\224 Information Sciences 76 1994 pp 87-109 D A Chiang L R Chow and Y F Wang 223Mining time series data by a fuzzy linguistic summary system,\224 Fuzzy Sets and Systems 112,2000 Pages, pp 419-432 J Han Y Fu K,Kopershi W Wang and O.Zaiane 223DMQL A Data Mining Query Language for Relational Databases,\224 1996 SIGOMOD Workshop on Research Issues on Data Mining and Knowledge Discovery DMKD\22296\Montreal, Canada June 1996 M R Civanlar and H J Trussell 223Constructing membership functions using statistical data 224 Fuw Sets and Systems vol 18, 1986 pp 1-14 T Kohonen, \223Self-Organization and Associate Memory,\224 Springer, Berlin 1988 Page 2993 


I I Fig.3 BNN-based fuzzy control system FN-16 MMC  l6rules-Nonlinear ____ 16rules-Linear  33rules-Nonlinear   33rules-Linear 16.78 810.27 3.50 558.87 16.77 808.43 3.48 557.06 Training Number\(*50 Fig.4 SSE vs training cycles Table 2 Control Performances NUMBERS OF SAMPLE Fig.5 Output responses of the system with net of FS-33 Table 1 Control rules cl NB\(NM\(NS ZR IPS iPM IPB 1 E I  16 rules II-707 


 I                  222                                I I m 2s a Io 40 I 20 10 I..sdlima Irnssap Fig 7 Extracted measusement trajectories for experiment I compare with Figure 6 Fig 10 Range VI time for detected rems for experiment 2 Returns that are not grouped into a mjectary are discarded 0.5 Oi I 5 4 8 2 lrnMonirnl Fig 8 Estrmated tralectory and map for experiment 1 c        221          2236     I A*+-&s     2       ti i if t  o 4 2 0 1 1 6 I 10 I2 1 16 e Fig 9 Canesian projection of detected returns for experiment 2 i yI 40 I a 10 Wa*Ium,_ Fig 11 Extracted measusemem trajectories far experiment 2 IS 6 Fig 12 Raw data estimated sensor uajecton and object locations far experiment 2 969 


appear complex and hard-to-interpret when considered via Cartesian projection are much easier to explain us ing segmented echo trajectories The system incorporates automatic fixation which we believe is an essential part of this type of sonar interpretation approach This has been applied for example to good success in Kuc's work on object recognition 14 The problem of balancing multiple fixations is an unexplored problem \(for example observation of known objects to provide good navigation vs observation of unknown objects for mapping Future work will address 1 3-D tracking 2 the incorporation of spectral information and 3 extension to rough surfaces The primary motivating application for this work detection of undersea buried mines via low frequency synthetic aperture sonar is inherently a 3-D problem Recently we have obtained a large amount of data at the GOATS 2002 experiment using a synthetic aperture sonar that provides this information via use of two eight-element arrays The formulae presented in this paper apply in two dimensions the three dimensional equivalents are presently being derived and tested and initially results are very promising The issues of exploiting spectral information and cop ing with rough surfaces are inter-related The study of echo reflection from rough surfaces has been studied by Bozma and Kuc 3 The matching of new data to previ ously mapped features \(for recognition andlor navigation purposes is tremendously important This paper has not addressed this issue, hut it is anticipated that the trajectory segmentation method will be a valuable tool for early processing as input to recognizing previously mapped features Acknowledgments The authors are grateful to the entire MIT GOATS 2002 team, including H Schmidt P Newman M Bosse D Eickstedt J Edwards W Xu T.C Liu R. Damus J Morash S Desset J Dryer C Bohner and M Grund The authors also thank P Damhra for his efforts in creation of the robotic gantry for the testing tank This research has been funded in part by NSF Career Award BES-9733040, the MlT Sea Grant College Program under grant NA86RG0074 project RCM-3 and the Office of Naval Research under grants N00014-97-0202 and N00014-02-C-02 10 VIII REFERENCES I W Au The Sonar of Dolphins New York Springer Verlag 1993 121 B Barshan and R Kuc Differentiating sonar reflections from corners and planes by employing an intelligent sen sor IEEE Transactions on Panern Analysis and Machine Intelligence PAh41-12\(6 lune 1990 31 0 Bozma and R Kuc Characterizing pulses reflected from rough surfaces using ultrasound J Acoustical Society of America 89\(6 lune 1991 141 0 Bozma and R Kuc Single sensor sonar map building based on physical principles of reflection In Pmc IEEE Int Workshop on Intelligent Robots and Systems 199 1 5 V Braitenberg Vehicles: Experiments in Synthetic Psy chology The MIT Press 1984 61 R A Brooks Cambrian Intelligence The Early History of the Nw AI The MIT Press 1999  J Edwards, H. Schmidt and 1 LePage Bistatic synthetic apenure target detetection and imaging with an auv IEEE J Ocean Engineering 26\(4 2001 181 A Freedman A mechanism of acoustic echo formation Acustica 12\(1 1962 9 W E L Grimson Object Recognition by Computer The Role of Geometric Constraints MIT Press 1990 With contributions from T Lozano-Perez and D P Hut tenlocher IO A Heale and L. Kleeman Fast target classification using sonar In Pmc IEEE Inr Workshoo on Intellinent Robots and Systems pages 14461451 2601  Ill B K P Horn Robot Vision MIT Press 1986 I121 L Kleeman and R Kuc Mobile robot sonar for target localization and classification. Technical Repolt ISL-9301 Intelligent Sensors Laboratory Yale University, 1993 Perception as Bayesian Inference 1996 Fusing binaural sonar information for object recognition In IEEWSICWRSJ International Conference on Multisensor Fusion and Integrarion for Intelligent Sys tems pages 127-135 1996 I51 R Mann and A Jepson Non-accidental features in learning In AAAI Foll Symposium on Machine Learning in Vision 1993 161 D Mm Vision New York W H Freeman and Co 1982 I71 B A Moran I I Leonard and C Chryssostomidis Curved shape reconstruction using multiple hypothesis tracking IEEE J Ocean Engineering 22\(4 1997 I81 I N Newman Marine Hydrodynamics The MIT Press 1977 191 H Peremans K Audenaen and C J M Van A high resolution sensor based on tr-aural perception IEEE I131 D C bill and W Richards editors I41 R Kuc  Transactions on Robotics And Automation 9 1 Feb 1993 USA 20 W Richards editor Natural Computation The MIT Press 1988 PI1 W Richards I Feldman and A. Jepson From features to perceptual categories In British Machine Vision Confer ence 1992 22 R J Rikoski John I Leonard and Paul M Newman Stochastic mapping frameworks In IEEE International Conference on Robotics and Automation 2002 1231 K D Rolt Ocean platform and signal processing effects on synthetic aperture sonar performance Master's thesis MIT Februaty 1991 High-Level Vision Object Recognition and Visual Cognition The MIT Press 1996 24 S Ullman 970 


I Plenary Panel Session J Future Directions in Database Research  456 Chair Surajit Chaudhuri Microsoft Corporation Panelists Hector Garcia-Molina Stanford University Hank Korth, Bell Laboratories Guy Lohman IBM Almaden Research Center David Lomet Microsoft Research David Maier Oregon Graduate Institute I Session 14 Query Processing in Spatial Databases I Chair Sharma Chakravarthy University of Florida Processing Incremental Multidimensional Range Queries in a Direct Manipulation Visual Query Environment  458 High Dimensional Similarity Joins Algorithms and Performance Evaluation  466 S Hibino and E Rundensteiner N Koudas and K.C Sevcik Y Theodoridis E Stefanakis and T Sellis Cost Models for Join Queries in Spatial Databases  476 Mining Association Rules Anti-Skew Algorithms  486 J.-L Lin and M.H Dunham Mining for Strong Negative Associations in a Large Database of Customer Transactions  494 A Savasere E Omiecinski and S Navathe Mining Optimized Association Rules with Categorical and Numeric Attributes  503 R Rastogi and K Shim Chair: Anoop Singhal AT&T Laboratories S Venkataraman J.F Naughton and M Livny Remote Load-Sensitive Caching for Multi-Server Database Systems  514 DB-MAN A Distributed Database System Based on Database Migration in ATM Networks  522 T Hara K Harumoto M Tsukamoto and S Nishio S Banerjee and P.K Chrysanthis Network Latency Optimizations in Distributed Database Systems  532 I Session 17 Visualization of Multimedia Data I Chair Tiziana Catarci, Universita di Roma 223La Sapienza\224 W Chang D Murthy A Zhang and T.F Syeda-Mahmood Global Integration of Visual Databases  542 X 


The Alps at Your Fingertips Virtual Reality and Geoinformation Systeps  550 R Pajarola l Ohler P Stucki K Szabo and P Widmayer C Baral G. Gonzalez and T.C Son Design and Implementation of Display Specifications for Multimedia Answers  558 1 Session 18 Management of Objects I Chair: Arbee Chen National Tsing Hua University P Boncz A.N Wilschut, and M.L. Kersten C Zou B Salzberg, and R Ladin 0 Wolfson S Chamberlain S Dao L Jiang, and G. Mendei Flattening an Object Algebra to Provide Performance  568 Back to the Future Dynamic Hierarchical Clustering  578 Cost and Imprecision in Modeling the Position of Moving Objects  588 ROL A Prototype for Deductive and Object-Oriented Databases  598 A Graphical Editor for the Conceptual Design of Business Rules  599 The Active HYpermedia Delivery System AHYDS using the M Liu W Yu M Guo and R Shan P Lang W Obermair W Kraus and T Thalhammer PHASME Application-Oriented DBMS  600 F Andres and K. Ono S Chakravarthy and R Le S Mudumbai K Shah A Sheth K Parasuraman and C Bertram ECA Rule Support for Distributed Heterogeneous Environments  601 ZEBRA Image Access System  602 Author Index  603 xi 


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


