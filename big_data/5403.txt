Collaborative Book Recommendation Based on Readersê Borrwoing Records 
liuxinjackie@gmail.com,{ ehaihong,jdsong,mnsong,tongjunjie}@bupt.edu.cn 
  
Abstract 
  Liu Xin 1 E Haihong 1 Song Junde 1 Song Meina 1 Tong Junjie 1 1 PCN&CAD Center Lab, Beijing University of Posts and Telecommunications, China 
Book recommendation is an important part and task for personalized services and educations provided by the academic libraries. Many libraries have the readers borrowing records without the readersê rating information on books. And the collaborative fi ltering \(CF\orithms are not proper under this circumstance. To apply the CF algorithms in 
book recommendation, in this paper, we construct the ratings from the readersê borrowing records to enable the CF algorithms. And to evaluate the traditional CF algorithms, we show that linearly combining \(blending\ a set of CF algorithms increases the accuracy and outperforms any single CF algorithms. At last, we conduct the experiments based on the real world dataset and the results invalidate the efficiency of the blending methods 
Keywords-library recommendation; collaborative filtering supervised learning ensemble learning 
 
 I NTRODUCTION  Because of the recent rise in library 3.0, academic libraries have become medium facilitating interactions 
I 
between the community and librarians. Numerous academic libraries have begun to provide personalized services, such as book recommendations, to attract readers to use library resources [1, 2, 3, 4 Recommender systems help users to discover items in large web shops, to navigate through portals or to find friends with similar interests. And in recommendation systems, CF algorithms are widely used and have enjoyed much interest and progress [5  It p r e d ic ts th e p o t e n tia l  interests of a given user \(called an active user\ by taking into account the opinions of users with similar taste \(i.e., social wisdom\. Compared to other recommendation technologies e.g., content-based filtering [6   c o l l a b o r ativ e f ilte rin g  
technologies have the capability of working in domains where itemsê attribute contents are difficult to obtain or cannot easily be parsed by automatic processed. In addition CF algorithms can provide serendipitous recommendations and can be easily adopted in different recommender systems without requiring any domain knowledge [7  I n ot he r w o rd s   CF helps users discover new items. And there are various research works on the evaluation of the CF algorithms, such as real time online recommendation [8, 9 a n d s p ar s e  problem [10, 11 c In order to establish recommendations, CF algorithms need to relate two fundamentally different entities: items and users. There are two primary approaches to facilitate such a 
comparison, which constitute the two main techniques of CF the neighborhood approach and latent factor models Neighborhood methods focus on relationships between items or, alternatively, between users. An item-item approach models the preference of a user to an item based on rating of similar items by the same user. Latent factor models, such as matrix factorization, comprise an alternative approach by transforming both items and users to the same latent factor space. The latent space tries to explain rating by charactering both products and users on factors automatically inferred from user feedback  From the above description about the traditional CF algorithms, CF algorithms are based on the history of the feedbacks from the users, especially the rating of the items 
But in the history of the borrowing records recorded in the academic libraries, there is no rating information but just the records of borrowing and returning with time stamp. And the traditional CF algorithms cannot be adopted well for that situation In this paper, we focus on how to implement accurate CF algorithms based on readersê borrowing records without the feedback ratings. We transform the usersê ratings based on the borrowing records with time stamp for implementing CF algorithms. And at the same time, to make the traditional CF algorithms more accurate, we linearly combine \(blend\ the traditional ones by different methods. The results from the experiment on the real world data set invalidate the 
efficiency and accuracy of our methods. And as the ratings on the books can be transformed into time, by predicting the ratings, we can easily figure out how long the reader will borrow the book This paper is organized as follows: in section II, we give some details of some traditional CF algorithms and describe them in mathematics; in section III, we introduce the algorithms for blending. And in section IV, we show how to transform the readersê borrowing records to the rating and give the results by implementing the algorithms on the real world data. In section V, we give the conclusion and some plans about the future works II  
C OLLABORATIVE F LITERING  Collaborative filtering uses the history of user-items events in order to predict future ones. These events can be any source of user-generated information, such as purchases rating, clicks, bookmarks, favorite-adds etc. Within this work we focus on rating based collaborative filtering The problem of rating prediction can be described with the help of the user-item rating matrix  where 
ui R r 
 
2013 International Conference on Advanced Cloud and Big Data 978-1-4799-3261-0/14 $31.00 © 2014 IEEE DOI 10.1109/CBD.2013.14 159 


 
OU M u u u R while the KNNuser and KNNitem usually predict a part of the missing values Sometimes, LFM can be acted as SVD and there many extensions to SVD in the literature, for example the SVD model described in [14   In this section, we give some details of the traditional CF algorithms on how to predict the ratings. And there are many other CF algorithms and we do not focus on them in this paper III 
Si k i C k OM OM M C Su k f rows. And to train the parameters, we have to minimize the following cost function 22 2  min    
   
  
entry is the rating of user for item This matrix has the size with being the number of users and the number of items. In general for how an item would be rated by user In collaborative filtering, the system infers a model from all available data in A prediction 032 for the rating of a user on item is an item based k-nearest neighborhood model is obtained by calculating a weighted sum over the rating of the user for the items most similar to item The weights and the similarities are proportional to the correlations between item and other item is useful, due to the constant access time to any item-item correlation in this case. For one prediction, the KNNitem algorithm selects the best correlated items to item And usually employ the following equation to predict the rating    032 1 In the above equation and represent the average ratings of item and And  represents the neighbor set of the best correlated items to item  The training time is dominated by the calculation of the item-item correlation matrix which needs 2   operations. And to select the best correlated items, this can take up to  operations. The sorting of this list takes  log  time This model is exactly the same as the KNNitem, but items and users are flipped. Hence the prediction and training bounds are also reversed. In this model, a precalculated useruser correlation matrix  and usually employ the following equation to predict the rating      032 2 In the above equation and represent the average ratings of user and And  represents the neighbor set of the best correlated users to user  This is probably the most popular collaborative filtering technique. A prediction is given by the dot product of a user feature vector and the item feature vector then the rating is predicted as follows 032 3 The LFM learns two factor matrices, user features 12    and item features 12    via stochastic gradient descent. The model parameterizes two matrices with 4 In the above equation, the parameter The parameter  And for storing the two parameter matrices, the memory consumption is    Both training and prediction time have optimal asymptotic runtime behavior, which makes LFM an excellent candidate for large scale recommendation applications. And it usually can predict all the missing values in B LENDING  The combination of different kinds of collaborative filtering algorithms leads to significant performance improvements over individual algorithms. Blending predictions is supervised machine learning problem. Each 
002 
ui r ui r ui r 
k i k i a u a u u p i q T ui u i rpq U P pp p M Qqqq 
003\003 002   
002 002   002 002   
A KNNitem B KNNuser C LFM \(latent factor model 
003 is the regularization ratio.  And to implement the stochastic gradient descent, we have the learning rate 
u M R is sparsely filled because a user typically dose not rate every item. The goal of the prediction model is to accurately predict missing values of this matrix, i.e. to produce predictions 032 u R One possibility to do so is to use a low-rank matrix factorization And for testing the accuracy and efficiency, the dataset usually contains two parts: the train set and the test set. We train the parameters for CF models based on the train set and test the accuracy and efficiency of the CF models based on the test set There are two main techniques for CF: the neighborhood approach and latent factor models. And in this section, we give details on the neighborhood approaches including knearest neighbor \(KNN\ethods \(KNNitem, KNNuser\and latent factor model \(LFM  u u j Therefore, a precalculated itemitem correlation matrix 
ui T ui u i u i PQ rR rpq p q 
kk k k k ii ui k iSi ui ii iSi cr i ri c aa a a a uu u i a uSu ui uu uSu cr u ru c 
 
ij c ij c 
i UM U i i 
k i 
002 002 002 002 
004 
i C k i i 
 
f is usually predefined before parameter training In practice the whole training needs a few tens of epochs over whole dataset until convergence, leading to   O OM U 
005  
i 
160 


011 
i x N x xx 
    
F T x xw 
 006\007 007 
  
002 
input vector is the in an matrix of predictions. The target values for these data points are collected in an In some film recommendation systems, the entries of are integer rating between 1 and 5. The blending algorithm is formally a function  The input In statistics, linear regression is an approach to modeling the relationship between a scalar dependent variable and one or more explanatory variables denoted as In linear regression, data are modeled usi ng linear predictor functions and unknown model parameters are estimated from the data Such models are called linear models. Most commonly linear regression refers to a model in which the conditional mean of given the value of  Assuming a quadratic error function, optimal linear combination weighs vector of length  We use the traditional cost function while calculating the parameter of the model. And then compare the prediction with the real data for the test set Neural networks are a different paradigm for computing von Neumann machines are based on the processing/memory abstraction of human information processing; neural networks are based on the parallel architecture of animal brains. Neural networks are a form of multiprocessor computer system with simple processing elements, a high degree of interconnection, simple scalar messages and adaptive interaction between elements Small neural networks can be trained efficiently on huge data sets. The training of neural networks is performed stochastic gradient descent. The output neuron has a sigmoid activation function with an output swing of -1 to +1. To generate rating predictions in the range between two integers we can use a simple output transformation. For example, the output is multiplied by 1.6 and the constant 1 is added. The learning rate is  IV E XPERIMENETS  In this section, we implement the traditional CF algorithms and the blending methods on the real world dataset. Before implementing CF algorithms to predict the integer between a certain ranges, we have to transform the borrowing records This real world dataset is from some academic library And it contains 82, 987 records form 2011-06-07 to 201206-27 which contains five different operations including borrowing, returning, renewing, booking and removing the booking with the time stamp While a student borrow a book, he should renew it after 30 days. And he can renew twice, after one renew, he can borrow the book for another 15 days. It means that a book can be borrowed at most for 60 days without penalty on the extra days To generate the ratings for the user on the item, we first calculate the interval between the book borrowing record and the corresponding returning record. And we generate the ratings based on the interval as in table I. And the distribution of scores over the whole dataset is shown figure 2. And we can easily figure out that most of the books just have ratings with 1 TABLE I We first extract the borrowing records from the whole dataset. And we have 26, 897 pairs of borrowing and return records between 334 users and 25, 018 books. We separate the data set into two parts: the train set and the test set. The records between 2011-06-07 and 2012-04-30 are set to be the train set and it has 26, 640 records. And we can see that 
  010 010 
R ATING TRSANFORMATION  0,15\ 1 15,30\ 2 30,45\ 3 45,60\ 4 60  5   Figure 1 Distribution of the scores 
N data samples, one collection the vectors 12   N F N dimensional vector y y y y N be obtained by solving the least squares problem. For any input vector a b t t 
006 
F dimensional vector of the predictions in the ensemble. For X n x is a vector of individual predictions, the output is a scalar And sometimes, we cannot predict all the missing values so have another parameter for CF algorithmsÑSR successful rate\. SR is the rati o of the predicted values in all the missing values X x is an affine function of X w x the prediction is  R  is very sparse with density is 0.3188%. The records between 2012-04-30 and 2012-06-30 are set to be the test set and it has 257 records To implement CF algorithms and the blending methods we have the following parameters 
Days Rating 
 
A Linear Regreesion - LR B Neural Network - NN A Dataset and transformation B Set the data set and parameters 
004 
161 


C Results 1 LR 2 NN D Comparision 
  
004  
N ii i R MSE x y N i x 
      
002 
    
5 And RMSE is defined as follows  2  1 1 P   6 In the above equation P  represents the predicted value by the individual CF algorithm or the blending of different CF algorithms We first implement the three traditional algorithms and the performance is shown in table III TABLE III 1  KNNitem-1 1.43217 KNNitem Pearson correlation 1  LFM-1 1.110883 LMF  0.01 0.01 0.01 0.01 0.01 We use LR to blend the different CF algorithms And the performance is shown in table IV TABLE IV We use NN to blend the different CF algorithms Here, we set NN with 3 hidden layers and run NN 1000 times with different initial parameters to find the best one by using PyBrain [15    While we use NN for blending the traditional CF algorithms, we set 0.01 1 and the constant 0 in our experiment and the algorithm performs well with the above parameters And the training error of 1000 times is shown in figure 2  Figure 2 We list the results of both the individual CF algorithms and the combined CF algorithms in table VI TABLE VI 
TABLE II P ARAMETERS   KNNitem KNNuser LMF NN   NN And we have the two parameters to evaluate CF algorithm and the blending methods: SR and RMSE. We first denote the number of predicted value is  P ERFORMANCE OF THE I NDIVIDUAL CF ALGORITHM  KNNuser-1 1.33426 KNNuser, Pearson correlation LMF  R ESULTS OF LR  B LENDING  KNNuser1+KNNitem-1 0,0 1.0 KNNuser-1+LFM-4 0,0 1.0 We can see that both of the results are with RMSE=1.0 and are much better than the individual algorithm Train Error with different initializations From the figure 2, we can see that most of the time the train error is below 1 and NN is a good at training models and extracting the parameters And the performance on the test set is shown in table V TABLE V R ESULTS OF NN  B LENDING  KNNuser1+KNNitem-1 0.0567 0.6321 KNNuser-1+LFM-4 0.0559 0.5126 P ERFORMANCE C OMPARISON  
ab a 
004 004 004 004 004 004 
1.110747 
Parameter\(s Related Methods/A lgorithms name RMSE description name weights RMSE name Average Training Error\(1000 times RMSE\(the best 
003  LMF 003 0.05 003 0.05 003 0.05 003 0.05 003 0.05 
  
k f  LMF N and as there N data samples to predict. The SR is defined as follows   SR N N k k f 5  LFM-2 1.110819 LMF  f 10  LFM-3 1.110815 LMF  f 15  LFM-4 f 18  LFM-5 1.110839 LMF  f 20  From the above table, we can see that LFM-4 performs the best of all and its RMSE is 1.110747. There is a certain value of f which make LFM change towards differently directions b 
162 


name RMSE Evaluation 
012 
                
KNNitem-1 1.43217 0 KNNuser-1 1.33426 6.84 LFM-4 1.110747 22.44 KNNuser1+KNNitem-1 \(LR 1.0 30.18 KNNuser1+KNNitem-1 \(NN 0.6321 55.86 KNNuser-1+LFM-4 NN J. Serrano-Guerrero, E. Herrera-Viedma, J. A. Olivas et al. çA google wave-based fuzzy recommender system to disseminate information in university digital libraries 2.0é, Information Sciences, 2011, vol. 181 no. 9, pp. 1503-1516 2 S. Yu Chen, F. Yi Feng, çApplication research of association analysis with Clementineé, the International Conference on Software Engineering and Data Mining, 2010, pp. 445-449 3 I. S. Sitanggang, N. Husin, A. Agustina, N. Mahmoodian, çSequential pattern mining on library transaction dataé, the Symposium in Information Technology, 2010, pp. 1-4 4 S. Maneewongvatana, çA recommendation model for personalized book listsé,  the Symposium on Communications and Imformation Technologies, 2010, pp. 389-394 5 B. Sarwar, G. Karypis, J. Konstan and J. Riedl, çAnalysis of recommendation algorithms for e-commerceé, in the proceedings of the 2 nd ACM conference on Electronic commerce, 2000, pp. 158-167 6 G. Adomavicius and A. Tuzhilin, çToward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensionsé, IEEE Trans. on Knowl. and Data Eng., 2005 vol. 17, no. 6, pp. 734-749 7 N. N. Liu and Q. Yang, çEigenrank: A ranking-based approach to collaborative fitleringé, In Proceedings of the 31 st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2008, pp. 83-90 8 B. Chandramouli, J. J. Levandoski, A. Eldawy and M. F. Mokbel StreamRec: A Real-Time Recommender Systemé, SIGMOD, 2011 pp. 1243-1245 9 N. N. Liu, M. Zhao, E. Xiang and Q. Yang, çOnline Evolutionary Collaborative Filteringé, RecSys, 2010, pp. 95-102  X. Yang, Z. X. Zhang and K. Wang, çScalable Collaborative Filtering Using Incremental Update and Loal Link Predictioné, CIKM, 2012 pp. 2371-2374  V. Formoso, D. FernaNdez, F. Cacheda and V. Carneiro, çUsing profile expansion techniques to alleviate the new user problem International Journal of Informantion Processing and Management 2013, vol. 49, no. 3, pp. 659-672  Yehuda Koren and R. M. Bell, çChapter: Advances in Collaborative Fliteringé, Recommender Systems Handbook, Springer US, Online ISBN 978-0-387-85819-7, 2011, pp. 145-186  Y. Koren, R. M. Bell and C. Volinsky, çMatrix factorization techniques for recommender systemsé, IEEE Computer, 2009, vol. 42 no. 8, pp. 30-37  Y. Koren, çFactorization meets the neighborhood: a multifaceted collaboratie filtering modelé, KDD, 2008, pp. 426-434  S. Tom, B. Justin, W. Daan, S. Yi et al., çPyBrainé, Journal of Machine Learning Reseach, vol. 11, 2011, pp. 743-746  M. Jarhrer, A. Toscher, R. Legenstein, çCombining Predictions for Accurate Recommender Systemsé, SIGKDD, 2010, pp. 693-702  
By blending the traditional CF algorithms, the performance is more than 60% at the most better than the individual CF algorithm. And the lowest RMSE is 0.5126 which means that while a reader borrows a book we can predict how long he will borrow with error not exceeding 0.5126 15 8 days V C ONCLUSION AND F UTURE W ORKS  In this paper, we focus on how to implement efficient and accurate CF algorithms for the academic library. By transforming the rating information from the borrowing records, we can use the traditional CF algorithms to predict how long the reader will borrow the book And by blending the individual different CF algorithms we can have much more accurate algorithms for predictions And NN blending method performs better than LR blending method. By using NN with KNNuser and LFM, the error of the prediction will not exceed 8 days In the future, we will focus on how to evaluate SR under the individual CF algorithms and the blending methods as while we use NN with KNNuser and LFM, SR is just 2.95 And we will try also more blending methods which are described in [16 A CKNOWLEDGMENT  This work is supported by the National Key project of Scientific and Technical Supporting Programs of China Grant No 2012BAH01F02,2013BAH10F01,2013BAH07F02\; the National Natural Science Foundation of China\(Grant No.61072060\; the National High Technology Research and Development Program of China Grant No 2011AA100706\;the Research Fund for the Doctoral Program of Higher Education \(Grant No. 20110005120007 the Fundamental Research Funds for the Central Universities Engineering Research Center of Information Networks Ministry of Education R EFERENCES   1 
0.5126 64.21 
 
163 


the namenode fsimage even if we lose the namenode EC2 instance and allows us to restore the cluster To ensure consistency across the 2 clusters we considered HBase replication However it was not heavily used/tested at our scale So we decided to implement dual writes from our message queue The message queue would be responsible to persist the writes and retry until a write is successful to both clusters D Challenges at scale Certain challenges unique to the Following Feed problem emerge as the scale increases to millions of users and hundreds of millions of follow relationships 1 Data consistency The write path depicted by Figure 2 shows that user actions are enqueued to the message queue Workers are responsible for dequeuing and issuing the writes However our message queue may not preserve time ordering of user actions It is possible for writes to be issued out of order and result in a following feed inconsistent with user actions Such a situation occurs when user A follows user B and unfollows user B quickly afterwards Here AB f ollow represents the user action and M  AB f ollow  represents the writes being issued by the message queue AB f ollow   t 1 AB unf ollow   t 2 t 1 t 2 M  AB f ollow    t  1 M  AB unf ollow    t  2 t  1 t  2 This situation becomes more prevalent when the difference between t 1 and t 2 is of the order of milliseconds or when there are unexpected message queue delays Note that within HBase the unfollow action results in a delete operation which inserts delete markers to mask values To resolve these inconsistencies we use t 1 and t 2 as cell timestamps for the follow/unfollow actions In the above scenario the follow action will insert pins at t 1 which would still be masked by delete markers inserted by the unfollow action at t 2  Hence we are able to enforce a mutation ordering by making use of cell based timestamps in HBase 2 Unbounded data growth As new pins are created and pushed to followerês feeds the dataset grows exponentially The dataset growth is a function of the rate of incoming content and the cardinality of follow relationships in the follow graph We cap the number of pins in a userês following feed to a certain threshold One approach to achieve this would be to trim the feed in real time during writes Since the majority of writes stem from the fanout operation we would need to read the excess pins on every write and issue deletes to remove these pins This would result in a high random read volume and would completely defeat the utility of an LSM tree Another possibility would be to run a mapreduce job to trim the feeds The mapreduce job would likely impact the performance of the online serving cluster Also both these approaches would simply insert delete markers into the system rather than truly deleting data We came up with a based approach to this problem We implemented a coprocessor which would hook into the major and minor compactions and would only retain the required number of columns for each row Since the compaction sees columns within each row in sorted order we would end up retaining the most recent pins in the following feed This solved the problem without introducing any additional overhead into the system VIII C OMPARISON WITH R EDIS Previously Pinterest deployed a Redis backed storage for the Following Feed Since Redis is an in-memory store it provided high throughput and low latencies We used Redis sorted sets as the underlying data structure with pin creation timestamp as the sorting key and pin id as the object We used Redis master slave replication for redundancy and fault tolerance Moving to HBase came with a number of beneìts  Scalability The Redis deployment was manually sharded and required application level support for failing over to slaves in the event of machine failures As of today there is no well tested generic Redis clustering solution deployed at scale Adding capacity and machines was more cumbersome and required manual steps On the other hand HBase came with built-in support for fault tolerance automatic sharding and load balancing  Deeper Feeds at Lower Costs With HBase we were able to keep the hot/recently accessed feeds in memory and exploit the temporal nature of retrieval queries The rest of the data could reside on disk On the other hand we were reluctant to keep all the data in memory for Redis Hence HBase enabled us to signiìcantly increase the length of the Following Feed for our users In fact we were able to cut down our machine footprint and save costs  Data consistency As we discussed in the previous section the mutation ordering support provided by HBase allows us to resolve data inconsistencies  Durability Redis buffers edits in memory and fsyncês a secondês worth of edits to disk for durability In the case of a single machine crash some data loss is inevitable This could be rectiìed by fsyncêing every edit or fsyncêing group edits to disk for better durability However this would likely have implications on performance of Redis On the other hand HBase requires a ush to at 3 replicas before acking that a write is complete Even though durability was not a strict requirement it became a nice to have feature for the new system 782 


IX C ONCLUSION The follow graph and the following feed are an integral component of social networks today Each poses its own data management challenges In this work we have focused on the following feed problem We have described the current following feed architecture at Pinterest and shared our experiences with building a system at scale which exploits the write heavy nature of the problem Having researched various popular storage technologies we chose HBase because of its superior write throughput wide spread usage and excellent read performance Additional advantages of HBase included automatic cluster management improved data consistency and durability We have also described a user facing and mission critical application on top of Apache HBase and HDFS HBase is widely used for powering analytics machine learning and other ofîine big data applications However there are only a few examples using  for po wering user facing highly available applications Our HBase deployment of the following feed is one such example We feel that this work would be useful to social networks as an example for scaling up their feed storage and serving systems We have also included a systemic description of how we achieved a low MTTR with HBas e a topic not particularly well understood in industry but indispensable for building highly available applications on top of Hadoop and HBase Such knowledge should be generally useful for people looking at Hadoop for real time serving R EFERENCES  D Comer  Ubiquitous b-tree  ACM Computing Surveys  vol 11 no 2 pp 121Ö137 June 1979  B F  Cooper  A  Silberstein E T am R Ramakrishnan and R Sears Benchmarking cloud serving systems with ycsb in 1st ACM symposium on Cloud computing  2010  Redis  http://redis.io  F  Chang J Dean S Ghema w at W  C Hsieh D A Wallach M Burrows T Chandra A Fikes and R E Gruber Bigtable A distributed storage system for structured data ACM Trans Computer Systems  vol 26 no 2 November 2008   Apache hbase  http://hbase.apache.or g  A Lakshman and P  Malik Cassandra a decentralized structured storage system Operating Systems Review  vol 44 no 2 pp 35Ö40 April 2010  P  OêNeil E Cheng D Ga wlick and E OêNeil The logstructured merge-tree lsm-tree vol 33 no 4 pp 351Ö385 June 1996  A Aiyer  M  Bautin G J Chen P  Damania P  Khemani K Muthukkaruppan K Ranganathan N Spiegelberg L Tang and M Vaidya Storage infrastructure behind facebook messages using hbase at scale Bulletin of the IEEE Computer Society Technical Committee on Data Engineering  2012  P  Hunt M K onar  F  P  Junqueira and B Reed Zook eeper Wait-free coordination for internet-scale systems in 7th Symposium on Operating Systems Design and Implementation  2010  M Burro ws The chubby lock service for loosely-coupled distributed systems in 7th Symposium on Operating Systems Design and Implementation  2006 pp 335Ö350   Apache hdfs  http://hadoop.apache.or g/hdfs  S Ghema w at H Gobiof f and S.-T  Leung The google le system in Symposium on Operating Systems Principles  vol 37 no 5 December 2003 pp 29Ö43  Ja v a se 6 hotspot[tm virtual machine garbage collection tuning http://www.oracle.com/technetwork/java/javase/gctuning-6-140523.html  J Dean Designs lessons and advice from b uilding lar ge distributed systems in 3rd ACM SIGOPS International Workshop on Large Scale Distributed Systems and Middleware  2009  D Borthakur  K  Muthukkaruppan K Ranganathan S Rash J S Sarma N Spiegelberg D Molkov R Schmidt J Gray H Kuang A Menon and A Aiyer Apache hadoop goes realtime at facebook in SIGMOD  June 2011 pp 1071 1080 783 


particularly given that some of our workload will be portalbased user-services for whom different metrics of service and performance will be necessary There is a tension here between the constraints of working with a shared massively parallel le system and massively scalable compute We believe the scale of our upgrades will both allow us to serve the user communities and to explore both sides of this debate Whatever the workîow users settle on whether batch or cloud orientated most will have to make signiìcant improvements to their workîow to exploit massive parallelisation We have presented an example of how such workîows can be developed within a constrained virtual environment before exploiting massive batch computing but thus far these sort of interventions are very resource intensive We do not have the resources to intervene on such a scale for most users and we do not yet have suitable documentation in place to help users develop their own solutions To that end we are also procuring improvements in documentation and training materials V S UMMARY We have presented the JASMIN architecture rst year of usage and near term plans The physical architecture consists of 600 cores and 5 PB of fast disk connected by low latency networking The compute environment supports a range of virtualisation options from batch to cloud computing A diverse and growing user community is exploiting JASMIN examples include high resolution climate modelling and whole satellite mission analysis for cloud and land surface retrievals The use of Panasas for storage has been very successful with exibility reliability and low management overheads being key to that success However the existing JASMIN environment is underpowered in compute the storage is lling and difìculties exporting the high performance disk into the local VMware cloud computing environment remain JASMIN users are becoming accustomed to a new analysis environment and early adopters are getting signiìcant improvements in their workîow completely changing the nature of the science they can undertake However thus far the JASMIN support team has not yet been able to invest in comprehensive user documentation or training so not all the community has seen the beneìts of these investments To fully exploit JASMIN changes in software and workîow will be necessary for most users and these will take time to engender Recognising the limitations with the existing physical infrastructure and with new user communities anticipated hardware software and documentation will all be upgraded over the next two years R EFERENCES  B N La wrence V  Bennett J Churchill M Juck es P  K ersha w  P Oliver M Pritchard and A Stephens The JASMIN super-datacluster ArXiv e-prints  Apr 2012  K E T aylor  R  J  Stouf fer  and G A Meehl  A n o v ervie w o f CMIP5 and the experiment design Bulletin of the American Meteorological Society  vol 93 pp 485Ñ498 Oct 2011 A v ailable http://journals.ametsoc.org/doi/abs/10.1175/BAMS-D-11-00094.1  D N W illiams B N La wrence M Lautenschlager  D  Middleton and V Balaji The earth system grid federation Delivering globally accessible petascale data for CMIP5 in Proceedings of the 32nd Asia-Paciìc Advanced Network Meeting  New Delhi Dec 2011 pp 121Ö130 A v ailable http://usymposia.upm.my/inde x.php APAN  Proceedings/32nd APAN/paper/view/155  M S Mizielinski M J Roberts P  L V idale R Schiemann M E Demory J Strachan T Edwards A Stephens M Pritchard P Chiu A Iwi J Churchill C D C Novales J Kettleborough W Roseblade P Selwood M Foster M Glover and A Malcolm High resolution climate modelling the UPSCALE project a large simulation campaign Geoscientiìc Model Development  vol In preparation 2013  J.-P  Muller  P  L e wis J Fischer  P  North and U Framer  The ESA GlobAlbedo project for mapping the earths land surface albedo for 15 years from european sensors in Geophysical Research Abstracts  vol 13 Vienna 2011 p 10969 A v ailable http://www.globalbedo.org/docs/Muller-GlobAlbedo-abstractV4.pdf  D Ghent and J Remedios De v eloping rst time-series of land surface temperature from AATSR with uncertainty estimates in Geophysical Research Abstracts  vol 15 2013 p 5016 Available http://adsabs.harvard.edu/abs/2013EGUGA..15.5016G  C A Poulsen R Siddans G E Thomas A M Sayer  R  G  Grainger  E Campmany S M Dean C Arnold and P D Watts Cloud retrievals from satellite data using optimal estimation evaluation and application to ATSR Atmospheric Measurement Techniques  vol 5 no 8 pp 1889Ñ1910 Aug 2012 A v ailable http://www.atmos-meas-tech.net/5/1889/2012  J Cohen B Dolan M Dunlap J M Hellerstein and C W elton MAD skills new analysis practices for big data Proceedings of the VLDB Endowment  vol 2 no 2 pp 1481Ñ1492 2009 Available http://dl.acm.org/citation.cfm?id=1687576  K Shv achk o H K uang S Radia and R Chansler  The hadoop distributed le system in Mass Storage Systems and Technologies MSST 2010 IEEE 26th Symposium on  2010 pp 1Ñ10 A v ailable http://ieee xplore.ieee.or g/xpls/abs all.jsp arnumber=5496972  H Herodotou H Lim G Luo N Boriso v  L Dong F  B Cetin and S Babu Starìsh A self-tuning system for big data analytics in Proc of the Fifth CIDR Conf  2011 A v ailable http://x86.cs.duke.edu  gang/documents/CIDR11 Paper36.pdf  J Buck N W atkins J Lefe vre K Ioannidou C Maltzahn N Polyzotis and S Brandt Scihadoop Array-based query processing in hadoop Technical Report UCSC-SOE-11-04 UCSC Tech Rep 2011  G Sak ellari and G Loukas  A surv e y of mathematical models simulation approaches and testbeds used for research in cloud computing Simulation Modelling Practice and Theory  A v ailable http://www.sciencedirect.com/science/article/pii/S1569190X13000658 75 


Copyright © 2009 Boeing. All rights reserved  Architecture Server-1 Server-2 DB2 SURVDB XML Shredder WebSphere Message Broker Ext.4 H Ext.3 G Ext.2 F Ext.1 E C WebSphere MQ TCP/IP Live ASDI Stream IBM Cognos Server-3 IBM SPSS Modeler SPSS Collaboration Deployment Services 


Copyright © 2009 Boeing. All rights reserved  Database Modeling Schemas for correlated ASDI messages translated into equivalent relational schemas  Database tables generated based on classes created from schema definitions  Nine main, eleven supporting tables  Each main table contains FLIGHT_KEY 


Copyright © 2009 Boeing. All rights reserved  Database Modeling 


Copyright © 2009 Boeing. All rights reserved  Correlation Process To archive received ASDI data  Track messages must be correlated with flight plan messages FLIGHT_KEY assigned Uncorrelated data tagged Approx 30 minutes to correlate one day of data 


Copyright © 2009 Boeing. All rights reserved  Historical Data Processing To load correlated data  Uncompress, unmarshall  Create a list of files containing the correlated data  Write data to warehouse 


Copyright © 2009 Boeing. All rights reserved  Live Data Processing Processed using IBM MQ IBM Message Broker and a technique called XML Shredding Message Broker Compute Nodes  Uncompress Node  Extract correlated messages  Shred Node adds to DB Stored Procedure ìshreds XML docs and adds to tables 


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a ìkey, valueî list using an XSTL  Queries made against this list of ìkey, valueî pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


