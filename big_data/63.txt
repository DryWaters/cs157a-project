A thorough experimental study of datasets for frequent itemsets Fr  ed  eric Flouvat 1  Fabien De Marchi 2  3  Jean-Marc Petit 2  4 1 Laboratoire LIMOS UMR CNRS 6158 Universit  e Clermont-Ferrand II 63 177 Aubi  ere France ouvat@isima.fr 2 Laboratoire LIRIS UMR CNRS 5205 3 
Universit  e Lyon I 4 INSA Lyon 69 621 Villeurbanne France  fabien.demarchi jmpetit  liris.cnrs.fr Abstract The discovery of frequent patterns is a famous problem in data mining While plenty of algorithms have been proposed during the last decade only a few contributions have tried to understand the inîuence of datasets on the algorithms behavior Being able to explain why certain algorithms are likely to perform very well or very poorly on some datasets is still an open question 
In this setting we describe a thorough experimental study of datasets with respect to frequent itemsets We study the distribution of frequent itemsets with respect to itemsets size together with the distribution of three concise representations frequent closed frequent free and frequent essential itemsets For each of them we also study the distribution of their positive and negative borders whenever possible From this analysis we exhibit a new characterization of datasets and some invariants allowing to better predict the behavior of well known algorithms 
The main perspective of this work is to devise adaptive algorithms with respect to dataset characteristics 1 Introduction The discovery of frequent patterns is a famous problem in data mining introduced in as a rst step for mining association rules While plenty of algorithms have been proposed during the last decade 3 10 20 22 only a few contributions have tried to understand the inîuence of dataset characteristics on the algorithms behavior 19 20 These studies focus on the number of transactions average length of transactions or frequent item 
sets distribution i.e statistics from frequent itemsets and maximal frequent itemsets are usually given Nevertheless algorithms could have quite different behaviors for apparently similar datasets Benchmarks comparing algorithms performances have been done on real and synthetic datasets 5 see FIMI website 18 Algorithm implementations and datasets are freely available from for mining frequent frequent closed or frequent maximal itemsets Even with all these informations being able to explain why certain algorithms are likely to perform very well or very poorly on some datasets is still an open question 
More generally studying datasets can provide useful hints for devising adaptive algorithms 17 i.e algorithms which adapt themselves to data characteristics in order to increase their time or memory efìciency Adaptive behavior of algorithms is not new in the setting of frequent itemsets mining for example 7 use heuristics to decide when tries-like data structure representing datasets and/or itemset collections have to be rebuilt The promising results obtained by these algorithms show the interest of applying speciìc strategies according to dataset features Another key point is that some problems have speciìc in 
variant characteristics whatever the studied datasets Their impact on algorithms could give useful information about the difìculty to solve these problems while giving hints on the more appropriate strategies to cope with these difìculties Related works Classical characteristics of datasets were studied in and more particularly a density criteria Up to our knowledge no formal deìnition of density does exist According to a dataset is dense when it produces many long frequent itemsets even for high values of minimum support threshold The authors studied seven datasets 
each of them capturing a fairly large range of typical uses The result of these experimentations is a classiìcation of datasets in four categories according to the density The density is estimated by using the characteristics of maximal frequent itemsets The main problem of their classiìcation concerns its variability with respect to minimum support threshold valProceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


ues For example a dataset could belong to the rst category for a given threshold value and to the second category for another threshold value 1  Moreover there is no clear relationship between the proposed classiìcation and algorithms performances Even worse a surprising result was obtained in the last FIMI workshop algorithms seem to be more efìcient on some very dense datasets than on some other sparser datasets Note also that in in order to easily compare different implementations a tool has been developped from information available at the FIMI website Based on the works done in 32 proposed a s tatistical property of transactional datasets to characterize dataset density Actually they consider the dataset as a transaction source and measure an entropy signal i.e the transactions produced by such a source Moreover they show how such a characterization can be used in many elds from performance prediction minimum support threshold range determination sampling to strategy decisions As for the previous work it does not explain algorithms performances anymore This may be due to the fact that only frequent itemsets are used to calculate the entropy measure In the positi v e border distrib ution i.e the number of maximal elements in each level is considered as a key parameter to characterize transaction databases It is proved that any distribution is feasible and thus susceptible to be met in practice Moreover a constructive theorem is proposed to compute a synthetic transaction database given a positive border distribution as input Nevertheless the negative border is never considered and as a result such synthetic databases do not match the complexity of realworld datasets Contribution In this setting we describe a thorough experimental study of datasets with respect to frequent itemsets We study the distribution of frequent itemsets with respect to itemsets size together with the distribution of three concise representations frequent closed frequent free and frequent essential itemsets For each of them we also study the distribution of their positive and negative borders whenever possible From this analysis we exhibit a new classiìcation of datasets and some invariants allowing to better predict the behavior of well known algorithms The main perspective of this work is to devise adaptive algorithms with respect to dataset/problem characteristics Paper organization In section 2 we introduce some preliminaries Experimental study of datasets is given in section 3 including usual representations of frequent itemsets experimental protocol results and analysis The section 4 1 As a concrete example this case arises with Pumsb  dataset with minimum support threshold values equal to 15 and 25 respectively Other examples are given in presents the main results of this work a new dataset classiìcation and a study of the inîuence of anti-monotone predicate on the resolution of some problems Finally we conclude and give some perspectives for this work 2 Preliminaries Let R be a set of symbols called items  and r a database of subsets of R  The elements of r are called transactions An itemset X is a set of some items of R  The support of X is the number of transactions in r that contain all items of X  An itemset is frequent if its support in r exceeds a minimum support threshold value called minsup Givena minimal support threshold and a database the goal is to nd all frequent itemsets We recall the notion of borders of a set using notations given in  Let  I   be a partially ordered set of elements A set S  I is closed downwards if for all X  S  all subsets of X arealsoin S  S can be represented by its positive border B d   S  or its negative border B d   S  deìned by B d   S  max   X  S  B d   S  min   Y  I  S  Let p be an anti-monotone predicate on  I    i.e  X Y  I,X  Y if p  Y  is true then p  X  is true If S is the set of elements of I satisfying p  then S is closed downwards For instance a set of frequent itemsets FI in a database with respect to a given minimum support threshold value is closed downwards In this case B d   FI  is often called the set of maximal frequent itemsets  3 Thorough experimental study of datasets In order to introduce our experimental study we rst describe three classical representations of frequent itemsets Then our experimental protocol is explained and our experimental results are given and discussed To end up a relationship between these results and algorithms performances is also pointed out 3.1 Usual representation of frequent itemsets Several concise or condensed representations of frequent itemsets have been studied 12 Their goal is twofold improving efìciency of frequent itemsets mining whenever possible and compacting the storage of frequent itemsets for future usages Formally a condensed representation must be equivalent to frequent itemsets one can retrieve each frequent itemset together with its frequency without accessing data Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


Such a representation is known as closed sets 33 34 Two other representations are considered in this paper frequent free itemsets 4 8 and frequent essential itemsets  Notice that these sets are not e xactly suf cient to represent frequent sets since they need a subset of the frequent itemsets border to become condensed representations We brieîy describe these representations in the rest of this section Frequent Closed sets Given an itemset X  the closure of X is the set of all items that appear in all transactions where X appears Formally given a transaction database r  Cl  X    t  r  X  t  If Cl  X  X then X is said to be closed Frequent free itemsets An itemset X is said to be free if there is no exact rule of the form X 1  X 2 where X 1 and X 2 are distinct subsets of X  Free sets can be efìciently detected through the following property X is free   x  X sup  X  sup  X  x  Frequent essential itemsets The notion of essential itemsets has been deìned recently in It is based on the notion of disjunctive rule 11 A disjunctive rule is of the form X  A 1  A 2   A n  Such a rule is satisìed if every transaction that contains X contains at least one of the elements A 1   A n  An itemset X is said to be essential if there is no disjunctive rule of the form A 1  A 2    A k  where  A i  i 1 k are distinct elements in X  As for free sets they can be efìciently tested exploiting the following property X is essential   x  X sup dij  X  sup dij  X  x  where sup dij  X   t  r  t 012 X    The predicates being a frequent free itemset and being a frequent essential itemset are anti-monotone w.r.t set inclusion In the following we study the distributions of these three collections w.r.t itemsets size Other concise representations based on the notion of disjunctive rules have been deìned the reader is referred to the general framework proposed in for more details To end up we believe that our choice of concise representations covers a fairly large range of typical cases 3.2 Experimental protocol For frequent itemsets a benchmark of fourteen datasets is commonly used Most of them are real-life datasets only two being synthetic ones generated by the generator from the IBM Almaden Quest research group All e x periments have been done on these datasets Each dataset has been studied for many representative minimum support thresholds from very high to very low values For each one frequent itemsets frequent closed frequent free and frequent essential itemsets have been collected We have studied their distribution with respect to itemsets size i.e the number of elements in each level from one to the size of the largest itemsets Moreover we have studied the positive and negative borders distributions of frequent frequent free and frequent essential itemsets 2  To perform these tests we used algorithms available at the FIMI website The disco v e ry of frequent itemsets and frequent closed itemsets has been done using FPClose and FP  growth 015 algorithms from ABS  has been updated to nd frequent free and frequent essential itemsets To the best of our knowledge this work is the rst one to address the understanding of datasets for frequent itemsets and other concise representations by using their negative borders 3.3 Experimental results In order to perform a fair comparison with results given in this paper focus on the same datasets i.e Chess  Pumsb  Connect  Pumsb 015  M ushroom and T 10 I 4 D 100 K  Notations used in the sequel are reported in Table 1 FI frequent itemsets FCI frequent closed itemsets FFI frequent free itemsets FEI frequent essential itemsets Table 1 Notations Given a dataset and a minimum support threshold value the Table 2 describes a typical example of our experimental results Due to space limitations the reader is referred to for comprehensi v e results from which the analysis made in this paper has been performed A wider range of minimum support threshold values and other datasets are also described in In the rest of this section we discuss our experimental results with respect to three main axes borders distribution of frequent itemsets stability of borders distribution with respect to support threshold values and borders distribution of frequent free and essential itemsets 2 The set of closed itemsets is not closed downwards and thus the notion of borders does not apply Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


Itemset size FI FCI FFI FEI Bd-\(FI Bd+\(FI Bd-\(FFI Bd+\(FFI Bd-\(FEI Bd+\(FEI 1 50 27 50 50 25 25 1 25 1 2 896 338 828 828 329 397 2 397 149 3 9049 2568 7628 4240 928 1 988 34 4376 853 4 59589 13221 44096 6283 3371 9 3440 268 8519 3178 5 273069 49002 170161 1635 10118 89 10178 1343 1764 1186 6 907800 137564 456826 116 21405 439 21416 4876 36 109 7 2255159 303661 875938 1 33711 1369 33720 11963 1 8 4276852 540861 1216501 39910 3686 39910 22521 9 6291848 787143 1231162 33890 8200 33890 31137 10 7263312 940504 903996 21894 14804 21894 32243 11 6626801 923310 474618 10160 21183 10160 25491 12 4790827 740773 172688 3507 24638 3507 15326 13 2738089 481499 41186 791 23766 791 6403 14 1227702 250715 5787 114 18088 114 1951 15 425896 102977 360 8 10934 8 314 16 111726 32875 3 5085 3 17 21328 7908 1734 18 2757 1370 496 19 206 145 97 20 6 6 6 total 37282962 5316467 5601828 13153 180161 134624 180438 153876 15117 5477 Table 2 Chess dataset minsup  30 Borders distribution of frequent itemsets Consider the positive and negative borders of frequent itemsets from ve datasets as given in Figure 1 First of all we observe bell curve distributions for the two borders in almost all datasets Since every distribution of positive border is feasible in theory other properties should e xist to e xplain these distributions Moreover the negative and positive borders seem to follow the same behavior even if the negative border is always lower than its corresponding positive border From we also kno w t hat the ne g ati v e border may have elements just one level after the positive border This case never occurs in our experiments for frequent itemsets Moreover we denote two different behaviors of the distance between the two borders For Chess P umsb and T 10 I 4 D 100 K Figure 1 the borders distributions are very close i.e the mean of the negative border curve is only a few levels before the mean of the positive border curve The dataset T 10 I 4 D 100 K is different from the two others since its borders are made of small itemsets For datasets Connect  Pumsb 015 and M ushroom in Figure 1 a larger distance between the borders exists These simple observations will be used as predictors of the hardness of a dataset in the sequel Stability of borders distribution Now we study the variation of minimum support threshold values on borders distribution of frequent itemsets To do that we consider Chess and Connect for the following minimum support threshold values 30 50 and 80 For the rst minimum support threshold value the results are in Figure 1 and for the two others in Figure 2 A surprising observation is that the relative position of borders distributions is stable w.r.t variation of minimum support threshold values  0 5000 10000 15000 20000 25000 30000 35000 40000 0 2 4 6 8 10 12 14 16 18 20 Frequency Length Chess \(minsup 30 Bd-\(FI Bd+\(FI   0 5000 10000 15000 20000 25000 30000 0 5 10 15 20 25 Frequency Length Pumsb \(minsup 60 Bd-\(FI Bd+\(FI   0 25000 50000 1 2 3 4 5 6 7 Length  Bd+\(FI  0 5e+06 1e+07 Frequency T10I4D100K \(minsup ~0.01 Bd-\(FI   0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 0 5 10 15 20 25 Frequency Length Connect \(minsup 30 Bd-\(FI Bd+\(FI   0 5000 10000 15000 20000 25000 0 5 10 15 20 25 30 35 40 Frequency Length Pumsb* \(minsup 10 Bd-\(FI Bd+\(FI   0 5000 10000 15000 20000 25000 30000 0 5 10 15 20 25 Frequency Length Mushroom \(minsup 0.1 Bd-\(FI Bd+\(FI   Figure 1 Borders of frequent itemsets Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


 0 500 1000 1500 2000 2500 3000 3500 0 2 4 6 8 10 12 14 16 Frequency Length Chess \(minsup 50 Bd-\(FI Bd+\(FI   0 20 40 60 80 100 120 140 1 2 3 4 5 6 7 8 9 10 Frequency Length Chess \(minsup 80 Bd-\(FI Bd+\(FI   0 200 400 600 800 1000 1200 1400 1600 0 5 10 15 20 25 Frequency Length Connect \(minsup 50 Bd-\(FI Bd+\(FI   0 50 100 150 200 250 300 0 2 4 6 8 10 12 14 16 Frequency Length Connect \(minsup 80 Bd-\(FI Bd+\(FI   Figure 2 Borders of frequent itemsets with a different minimum support threshold In other words this observation suggests a kind of global structure for frequent itemsets borders distribution invariant to variation of minimum support threshold values Borders of concise representations Now we consider the positive and negative borders of frequent free itemsets and frequent essential itemsets on Chess and Connect given in Figure 3 and 4 From these two gures one can remark that distributions of the two borders look like bell curves Recall that the same behavior has already been pointed out for frequent itemsets suggesting that such kind of curves is almost independent of the considered anti-monotone predicate Moreover the distance between the mean of the negative and positive borders appears to be small for each concise representation The same behavior has been observed in all our experiments 0 5000 10000 15000 20000 25000 30000 35000 40000 0 2 4 6 8 10 12 14 16 Frequency Length Chess \(minsup 30 Bd-\(FFI Bd+\(FFI   0 5000 10000 15000 20000 25000 30000 35000 1 2 3 4 5 6 7 8 Frequency Length Connect \(minsup 30 Bd-\(FFI Bd+\(FFI   Figure 3 Borders of frequent free itemsets 3.4 Impact on algorithms performances We focus on the discovery of maximal frequent itemsets  and we study the performances of implementations available at the FIMI website Let u s consider results 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 1 2 3 4 5 6 7 Frequency Length Chess \(minsup 30 Bd-\(FEI Bd+\(FEI   0 1000 2000 3000 4000 5000 6000 1 2 3 4 Frequency Length Connect \(minsup 30 Bd-\(FEI Bd+\(FEI   Figure 4 Borders of frequent essential itemsets given in Figure 5 showing algorithms execution times on four datasets On Chess dataset for every implementation given in Figure 5 upper-left corner algorithms execution times increase exponentially with decreasing minimum support threshold values whereas for Connect upper-right corner they appear to be almost linear for Mafia 9 fp  zhu 21 LCM  and af opt  Moreo v e r  recall that Connect has more and longer transactions and more items than Chess  The same kind of behavior can be noticed for datasets such as Pumsb and Pumsb 015 Figure 5 These two datasets are very similar w.r.t the transactions and number of items but their borders distribution is very different Figure 1 Algorithms for Pumsb 015 are still very effective for very low minimum support threshold whereas for Pumsb  algorithms do not perform very well for relatively high minimum support threshold values Therefore we deduce that pruning strategies are much more efìcient on datasets having a large distance between their positive and negative borders  A possible explanation could be obtained by looking at algorithms pruning strategies since most of them take advantage of minimal unfrequent itemsets to nd maximal frequent itemsets and prune the search space 4 Toward new classiìcations Observations described in previous section lead us to devise a new classiìcation for datasets w.r.t borders distribution We also intent to use these results for other data mining problems i.e those problems said to be representable as sets 4.1 A new dataset classiìcation This new classiìcation differs from the classiìcation given in  since it tak es into account both the n e g ati v e border and the positive border of frequent itemsets Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


 0.01 0.1 1 10 100 1000 0 500 1000 1500 2000 2500 3000 Total Time \(sec Minimum Support Chess MAFIA fp-zhu   LCM   ABS   afopt  eclat-borgelt   apriori-borgelt   0.1 1 10 100 1000 30000 40000 50000 60000 70000 Total Time \(sec Minimum Support Connect MAFIA fp-zhu   LCM   ABS   afopt  eclat-borgelt   apriori-borgelt   0.1 1 10 100 1000 20000 25000 30000 35000 40000 45000 Total Time \(sec Minimum Support Pumsb MAFIA fp-zhu   LCM   ABS   afopt  eclat-borgelt   apriori-borgelt   0.1 1 10 100 1000 0 5000 10000 15000 20000 25000 Total Time \(sec Minimum Support Pumsb MAFIA fp-zhu   LCM   ABS   afopt  eclat-borgelt   apriori-borgelt  Figure 5 Algorithms performances This classiìcation follows from remarks done in the previous section Its main interests are  a better correspondence between algorithms performances and the classiìcation In other words this classiìcation is a rst attempt in order to evaluate the hardness of a dataset  a stability w.r.t the variation of minimum support threshold Based on the distance between positive and negative borders distributions of frequent itemsets different types of datasets have been identiìed As a consequence we introduce a new classiìcation of datasets made of three types  Type I datasets are datasets where borders distributions are very close i.e the mean of the negative border curve is not far from the mean of the positive border curve In other words most of the itemsets in the two borders have approximately the same size Chess and Pumsb fall into this category such datasets can be expected to be hard for frequent itemsets mining  Type II datasets are datasets where there is a large distance between the two borders distributions In other words the itemsets in the negative border are much smaller than those of the positive border Connect  Pumsb 015 and M ushroom fall into this category in practice this type is easier than the previous one  Type III is a very special case of type I the two distributions are very close but they are concentrated in very low levels This type allows to catch the notion of sparseness for example T 10 I 4 D 100 K  It might be the most easy dataset type in practice This classiìcation is simpler than the one presented in  while being v e ry stable w  r t v a riation of minimum support threshold In addition to classical characteristics of datasets the distance between the mean of the negative and positive border distributions makes possible a better evaluation of the difìculty of a dataset For the two other concise representations previously described this classiìcation suggests that almost all datasets belong to type I or III 4.2 Predicate classiìcation In the setting of this paper we focus our analysis on datasets with respect to frequent itemsets In our experiments we studied three anti-monotone predicates one for frequent itemsets another one for frequent free itemsets and the last one for frequent essential itemsets These three predicates exhibit very different behaviors on the same datasets see Figure 1 to 4 on Connect and Chess for different minimum support threshold values Quite clearly this work could be generalized to other data mining problems i.e those which are representable as sets  W e ar gue that the study of both positi v e and negative borders for a given anti-monotone predicate may allow us to come up with some general results From the previous sections we deduced that studying the Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


R  XY   S  UV  R  XZ   S  UW   R  XY Z   S  UV W  S  U  V Figure 6 An interaction between FD and IND gap between the negative and positive borders may be very insightful to explain the behavior of algorithms and may also give some hints to guess the existence of properties associated with anti-monotone predicates In spite of the huge amount of work done for frequent itemset mining we are not aware of such kind of contributions Nevertheless we introduce in the sequel another data mining problem known to be representable as sets where such properties have been clearly identiìed Application to inclusion dependency mining Inclusion dependencies IND are fundamental semantic constraints for relational databases Let r and s be two relations over schemas R and S  and X and Y be sequences of attributes into R and S respectively The IND R  X   S  Y  is true in  r s  if all the values of X in r are also values of Y in s  This notion generalizes foreign keys constraints very popular in practice The underlying data mining problem can be stated as follows Given a database nd all inclusion dependencies satisìed in this database 23 30 24 From 30 the set of IND candidates can be organized in a levelwise manner a given level say k  corresponds to INDs whose arity is equal to k  Moreover a partial order for INDs can be deìned as follows if i and j are two INDs j  i if j can be obtained by performing the same projection on the two sides of i  For example R  AB   S  EF   R  ABC   S  EFG   In this setting the predicate being satisìed in a database is anti-monotone with respect to  30 Consider now the well known inference rule for inclusion dependencies together with functional dependencies  gi v e n i n F igure 6 Intuiti v e ly  consider an inclusion dependency i  R  XAB   S  YEF  where X and Y are attribute sequences and A B E and F are single attributes Suppose that every IND j such that j  i is satisìed and let j 1  R  XA   S  YE  and j 2  R  XB   S  YF  be two of them The more  Y  is large the more Y is likely to determine E or F  In other words i is likely to be satisìed from inference rule of Figure 6 From this result one may logically expect that large INDs should never appear in the negative border even if large INDs exist It implies a potentially large gap between the two borders distribution like for type II datasets for frequent itemsets All our experiments corroborate this hypothesis We tested three synthetic databases built using the chase procedure W e enforced lar ge INDs in their positi v e border  until size 18 For all databases INDs in the negative border were all of size lower than 3 This particular behavior of the positive border of INDs justiìes an algorithm based on the negative border discovery 5 Conclusion and perspectives In this paper we have thoroughly studied datasets for problems related to frequent itemset mining We have shown that the distribution of the negative and positive borders have an important impact on datasets classiìcation and algorithms performances For frequent itemsets mining a new classiìcation of datasets has been proposed This work is a rst step toward a better understanding of the behavior of algorithms with respect to the search space to be discovered This work has two main perspectives The former is to nd out theoretical foundation of bell curves and stability obtained for the distributions in most of our experiments The latter is the design of adaptive algorithms with respect to dataset characteristics i.e changing dynamically their strategy during runtime References  Synthetic data generation code for associations and sequential patterns Intelligent information systems IBM almaden research center http://www.almaden.ibm.com/software/quest/resources  R Agra w al T  Imielinski and A N Sw ami Mining association rules between sets of items in large databases In P Buneman and S Jajodia editors SIGMOD conference Washington  pages 207Ö216 ACM Press 1993  R Agra w a l and R Srikant F ast algorithms for mining association rules in large databases In J B Bocca M Jarke and C Zaniolo editors VLDB conference Santiago de Chile Chile  pages 487Ö499 Morgan Kaufmann 1994  Y  Bastide R T a ouil N P a squier  G  S tumme and L Lakhal Mining frequent patterns with counting inference In SIGKDD Explorations 2\(2  pages 66Ö75 2000  R J Bayardo B Goethals and M  J  Zaki editors FIMI 04 Proceedings of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations  volume 126 of CEUR Workshop Proceedings  CEUR-WS.org 2004  C Beeri and M  V ardi A proof procedure for data dependencies Journal of the ACM  31\(4 1984  C Bor gelt Ef cient implementations of Apriori and Eclat In FIMI 03 Proceedings of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations  November 2003  J.-F  Boulicaut A Byk o wski and C  Rigotti Free-sets A condensed representation of boolean data for the approximation of frequency queries Data Mining and Knowledge Discovery  7\(1 2003 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


 D Burdick M Calimlim J Flannick J Gehrk e and T  Y iu MAFIA A performance study of mining maximal frequent itemsets In FIMI 03 Proceedings of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations  2003  D Burdick M Calimlim and J  Gehrk e MAFIA A m aximal frequent itemset algorithm for transactional databases In ICDE conference Heidelberg Germany  pages 443Ö452 IEEE CS 2001  A Byk o wski and C Rigotti A c ondensed representation to nd frequent patterns In PODSê01 Santa Barbara California USA  ACM 2001  T  Calders and B  Goethals Minimal k free representations of frequent sets In PKDD  pages 71Ö82 2003  A Casali R Cicchetti and L Lakhal Essential patterns A perfect cover of frequent patterns In DaWaK conference Copenhagen Denmark  Lecture Notes in Computer Science 2005  M Casano v a  R  F agin and C  P apadimitriou Inclusion dependencies and their interaction with functional dependencies Journal of Computer and System Sciences  24\(1 59 1984  F  De Marchi and J.-M Petit Zigzag  a ne w a lgorithm for discovering large inclusion dependencies in relational databases In ICDM conference Melbourne USA  pages 27Ö34 IEEE Computer Society 2003  F  Flouv at Experimental study of frequent itemsets datasets Technical report LIMOS France http://www.isima.fr/îouvat/papers/rr05ExpStudyDatasets.pdf june 2005  F  Flouv at F  De Marchi and J.-M Petit ABS Adapti v e borders search of frequent itemsets In FIMI 04 Proceedings of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations   B Goethals Frequent itemset mining implementations repository http://ìmi.cs.helsinki 2004  B Goethals and M J Zaki editors FIMI 03 Proceedings of the ICDM 2003 Workshop on Frequent Itemset Mining Implementations 19 December 2003 Melbourne Florida USA  volume 90 of CEUR Workshop Proceedings CEURWS.org 2003  K Gouda and M J Zaki Ef ciently mining maximal frequent itemsets In N Cercone T Y Lin and X Wu editors ICDM conference San Jose USA  pages 163Ö170 IEEE Computer Society 2001  G Grahne and J Zhu Ef ciently using preìx-trees in mining frequent itemsets In FIMI 03 Proceedings of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations  November 2003  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation In SIGMOD Conference  pages 1Ö12 2000  M Kantola H Mannila K J Rih and H  Siirtola Discovering functional and inclusion dependencies in relational databases International Journal of Intelligent Systems  7:591Ö607 1992  A K oeller and E  A  Rundensteiner  D isco v e ry of highdimentional inclusion dependencies poster In Poster session of ICDE conference  IEEE Computer Society 2003  M Kryszkie wicz and M Gajek Concise representation of frequent patterns based on generalized disjunction-free generators In PAKDDê02 Taipei Taiwan  volume 2336 of Lecture Notes in Computer Science  pages 159Ö171 Springer 2002  G Liu H Lu J X Y u  W  W ei and X Xiao AFOPT An efìcient implementation of pattern growth approach In FIMI 03 Proceedings of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations  2003  W  A Maniatty  G  R amesh and M J Zaki Feasible itemset distributions in data mining Theory and application In SIGMOD conference San Diego USA  pages 284Ö295 ACM June 2003  H Mannila and K J R  aih  a The Design of Relational Databases  Addison-Wesley second edition 1994  H Mannila and H T o i v onen Multiple uses of frequent sets and condensed representations extended abstract In KDD conference,Portland USA  pages 189Ö194 AAAI Press 1996  H Mannila and H T o i v onen Le v e l w ise Search and Bor ders of Theories in Knowledge Discovery Data Mining and Knowledge Discovery  1\(1 1997  S Orlando C Lucchese P  P a lmerini R Pere go and F  Silvestri kDCI a multi-strategy algorithm for mining frequent sets In FIMI 03 Proceedings of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations  November 2003  P  P a lmerini S Orlando and R Pere go Statistical proper ties of transactional databases In ACM symposium on Applied computing  pages 515Ö519 New York USA 2004 ACM Press  N P a squier  Y  B astide R T a ouil and L Lakhal Ef cient mining of association rules using closed itemset lattices Information Systems  24\(1 1999  J Pei J Han and R  Mao Closet An ef cient algorithm for mining frequent closed itemsets In SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery  pages 21Ö30 2000  T  Uno T  Asai Y  Uchida and H Arimura LCM An efìcient algorithm for enumerating frequent closed item sets In FIMI 03 Proceedings of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations  November 2003  T  Uno M Kiyomi and H  Arimura LCM v e r  2 Ef cient mining algorithms for frequent/closed/maximal itemsets In FIMI 04 Proceedings of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations  2004  O R Za  ane M El-Hajj Y Li and S Luk Scrutinizing frequent pattern discovery performance In ICDE  pages 1109 1110 2005  M J Zaki and C.-J Hsiao Charm An ef cient algorithm for closed itemset mining In SIAM International Conference on Data Mining  2002 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


xt n 33284 xt n 20304 xt n 25016 xt t e xt  pl a i n 39404 A S 2005#219_184_01647786.pdf.t xt t e xt  pl a i n 32135 xt n 7439 xt t e xt  pl a i n 20831 A S 2005#219_187_01578757.pdf.t xt a ppl i c a t i on oc t e t s t re a m 29401 xt n 22909 xt t e xt  pl a i n 7901 A S 2005#219_190_01565747.pdf.t xt t e xt  pl a i n 18145 xt n 19319 xt a ppl i c a t i on oc t e t s t re a m 158 A S 2005#219_193_01565708.pdf.t xt a ppl i c a t i on oc t e t s t re a m 37537 A S 2005#219_194_01587556.pdf.t xt t e xt  pl a i n 22203 xt a ppl i c a t i on oc t e t s t re a m 34180 xt t e xt  pl a i n 14418 A S 2005#219_197_01616237.pdf.t xt t e xt  pl a i n 14621 xt n 23075 xt t e xt  pl a i n 28629 A S 2005#219_200_01607471.pdf.t xt t e xt  pl a i n 27671 xt a ppl i c a t i on oc t e t s t re a m 25191 xt t e xt  pl a i n 34132 A S 2005#219_203_01587551.pdf.t xt t e xt  pl a i n 24699 xt a ppl i c a t i on oc t e t s t re a m 24690 xt t e xt  pl a i n 27266 A S 2005#219_206_01662306.pdf.t xt t e xt  pl a i n 16968 xt n 29446 xt t e xt  pl a i n 24323 A S 2005#219_209_01461779.pdf.t xt t e xt  pl a i n 19792 xt n 28753 xt n 15442 xt a ppl i c a t i on oc t e t s t re a m 20877 A S 2005#219_213_01631278.pdf.t xt a ppl i c a t i on oc t e t s t re a m 29361 xt n 22678 A S 2005#219_215_015001 1 1.pdf.t xt t e xt  pl a i n 14579 xt t e xt  pl a i n 36462 A S 2005#219_217_01553005.pdf.t xt t e xt  pl a i n 24200 xt  33655 A S 2005#219_219_01562644.pdf.t xt a ppl i c a t i on oc t e t s t re a m 20212 xt n 27490 xt t e xt  pl a i n 25676 A S 2006#237_3_04216805.pdf.t xt t e xt  pl a i n 26780 xt n 24003 xt t e xt  pl a i n 25270 A S 2006#237_6_04052773.pdf.t xt t e xt  pl a i n 20034 xt a ppl i c a t i on oc t e t s t re a m 31 129 xt  34205 A S 2006#237_9_01684993.pdf.t xt t e xt  pl a i n 26787 xt n 28994 xt a ppl i c a t i on oc t e t s t re a m 25060 


A S 2006#237_1 1_04021078.pdf.t xt a ppl i c a t i on oc t e t s t re a m 25060 xt t e xt  pl a i n 17067 A S 2006#237_13_04085462.pdf.t xt a ppl i c a t i on oc t e t s t re a m 24865 xt n 16444 xt t e xt  pl a i n 29459 A S 2006#237_16_04030898.pdf.t xt a ppl i c a t i on oc t e t s t re a m 31967 xt n 44817 xt t e xt  pl a i n 30604 A S 2006#237_19_01692192.pdf.t xt t e xt  pl a i n 14301 xt n 47081 xt a ppl i c a t i on oc t e t s t re a m 21216 A S 2006#237_22_01651995.pdf.t xt t e xt  pl a i n 19500 xt n 24747 xt t e xt  pl a i n 21056 A S 2006#237_25_04072208.pdf.t xt t e xt  pl a i n 20440 xt n 26159 xt t e xt  pl a i n 51484 A S 2006#237_28_0404161 1.pdf.t xt t e xt  pl a i n 34488 xt n 36884 xt t e xt  pl a i n 15297 A S 2006#237_31_04028223.pdf.t xt t e xt  pl a i n 19681 xt n 29194 xt a ppl i c a t i on oc t e t s t re a m 25157 A S 2006#237_34_04061370.pdf.t xt a ppl i c a t i on oc t e t s t re a m 30602 xt a ppl i c a t i on oc t e t s t re a m 31343 xt t e xt  pl a i n 26734 A S 2006#237_37_01647707.pdf.t xt a ppl i c a t i on oc t e t s t re a m 55637 xt a ppl i c a t i on oc t e t s t re a m 50522 xt t e xt  pl a i n 32709 A S 2006#237_40_04104869.pdf.t xt t e xt  pl a i n 26395 xt a ppl i c a t i on oc t e t s t re a m 23362 xt a ppl i c a t i on oc t e t s t re a m 23314 A S 2006#237_43_04028215.pdf.t xt t e xt  pl a i n 23042 xt n 27599 xt a ppl i c a t i on oc t e t s t re a m 19 A S 2006#237_46_04155477.pdf.t xt t e xt  pl a i n 27219 A S 2006#237_47_04018542.pdf.t xt t e xt  pl a i n 30149 xt n 27958 xt t e xt  pl a i n 29943 A S 2006#237_50_04072193.pdf.t xt a ppl i c a t i on oc t e t s t re a m 26669 xt n 31 120 xt n 17300 xt a ppl i c a t i on oc t e t s t re a m 21930 A S 2006#237_54_04077818.pdf.t xt t e xt  pl a i n 20656 xt n 34280 xt t e xt  pl a i n 22732 A S 2006#237_57_04028269.pdf.t xt t e xt  pl a i n 21082 xt n 24550 xt t e xt  pl a i n 30583 A S 2006#237_60_04053125.pdf.t xt a ppl i c a t i on oc t e t s t re a m 22832 


xt a ppl i c a t i on oc t e t s t re a m 22832 xt t e xt  pl a i n 17993 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


