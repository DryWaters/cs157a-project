Combining Fuzzy Rules and a Neural Network in an Adaptive System Rainer Spiegel M.E Le Pelley Mark Suret  IPL McLaren University of Cambridge Department of Experimental Psychology Downing Site Cambridge CB2 3EB UK Email addresses rspiegel m.lepelley msuret i.mclaren Abstract  lt has been shown that humans can rely on both rules or associations to solve a problem We present a model in which rules may be applied to a particular sequence learning task hut rather than the rules being applied in an all-or-none fashion a continuum from fully representing 
to not representing a rule is required in order to model human task performance I GENERAL INFORMATION Computational models have had a large impact in trying to understand how humans solve a certain problem or how they perform on a particular task In human learning two main approaches have received considerable attention one is to assume that human performance on a task is purely statistical also called ussociafive and thus not guided by the application of explicit rules I In an attempt to simulate this kind of human learning artificial neural networks have become very popular One reason for this popularity is the 
fact that all leaming occurs through the changes of the connection weights and is thus guided by a purely statistical and general learning algorithm rather than an explicit rule to solve a particular problem In this context it is worth mentioning that the type of neural networks applied have been shown to converge to Bayesian optima 2]-[4 Other insights have revealed that a purely statistical mechanism is not enough to capture the variety of human responses in a range of learning experiments. In an attempt to simulate task performance here the use of explicit rules was considered 5]-[SI We agree that it is essential to have two learning systems \(a statistical one and 
a rule-based one The way that it bas been suggested that the rules in the rule-based part be implemented however postulates deterministic rather than stochastic mechanisms SI At this point we diverge from previous discussions of this issue We do not intend to deny the fact that children or adults may apply deterministic rules under some circumstances and we do not doubt that may have been the case in the studies mentioned 5]-[SI In our learning experiments however which used different procedures than the previous ones there is evidence that humans seem to have rather fuzzy representations of the rules or sometimes no representation at all I 9 whilst 
only a minority represent the rules in their entirety. Given this it is difficult to predict to what extent humans make use of a rule in our experiments Moreover, we have found that rather than applying a single rule humans often combine aspects of different ones in a rather fuzzy way The emphasis on fuzzy representation arises because humans usually did not apply a combination of each complete rule, but rather seemed to use parts of different rules as a consequence of trying to make analogies between different learning problems It should be noted that the word fuzzy in our paper is not used as in the classical form of 
fuzzy logic IO hut rather as a type of soft computing that denotes the partial representation of the rules in question A more detailed explanation will follow after the rules themselves have been introduced Based on our experimental evidence we have implemented a computational model that tries to capture the types of human processes used in dealing with the particular sequence learning task that provided the evidence for fuzzy representations rather than deterministic rules We start by giving a brief overview of the learning task followed by a detailed description of our model 11 THE LEARNING TASK Participants were seated in front of a computer screen on which 
a signal flashed in a particular order at different screen locations and they were told to react as quickly and accurately as possible by pressing the appropriate key on their keyboard a unique key was assigned to each screen location similar to Ill 12 They were not told that the signal flashes followed an underlying sequential structure at this stage. Lower reaction times accompanied by increasing accuracy on screen locations which were consistent with the underlying sequence structure previously experienced in the training phase were regarded as evidence of learning on test In addition, interviews after the experiment tried to explore the strategies and rules 
that people may have applied The reaction times and accuracy of people who were able to verbalize particular rules were compared to those who did not verbalize rules. Moreover, the verbalized rules were analyzed in detail. More information ahout the experimental procedure and how the exact experimental conditions e.g the Experimental and the Control groups were created can be found in I 9 13 In these tasks humans were trained on the following language sequences derivable from the expression below where each letter refers to a particular screen location on which a signal flashed on the keys on the keyboard did not carry the letters ABC hut were just spatially different keys 0-7803-7280-8/02/$10.M 02002 IEEE 340 


XJY X AB\(C Y ABB\(C where XIY denotes choice between sequences X and Y and C denotes a presentation of at least one C In X a single B follows the Cs because there had been a single B before the Cs In Y an additional B is added to each B in X Hence people should predict an A in sequence type X once a B has followed the unpredictable number of Cs In sequence type Y people should predict another B and then an A if a B follows the unpredictable numbers of C The general result of these experiments was that humans can exhibit both statistical and rule-based aspects of task performance depending on the number of sequences they had to learn. If they are trained on more numerous sequences with odd numbers of C only they will learn those sequences and fail to generalize to even numbers of C If however they are trained on more numerous sequences with both odd and even numbers of C it was found that they not only learn those sequences, but also generalize to both novel odd and even numbers of C In both of these cases people were not able to verbalize substantial parts of the rules Therefore, they were probably not able to make use of any rules to tackle these problems. Because they nevertheless learned the task it may be hypothesized that their performance was based on purely associative learning More evidence for this hypothesis stems from a simulation with a type of recurrent neural network I41 that was shown to converge to Bayesian optima 2]-[4 This network almost entirely overlapped in terms of its performance on the task with both learning and generalization results from the participants in the experiments which suggests that humans may have solved the task by extracting the statistical structure of the task in an associative way If humans are trained on less numerous sequences with odd numbers of C however they also show generalization to even numbers of C the same was found for humans who are trained on even numbers of C and show generalization to odd numbers of C In this case, their performance is unlikely to be purely statistical and the recurrent network simulation fails to model the results  network showed no generalization to even numbers The reason why their performance does not seem to rely on statistical structure alone is because some humans partially verbalize aspects of the underlying rule whilst a minority verbalizes them completely. Moreover the rule-verbalizers were substantially better at generalizing to even numbers of C and when they were left out of the analysis I no significant evidence for generalization to even numbers of C was found anymore In the sequences above participants who verbalize the rules would say that all sequences share the same analogies in that they are symmetric across the Cs and that the number of As and Bs before the Cs is the same as the number of As and Bs after the Cs One person could even say that the signals on the locations corresponding to the Cs had flashed an odd number of times The overwhelming majority however would only realize some aspects of the sequences, e.g that all sequences had the basic structure ABCBA which is symmetric but when asked about the number of flashes at each location they typically had no idea Others would verbalize chunks i.e repetitions of the same locations such as BB or CCC but they were not quite sure in which sequences those repetitions occurred and how they related to the overall structure of the sequence They could not tell for example that the Cs always repeated an odd number of times Others were able to tell that there were either one or two Bs before and after the Cs but did not detect the dependency between the number of Bs before the Cs and aAer the Cs As previously mentioned already, when the two participants who verbalized the entire rule were left out of the data analysis, those left would reveal similar results to the entirely statistical neural network i.e learning the trained odd numbers of Cs but showing little generalization to even numbers of Cs These considerations suggest that there might be a continuum between not representing aspects of the rule at all and having a full rule representation We note that this was first suggested by Cleeremans based on a large number of experiments \(summarized in 15 So far it also stands in line with our own empirical data 9 13 It would be difficult to generate such a continuum with classical Boolean logic, which would if applied to our task only allow either the representation of a rule or not Our studies suggest that on more numerous sequences, performance is associative, which could be modeled without any reference to rules consistent with the Boolean approach However on less numerous sequences there is an interplay between rule-based and statistical processing Moreover only a minority of the participants represents the rules completely whilst the majority can only tell about partial aspects of the rules or nothing at all Purely statistical models e.g 14 are generally not able to model the generalization performance of humans on this task as humans generalized to sequences with an even number of Cs where the models failed l Although models based on Boolean logic 191 if they interact with a statistical model are capable of simulating human performance on our task with less numerous sequences where humans show generalization to even numbers of C the complete representation of rules on these tasks would also lead to generalization on the tasks with more numerous sequences i.e where humans show statistical performance and thus do not generalize to an even number of Cs\Hence a model is needed that switches adaptively from partly statistical/rule-based representations to purely statistical ones as the trained sequences become more numerous It may still be possible to defend a model relying on Boolean logic by adding an external teacher that tells the model at which number of sequences it has to switch off the rule-based representations But this would be a rather abrupt change as the sequences increase in number Moreover this change does not stand in line with the results we have obtained from studying humans Humans rather show a gradual decline in terms of rule-representations i.e the representations become fuzzier as the sequences increase in number and partial representation of rule-based aspects may be present whilst they perform in a statistical way 0-7803-7280-8/02/$10.00 02,2002 IEEE 341 


IILTHE MODEL In order to implement the computational model we closely adhered to the information that the participants gave us in the interviews after the experiment It could not he decided whether humans reason using rules during the task because the interviews were carried out after the experiment Nevertheless we were able to infer that some participants have no rule-based representation i.e no verhalizable knowledge at all whilst a minority are able to verbalize the entire rule after the experiment In between however there seem to exist a variety of partial representations or fragments of the rule e.g some people become aware of symmetries others of repeating elements or the basic sequential structure In addition the more they could verbalize about the individual sequences the more they seemed to be able to draw analogies between them e.g that ABCBA and ABCCCBA are analogous to each other because they only vary in terms of the number of embedded Cs Two participants were even able to draw analogies between all sequences e.g that for all sequences the number of Bs before the Cs determines how many Bs appear after the Cs In order to make use of those analogies however the participants had to perceive the individual aspects of the sequences first i.e. they had to detect the symmetries and the repetitions in each of the individual sequences For this reason we made the assumption that each verbalized individual aspect of a sequence can vary between no representation or full representation and thus could he indexed by values between 0 no repr and 1 full repr In classical Boolean logic 0 and 1 are regarded as discrete values with no continuum between them Our decision to rely on fuzzy logic, where continuous values exist between 0 and 1 was in line with OUT experimental evidence i.e that people gradually become aware of the entire rule-representation by first perceiving individual parts of the sequences which are verbalized in the interviews i.e symmetries repetitions etc If there had been only people who fully represented the rule I as well as people who have no representation of the rule at all 0 then both Boolean and Fuzzy Logic would converge on the same results. In our sample, however there were many people who verbalized aspects of the rule hut not the entire rule so rule-representation became a matter of degree I Complete rule representation and analogy-making between the sequences modeled using mechanisms inspired by Hofstadter Mitchell and Marshall 16]-[19 would therefore happen automatically once the individual aspects symmetries repetitions basic sequential order ABCBA etc had passed certain thresholds Additionally some aspects of the sequence were more frequently verbalized than others, which had to be taken into account as well e.g the interviews indicated that it was more likely that participants became aware of the fact that the sequence of signals on the screen always followed the order ABCBA where different letters stand for different screen locations than it was for them to become aware of the fact that there were repetitions in some sequences, such as BB or CCC or that the AS and Bs are symmetric across the Cs. The probabilities of becoming 0-7803-7280-8/02/%10.00 02002 IEEE 342 aware of these sequential aspects as well as the exact data that our computational model achieved in an attempt to simulate human learning will not he considered in this paper and can he found in I 20 The purpose of this paper is to present an approach that models gradual rule acquisition in this learning task and takes into account fuzzy representations of the rule as well Each aspect in each sequence which is necessary for representing the rule i.e a symmetry a repetition etc will have an activity value between 0 and I In order to generate rule representations at least each element in at least two sequences must be represented in the correct order ABCBA i.e each of these elements must have an activity that passes a certain threshold Moreover both symmetries between the As and'the Bs must have activities that pass this threshold The same must hold true for at least one BB sequence with the addition that there is one more symmetry between the Bs as well as a repetition on the B elements before and after the Cs Only if these symmetries and repetitions have reached activities that pass the threshold can the rule representation that the number of Bs before the Cs is the same as the number of Bs after the Cs he formed In order to arrive at continuous activity values for each of these elements we created a squashing function that was inspired by findings from human learning experiments It has excitatory p and decay h elements and only one of them can he active at a time i.e either there is learning and thus an increase in activity or there is forgetting and thus a decrease in activity It also has a term that modulates interference between other information that is learnt i.e the more sequences q a person has to learn the less slhe can concentrate on a particular sequence and the more the activity will decay. On the other hand the more often excitation of a particular element has happened in the past the less its activity will decay which is controlled through the number of previous excitations q Bringing all this evidence together finally resulted in Equation 1 U  1 il a a  where h=\(l-P and p 0 I when applied and A=p=O when dormant 


At first sight this looks like a very complex equation because it combines several things at once e stands for exponential Euler s number 2.718 However it becomes much easier if we consider that at the most either the upper part or the lower part of this equation can he active at any one time either none of them is active i.e the equation stays dormant  nothing happens with p=h=O or there is excitation and therefore an increase in activity which has the consequence that p takes on a value of 1 and h takes on a value of 0 or there is decay with h=l and p=O This is because there cannot be both excitation as well as decay at the same time Moreover if there had not been any excitation in the past i.e no learning there can be no decay now i.e no unlearning In this equation, the activity of one time step is dependent on its own activity on the previous time step i.e ai+, is the activity of the next time step on the left hand side of the equation and is calculated by inserting the activity of the present time step ai into the right hand side of the equation At the start of training this activity will be 0 but as soon as it is first excited it will generate a positive value in the range between 0 and I which will change according to future excitation and decay The number of times an element gets excited depends on a probabilistic process, which is based on the percentage of humans who verbalized this aspect of the rule in the interview after the experiment ZO As 50 percent verbalized the basic order ABCBA then in order to implement appropriate parameters into the model with the right proportions in relation to the other verbalized aspects we assigned a probability of 0.5 to excitation of the activity of a particular screen location in a sequence even though this in itself would not directly result in the structure ABCBA being realized in the model 50  of the time that depends on other parameters such as threshold As an example when the sequence ABBCCCBBA is presented to the model \(which has the sequential order ABCBA, because repetitions do not change the order of screen locations each of the elements first A first B C second B second A has a 50 percen probability of being excited Imagine that the first A and the C get excited and this sequence is not presented again for a while For each training trial that is each time another sequence is presented to the model the previously excited activities will decay If however this sequence is presented many times then the activities of those elements will on average be excited more often The more often 11 they are excited the slower will be the decay The decay is further influenced by the number of other sequences c the model experiences in training the less this number is the slower will he the decay. Running a simulation on an element in the experiment described above I for 1,000 trials 4 sequences in training  on average each sequence gets presented 250 times reveals the following change in activity if the probability of selecting an element is 0.5 The time series of the activity change over the 1,000 trials is displayed in Figure 1 note that we were running 40,000 training trials in the simulation because this was the time taken by recurrent neural networks such as the SRN 14 and we aimed for a reliable comparison between this model and our own This non-linear process shows the high impact of decay at the beginning of training when the problem is not represented strongly enough As training proceeds however decay will have less and less impact and the activity of this element will increase to a value close to I which means full representation Only if all the elements relevant to perceiving the rule in each sequence have values that indicate full representation in our case values  95 will analogy making between different sequences be possible This implementation reflects the empirical results in the human experiment I where only a minority of the participants were able to report the entire rule. A larger number, however had partial representations e.g some aspects relevant to the rule may have passed the 95 threshold but not all of them so that no analogy-making leading to full rule comprehension was possible In many cases especially if the model has to learn a large number of sequences the activity values of each element are so small that not even a partial representation is possible In these cases in line with empirical evidence from studying humans 13 processing can be regarded as purely statistical This equation therefore ensures that more numerous sequences 4 will he less likely to cause the model to pass threshold and thus represent the rules whilst less numerous sequences will be more likely to make the model represent the rules 1 0.8 0.6 0.4 0.2 0 lo 500 1000 FIGURE 1 Time series of activity change values ranging from 0 to I on the y-axis from training trial 1 to 1000 represented an the x-axis We would like to underline though that this kind of equation is only one way of implementing the human process of gradually becoming aware of a rule whilst making some reference to what has been found out about learning decay and interference The particular advantage of this equation however is that it is well suited to interface with the recurrent neural network that we are going to describe later on. Trained with full gradient descent, this recurrent neural network takes a long time to converge on this problem 40,000 sequence presentations which is on average 10,000 presentations of each of the 4 sequences By taking this 0-7803-7280-8/02/$10.00 02002 EEE 343 


equation for the rule-based part where the excitation of activity is in proportion to the slow statistical model this is why we apply the square root to the exponent of the exponential in the upper right part of Equation I we were repeatedly able to simulate the interaction between the statistical part and the rule-based If we had another faster statistical model which required less training trials to converge on this problem, the rule-based part would need to build up its activity faster which would he possible if we omitted the square root of the exponent of the exponential Now we will describe the statistical part of the model In our experiments a particular recurrent neural network the SRN  141 provided a very successful simulation of the human data when they seemed to rely on the extraction of statistical structure, both in terms of learning as well as generalization An overview of the SRN Figure 2 and how it compares with other recurrent networks is given in the taxonomies of Home and Giles 21 or Kremer 22 Recurrent neural networks including the SRN constitute the largest family of temporal neural networks 23 A recurrent network in general can be defined as a network containing a state that depends on at least one of its previous states 23 In the SRN the network receives input from the input vector x t and the purpose is to predict the next input vector x t+l at the output values 0 The SRN has connections from the bidden units h t to the so-called context units which are exact copies of the hidden units one time step ago h GI At the next time step the hidden units will receive input from the input units as well as the context units I Output units o I uredict x t+l t weights Hidden units h t Context units h t-1 Hidden units h t Context units h t-1 FIGURE 2 Elman s simple recurrent network The hidden units will apply both kinds of information to predict the next step on the output level The context units play an important role as they provide the network with a dynamic memory i.e depending on the sequence position the very same inputs can result in different predictions from the network, because the hidden units will be able to develop different activities i.e even if the very same inputs appear repeatedly their representations on the hidden units will be different The connections from the hidden units to the context units are exact copies, whilst all other weights in the network are adjustable The weight updates are by backpropagation of error first discovered by Werbos 24 0-7803-7280-8/02/$10.00 02002 IEEE 344 and then independently developed by a number of scientists 25]-[28 We decided to use the most widely accessible form in our simulations 27 which includes a bias term to compute the activity of each hidden and output unit In order to explain how the rule-based model interacts with the neural network let us consider the following novel sequences ABCCBA and ABBCCBBA after having experienced sequences with 1 and 3 Cs in training with the model asked to predict the letter following the final B in the first sequence and the second last B in the second sequence i.e. ABCCB and ABBCCB Humans who rely only on statistics and a statistical model such as the SRN have not been found to generalize to this case I ZO Humans who rely partly on statistics and partly on rules generalize to this case by making analogies to the underlying rules in the trained sequences Our model also makes these analogies if it passes the thresholds of the sequential elements that are necessary to understand the rule i.e it would need to pass the threshold activities for each location A B C B and A in a sequence to which an analogy is made Moreover it would need to pass the threshold activities of the symmetries between the As and between the Bs in this sequence If this is the case not only in one sequence but in a number of sequences including the BB sequences where threshold activities for the repetitions must be passed as well the model can induce the rule by drawing analogies to the respective predictions of those sequences In an attempt to predict the final letter in the first sequence it would make analogies to the rules in all the other sequences, i.e take the activity of the symmetry between the As in the ABCBA sequence and multiply it with the activity of the location preceding it i.e the B in ABCBA It would do the same in the ABCCCBA sequence If it passes the thresholds for BB repetition it would also make analogies to the BB sequences by taking the activity of the B locations in the ABBCBBA and ABBCCCBBA sequence and multiplying them with the symmetries between the first and final As in both sequences This process is described in great detail elsewhere submitted Consequently we have an analogy value a for each trained sequence that has passed its respective threshold activities This analogy value rule-based part can then be multiplied with the output of the associative representation In the simple recurrent neural network this output is a vector and the analogy value a coming from the rule-based part is a scalar The formal way of writing this is displayed in Equation 2 At first sight this equation may again look pretty complicated, but in fact it is easier than one might think The output is simply a component of the 3 dim output vector which represents the prediction of the next location e.g the desired values of an A could he l,O,O the desired values of a B could be O,l,O etc The upper part of the equation is the output of a normal SRN which is based on the hackpropagation algorithm A detailed explanation of this equation can be found in 27 In this equation wik represents the weights from all the hidden units hr to the th component of output vector o and  is a bias term to it The value e is the exponential Euler s number 


2.718 If there are no analogies i=O analogies the performance will be purely statistical In this case, the output will reduce to the one of the SRN which is reflected on the upper right hand side of the equation If there are analogies all the analogies are added \(lower right hand side of equation and divided by the total number of analogies plus 1 which reflects the purely statistical prediction IV CONCLUSIONS It was postulated that the underlying processes of human sequence learning in this task seem to be fuzzy to at least some extent Many times an interaction is found between purely statistical and partly rule-based processes As demonstrated in other papers I ZO the rules that help to generalize to novel sequences with the same underlying structure are induced by making analogies to the rules in the trained sequences if some thresholds of rule-awareness have been passed e.g in less numerous sequences whilst the performance remains statistical if none of the thresholds of rule-awareness have been passed Our model presents one possible way to simulate these results including the gradual and often partial acquisition of rules It briefly sketches how these tules could interact with a purely statistical neural network Although other ways to integrate fuzzy logic and neural networks exist this model shows that fuzzy representations give rise to an interesting phenomenon in partly statisticalhle-based human sequence learning References 111 Spiegel R and McLaren I.P.L Human Sequence Learning Can Associations Exolain EvcMhine  Proceedines of the 23   I Annual Conference of the Cognitive Science Society pp 976 981, Edinburgh, August 14,2001 Mitchell, T.M., Machinc Learning, McGraw-Hill, 1997 Lebiere C and D Wallach Seouence Leamine in the ACT-R 121 131 141 Sun R and C.L Giles Sequence Learning Paradigms Algorithms, and Applications, Springer 2001 SI S Pinker Out of the Minds of Babes Science  Vol 283 pp 40-41 1999 161 G.F Marcus S Vijayan S Bandi Rao and P.M Vishton Rule learning in sevcn-month-old infants Science Vol 283 pp 77 80 1999 S Pinker and A Prince On language and connectionism Analysis of a parallel distributed processing model of language acquisition Cognition Vol 28, pp 73-193 1988 Marcus G.F The Algebraic Mind Integrating Connectionism and Cognitive Science, MIT-Press 2WI Spiegel R and McLaren I.P.L A Hybrid Model Approach to Generalization in Sequence Learning INNSilEEE Joint Neural Nehvork Conference, Washington DC July 15-19 2001 Zadeh L Fuvv Sets Information and Control Vol 8 DD  I   338-353 1965 M.J Nisscn and P Bullemer Attentional requirements of learning Evidence from performance measures  Cognitive Psyehology Vol 19,pp 1-32 1987 D.B Willingham M.J Nissen and P Bullemer On the development of procedural knowledge  Journd of Experimentol Psychology Learning Memory ond Cognilion Vol 15 pp 1047-1060, 1989 Spiegel R and McLaren I.P.L Rccurrent Neural Networks and Symbol Grounding  INNSiIEEE Joint Neural Network Conference, Washington, DC, July 15-19,2001 J.L Elman Finding swchxe in time  Cognitive Science Val 14,pp 179-211, 1990 French R and A Cleeremans Implicit learning and consciousness An empirical, philosophical and computational consensus in the making Psychology-press 2001 Hofstadter D and M. Mitchell The Copycat project A Model of Mental Fluidiw and Analoev-Makine in IHofstadter 19951 I 1    pp 205-267 199 Hofstadter D Fluid Concepts and Creative Analogies Basic Books 1995 Mitchell M Analogy-Making as Perception A Computcr Model, MIT-press, 1993 Marshall J.B Metacat A Self-watching Cognitive Architechlre for Analogy-Making and High-Level Perception PhD-thesis Indiana University, 1999 Spiegel R and McLaren I.P.L Modeling the Resulu of Spiegel and McLaren ZOOI Proceedings of the 23'4 Annual Conference of the Cognitive Science Society p 1239 CD ROW Edinburgh, August 14,200l B G Home and C L Giles An cxperimental comparison of recurrent neural networks In G Tesaum D Tauretzky and T Leen Eds Advances in neural information processing systems 7, pp 697-704, MIT-press, 1995 S C Kremer Spatiotemporal Connectionist Networks A Taxonomy and Review Neural Compulotion Vol 13 pp. 249 306,2M I C Chappelier M Gori and A Grumbaeh Time in Connectionist Models In R Sun and C.L Giles Eds Sequence Learning Paradigms, Algorithms and Applications 7 pp 105-134, Springer 2000 Werbos P Beyond regression new tools for prediction and analysis in behavior seienccs PhD Thesis Harvard University Cambridge MA 1974 Y Le Cun Une hoe dvre d Apprenlissage pour R sea Sed Asym trique Cognitiva 85 A la Frontierc de I Intelligence Artificielle des Scicnees de la Canaissance des Neurosciences pp. 599-604, 1985 Parker D.B Learning logic, Technical Report TR-47. Center for Computational Research in Economies and Management Science MIT, Cambridge MA 1985 Rumelhart D.E G.E Hinton and R.J Williams Learning lntemal Representations by Error Propagation  in  Rumelhart McClelland 19861 pp 318-362, 1986 Rumelhart D.E and J.L McClelland Parallel distributed processing, Val I MIT-Press, 1986 0-7803-7280-8/02/$10.00 02,2002 EEE 345 


T 2 001 in Figure 6 e two connected subtrees have the same root label 2  In Figure 6 the pair of tree ids are shown as x  y in parentheses on each node where x and y are e original and new tree ids r espectively After merging the root node has 3 children Without the new tree ids it could generate an invalid subtree e.g 2 1 1 3 1 since children with labels 1 and 3 do not appear to be the children of the node with label 2 at the same time By checking the new tree ids such invalid subtrees are avoided since the two children have different new tree ids  1 6 T2\220 8 2 3 4 5 2 5 10 Subtree: S2 15 2 1:3 1:3 1:3 1 6 After Merging 1:2,1:3 1:2,1:3 1:2 1:2 1:3 5 2 3 1:2 4 Subtree: S1 2 3 4 5 6 1:2 1:2 1:2 1:2 1:2 Original Tree Trimed Trees After Merging Figure 6 Handling Duplicate Labels The frequency counting is changed as follows for weighted support the frequency at each node is the number of new tree ids and for unweighted support the frequency is the number of original tree ids 4 Experimental Results The performance of PathJoin was examined through a series of simulation experiments All experiments were conducted on a Sun Blade 1000 with 1GB main memory and running Sun OS 5.8 The algorithm was implemented in C using Standard Template Library Three synthetic data sets D10 F5 and T1M were tested These data sets were generated using the method in 12 The synthetic data generation mimicks the Web site browsing behavior of the user The parameters used in the data generation include the number of labels N  100  the number of nodes in the master tree M  10000  the maximum fanout of a node in the master tree F 10  F 5 for data set F5 and the maximum depth of the master tree D 10  and the total number of trees in the data set T  100000  T  1000000 for T1M Two variations of the PathJoin algorithm were compared to examine the effect of pruning in candidate subtree generation one uses pruning in candidate subtree generation i.e function FST PathJoin checks whether the subsets of a candidate itemset are frequent and the other does not We did not compare our algorithm to others because of the difference of the problem de\223ned by us from others 4.1 Execution Time and Number of Candidate Subtrees The execution time for the three data sets with varying minimum support is own in Figures 7 and 8a The ecution time increases as the minimum support decreases since there are more frequent subtrees with smaller minimum support It can be also seen that the two variations of the algorithm with or without pruning in candidate subtree generation have no big difference in execution time This is because the pruning does not prune away many candidate subtrees less than 5 as shown in Figure 8b Thus the overhead for checking e subsets of the candidate subtrees offsets the saved e for frequency counting of the pruned candidate subtrees Similar results were obtained for the other two data sets which are not shown due to space limit The reason that the pruning is not very effective is that e candidate subtree generation in function computeF ST is limited to the children of a node in a tree in Forest i.e it is localized Such localizatio n reduces the number of candidate subtrees compared to apriori gen in 1 which i s applied to all the frequent itemsets found in a pass 2.2 2.4 2.6 2.8 3 3.2 3.4 3.6 3.8 0.05 0.01 0.005 0.001 0.0005 Execution time\(seconds Support threshold PathJoin-NoPruning  PathJoin-Pruning  a set F5 0 5 10 15 20 25 30 0.05 0.01 0.005 0.001 0.0005 Execution time\(seconds Support threshold PathJoin-NoPruning  PathJoin-Pruning  b set D10 Figure 7 Execution Time 20 40 60 80 100 120 140 0.05 0.01 0.005 0.001 0.0005 Execution time\(seconds Support threshold PathJoin-NoPruning  PathJoin-Pruning  a\ecution Time 0 2000 4000 6000 8000 10000 12000 14000 0.05 0.01 0.005 0.001 0.0005 Candidate subtrees Support threshold\(unweighted PathJoin-NoPruning  PathJoin-Pruning  b of Candidate Subtrees Figure 8 Data set T1M Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


4.2 Memory Usage and Scaleup The memory usage for the compact data structure FSTForest has two parts one for the compressed tree structure the other for the tree ids The memory for the compressed tree structure is 223xed for a data set given some minimum support threshold while the memory for the tree ids will grow as the number of trees in the data set increases In the experiments the data set s with different number of trees were generated with the same parameters as for data set T1M The minimum support was 223xed to 0.5 Figure 9a shows the memory usage at two stages of the algorithm the lower curve shows the the memory before expanding all subtrees with the same label and the upper curve after computing the maximal frequent paths and merging the treeids upward It can be seen that the memory usage is about doubled after expansion and merging Figure 9 also shows that both the memory usage and the execution time scales linearly with respect to the change of the number of trees in thedataset  0 20 40 60 80 100 120 140 160 180 0 500 1000 1500 2000 2500 3000 3500 4000 Memory usage\(MBs Number of trees\(in 1000\220s FST-Forest\(before expanding  FST-Forest\(after MFP  a Memory usage 0 20 40 60 80 100 120 140 0 500 1000 1500 2000 2500 3000 3500 4000 Execution time\(seconds Number of trees\(in 1000\220s PathJoin-Pruning b Scaleup Figure 9 Memory Usage and Scaleup 5 Conclusion A new type of tree mining maximal induced subtrees in unordered trees is de\223ned in the paper A novel algorithm PathJoin is proposed to discover all maximal frequent subtrees given some minimum support threshold The algorithm uses a compact data structure FST-Forest to compress the trees in the database and at the same time still keeps the original tree structure A localized candidate subtree generation method is used in the algorithm which reduces the number of candidate subtrees substantially The algorithm is evaluated with synthetic data sets The future work includes 1 earlier identi\223cation of maximal frequent subtrees which could potentially save a lot of e for computing the non-maximal frequent subtrees 2 extension of FST-Forest for mining maximal frequent embedded subtrees which allow ancestor/descendent relationship Acknowledgement We would like to thank Prof Mohammed J Zaki for sending us the source code for e tree generation program References  R  A gra w al and R  S r i kant  F ast al gori t h ms for m i n i n g a ssociation rules in large databases In Proceedings of the Twentieth International Conference on Very Large Databases  pages 487\205499 Santiago Chile 1994  R  A gra w al and R  S r i kant  M i n i n g s equent i a l p at t e rns In Proceedings of the 11th International Conference on Data Engineering  Taipei Taiwan Mar 1995 IEEE Computer Society Press  T  A sai K Abe S  Ka w a so e H Arimura H Satamoto and S Arikawa Ef\223ciently substructure discovery from large semi-structured data In Proceedings of the 2nd SIAM Int\325l Conference on Data Mining  april 2002 4 M  S  C h en J S  Par k  and P  S  Y u  E f 223 ci ent dat a m i n i n g for path traversal patterns IEEE Transactions on Knowledge and Data Engineering  10\(2\:209\205221 1998 5 G  C ong L  Y i  B  L i u  a nd K W a ng Di sco v e r i ng f r e quent substructures from hierarchical semi-structured data In Proceedings of the 2nd SIAM Int\325l Conference on Data Mining  Arlington VA april 2002 6 J  H an J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In Proceedings of the M SIGMOD Conference  2000 7 A  I nokuchi  T  W ashi o and H Mot oda An apr i or i based al gorithm for mining frequent substructures from graph data In Proceedings of the 4th European Conference on Principles of Knowledge Discovery and Data Mining  sep 2000 8 M  K ur amochi and G Kar ypi s F r equent subgr aph di sco v ery In Proceedings of the 1st IEEE Int\325l Conference on Data Mining  nov 2001 9 D  S hasha J W a ng and R  Gi ugno Al gor i t h ms and appl i cations of tree and graph searching In Proceedings of the 21st M SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  pages 39\20552 Madison Wisconsin june 2002  Y  Xi ao and M H Dunham E f 223 c i e nt mi ni ng of t r a v er sal patterns Data and Knowledge Engineering  39:191\205214 2001  X Y a n a nd J Han gspan Gr aphbased subst r uct ur e pat tern mining In Proceedings of the 2002 IEEE International Conference on Data Mining ICDM 2002 9-12 December 2002 Maebashi City Japan  pages 721\205724 IEEE Computer Society 2002  M J Z a ki  E f 223 ci ent l y mi ni ng f r e quent t r ees i n a f or est In Proceedings of the 8th M SIGKDD Int\325l Conference on Knowledge Discovery and Data Mining  Edmonton Canada jul 2002 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


association-cube, base-cube and population-cube are derived from the volume cube; the confidence-cube is derived from the association cube and population cube and the support-cube is derived from the associationcube and base-cube. The slices of these cubes shown in Figure 2 correspond to the same list of values in dimension merchant, time, area and customer_group  Multidimensional and multilevel rules Representing association rules by cubes and underlying cubes by hierarchical dimensions, naturally supports multidimensional and multilevel rules. Also these rules are well organized and can be easily queried  First, the cells of an association cube with different dimension values are related to association rule instances in different scopes. In the association cube CrossSales cell CrossSales product \221A\222, product2 \221B\222  customer_group 221engineer\222, merchant \221Sears\222, area \221Los Angeles\222, time 221Jan98\222 represents the following multidimensional rule x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x,\221B\222  275 customer_group = \221engineer\222, merchant = \221Sears\222, area 221Los  Angeles\222, time =  \221Jan98\222 If this cell has value 4500, and the corresponding cell in the population cube has value 10000, then this rule has confidence 0.45 Next as the cubes representing rules can have hierarchical dimensions, they represent not only multidimensional but also multi-level association rules. For example, the following cells CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221Jan98\222 CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221 Year98 222 represent association rules at different area levels \(i.e the city level and the state level\d different time levels \(i.e., the month level, the year level x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221Jan98\222 x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221 Year98 222 The cell CrossSales\(product \221A\222, product2 \221B\222,  customer_group 221top\222, merchant \221top\222, area \221top\222,  time \221top\222 represents the customer-based cross-sale association rule for all customers, merchants, areas, and times in the given range of these dimensions, expressed as x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222 4.3  Generating Association Rule Related Cubes The basic task of our OLAP based association rule mining framework, either at the GDOS or at an LDOS is to convert a volume cube i.e. the cube representing the purchase volumes of customers dimensioned by product  area etc, into an association cube a base cube and a population cube These cubes are then used to derive the confidence cube and the support cube of multidimensional association rule instances. The following general steps are involved in cross-sale association rule mining 267  Roll up the volume cube SaleUnits by aggregating it along merchant, time, area dimensions 267  Derive cube NumOfBuyers from SaleUnits based on the antecedent condition SaleUnits 0 267  Populate cube NumOfShoppers by the counts of customers dimensioned by merchant, area  time not by product\at satisfy the antecedent conditions 267  Derive cube CrossSales from SaleUnits based on the association conditions SaleUnits  product  p 1  0 and SaleUnits  product2  p 2 0 267  Derive cube Confidence and cube Support using cell-wise operations 214  Confidence = CrossSales  NumOfBuyers 214  Support  CrossSales  NumOfShoppers  Cubes Confidence  Support  CrossSales are dimensioned by product  product2 customer_group,merchant  time, area NumOfBuyers is dimensioned by product  customer_group, merchant time, area  NumOfShoppers is dimensioned by customer_group, merchant  time, area Rules with confidence and support that exceed specified thresholds  may be considered interesting 4.4. Rules with Conjoint Items Cubes with conjoint dimensions can be used to represent refined multidimensional association rules For example, using OLAP, we can derive association rules across time  Time-variant or temporal association rules such as 


x 316 Customers buy_product\(x,\222 A\222, \221 Jan98\222  336 buy _product\(x, \221B\222, \221 Feb98\222   275 area = \221Los Angeles\222 can be used to answer such questions as \223 How are  the sales of B in Feb98  associated with the sales of A in Jan98 224 The items in this rule are value pairs of dimensions product and time In order to specify this kind of association rule we introduce a conjoint dimension product, time and mirror it with dimension product2, time2 This allows a cell in the association cube to cross two time values. Accordingly, the cubes related to association rule mining are defined as follows Association cube  CrossSales.2 \(<product, time>, <product2, time2 customer_group, merchant, area  Population cube  NumOfBuyers.2  \(<product, time>, customer_group merchant, area Base cube  NumOfShoppers.2  \( customer_group, merchant, area Confidence cube Confidence.2 \(<product, time>, <product2, time2 customer_group, merchant, area Support  cube  Support.2  product, time>, <product2, time2 customer_group, merchant, area  The steps for generating these cubes are similar to the ones described before. The major differences are that a cell is dimensioned by, besides others product, time and product2, time2 and the template of the association condition is  SaleUnit s  product p 1 time t 1  0 and  SaleUnits  product2 p 2 time2 t 2  0 where, in any instance of this condition, the time expressed by the value of time2 is not contained in the time expressed by the value of time The template of the antecedent condition is SaleUnits   product p 1 time t 1  0 In general, other dimensions such as area may be added to the conjoint dimensions to specify more refined rules 4.5. Functional Association Rules A multidimensional association rule is functional if its predicates include variables, and the variables in the consequent are functions of those in the antecedent.  For example, functional association rules can be used to answer the following questions, where a_month and a_year are variables q  What is the percentage of people in California who buy a printer in the next month after they bought a PC x 316 Customer buy_product\(x, \221PC\222, a_ month 336 buy_product\(x, \221printer\222, a_month+1  275 area = \221California\222 q  What is the percentage of people who buy a printer within the year when they bought a PC  x 316 Customer: buy_product\(x, \221PC\222, a_ year 336 buy_product\(x, \221printer\222, a_year 275 area = \221California\222 To be distinct, we call the association rules that are not functional as instance association rules; e.g x 316 Customer: buy_product\(x,\222 PC\222, \221Jan98\222 336 buy_product\(x,\222 printer\222, \221Feb98\222  275 area =  \221California\222 Time variant, functional association rules can be derived from time variant, instance association rules through cube restructuring. Let us introduce a new dimension time_delta that has values one_day, two_day 205, at the day level, and values one_month, two_month, \205, at the month level, etc. Then, let us consider the following functional association rule related cubes Association cube  CrossSales.3 \(product, product2, customer_group merchant, area, time_delta  Population cube  NumOfBuyers.3 \(product, customer_group, merchant area Base cube  NumOfShoppers.3 \( customer_group, merchant, area Confidence cube  Confidence.3 \(product, product2, customer_group merchant, area, time_delta Support cube  Support.3 \(product, product2, customer_group, merchant area, time_delta The association cube CrossSales.3  can be constructed from CrossSales.2   The cell values of CrossSales.2  in the selected time and time2 ranges are added to the corresponding cells of CrossSales.3 For example, the count value in cell  CrossSales.2\(<PC, Jan98>, <printer, Feb98>\205 is added to cell \(bin CrossSales.3\(PC, printer, one_month,\205 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


