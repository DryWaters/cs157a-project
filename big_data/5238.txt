Tree Labeled LDA: A Hierarchical Model for Web Summaries Anton Slutsky, Xiaohua Hu and Yuan An College of Information Science and Technology Drexel University Philadelphia, PA, USA as3463, xh29, yuan.an}@drexel.edu   Abstract We study the applications of hierarchical topic models to represent the content of website summaries. We concentrate on the DMOZ collection of Web extracts and propose a novel Tree Labeled LDA \(tLLDA\gorithm to infer topic models using its manually compiled ontology.  The algorithm takes advantage of the ontology structure and infers topic models by jointly modeling word and ontology node assignments for documents.  We evaluate the performance of our topic modeling approach against that of four state-of-the-art algorithms \(Labeled LDA, Hierarchically Labeled LDA Hierarchically Supervised LDA and Supervised LDA\nd show improvement in terms of perplexity and accuracy.  Our evaluation shows that topic models produced by tLLDA outperform other algorithms in terms of perplexity for all test sets and all but one test case in terms of accuracy  I  I NTRODUCTION  The Open Directory project \(also known as DMOZ\ aims to organize web content by compiling a comprehensive directory of public web sites available on the Web.  This Web directory compilation is accomplished through the hard work of thousands of human volunteers who inspect and manually map websites based on their content into a well-defined ontology   Unfortunately, DMOZ ontology elements provide no information beyond their textual labels as to the meaning of associated documents.  Even though much work has been done by the DMOZ project towards organizing and classifying collections of Web documents, the project makes no attempt to extract path-level and statistical views of content.  Such additional information may facilitate tasks such as browsing searching, and assessing document similarity  Therefore, in this work, we attempted to leverage the manually created ordered ontology structures published by the Open Directory project to model the content of underlying document collections.  The aim was to produce a view of the content that would enable individuals to quickly grasp underlying themes of documents associated with ontology nodes.  Using topic modeling techniques, we developed an approach for constructing statistical views of ontology nodes by associating them with topics, which were probability distributions over a fixed vocabulary.  Our technique estimated distribution parameters as word-multinomials and used these multinomials to produce sorted lists of vocabulary terms for all nodes in the ontology.  Qualitative evaluation of these lists suggested that top most probable terms, as specified by the corresponding word-multinomial parameters, were indicative of the underlying general theme of Web documents We propose a new probabilistic generative model based on the Labeled-LDA proac h  w h ic h is a su perv i s ed v a ria n t of the well-known LDA odel.  T h e n e w T r ee L a beled LDA model takes advantage of the hierarchical nature of DMOZ ontology and jointly models word and ontology node assignments as a generative process Quantitative evaluation of our approach conducted by comparing predictive power of resulting topic models with that of topic models produced by other state-of-the-art algorithms in terms of perplexity and accuracy for held-out data showed that, for datasets used in the study, the new tLLDA model outperformed Labeled LDA and Hierarchically Labeled LDA topic modeling approaches in terms of perplexity in all cases and Supervised LDA and Hierarchically Supervised LDA in terms of classification accuracy in all but one case where Supervised LDA beats our model by a small margin.  The subsequent qualitative evaluation of the resulting topic models as compared to topic models produced by Labeled LDA and Hierarchically Labeled LDA suggested that topic models produced by the new tLLDA were more semantically indicative of the underlying content The rest of the paper is organized as follows.  Section 2 presents the review of notable related works.  In section 3, we introduce the generative Tree Labeled LDA \(tLLDA\ model in detail.  Section 4 discusses procedures for estimate distribution parameters from data.  In section 5, we discuss experimental setup and evaluate language models produced by the tLLDA approach as compared with other algorithms.  In sections 6 and 7 we discuss conclusions and outline future work II  R ELATED W ORKS  The now classical work on Latent Dirichlet Allocation LDA a s prov ided an exten s i ble m odu lar f r a m e w or k  for  many topic modeling approaches.  In LDA, the basic idea is that documents are represented as random mixtures over latent topics with each topic characterized by a distribution over wo h ile L a ten t Dir i ch let A llocatio n h a s serv ed as basis for many approaches [2  fu ll y  u n s u pervis e d a n d may not be the best choice when the goal is prediction.  That is, for instance, in a system concerned with movie ratings intuitively good predictive topics could differentiate between 134 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 


excellent”, “terrible” and “average” ignoring the genre Unsupervised machinery of the basic LDA, however, may estimate topics that correspond to genres if that is the intrinsic structure of the corpus The Supervised LDA model \(sLDA ten d s L D A b y  taking supervision into account and adding a response variable associated with each document. This variable is usually associated with the supervisory labels for documents, such as the film rating adjectives in the above example.  The sLDA model jointly models documents and responses with the goal of finding latent topics that will best predict responses for test  T h e res pon s e v a l u es co m e  f r o m a n o r m al lin e a r model, which covariates in the sLDA model with empirical frequencies of topics in documents A further refinement on the Supervised LDA model – the Hierarchically Supervised LDA \(HSLDA te n d s sLD A  to take advantage of hierarchical supervision.  The HSLDA model is based on the intuition that hierarchical context of labels provides valuable information about labeling.  As in sLDA, HSLDA jointly models documents and responses by drawing response variable realizations from a Normal distribution, but unlike sLDA it generates label responses using a hierarchy of conditionally dependent probit regressors  In th e join t m odeling of each docum en t, both e m piri cal topic distribution and whether or not the parent label is applied to the document determine whether or not a label is to be applied.  The HSLDA model views word-multinomials topics\lobal constructs and links them to hierarchy nodes through per-label topic distributions.  This makes HSLDA output difficult to interpret, as the global topics do not directly correspond to nodes While Supervised LDA and Hierarchically Supervised LDA have been shown to work well in some applications they have the limitation of allowing only a single label to be applied to a document and are thus not applicable to document collections where multiple labels can be assigned to tex   Therefore, a radically different way of providing supervisory input to topic modeling was developed.  Named Labeled LDA L-LDA\is model aimed at joining the multi-label supervision frequently found in modern text databases with word-assignment disambiguation of LDA family of models  In L L D A  each un iqu e l a bel is v i e w ed as a topic an d t h e  goal of the model is to restrict the generative process to operate over a subset of topics, thus allowing for each document to be supervised Similarly to L-LDA, Hierarchically Labeled LDA \(or hlLDA s tricts t h e g e neral L D A  m odel to operate o v e r a subset of label-bound topics.  While L-LDA is a general model for corpora where documents may be associated with multiple topics that may or may not be related to each other in any way, hlLDA considers each document to be associated with a set of topics which correspond to the set of nodes on the hierarchical path for each document.  The hlLDA model relies on some of the formalisms described in yet another LDA successor model called Hierarchical LDA \(hLDA\6  which is an unsupervised generative model which infers hidden hierarchical structures from data.  The hlLDA variant extends the hLDA approach by assuming that the hierarchical structure -- which is hidden from hLDA -- is known and restricts topics accordingly.  Unfortunately, the hlLDA model is limited as it only considers supervision from observed hierarchy paths for each document and does not make use of the hierarchy structure as a whole It is important to note that hierarchical topic modeling techniques such as hlLDA and HSLDA place statistical pressure on the posterior to have more general terms in topics towards the root of the hierarchy as there are more paths through nodes at higher levels than there are through lower level nodes [7 h ile th is is d e sirab l e in so m e case s th e  statistical pressures may become overwhelming for sparsely populated hierarchies.  The proposed tLLDA model attempts to combat the tendency to favor higher level nodes by offering a statistical counterbalance derived from the structure of the hierarchy III  M ODEL  In this section we introduce the Tree Labeled LDA tLLDA\which is a generative probabilistic model that describes the process of generating a document collection where each document is associated with a distribution over hierarchy nodes.  It improves upon hlLDA by jointly modeling word and node label assignments, which allows it to take the hierarchy structure as a whole into consideration.  Further, the proposed tLLDA model estimates a single word-multinomial for each node of the target ontology, which allows for an easier interpretability when compared with the HSLDA model The tLLDA model is intuitively motivated by the nature of manually constructed taxonomies, where conceptual meanings of hierarchy labels often represent more general concepts at levels closer to the root node and increase in specificity towards the leaf nodes [7 tL L D A atte m p ts to tak e ad v a n t a g e  of hints as to the specific meaning of each document by biasing the generative process towards hierarchy nodes representing narrower conceptual meanings while still allowing words to be generated from all hierarchy levels Further, we are motivated by the intuition that hierarchical nodes with edges to large number of children are representative of more general concepts whereas fewer child elements may be indicative of narrower meanings.  The tLLDA model takes the later intuition into account by relaxing the bias towards lower hierarchy levels in those cases where paths are comprised of relatively barren nodes  The tLLDA model aims to incorporate both the multi-label supervision and supervision derived from the structure of the target hierarchy.  Similar to other topic modeling techniques [5 8  it  a d o p ts  th e m i x e d m e m b ersh ip f o rm alism  4  w h ere  a  document is thought of as a mixture over a set of wordmultinomials [8   Th e tL L D A ap p r o a ch c o m b in es th e m u ltilabel supervision of hlLDA with hierarchical supervision by jointly modeling word and label assignment generation.  Also unlike HSLDA which generates label responses using conditional hierarchy of probit regressors [5 a ssum i ng a Normal distribution, tLLDA draws a path  through the hierarchy for each topic directly from a distribution 135 


parameterized by a global vector of multinomial parameters without assuming any underlying distribution shape A  Notation Let each document d be represented by a tuple consisting of list of word indices    where each   is the vocabulary and is the document length. Let hierarchy  be a tree which is a directed acyclic graph with known structure where  is the set of vertices \(or nodes\of size  is the set of edges is the root node and each node may have at most one parent.  Let be the total number of topics equal to the size of the set Note that since the number of topics equals the number of vertices in the hierarchy, topics and vertices \(nodes\ay be used interchangeably in this context. Let a path to a node be the list   such that  and each subsequent node in the list  is a child of its predecessor.  For each node let  be a set such that        Let represent the sub-tree of node such that and    B  Theory The tLLDA approach models document creation by imagining a process that randomly generates two strings of equal length – a string of words and a string of hierarchy node labels.  The string of words is generated in a way common to other topic modeling approaches [3, 5, 6, 8  T h at is as  in  other related approaches, tLLDA draws a topic from a distribution parameterized by a topics proportions vector and then generates a word by drawing it from a distribution parameterized by a word-multinomial vector.  At this stage tLLDA diverges from other topic modeling algorithms [3, 5  d adds a n e w s t ep t o g e n e rate a label b y ran d o m l y  selecting a node from the distribution over labels conditioned on the topic drawn earlier.  The probability of drawing node label from some node \(topic is     Because probabilities of drawing a word from a topic and drawing a label from a distribution conditioned on the topic are independent, the chance of drawing a word string and a label string  of length is           where is the mixture proportion vector and     T h e g e n e rati v e process o f th e al g o rithm is ou t lin ed  in Figure 1         1  For each topic   2  Generate     3  For each topic   4  For each topic   5  Deterministically set  using Eq. 1 6  Generate    7  Generate       8  For each document  9  For each topic   10  Generate    11  Generate    12  Generate      13  For each word   14  Draw      15  Draw    16  Draw    Fig. 1  tLLDA generative algorithm Steps 1 and 2 where the multinomial topic distributions  over vocabulary for each topic  conditioned on the Dirichlet prior are drawn remain identical to the standard LDA, L-LDA and hlLDA To use the hierarchy structure as a supervising agent for the generative process we deterministically construct vector    such that  for each topic in steps 4-5     1 Because the hierarchy structure is known apriori and is fixed for all documents, this deterministic step does not jeopardize the generative nature of the approach.  Having thus generated vector  we define a node-specific matrix  over  for each node where    for each row  and column         The matrix  is used to project parameter vector of the Dirichlet prior to lower dimensional vector  in step 6       In step 7, path assignment proportion vector is drawn for each topic with parameter vector  Since  is constructed by the use of the makeup of vector depends on where the node corresponding to topic is in the target hierarchy in relation to other nodes. Therefore, the parameter vector  encodes the supervisory input of the hierarchy structure as it is a projection of K-dimensional vector onto the lower dimensional space defined by s sub-tree Then, for number of topics let   be the list of binary topic presence/absence indicators for document such that  As in L-LDA, vector  is generated in steps 9-10 by using a binomial distribution for each topic with prior 3  W i t h th a t  a d o c u m en ts p e c i f i c 136 


label projection matrix  over is defined for each document where    for each row  and column          The matrix  is used to project the parameter vector of the Dirichlet prior to lower dimensional vector           Step 12 draws  by parameterizing a Dirichlet distribution with  computed in step 11.  Then, to generate a word, topic is sampled from a distribution parameterized by and a word is sampled from distribution over words parameterized by the word proportion vector  Unlike other algorithms that repeat the word generation process at this point, tLLDA draws a label assignment  from a distribution pa rameterized by vector where is the topic drawn in step 7.  As the vector is projected onto the lower dimensional space defined by s subtree \(see definition of in this step that the structure of the hierarchy is included into the supervision -- makeup of and consequently the values of vector depend on where the th node is in the target hierarchy in relation to other nodes   Fig. 2  hLLDA graphical model showing the plate diagram with solid lines representing probabilistic links and dashed lines representing deterministic relationships.  The shaded circles represent observed nodes whereas un-shaded nodes are latent IV  P ARAMETER E STIMATION  The model of document generation described in the previous section outlines a process that is not directly observed.  Instead, an observer is presented with the output of the process, which may be used to infer latent parameters that governed the algorithm responsible for producing the output In much of the topic modeling literature, the output is limited to word strings that constitute documents in a given corpus The tLLDA model, however, posits that there exists a second output detectible to an outside observer – namely, the document-length string of label assignments And indeed, such additional output can be detected by considering the placement of each document in the target hierarchy.  Since document is associated with a hierarchy node, it is easy to construct a document-length string of label assignments by repeating the node’s label as many times as there are words in the document.  That is, in the tLLDA context, we say that in addition to observing a randomly generated string of words, we observe the eventuality of the randomly generated string of labels being entirely made up of identical elements We used Gibbs sampling to estimate topic-word distribution parameter  since compared to other parameter estimation methods, Gibbs sampling yields a relatively simple and computationally efficient algorithm m p li n g  equation for a topic for document d and path leading to  notation outlined in Table 1                  TABLE I  N OTATION   Dirichlet hyperparameters   The number of times word assigned to topic not counting the current instance   Number of times any word is assigned to topic  excluding the current instance   The number of times topic is assigned to document excluding the current instance   Number of times any topic is assigned to document            Observed node assignment for document    After a predetermined number of iterations of the sampling process based on distributions estimated using the above equation, parameters can be estimated for any single sample as using the following equations        V  E XPERIMENTAL R ESULTS  A  Experimental Data We tested the tLLDA on the Web summary dataset provided by the Open Directory project.  The Open Directory project publishes four distinct data dump pairs \(structure file and content file\hese are ‘World’, ‘Kids and Teens 137 


Adult’ and ‘AOL’. The well-structured DMOZ ontology is published in a separate RDF documents devoid of review content.  Review content RDF files are made up of sections corresponding to ontology identifiers \(topics\ section contains a set of URL references to websites, their titles and description strings that are relatively short with the average of 4.2 and 18.1 words per string respectively The ‘World’ RDF download contains the ontology structure and corresponding reviews for all web sites reviewed by the project excluding those found in ‘Kids and Teens Adult’ and ‘AOL’ repositories. The ‘Kids and Teens download contains reviews of web sites related to children and teenagers, such as reviews of cartoons and primary school activities.  In our experiments, we excluded the ‘AOL repository as the latest file published at t ain ed n o  review content.  We also excluded the ‘Adult’ section from our experiments for ethical reasons and tested our approach on the ‘World’ and the ‘Kids and Teens’ datasets.  As both the World’ and ‘Kids and Teens’ repositories were large \(~2.1 million records and ~26,000 respectively\ and due to hardware and time constraints we tested our approach using a set of smaller subsets of records extracted from each repository For our experiments, we used the English language portions of the ‘World’ and the ‘Kids and Teens’ DMOZ datasets.  As preprocessing st eps, each raw record, title and description strings were extracted and concatenated into a single document.   Resulting strings where case-folded and tokenized using simple tokenization rules followed by nonletter characters \(numbers and punctuation\ removal Stemming and lemmatization was not applied.  Following the process in related works o cabu l ar y  w a s ex tracted b y a  single pass through the data and documents were regenerated by replacing English terms with numerical identifiers of terms in the vocabulary Because of the sparse nature of data sets which are made up of relatively small number \(average of 5.4 documents per hierarchy node\ short review documents and in order to preserve the underlying relationships within the hierarchy, we extracted six fixed size data windows of various sizes from each of the two data sets. Further, we reserved 10% of the data in each data window for testing by applying simple random sampling without replacement.  The maximum number of records used for testing the a pproach was limited to 6000 elements.  This number was chosen as it was empirically determined to be the largest number of records that the available hardware was able to process in one twenty-four hour period for each algorithm B  Comparison Models We compared the predictive power of the language models produced by our approach to four related models against the Open Directory \(DMOZ\ dataset.  The four comparison models included the Hierarchically Labeled LDA d t h e Labeled LDA icall y Su pervis e d L D A 5] a n d Supervised LDA e cons idered t w o distin ct co m p arison  approaches as the evaluation criteria.  We used perplexity to compare tLLDA performance with that of hlLDA and L-LDA as perplexity is a common way to compare language models   T o c o m p are tLL D A pre d ictiv e po w e r w i t h th at of s L DA and HSLDA we used multilabel classification  Exact Match MR as the evaluation criteria.  We chose to use multilabel classification as opposed to perplexity for sLDA and HSLDA because sLDA-type approaches associate supervisory topics with distributions over language models rather than with single language model, making evaluation in terms of perplexity not feasible We compared language models produced by our approach to those learned by hlLDA and L-LDA as the output of these models lends itself to per-topic language model comparison The perplexity measure used for comparison over held-out subset of data   given language model and the training com p u t ed v i a th e f o llo w i ng f o r m ula        where     is the jth term in the ith string in the held-out collection and  is the probability of term as per the learned language model  The tLLDA approach outperformed L-LDA and hlLDA algorithms in terms of perplexity.  Tables 2 and 3 summarize experimental results for ‘World’ and ‘Kids and Teens’ data subsets respectively in terms of perplexity values.  We set  and  for all experiments as these values are commonly chosen in the related literatu W e  set the number of iterations to 1000 as the algorithm appeared to converge past that number of iterations TABLE II  W ORLD   R ESULTS   1000 2000 3000 4000 5000 6000 L-LDA 2642.6 3952.4 4661.7 6016.8 6740.4 8142.8 hlLDA 3159.3 4598 5714.7 7046.4 8264.21 9787.72 tLLDA 690.25 795.99 779.99 877.47 1077.13 1356.14 World’ dataset perplexity values for L-LDA, hlLDA and tLLDA rows\ and 1000-6000 record data windows \(columns TABLE III  K IDS AND T EENS   R ESULTS   1000 2000 3000 4000 5000 6000 L-LDA 4170.66 4958.17 6232.11 7450.27 8226.14 8976.4 hlLDA 4081.14 6247.47 7767.5 8809.1 9421.62 10035.43 tLLDA 838.26 965.94 1165.07 1297.19 1400.46 1473.13 Kids and Teens’ dataset perplexity values for L-LDA, hlLDA and tLLDA \(rows\nd 1000-6000 record data windows \(columns Since sLDA-type approaches associate supervisory labels with distributions over language models rather than with single language model per label as in the case of hlLDA and L-LDA, it is difficult to meaningfully compare language models of tLLDA with those of sLDA and HSLDA using the perplexity quantifier.  Therefore, in addition to perplexity evaluation we compared performance of tLLDA to that of 138 


other related models by conducting multilabel classification which is a task of predicting the set of labels appropriate for each document given a training set of documents with mu In this study, the classification algorithm remained the same for tLLDA, hlLDA, L-LDA, sLDA 1 and HSLDA 2 with the only variation being the posterior probability approximation procedure, which was specific to each algorithm.  Since the goal of the classification task is to predict the set of labels \(a path\or each test document document-topic approximation routine used during testing must differ from that which was used during model training for topic modeling approaches that take supervision into account.  Therefore, we employed the inference procedure described in ti m a te docum e n t topic dis t ribu ti on  parameters.   There, global parameters that were estimated during training were used to initialize the state of the unsupervised LDA sampling procedure, which was then used to estimate document-level distributions Recalling that our ultimate goal was to help accurately classify web documents and associate them with correct hierarchical paths, we measured the results in terms of example-based classification accuracy which is defined as   where is the indicator function is the number of test documents and and are sets of predicted and actual labels for the i th test document respectively [1  Figures 3 and 4 show accuracy results for tLLDA, hlLDA, LLDA, sLDA and HSLDA for each of the test data windows in the ‘World’ and ‘Kids and Teens’ datasets respectively  Fig. 3. Accuracy results for each test data window in the ‘World’ dataset    1  Implementation available at https://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/chongw/slda  2 Implementation in Python graciously provided by authors of HSLDA in personal communication   Fig. 4  Accuracy results for each test data window in the ‘Kids and Teens dataset C  Topic Visualization Recalling that the goal of this study was to produce a view of the content which would enable individuals to quickly grasp the underlying theme of documents associated with each ontology node and realizing that no clear means existed for quantifying the quality of topic multinomial w e ev a l u a t e d the topics discovered by our model by examining the top words assigned to each topic.  We observed that the top word assignments produced by the tLLDA model \(Table 4 appeared semantically more meaningful as compared to those produced by hlLDA and L-LDA, as the language models produced by the later algorithms seemed to favor proper nouns for language models associated with lower-level hierarchy nodes and marginalize other terms that added semantic consistency to evaluation TABLE IV  TOPIC  VISUALIZATION Sample topic visualizations for top performing evaluated algorithms rows\d several hierarchy levels \(columns\hlighted terms indicate words that appeared semantically indicative of the content theme during qualitative evaluation by the authors VI  D ISCUSSION  Similar to L-LDA, one of the advantages of tLLDA is the document-specific topic mixture In the context of Web documents, the topic mixture can be inferred for each Web page and since each topic is associated with a node in the target ontology, insight as to the subject matter of each document can be gained by evaluating the topic proportions Such insight may be instrumental in helping organizations such as the Open Directory project in their important tasks of digesting the Web content and presenting it to the public in an accessible way 0 0.1 0.2 0.3 0.4 1000 2000 3000 4000 5000 6000 Accuracy Number of records sLDA HSLDA LLDA hLLDA tLLDA 0 0.1 0.2 0.3 0.4 0.5 0.6 1000 2000 3000 4000 5000 6000 Accuracy Number of records sLDA HSLDA LLDA hLLDA tLLDA Composers Classical Beethoven,_Ludwig_v an tLLDA and with includes for a  his samples biography  of composer the on work classical  grieg edvard in a information  biography beethoven van ludwig and brief  works of a includes list  L-LDA biography brief composer the of with for key mozart amadeus life wolfgang work strings mozarts van beethoven  ludwig includes compositions 139 


As node-specific word-multinomials are learned by the tLLDA approach from the underlying collections of documents, it may be possible to consider words in terms of their probability vis-à-vis the corresponding node-level language model and revisit the structure of the target hierarchy.  As target ontologies, such as the one used by DMOZ, are manually compiled, they may contain omissions or overgeneralizations.  Such omissions and overgeneralizations may become prominent when viewed through the prism of their learned language models To exemplify, consider the sample cited in Table 4.  There the set of terms  biography beethoven van ludwig and brief works of a includes list are top most probable for the hierarchy node with the path ‘Beethoven,_Ludwig_van’ as per the node-specific language model learned by the tLLDA approach.  Examination of this list of most probable terms may suggest that the underlying collection of documents related to the famous composer may be further partitioned into two more specific subcategories – one containing documents related to the biography of the composer and the other related to his works VII  C ONCLUSIONS AND F UTURE W ORK  In this paper, we proposed a new topic modeling approach of Web summaries using a popular Web ontology.  This approach took advantage of the hierarchical structure of the ontology to improve predictive power of resulting topic models.  While we focused on the Web summary data provided by the Open Directory project, the topic modeling approach introduced here can be easily adopted to any hierarchically organized content.  This type of model can be useful in identifying key notions in large collections of text In this work, we took advantage of tree-like hierarchical structures exhibited by the Open Directory ontology.  In addition to the hierarchical relationships, however, the ontology also provides lateral links between related nodes that often breach hierarchical boundaries of parent-child relationships.  Understanding how to take advantage of these links to further improve predictive power of topic models is one area of future research Since much of our evaluation was hindered by the computational complexity of parameter estimation algorithms in future works we will attempt to leverage hierarchical structures to reduce time required to estimate parameters for the proposed model for larger datasets by distributing the parameter estimation process.  We will attempt to build on earlier works on distributed topi c modeling algorithms \(e.g  to im prov e perf orm a n ce of th e t L L D A para m e ter estimation procedure R EFERENCES  1  D M O Z A v ail a bl e  http   w w w dm o z  o r g   2 D B l ei  a nd J  Mc A u li ffe Su p e rvi s ed  t opi c m o d e ls    Advances in Neural Information Processing Systems 21 2007 3 D   R a m a g e et al Labeled LDA}: A supervised topic model for credit attribution in multi-labeled corpora," in Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing 2009, pp. 248-256 4 D   B l e i et al Latent Dirichlet Allocation Journal of Machine Learning Research vol. 3, pp. 993-1022, 2003 5 F  W   A d le r J Pe rot t e  Noe m i e E l h a da d  N i c h ola s B a rt le t t   Hierarchically Supervised Latent Dirichlet Allocation NIPS'11 pp. 2609-2617, 2011 6 D   B l e i et al Hierarchical topic models and the nested Chinese restaurant process," in Neural Information Processing Systems\(NIPS ed, 2003 7 T   W e ning e r et al Document-topic hierarchies from document graphs," presented at the Proceedings of the 21st ACM international conference on Information and knowledge management, Maui, Hawaii, USA, 2012 8 Y   P e ti no t et al A hierarchical model of web summaries presented at the Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, Portland, Oregon, 2011 9 C   L u et al The topic-perspective model for social tagging systems," presented at the Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining Washington, DC, USA, 2010 10  D  RD F  A v ail abl e  http   r d f  dm o z o r g r df      T s ou m a k a s et al Mining Multi-label Data," in Data Mining and Knowledge Discovery Handbook O. Maimon and L. Rokach Eds., ed: Springer US, 2010, pp. 667-685   Ne w m a n et al Distributed Algorithms for Topic Models J Mach. Learn. Res vol. 10, pp. 1801-1828, 2009    140 


 T  Ho The random subspace method for constructing decision forests IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 20 no 8 pp 832…844 1998  J Rodriguez L K unche v a  and C Alonso Rotation forest A new classi“er ensemble method Pattern Analysis and Machine Intelligence IEEE Transactions on  vol 28 no 10 pp 1619 1630 oct 2006  I T  Jollif fe Principal Component Analysis  2nd ed Springer 2002  M Hall Correlation-based feature selection for machine learning Ph.D dissertation Citeseer 1999  M Hall E Frank G Holmes B Pf ahringer  P  Reutemann and I H Witten The weka data mining software An update SIGKDD Explorations  vol 11 no 1 2009  Url Functional status npcrc http://www npcrc.or g resources/resources  show.htm?doc id=376169 accessed may 22 2013  Url Intensi v e-care unit wikipedia https://en.wikipedia.or g wiki/Intensive-care unit accessed may 22 2013  Url Lung allocation score wikipedia http://en.wikipedia org/wiki/Lung allocation score accessed may 22 2013  Url What to e xpect before a heart transplant national heart lung and blood institute http://www.nhlbi.nih.gov/health//dci Diseases/ht/ht before.html accessed may 15 2013  Url Intubation wikipedia http://en.wikipedia.or g/wiki Intubation accessed may 15 2013  Url Glomerular ltration rate medlineplus http://www nlm nih.gov/medlineplus/ency/article/007305.htm accessed may 15 2013  Url Extracorporeal membrane oxygenation wikipedia http://en.wikipedia.org/wiki/Extracorporeal membrane oxygenation accessed may 15 2013 8 


overhead of job initialization in Hadoop is much larger than cNeural VIII C ONCLUSION AND F UTURE W ORK The past several years have witnessed an ever-increasing growth speed of data To address large scale neural network training problems in this paper we proposed a customized parallel computing platform called cNeural Different from many previous studies cNeural is designed and built on perspective of the whole architecture from the distributed storage system at the bottom level to the parallel computing framework and algorithm on the top level Experimental results show that cNeural is able to train neural networks over millions of samples and around 50 times faster than Hadoop with dozens of machines In the future we plan to develop and add more neural network algorithms such as deep belief networks into cNeural in order to make further support training large scale neural networks for various problems Finally with more technical work such as GUI done we would like to make it as a toolbox and open source it A CKNOWLEDGMENT This work is funded in part by China NSF Grants No 61223003 the National High Technology Research and Development Program of China 863 No 2011AA01A202 and the USA Intel Labs University Research Program R EFERENCES  C Bishop Neural networks for pattern recognition  Clarendon press Oxford 1995  J Collins Sailing on an ocean of 0s and 1s  Science  vol 327 no 5972 pp 1455…1456 2010  S Haykin Neural networks and learning machines  Englewood Cliffs NJ Prentice Hall 2009  R Hecht-Nielsen Theory of the backpropagation neural network in Proc Int Joint Conf on Neural Networks,IJCNN IEEE 1989 pp 593…605  Y  Loukas  Arti“cial neural netw orks in liquid chromatography Ef“cient and improved quantitative structure-retention relationship models Journal of Chromatography A  vol 904 pp 119…129 2000  N Serbedzija Simulating arti“cial neural netw orks on parallel architectures Computer  vol 29 no 3 pp 56…63 1996  M Pethick M Liddle P  W erstein and Z Huang P arallelization of a backpropagation neural network on a cluster computer in Proc Int Conf on parallel and distributed computing and systems PDCS  2003  K Ganeshamoorthy and D Ranasinghe On the performance of parallel neural network implementations on distributed memory architectures in Proc Int Symp on Cluster Computing and the Grid CCGRID  IEEE 2008 pp 90…97  S Suresh S Omkar  and V  Mani P arallel implementation of back-propagation algorithm in networks of workstations IEEE Trans Parallel and Distributed Systems  vol 16 no 1 pp 24…34 2005  Z Liu H Li and G Miao Mapreduce-based backpropagation neural network over large scale mobile data in Proc Int Conf on Natural Computation ICNC  vol 4 IEEE 2010 pp 1726…1730  M Glesner and W  P  ochm  uller Neurocomputers an overview of neural networks in VLSI  CRC Press 1994  Y  Bo and W  Xun Research on the performance of grid computing for distributed neural networks International Journal of Computer Science and Netwrok Security  vol 6 no 4 pp 179…187 2006  C Chu S Kim Y  Lin Y  Y u  G  Bradski A Ng and K Olukotun Map-reduce for machine learning on multicore Advances in neural information processing systems  vol 19 pp 281…288 2007  U Seif fert  Arti“cial neural netw orks on massi v ely parallel computer hardware Neurocomputing  vol 57 pp 135…150 2004  D Calv ert and J Guan Distrib uted arti“cial neural netw ork architectures in Proc Int Symp on High Performance Computing Systems and Applications  IEEE 2005 pp 2…10  H Kharbanda and R Campbell F ast neural netw ork training on general purpose computers in Proc Int Conf on High Performance Computing HiPC  IEEE 2011  U Lotri  c and e a Dobnikar A Parallel implementations of feed-forward neural network using mpi and c on  net platform in Proc Int Conf on Adaptive and Natural Computing Algorithms  Coimbra 2005 pp 534…537  Q V  Le R Monga and M e a De vin Building high-le v e l features using large scale unsupervised learning in Proc Int Conf on Machine Learning ICML  ACM 2012 pp 2…16  J Ekanayak e and H e a Li T wister a runtime for iterati v e mapreduce in Proc of the 19th ACM International Symposium on High Performance Distributed Computing  ACM 2010 pp 810…818  Y  Bu B Ho we M Balazinska and M D Ernst Haloop Ef“cient iterative data processing on large clusters Proc of the VLDB Endowment  vol 3 no 1-2 pp 285…296 2010  M Zaharia M Cho wdhury  T  Das A Da v e  J  Ma M McCauley M Franklin S Shenker and I Stoica Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing in Proc USENIX Conf on Networked Systems Design and Implementation  USENIX Association 2012 pp 2…16 384 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


