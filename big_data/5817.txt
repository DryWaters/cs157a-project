Learning from Multiple Data Sets with Different Missing Attributes and Privacy Policies: Parallel Distribute d Fuzzy Genetics-Based Ma chine Learning Approach  Hisao Ishibuchi, Masakazu Yamane, and Yusuke Nojima Department of Computer Science and Intelligent Systems Graduate School of Engineering, Osaka Prefecture University Sakai, Osaka 599-8531, Japan hisaoi@, masakazu.yamane@ci., nojima@}cs.osakafu-u.ac.jp   Abstract This paper discusses parallel distributed geneticsbased machine learning \(GBML\ fuzzy rule-based classifiers from multiple data sets. We assume that each data set has a similar but different set of attributes. In other words, each data set has different missing attributes. Our task is the design of a fuzzy rule-based classifier from those data sets. In this paper we first show that fuzzy rules can handle missing attributes easily. Next we explain how parallel distributed fuzzy GBML can handle multiple data sets with different missing attributes Then we examine the accuracy of obtained fuzzy rule-based classifiers from various settings of available training data such as a single data set with no missing attribute and multiple data sets with many missing attributes. Experimental results show that the use of multiple data sets often increases the accuracy of obtained fuzzy rule-based classifiers even when they have missing attributes. We also discuss the learning from a data set under a severe privacy preserving policy where only the error rate of each candidate classifier is available. It is assumed that no information about each individual pattern is available. This means that we cannot use any information on the class label or the attribute values of each pattern. We explain how such a black-box data set can be utilized for classifier design Keywords-Evolutionary algorithms, genetics-based machine learning, parallel distributed implementation, fuzzy rule-based classifiers, horizontally partitioned data sets I   I NTRODUCTION  In knowledge extraction and data mining, useful data are often collected and stored in different organizations around the world \(e.g., hospitals in different countries\. As a result data can be viewed as a mixture of data sets, each of which is from a different site. In this case, data sets are heterogeneous due to various reasons such as different testing equipments in each organization and different regulations in each country The availability of each data set is also often totally different in each site due to different security and privacy policies In this paper, we discuss the learning from multiple data sets under the following two scenarios. In one scenario, it is assumed that each data set has different missing attributes. In the other scenario, some data sets are assumed to have a very severe privacy policy as well as different missing attributes The first scenario, in which each data set has different missing attributes, is illustrated in Fig. 1 using three data sets D A  D B and D C Missing attributes in each data set in Fig. 1 are x 1  x 4 in D A  x 4  x 6 in D B and x 1  x 3  x 5 in D C  among the seven attributes x 1  x 2  x 6  y where y is a special attribute for a class label. We assume that the class label attribute is not missing in any data set. In privacy preserving data mining [1  4  m u ltip le d ata sets o f  th e ty p e  in Fig. 1 are called “horizontally partitioned data” [5  wher e the existence of missing attributes is not usually assumed explicitly. The main feature of our multiple data sets is that each data set has different missing attributes. Our multiple data sets are different from “vertically partitioned data” [5 in privacy preserving data mining where attributes are partitioned into several attribute subsets   ID x 1 x 2 x 3 x 4 x 5 x 6 y C 1 9180 C 2 6171 C 3 5271 Site C Data Set D C ID x 1 x 2 x 3 x 4 x 5 x 6 y A 1 92 291 A 2 81 361 A 3 95 150 Site A Data Set D A ID x 1 x 2 x 3 x 4 x 5 x 6 y B 1 18110 B 2 18230 B 3 39211 Site B Data Set D B  Figure 1  Multiple data sets with different missing attributes \(Scenario 1 In the second scenario, we assume that some sites with data sets have a very severe privacy preserving policy. More specifically, we assume that only error rates of candidate classifiers can be obtained from each of those sites. No other information is available. This scenario is illustrated in Fig. 2 where Sites A and C are assumed to have such a severe privacy preserving policy. Whereas the data set D B is fully available to the classifier designer, the other data sets D A and D C are black-box data sets for the classifier designer. The classifier designer cannot see the contents of D A and D C We assume that these two black-box data sets can be used only for the calculation of error rates of candidate classifiers  ID x 1 x 2 x 3 x 4 x 5 x 6 y C 1 9180 C 2 6171 C 3 5271 Site C Data Set D C ID x 1 x 2 x 3 x 4 x 5 x 6 y A 1 92 291 A 2 81 361 A 3 95 150 Site A Data Set D A ID x 1 x 2 x 3 x 4 x 5 x 6 y B 1 18110 B 2 18230 B 3 39211 Site B Data Set D B Full access Classifier Designer Candidate classifier Candidate classifier Error rate Error rate  Figure 2  Learning under a severe privacy preserving policy \(Scenario 2 63 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 


Applications of evolutionary computation to knowledge extraction and data mining are called genetics-based machine learning \(G  1 0  A rule  set i s  o f ten han d le d as an  individual and coded as a string. Recently Franco et al  demonstrated the high performance of GBML in comparison with other classifier design approaches such as decision trees and support vector machines. GBML has also been actively applied to the design of fuzzy rule-based classifiers under the name of fuzzy GBML. Fuzzy GBML is called genetic fuzzy systems \(GFS\fuzzy system community [11  14     The main advantage of GBML is its flexibility in the handling of various goals in classifier design. Complexity measures such as the number of rules and the number of conditions in each rule as well as accuracy measures such as false positive and false negative can be integrated into a single fitness function. They can also be handled as separate objectives in multiobjective GBML algorithms to search for a number of non-dominated tradeoff classifiers [15 16 Classifiers with different tradeoffs can be obtained by a single run of a multiobjective GBML algorithm \(e.g., simple classifiers with a few rules and complicated classifiers with high accuracy\. In GBML, it is also easy to use constraint conditions such as the upper bound on the number of rules Whereas GBML is a flexible classifier design approach its real-world applications are not always easy due to its heavy computation load. Evolutionary search for an accurate classifier in a complicated large-scale real-world task often needs thousands of generations. In each generation, the creation and the fitness evaluation of hundreds of candidate classifiers are performed As an efficient speed-up trick of GBML algorithms, we proposed their parallel distributed implementatio   A population of classifiers is divided into multiple subpopulations as in an island model of parallel evolutionary computation 20 Trai ning data are al so  di vided i n to multiple subsets. A single training data subset is assigned to each sub-population. To avoid the overfitting of classifiers in each sub-population to the assigned training data subset, the assignment of the training data subsets is rotated periodically over the sub-populations. We showed in our former study 18 t h a t the c o m p u t a t i o n  ti m e o f  our f u zzy  GBML a l gori t h m  21  was decre a s e d by  its p a r all e l di s t r i bute d  i m ple m en ta tion  to about 2% of that of its standard implementation. This is because both the training data and the population were divided into seven subsets \(i.e., \(1/7 2 1/49 2 In this paper, we show that our fuzzy GBML algorithm 18 21  ca n handle t h e a b ovem e n t io n e d t w o scenari o s  data sets with missing attributes and black-box data sets with the severe privacy preserving policy. After explaining how these two scenarios can be handled, we examine the effect of using multiple data sets in each scenario on the accuracy of obtained fuzzy rule-based classifiers. Experimental results obtained from the use of multiple data sets are compared with those from a single data set \(e.g D B in Fig. 2 This paper is organized as follows. In Section II, we first explain fuzzy rules and fuzzy rule-based classifiers. The handling of data sets with missing attributes in Fig. 1 is also discussed in Section II. Then we explain our fuzzy GBML algorithm in Section III where we also show its variant for black-box data sets with the severe privacy preserving policy in Fig. 2. In Section IV, we examine the accuracy of fuzzy rule-based classifiers designed from multiple data sets in each of the two scenarios through computational experiments Finally we conclude this paper in Section V II  F UZZY R ULE B ASED C LASSIFIERS  A  Fuzzy Rules for Classification Problems A standard classification problem is defined by a set of labeled patterns in a pattern space. Let us assume that we have m training patterns x p  x p 1  x p 2  x pn  p  1, 2 m  with n continuous attributes from M classes where p is a pattern index and x pi is an attribute value of the i th attribute x i in the p th pattern x p Each pattern is assumed to have a class label, which is one of the M classes. For ease of explanation, we further assume that each attribute value x pi  has already been normalized into a real number in the unit inte This m e ans t h a t the p a tter n space o f ou r  classification problem is an n dimensional hypercub n  The basic form of fuzzy rules for our n dimensional classification problem is as follows Rule R q If x 1 is A q 1 and ... and x n is A qn then Class C q 1 where R q is a rule label of the q th fuzzy rule A qi is an antecedent fuzzy set of R q on the i th attribute x i and C q is a consequent class of R q  Antecedent fuzzy sets often have linguistic labels such as  small  medium and large In Fig. 3, we show typical examples of antecedent fuzzy sets with linguistic labels. The meaning of each antecedent fuzzy set A qi is mathematically specified by its membership function A qi  For example the membership functions of the three fuzzy sets in Fig. 3 are written for 0 x 1 as follows Small  x max 1 2 x 0},                                   \(2 Medium  x min{2 x 2 2 x 3 Large  x max{2 x 1, 0}.                                 \(4 When antecedent fuzzy sets have linguistic labels, fuzzy rules can be interpreted as linguistic rules such as “If x 1 is small and x 2 is large then Class 1” and “If x 1 is large and x 2  is small then Class 2”. These rules are easy for human users to understand. Linguistic interpretability of fuzzy rules is one advantage of fuzzy rule-based classifiers  Attribute Value Membership 0.0 1.0 0.0 1.0 small medium large    Figure 3  Typical antecedent fuzzy sets with linguistic labels 64 


Fuzzy rules of the form in \(1\ have high interpretability However, their classification ability is not always so high. In this paper, we use the following type of fuzzy rules with a rule weight CF q to enhance their classification ability Rule R q If x 1 is A q 1 and ... and x n is A qn  then Class C q with CF q 5 The rule weight CF q of R q which is a real number in the unit interval [0 1 w o r k s a s the s t r e ngth o f  R q in fuzzy reasoning as explained in the next subsection [22 23 B  Fuzzy Reasoning in Fuzzy Rule-Based Classifiers A fuzzy rule-based classifier is a set of fuzzy rules of the form in \(5\. Let S be a fuzzy rule-based classifier. When an input pattern x p  x p 1  x p 2  x pn is presented to the fuzzy rule-based classifier S first the compatibility of x p with the antecedent part of each fuzzy rule R q in S is calculated from the membership function A qi  each antecedent fuzzy set A qi using the product operation as follows A q  x p  A q 1  x p 1  A q 2  x p 2  A qn  x pn 6 where A q  A q 1  A q 2  A qn hows the antecedent part of R q   The input pattern x p is classified by a single winner rule. The winner rule R W is defined for the input pattern x p as A W  x p  CF W max A q  x p  CF q  R q S 7 The input pattern x p is classified as the consequent class C W of the winner rule R W If multiple fuzzy rules have the same maximum value in \(7\ and their consequent classes are different, the classification of the input pattern x p is rejected Its classification is also rejected when no fuzzy rule in S is compatible with x p When we use fuzzy rules of the basic form in \(1\e most compatible rule is chosen as the winner rule in \(7\ As shown in \(7\he rule weight CF q of each fuzzy rule R q works as the strength of R q in the winner rule selection. The single winner-based fuzzy reasoning method in \(7\has been widely used since the early 1990  advantage of this reasoning method is its high explanation ability for the classification decision. The winner rule can be used to explain the reason for the classification decision C  Handling of Missing Attribute Values We can easily handle input patterns with missing values by the single winner-based fuzzy reasoning method. Let us assume that the j th attribute value x pj of the input pattern x p  is missing. In this case, we calculate the compatibility grade A q  x p the input pattern x p with the antecedent part A q of the fuzzy rule R q in \(6\fying A qj  x pj  A qj  x pj 1 The reason for using this heuristic is its simplicity. Since x pj  is missing, we only know that its value is a real number in the normalized domain interval [0 1 By r e pl ac in g  x pj with 0 1 n  A qj  x pj we have A qj 0 1  since the d o m a i n  interval [0 fully  co m p at i b le wit h any ant ece de nt f u zz y  set on [0 1  I t shou l d be not e d t h at t h e speci f ica ti o n  of A qj  x pj  A qj  x pj 1 is the same as removing A qj  x pj rom 6\ is, we can ignore the condition corresponding to the missing attribute value. In this manner, we can easily handle incomplete patterns with missing values This simple mechanism for the handling of missing value has the following potential drawback. Let us assume that a fuzzy rule-based classifier is trained using a data set D A with two missing attributes x 1 and x 4 in Fig. 2. In this case classifier performance is independent of the 1st and 4th conditions A q 1 and A q 4 of each fuzzy rule. That is, any random changes of A q 1 and A q 4 have no effect on the error rate. As a result, it is not likely that each rule in the obtained fuzzy rule-based classifier has appropriate conditions on x 1  and x 4 This issue should be addressed in future research D  Heuristic Fuzzy Rule Generation If the antecedent part of a fuzzy rule has already been specified, its consequent class and rule weight can be easily determined using compatible training patterns with its antecedent part in a simple heuristic manner [22  24   Let us  assume that the antecedent part A q of a fuzzy rule R q has already been specified. In this case, its consequent class C q  and rule weight CF q are determined in the following manner First the compatibility grade of each training pattern x q with the antecedent part A q is calculated by \(6\. Next the confidence value from the antecedent part A q to each class i.e., Class k  k 1, 2 M calculated as follows Conf  A q Class k   m p p k p q p q 1 Class     x x A x A 8 When no class has a confidence value larger than 0.5, we do not generate any fuzzy rule with the antecedent part A q  When the confidence value for a particular class C q is larger than 0.5, we generate a fuzzy rule with the antecedent part A q and the consequent class C q Then its rule weight is specified as   M C k k q q q q q k Conf C Conf CF 1  Class   Class  A A 9 From \(8\, this formulation can be rewritten as follows  1  Class  2 q q q C Conf CF A 10 In this manner, we can determine the consequent class C q  and the rule weight CF q for the antecedent part A q The antecedent part of each fuzzy rule in a fuzzy rule-based classifier is specified by our fuzzy GBML algorithm as we will explain in the next section. It should be noted that our heuristic rule generation method is applicable to data sets with missing attributes. However, it is not applicable to black-box data sets. This is because the confidence value in 8\alculated from black-box data sets III  F UZZY G ENETICS B ASED M ACHINE L EARNING  A  Outline of Our Fuzzy GBML Algorithm Our fuzzy GBML algorithm in this paper is the same as in our former study [1  Our fu zzy GBML a l gor it h m  is u s e d  to design a fuzzy rule-based classifier with fuzzy rules of the 65 


form in \(5\. Each fuzzy rule R q is represented by a string of length n using its n antecedent fuzzy sets A q 1  A q 2  A qn  As antecedent fuzzy sets, we use don’t care i.e., an unit interval [0 and 14 tria ngu lar f u zzy  sets in Fi g  4  T h es e fuzzy sets are denoted by 15 integers from “0” for don’t care  to “14” for the right-most fuzzy set in the bottom-right plot in Fig. 4. Thus a fuzzy rule is represented by an integer string of length n using the alphabet of the 15 integers. As a result the total number of different strings is 15 n Each of those strings shows a different fuzzy rule  1.0 0.0 1.0 Attribute value 34 5 Membership 1.0 Attribute value 12 1.0 0.0 Membership  1.0 Attribute value 1.0 Attribute value 10 12 14 13 11 69 78 1.0 0.0 Membership 1.0 0.0 Membership  Figure 4  Fuzzy partitions with 14 antecedent fuzzy sets A fuzzy rule-based classifier S with S fuzzy rules is represented by a concatenated integer string of length n  S  where each substring of length n shows a single fuzzy rule The number of fuzzy rules is not pre-specified \(i.e S is not a pre-specified constant parameter\. Thus the length of each string can be different. The number of fuzzy rules can be automatically determined for each application task in our fuzzy GBML algorithm A fuzzy rule-based classifier S is evaluated in our fuzzy GBML algorithm by the following fitness function fitness S w 1 f 1  S  w 2 f 2  S  w 3 f 3  S 11 where w 1  w 2 and w 3 are non-negative weights, and f 1  S  f 2  S d f 3  S are as follows f 1  S Training data error rate of S in percentage f 2  S The number of fuzzy rules in S i.e f 2  S  S  f 3  S The total rule length of S  The number of antecedent conditions of a fuzzy rule excluding don’t care conditions is called the rule length. The total rule length is the total number of antecedent conditions in S excluding don’t care conditions \(i.e., the sum of the rule length of fuzzy rules in S The fitness function in \(11 should be minimized. The use of \(11\ to simultaneously perform the accuracy maximization and the complexity minimization. The weight values in \(11\ are specified as w 1  100 w 2 1 and w 3  1 in our computational experiments Our fuzzy GBML algorithm is a hybrid algorithm of Pittsburgh-style GBML and Michigan-style GBML as shown in Fig. 5. In this subsection, we explain the Pittsburgh-style framework. The Michigan-style part is explained later Let us denote the population size by N Pop In the Initialization” step in Fig. 5, an initial population with N Pop  integer strings is generated. The number of substrings in each initial string \(i.e., the number of fuzzy rules in each initial rule set\ is 30 in our computational experiments. To generate an initial string, we randomly choose 30 training patterns. A single fuzzy rule is generated from each training pattern x p  x p 1  x p 2  x pn by probabilistically choosing an antecedent fuzzy set for each attribute x i using its compatibility grade with the attribute value x pi The selection probability of each antecedent fuzzy set is proportional to its compatibility grade with x pi After choosing n antecedent fuzzy sets, each of them is replaced with don’t care using a pre-specified don’t care probability. This probability is specified as n 5 n in our computational experiments. Thus each initial fuzzy rule has five conditions on average In the “Selection” step in Fig. 5, each string in the current population is evaluated by the fitness function in \(11\en N Pop pairs of parents are selected by binary tournament selection with replacement based on their fitness values In the “Genetic Operations” step, a string is generated from each pair of parents by randomly choosing substrings from each parent. The number of selected substrings from each parent is randomly specified. Thus the length of a new string can be different from that of its parents. This crossover operation is applied to each pair of parents with a prespecified probability, which is 0.9 in our computational experiments. When the crossover operation is not applied one of the two parents is randomly selected. A new string is generated as a copy of the selected parent Each integer in the newly generated string is randomly replaced with another integer using a pre-specified mutation probability. In our computational experiments, the mutation probability is specified for the new string S as 1 n  S where n  S is the string length of S In the “Genetic Operations step N Pop strings are newly generated by crossover and mutation. A Michigan-style algorithm, which is explained in the next subsection, is applied to each of the generated new strings with a pre-specified probability. This probability is specified as 0.5 in our computational experiments In the “Population Update” step in the left plot of Fig. 5 the current population and the newly generated N Pop strings are merged. The best N Pop strings in the merged population are selected to form the next population  Initialization Selection Genetic operations Crossover and Mutation Michigan-style part Termination condition Ye s No Population update Choose the best individual Michigan probability New rule generation  Genetic rule generation  Heuristic rule generation Ye s No Michigan-style part Population update Pittsburgh-style framework Rule set  Figure 5  Our hybrid fuzzy GBML algorithm 66 


If the termination condition \(50,000 generations in this paper\ does not hold, the updated population goes back to the Selection” step. Otherwise, the best string in the updated population is chosen as the finally obtained classifier B  Michigan-Style Part of Our Fuzzy GBML Algorithm The Michigan-style part in Fig. 5 can be viewed as a local search procedure for each string. A single string S in the Pittsburgh-style framework is handled as a population of fuzzy rules. Each fuzzy rule R q in the string is an individual in the Michigan-style part. The fitness of R q in S is defined by the number of correctly classified training patterns by R q  when S is used to classify all training patterns. Since we use the single winner-based fuzzy reasoning method, we can identify a single responsible fuzzy rule for the classification of each pattern. The worst 20% of fuzzy rules are removed from S The number of fuzzy rules to be generated is the same as that of the removed rules. A half of new fuzzy rules are directly generated from misclassified training patterns in the same manner as in the initial fuzzy rule generation. The other fuzzy rules are generated from the existing fuzzy rules in S by binary tournament selection, uniform crossover with the probability 0.9, and mutation with the probability 1 n  The modified rule set S is sent back to the Pittsburgh-style part after only a single iteration of the Michigan-style part C  Parallel Distributed Fuzzy GBML Algorithm Parallel distributed implementation in our former study 18 is i l lustrat e d  i n  Fig 6. As in 1 8 a popu l a t i on of siz e  210 is divided into seven sub-populations of size 30 in this paper. Training data are also divided into seven subsets of the same size D 1  D 2  D 7 The seven sub-populations and the seven data subsets are distributed over seven CPUs. Our fuzzy GBML algorithm is locally executed at each CPU for a pre-specified number of generations \(i.e., rotation interval Then the training data subsets are rotated over the CPUs Three specifications of the rotation interval are examined: 10 100 and 1000 generations. A copy of the best rule set in each island is migrated to an adjacent island. The migration is always performed every 100 generations. As shown in Fig. 6 the rotation and the migration are executed in the opposite directions. After the 50,000th generation, 210 rule sets are evaluated using all data sets. The best rule set with respect to the fitness function is chosen as a finally obtained classifier  CPU Training data rotation Training data Population CPU CPU CPU CPU CPU CPU Rule set migration D 2 D 3 D 4 D 5 D 6 D 7 D 1  Figure 6  Parallel distributed implementation [18  D  Modification of Fuzzy GBML for Black-Box Data Sets Let us assume that the classifier designer cannot access any data sets except for a single data set D 1 As we explained in Section I using Fig. 2, we assume that only the error rate of a candidate fuzzy rule-based classifier is available from each of those black-box data sets. In this situation, we cannot use the Michigan-style part since no information about the fitness of each fuzzy rule is available. Thus we remove the Michigan-style part when our fuzzy GBML algorithm is executed on a black-box data set In the Pittsburgh-style framework, we need the full information on training patterns to generate initial fuzzy rules. Since D 1 is fully available, initial fuzzy rules are always generated from D 1 When the antecedent part of a fuzzy rule is changed by mutation, the information on training patterns is needed to determine the consequent class and the rule weight. In this case, we use the available data set D 1 That is D 1 is always used to generate new fuzzy rules even when fuzzy rule-based classifiers are evaluated on another data set. In this manner, we utilize a fully available data set and other black-box data sets IV  C OMPUTATIONAL E XPERIMENTS  A  Settings of Computational Experiments The main objective of our computational experiments is to examine the potential usefulness of our parallel distributed implementation in the handling of the two scenarios: One is multiple data sets with different missing attributes in Fig. 1 and the other is multiple black-box data sets in Fig. 2. We use four data sets in Table I in the KEEL database Fo r each data set, we artificially generate multiple training data subsets with different missing attributes within the ten-fold cross-validation \(10CV\ framework. In 10CV, given patterns are divided into ten subsets: One is used as test data and the others are used as training data TABLE I  F OUR D ATA S ETS USED IN T HIS P APER    Data Set Patterns Attributes Classes Phoneme 5,404 5 2 Page-blocks 5,472 10 5 Segment 2,310 19 7 Satimage 6,435 36 6  The training data are further divided into seven subsets of the same size D 1  D 2  D 7 We remove some attributes from these subsets in order to generate multiple data sets with different missing attributes in the first scenario. In the second scenario, we assume that only the first training data subset D 1 is fully available. Due to the severe privacy preserving policy, only error rates of candidate classifiers on each of the other data subsets D i  i  2, 3, ..., 7\are available Our two scenarios can be rewritten in our computational experiments as follows Scenario 1 The classifier designer can fully access all the seven training data subsets D i  i  1, 2, ..., 7 Scenario 2 The classifier designer can fully access D 1  Due to the severe privacy preserving policy, only error rates of candidate classifiers on D i  i  2, 3, ..., 7\are available 67 


We artificially generate multiple data sets with different missing attributes in the following two manners Setting 1 We remove only a single attribute from each training data subset D i except for D 1 More specifically, we remove the ith attribute x i from D i  i  2, 3, ..., 7 x i is missing in D i  i  2, 3, ..., 7\attribute is missing in D 1 In the case of the phoneme data with five attributes x 1  x 2  x 3  x 4  x 5  x i is removed from D i  i  2, 3, 4, 5 x 1 and x 2 are removed from D 6 and D 7 respectively Setting 2 Let n  be the number of attributes to be removed from D i  i  2, 3, ..., 7\ata set in Table I n  is specified as the smallest integer not smaller than n 6 so that every attribute is removed from at least one training data subset D i For example n  is specified as n  4 for the segment data with 19 attributes since n 6 19/6 3.17. We randomly choose n  attributes to be removed from each D i   i  2, 3, ..., 7\der the following conditions: \(i attribute should be removed from at least one subset, and \(ii no attribute should be removed from more than two subsets As a result, a different subset of n  attributes becomes missing in D i  i  2, 3, ..., 7\ while no attribute is missing in D 1 The random selection of removed n  attributes is updated when the entire 10CV procedure is completed. In this paper we iterate 10CV five times using different data partitions into ten subsets \(i.e., 5 10CV\hus the selection of removed attributes is updated five times These two settings are examined under the two scenarios Since D 1 with no missing attributes is always available, it looks a good idea to design a fuzzy rule-based classifier from D 1 without using any other training data subsets. This idea is illustrated in Fig. 7 where D 1 is used in all sub-populations In this paper, Fig. 6 with the seven data subsets and Fig. 7 with only D 1 are compared. The same parallel distributed algorithm is used in Fig. 6 and Fig. 7. In order to choose the final fuzzy rule-based classifier under the same condition in Fig. 6 and Fig. 7, only D 1 is used for classifier selection. That is, 210 individuals at the final population are evaluated using D 1 Then, the best individual is selected as the final classifier   CPU Training data rotation Training data Population CPU CPU CPU CPU CPU CPU Rule set migration D 1 D 1 D 1 D 1 D 1 D 1 D 1  Figure 7  Parallel distributed implementation with seven duplicates of D 1  Our computational experiments are performed in the same parameter specifications in our former study  suc h  as the use of seven sub-populations of size 30 \(i.e., 210 rule sets in each generation in total\he termination at the 50,000th generation. The training data rotation interval is specified as 10, 100 and 1000 generations while the migration interval is specified as 100 generations. We also examine an additional specification: no training data rotation and no rule set migration. Average accuracy of the finally obtained fuzzy rule-based classifiers on test data is evaluated over five iterations of 10CV \(i.e., 5 10CV\. We do not remove any attributes from the test data. That is, each test pattern is classified using its all attribute values in 10CV. In the following, we report the average accuracy on test data B  Experimental Results from Setting 1 In the first setting D 1 has no missing attribute while each of the other six subsets D i  i  2, 3, ..., 7\ has a single missing attribute. When a data set has many attributes such as the satimage data with 36 attributes, negative effects of a single missing attribute seem to be small. Thus it may be a good idea to use not only D 1 with no missing attribute but also the other six data sets with a single missing attribute. However when a data set does not have many attributes such as the phoneme data with five attributes, negative effects of a single missing attribute may be large. Thus the benefit of using all the seven training data subsets may be small or negative To numerically examine these discussions, we compare the following three cases: the use of D 1 as in Fig. 7, the use of all the seven subsets D i  i  1, 2, ..., 7\n Fig. 6 in Scenario 1, and the use of all the seven subsets in Scenario 2 i.e., only D 1 is fully available\. Experimental results on the four data sets in Table I are summarized in Figs. 8-11. When we use only D 1 the same average accuracy is obtained from different specifications of the rotation interval. However, a slightly different average accuracy is obtained from “None with no rotation and no migration In Fig. 8 on the phoneme data with five attributes, we can observe a clear negative effect of using all the seven training data subsets \(black and red open circles\n comparison with the use of only D 1 blue closed circles\ as expected. In Figs 9-11, better results are obtained by the use of all the seven training data subsets \(black and red open circles\ than the use of only D 1 blue closed circles\many cases Regarding the comparison between the two scenarios better results are obtained in many cases from Scenario 1 with full access to all data subsets \(e.g., Fig. 10 and Fig. 11 However, better results are obtained from Scenario 2 in some cases than Scenario 1 \(e.g., rotation interval 1000 in Fig. 9  100 1000 10 Rotation interval Test Data Accuracy 80 76 78 82 None Seven Subsets \(Scenario 1 Use of only D 1 Seven Subsets \(Scenario 2   Figure 8  Results on the phoneme data with five attributes \(Setting 1 68 


100 1000 None 10 Test Data Accuracy Rotation interval 95 93 96 94 Seven Subsets \(Scenario 1 Use of only D 1 Seven Subsets \(Scenario 2   Figure 9  Results on the Page-blocks data with ten attributes \(Setting 1 100 1000 10 Test Data Accuracy 92 88 86 90 94 Rotation interval Seven Subsets \(Scenario 1 Use of only D 1 Seven Subsets \(Scenario 2 None  Figure 10  Results on the Segment data with 19 attributes \(Setting 1 100 1000 10 Test Data Accuracy 86 82 80 84 88 Rotation interval None Seven Subsets \(Scenario 1 Use of only D 1 Seven Subsets \(Scenario 2  Figure 11  Results on the Satimage data with 36 attributes \(Setting 1 C  Experimental Results from Setting 2 In the second setting D 1 has no missing attribute while about n 6 attributes are missing in the other six data subsets Similar results are obtained from the first and second settings for the phoneme data with five attributes since the number of missing attributes is one in both settings. Figs. 12-14 show experimental results on the other data sets in Setting 2 Since much more attributes are missing in Figs. 12-14 i.e., two in Fig. 12, four in Fig. 13, and six in Fig. 14\han Figs. 9-11 with a single missing attribute, positive effects of using all the seven data subsets decrease from Figs. 9-11 to Figs. 12-14. However, better results are still obtained by the use of all the seven data subsets \(black and red open circles than the use of D 1 blue closed circles\gs. 12-14 100 1000 10 Test Data Accuracy Rotation interval 95 93 96 None 94 Seven Subsets \(Scenario 1 Use of only D 1 Seven Subsets \(Scenario 2  Figure 12  Results on the Page-blocks data with ten attributes \(Setting 2 100 1000 10 Test Data Accuracy 92 88 86 90 94 Rotation interval None Seven Subsets \(Scenario 1 Use of only D 1 Seven Subsets \(Scenario 2  Figure 13  Results on the Segment data with 19 attributes \(Setting 2 100 1000 10 Test Data Accuracy 86 82 80 84 88 Rotation interval None Seven Subsets \(Scenario 1 Use of only D 1 Seven Subsets \(Scenario 2  Figure 14  Results on the Satimage data with 36 attributes \(Setting 2 One interesting observation in Figs. 12-14 is that better results are obtained in many cases from Scenario 2 than Scenario 1. This may be because new fuzzy rules are always generated from D 1 with no missing attributes in Scenario 2 In Scenario 1, new fuzzy rules are generated from D 1 with no missing attributes and also from the other six data subsets D i   i  2, 3, ..., 7\th missing attributes V  C ONCLUSION  In this paper, we examined the potential usefulness of our parallel distributed fuzzy GBML algorithm for fuzzy rulebased classifier design from multiple data sets with different missing attributes. We also examined its potential usefulness under the assumption of the severe privacy preserving policy where data sets were handled as black-box data sets. More 69 


specifically, we assumed that a black-box data set was used only for calculating the error rate of each classifier. No other information \(e.g., attribute values of each pattern, its true class, and its classification result\ were available In our computational experiments, we divided training patterns into seven data subsets: one complete data subset with no missing attributes and six incomplete data subsets with different missing attributes. We observed that classifier performance was improved by the use of all the seven data subsets in comparison with the use of only the complete data subset. This improvement was also observed even when the six incomplete data subsets were black-box data sets It was always assumed in our computational experiments that one data subset was fully available. It is an interesting future research issue to examine the learning from multiple data subsets where no data subsets are fully available \(i.e., all data subsets have different missing attributes and the severe private preserving policy\ch a difficult situation, more sophisticated handling of missing values may be needed since all data subsets have missing attributes. More efficient evolutionary learning may be also needed since we cannot generate fuzzy rules directly from training patterns R EFERENCES  1  R. Agrawal and R. Srikant, “Privacy-preserving data mining Proc of the 2000 ACM SIGMOD International Conference on Management of Data pp. 439-450, Dallas, May 15-18, 2000. DOI: 10.1145 342009.335438 2  Y. Lindell and B. Pinkas, “Priva cy preserving data mining Proc. of the 20th Annual International Cryptology Conference \(LNCS 1880 Advances in Cryptology - CRYPTO 2000 pp. 36-54, Santa Barbara August 20-24, 2000. DOI: 10.1007/3-540-44598-6_3 3  Y. Lindell and B. Pinkas, “Privacy preserving data mining Journal of Cryptology vol. 15, no. 3, pp. 177-206, June 2002. DOI 10.1007 s00145-001-0019-2 4  V. S. Verykios, E. Bertino, I. N. Fovin, L. P. Provenza, Y. Saygin and Y. Theodoridis, “State-of-the-art in privacy preserving data mining Sigmod Record vol. 33, no. 1, pp. 50-57, March 2004. DOI 10.1145/974121.974131 5  M. Kantarcioglu and C. Clifton, “Privacy-preserving distributed mining of association rules on horizontally partitioned data IEEE Trans. on Knowledge and Data Engineering vol. 16, no.9, pp. 10261037, September 2004. DOI: 10.1109/TKDE.2004.45 6  J. Vaidya and C. Clifton, “Privacy preserving association rule mining in vertically partitioned data Proc. of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pp. 639-644, Edmonton, July 23-25, 2002. DOI: 10.1145 775047.775142 7  A. Fernández, S. García, J. Luengo, E. Bernadó-Mansilla, and F Herrera, “Genetics-based machine learning for rule induction: State of the art, taxonomy, and comparative study IEEE Trans. on Evolutionary Computation vol. 14, no. 6, pp. 913-941, December 2010. DOI: 10.1109/TEVC.2009.2039140 8  S. García, A. Fernández, J. Luengo, and F. Herrera, “A study of statistical techniques and performance measures for genetics-based machine learning: Accuracy and interpretability Soft Computing  vol. 13, no. 10, pp. 959-977, August 2009 DOI: 10.1007/s00500008-0392-y  9  J. Bacardit and N. Krasnogor, “Performance and efficiency of memetic Pittsburgh learning classifier systems Evolutionary Computation vol. 17, no. 3, pp. 307-342, Fall 2009. DOI: 10.1162 evco.2009.17.3.307 10  M. A. Franco, N. Krasnogor and J. Bacardit, “GAssist vs. BioHEL critical assessment of two paradigms of genetics-based machine learning,” Soft Computing, vol. 17, no. 6, pp. 953-981, June 2013 DOI: 10.1007/s00500-013-1016-8 11  F. J. Berlanga, A. J. Rivera, M J. del Jesus, and F. Herrera, “GPCOACH: Genetic Programming-based learning of COmpact and ACcurate fuzzy rule-based classification systems for Highdimensional problems Information Sciences vol. 180, no. 8, pp 1183-1200, April 2010. DOI:10.1016/j.ins.2009.12.020 12  J. Alcalá-Fdez, R. Alcalá, and F. Herrera, “A fuzzy association rulebased classification model for high-dimensional problems with genetic rule selection and lateral tuning IEEE Trans. on Fuzzy Systems vol. 19, no. 5, pp. 857-872, October 2011. DOI: 10.1109 TFUZZ.2011.2147794 13  J. A. Sanz, A. Fernández, H. Bustince, and F. Herrera, “IVTURS: a linguistic fuzzy rule-based classification system based on a new Interval-Valued fuzzy reasoning method with TUning and Rule Selection IEEE Trans. on Fuzzy Systems In Press. Available from http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6420926 DOI: 10.1109/TFUZZ.2011.2147794 14  F. Herrera, “Genetic fuzzy systems: Taxonomy, current research trends and prospects Evolutionary Intelligence vol. 1, no. 1, pp. 2746, March 2008. DOI: 10.1007/s12065-007-0001-5 15  H. Ishibuchi and Y. Nojima, “Analysis of interpretability-accuracy tradeoff of fuzzy systems by multiobjective fuzzy genetics-based machine learning International Journal of Approximate Reasoning  44 \(1\ 2007. DOI: 10.1145/775047.775142 16  M. Fazzolari, R. Alcalá, Y. Nojima, H. Ishibuchi, and F. Herrera, “A review of the application of multiobjective evolutionary fuzzy systems: Current status and further directions IEEE Trans. on Fuzzy Systems vol. 21, no. 1, pp. 45-65, February 2013. DOI: 10.1109 TFUZZ.2012.2201338 17  Y. Nojima, H. Ishibuchi, and I. Kuwajima, “Parallel distributed genetic fuzzy rule selection Soft Computing vol. 13, no. 5, pp. 511519, March 2009. DOI: 10.1007/s00500-008-0365-1 18  H. Ishibuchi, S. Mihara, and Y. Nojima, “Parallel distributed hybrid fuzzy GBML models with rule set migration and training data rotation IEEE Trans. on Fuzzy Systems vol. 21, no. 2, pp. 355-368 April 2013. DOI: 10.1109/TFUZZ.2012.2215331 19  E. Alba and M. Tomassini, “Parallelism and evolutionary algorithms IEEE Trans. on Evolutionary Computation vol. 6, no. 5 pp. 443-462, October 2002. DOI: 10.1109/TEVC.2002.800880 20  S. Cahon, N. Melab, and E. G. Talbi, “ParadisEO: A framework for the reusable design of parallel and distributed metaheuristics Journal of Heuristics vol. 10, no. 3, pp. 357-380, May 2004. DOI 10.1023/B:HEUR.0000026900.92269.ec 21  H. Ishibuchi, T. Yamamoto, and T. Nakashima, “Hybridization of fuzzy GBML approaches for pattern classification problems IEEE Trans. on Systems, Man, and Cybernetics - Part B vol. 35, no. 2, pp 359-365, April 2005. DOI: 10.1109/TSMCB.2004.842257 22  H. Ishibuchi and T. Nakashima, “Effect of rule weights in fuzzy rulebased classification systems IEEE Trans. on Fuzzy Systems vol. 9 no. 4, pp. 506-515, August 2001. DOI: 10.1109/91.940964 23  H. Ishibuchi and T. Yamamoto, “Rule weight specification in fuzzy rule-based classification systems IEEE Trans. on Fuzzy Systems vol 13, no. 4, pp. 428-435, August 2005. DOI: 10.1109/TFUZZ.2004 841738 24  H. Ishibuchi, K. Nozaki, and H. Tanaka, “Distributed representation of fuzzy rules and its application to pattern classification Fuzzy Sets and Systems vol. 52, no. 1, pp. 21-32, November 1992. DOI 10.1016/0165-0114\(92\90032-Y 25  J. Alcalá-Fdez, L. Sánchez, S. García, M. J. del Jesus, S. Ventura, J M. Garrell, J. Otero, C. Romero, J. Bacardit, V. M. Rivas, J. C Fernández, and F. Herrera, “KEEL: A software tool to assess evolutionary algorithms for data mining problems Soft Computing  vol. 13, no. 3, pp. 307-318, February 2009. DOI: 10.1007/s00500008-0323-y  70 


Copyright © 2009 Boeing. All rights reserved  Architecture Server-1 Server-2 DB2 SURVDB XML Shredder WebSphere Message Broker Ext.4 H Ext.3 G Ext.2 F Ext.1 E C WebSphere MQ TCP/IP Live ASDI Stream IBM Cognos Server-3 IBM SPSS Modeler SPSS Collaboration Deployment Services 


Copyright © 2009 Boeing. All rights reserved  Database Modeling Schemas for correlated ASDI messages translated into equivalent relational schemas  Database tables generated based on classes created from schema definitions  Nine main, eleven supporting tables  Each main table contains FLIGHT_KEY 


Copyright © 2009 Boeing. All rights reserved  Database Modeling 


Copyright © 2009 Boeing. All rights reserved  Correlation Process To archive received ASDI data  Track messages must be correlated with flight plan messages FLIGHT_KEY assigned Uncorrelated data tagged Approx 30 minutes to correlate one day of data 


Copyright © 2009 Boeing. All rights reserved  Historical Data Processing To load correlated data  Uncompress, unmarshall  Create a list of files containing the correlated data  Write data to warehouse 


Copyright © 2009 Boeing. All rights reserved  Live Data Processing Processed using IBM MQ IBM Message Broker and a technique called XML Shredding Message Broker Compute Nodes  Uncompress Node  Extract correlated messages  Shred Node adds to DB Stored Procedure “shreds XML docs and adds to tables 


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a “key, value” list using an XSTL  Queries made against this list of “key, value” pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


