Non-Almost-Derivable Frequ ent Itemsets Mining Yang Xiaoming , Wang Zhibin, Liu Bing Zhang Shouzhi, Wang Wei, Shi Bole Department of Computer and Information Technology, Fudan University, Shanghai 200433, China 032021170, 0124088, 031021057, compiler, weiwang1, bshi}@fudan.edu.cn Abstract rge to handle, so it is very necessary to work out a condensed representation of the collection of all frequent itemsets this paper, we propose a new condensed representation called frequent non-almost-derivable itemsets This representation is a subset of the original collection of frequent itemsets. For any removed itemset X\(which is called an frequent ivable itemset\, we can om this representation, and the lower bound and the upper by a userriori-like algorithm, which can extract all frequent nonderivable itemsets. Extensive empirical results on real sets show the compactness and good approximation of this representation 1 Introduction The frequent itemsets mining plays an essential role in many important data mining task, e.g., association rules mining H o we v e r, t h e n u m b e r of fr e que n t  itemsets is often huge, which greatly affects the efficiency and effectiveness of frequent itemsets mining Since the collection of frequent itemsets often show redundancy, one solution to this problem is based on the following idea: Instead of mining all frequent patterns, we only extract a particular compact subset of the frequent itemsets. The size of this subset is often far smaller, but it keeps. This kind of subset can be called a n of the set of  representation, we can greatly reduce the number of itemsets to be mined, stored and analyzed, while allowing derivation and support determination of all frequent itemsets without accessing the original data  In this paper, we introduce a new condensed representation. We defined a structure called stderivable itemsets For any ivable itemset we can derive a lower bound and an upper bound of its support by the support of its subsets. We show the collection of all frequent non-almost-derivable itemsets can be taken as a condensed representation of the collection of all frequent itemsets. We verify our approach in two real-life datasets. The experiments d good approximation of this representation The organization of the paper is as follows. In the next section, we introduced some preliminary knowledge,   especially the way how support is deduced. In section 3, we propose a structure and a condensed representation. In section 4 we give an algorithm extract this condensed representation. Section 5 discusses the empirical results and the related works are discussed in section 6 briefly 2. Basic notions and support deducing Let I  x 1 xn be a set of items A transaction T is a non-empty subset of I A saction database D is a set of transactions. An itemset X is a set of items. Itemset X is called an m-itemset iff the number of items in X is m t X is contained in a transaction T iff X 000\216 T The support of an itemset X in database D denoted by su  D,X is the number of transactions in D that contain X e\223ned minimum support threshold min_sup>0 an itemset X is called frequent iff su  D,X  000\225 min_sup The frequent itemset mining problem is to find the full collection of frequent itemsets, denoted by Freq  D,min_sup   Freq in short when D and min_sup is clear from the context Next we\220ll show how to compute the lower and et by the support of its subsets using the well-known inclusionexclusion principle [12  Th is m e th od is a l so il lu strated  by Calder in [8   Definition 1 \(Generalized itemset A generalized itemset  WVG 000\211\000 d negations of items. A transaction contains a general itemset WVG 000\211\000 means all the items of V appeared in the transaction and none of the items of W appeared in the Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


transaction. The support of a generalized itemset G  denoted by su  G is the number of transactions contains the generalized itemset. We say that a general itemset  WVG 000\211\000 000\001 is based n itemset X iff WVX 000\211\000 We call WVG 000\211\000 an odd generalized itemset iff W is odd it is called an even generalized itemset  For a general itemset WVG 000\211\000 based on X  1\(\\(\1 1   XsuYsu YsuGsu w XYV VY XYV VY 000\020\000\016\000\020\000 000\020\000 000\246 000\246 000\215\000\216 000\216\000\216 So 1\(\\(\1 1 GsuYsuXsu w XYV YX 000\020\000\016\000\020\000 000\246 000\215\000\216 000\016  So, we have oddis\\(\1 evenis\\(\1 1 1 GYsuXsu GYsuXsu XYV YX XYV YX 000\246 000\246 000\215\000\216 000\016 000\215\000\216 000\016 000\020\000d 000\020\000t So the term 000\246 000\215\000\216 000\016 000\020\000 \000\211\000 XYV YX X YsuWVGS 1 1 is a lower W is even\und or an upper W is odd bound of su  X epending on the parity of W  Definition 2 dd ois  max  even is  max WWVGS UB WWVGS B L XX XX 000\211\000 000 000\211\000 000 As is shown  the bound is tight, i  e., for every smaller interval        XX UBLBublb 000\215 there doesn\220t exist such a database such that the support of each subset of X is satisfied, but       ublbXsu 000\220  3. Frequent non-almost-derivable itemsets as a condensed representation Given an itemsets X as is illustrated in section 2 we can use the support of all its subset to compute a lower bound LB X and an upper bound UB X of its support. If UB X LB X 000\224 001 where 001 is a user-defined errortolerance threshold, this means we can get enough to mine and store the support information in our collection because this information can be inferred from the collection Definition 3 almost-derivable itemset non-almostderivable itemset  Given a user-defined error-tolerance threshold 001 000\225 0, itemset X is called an almost-derivable itemset iff UB X LB X 000\224 001 otherwise it is called a nonalmost-derivable itemset The collection of non-almost-derivable itemsets with error-tolerance threshold 001 by NAD 001   in brief NAD if the 001 is clear in the context. Similarly the collection of almost-derivable itemsets ted by AD  001  in brief AD Clearly NAD 000\210 AD 000I  Lemma 1 Let X  almost-derivable itemset there must exist an even generalized itemset G 1 and an odd generalized itemset G 2 based on X  su  G 1  su  G 2  000\224\001  and vice verse Proof There exist an even generalized itemset G 1 and an odd generalized itemset G 2 based on X  LB X su  X  su  G 1  UB X su  X  su  G 2  UB X LB X su  G 1   su  G 2  X 000\217 AD  001  000\234 UB X LB X 000\224 001 000\234 su  G 1   su  G 2  000\224 001  001\221 Theorem 1 \(Anti-Monotonicity Let X be an itemset if  000G ADX 000\217 then 000\005 Y 000\212 X, Y 000\217 AD  001  Proof According to lemma 1, there exist an even gent G 1 and an odd generalized itemset G 2 based on X  UB X LB X su\(G 1   su\(G 2  000\224 001  000\005 Y 000\212 X  Let G 1 220 G 1 000\211  Y  X  G 2 220 G 2 000\211  Y  X  G 1 220 and G 2 220 are both based on Y  UB Y 205LB Y 000\224 su\(G 1 220  su\(G 2 220 000\224 su\(G 1   su\(G 2  000\224 001 For G 1 220 is an even generalized itemset and G 2 220 is odd so Y 000\217 AD  001   001\221 llary 1 If X 000\217 NAD  000\005 Y 000\215 X  Y 000\217 NAD  This corollary is immediate from the theorem 1 Definition 4 frequent almost-derivable itemset frequent non-almost-derivable itemset  Itemset X is called a frequent almost-derivable itemset iff X is almost-derivable and also frequent. Similarly X is called a frequent non-almost-derivable itemset iff X is nonalmost-derivable and also frequent reshold 001 000\225 0 and a minireshold min_sup the collection of all st-derivable itemsets is denoted by FNAD  001 min_sup  in brief FNAD if 001 and min_sup is clear from the context; similarly, the collection of all frequent almost-derivable itemsets is denoted by FAD  001 minsup  in brief FAD Clearly FNAD  NAD 000\210 Freq  FAD  NAD 000\210 Freq  FNAD 000\211 FAD  Freq  Corollary 2 If X 000\217 FNAD  000\005 Y 000\215 X  Y 000\217 FNAD If X 000\217 FAD  000\005 Z 000\212 X  Z 000\217 FAD  This corollary tells the anti-monotonicity property of frequent non-almost-derivable itemset, which is very important for the mining algorithm Theorem 2 Let X be an itemset. If UB X su  X  000\224 001 2 or su  X  LB X 000\224 001 2  then 000\005 Y 000\212 X  Y 000\217 AD  001  The proof is omitted for the limit of space n of frequent nonalmost-derivable itemsets can be taken as a condensed representation, which can be used to answer frequency queries of any itemset approximately Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


The support of the any itemset X can be inferred from FNAD as follows 1 If X 000\217 FNAD then return its support directly 2 If X  FNAD but all its subsets do then compute X LB and X UB If UB X LB X  001 or UB X  min_sup then X frequent otherwise return its bounds 3 If some of its subsets are not in FNAD  compute the support of these subsets recurme of them are found out infrequent, then X is also infrequent; othere support bounds of X Notice in this case, the supports of some of its subsets are given in bound interval. Either  it in the inequalities. The bounds of X computed by this way is denoted by lb X and ub X If ub X  min_sup then X is infrequent, otherwise return its bounds Case 3 may lead to two side-effects. One is approximate error accumulation, i.e., it is possible for some long itemsets, its bound interval are larger than 001  In section 5, experiments show the effect of error accumulation is very acceptable. Another side-effect is that the supports of some \215borderline\216 infrequent itemsets may fail to be determined infrequent. But this can not be a big issue because the error-tolerance threshold is usually much less than the minimum support threshold. Further, in a somehow extreme condition, even we known exactly whether it is frequent or not, an extra database scan can always be made to ne itemsets 4. Discovering frequent non-almostderivable itemsets In this section, we give an Apriori-based levelwise algorithm [1,2  find in g o u t  t h e co l l ection  o f  f r eq u e nt almost-derivable itemsets Since frequent non-almost-derivable itemsets has the anti-monotonic characterize, some prune technique similar with the well-known prune technique in Apri process, which means if X can be removed from the collection, then all of its supersets can be removed as well The process of this algorithm is very similar with Apriori. The main difference is that there is an additional check step in FNADMINER After the candipport bounds of them are computed and checked. For a candidate X e minimal support threshold, then it cannot be frequent, so it is pruned. If UB X LB X 000\224 001 then X is an almost-derivable so it is also pruned Algorithm 1 FNADMINER Input Transaction database D minimal support threshold 0011 maximal error bound 001 Output 002\326 FNAD  D  0011  001   1  and andcompute  to1levelofcandidatesgenerate  1:};1 11 1 1 1 1 FNAD ii XSuXCC LBUBUB UBLB CCX C CCi XsuCXXsuXFNADFNAD C iitemsetsCFNAD ii XXX XX i i i i i output while;end for;end if;end thenif  doallfor dowhile 000\016\000 000!\000\037\000\211\000 000!\000t 000\217 000\\000 000\016 000t\000\232\000\217\000!\000\037\000\211\000 000\\000z 000 000\020\000 \000\\000 000\016\000\016 000\016 000\016 000\016 000G 000V 000V Theorem 3 \(Correctness Algorithm FNADMINER find out the collectio n of all frequent nonalmost-derivable itemsets correctly 6. Experiments Our experiments are conducte d on two real-life datasets Mushroom and Chess They are both obtained from the machine learnin g repository of UC Irvine 1  The dataset Mushroom contains 8124 transactions each transaction contains 23 items, and 120 items in total. The dataset Chess contains 3196 transactions each contains 37 items, and 75 items in total 6.1 Collection size Table 1 shows the size of original collection of frequent itemsets and the collection of frequent nonalmost-derivable itemset at different error bound on the   dataset Mushroom table 2 are results on the dataset Chess It can be observed that the size of this representation is far smaller than the original collection sides the size of collection, the maximal itemset length is also reduced substantially. The detail information of the max itemset length is omitted from this paper because of the limit of space  1 http://www.ics.uci.edu/~ml earn/MLRepository.html Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


minsup 15% 20% 25% 30% 35  Freq 98575 53583 5545 2735 1189 001  0 2153 1319 661 421 302 001  10 2034 1242 627 401 285 001  20 1932 1190 605 390 277 001  30 1899 1166 602 389 276 001  40 1855 1139 598 388 275 001  50 1745 1090 580 379 270 Table 1: Collection size on Mushroom dataset  000\023 000\025\000\023 000\027\000\023 000\031\000\023 000\033\000\023 000\024\000\023\000\023 000\024\000\025\000\023 000\024\000\027\000\023 000\024\000\031\000\023 000\024\000\030 000\025\000\023 000\025\000\030 000\026\000\023 000\026\000\030 000V\000X\000S\000S\000R\000U\000W\000\003\000W\000K\000U\000H\000V\000K\000R\000O\000G 000\013\000\b\000\f 000D\000E 000V\000R\000O 000X\000W\000H 000\003\000H 000U\000U\000R 000U 000\024\000\023 000\025\000\023 000\027\000\023 000\030\000\023 Figure 1    Maximal absolute error on Mushroom 000\023 000\023\000\021\000\023\000\030 000\023\000\021\000\024 000\023\000\021\000\024\000\030 000\023\000\021\000\025 000\023\000\021\000\025\000\030 000\023\000\021\000\026 000\023\000\021\000\026\000\030 000\023\000\021\000\027 000\024\000\030 000\025\000\023 000\025\000\030 000\026\000\023 000\026\000\030 000V\000X\000S\000S\000R\000U\000W\000\003\000W\000K\000U\000H\000V\000K\000R\000O\000G\000\013\000\b\000\f 000U 000H 000O 000D\000W\000L 000Y\000H 000\003\000H\000U 000U\000R 000U\000\013 000\b\000\f 000\024\000\023 000\025\000\023 000\027\000\023 000\030\000\023 Figure 3    Average relative error on Mushroom 000\023 000\023\000\021\000\024 000\023\000\021\000\025 000\023\000\021\000\026 000\023\000\021\000\027 000\023\000\021\000\030 000\023\000\021\000\031 000\023\000\021\000\032 000\023\000\021\000\033 000\024\000\025 000\026\000\027\000\030 000\031\000\032 000L\000W\000H\000P\000V\000H\000W\000\003\000O\000H\000Q\000J\000W\000K 000U\000H 000O\000D\000W 000L\000Y\000H 000\003\000H 000U\000U\000R 000U\000\013 000\b\000\f 0000\000D\000[\000\003\000U\000H\000O\000\003\000H\000U\000U 000$\000Y\000J\000\003\000U\000H\000O\000\003\000H\000U\000U Figure 5   Error w.r.t. itemset length on Mushroom  min_sup 35 001 20 minsup 85 80 75% 70 65  Freq 2669 8227 19951 48969 111778 001  0 446 684 1042 1527 2251 001  10 296 462 699 1016 1424 001  20 233 381 562 826 1171 001  30 180 304 454 688 1000 001  40 176 292 438 664 966 001  50 161 177 411 627 916 Table 2: Collection size on Chess dataset 000\023 000\030\000\023 000\024\000\023\000\023 000\024\000\030\000\023 000\025\000\023\000\023 000\025\000\030\000\023 000\031\000\030 000\032\000\023 000\032\000\030 000\033\000\023 000\033\000\030 000\034\000\023 000V\000X\000S\000S\000R\000U\000W\000\003\000W\000K\000U\000H\000V\000K\000R\000O\000G\000\013\000\b\000\f 000D\000E\000V 000R\000O 000X\000W\000H 000\003\000H 000U\000U\000R 000U 000\024\000\023 000\025\000\023 000\027\000\023 000\030\000\023 Figure 2 Maximal absolute errors on Chess 000\023 000\023\000\021\000\025 000\023\000\021\000\027 000\023\000\021\000\031 000\023\000\021\000\033 000\024 000\024\000\021\000\025 000\031\000\030 000\032\000\023 000\032\000\030 000\033\000\023 000\033\000\030 000\034\000\023 000S\000S\000R\000U\000W\000\003\000W\000K\000U\000H\000V\000K\000R\000O\000G\000\013\000\b\000\f 000U\000H\000O 000D\000W\000L\000Y 000H\000\003\000H 000U\000U\000R 000U\000\013\000\b 000\f 000\024\000\023 000\025\000\023 000\027\000\023 000\030\000\023 Figure 4  Average relative error on Chess 000\023 000\025 000\027 000\031 000\033 000\024\000\023 000\024\000\025 000\024\000\025\000\026\000\027\000\030\000\031\000\032\000\033\000\034\000\024\000\023\000\024\000\024\000\024\000\025\000\024\000\026 000L\000W\000H\000P\000V\000H\000W\000\003\000O\000H\000Q\000J\000W\000K 000U\000H 000O\000D\000W 000L\000Y\000H 000\003\000H 000U\000U\000R 000U\000\013 000\b\000\f 0000\000D\000[\000\003\000U\000H\000O\000\003\000H\000U\000U 000$\000Y\000J\000\003\000U\000H\000O\000\003\000H\000U\000U Figure 6 Error w.r.t. itemset length on Chess  min_sup 70 001 50 Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


6.2 Approximation error The absolute error is defined as the interval width between the lower and upper support bounds of a frequent almost-derivable itemset. The relative error is the absolute error divided by its support The e relative error is the total relative error divided by the number of frequent almost-derivable itemsets Figure 1 and figure 2 show the maximal absolute error under different support th resholds and different error-tolerance thresholds. From these two figures, we can see the maximal absolute error is usually only several times of the error-tolerance threshold 001 Since the tolerance threshold is often very small, the maximal error is in fact very controllable Figure 3 and Figure 4 show average relative error  Figure 3 shows the re lative error on Mushroom is often below 0.4%. The results on Chess are a little bigger because there are 3196 transactions in Chess fewer than 8124 in Mushroom  threshold is in fact relative larger for Chess  Figure 5 and figure 6 show the error distribution with respect to the length of itemsets. For the Mushroom dataset, when min_sup 35% and 001 20, as can be seen from figure 5, with the itemset length grows from 2 to 4\(there are no frequent almost-derivable itemsets with length less than 2\or grows accordingly because of error accumulat ion. But when itemset length grows continually, th e error drop s instead. This is because the longer an itemset be, there are more generalized itemsets based on it, thus possibly it can get tighter bound of its sup port with more inequalities This fact also prevents the error from accumulating too much for long itemsets 7. Related work The concept of closed set was firstly proposed by Par closed set is an itemset such that its support does not equal the s upport of any of its supersets. For any non-closed set, the support can be inferred by the maximal support of its closed subsets Free-sets 5 oduced by Boulicaut and Bykowski is an approximate representation. An itemset X is said to be a 001 free-sets iff there doesn\220t exist its subset Y the support of Y is close with the support of X within a user-defined threshold 001  Disjunction-free sets 6  a l s o  proposed by Bykowski, is an itemset which have no disjunction rule based on it Non-derivable itemsets   can be seen as a special case of our non-almostderivable itemset when error-tolerance threshold is 0 Generators  are item s ets w hose support does not equal the support of any of its subsets. Other condensed representations ha ve also been introduced by researchers, such as disjunction-free generators 11  introduced by Kryszkiewicz B d and B m by Pei  8. Conclusion In this paper, we introduce two structures called almost-derivable itemset and non-almost-derivable itemset We show the collection of frequent nonalmost-derivable can be server as a condensed representation of the collection of all frequent itemsets. We test our approach on two real-time datasets and the experiments show: \(1\representation is very small; \(2\ The representation can give a very good approximation of the original collection References  R.Agrawal, T.Imieli nski a n d A. Swami. M i ning as so ciation rules between sets of items in large databases In SIGMOD'93 pp. 207-216, Washington,D.C., 1993  H.M annila and H.To ivonen. Levelwis e s e arc h a n d  borders of theories in knowledge discovery Data Mining and Knowledge Discovery pp. 241-258, 1997  H.M annila and H.To ivonen. M u ltiple us es of fr e q u e nt  sets and condensed representations. In Proceedings KDD'96 pp. 189-194, Portland, USA, 1996  N. Pas quier, Y Bas tide, R. Taouil, and L. Lakhal. Dis coverying frequent closed itemsets for association rules In 7th International Conference on Database Theory pp. 398\205416, 1999  J  F. Boulicaut, A. By kows ki, and C. Rigotti. Approximation of frequency queris by means of free-sets. In Principles of Data Mining and Knowledge Discovery  pp. 75\20585, 2000  By kows ki, A. and Rigotti C. \(2001 Condens ed Representation to Find Frequent Patterns. In Proc. of PODS \22001 pp. 267\205273, Santa Barbara, CA, USA  J  Pei, G. Dong, W  Zou, and J  Han. Mining Condensed Frequent Pattern Bases Knowledge and Information Systems: An International Journal Volume 6 Number 5, 2004, pp. 570-594, Springer-Verlag  T. Cald e r s. Deducing bounds o n the fre quency of itemsets. In EDBT Workshop DTDM Database Techniques in Data Mining 2002  T. Cald e r s and B. Goethals M i ning all no n-derivable  frequent itemsets. In Proc. PKDD Int. Conf. Principles of Data Mining and Knowledge Discovery pp. 74\20585 Springer, 2002  Kry szkiewicz, M  2001b entation of Frequent Patterns based on Disjunction\205free Generators. In Proc. of ICDM 22001, pp. 305\205312, San Jose, CA  Kry szkiewicz, M  and Gajek, M. \(2002a e Representation of Frequent Patterns based on Generalized Disjunction-Free Generators. In Proc. of PAKDD 220 02, pp. 159\205171, Taipei, China  D.E. Knuth. Fun da m e n tal Algorithms  Addison-Wesley  Reading, Massachusetts,1997 Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


sented as a gray node or a black node, depending on whether it is a internal node or a leaf node in the prefix tree Construction of the Prefix Tree: Initially, the prefix tree has only the root node whose brother and child pointers are null. Then, we insert the candidate itemsets, one by one Suppose there is a candidate k-itemset 111, Iz, .  . . , Ik This candidate itemset can be inserted into the prefix tree in k steps, one for each item. First, we check whether the root node has children. As described above, every node at level I should he the first item of a candidate itemset. If the root node doesn't have a child at all, then we need to create a node of I1 and link it as the first child of root node. If the root node has children, we can reach the first child of the root node through its child link, then check other children through the brother links to make sure whether 11 is already at level 1. If not, we need to create a node of I1 at the right position to keep the lexicographical order of the items at level 1. From the node of Il at level 1, we repeat the same procedure for the next item I2 in order to have it at level 2 This procedure is repeated until the last item I ,  is located or inserted at level IC, and then it will reference the candidate itemset through its candidate pointer same level in their lexicographical order, but only the first child is directly connected to the prefix item through a child link. The root node corresponds to a null item and has four children in Figure 2: A, B, C and E ,  which are the first Counting the Global Candidates Using the P r e b  Tree For each transaction, we use a recursive method to count all candidates appear in the transaction. Figure 3 shows 502 2004 IEEE International Symposium on Cluster Computing and the Grid how to count the candidates in a transaction containing A ,  B, D ,  E ,  G}. We begin with the whole transaction at the root node. At level I ,  items A,  B and E are matched and only E references a candidate, so we increase the count of the candidate itemset { E }  by one. Then, we recursively process the transaction segment {B ,  D ,  E ,  G} on node A D ,  E ,  G }  on node B, and {G} on node E. These three operations let us enter the next level. At level 2, A's first child B appears in {B, D ,  E ,  G}  but does not reference any candidate. So, we continue processing { D ,  E ,  G} on this B node. Back to the node B at level 1, where we pro cess { D ,  E ,  G} on it, we can find it has no child appearing in { D ,  E,G}, so we simply stop here without processing on any of its children. For the node E at level I ,  its one child G is matched. Since this G references a candidate we increase its count by one. On this branch, the transac tion segment becomes cmpty after processing G, so we also stop here. Now, from thc node B at level 2, we can enter level 3. Node E,  the second child of node B, is matched and the corresponding candidate is counted. When we try to process {G} on node E ,  we cannot find a matching child since E has no child. So, after the transaction is scanned the counts of {A,  B,  E} ,  { E } ,  and { E ,  G }  are increased by one  V".GI i Figure 3. Counting the global candidates us ing the prefix tree 3.2.2 Reduction of the Global Candidate Set For DMM, how to reduce the sire of the initial global can didate set GCl and the subsequent global candidate sets as much as possible is very important for the overall pertor mance. Three techniques were used to solve this problem global support estimation, subset-infrequency based prun ing, and superset-frequency based pruning Support Estimation of the Global Candidates: When we merge the local maximal frequent items \(MFIs processing nodes, we need to keep just one copy of each  503 


503 MFI that is frequent in more than one node as a global can didate. If a maximal itemset is frequent at all nodes, obvi ously it is also a global maximal frequent itemset. We just need to accumulate its local support counts and put it into the FrequentSet. Such global candidates, however, are very few. Fortunately, even though most itemsets in GC1 appear as local MFls in just one or a few nodes, many of them of ten have their supersets frequent in other nodes. In that case the support counts of the supersets of a candidate allow us to estimate the minimum support count of the candidate in those nodes. For example, suppose that itemset { A ,  B,  G is a local MFI with the local count of 4000 in node I, while A,  B ,  C, E ,  G} and { A ,  B ,  G, K }  are local MFIs in node 2 with local counts of 3800 and 4200, respectively. We can then estimate that the local support count of {A, B ,  G}  in node 2 should he at least 4200. By this way, we can esti mate the minimum support count of any itemset in a node if any of its supersets is frequent in that node. Obviously the estimated minimum support count o f a  candidate is the lagest  support count of all its supersets in that node Subset-Infrequency Based Pruning and Superset Frequency Based Pruning: During thc global mining phase, DMM maintains the following scts: GCk \( k  2 11 FrequentSet and InfrequentSer. They are changing dy namically with the progress of the mining process. The global mining phase continues until GC, is empty, for some k 2 1, to ensure that we will not miss any global maximal frequent itemset. Eventually, FrequentSet will include all the global MFIs. Since DMM uses Frequentset and InfrequentSet to perform the superset-frequency based pruning and the subset-infrequency based pruning maintaining these two sets without any rediindancy is important to make the pruning steps efficient. After each pass in the global mining phase, we determitle whether each global candidate is maximally frequent or not. If a global candidate is frequent, we put it into FrequentSet only if none of its supersets is already in that set. On the other hand, i f  a global candidate is infrequent. we put it into InfrequentSet only if none of its subsets is already in that set. For example, if {A, B, G, H }  is infrequent but A ,  H }  is already in InfrequentSet, we do not insert it into InfrequentSer, because any superset of { A ,  B,  G, H }  will be also pruned by {A,H} when the subset-infrequency based pruning is applied If a global candidate k-itemhet is identified as infrequent we split it into k \(k  - 1 as new candidates. However, some of them may not be a valid candidate for the next global pass if it appears in In frequentSet or has a subset in it. In that case, we need to split the invalid candidate into its largest proper subsets. For ex ample, if {A,  B, C, D }  is infrequent and its subset {A, D is in InfrequentSer, we will continue the splitting until we 2004 IEEE International Symposium on Cluster Computing and the Grid get the following new candidates: { A ,  B, C}, {B ,  C,  D A,B},  \( A ,  C}, { B , D }  and {C,D}.  In practice, these two pruning techniques can make the global candidate set shrink drastically for each pass 3.2.3 Cube-based Communication between Processors To perform the communication between processing nodes efficiently, we impose a logical binary n-cube structure on the processing nodes. Then, the nodes can exchange and merge the local count information through increasingly higher dimensional links between them [4]. In the n-cube there are 2n nodes, and each node has n-bit binary address Also, each node has n neighbor nodes which are directly linked to that node through differcnt dimensional links. For example, there are 8 nodes in a 3-cube structure, and node 0 0 0 001 010 100 through a I st-dimensional link, a 2nd-dimensional link, and a 3rd-dimensional link, respectively. Thus, in the n-cube all the nodes can exchange and merge their local counts in 


all the nodes can exchange and merge their local counts in T I  steps, through each of the n different dimensional links When n = 3, the three exchange and merge steps are step 1: node \( 1 where denotes a don't-care bit step 2: node \( 0 1 step 3: node \(0 1 O  3.3 Pseudo-code of DMM As we assume a homogeneous distributed computing environment where all the processing nodes are the same we just give the pseudo-code of the DMM algorithm running on a node P Local Mining Phase P" applies the sequential Max-Miner algorithm on D' and stores local MFIS into LM n = log, N :  I* N processing nodes are used for mining *I for \( j  = 1;j 5 n;j P' exchange and merge LM' with that of a neighbor node through the jth-dimensional link and the result is stored in LiM GC FrequenrSer= 6 foreach local MFI x in LM if the estimated global support o fx  is above the minimum support superset of x else put x into.GC1 then put x into FrequenrSer unless it contains a I apply the suprrset-frequency based pruning on GCI  504 Global Mining Phase InfrequenrSer= 4 global pass k ,  for k 2 1 while \(GCk # 4 Pi scans D' to count the candidates in GCk for \( j  = I ; j  5 n ; j P' exchange and merge the local counts of GCk members with a neighbor node through the jth-dimensional link foreach candidate x in GCx if the support of x is above the minimum support then put x into FrequentSer unless it contains a superset of x subset of x else put I into InfreqrcenrSer unless it contains a  foreach candidate inserted into InfrequenrSer in the current pass split the infrequent candidate into new candidates i.e., its largest subsets apply the subset-infrequency based pruning on these new candidates those candidates pruned by the subset-infrequency based pruning are put back into InfmquenrSer this process will continue until no new candidate either appears in InfrequenrSer or has any subset in i t 1 apply the superset-frequency based pruning on the new candidates remove those candidates which appear in Frequentset or put the new candidates that passed the two pruning operations into GCI,+~ for the next global pass k + I k have any superset in  it *I 1 GM = FrequentSer I* GM is the set of all maximal frequent itemsets 4 Performance Evaluation Our test platform is an &amp;node Linux cluster system 


Our test platform is an &amp;node Linux cluster system where nodes are connected by a Fast Ethernet switch. Each node has a 800 Mhz Pentium processor, 512 MB memory and a 40 GB disk drive. The processes are communicating using the MPI \(Message Passing Interface The databases used in our experiments are synthetic sales transaction databases generated as in [I]. All parameters used for generating databases are described in Table 1. For all databases, c = 0.5, m = 0.5, U = 0.1, ILI = 2000 and N I  = 1000. Table 2 lists all databases used in our perfor mance evaluation experiments. The size of each database is about 360 MB. When running the parallel algorithm on a database, we need to partition it into local databases. To balance the size of the local databases, each transaction is randomly allocated to a node 2004 IEEE Interna6onal Symposium on Cluster Computing and the Grid ID1 TI 111 ILI N I c m D In order to compare the performance of DMM and Count Distribution, we also implemented Count Distribution on the same platform Number of vansactiom in the database Average size of the transactions Average size of the maximal potentially frequent itemxu Numkraf maximal potentially frequent itemels Number of items Comlaliun level Mean of the comption level Variance of the camption level Table 1. Synthetic database parameters Table 2. Databases 4.1 Improvement of DMM over Count Distribu tion We ran both DMM and Count Distribution on different synthetic databases with different minsup values. If we de tine TCD and TDM,U as the execution times of CD and DMM, respectively, then the speedup of DMM over CD is TCD/TDMA,. In Table 3, the speedup of DMM is shown for different databases listed in the first column and for dif ferent values of minsup listed in the first row. In these ex periments, all 8 nodes in our cluster system were used Table 3. Speedup of DMM over CD \(8-node case As minsup decreases, DMM begins to show more and more improvement in our tests. As shown in Table 3, when the 111 value of the database is large, such as 8 or IO, even if minsup is as high as 0.58, DMM is faster than Count Dis tribution with a speedup above 2.5. It is because a large 11 value results in large frequent itemsets \(i.e., long patterns which benefits DMM. If minsup is less than 0.25%, DMM outperforms Count Distribution considerably DMM uses the local and global mining phases to re duce the overall synchronization and communication re quirement, but the global mining phase still needs several passes over the database and incurs some extra computation overhead. In our cluster system, since the communication speed between nodes is high, the benefit of reduced syn chronization and communication overhead is not enough to offset the effect of extra passes during the global mining phase. However, this feature of DMM may be attractive to some distributed systems where the communication cost is relatively high 4.2 Synchronization Requirement of DMM and Count Distribution We compared the number of synchronizations needed between processing nodes in DMM and Count Distribu 


between processing nodes in DMM and Count Distribu tion. In DMM, the local mining phase needs only one syn chronization. So, the total number of synchronizations is the number of passes needed in global mining phase plus one. Table 4 shows the comparison results. Here, we de fine SDMM and SCD as the numbers of synchronizations needed in DMM and CD, respectively. The first row of the table lists various values of minsup, and the first column lists the names of databases. The values in each entry of the table represents SD,MM :SCD Table 4. Comoarison of svnchronization re quirement When minsup is high, DMM is comparable to or a lit tle bit slower than Count Distribution. We also ran Apriori and Max-Miner for these cases, and found that Max-Miner doesn  t show much improvement over Apriori, either. That is because the high minsup limits the number of frequent itemsets and the length of those frequent itemsets. Thus the effect of look-ahead technique used by Max-Miner is not clearly shown, and naturally DMM has the same result DMM needed just two times of synchronization in the best cases. In other cases, the number of synchronizations needed for DMM was also much smaller than that of CD mainly because DMM requires only one synchronization during the local mining phase 505 2004 iEEE International Symposium on Cluster Computing and the Grid 4.3 Communication Requirement of DMM and Count Distribution In Count Distribution, all nodes have the same set of candidate itemsets in each pass. So, every node needs to exchange the same amount of count information with oth ers. In DMM, nodes need to exchange two types of data candidates and their counts. For the merging of local MFIs to construct the first global candidate set, each node per forms log, N send and log, N receive operations when N processing nodes are used. Since the set of local MFIs in one node may he different from those in other nodes, the amount of data each node sends or receives varies at each communication step. In each global pass, all nodes have the same global candidate set and exchange the same count in formation in log, N steps. To make it simple, we computed the average amount of data each node sends and receives during the whole mining Let  s consider the difference in the meaning of candi dates of the two algorithms as the number of candidates determines how much data need to he exchanged between processing nodes during the mining. In Count Distribution its candidates are the potential frequent itemsets generated as in Apriori. In DMM, after the local mining phase, can didates involved in the communication are just the potential maximal frequent itemsets; i.e., all local MFIs and some of their subsets that are not global MFIs. Compared with the set ofcandidates in Count Distribution, the set of candidates in DMM is very small. Thus, DMM requires much less communication than Count Distribution even though DMM needs to merge the candidates first \(after the local mining phase during the global mining phase When the minsup is very low, Count Distribution tends to discover a large number of short frequent patterns, so that there are a large number of candidates in early passes This results in a very high communication overhead be tween nodes. On the other hand, in  DMM, the increase in the number of short frequent patterns usually results in a small change in the number of local MFIs. Thus, even though low minsup value may affect the local mining phase of DMM, it has a relatively small impact on the communi cation overhead during the global mining phase. Therefore as the minsup decreases, DMM performs better than Count Distribution in terms of communication requirement 


Distribution in terms of communication requirement We implemented two versions of Count Distribution one is using the n-cube communication, and the other is using the all-to-all communication. We compared the av erage amount of data each node communicates with oth ers when we executed DMM and Count Distribution on the T30J08D2954K database with various values of minsup and the results are shown in Figure 4 As shown in Figure 4, the communication overhead of 500 450 400 a 350 B 300 g 200 250 150 s d 10.3 50 0 1 075 0 5  0 4  0 3  025 0 2  015 0 1 Minimum Suppon Figure 4. Comparison of communication re quirement DMM is much lower than that of Count Distribution. Even though DMM needs to cxchange candidates at the end of the local mining phase and some candidates may consist of many items, the total amount ofdata to he transferred is still relatively small, because Count Distribution must exchange the count information for much larger candidate sets. Com pared with Count Distribution using the all-to-all commu nication scheme, DMM demonstrates a big improvement in communication for all cases. Here, we  d like to emphasize that the advantage of DMM in communication requirement comes from its much smaller size of candidate sets and the n-cube communication scheme 4.4 Sensitivity Analysis of DMM In this section, we analyze the characteristics of the DMM algorithm in terms of speedup and sizeup. All tests were performed with a minsup of 0.25 4.4.1 Speedup We measured the speedup of DMM as the number of pro cessing nodes was increased while the database size re mained the same. For the databases listed in Table 2, we kept the same database size of 360 MB, but the database was partitioned into 2,  4, and 8 parts when the number of nodes were 2,4, and 8, respectively Figure 5 shows the execution time of DMM on the 2 node, 4-node, and 8-node systems. To demonstrate the speedup, we also ran the sequential Max-Miner for each database on a single node. As the number of nodes was doubled, the execution time of DMM was reduced by about 40% to 50%. Even though DMM may not achieve the linear speedup, it still shows a good speedup When DMM is executed on the T40110D2256K database using 2 nodes, its execution time is small. This is 506 2004 IEEE International Symposium on Cluster Computing and the Grid 16000 4000 3 12000 I ; iwoo E P 2 6000 w 4wo 2000 5 8000 8 0.7 because, when the number of nodes is small, the datadistri bution characteristic of each data partition is very similar to that of the whole database. So, after the local mining phase the initial global candidate set would be similar to the set of 


the initial global candidate set would be similar to the set of global MFIs. As a result, during the global mining phase the communication and synchronization overhead is low  0 2 4 6 8 1 0 Number of Nodes Figure 5. Speedup of DMM 4.4.2 Sizeup For the sizeup test, we fixed the system to the 8-node con figuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local database sire at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remained the same as the local database size was increased. This is different from the speedup test, where the database repartitioning was per formed when the number of nodes was increased. The per formance of DMM is affected by the database repartitioning to some extent, although it is usually very small. During the sizeup test, the local mining result of DMM is not changed at all at each node The results shown in Figure 6 indicate that DMM has a very good sizeup property. Since increasing the size of local database did not affect the local mining result of DMM at each node, the total execution time increased just due to more disk U 0  and computation cost which scaled almost linearly with sizeup 5 Conclusions In this paper, we proposed a new parallel maximal fre quent itemset \(MFI Max-Miner \(DMM tems. DMM is a parallel version of Max-Miner, and it re quires low synchronization and communication overhead compared to other parallel algorithms. In DMM, Max Miner is applied on each database partition during the lo 0 45 90 135 180 225 270 Amwnt of Data per Node \(ME Figure 6. Sizeup of DMM cal mining phase. Only one synchronization is needed at thc end of this phase to construct thc initial global candi date set. In the global mining phase, a top-down search is performed on the candidate set, and a prefix tree is used to count the candidates with different length efficiently. Usu ally, just a few passes are needed to find all global maximal frequent itemsets. Thus, DMM largely reduces the number of synchronizations required between processing nodes Compared with Count Distribution, DMM shows a great improvement when some frequent itemscts are large \(i.e long patterns employed by DMM for efficient communication between nodes; and global support estimation, subset-infrequency based pruning, and superset-frequency based pruning are used to reduce the size of global candidate set. DMM has very good speedup and sizeup properties References I ]  R. Agrawal and R. Srikant  FdSt Algorithms for Mining As sociation Rules  Pmc. o f f h e  ZOrh VLDB Conf, 1994, pp 487499 2] R. Agrawal and I. C. Shafer  Parallel Mining of Association Rules  IEEE Trans. on Knowledge and Dura Engineering Vol. 8, No. 6, 1996, pp. 962-969 3] R. I. Bayardo  Efficient Mining Long Patlems from Databases  Proc. ofrhe ACM SIGMOD Inf  l Conf on Man ogemenr ofDara, 1998, pp. 85-91 4] S.  M. Chung and J. Yang  A Parallel Distributive Join Al gorithm for Cube-Connected Multiprocessors  IEEE Trans on Parallel and Disrribured Systems, Vol. 7, No. 2, 1996, pp 127-137 51 M. Snir, S. Otto. S. Huss-Lederman, D. Walker, and J. Don gana, MPI: The Complete Reference, The MIT Press, 1996 


gana, MPI: The Complete Reference, The MIT Press, 1996 6] R. Rymon  Search through Systematic Set Enumeralion   Pmc. of3rd Inr  l Con$ on Principles of Knowledge Repre sentation and Reasoning, 1992, pp. 539-550 507 pre></body></html 


sketch-index in answering aggregate queries. Then Section 5.2 studies the effect of approximating spatiotemporal data, while Section 5.3 presents preliminary results for mining association rules 5.1 Performance of sketch-indexes Due to the lack of real spatio-temporal datasets we generate synthetic data in a way similar to [SJLL00 TPS03] aiming at simulation of air traffic. We first adopt a real spatial dataset [Tiger] that contains 10k 2D points representing locations in the Long Beach county \(the data space is normalized to unit length on each dimension These points serve as the  airbases  At the initial timestamp 0, we generate 100k air planes, such that each plane \(i uniformly generated in [200,300], \(ii, iii destination that are two random different airbases, and iv  the velocity direction is determined by the orientation of the line segment connecting its source and destination airbases move continually according to their velocities. Once a plane reaches its destination, it flies towards another randomly selected also uniform in [0.02, 0.04 reports to its nearest airbase, or specifically, the database consists of tuples in the form &lt;time t, airbase b, plane p passenger # a&gt;, specifying that plane p with a passengers is closest to base b at time t A spatio-temporal count/sum query has two parameters the length qrlen of its query \(square number qtlen of timestamps covered by its interval. The actual extent of the window \(interval uniformly in the data space \(history, i.e., timestamps 0,100 air planes that report to airbases in qr during qt, while a sum query returns the sum of these planes  passengers. A workload consists of 100 queries with the same parameters qrlen and qtlen The disk page size is set to 1k in all cases \(the relatively small page size simulates situations where the database is much more voluminous specialized method for distinct spatio-temporal aggregation, we compare the sketch-index to the following relational approach that can be implemented in a DBMS. Specifically, we index the 4-tuple table lt;t,b,p,a&gt; using a B-tree on the time t column. Given a count query \(with window qr and interval qt SELECT distinct p FROM &lt;t,b,p,a&gt WHERE t?qt &amp; b contained in qr The performance of each method is measured as the average number of page accesses \(per query processing a workload. For the sketch-index, we also report the average \(relative Specifically, let acti and esti be the actual and estimated results of the i-th query in the workload; then the error equals \(1/100 set the number of bits in each sketch to 24, and vary the number of sketches The first experiment evaluates the space consumption Figure 5.1 shows the sketch index size as a function of the number of sketches used \(count- and sum-indexes have the same results more sketches are included, but is usually considerably smaller than the database size \(e.g., for 16 signatures, the size is only 40% the database size 0 20 40 60 80 


80 100 120 140 160 8 16 32 number of sketches size \(mega bytes database size Figure 5.1: Size comparison Next we demonstrate the superiority of the proposed sketch-pruning query algorithm, with respect to the na  ve one that applies only spatio-temporal predicates. Figure 5.2a illustrates the costs of both algorithms for countworkloads with qtlen=10 and various qrlen \(the index used in this case has 16 sketches also illustrate the performance of the relational method which, however, is clearly incomparable \(for qrlen?0.1, it is worse by an order of magnitude we omit this technique Sketch-pruning always outperforms na  ve \(e.g., eventually two times faster for qrlen=0.25 increases with qrlen, since queries returning larger results tend to set bits in the result sketch more quickly, thus enhancing the power of Heuristics 3.1 and 3.2. In Figure 5.2b, we compare the two methods by fixing qrlen to 0.15 and varying qtlen. Similar to the findings of [PTKZ02]4 both algorithms demonstrate  step-wise  growths in their costs, while sketch-pruning is again significantly faster The experiments with sum-workloads lead to the same observations, and therefore we evaluate sketch-indexes using sketch-pruning in the rest of the experiments 4 As explained in [PTKZ02], query processing accesses at most two paths from the root to the leaf level of each B-tree regardless the length of the query interval Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE sketch-pruning naive relational 0 100 200 300 400 500 600 700 800 900 0.05 0.1 0.15 0.2 0.25 number of disk accesses query rectangle length 300 0 100 200 400 500 600 1 5 10 15 20 number of disk accesses query interval length a qtlen=10 b qrlen=0.15 Figure 5.2: Superiority of sketch-pruning \(count As discussed in Section 2, a large number of sketches reduces the variance in the resulting estimate. To verify this, Figure 5.3a plots the count-workload error of indexes 


using 8-, 16-, and 32- sketches, as a function of qrlen qtlen=10 error \(below 10 it increases slowly with qrlen used, however, the error rate is much higher \(up to 30 and has serious fluctuation, indicating the prediction is not robust. The performance of 16-sketch is in between these two extremes, or specifically, its accuracy is reasonably high \(average error around 15 much less fluctuation than 8-sketch 32-sketch 16-sketch 8-sketch relative error 0 5 10 15 20 25 30 35 0.05 0.1 0.15 0.2 0.25 query rectangle length relative error 0 5 10 15 20 25 30 35 1 5 10 15 20 query interval length a qtlen=10, count b qrlen=0.15, count relative error query rectangle length 0 5 10 15 20 25 0.05 0.1 0.15 0.2 0.25 relative error query interval length 0 5 10 15 20 25 30 1 5 10 15 20 c qtlen=10, sum d qrlen=0.15, sum Figure 5.3: Accuracy of the approximate results The same phenomena are confirmed in Figures 5.3b where we fix qrlen to 0.15 and vary qtlen 5.3d \(results for sum-workloads number of sketches improves the estimation accuracy, it also leads to higher space requirements \(as shown in Figure 5.1 Figures 5.4a and 5.4b show the number of disk accesses for the settings of Figures 5.3a and 5.3b. All indexes have almost the same behavior, while the 32-sketch is clearly more expensive than the other two indexes. The interesting observation is that 8- and 16-sketches have 


interesting observation is that 8- and 16-sketches have almost the same overhead due to the similar heights of their B-trees. Since the diagrams for sum-workloads illustrate \(almost avoid redundancy 32-sketch 16-sketch 8-sketch number of disk accesses query rectangle length 0 50 100 150 200 250 300 350 400 0.05 0.1 0.15 0.2 0.25 number of disk accesses query interval length 0 50 100 150 200 250 300 350 1 5 10 15 20 a qtlen=10 b qrlen=0.15 Figure 5.4: Costs of indexes with various signatures Summary: The sketch index constitutes an effective method for approximate spatio-temporal \(distinct aggregate processing. Particularly, the best tradeoff between space, query time, and estimation accuracy obtained by 16 sketches, which leads to size around 40 the database, fast response time \(an order of magnitude faster than the relational method average relative error 5.2 Approximating spatio-temporal data We proceed to study the efficiency of using sketches to approximate spatio-temporal data \(proposed in Section 4.1 as in the last section, except that at each timestamp all airplanes report their locations to a central server \(instead of their respective nearest bases maintains a table in the form &lt;time t, plane p, x, y&gt;, where x,y with parameters qrlen and qtlen distinct planes satisfying the spatial and temporal conditions. For comparison, we index the table using a 3D R*-tree on the columns time, x, and y. Given a query, this tree facilitates the retrieval of all qualifying tuples, after which a post-processing step is performed to obtain the Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE number of distinct planes \(in the sequel, we refer to this method as 3DR method introduces a regular res  res grid of the data space, where the resolution res is a parameter. We adopt 16 sketches because, as mentioned earlier, this number gives the best overall performance Figure 5.5 compares the sizes of the resulting sketch indexes \(obtained with resolutions res=25, 50, 100 the database size. In all cases, we achieve high compression rate \(e.g., the rate is 25% for res=25 evaluate the query efficiency, we first set the resolution to the median value 50, and use the sketch index to answer workloads with various qrlen \(qtlen=10 


workloads with various qrlen \(qtlen=10 size \(mega bytes database size 0 20 40 60 80 100 120 140 160 25 50 100 resolution Figure 5.5: Size reduction Figure 5.6a shows the query costs \(together with the error in each case method. The sketch index is faster than 3DR by an order of magnitude \(note that the vertical axis is in logarithmic scale around 15% error observations using workloads with different qtlen Finally, we examine the effect of resolution res using a workload with qrlen=0.15 and qtlen=10. As shown in Figure 5.6c, larger res incurs higher query overhead, but improves the estimation accuracy Summary: The proposed sketch method can be used to efficiently approximate spatio-temporal data for aggregate processing. It consumes significantly smaller space, and answers a query almost in real-time with low error 3D Rsketch number of disk accesses query rectangle length 1 10 100 1k 10k 0.05 0.1 0.15 0.2 0.25 16 14% 15 15% 13 relative error number of disk accesses query interval length 1 10 100 1k 10k 1 5 10 15 20 16 15% 15% 12% 11 relative error a qtlen=10, res=25 b qrlen=0.15, res=25 0 500 1000 1500 2000 2500 25 50 100 number of disk accesses resolution 20% 15% 14 relative error c qrlen=0.15, qtlen=10 


c qrlen=0.15, qtlen=10 Figure 5.6: Query efficiency \(costs and error 5.3 Mining association rules To evaluate the proposed algorithm for mining spatiotemporal association rules, we first artificially formulate 1000 association rules in the form \(r1,T,90 with 90% confidence i randomly picked from 10k ones, \(ii in at most one rule, and \(iii Then, at each of the following 100 timestamps, we assign 100k objects to the 10k regions following these rules. We execute our algorithms \(using 16 sketches these rules, and measure \(i  correct  rules divided by the total number of discovered rules, and \(ii successfully mined Figures 5.7a and 5.7b illustrate the precision and recall as a function of T respectively. Our algorithm has good precision \(close to 90 majority of the rules discovered are correct. The recall however, is relatively low for short T, but gradually increases \(90% for T=25 evaluated in the previous sections, the estimation error decreases as the query result becomes larger \(i.e., the case for higher T 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 precision HT 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 recall HT a b Figure 5.7: Efficiency of the mining algorithm Summary: The preliminary results justify the usefulness of our mining algorithm, whose efficiency improves as T increases Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 6. Conclusions While efficient aggregation is the objective of most spatio-temporal applications in practice, the existing solutions either incur prohibitive space consumption and query time, or are not able to return useful aggregate results due to the distinct counting problem. In this paper we propose the sketch index that integrates traditional approximate counting techniques with spatio-temporal indexes. Sketch indexes use a highly optimized query algorithm resulting in both smaller database size and faster query time. Our experiments show that while a sketch index consumes only a fraction of the space required for a conventional database, it can process 


required for a conventional database, it can process queries an order of magnitude faster with average relative error less than 15 While we chose to use FM sketches, our methodology can leverage any sketches allowing union operations Comparing the efficiency of different sketches constitutes a direction for future work, as well as further investigation of more sophisticated algorithms for mining association rules. For example, heuristics similar to those used for searching sketch indexes may be applied to improve the brute-force implementation ACKNOWLEDGEMENTS Yufei Tao and Dimitris Papadias were supported by grant HKUST 6197/02E from Hong Kong RGC. George Kollios, Jeffrey Considine and were Feifei Li supported by NSF CAREER IIS-0133825 and NSF IIS-0308213 grants References BKSS90] Beckmann, N., Kriegel, H., Schneider, R Seeger, B. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD, 1990 CDD+01] Chaudhuri, S., Das, G., Datar, M., Motwani R., Narasayya, V. Overcoming Limitations of Sampling for Aggregation Queries. ICDE 2001 CLKB04] Jeffrey Considine, Feifei Li, George Kollios John Byers. Approximate aggregation techniques for sensor databases. ICDE, 2004 CR94] Chen, C., Roussopoulos, N. Adaptive Selectivity Estimation Using Query Feedback. SIGMOD, 1994 FM85] Flajolet, P., Martin, G. Probabilistic Counting Algorithms for Data Base Applications JCSS, 32\(2 G84] Guttman, A. R-Trees: A Dynamic Index Structure for Spatial Searching. SIGMOD 1984 GAA03] Govindarajan, S., Agarwal, P., Arge, L. CRBTree: An Efficient Indexing Scheme for Range Aggregate Queries. ICDT, 2003 GGR03] Ganguly, S., Garofalakis, M., Rastogi, R Processing Set Expressions Over Continuous Update Streams. SIGMOD, 2003 HHW97] Hellerstein, J., Haas, P., Wang, H. Online Aggregation. SIGMOD, 1997 JL99] Jurgens, M., Lenz, H. PISA: Performance Models for Index Structures with and without Aggregated Data. SSDBM, 1999 LM01] Lazaridis, I., Mehrotra, S. Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure. SIGMOD 2001 PGF02] Palmer, C., Gibbons, P., Faloutsos, C. ANF A Fast and Scalable Tool for Data Mining in Massive Graphs. SIGKDD, 2002 PKZT01] Papadias,  D., Kalnis, P.,  Zhang, J., Tao, Y Efficient OLAP Operations in Spatial Data Warehouses. SSTD, 2001 PTKZ02] Papadias, D., Tao, Y., Kalnis, P., Zhang, J Indexing Spatio-Temporal Data Warehouses ICDE, 2002 SJLL00] Saltenis, S., Jensen, C., Leutenegger, S Lopez, M.A. Indexing the Positions of Continuously Moving Objects. SIGMOD 2000 SRF87] Sellis, T., Roussopoulos, N., Faloutsos, C The R+-tree: A Dynamic Index for MultiDimensional Objects. VLDB, 1987 TGIK02] Thaper, N., Guha, S., Indyk, P., Koudas, N Dynamic Multidimensional Histograms 


SIGMOD, 2002 Tiger] www.census.gov/geo/www/tiger TPS03] Tao, Y., Papadias, D., Sun, J. The TPR*Tree: An Optimized Spatio-Temporal Access Method for Predictive Queries. VLDB, 2003 TPZ02] Tao, Y., Papadias, D., Zhang, J. Aggregate Processing of Planar Points. EDBT, 2002 TSP03] Tao, Y., Sun, J., Papadias, D. Analysis of Predictive Spatio-Temporal Queries. TODS 28\(4 ZMT+01] Zhang, D., Markowetz, A., Tsotras, V Gunopulos, D., Seeger, B. Efficient Computation of Temporal Aggregates with Range Predicates. PODS, 2001 ZTG02] Zhang, D., Tsotras, V., Gunopulos, D Efficient Aggregation over Objects with Extent PODS, 2002 Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE pre></body></html 


