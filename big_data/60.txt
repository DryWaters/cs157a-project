Evaluation of Sampling for Data Mining of Association Rules  Mohammed Javeed Zaki, Srinivasan Parthasarathy Wei Li Mitsunori Ogihara Computer Science Department University of Rochester, Rochester NY 14627 zaki,srini,wei,ogihara}@cs.rochester.edu Abstract Discovery of association rules is a prototypical problem in data mining The current algorithms proposed for data mining of association rules make repeated passes over the database to determine the commonly occurring itemsets or set of 
items For large databases, the i/O overhead in scan ning the database can be extremely high in this paper we show that random sampling of transactions in the database is an effective method for finding association rules Sam pling can speed up the mining process by more than an order of magnitude by reducing U0 costs and drastically shrinking the number of transactions to be considered We may also be able to make the sampled database resident in 
main-memory Furthermore, we show that sampling can accurately repre sent the data patterns in the database with high confidence We experimentally evaluate the effectiveness of sampling on different databases, and study the relationship between the pelformance and the accuracy and confidence of the chosen sample 1 Introduction With large volumes of routine business data having been collected business organizations are increasingly turning to the extraction of useful information from such databases Such high-level inference process may provide information on customer buying patterns, shelving criterion in supermar kets stock trends, etc Data mining 
is an emerging research area whose goal is to extract significant patterns or interest ing rules from such large databases It combines research in machine learning, statistics and databases In this paper we will concentrate on the discovery of association rules The problem of mining association rules over basket data was introduced in l Basket data usually consists of a record per customer with a transaction date along with items bought by the customer The main computation step consists of finding the frequently occurring item sets 
via an iterative This work was supported in part by an NSF Research Initiation Award CCR-9409120 and ARPA contract F19628-94-C-0057 process In the k-th scan of the database all frequent items sets of length k are obtained For disk resident databases the I/O overhead in scanning the database during each iteration can be extremely high for large databases Random sampling from databases has been successfully used in query size estimation Such information can be used for statistical analyses 
of databases, where approxi mate answers would suffice It may also be used to estimate selectivities or intermediate result sizes for query optimiza tion  1 11 In the context of association rules, sampling can be utilized to gather quick preliminary rules. This may help the user to direct the data mining process by refining the criterion for \223interesting\224 rules In this paper we show that random sampling of transac tions in the database is an effective way for finding associa tion rules We empirically compare theory and experimen tation present results on the percentage 
of errors and correct rules derived at different sampling values the performance gains and also the relationship between performance, accu racy and confidence of the sample size More specifically we make the following contributions Sampling can reduce I/O costs by drastically shrink ing the number of transaction to be considered We show that sampling can speed up the mining process by more than an order of magnitude Sampling can provide great accuracy with respect to the association rules We show that the theoretical results \(using Chernoff 
bounds\are extremely conser vative and that experimentally we can obtain much better accuracy for a given confidence or we can do with a smaller sample size for a given accuracy We begin by formally presenting the problem of finding association rules in section 2 Section 3 presents an analy sis of random sampling from databases The effectiveness of sampling is experimentally analyzed in section 4 and section 6 presents our conclusions 42 0-8186-7849-6/97 $10.00 0 1997 IEEE 


2 Data mining for association rules We now present the formal statement of the problem of mining association rules over basket data The discussion below closely follows that in I 31 Let Z  il,i2     im be a set of m distinct attributes also called items A set of items is called an itemset and an itemset with IC items is called a k-itemset Each transaction T in the database V of transactions has a unique identi fier TID and contains a set of items such that T c Z An association rule is an expression A B where item sets A B C Z and A n B  0 Each itemset is said to have a support s if s of the transactions in 2 contain the itemset The association rule is said to have conjidence c if c of the transactions that contain A also contain B i.e c  support\(A U B A i.e., the conditional probability that transactions contain the itemset B given that they contain itemset A The data mining task for association rules can be broken into two steps The first step consists of finding all large itemsets i.e itemsets that occur in the database with a certain user-specified frequency, called minimum support The second step consists of forming implication rules among the large itemsets 3 In this paper we only deal with the computationally intensive first step Many algorithms for finding large itemsets have been proposed in the literature l 7 3 10, 12 6 13 21 In this paper we will use the Apriori algorithm to evaluate the effectiveness of sampling for data mining We chose Apriori since it fast and has excellent scale-up properties We would like to observe that our results are about sampling and as such independent of the mining algorithm used 2.1 The Apriori algorithm The naive method of finding large itemsets would be to generate all the 2m subsets of the universe of m items count their support by scanning the database and output those meeting minimum support criterion It is not hard to see that the naive method exhibits complexity exponential in m and is quite impractical Apriori follows the basic iterative structure discussed earlier However the key obser vation used is that any subset of a large itemset must also be large In the initial pass over the database the support for all single items \(1-itemsets\is counted During each iteration of the algorithm only candidates found to be large in the previous iteration are used to generate a new candidate set to be counted during the current iteration A pruning step eliminates any candidate which has a small subset Apriori also uses specialized data structures to speed up the count ing and pruning hash trees and hash tables, respectively The algorithm terminates at step t if there are no large t itemsets Let Lk denote the set of Large IC-itemsets and Ck  Lk-lXLk-1 the set of candidate k-itemsets The general structure of the algorithm is given in figure 1 We refer the reader to  for more detail on Apriori and its performance characteristics L1  large 1-itemsets  for IC  2 Lk-1  0 IC   ck  Set of New Candidates for all transactions t E 2 for all k-subsets s oft if s E ck s.count   Set of all large itemsets  Uk Lk Lk  c E Cklc.count 2 minimumsupport Figure 1 The Apriori algorithm We now present a simple example of how Apriori works Let the database 2  TI  1,4,5  1,2  3,4,5  1,2,4,5 Let theminimumsupportvalue MS  2 Running through the iterations we get Note that while forming C3 by joining L2 with itself we get three potential candidates  1,2,4 1,2,5}, and  1,4,5 However only  1,4,5 is a true candidate and the first two are eliminated in the pruning step since they have a 2 subset which is not large \(the 2-subset 2,4 and 2,5 respectively 3 Random sampling for data mining Random sampling is a method of selecting n units out of a total N such that every one of the C distinct samples has an equal chance of being selected In this paper we consider sequential random sampling without replacement i.e the records are selected in the same order as they appear in the database and a drawn record is removed from further consideration 3.1 Sampling algorithm For generating samples of the database we use the Method A algorithm presented in  151 which is simple and very efficient for large sample size n A simple algorithm for sampling generates an independent uniform random vari ate for each record to determine whether that record should be chosen for the sample If m records have been chosen 43 


from the first t records then the next record will be cho sen with the probability n  m N  t This algorithm called Method S SI generates O\(N random variates and also runs in O\(N time Method A significantly speeds up the sampling process by efficiently determining the number of records to be skipped over before the next one is chosen for the sample. While the running time is still O\(N only n random variates are generated \(see  151 for more details 3.2 Chernoff bounds Let 7 denote the support of an itemset I We want to select n transactions out of the total N in the Database D Let the random variable Xi  1 if the i-th transac tion contains the itemset I Xi  0 otherwise Clearly P\(Xi  I  T for i  1,2,...n  We further assume that all XI X2    X are independent 0 1 random vari ables The random variable X giving the number of trans actions in the sample containing the itemset I has a bi nomial distribution of n trials with the probability of suc cess T note the correct distribution for finite populations is the Hypergeometric distribution although the Binomial distribution is a satisfactory approximation Moreover X  CYX and the expected value of X is given as p  E[X  EICY=l Xi  Cy E  71.7 since For any positive constant 0 5 E 5 1 the Chernoff E[X  0 P\(X  0  1  P\(X  1  7 bounds 5 state that Chernoff bounds provide information on how close is the actual occurrence of an itemset in the sample as compared to the expected count in the sample This aspect which we call as the accuracy of a sample is given by 1  E The bounds also tell us the probability that a sample of size n will have a given accuracy We call this aspect the confidence of the sample defined as 1 minus the expression on the right hand size of the equations Chernoff bounds give us two set of confidence values Equation 1 gives us the lower bound  the probability that the itemset occurs less often than expected  by the amount nm while equation 2 gives us the upper bound  the probability that the itemset occurs more often than expected, for a desired accuracy A low probability corresponds to high confidence and a low E corresponds to high accuracy It is not hard to see that there is a trade-off between accuracy and confidence for a given sample size This can been seen immediately since E  0 maximizes the right hand side of equations 1,2 while E  1 minimizes it 3.3 Sample size selection Given that we are willing to accommodate a certain ac curacy d  1  E and confidence C  1  c of the sample the Chernoff bounds can be used to obtain a sample size We\222ll show this for equation 1 by plugging in c  e--E2nT/2 to obtain n  21n\(c 7e2 3 If we know the support for each itemset we could come up with a sample size n1 for each itemset I We would still have the problem of selecting a single sample size from among the n One simple heuristic is to use the user specified minimum support threshold for T The ra tionale is that by using this we guarantee that the sam ple size contains all the large itemsets contained in the original database For example, let the total transactions in the original database N  3,000,000 Let\222s say we desire a confidence C  0.9\(c  O.l and an accuracy d  0.99  0.01 Let the user specified support thresh old be 1 Using these values in equation 3 we obtain a sample size of n  4,605 170 This is even greater than the original database! The problem is that the sample size expression is independent of the original database size Moreover the user specified threshold is also independent of the actual itemset support in the original database Hence using this value may be too conservative as shown above In the next section we will compare experimental results obtained versus the theoretical predictions using Chernoff bounds 4 Experimental evaluation In this section we describe the experiments conducted in order to determine the effectiveness of sampling We demonstrate that it is a reasonably accurate technique in terms of the associations generated by the sample as com pared to the associations generated by the original database At the same time sampling can help reduce the execution time by more than an order of magnitude 4.1. Experimental framework All experiments were conducted on a 233MHz DEC Al phaserver 2100 processor with 256MB of main memory The databases are stored on an attached 2GB disk and data is obtained from the disk via an NFS file server We used four different databases to evaluate the effectiveness of sam pling. These are 0 SYNTHSOO SYNTH250 These are synthetic databases which mimic the transactions in a retailing environment They have been used as benchmark databases for many as sociation rules algorithms 3,6 12 13,2 Each transaction 44 


3500 3000 2500 3 c  L U 2000 5 1500 1000 500 L a 5 z o 0 SYNTHBOO minimum support  0.25 ORlG  SAMPO.5  ACT0.5 o SAMPl  ACT1qo A SAMPS ACT5  SAMP10  ACTIO e 1 SYNTH250 minimum support  0.50 8001 0 8 I I  I  I 700 SAMPO.5  600 500 400 ACT10 e 300 200 100 0 ACTO.5 a      ________ SAMP10  12 3 4 5 6 7 8 9 10 12345.678910 ltemset Size k ltemset Size fk 1200 1000 Y  E 800 L 600 I 3 5  5 400 n 200 ENROLL minimum support  1.0 ORlG  A SAMPI  ACT1 w ACT5 A I SAMP10 I i ACT1 0  i SAMP5      7000 6000 5000 L p 4000 3 I c  P 2 3000 s 5 a 2000 1000 z TRBIB minimum su~port  1.5 has a unique ID followed by a list of items bought in that transaction We obtained the database T10.16.D800K by setting the number of transactions ID1  800,000 average transaction size IT1  10, average maximal potentially large itemset size 11  6 For T10.14.D250K 1231  250000 TI  10 111  4 For both databases the number of max imal potentially large itemsets ILI  2000 and the number of items N  1000 We refer the reader to for more detail on the database generation 0 ENROLL This is a database of student enrollments for a particular graduating class Each transaction consists of a student ID followed by information on the college major department, semester and a list of courses taken during that semester There are 39624 transactions, 3581 items and the average transaction size is 9 0 TRBIB This is a database of the locally available techni cal report bibliographies in computer science. Each item is a key-word which appears in a paper title and each transaction has a unique author ID followed by a set of such key-words items There are 13793 transactions, 10363 items and the average transaction size is 22 4.2 Accuracy measurements We report experimental results for the databases de scribed above Figure 2 shows the number of large itemsets found during the different iterations of the Apriori algo rithm, for the different databases and sample size In the graphs ORIG indicates the actual number of large item sets generated when the algorithm operates on the entire database SAMPx refers to the large itemsets generated when using a sample of size 2 of the entire database ACTx refers to the number of itemsets generated by SAMPz that are true large itemsets in the original database The number of false large itemsets is given as SAMPx  ACTx From figure 2 we can observe that the general trends of sampled databases resemble actual results. Smaller sam ple sizes tend to over-estimate the number of large itemsets i.e they find more false large itemsets. On the other hand larger sample sizes tend to give better results in terms of fi delity or the number of true large itemsets. This is indicated by the way ACT.a comes closer to ORIG as z the sample percentage is increased More detailed results are shown in figure 3, which shows the percentage of true and false itemsets generated for dif ferent values of sampling and minimum support The values of minimum support were chosen so that there were enough large k-itemsets, for k  2 For example, for SYNTH800 and SYNTH250, only large 1-itemsets were found at sup port more than 1  Therefore only support values less than those were considered Furthermore, support values were 45 


m c I  E P 5  a 1 a c 3 m  I  E t  a e 1 3 s  E  a P 5  0 z m U  c E  a 1 a k P 2 z  90 85 80 75    D    __ U W   x  xx  x  q%&mpkng      i _.__._---A.--._____ 0.5 Sampling  98  94 96 94 92 90     t 92  90  88  e  m  5 Sampling  tOo Sampling 4  2~haTpling   I TI i            86l         0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Minimum Support \(Percentage ENROLL 100 98 96 94 92 90 J 12345678 Minimum Support \(Percentage SYNTHBOO 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Minimum Support \(Percentage SYNTH250 0 5 Sampling  1 Sampling  60 50 40 30 20 10 n  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Minimum Support \(Percentage 5 Sampling  10 Sampling e 70 25 Sampling   30 20 10 0 0 12 3 4 5 6 7 8 9 10 Minimum Support \(Percentage _ TRBIB 100 I I gOl 80 i 5 Sampling  10 Sampling  25% Sampling  io  12345678 Minimum Support \(Percentage Figure 3  of True and false large itemsets vs threshold for various sample sizes 46 


chosen so that we don't generate too many large itemsets For example, for ENROLL at 1 sampling size we get a sample of 396 transactions. For support of OS we must find all itemsets which occur at least 2 times in effect find ing all possible large itemsets Thus only support values greater than 0.5 were used The figure shows that at higher sampling size we generate a higher percentage of true large itemsets and a smaller number of false large itemsets. It is interesting to note that in all cases we found more than 80 of all the large itemsets We further observe that for other than very small sampling size we can keep the false large itemsets under 20 4.3 Performance Figure 4 shows the speedup obtained for the databases on different minimum support and different sampling size Val ues. The speedup is relative to the algorithm execution time on the entire database For SYNTH800 we obtain a speedup of more than 20 at small sample size and high support. For SYNTH250 we get more than 10 speedup in the same range The performance at lower support is poor due to the large number of false large itemsets found At higher sampling we get lower performance, since the reduction in database U0 is not that significant and due to the introduction of more inaccuracies For the smaller databases ENROLL and TRBIB at small sample size we get no speedup, due to the large number of false large itemsets generated We can observe that there is a trade-off between sampling size minimum support and the performance The performance gains are negated due to either a large number of false large itemsets at very low support or due to decreased gains in U0 vs computation We can conclude that in general sampling is a very effective technique in terms of performance and we can expect it to work very well with large databases, as they have higher computation and U0 overhead 4.4 Confidence comparison with chernoff bounds In this section we compare the Chernoff bound with experimentally observed results We show that for the databases we have considered the Chernoff bound is very conservative Consider equations 1 and 2 For different values of accuracy, and for a given sampling size, for each itemset I we can obtain the theoretical confidence value by simply evaluating the right hand side of the equations For example for the upper bound the confidence C  1  e-E2nr/3 Recall that confidence provides information about an item's actual support in the sample being away from the expected support by a certain amount n~e We can also obtain experimental confidence values as follows We take s samples of size n and for each item we compute the confidence by evaluating the left hand side of the two equations, as follows Let z denote the sample number 1 5 z 5 s Let 1 if\(m  X 2 nn in sample z 1 if\(X  n 2 nr6 in sample z 0 otherwise The confidence can then be calculated as 1  E hl\(z for the upper bound and 1  E ZI\(z for the lower bound SYNTH Probability Distribution of I-itemsets Exoerimental Chemoff 1401 I 99.8 99.9 I Probabill Dist Probabilitv Dist Figure 5 Probability distribution: experiment vs chernoff Figure 5 compares the distribution of experimental confi dence to the one obtained by Chernoff upper bounds, for all m 1-itemsets or single items. It is possible \(though imprac tical to do this analysis for all the 2 itemsets however we present results for only single items. This should give us an indication whether the sample faithfully represents the orig inal database The results shown are for the SYNTH250 database with E  0.01 n  2500 1 of total database size and the number of samples taken s  100 We can see that the probability distribution across all items varies from 0.30 to 0.60 for the experimental case with a mean probability close to 0.43 The Chernoff bounds produce a distribution clustered between 0.998 and 1 O with an aver age probability of 0.9992. Chernoff bounds indicate that it is very likely that the sample doesn't have the given accuracy i.e with high probability the items will be overestimated by a factor of 1.01 However in reality the probability of being over-estimated is only 0.43 The obvious difference in confidence depicts the limitation of Chernoff bounds in 47 


25 20 SYNTHEOO 0.1 Minimum Support  0.25 Minimum Support  0.5 Minimum Support B 0.75 Minimum Support   01 I 0 5 10 15 20 25  Sampling ENROLL 7 q 4    0.25 Minimum Support  0.5 Minimum Support  0.75 Minimum Support E 1 Minimum Support    5 Minimum Support   10 Minimum Support   25% Minimum Support   0 5 10 15 20 25  Sampling SYNTHEOO Upper Bound SYNTH250 12 31  0 5 10 15 20 25  Sampling TRBIB 7 I 1.5 Minimum Support  2.5 Minimum Support   7.5 Minimum Support  6   5 Minimum Support  5 1 5 10 15 20 25  Sampling Figure 4 Sampling performance 0 0.05 0.1 0.15 0.2 0.25 0.3 Epsilon  1 Accuracy ENROLL Upper Bound        n  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Epsilon  1 Accuracy SYNTH250 Upper Bound I ann I_ T.1  E.1  T in m I uu 80 60 40 20 n  30  40 o E.40  0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Epsilon  1 Accuracy TRBIB Upper Bound   I   __   0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Epsilon  1 Accuracy Figure 6 Accuracy vs mean confidence for single items 48 


this setting This was observed in all of the databases we looked at Figure 6 gives a broader picture of the large gap between Chernoff bounds and experimentally obtained effectiveness of sampling. For all four databases we plot the mean of the confidence or the probability distribution for different accu racies 1  E The mean confidence obtained from Chernoff bounds is marked as T.s and that obtained experimentally is marked as Ex Different values of the sample size z are plotted from 1 to 50 and results for only the upper bound are shown. For all the databases the upper and lower bounds give similar results There is a small difference in the Chernoff bound values due to the asymmetry in equa tions l and 2. This is also true for the experimental results For both cases the lower bounds give a slightly higher con fidence for the same value of accuracy as expected from the Chernoff bounds For SYNTH800 and SYNTH250 we observe that as the accuracy is compromised \(as e increases\the mean confi dence across all items increases exponentially \(therefore only E values upto 0.5 are shown\Furthermore, as the sam ple size increases the curve falls more rapidly so that we have higher confidence even at relatively higher accuracies For SYNTHSOO we get higher confidence for higher accu racy when compared to SYNTH250 For both ENROLL and TRBIB we get the same general trends however the increase in confidence for lower accuracies is not as rapid This is precisely what we expect For example, consider the right hand side of Chernoff upper bounds \(equation 2 e-c2nT/3  C For a given E and T the support for an item a higher value of n gives us high confidence as it results in a lower value for C For a given sampling percentage since SYNTH800 and SYNTH2.50 are large we expect a higher confidence than that for ENROLL or TRBIB for example with sampling  lo E  0.1 and T  0.01 we get n  80000 C  0.07 for SYNTHSOO n  25000 C  0.43 for SYNTH250 n  3962 C  0.88 for EN ROLL; and n  1379 C  0.96 for TRBIB We get the same effect for the experimental results We can observe that for all the databases the experimen tal results predict a much higher confidence, than that using Chernoff bounds. Furthermore, from the above analysis we would expect sampling to work well for larger databases The distribution of the support of the itemsets in the original database also influences the sampling quality 5 Related Work Many aIgorithms for finding large itemsets have been pro posed in the literature since the introduction of this problem in  13 AIS algorithm The Apriori algorithm reduces the search space effectively by using the property that any subset of a large itemset must itself be large The DHP algorithm uses a hash table in pass k to do efficient pruning of k  1 to further reduce the candidate set. The Partition algorithm  131 minimizes I/O by scanning the database only twice In the first pass it generates the set of all potentially large itemsets and in the second pass their support is obtained. Algorithms using only general-purpose DBMS systems and relational algebra operations have also been proposed 6,7 A theoretical analysis of sampling using Chernoff bounds for association rules was presented in 2 101 We look at this problem in more detail empirically and com pare theory and experimentation. In 8 the authors compare sample selection schemes for data mining They make a claim for collecting the sample dynamically in the context of the subsequent mining algorithm to be applied A recent paper presents an association rule mining algorithm using sampling. A sample of the database is obtained and all association rules in the sample are found. These results are then verified against the entire database The results are thus exact and not approximations based on the sample They also use Chernoff bounds to get sample sizes and low ered minimum support values for minimizing errors Our work is complementary to their approach and can help in determining a better support value or sample size We also show results on the percentage of errors and correct rules derived at different sampling values the performance gains and also the relationship between performance accuracy and confidence of the sample size 6 Conclusions We have presented experimental evaluation of sampling for four separate databases to show that it can be an effective tool for data mining. The experimental results indicate that sampling can result in not only performance savings such as reduced I/O cost and total computation but also good accu racy with high confidence in practice, in contrast to the con fidence obtained by applying Chernoff bounds However we note that there is a trade-off between the performance of the algorithm and the desired accuracy or confidence of the sample. A very small sample size may generate many false rules and thus degrade the performance With that caveat we claim that for practical purposes we can use sampling with confidence for data mining References  11 R Agrawal T Imielinski and A Swami. Mining association rules between sets of items in large databases In ACM SIGMOD Intl Con5 Management of Data May 1993 2 R Agrawal H Mannila R Srikant H Toivonen and A I Verkamo Fast discovery of association rules In Advances in Knowledge Discovery and Data Mining U Fayyad G 49 


Piatetsky-Shapiro l Smyth R Uthurusamy Eds AAAI Press, Melo Park, CA 1996  R Agrawal and R Srikant Fast algorithms for mining asso ciation rules In 20th VLDB Conference Sept. 1994 4 W G Cochran Sampling Techniques John Wiley  Sons 1977 5 T Hagerup and C Rub A guided tour of chernoff bounds In Information Processing Letters pages 305-308 North Holland 1989/90 6 M Holsheimer M Kersten H Mannila, and H Toivonen A perspective on databases and data mining In 1st Intl Conj Knowledge Discovery and Data Mining Aug. 1995 7 M Woutsma and A. Swami Set-oriented mining of associa tion rules In RJ 9567 IBM Almaden, Oct 1993 SI G John and P Langley Static versus dynamic sampling for data mining In 2nd Intl Con8 Knowledge Discovery and Data Mining Aug 1996 9 D E. Knuth The Art of Computer Programming Volume 2 Seminumerical Algorithms Addison-Wesley 198 1 IO H Mannila H Toivonen, and 1 Verkamo Efficient algo rithms for discovering association rules In AAAI Wkshp Knowledge Discovery in Databases July 1994 Ill E Olken and D Rotem Random sampling from database files  a survey In 5th Intl Conj Statistical and Scientific Database Management Apr. 1990 12 J S Park M Chen, and P S Yu An effective hash based algorithm for mining association rules In ACM SIGMOD lntl Con Management of Data May 1995 13 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases In 21st VLDB Conference 1995  141 H Toivonen Sampling large databases for association rules In 22nd VLDB Conference 1996 15 J S Vitter An efficient algorithm for sequential random sampling In ACM Trans Mathematical Software volume 13 I pages 58-67 Mar 87 50 


Figure 5 Notional uncertainties with uniform and nonuni form sampling In the former case the uncer tainties \(illustrated by covariance ellipses are rel atively constant in size In the latter nonuni form case the ellipses increase significantly in size between pairs of samples and allow more false alarms to enter Figure 6 Representation of the one-step tracking as per formed by the PDAF Figure 7 Procedure of track split filter a Uniform sampling scheme  Nom sampling shew 0.W OW 0.W 0.WB 0.01 0.012 0.014 0.016 0.018 Fabe Alarm Density 2 Figure 8 Uniform and nonuniform sampling schemes in PDAF with Tl  O.ls T2  1.9s and T  1s PD  0.9 4  10 0  1 4-1606 


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


