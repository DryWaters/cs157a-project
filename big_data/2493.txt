Mining Frequent Closed Itemsets from Distributed Dataset  Chunhua Ju                                                                            Dongjun Ni College of Computer and Information College of Computer and Information Engineering Zhejiang Gongshang University Engineering Zhejiang Gongshang University Hangzhou, Zhejiang 310018 China                                   Hangzhou, Zhejiang, 310018 China juchunhua@hotmail.com                                                       godndj@126.com  Abstract  In this paper we address the problem of mining frequent closed itemsets in a highly distributed setting The extraction of distributed frequent \(close\ itemsets is an important task in data mining. The paper shows how frequent closed itemsets 002 mined  independently in each site, can be merged in order to derive globally frequent closed itemsets. Unfortunately, as distributed setting is various, it is unreasonable to adopt only one mining approach. The paper analyzes the distributed setting from three perspectives: 1.communication bandwidth; 2.site quantity; 3 the characteristic of each site dataset, and presents two mining approaches and their algorithms in their corresponding distributed setting. The experiment results indicate that our algorithms are efficient to their corresponding distributed setting  1. Introduction  Frequent Itemset Mining \(FIM\ is the most important and demanding task in many data mining applications. It requires to discover all the itemsets in a given database D which have a support higher \(or equal\ than a given minimum frequency threshold minsup. The FIM problem has been extensively studied in the last years. The first proposed algorithm is Apriori d m a ny di ff eren t approach es h a v e bee n  proposed such as DepthProject P G R O W T H  d m a ny ot h e rs O n e of t h e m a i n i s su es e m erg i ng from these studies regards the size of the collection of frequent itemsets F while decreasing of the minimum support thresholds. It makes the mining task very hard at very low support thresholds Frequent closed itemsets mining \(FCIM\ is a solution to this problem. Frequent Closed itemsets is a small subsets of frequent itemsets, but they represent the same knowledge in a more succinct way. Using closed itemsets, we implicitly benefit from data correlations which allow to strongly reduce problem complexity. Moreover association rule extracted from closed sets have been proved to be more concise and meaningful, because all re dundancies are discarded Currently many efficient frequent closed itemsets mining\(FCIM\ algorithms have been proposed, such as A-CLOS H A R M [5], CL OS ET 6  While many papers address the problem of FCIM or parallel/distributed FIM, there are few proposals for distributed closed itemsets mining exists. In this paper we address the problem of mining frequent closed itemsets in a distributed dataset. In order to satisfy the mining task under the real distributed setting, we consider of two different distributed settings.  One of different distributed settings has few sites or has small bandwidth or store huge data in site dataset, the other is reverse. We have presented a distributed mining approach respectively in term of two different distributed settings  2. Basic Conceptions  2.1 Frequent and Closed Itemsets  Given a dataset D, D={t 1 t 2 t n contains a finite set of transactions. Let I={a 1 a 2 a n be a finite set of items where each transaction t is a subset of I. Given a itemset X, let sup\(X\ be its support, defined as the number of transactions in D that include X, we call X is frequent itemsets if sup\(X 002 minsup  DB, where 0<minsup 003 1 is a given minimum support threshold To define the conception of closed itemset, we use description of referen i v e s t w o f o llo w i n g  functions f and s           f TiItTit g AtDiAit 004 005\004 004 004 005\004 004  Where TD 006 and A I 006 function f returns the set of items which appears in all the transactions of T, while function g returns the set of transactions which supports a given itemset A Definition 1 An itemset X is closed if and only if     cX f gX f gX X 007   
2008 International Symposium on Computational Intelligence and Design 978-0-7695-3311-7/08 $25.00 © 2008 IEEE DOI 10.1109/ISCID.2008.24 37 
2008 International Symposium on Computational Intelligence and Design 978-0-7695-3311-7/08 $25.00 © 2008 IEEE DOI 10.1109/ISCID.2008.24 37 


Where the composite function c f g  is called Galois operator or closure operator Definition 2 if an itemset X is both closed and frequent, then X is said to be frequent closed itemsets Lemma 1 given a dataset D, there is one and only one frequent closed itemset which include frequent itemset X and have the same support with X From Lemma 1, it is easily getting a principle to judging a itemset is closed or not. The principle is as below Principle 1 we assume itemset X and Y are both frequent, if XY  and sup\(X\ =sup\(Y\hen X is not frequent closed itemset  2.2 Distributed Frequent Closed Itemsets Mining  In the distributed system, there are several data sources i D i=1,2,…,n\d each i D are partitioned on one site i S i=1,2,…,n\,for the sake of discussing ,we give a virtual dataset  i D D  for any closed item X X.sup is the support of X in D and X.sup i is the support of X  in i D  Definition 3 given a closed itemset X if sup minsup X D  X is said to be globally frequent closed itemset; if  sup min sup i i X D  X is locally frequent closed itemset on site i S  Lemma 2 if X is globally frequent closed itemset, and then X must be locally frequent in at least one site i S  Proof. Suppose X is not locally frequent closed on any site, then  sup min sup i i X D  we can get 11 1 1 sup minsup sup sup min sup min sup nn i i ii n i i n i i X D XX D D         So sup minsup X D  X would not be a frequent closed itemset, and this is in contradiction with the hypothesis Lemma 3 there are n sites in the distributed system L i  1 in  e locally FCI on site S i  1 in    1 n ii L L   then  L must be the supersets of the global frequent closed itemsets L  Proof: it can be proved easily by Lemma 2 The main task of mining frequent closed itemset from distributed dataset is: firstly parallel mining locally frequent closed itemsets on every site i S  Secondly merge the local result to derive the globally frequent closed itemsets  2.2.1 Identities of Frequent Closed Itemsets  FCIM not only judge X  s frequence, but also judge X  s closure. When merging each local results, two problem must be solved. The first is the associated supports could be smaller than exact ones, because some globally FCI might be not locally frequent in some site; the second is it may produce a superset of all the FCI, as for the first problem, different distribute settings adopts different solution; as for the second problem, we adopts subset examination technology to judge X  s closure, it performs as below Step one Sort the candidate FCI according to the length of itemsets Step two get the shortest item X and compare X to the items behind , if there is a item Y and meet X Y  and X.sup=Y.sup condition. according to principle 1, X will be pruned, then compare Y to the items behind, repeat the same process with X,to scan the candidate closed itemsets one time, it would generate one closed item in terms of Lemma 1 Step three repeat the above process until all the candidate itemsets are closed itemsets  3. Distribute Frequent Closed Mining Approach and Its Algorithm  3.1 Analyse the Character of Distribute Setting  In reality, distributed settings are various. We can analyze them from three aspects: 1.site quantity; 2.the bandwidth between sites; 3.the characteristics of dataset, characteristics includes whether it is dense or sparse, transactions’ quantity etc. Based on the above analysis, we consider of two different distributed settings. The first distributed setting is that there are few sites or has small bandwidth between sites or the local FCI is very large.  The second distributed setting is reverse. As for the first distributed setting, the communication load can affect the mining efficiency much compared with the communication frequency As for the second distributed setting, the communication frequency can affect mining efficiency much compared with the correspondence load.  In the following, we give a distributed mining approach respectively in term of these two different distributed settings  
38 
38 


3.2 Distributed Freque nt Closed Mining Regarding the First Distributed Setting  Approach Firstly every site sends its locally FCI to other sites for collecting their support, after knowing its support of all sites, do globally frequent pruning on each site, after pruning every site ,it can generate the globally FCI. Secondly choose the biggest bandwidth site as a meta-learning [9 S other sites send their local results to S Such merging may produce a superset of all the FCI, adopt subset examination technology to solve the problem . Finally get the globally FCI. This mining approach can generate the globally FCI on each site, and communication load between site is small however, communication is frequent between sites. We give the algorithm DMCI \(distributed mine closed itemsets\elow DMCI algorithm  Input: distributed data sources  1 2   i D in   minimum support threshold minsup Output: the globally frequent closed itemsets 1  for i=1,2,…,n do 2  L\(i\=Local-Mining minsup i D  3  For j=1,2,…,n and ji  do 4  Send L\(i j D  5  if L\(j\ has been mined from j D  6   X Li  if  X Lj  write down sup j X  7  scan j D and check L\(i  8  else 9  scan j D and check L\(i   10  receive\(X.sup i    X Li   11  if sup minsup i XD  then delete X  following code were performed on meta-learning site 12  L=receive\(X,X.sup i  adopt subset examination technology 13  L=check_close\(L  14  return L  3.3 Distributed Freque nt Closed Mining Regarding the Second Distributed Setting  Approach Firstly choose the biggest bandwidth site as a meta-learning site S, merge every local results to S. then collect its support of the candidate globally itemsets which is infrequent on some site, accumulate the support of the candidate FCI for all sites, do globally frequent pruning on S. Secondly judge itemsets’s closure by subset examination technology .Finally generate the globally FCI Modify DMCI algorithm to get the algorithm for this mining approach. It is called DMCI-ML algorithm DMCI-ML algorithm  Modify the DMCI algorithm’s pseudo codes from step 3 to step 11 3\e \(i,L\(i\ on meta-learn site, collect itemsets and their support from every site  4\or i=1 to n do i\find infrequent itemsets for ith site from the candidate frequent itemsets 6\end\(i,C\(i\ send C\(i\o i site 7\can dataset Di a time, and get support of itemset which is in C\(i 8\e\(i,C\(i\ on meta-learn site, collect support of itemset which is in C\(i   on meta-learn site , prune itemsets that is infrequent 9 sup min_ sup X DB  delete X  XL    4. Experience and Analyze  We carried out four sets of experiment. The first set compared DMCI with DMCI-ML on a range of site values in Ethernet-100 network, which simulated the distributed setting that has high bandwidth and many sites. The second set compared DMCI with DMCI-ML on a range of support threshold values in Ethernet-100 network which simulated the distributed setting that has high bandwidth and huge translation number. The third set compared DMCI with DMCI-ML on a range of site values in NON-LAN which simulated the distributed setting that has low bandwidth and many sites. The fourth set compared DMCI with DMCI-ML on a range of support threshold values in NON-LAN which simulated the distributed setting that has low bandwidth and huge translation number. We ran our experiments on a cluster of PentiumIII 1000MHZ CPU, 256MB Memory, 20GB HD PC.\(PC machine represents site\he four experiments are performed with synthetic databases produced by IBM Almaden  ery  s i t e i s st ored 10 0k t r ans act i o n dat a a n d  adopts A-CLOSE algorithm [4 o m i n e l o cal l y FC I   
39 
39 


       Figure 1 compare DMCI with DMCI–ML on a range of site values in Ethernet-100 network when min_sup=1        Figure 2 compare DMCI with DMCI-ML on a range of support threshold values in Ethernet100 network when site number=4              Figure 3 compare DMCI with DMCI-ML on a range of site values in NON-LAN when min_sup=1              Figure 4 compare DMCI with DMCI-ML on a range of support threshold value in NON-LAN when site number=4  The figure 1 shows the execution time radio of DMCI algorithm and DMCI -ML algorithm on a range of site values under the same support in Ethernet-100 network. Due to the same Ethernet-100 network, the communication bandwidth is very large, so the most crucial factor that affects performance of mining is the number of communication rather than communication load. For DMCI algorithm, its communication frequency complexity is O \(n 2 hile for DMCI-ML algorithm, its communication frequency complexity is no more than O \(n\he execution time radio is larger and larger with increasing of the number of site The figure 2 shows the execution time radio of DMCI algorithm and DMCI -ML algorithm on a range of support threshold values under the same site’s number in Ethernet-100 network, the execution time radio is slightly decline with decreasing of support threshold. The reason for decline is that the amount of patterns becomes larger and larger with decreasing of support threshold, which leads DMCI-ML algorithm dealing with much more patterns than DMCI algorithm does. Each time the communication load of the DMCIML algorithm is the sum of patterns from all sites however, the communication load of DMCI algorithm are patterns of each site The figure 3 shows the execution time radio of DMCI algorithm and DMCI-ML algorithm on a range of site values under the same support threshold in NON-LAN, the execution time radio is trend to decline with increasing of site’s number, in NON-LAN environment, the communication bandwidth is low, so the communication load also affects performance of mining, and the communication load of DMCI algorithm is much smaller than that of DMCI-ML algorithm The figure 4 shows the execution time radio of DMCI algorithm and DMCI-ML algorithm on a range of support threshold values under the same site’s 
40 
40 


number in NON-LAN, the execution time radio is decline faster than that in figure 3, the reason for decline faster is that  as the site number is invariant in figure 4, the different value of communication of two algorithms is invariant , while in figure 3 the different value of communication of two algorithms is become larger and larger with of increasing site’s number. The reason for decline is the same with figure 3  5. Conclusion  We analyze the distributed setting and have presented a mining approach and its algorithm respectively on two different kinds of distributed settings. Compare with frequent itemsets, closed frequent itemsets are more valuable to find nonredundant association rules and are helpful to make correct decision on each site. However, in the distributed setting, local FCIM is still a time consume task, moreover in order to find the globally frequent closed itemsets, it has to scan dataset several times Future works regards improving mining efficiency on each site and decreasing of the scan-number  Acknowledgement  It is a project supported by the National Social Science Foundation of China \(05BTJ019\ foundation of National Natural Science of China \(70671094 foundation of National Doctor Fund under 20050353003\hejiang Social Science Programming 06CGL29YBB\ ZheJiang Science and Technology Plan \(2007C24004  References  1 R.A g ra wa l a nd R.Srik a n t, F a st A l g o rithm s  f o r Mining  Association Rules, In Proc. VLDB’94, pp. 487–499 September 1994   R A g ar w a l  C Aggar w al  V P r asad 1  Dep t h F i rst  Generation of Long Patterns1,The 2000 ACM2SIGKDD Boston , MA , 2000  3 J i a w e i H a n, J i a n  P e i, a n d Y i w e n Y i n. Min i ng Fr e q ue nt  Patterns Without Candidate Generation, In Proc. SIGMOD 00, pp. 1–12, 2000  4   P a sq uie r N Ba stide Y, T a ou il R L a k h a l L  Disc ov e r ing  Frequent Closed Itemsets for Association Rules, In: Beeri C et al, eds. Proc. of the 7th Int’l. Conf. on Database Theory Jerusalem: Springer-Verlag, pp. 398~416. 1999  5 Za k i MJ H s ia o CJ CH A R M: A n E f f i c i e n t A l g o rithm  f o r  Closed Itemset Mining, In: Grossman R, et al, eds. Proc. of the 2nd SIAM Int’l. Conf. on Data Mining. Arlington: SIAM pp. 12~28, 2002   Ji an P e i  Ji aw ei Han  an d Ji an y o n g W a n g  Cl o s et   Searching for The Best Strategies for Mining Frequent Closed Itemsets. In SIGKDD ’03, August 2003  7 L u c c h e s e C  O r la ndo S   Pe r e g o R  Mining Fr e q ue nt  Closed Itemsets without Duplicates Generation. Technical Report,ISTI-2004-TR-13,2004. http://dienst.isti.cnr.it/cover tmp.html  8  Miao Yu Qi n g  A  P a rallel A l g o rith m f o r Min g Freq u e n t  Closed Itemsets in Computer Science Vol.31\(2004\ pp.166168  9 C h a n  P  A n Ex te ns ible M e ta l e a r ning A ppr oa c h f o r  Scalable and Accurate Inductive Learning [Ph D dissertation D e pa r t m e nt of C o m pute r Sc ie nc e  C o lum b ia  University  10  Srik a n t R Que s t Sy nthe tic Da ta Ge ne ra tion Co de  Sa n  Jose: IBM Almaden Research Center, 1994 http://www.almaden.ibm.com software/quest/Resources/inde x.shtml 
41 
41 


6.1 Simulation Model  In the section, we evaluate the performances of four data structures for algorithm Ap  E clat  C H A R M [10 an d F C ET on a D E L L G X 270 w i t h  Intel Pentium 4 3.2Ghz and 1 GB main memory running Windows XP. All the experimental data are generated from a normal distribution. The relative simulation parameters are shown in Table 4. To make our dataset representative, we generate two types of databases in the experiments; i.e., DENSE databases and SPARSE databases. Each item in the DENSE database is randomly generated from a pool P called potentially frequent itemsets with size 300, whereas each item in the SPARSE database is randomly generated from a pool N i.e., the set of all the items\ with size 1000. Since the items in the DENSE database are more clustered than those in the SPARSE database, larger frequent itemsets will probably be produced in the DENSE database for the same minimum support. In addition, we use the notations T for the average number of items per transaction I for the average number of items in a frequent itemset, and D for the number of transactions. For example, the experiment labeled with T 10 I 3 D 1K represents the simulation environment with 10 items on the average per transaction 3 items on the average in a frequent itemset, and 1000 transactions in total Table 4 Simulation parameters with default values D Number of transactions 100,000 T Number of the items per transaction 5-40 P Number of potentially frequent itemsets 300 I Number of the items in a frequent itemset 2-5 N Number of items 1000 R Number of taxonomy trees 31-75 L Number of levels in a taxonomy tree 3  6.2 Experimental Results  Experiment 1  In this experiment, we explored the required storage spaces of Apriori, CHARM, MaxEclat and FCET in the environment T10.I5.D10K under different minimum supports in SPARSE databases \(Fig 7\. We found that the FCET saves almost 10 times as much space compared with Apriori, especially under smaller minimum supports. The reason is that the FCET only stores maximal frequent itemsets, rather than the frequent itemsets  T10I5D10K 0 2000 4000 6000 8000 10000 12000 0.02 0.01 0.007 0.005 minimum support stora g e s p ace \(b y tes FCET MaxEclat Apriori CHARM Fig. 7 Required storage spaces in SPARSE databases  Experiment 2 In this experiment, we also explored the required storage spaces of Apriori, CHARM, MaxEclat and FCET in the environment T 40 I 20 D 10K under different minimum supports in DENSE databases \(Fig. 8 We found that the FCET saves almost 20 times as much space compared with Apriori, especially under smaller minimum supports. The reason is that the FCET can store the maximal itemsets generated in DENSE databases more efficiently than those in SPARSE databases  T40I20D10K 0 10000 20000 30000 40000 50000 60000 70000 80000 90000 0.02 0.01 0.005 0.003 0.002 minimum support storage space \(bytes  FCET MaxEclat Apriori CHARM Fig. 8 Required storage spaces in DENSE databases   Experiment 3 In this experiment, we explored the execution time of BASIC [17 Cu m u late [1 7  GMFI [9  and GMAR o rithm s  in t h e env i ronm e n t T10.I5.D10K/T40.I20.D10K under different minimum supports, as shown in Fig. 9 and 10, respectively. From Fig. 9 and 10, we found that GMFI/GMAR m pl o y i n g  the FCET perform better than the other ones in either SPARSE databases or DENSE databases   0 2 4 6 8 10 12 14 16 18 3210.75 minimum support execution time \(minutes GMFI GMAR BASIC Cumulate Fig. 9 Mining time in SPARSE databases  
570 


0 10 20 30 40 50 60 3210.75 minimum support execution time \(minutes GMFI GMAR BASIC Cumulate  Fig. 10 Mining time in DENSE databases  7.  Conclusions  Through several comprehensive experiments, we found that the FCET and IFECT can save a larger amount of storage spaces than Apriori, MaxEclat, and CHARM in both SPARSE databases and DENSE databases. Since the FCET stores fewer elements for a long pattern, when matched with GMFI/GMAR algorithm, it also revealed efficient execution time than BASIC and CUMULATE in mining generalized association rules The time complexity to find the maximal itemsets is O\(log2n\ where n is the total number of maximal itemsets. For a long pattern, we used a partition tree to count the SUB_TID of itemsets, and then got their merged results. Although the memory required for the FCET is still exponentially large, through limiting the size of maximal itemsets and the size of clustering to a reasonable memory requirement, we do save a large amount of storage spaces, especially in dense databases  7.    Conclusions  Through several comprehensive experiments, we found that the FCET and IFECT can save a larger amount of storage spaces than Apriori, MaxEclat, and CHARM in both SPARSE databases and DENSE databases. Since the FCET stores fewer elements for a long pattern, when matched with GMFI/GMAR algorithm, it also revealed efficient execution time than BASIC and CUMULATE in mining generalized association rules The time complexity to find the maximal itemsets is O\(log 2 n\ where n is the total number of maximal itemsets For a long pattern, we used a partition tree to count the SUB_TID of itemsets, and then got their merged results Although the memory required for the FCET is still exponentially large, through limiting the size of maximal itemsets and the size of clustering to a reasonable memory requirement, we do save a large amount of storage spaces especially in dense databases 8. References  1 g ra w a l, T  Im ieli n s k i an d A  S w a m i M in i n g  association rules between sets of items in large databases,” Proc. ACM International Conference on Management of Data \(1993\, pp. 207-216 2 g ra w a l an d R. Srik an t F a s t alg o rit h m s f o r  mining association rules,” Proc. 20th International Conference on Very Large Data Bases \(1994\ pp.487499  J i aW e i Han  J i an  P e i an d YiW en Yi n   Mi n i n g frequent patterns without candidate generation,” Proc ACM International Conference on Management of Data 2000\p. 1-12 4 J  S   Pa r k  M  S  C h en  an d P S  Y u     A n ef f e c t iv e hash-based algorithm for mining association rules,” Proc ACM International Conference on Management of Data 1995\p.175-186 5 A  Sa va se r e  E  O m i e c i ns ki  a n d S N a va t h e    A n  efficient algorithm for mining association rules in large databases,”  Proc. 21st International Conference on Very Large Data Bases \(1995\ pp.432-443 6 Y i n-F u H u a ng a n d  Chi e h M i ng W u   M i n i n g  generalized association rules using pruning techniques Proc. IEEE International Conference on Data Mining 2002\p.227-234 7 a k i a n d C.J  Hsia o   E ff icie n t al g o rith m s f o r  mining closed itemsets and their lattice structure, ”  IEEE Transactions on Knowledge and Data Engineering, vol 17, no. 4, April \(2005\p. 462-478   Bu rdick  M. C a li m l i m  an d J. Geh r k e  M AF I A a maximal frequent itemset algorithm for transactional databases,” Proc. 17th International Conference on Data Engineering, \(2001\p.443-452  a k i S  P a rth a s a rat h y   M. Ogih ara, a n d W. Li   New algorithms for fast discovery of association rules Proc. 3rd ACM International Conference on Knowledge Discovery in Databases and Data Mining, \(1997\pp 283-286 10 M  J  Za ki a n d K  G o ud a  Fa s t ve r t i c a l  mi ni ng usi n g  diffsets,” Proc. 9th ACM International Conference on Knowledge Discovery and Data Mining, Aug. \(2003   Mam o u l i s D  W. C h e u ng an d W. L i a n    Similarity search in sets and categorical data using the signature tree,” Proc. 19th International Conference on Data Engineering, \(2003 12 R. Srik a n t a n d R Ag ra w a l   M i n i n g g e n e ralized  association rules,” Proc. 21st International Conference on Very Large Data Bases, \(1995\.407-419    
571 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efÞcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efÞciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efÞciency When the clustering model is available it is a signiÞcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques SufÞcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efÞcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efÞcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conÞdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conÞdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conÞdence The sufÞcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efÞcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efÞciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufÞciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70Ð81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207Ð216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487Ð499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145Ð154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146Ð153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9Ð15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597Ð600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern ClassiÞcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155Ð162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512Ð521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1Ð12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265Ð278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283Ð304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476Ð482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559Ð563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220Ð231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13Ð24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10Ð17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549Ð550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188Ð201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259Ð283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909Ð921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305Ð345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432Ð444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conÞdence Intelligent Data Analysis  9\(4\:381Ð395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49Ð73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407Ð419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1Ð12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki EfÞciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642Ð658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483Ð490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194Ð203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344Ð353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efÞcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103Ð114 1996 
618 
618 


