Better Rules, Fewer Features A Semantic Approach to Selecting Features from Text Catherine Blake and Wanda Pratt Information  Computer Science University of California Irvine  cblake, pratt}@ics.uci.edu Abstract The choice of features used to represent a domain has a profound effect on the quality of the model produced yet few researchers have investigated the relationship between the features used to represent text and the quality of the final model We explored this relationship for medical texts by comparing association rules based 
on features with three different semantic levels 1 words 2 manually assigned keywords and 3 automatically selected medical concepts Our preliminary findings indicate that bi-directional association rules based on concepts or keywords are more plausible and more useful than those based on word features The concept and keyword representations also required 90 fewer features than the word representation This drastic dimensionality reduction suggests that this approach is well suited to large textual corpus of medical text such as parts of the Web 1 Introduction Selecting features that are necessary and sufficient is 
critical if you are to construct a model that can accurately predict future events or describe a problem space In addition each individual feature should be informative that is it clearly captures some aspect of the problem space Intuitively models based on informative features will be easier to interpret than models based on uninformative features. For text the feature representation is tightly coupled to model quality because they are embedded in natural language. Unlike traditional numeric categorical and Boolean data types a textual resource must first be transformed to an alternative representation before a data-mining technique can be applied We designed an experiment to 
understand the relationship between the quality of features used to represent text and the quality of a descriptive model as measured by plausibility and usefulness We describe the quality of features in terms of semantic richness For example breast cancer is a disease occurring in a particular part of the body If a text-mining system represented this phrase using the two individual features 0-7695-1 119-8/01 $17.00 0,2001 IEEE breast and cancer it would not capture the meaning of the phrase breast 
cancer Our approach uses a concept feature identified with the entire phrase breast cancer which also captures the semantically equivalent expression neoplasm of the breast We call this approach a semantic approach because it uses a semantic model to determine the features rather than simply identifying commonly occurring phrases in text Thus, we say that the concept feature breast cancer is semantically richer than the individual features breast and cancer For a model based on text to be valid our 
first requirement is that it be plausible that is 221seemingly reasonable or probable\222 I Clearly no one will use a system that produces implausible models For example neurologists were unwilling to follow decision rules that violated monotonicity constraints where the neurologist expected an increase decrease in an attribute to correspond to an increase decrease in a predictive variable[2 We argue that a model based on text must be plausible before it can be either meaningful or interesting Our second requirement is that the generated model be 
useful By useful we refer to task-specific usefulness such as 221would this rule be useful if you were treating a patient with breast cancer?\222 Although usefulness is a stated goal of data mining 3 it is rarely included as a metric in evaluation perhaps because it is inherently subjective Our work explicitly measures the usefulness of a model based on text Our hypothesis is that increasing the information content semantic richness of features used to represent text will correspond to an increase in the plausibility and usefulness 
of the descriptive model produced We used features at three different semantic levels 1 words 2 manually assigned keywords and 3 automatically selected medical concepts The model we used was a set of bi-directional association rules In addition to model quality our goal was to study the effect of dimensionality when features have varying levels of semantic richness 2 A Text Mining Scenario As the amount of text available in 221electronic form continues to increase at an alarming rate the tools to 59 


manage these textual resources effectively will become critical Consider MEDLINE a bibliographical database of medical abstracts from conferences and journals that contains more than 11 million The National Library of Medicine NLM who maintains this resource estimates that more than 400,000 additional references will be added during 2001 8,000 new entries each week Although access to abstracts or the full-text of articles in more than 4,000 biomedical journals has the potential to be useful for medical researchers the quantity and unstructured nature of this information often results in information overload Text mining has the potential to reduce information overload by providing a user with patterns from the underlying text Our scenario starts with a medical researcher who wants to learn about breast-cancer treatments She could search MEDLINE to retrieve bibliographic details of documents related to the topic however if she spent only 1 minute on each abstract and worked 10 hours a day it would take her a month to read the 19,167 related abstracts Clearly reading every abstract is infeasible If her task related to a specific treatment then she would provide additional constraints to narrow the search space However her goal is to learn about treatments thus it is unreasonable to assume that she knows the names of the available treatments A text-mining system would enable her to understand treatments by providing a model of the important relationships within the published scientific literature The model, which would be at a finer level of granularity than the entire abstract might identify a co-occurrence between a treatment and its side effects For example Docetmel a chemotherapy drug that destroys cancerous cells sometimes destroys cells that grow at a fast rate such as those responsible for hair growth; thus a patient may suffer from Alopecia hair loss A rule associating Docetmel with alopecia would be useful to our medical researcher because it would help her understand the nature of the treatments available for breast cancer Further co-occurrences previously unreported in any individual study would be of particular interest. Let us use an example from a different domain Tengs and Osgood found evidence that impotence correlated with smoking They discovered this correlation by using clinical trials that did not specifically analyze the relationship between impotence and smoking but rather studied impotence and happened to report tobacco  Although they used a manual rather than an automated approach we believe that a text-mining system should also identify a correlation between concepts that are not the primary focus of an individual study 3 Related Work Identifying informative features from natural language text can be difficult thus existing approaches use semantically poor features such as words[6-141 This approach has the advantage of being domain independent and easy to implement it has the disadvantage of producing the same number of attributes as the size of the vocabulary The Apriori algorithm requires potentially 2 item sets where m is the number of terms in the vocabulary see section 5 Although it is unlikely that every term will appear in every textual resource the condition that makes 2 item sets necessary in practice the number of features can seriously impede the application of this data-mining algorithm A representation that does not account for natural language characteristics such as synonymy or polysymy could cause a data-mining system to generate a misleading model Consider the phrase hair loss which is synonymous with the medical term alopecia The actual number of times that this concept occurs is the number of times that either hair loss or alopecia occurs in the text but a word-based representation would distribute the count between the three features hair loss and alopecia Thus the word-based count would be smaller than the actual number of occurrences of the medical concept alopecia The word-based representation could also over estimate the count for a concept The word-based feature count for hair would also include the number of times that the expression hair loss hair gain and hair color occurred in the collection In contrast a concept representation would unify the expression hair loss with alopecia and thus account for synonymy Although you could augment a word-based system with list of synonymous word pairs such as nausea and queasiness it is unclear if this would be effective in removing the influence of synonymy that is present in text Other researchers have explored the use of manually assigned keywords For example Feldman and colleagues used keywords as features for the generation of association rules[l5-17 However they did not evaluate the effect of their feature-selection method on the plausibility or usefulness of the generated rules but rather they determined the time it took to generate the rules The drawbacks of approaches that use manually assigned keywords are that 1 it is time consuming to manually assign the keywords 2 the keywords are fixed i.e they do not change over time or vary based on a particular user 3 if the keywords are manually assigned, they are subject to discrepancy 4 the textual resources are constrained to only those that have keywords Other researchers have used representations of medical concepts that are similar to ours for automatically determining a diagnosis category based on a textual description of a patient's clinical report[lS They used a physician's diagnosis also based on the clinical report as a gold standard and found that the automatically determined diagnoses based on concepts were more accurate than those based on words We use the same extensive medical knowledge base however our 60 


approach identifies concepts from free-form text in scientific abstracts, rather than from clinical reports Their work also relates more closely to a text categorization task than the text-mining scenario outlined in section 2 Other studies have examined the effect of feature selection from text on learned models[ 19,201. However these studies use statistical techniques with no semantic component for feature-selection In addition the studies measured the performance of their approach on a document-categorization task rather than the text-mining task in section 2 Other related research has focused on constructing techniques to improve the quality of text-mined association rules. Most of these approaches first generate a set of rules and then apply pruning or ranking techniques such as 221interestingness\222  14,21-251 We consider metrics as a group rather than ranking individual rules We anticipate that the average interestingness for rules based on semantically richer representations would be higher than for a semantically poorer representation We consider plausibility to be a necessary but not sufficient condition for interestingness Several systems enable users to provide expectations to a system and then rank rules with respect to how they differ from the users\222  Although this approach enables a rule ranking for a particular user it requires considerable effort from users to specify their expectations such as the overall instruction ratings are higher than the overall course ratings[22 Users are also required to give the degree of belief a probability however there is strong evidence that people are not good at estimating such probabilities Although we agree that ordering the rules could be valuable we advocate ranking in conjunction with using informative features We would be surprised if a plausible and useful rule would ever use the feature 221is\222 yet according to the results of one study based on the x2 ranking, the correlation between the features government is and number was ranked the most interesting 27 4 Feature Extraction in our experiments we varied the semantic richness of the features used to represent the title and abstract of each bibliographic reference We now describe the process that we used to generate each of the feature sets 4.1 Concept Features Few researchers would claim that a word representation is optimal but the difficulty of automated natural-language understanding has limited our ability to use a richer representation scheme Unlike many natural-language systems, our system does not use a part-of-speech tagger to identify candidate phrases Instead it uses a heuristic approach to break each sentence in the document\222s title and abstract into a set of clauses It separates a sentence into a set of clauses based on stopwords words such as the in and it that occur frequently in text but are not meaningful We used a generic list of stopwords that was developed for information retrieval and we added to this list numbers days of the week and month names We also developed and included a second set of 31 medical stopwords that occur frequently but are not meaningful such as study and test We used an existing knowledge base the Unified Medical Language System UMLS to map each clause to a medical concept The UMLS which was created and is maintained by the NLM consists of three components i a semantic network that links each concept to one of 132 high-level concepts called semantic types e.g the concept Tamoxifen a chemotherapy drug has a semantic type of Organic Chemical ii the Metathesaurus a medical thesaurus that contains synonymy and hierarchical links among about 800,000 concepts and 1.9 million concept names and iii the SPECIALIST lexicon which provides lexical information on 140,000 concepts For example, the UMLS maps the clause MR-guidance to the concept magnetic resonance imaging guidance The last step was to impose semantic constraints on the concepts provided by the UMLS so that the concepts related to findings and treatments Based on the semantic type hierarchy we selected the following types as suitable Therapeutic or Preventive Procedure Sign or Symptom; Pharmacologic Substance; Neoplastic Process Amino Acid Peptide or Protein; Antibiotic or Organic Chemical 1 ConceptList t I3 2 For each sentence in the title and abstract 3 For each clause in the sentence 4 5 End for 6 End for 7 ConceptList t constrain using semantic types concept List t  UMLS concept using the clause Figure 1  Concent Extraction Process Any knowledge-based approach such as ours is clearly dependent on the comprehensiveness and quality of the knowledge it is based on To assess the quality of our clause-concept mappings we manually reviewing the original 367 concepts provided by the UMLS We identified 18 5 blatant mismatches and have since notified the NLM We also noticed that some clauses were mapped to a concept that was more specific or more general than the original clause e.g tolerated was mapped to the concept maximum tolerated dose and hematologic recovery to the concept recovery from disease Although such errors affect model quality we did not remove these concepts from our feature set 4.2 Keyword Features Employees of the NLM assign 10-12 keywords from a controlled vocabulary to each bibliographic reference in 61 


the MEDLINE database Documents are indexed organized and retrieved using this medical ontology the Medical Subject Headings or MeSH In addition to the 19,270 MeSH terms the employees also use 3-4 of the 800 available subheadings or qualifiers to index the medical literature A qualifier provides specific details about the application of a MeSH term in a document. For example the qualifier drug effects when added to the MeSH term Liver indicates that the article or book is not about the liver in general but rather is specifically about the effect of drugs on the liver We used both the MeSH terms and qualifiers as our keyword features 4.3 Word Features Researchers generally use word features to represent text 9,11,12,27 We used the same stopwords as those used to generate concepts that is a generic set of stopwords 29 augmented with numbers months days of the week and 3 1 medical stopwords The approach most often used is to remove stopwords and then do word stemming a process that removes a word\222s prefixes and suffixes Instead of using a generic stemming algorithm such as Porter\222s we used the Lexicon Variant Generator Ivg a stemming tool provided by NLM that was specifically designed for the medical domain In addition to removing suffixes and prefixes \(such as unifying both analyzed and analyzing to anahze Ivg unifies morphological changes such as transforming wound to wind We applied pre processing operations in the following order convert to lower-case remove stopwords strip genitive or possessive, strip punctuation uninflect canonicalize We then removed duplicates of the same canonical form from each abstract 5 Generation of Association Rules Following the scenario in section 2 we started with the 19,167 abstracts from MEDLINE that relate to the query breast cancer treatment We then selected the most recent 100 abstracts from this set We discarded 9 abstracts because they did not have MeSH terms that corresponded to a treatment, and we did not want irrelevant documents to bias our results The title and abstract of the remaining 91 articles were used to generate the word and concept features We used the MeSH terms and qualifiers of those documents as the keyword features Table 1 shows the rules that we used in our experiment Rules based on Rules based on Rules based on Word Features Keyword Features Concept Features 1 Axillary t Background Drug therapy t Human Doxorubicin t Prekallikrein 2 Metastatic t Toxicity Disease-free survival t Survival analysis Alopecia t Methotrexate 3 Antineoplastic agents, phytogeiiic Chemotherapy-Oncologic Procedure t stage 1V breast cancer 4 Chemotherapy t Result Drug therapy -Therapeutic use Carcinoma of Breast t Vinorelbine 5 Advance t Phase Lymph Nodes t Radionuclide Imaging Anorexia t Progressive disease 6 Blue t Detection Cyclophosphamide t Methotrexate Nausea t Stable disease 7 Antineoplastic agents phytogenic 8 Antineoplastic agents, combined Cyclophosphamide 9 Support Non-US Govt Fatigue 10 Conclusion t Trial Adolescence t Lung Neoplasms Docetaxel t Pain 1 1 Common t Tamoxifen Aged t Drug therapy Fatigue t Nausea 12 Advance t Grade Etiology t Radiotherapy Doxorubicin tf Paclitaxel 13 Conclusion t Dose Adolescence t Nausea Enterotoxin F Staphylococcal Stable disease 14 Median e Respectively Postmenopause t Tamoxifen Paclitaxel t Vinorelbine 15 Nausea t Vomit Lung neoplasms t Nausea Nausea t Stage IV Breast Cancer 16 Antineoplastic Agents, Combined Axillary Lymph Node Dissection t Secondary 17 Baseline t Questionnaire Disease-free surviva1t\studies Gemcitabine Vomiting 18 Breast t Cancer Aged 80 and over t Postmenopause Lymphocyte antigen CD69 t Myalgia 19 Dose tf Phase Aged t Therapeutic use Enterotoxin F Staphylococcal t Gemcitabine 20 Progressive t Stable Female t Pathology Anthracycline Antibiotics t Epirubicin Grade t Response t Paclitaxel Cancer t Trial Milligram t Toxicity t Breast lasms Conclusion t Studied Docetaxel t Neoplasm Metastasis t Infusions, intravenous t Lymphocyte antigen CD69 t Granulocyte Colony-Stimulating Factor t Treatment Outcome t Drug Therapy Malignant Neoplasm of Lymph Node Cancer t Method Table 1 The bi-directional association rules used in our evaluation 62 


To identify patterns in the text we used the popular data mining technique of generating association rules An association rule is of the form A+B which means 223thepresence of A implies the presence of B\224 where A is the set of antecedents and B is the consequent set An individual rule identifies a co-occurrence between the antecedent and consequent sets Each association rule has an associated level of support and confidence The support is the probability that both A and B occur in a textual resource For example if 89 of the abstracts contained both the words breast and cancer then the support of the association rule breast+cancer is 89 The confidence is the probability that B will occur given that A has already occurred We constrain the rules to those with a single feature in each of the antecedent and consequent sets We define a bi-directional association rule indicated by e as one that satisfies the support and confidence levels in both directions i.e both A3B and B+A have support and confidence greater than the minimum that was set by the user We used Borgelt\222s implementation of the Apriori algorithm to generate association rules on each feature set separately[30,3 I The computationally intensive component of the Apriori algorithm is to identify the item sets those features that occur with a frequency greater than the specified minimum level of support The algorithm then generates rules that satisfy the minimum level of confidence based on these item sets We used only bi-directional rules in our experiments 6 Experiments Our goal was to determine whether using features of differing semantic richness has an effect on the plausibility or usefulness of bi-directional association rules based on those features We used two perspectives on plausibility and usefulness a physician\222s and the consumer information available from the American Cancer Society\222s website on breast cancer treatment 32 Support Confidence Bi-directional Yo Yo Rules Word 6 48 213 Keyword 4 32 104 ConceDt 2 16 156 Table 2  We lowered the minimum support and confidence because the semantically richer representations had fewer features Fewer features are required to represent an abstract as a set of concepts rather than as a set of words To produce the same number of rules you would have to lower the support so that approximately the same number of abstracts would satisfy the minimum support and confidence constraints Therefore to select a subset of rules for our experiment we imposed different minimum support and confidence levels for each feature set We did not select the rules with the highest support and confidence because there is growing evidence that these metrics do not capture interestingness  14,2 1,221 We did attempt to control for support and confidence by generating a set of approximately 100 to 200 bi directional association rules for each feature set We then randomly selected 20 rules from each feature set for inclusion in the evaluation \(see Table 1 for the rules used in this evaluation We considered three ways of controlling for support and confidence 1 fix support and lower confidence 2 fix confidence and lower support 3 fix the ratio of support and confidence As we did not have a principled way to determine the relative importance of support and confidence we chose to fix the ratio between the two parameters We set the ratio based on the default values in Borgelt\222s implementation 30 to 1:s see Table 2 for specific values 6.1 Assessment We asked an experienced physician to evaluate the bi directional association rules in Table 1 We provided the physician with definitions of the medical terms used in the associations because he was not an expert in breast cancer We assumed that if there was a plausible relationship between the features then a physician could write down that relationship Thus we asked him to state any relationship that he found We instructed him to answer question B based on plausibility and question C based on usefulness using a scale of not at all not really neutral mostly and definitely We used the same scale for question D which attempted to uncover the novelty of the rule Figure 2 shows the exact questions asked 1 Physician Questions A The relationship between these concepts could be  B Do you agree that the relationship in A is plausible C How useful would knowing this relationship be if you were treating a patient with Breast Cancer D Do you agree that this correlation contributes to scientific knowledge on Breast Cancer treatment 2 Consumer Questions A The relationship between these concepts could be  B Do you agree that the relationship in A is C How useful is the relationship in A with respect strongly implied to Breast Cancer Treatment Figure 2  Questions asked for each bi-directional association rule to measure plausibility and usefulness from a physician\222s perspective 1 and with respect to consumer information on available breast cancer treatment 2 Our second perspective on the plausibility and usefulness of the rules was with respect to basic health information 63 


We used the American Cancer Society\222s ACS\web page of breast cancer treatments as our gold standard. For each rule, we searched the ACS web page for any information that would relate the antecedent and consequent features As with the first experiment responses to B and C were expressed using a scale of not at all not really neutral, mostly and definitely Figure 2 shows the exact questions that we asked The person doing this evaluation the first author was not familiar with breast cancer treatments so they also referred to the definitions provided to the physician 7 Results and Discussion Our preliminary results indicate that bi-directional association rules based on keyword or concept features are more plausible and more useful than association rules generated on words Although this increase is not statistically significant our concept and keyword representation had the additional benefit of 90 fewer features than a word representation We start with an example of two plausible useful rules generated with concept features alopeciaeMethotrexate and alopeciaeTamoxifen Both Methotrexate and Tamoxifen are chemotherapy drugs associated with hair loss Neither the word nor the keyword representations identified this relationship with our minimum support and confidence see Table 1 even though the word features hair loss and alopecia occurred in 2 7 and 6 abstracts respectively In contrast the concept representation detected that hair loss is synonymous with alopecia and accurately recorded that this concept occurred 8 times in our set of abstracts thus the concept approach successfully identified this plausible useful rule We recognize that eventually haireMethotrexate would have emerged from the word-based rules if we lowered the minimum support and confidence However it is unlikely that a user would be willing to sift through many low quality rules to locate this relationship 7.1 Physician Assessment We grouped the physician\222s plausibility ratings of mostly and definitely into the general category of plausible Correspondingly we considered rules to be implausible if he rated them not at all not really or neutral We used the same approach to group usefulness ratings Plausible Useful Plausible and Yo YO Useful YO Word lO\(50 6 30 5 \(25 Concept 13 65 13 65 9 45 Keyword 17 85 ll\(55 10 50 Table 3  Plausible and useful ratings of bi-directional association rules with respect to a physician The results show that rules based on concept features produced more useful rules than those based on either keyword or word features Conversely he considered rules based on keyword features to be more plausible than rules based on concept or word features see Table 3 We use an example to illustrate that it may be reasonable for the physician rate a rule as useful, even though he did not consider the rule plausible The physician rated the rule between blue and detection which was generated from word-based features as not really plausible but definitely useful His possible relationship was 221Color of test marker and diagnosis of disease\222 and he annotated 221if it existed\222 next to the usefulness question Although plausibility estimates the degree to which the current medical literature supports the association usefulness reflects the value of a rule if it did exist In this example the physician doubted the plausibility of the relationship but recognized its usefulness if it existed The physician was unable to specify any relationship for 10 of the 60 associations Of the ten seven were from word features thus suggesting that word-based associations are neither understandable nor plausible Although we used lexical tools that were specifically developed for the medical domain only 5 of the 20 associations rules for word features were considered to be both plausible and useful by the physician To express the quality of the associations we used the information retrieval metric precision which in this case is the number of association rules that are plausible and useful divided by the total number of rules evaluated The precision of rules with respect to both plausibility and usefulness was 25 50 and 45 for word keyword and concept features respectively Thus the concept and keyword features appear to improve the quality of generated associations In addition to evaluating the plausibility we attempted to capture how novel or insightful each association was by asking the question 221Do you agree that this correlation contributes to scientific knowledge on breast cancer treatment?\222 If we group mostly and definitely into the insightful category the physician rated 5 rules as insightful that were generated on word features and 8 rules each for keyword and concept features After inspecting these rules however we believe that the physician misinterpreted this question For example the physician rated the association between Docetaxel a chemotherapy drug and Neoplasm Metastasis when cancer spreads to other parts of the body as a contribution to scientific knowledge but the American Cancer Society\222s web page lists Docetaxel as a known chemotherapy drug for treating breast cancer thus we believe this rule is not novel This discrepancy reflects the difficulty in developing accurate methods to measure subjective attributes such as novelty or insightfulness 64 


7.2 Consumer Health Assessment When compared to consumer information rules generated from concept features were more plausible and more useful than associations generated using either word or keyword features The plausibility and usefulness of associations with respect to basic information on breast cancer treatment was overall slightly lower than that from a physician's perspective Precision with respect to usability and plausibility was 15 30 and 85 for word keyword and concept features respectively When considering only plausibility precision increases to 40 60 and 90 The precision when considering usefulness only is close to the precision of both metrics specifically IS 35 and 85 for word keyword and concept representation respectively Plausible YO Useful YO Keyword 12 60 7 35 Concept 18 90 17 \(85 Table 4  Plausible and useful ratings of rules with respect to consumer health information No plausible explanation could be found between the antecedent and consequent for twenty-two of the sixty bi directional association rules Twelve of these associations were based on word features Eight associations were based on keywords and only the remaining two were based on concept features We assume that there are more unexplained rules in this survey than for a physician because of a lack of formal medical training by the person doing the assessment We used lexical tools specifically developed for the medical domain but only 3 of the 20 associations rules for word features were considered useful 7.3 Dimensionality Reduction Although the increase in the number of plausible and useful rules is encouraging, the results are not statistically significant The improvement is more impressive when you consider the drastic reduction in the number of features required to represent the problem space We summarize the dimensionality reduction in Table 5 We found that a keyword representation reduced the average number of features by 26 from 76 to 20 The concept representation reduced the average number of features required to represent the title and abstract by 90 to only 8 Both the concept and keyword representations required 84 fewer distinct terms This reduction has important implications to the running time of computationally expensive data-mining techniques Data mining algorithms often require data in a matrix format Storing the 91 titles and abstracts in this experiment as a matrix would require 194,012 cells if you used word features and 31,759 cells if you used concepts This reduction becomes increasing important if all 19,167 articles related to breast cancer treatments were used or if the full-text of the articles was considered Average Distinct Abstract unique terms Feature features Pairs Word 76 2132 6932 Keyword 20 35 1 1856 Concept 8 349 1161 Table 5  Feature selection has a drastic effect on the dimensionality required to represent the domain The concept representation reduced the number of abstract-feature pairs by 83 compared to the keyword approach that had a 73 reduction. Despite smaller space requirements rules produced using keyword and concept features were more plausible and useful than rules generated over words 8 Future Work As we mentioned in the related work section several researchers have explored approaches to rank rules based on interestingness We also plan to extend our approach to prune or rank rules based on semantic information such as the semantic relations from the UMLS Association rules show only that a correlation has occurred between concepts We are planning to augment this relationship with semantic information Consider rule 17 in Table I where Gemcitabine has a semantic type of Pharmacologic Substance and Vomiting has the semantic type Sign or Symptom A valid semantic relationship between these two semantic types is treats a Pharmacologic Substance treats a Sign or Symptom We will continue to explore the subjective qualities of a model by interviewing additional physicians Finally we plan to conduct experiments on the full text of a document instead of using only the title and abstract Such experiments will improve our understanding of the impact of the dimensionality reduction with a concept representation 9 Conclusion Our hypothesis was that increasing the semantic richness of features used to represent text would have a positive effect on the plausibility and usefulness of a set of bi directional association rules Our initial findings support this hypothesis Specifically our physician found only 25 of the rules based on word features to be useful and plausible compared with 50 and 45 for keywords and concepts respectively It was unclear which of the two semantically richer representations were preferred however because the physician evaluated rules based on keywords to be the most plausible and rules based on 65 


concepts to be the most useful The consumer information analysis also supported our hypothesis the concept-based rules were clearly more plausible and useful than either the keyword or the word representations Although the increased model quality was not statistically significant the 90 reduction in the number of features suggests that the semantically rich keywords or concepts features will enable association rules to be generated more efficiently The keyword representation constrains the suitable text to those with keywords but this constraint does not apply to the concept representation Thus the concept approach would be suitable to apply to any large corpus of medical text such as portions of the web 10 Acknowledgements We thank Dr Tony Greenberg Craig Evans and Henry Wasserman for their help with the evaluation This work was supported by the University of California's Life Science Informatics grant L98-05 11 References l H W Fowler and F G Fowler The Concise Oxford Dictionary of Current English 91h ed Oxford Clarendon Press 3995 2 M Pazzani S Mani and W R Shankle Comprehensible knowledge-discovery in databases 1 9Ih Annual Conference of the Cognitive Science Society, pp.235-238 1997 3 U Fayyad G Piatetsky-Shapiro P Smyth and R Uthurusamy Advances in Knowledge Discovery and Data Mining AAA1 Press 1996 4 National Library of Medicine Available at http://www.nlm.nih.gov 2001 5 T Tengs and N D Osgood The link between smoking and Impotence Two Decades of Evidence Preventive Medicine vol 32 pp.447-452 2001 6 R Feldman, "Practical Text Mining PKDD-98 p487 1998 7 R Ghani R Jones D Mladenic K Nigam and S Slattery Data Mining on Symbolic Knowledge Extracted from the Web KDD-2000 Workshop on Text Mining 2000 SI U Hahn and K Schnattinger Knowledge Mining from Textual Sources CIKM'97 Las Vegas, pp.83-90 1997 9 B Lent R Agrawal and R Srikant, "Discovering Trends in Text Databases KDD'97 pp.227-230, 1997 lo U Y Nahm Text Mining with Information Extraction Mining Prediction Rules from Unstructured Text Thesis proposal University of Texas, Austin, 2001 ll G W Paynter 1 H Witten S J Cunningham and G Buchanan Scalable browsing for large collections a case study 5th Conf Digital Libraries Texas, pp.215-218, 2000 12 I H Witten Z Bray M Mahoui and W J Teahan, "Text mining a new frontier for lossless compression Data Compression Conference pp.198-207 1999 I31 H Ahonen 0 Heinonen M Klemettinen and A I Verkamo Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections IEEE Form of Research and Technology Advances on Digital Libraries pp.2 1 I 1998 I41 M Klemettinen H Manila P Ronkianen H Toivonen and A 1 Verkamo Finding Interesting rules from Large Sets of discovered association rules CIKM'94 Maryland 1151 R Feldman and I Dagan Knowledge Discovery in Textual Databases KDT ECML-95 Workshop on Knowledge Discovery Crete Greece, pp 175-180 1995 16 R Feldman I Dagan and H Hirsh Mining text using keyword distributions Journal of Intelligent Information Sjstems Integrating Artificial Intelligence and Database Technologies vol IO pp.281-300 1998 I71 R. Feldman and H Hirsh Mining associations in text in the presence of background knowledge KDD-96 Portland I181 A Wilcox G Hripcsak and C Friedman Using Knowledge Sources to Improve Classification of Medical Text Reports poster KDD-2000 Workshop on Text Mining 2000 I91 Goldberg CDM an approach to learning in text categorization International Journal on ArtiJicial Intelligence Tools Architectures Languages Algorithms 20 Y Yang and J P Pedersen A Comparative Study on Feature Selection in Text Categorization ICML'97 1997  Silberschatz and A Tuzhilin On Subjective Measures of Interestingness in Knowledge Discovery KDD'95 1995 22 A Silberschatz and A Tuzhilin What Makes Patterns Interesting in Knowledge Discovery Systems IEEE Transactions on Knowledge and Data Engineering vol 8 23 R J Bayardo and R Agrawal Mining the Most Interesting Rules KDD99 San Diego pp 145-154 1999 24 B Padmanabhan and A Tuzhilin Unexpectedness as a Measure of Interestingness in Knowledge Discovery Decision Support Systems vol. 27, pp.303-3 18 1999 25 B Liu W Hsu and Y Ma Pruning and summarizing the discovered associations KDD'99 CA pp 125-134 1999  A Tversky and D Kahneman Judgment under uncertainty Heuristics and biases Science vol 385 pp.1124-1131, 1974 27 S Brin R Motwani and C Silverstein Beyond Market Baskets Generalizing Association Rules to Dependence Rules KDD98 pp.39-68 1998 28 M Sanderson Stop word list Available at http://www.dcs.gla.ac.uk/idom/ir_resources 1 999 29 S J Nelson W D Johnston and B L Humphrey Chapter 11 Relationships in Medical Subject Headings MeSH Available at http://~v.nlm.nih.gov/mesh meshrels.html, 2001 30 C Borgelt Apriori Implementation Available at http://fuzzy.cs.uni-magdeburg/-borgel 1999  31 R Agrawal H Mannila R Srikant H Toivonen and I Verkamo Fast Discovery of Association Rules in Advances in Knowledge Discovery and Data Mining U Fayyad G. Piatetsky-Shapiro P Smyth, and R Uthurusamy Eds AAAI/MIT Press 1995  32 American Cancer Society Treatment of Breast Cancer Consumer Information Available at http://www3 .cancer.org/cancerinfo, 2001 USA pp.401-407, 1994 USA pp.343-346 1996 vol 5 pp.229-153, 1996 pp.970-974, 1996 66 


 10 100 1000 10000 100000 1e+06 2 3 4 5 Number of candidates \(Normalized Size of the itemset Fanout = 9  Fanout = 3  Figure 7 Num b er of negativ e candidates t yp es of information a v ailable F or instance a kno wledge of substitute items Ho w to incorp orate other t yp es of information to impro v e the qualit y of rules needs to b e explored further 017 The n um b er of candidates generated is exp onential o v er the length of the large itemsets b eing considered More e\016cien t candidate generation tec hniques need to b e dev elop ed Ac kno wledgmen t The 014rst author wishes to thank Dr Rak esh Agra w al at IBM Almaden Researc h Cen ter for suggesting the problem and for man y insigh tful discussions References  R Agra w al T Imielinski  and A Sw ami Mining asso ciation rules b et w een sets of items in large databases In Pr o c e e dings of the 1993 A CM SIGMOD International Confer enc e on Management of Data  pages 207{216 W ashington DC Ma y 26-28 1993  R Agra w al and R Srik an t F ast algorithms for mining asso ciation rules in large databases In Pr o c e e dings of the 20th Internationa l Confer enc eonV ery L ar ge Data Bases  San tiago Chile August 29-Septem b er 1 1994  T J Blisc hok Ev ery transaction tells a story Creating customer kno wledge through mark et-bask et analysis Chain Stor eA ge Exe cutive  V71:50  57 Marc h 1995  J Han and Y F u Disco v ery of m ultiple-lev el asso ciation rules from large databases In Pr o c e e dings of the VLDB Confer enc e  pages 420  431 Septem b er 1995  M Houtsma and A Sw ami Set-orien ted mining of asso ciation rules In Pr o c e e dings of the International Confer enc e on Data Engine ering T aip ei T aiw an Marc h 1995  R Krishnam urth y and T Imielinski  Practitioner problems in need of database researc h A CM SIGMOD R e c or d  20\(3 Septem b er 1991  H Mannila H T oiv onen and A I V erk amo E\016cien t algorithms for disco v ering asso ciation rules In KDD-94 AAAI Workshop on Know le dge Disc overy in Datab ases  pages 181  192 Seattle W ashington July 1994  J S P ark M-S Chen and P S Y u An e\013ectiv e hash based algorithm for mining asso ciation rules In Pr oc e e dings of the A CM-SIGMOD Confer enc e on Management of Data  pages 229  248 San Jose California Ma y 1995  G Piatetsky-Shapiro Disc overy A nalysis and Pr esentation of Str ong R ules  pages 229  248 AAAI Press/The MIT Press Menlo P ark California 1991  G Piatetsky-Shapiro and W J F ra wley  editors Know le dge Disc overy in Datab ases  MIT Press 1991  A Sa v asere E Omiecinski and S Na v athe An e\016cien t algorithm for mining asso ciation rules In Pr oc e e dings of the VLDB Confer enc e  pages 432  444 Zuric h Switzerland Septem b er 1995  A Silb ersc hatz M Stonebrak er and J Ullman Database systems ac hiev emen ts and opp ortunities Communic ations of the A CM  34\(10 Octob er 1991  P Sm yth and R M Go o dman R ule Induction Using Information The ory  pages 159  177 AAAI Press/The MIT Press Menlo P ark California 1991  R Srik an t and R Agra w al Mining generalized association rules In Pr o c e e dings of the VLDB Confer enc e  pages 407  419 Septem b er 1995  M Stonebrak er R Agra w al U Da y al E Nuehold and A Reuter Database researc h at a crossroads The vienna up date In Pr o c e e dings of the 19th International Confer enc eon V ery L ar ge Data Bases  pages 688{192 Dublin Ireland August 1993 


with the same parameters but di\013eren t in size ranging from 25K to 100K The v alues of w minsup are set as the ab o v e scale-up exp erimen t In this 014gure the time is giv en in ln  sec  F rom the 014gure the execution time increases with the n um b er of transactions linearly with ln scale implying that the complexit yof the algorithms is exp onen tial in the n um b er of transactions 1  5.2.3 Exp erimen t for sp ecial case In this section w e are in terested in the p erformance in the sp ecial case whic h is the item w eigh ts equal to 0 or 1 only  In this case w e mak e the 014rst 900 w eigh ts b e 0 and the remaining w eigh ts b e 1 Other things including database and threshold equal as ab o v e section W e carried out the exp erimen t for the normalized w eigh ted case to compare the t w o algorithms There are t w o ma jor 014ndings 1 The p erformance of the sp ecial case is m uc h b etter than the general case where item w eigh ts follo w a distribution b et w een 0 and 1 2 Con trary to the previous cases MINW AL\(W p erforms b etter than MINW AL\(O F rom Figure 8 w e notice that the time needed in MINW AL\(W is m uc h less than the MINW AL\(O for all the thresholds This is b ecause in the joining step the n um b er of starting seed candidate itemsets in C 1 to generate itemsets in C 2  is less than MINW AL\(O case In this situation the 0/1 w eigh ts giv e the adv an tage to MINW AL\(W During the 014rst step the algorithm MINW AL\(W will easily prune all the small itemsets with 0 w eigh ts while MINW AL\(O will k eep those small itemsets with 0 w eigh ts As the starting seed is smaller in size MINW AL\(W w ould p erform w ell in this case 6 Conclusion W eha v e prop osed to study a new problem of mining w eigh ted asso ciation rule This is a generalization of the asso ciation rule mining problem In this generalization the items are assigned w eigh ts to re\015ect their imp ortance to the user The main di\013erence b et w een mining w eigh ted asso ciation rules and the mining un w eigh ted asso ciation rules is the do wn w ard closure prop ert y  W e prop osed t w o di\013eren t de\014nition of w eigh ted supp ort without normalization and with normalization W e prop osed new algorithms based on the supp ort b ounds  the algorithms MINW AL\(O and MINW AL\(W MINW AL\(O is applicable to b oth normalized and unnormalized cases and MINW AL\(W is applicable to the normalized case only  The p erformance ev aluation has b een done on these t w o algorithms W e found that MINW AL\(O outp erforms MINW AL\(W in most cases but MINW AL\(W p erforms b etter for the sp ecial case with only 0/1 item w eigh ts So far w eha v e only considered the mining of binary w eigh ted asso ciation rules Some of the researc hers did the researc h for the problem of the quan titativ e assoication rules suc has[4  3 W ema yin v estigate the problem of quan titativ e asso ciation rules with w eigh ted items whic his anin teresting topic in the future References  R Agra w al and R Srik an t F ast algorithms for mining asso ciation rules In Pr o c e e dings of the 20th VLDB Confer enc e  pages 487{499 1994  D Cheung V.T Ng A F u and Y F u Ef\014cien t mining of asso ciation rules in distributed databases In IEEE T r ansactions on Know le dge and Data Engine ering  pages 1{23 1996  T ak eshi F ukuda Y asuhik o Morimoto Shinic hi Morishita and T ak eshi T okuy ama Data mining using t w o-dimensional optimized asso ciation rules Sc heme algorithms an visualization In Pr o c e e dings of A CM SIGMOD  pages 13{23 1996  T ak eshi F ukuda Y asuhik o Morimoto Shinic hi Morishita and T ak eshi T okuy ama Mining optimized asso ciation rules for n umeric attributes T ec hnical Rep ort 1623-14 IBM T oky o Researc h Lab oratory  1996  J Han M Kam b er and J Chiang Mining m ultidimensional asso ciation rules using data cub es T ec hnical rep ort Database Systems Researc h Laboratory Sc ho ol of Science Simon F raser Univ ersit y  1997  J.S P ark M-S Chen and P S Y u An e\013ectiv e hash-based algorithm for mining asso ciation rules In Pr o c e e dings of A CM SIGMOD  pages 175{186 1995  A Sa v asere E Omiecinski and S Na v athe An e\016cien t algorithm for mining asso ciation rules in large databases In Pr o c e e dings of the 21th International Confer enc eon V ery L ar ge Data Bases  pages 432{444 1995 


expect this optimization to be of greatest bene\336t when the transaction sizes are large r example if our transaction is T 000 f A\000 B 000 C\000 D\000 E g  k 000 3 fan-out 000 2 then all the 3-subsets of T are f ABC,ABD,ABE,ACD,ACE,ADE,BCD,BCE,BDE,CDE g  Figure 2 shows the candidate hash tree C 3  We ave to increment the support of every subset of T contained in C 3  We egin with the subset AB C  and hash to node 11 and process all the itemsets In this downward path from the root we mark nodes 1 4 and 11 as visited We then process subset AD B  and mark node 10 Now consider the subset CDE  We see in this case that node 1 has already been marked and we can preempt the processing at this very stage This approach can r consume a lot of memory r a n fan-out F  for iteration k  e need additional memory of size F k to store the 337ags In the parallel implementation we have to keep a VISITED 336eld for each processor bringing the memory requirement to P\000F k  This can still get very large especially with increasing number of processors In we sho w a mechanism by which further reduces the memory requirement to only k 000F  The approach in the parallel setting yields a total requirement of k 000F 000P  5 Experimental Evaluation Database T I D Total Size T5.I2.D100K 5 2 100,000 2.6MB T10.I4.D100K 10 4 100,000 4.3MB T15.I4.D100K 15 4 100,000 6.2MB T20.I6.D100K 20 6 100,000 7.9MB T10.I6.D400K 10 6 400,000 17.1MB T10.I6.D800K 10 6 800,000 34.6MB T10.I6.D1600K 10 6 1,600,000 69.8MB Table 2 Database properties 5.1 Experimental Setup All the experiments were performed on a 12-node SGI Power Challenge shared-memory multiprocessor Each node is a MIPS processor running at 100MHz There\325s a total of 256MB of main memory The primary cache size is 16 KB 64 bytes cache line size with different instruction and data caches while the secondary cache is 1 B 128 bytes cache line size The databases are stored on an attached 2GB disk All processors run IRIX 5.3 and data is obtained from the disk via an NFS 336le server We used different synthetic databases with size ranging form 3MB to 70MB 2  and are generated using the procedure described in These databases mimic the transactions in a retailing en vironment Each transaction has a unique ID followed by a list of items bought in that transaction The 2 While results in this section are only shown for memory resident databases the concepts and optimization are equally applicable for non memory resident databases In non memory resident programs I/O becomes an important problem Solutions to the I/O problem can be applied in combination with the schemes presented in this paper These solutions are part of future research 11 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


  0 500 1000 1500 2000 2500 0 2 4 6 8 10 12 Number of Large Itemsets Iterations Large Itemset at Support = 0.5 222T5.I2.D100K\222  222T10.I4.D100K\222   222T15.I4.D100K\222   222T20.I6.D100K\222   222T10.I6.D400K\222   222T10.I6.D800K\222   222T10.I6.D1600K\222  Figure 3 Large Itemsets per Iteration data-mining provides information about the set of items generally bought together Table 2 shows the databases used and their properties The number of transactions is denoted as jD j  average transaction size as j T j  and the average maximal potentially large itemset size as j I j  The number of maximal potentially large itemsets j L j 000 2000 and the number of items N 000 1000 We refer the reader to for more detail on the database generation All the e xperiments were performed with a minimum support value of 0.5 and a leaf threshold of 2 i.e max of 2 itemsets per leaf We note that the  improvements shown in all the experiments except where indicated do not take into account initial database reading time since we speci\336cally wanted to measure the effects of the optimizations on the computation Figure 3 shows the number of iterations and the number of large itemsets found for different databases In the following sections all the results are reported for the CCPD parallelization We do not present any results for the PCCD approach since it performs very poorly and results in a speed-down on more than one processor 3  5.2 Aggregate Parallel Performance Table 3 s actual running times for the unoptimized sequential and a naive parallelization of the base algorithm Apriori for 2,4 and 8 processors without any f the techniques descibed in sections 3 and 4 In this section all the graphs showing  improvements are with respect to the data for one processor in table 3 Figure 4 presents the speedups obtained on different databases and different processors for the CCPD parallelization The results presented on CCPD use all the optimization discussed 3 Recall that in the PCCD approach every processor has to read the entire database during each iteration The resulting I/O costs on our system were too prohibitive for this method to be  12 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


