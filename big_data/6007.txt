Mining Fuzzy Association Rules in Data Streams Peng Chen, Hongye Su, Lichao Guo, Yu Qu State Key Lab. of Industrial Control Technology Institute of Cyber-Systems and Control, Zhejiang University Hangzhou, P. R. China hellochenpeng@163.com, {hysu, lcguo, yqu}@csc.zju.edu.cn AbstractMany algorithms have been proposed for mining fuzzy association rules in static datasets with quantitative attributes. However, there is few study on mining fuzzy association rules in data streams. This paper presents an algorithm FFI_Stream for fuzzy association rules mining in data streams. Efficient techniques are presented to find fuzzy association rules in data streams using time based sliding window model. The clustering technique is used to find the fuzzy sets. Membership Function Bias measure MFB_measure function in each sliding window. Selectively Updating Mechanism \(SUM PS proposed to update the fuzzy sets dynamically. Then experiments are carried out on both synthetic and real life datasets. The results show that the algorithm is effective and efficient Keywords- Fuzzy association rules mining; data stream change detection; membership function;  sliding window I. INTRODUCTION Association Rules Mining \(ARM important research topics in data mining. Each association rule is an expression denoted by X Y? , where X  and Y are sets of items. Usually, support and confidence are used to measure the significance and certainty of a rule. The procedure of ARM can simply be divided into two steps. In the first step, it finds the frequent itemsets whose supports are larger than the user defined minimum support \(min_sup In the second step, it generates association rules whose confidences are larger than the user defined minimum confidence \(min_confidence trivial, studies always focus on the first step. However, the traditional ARM can only mine on datasets with binary attributes, which means that it cannot deal with the datasets with categorical or quantitative attributes directly. It is easy to transform categorical attributes into binary attributes, but it is much more complicated to tackle the quantitative 


attributes. Mining quantitative association rules has been first proposed in [1], which used an algorithm to partition the attribute domain and combine adjacent partitions, thus transforms the problem into binary one. However, there is still a problem that the algorithm either ignored or overemphasized the elements near the boundary of the intervals 2]. To deal with this problem, fuzzy ARM was proposed in 3]. The algorithm in [3] used the membership degree to measure how much a data object belong to each fuzzy set but the algorithm assumed the fuzzy sets and membership functions to be given. [2, 5] further developed fuzzy ARM that used clustering techniques to get the fuzzy sets. This method is particularly suitable in the case that users with little domain knowledge on the datasets and applications Due to the emerging number of streaming applications such as network monitoring, clickstream analysis, intrusion detection etc, data stream mining has received increasing attentions in recent years. Data streams are different from static databases. Data in streams always come with high speed, continuous and unbounded. It brings three challenges for data stream mining. First, each item in a stream could be examined only once. Second, although the data are generated continuously, the memory resources are limited Third, the mining results should be generated as fast as possible over limited CPU resources [11 In the real applications, the streaming datasets always consist of both binary and quantitative attributes. Many algorithms have been proposed for mining frequent itemsets and association rules in data streams. But they couldnt deal with the quantitative attributes. To tackle this problem, a novel fuzzy ARM algorithm FFI_Stream is proposed. There are mainly two technical challenges in mining fuzzy association rules over data streams. First, the algorithm should get a well trade-off between efficiency and accuracy i.e. it should acquire most of the interesting association rules with the least cost. Second, due to the concept drift characteristic of data stream, the fuzzy sets and membership may become invalid and expired while time elapses. Thus the algorithm is necessary to detect the large concept drift in data stream effectively and adapt the fuzzy sets and membership functions timely The remainder of this paper is organized as follows. In 


section 2, a formal statement of the problem of fuzzy ARM in data streams and some terminologies are given. In section 3, FFI_Stream algorithm is proposed. In section 4, the experimental results are presented and analyzed. Finally, in section 5, the conclusions are drawn II. QUESTION DESCRIPTION In this paper, the time based sliding window model is applied. In the time based sliding window model, only the fixed time slots of transactions generated recently are considered in mining operations and the number of transactions in each time slot can be varied. Once adding a new time slot of transactions, the data in the oldest time slot are expired simultaneously Let D  be a data stream composed of continuous and unbounded multidimensional transactions 1 2, , .X X " Each transaction can consist of both binary attributes and V4-153978-1-4244-6349-7/10/$26.00 c2010 IEEE quantitative attributes. There is a time identifier tacked to each transaction. The transactions in the same time slot have the same time identifier. Let the time identifier of the first time slot in a sliding window be the time of the sliding window. The sliding window in time k  is denoted as kW Let where \(1 1 are the fuzzy sets characterizing the quantitative attributes Besides, let 1 2{ , , , },b mI b b b= " 1 2{ , , , }q nI q q q= " . Let the number of transactions in sliding window kW  be kW . If a transaction iX  is in kW , it is denoted  as i kX W? . If each binary item in bI is contained by a transaction iX , it is denoted as b iI X? . Let degree of fuzzy set jq in the transaction iX . Then the support of itemset I  in sliding window kW  is defined as follows   k q support I W   Given 1I  and 2I  are two non-empty itemsets with 1 2I I? , the confidence of the rule 2 1 1I I I? ?  in sliding 


window kW  is defined as follows 2 2 1 1 2 1 support I confidence I I I support I I  III. FFI_STREAM ALOGORITHM In this section, an algorithm for mining fuzzy association rules in data streams is proposed. First, several efficient techniques used in the algorithm are introduced. Then the FFI_Stream algorithm is presented A. Finding and Selectively Updating  Fuzzy Sets In [2] and [5], clustering algorithms CLARANS and CURE are used to find the fuzzy sets. But these clustering algorithms are not suitable for data streams, which require algorithms to be able to compute clusters incrementally At present, there are many clustering algorithms in data streams. An interesting adaptation of the k-means algorithm has been presented in [7], but it is to get clusters in entire data stream. In [8], Clustream is proposed, which used the technique called micro-clustering. Unlike the algorithms mentioned above, SWEM proposed in [4] employs EM Expectation Maximization soft clustering method and has properties such as robustness to noise and the ability to handle missing data. These properties are especially desirable for stream environments SWEM is composed of initial phase, incremental phase and expiring phase. In each phase, SWEM uses two-stage schema. In online stage, it processes raw data stream to partition the data into several micro clusters based on EM algorithm \(micro-clustering micro cluster. The summary for micro cluster i  is a triple, { }, , j ij i Ti i i i jX SX j i jST N S X XX  where iS  is the set of data that are assigned to micro cluster i . Then in the offline stage, it uses these summaries to get macro clusters \(global clusters algorithm \(macro-clustering Some techniques proposed in SWEM are applied to find fuzzy sets. The micro-clustering technique is used to achieve the triples for micro clusters in each time slot independently 


until the number of triples reaches the user defined minimum number of triples \(min_num_triples information about the distribution of data collected. Then the macro-clustering technique is used to get macro clusters and corresponding fuzzy sets Furthermore, due to the behavior of data streams, it is necessary to use clustering technique to update the fuzzy sets from time to time. Selectively Updating Mechanism \(SUM is proposed to update fuzzy sets. The clustering algorithm is used only on the projected dataset comprised of the attributes whose membership functions become worse. There is a problem that attributes may be added to cluster at different times, thus they have different time horizons to collect triples. To deal with this problem, a structure called PS Projected Summaries found to cluster at the same time and their corresponding triples. Let iS  denote the group of attributes found to have bad fuzzy sets at time it . And let corresponding to iS . To be efficient, it is assumed that each attribute is independent of one another. In each new time slot first, the micro-clustering technique is applied on all the attributes in PS together, rather than on the individual group respectively. Then the triples are decomposed into projected triples for each group according to the projection relations E.g., assume there are two attributions in PS, A  and B which are added at different times. If in the new time slot, a new triple 5 25 0 3 10 0 30  is acquired in which the first dimension denotes A, then the projected triples are {3, 5 25} on attribute A and {3, 10, 30} on attribute B respectively B. Generating the Membership Functions In this section, the membership functions are generated for the fuzzy sets. Because the techniques which is based on EM algorithm are applied to get the fuzzy sets, more accurate information about the underlying distribution of the clusters can be acquired than [2, 5]. The standard deviations of clusters are added to construct the membership function according to the following observation. Most of data \(99.7 


distribute within 3? ?  if the data is following Gaussian distribution in which ?  is the mean and ?  is the standard deviation of the distribution Let there be n quantitative attributes in the data stream Let imin i n? ? and imax i n? ? denote the thi attributes minimum and maximum. Assume that the dataset is clustered into k clusters. Let { }1 2, , , km m m" be the k centroids of clusters and { }1 2, , ,i i inm m m"  be the thi centroid. Let { }1 2, , , k? ? ?"  be the k  standard V4-154 2010 2nd International Conference on Computer Engineering and Technology [Volume 4 deviations of clusters and { }1 2, , ,i i i in? ? ? ?= "  be the thi standard deviation. Let { }1 2, , ,i i kif f f" denote the k fuzzy sets characterizing attribute i?  assuming that their corresponding centroids have been sorted by value from small to large. 1?  and 2?  \( 1 2 0 the shape of the membership functions. When 1 2? ?= , it is a discrete method using 1?  or 2?  to determine the intervals The membership function defined in this paper is as follows If 1i = , the membership function for fuzzy set ijf is  2 1 2 1 1 2 1 1   0 j ij ij ij ij ij ij ij ij ij ij ij ij if min x m m x x if m x m if x m    


     If 2 1i k? ? ? , the membership function for fuzzy set ijf is given by 1 1 21 11 2 11 21 2 2 2 0      1 ij ij ij ij ij ijij ij ij ijij ij ij ijij ij ij ijij ij ij ij ij if x m or x m if x mx m and x m x if x mm x and x m if x m and x m    


           If i k= , the membership function for fuzzy set ijf is  1 1 1 2 1 2 2 0   1 ij ij ij ij ij ij ij ij ij ij ij ij j if x m x m x if m x m if m x max          The membership function is more proper than the membership function defined in [2, 5]. When the distance 


between two consecutive fuzzy sets centroids is large, there is a long overlap length of their membership functions in [2 5], which is considered as a bad case C. Finding Frequent Itemsets with Fuzzy Sets To acquire the frequent itemsets with fuzzy sets, the algorithm proposed in [9] named UF_streaming is used and optimized. UF_streaming is originally proposed for mining frequent itemsets in uncertain data stream. However it is appropriate to mine frequent itemsets with fuzzy sets in data streams. The expressions of support are similar in two contexts. The only difference between them is that the existential probability in the uncertain data stream context is altered into the membership degree in this paper In each time slot \(batch UF_growth algorithm to mine frequent itemsets whose supports are larger than predefined threshold preMinsup preMinsup<min_sup structure called UF_stream and deletes the information of the expired time slot UF_streaming needs to traverse the nodes in the UF_stream to push results \(the support of the itemset new time slot. Once sliding the window, even when there are few of frequent itemsets in the new time slot, the algorithm needs to traverse all the nodes in the UF_stream. But there are always many nodes in the UF_stream especially when the preMinsup is low UF_stream structure and its maintaining mechanism are optimized. The support combined with time identifier of the time slot is pushed to the UF_stream for each frequent itemset in the new time slot. Thus the traversals can be limited to the nodes representing the frequent itemset in the new time slot only. At the same time, it allows to delay the deletion operations on the expired information until the user requires the results. It can increase memory cost slightly but greatly decrease the CPU cost in many cases Furthermore, the UF-growth algorithm is altered into UH_mine algorithm. According to [10], the hyper-structure and the candidate generate-and-test algorithms perform much better than tree-based algorithms in uncertain data. Thus UH_mine has a better performance than UF_growth D. Evaluation of Membership Function: MFB_measure Due to the concept drift characteristic of data stream, the 


membership functions should not be fixed during mining of data stream and it is necessary to use certain techniques to detect the shift of membership function.  MFB_measure Membership Function Bias measure effectively detect the significant changes that seriously deteriorate the membership function The MFB_measure is designed noting the following observations. First, it is necessary there should be enough frequent fuzzy sets if the membership functions are suitable Second, the membership functions are always well fitted to the sliding window just after the clustering operations are executed. To facilitate the illustration, let reference sliding window denote the first sliding window after the attributes fuzzy sets are updated. Because attributes may have different time horizons to update fuzzy sets, the reference sliding window for each attribute may be different. The MFB_mearsure is described as follows Given two time identifiers t  and 't , where t  is the time of the reference sliding window, 't is the time of the current sliding window. Let W denote the size of the sliding window. Let F denote the number of fuzzy sets for attribute i? . Furthermore, let  number of frequent fuzzy sets that become infrequent from frequent and inverse respectively. Then   _ F t F t T t tMFB measure t F W F       sets in the current sliding window compared to the Volume 4] 2010 2nd International Conference on Computer Engineering and Technology V4-155 Algorithm: FFI_Stream Input:   the size of sliding window: W ,                     the minimum support: min_sup, preMinsup the minimum confidence: min_confidence the maximum MFB_measure: max_MFB,     the minimum number of triples: min_num_triples Output: the frequent itemsets and interesting rules in the current sliding window if the user has required online Procedure FFI_Stream 


0;ct Initialize Initialize membership functions of all quantitative attributes randomly While the data stream is active do Read ct T Micro_clustering ct T  on all the attributes in PS Project them into For each iS do If  Macro_clustering the triples are used to get macro clusters Update  Delete iS and Update the UF_stream \(Delete all the nodes with some fuzzy sets charactering iS in their paths End if End for ct Transform T ;               // Transform the quantitative attributes in ct T  into fuzzy sets and membership degree UF_streaming ct T and find the frequent fuzzy sets in current sliding window Detect update.  If it is true, add these attributes into PS If required==1 do ResultOutput association rules in current sliding window End if 1;c ct t End while End procedure Figure 1. Steps and Pseudo Code of FFI_Stream reference sliding window. Thus the value of the first part of MFB_measure indicates the changed rate of membership 


function at 't  with respect to t , and it is ranging from 1?  to 1 . In general, the higher the first part of MFB_measure values, the more necessarily the attributes fuzzy sets should be updated. The time factor is also added to MFB_measure The time factor increases monotonically. It is inverse proportional to the number of fuzzy sets for the attribute considering that the semantics are more meaningful with more fuzzy sets in the condition not exceeding the user defined maximum in many cases. The time factor helps to avoid the case that fitting bad membership function well for a long time. The parameter T  can control the weight of the time factor. Let max_MFB be as a threshold to judge whether an attributes fuzzy sets need to update. To be more robust and stable, the frequency that the MFB_measure exceeds the max_MFB in certain time horizon can be applied E. Algorithm: FFI_Stream Based on the strategies and techniques presented above an algorithm named FFI_Stream is proposed. The steps and pseudo code of FFI_Stream could be described in Fig. 1 IV. EXPERIMENTS In this section, some experiments that test the performance of the proposed algorithm are presented. All experiments are performed on an Intel Xeon E5335 2.0 GHZ PC with 4 GB RAM \(OS Windows Server 2003 proposed algorithm is compared with discrete method on both synthetic and real life datasets. The algorithms are implemented in Microsoft Visual Studio 2008. To make the comparison fair, both methods use the clustering technique to find the centroids of the fuzzy sets or intervals A. Expreriment One: Synthetic Dataset In this experiment, a set of simple data streams with two dimensions is generated by MATLAB. Each data stream is composed of 100 time slots with 50 transactions in each time slot. To test the performance of the algorithm to detect the different changes in the dataset, the distributions generating the data in the first part of 50 time slots and the second part of 50 time slots are different in each data stream. The data in each part are generated by three Gaussian distributions, in which the middle cluster is given the largest weight nearly 0.8. Thus it can be assumed that there are no more than three fuzzy sets for each attribute and the fuzzy set representing 


the middle cluster is more possible to be frequent and to be in the interesting rules. And the coordinates of the second parts centroids are varied while the first parts centroids are fixed at \(0, 0 50, 50 100, 100 stream is presented. The horizontal axis represents quantitative attribute X and the vertical axis represents attribute Y . Fig. 2 \(a Fig. 2 \(b V4-156 2010 2nd International Conference on Computer Engineering and Technology [Volume 4 50 0 50 100 150 40 20 0 20 40 60 80 100 120 140 inner box outer box X a Y 0 50 100 150 200 20 40 60 80 100 120 140 160 180 inner box outer box X b Y Figure 2. Data of Experiment One: \(a b In this experiment, 5, 0.3, 0.01,W min_sup T 


0.03, 0.5, 0.4,preMinsup min_confidence max_MFB _ _ 30.min num triples = In the fuzzy method, the membership functions are set with 1 3.0? = and 2 0? = . In the discrete method, two different partitions are used, and MFB_measure is also used to detect the changes of intervals Each attribute is partitioned into three intervals, and boundaries for the intervals are also determined by clustering algorithm. In Discrete1, the boundaries are set at 2 21.5 , 1,2j jm j? =  i.e. the second intervals are around the middle cluster just like the inner box in Fig. 2, while at 2 22.2 , 1,2j jm j? =  in Discrete2 just like the outer box The experimental results are showed in TABLE. . In TABLE. , MEAN denotes the coordinates of the middle clusters centroids in the second part for each data stream and NUM_C is the average times calling the clustering operations per attribute. NUM_FI, NUM_RULE are the total number of frequent fuzzy sets and interesting rules found in all sliding windows. TIME is the execution time. D1 denotes the approach Discrete1, D2 denotes the approach Discrete2 and F denotes the approach using fuzzy sets i.e. FFI_Stream algorithm. According to NUM_C, they only call one time of clustering algorithm per attribute when MEAN is near \(50 50 It indicates that these methods dont detect the small changes. And they call more than one time per attribute when the change is significant. It validates the usefulness of the MFB_measure. The method using fuzzy sets is more sensitive to drastic changes in datasets. It detects changes when the 110, 110 and Discrete2 at \(100, 100 method using fuzzy sets always finds the most frequent itemsets and interesting rules even when the corresponding NUM_C are the same. Discrete2 finds nearly as many interesting rules as the method using fuzzy sets sometimes but the region for the second interval is so large that the semantics of the rules become trival.  Thus comparing to the discrete method, the method using fuzzy sets gives better results B. Experiment  Two: Real Dataset In this experiment, the experimental results on the real life dataset are given. KDD_CUP99 dataset [6] which evolves significantly over time is choosed. KDD_CUP99 


dataset contains a stream of TCP connection records from two weeks of LAN traffic over MIT Lincoln Labs. It consists of 42 attributes that usually characterize network traffic behavior, both categorical attributes and quantitative attributes such as duration of the connections, protocol type etc. Attribute src_byte denoting the number of data bytes from source to destination and attribute dst_bytes inverse are selected in this experiment.  They are both quantitative attributes The user specified parameters are set as follows 5, 0.3, 0.03, 0.01,W min_sup preMinsup T 0.5, 0.4, 30.min_confidence max_MFB min_num_triples And the number of transactions in each time slot is 250 It is assumed that there are no more than three fuzzy sets or intervals in the datasets i.e. 3F Four different approaches to mine association rules are compared using the following notations: Fuzzy+MFB: the approach that use both fuzzy method and MFB_measure with 1 3.0? = and 2 0 .5 2 0.5? = , Fuzzy+P: the approach using fuzzy method with 1 3.0? = and 2 0.5? = also but repressing the first part of the MFB_measure that ignores the changed rate of the membership function, Discrete1: the approach using discrete method with 1 2 1.5? ?= =  and Discrete2 : the approach using discrete method with 1 2 2.5 TABLE I. RESULT OF EXPERIMENT ONE NUM_C NUM_FI NUM_RULE TIME\(s 1 \(50,50 2 \(60,60 3 \(70,70 4 \(80,80 5 \(90,90 6 \(100,100 7 \(110,110 8 \(120,120 9 \(130,130 10 \(140,140 Volume 4] 2010 2nd International Conference on Computer Engineering and Technology V4-157 0 0.5 1 1.5 2 x 104 


0 200 400 600 800 1000 1200 1400 Size of Databases\(250 Ex ec u tio n Ti m e\(s ec  Fuzzy+MFB Discrete1 Discrete2 Fuzzy+P Figure 3. Comparison of Execution Time Fig. 3 shows the execution time of the four approaches The runtimes of them grow linearly as the data stream grows which confirms that they are scalable with respect to the size of data stream, and it is mainly because of the usage of sliding window model. Fuzzy+P uses the least time Fuzzy+MFB has similar execution time to Discrete2, and Discrete1 has the most execution time. The difference of runtimes between them is mainly influenced by the clustering operations they use. The more clustering operations were executed, the more runtime it was Fig. 4 and Fig. 5 show the number of frequent itemsets and interesting rules found with the data stream increased Fuzzy+P and Fuzzy+MFB used less clustering operations than Discrete1 and Discrete2. Fuzzy+MFB nearly finds the most number of frequent fuzzy sets and interesting rules with the second least of clustering operations. Sometimes Discrete2 returns nearly the same number of fuzzy sets and interesting rules but with more clustering operations and the 


semantics of Discrete2 are meaningless as discussed in experiment one. Furthermore, the number of interesting rules found by Discrete1 is even less than Fuzzy+P that used the clustering operations least which illustrates the superiority of the method using fuzzy sets V. CONCLUSIONS In this paper, a novel fuzzy ARM algorithm called FFI_Stream is presented to tackle quantitative attributes in data streams and some techniques are proposed in the algorithm. Both synthetic and real datasets are used to evaluate the performance of the proposed algorithm. The experimental results show both the effectiveness and efficiency of the proposed algorithm.  In comparison with the discrete method, the proposed algorithm using fuzzy sets and MFB_measure gets a trade-off between the number of interesting rules and efficiency ACKNOWLEDGMENT This work is supported by The National High Technology Research and Development Program of China 863 Program 2008AA042902 Technology Research and Development Program of China 863 Program 2009AA04Z162 Project \(B07031 0 500 1000 1500 2000 2500 3000 3500 4000 0 2000 4000 6000 8000 10000 12000 Size of Databases\(250 N um be r o f F re qu e n t I 


te m se ts Fuzzy+MFB Discrete2 Discrete1 Fuzzy+P Clustering Operation Figure 4. Number of Frequent Itemsets 0 500 1000 1500 2000 2500 3000 3500 4000 0 1000 2000 3000 4000 5000 6000 7000 8000 Size of Databases\(250 Nu m be r o f I n te re st in g Ru le s Fuzzy+MFB Discrete2 Discrete1 Fuzzy+P Clustering Operation Figure 5. Number of Interesting Rules REFERENCE 1] R. Srikant and R. Agrawal, Mining Quantitative Association Rules 


in Large Relational Talbes, Proc. ACM SIGMOD, 1996, pp. 1-12 2] A.W. Fu et al. Finding Fuzzy Sets for the Mining of Fuzzy Association Rules for Numerical Attributes, In Proceedings of the First International Symposium on Intelligent Data Engineering and Learning \(IDEAL'98 3] C. M. Kuok, A. Fu and M. H . Wong, Fuzzy Association Rules in Large Databases with Quantitative Attributes, In ACM SIGMOD Records, vol. 27, 1998, pp. 41-46 4] X. Dang, V. Lee, W. K. Ng and K.L Ong, Incremental and Adaptive Clustering Stream Data over Sliding Window, Database and Expert Systems Applications, vol. 5690, 2009, pp. 660-674 5] M. Kaya,?R. Alhajj, F. Polat, and A. Arslan, Efficient Automated Mining of Fuzzy Association Rules, Database and Expert System Applicaton, vol. 2453, 2002, pp.133-142 6] http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html 7] S. Guha, A. N. Mishra, R. Motwani, L. OCallaghan, Clustering Data Streams: Theory and Practice,  Proc. IEEE Transactions on Knowledge and Data Engineering, vol. 15, May/Jun. 2003, pp. 515528 8] C. Aggarwal, J. Han, J. Wang, P. Yu, A Framework for Clustering Evolving Data Streams,  Proc. VLDB Conference, 2003,  pp. 81-92 9] C. K. S. Leung, B. Y. Hao, Mining of Frequent Itemsets from Streams of Uncertain Data,  Proc. IEEE International Conference on Data Engineering \(ICDE 09 10] C. C. Aggarwal, Y. Li, J. Y. Wang, and J. Wang, Frequent Pattern Mining with Uncertain Data, Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Jun. 2009, pp 29-37 11] P. M. Tsai, Mining Frequent Itemsets in Data Streams using the Weighted Sliding Window Model, Expert Systems and Applications vol. 36, Nov. 2009, pp.11617-11625 V4-158 2010 2nd International Conference on Computer Engineering and Technology [Volume 4 


In all charts reported in this section, the X-axis is k, which denotes the size of sample under the space of a target rule drawn from deep web. The sample size for each point on X-axis is k x, where x is a ?xed value for our experiment, and depends upon the dataset. At each time, queries are issued to obtain kx data records under the space of a target rule. Overall, all our experiments show the variance of estimation, sampling costs and sampling accuracy with varying sample size Figure 1 shows the result from our strati?ed sample methods on the US census data set. The size of pilot sample is 2000, from which all of the 50 initial rules are derived. In this experiment the ?xed value x is set to be 300, which means the smallest sample size at k = 1 is 300, and the largest sample size at k 10 is 3000. Figure 1 a the ?ve sampling procedures. Figure 1 b cost for the sampling procedures. In order to better illustrate the experiment result, in each execution of sampling, the variance of 330 6DPSOLQJ9DULDQFH            9D UL DQ FH R I V WL PD WL RQ  


9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW           6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF            5 


 9DU 9DU 9DU 5DQG c Fig. 1. Evaluation of Sampling Methods for Association Rule Mining on US Census Dataset estimation and sampling cost for the sampling procedures var7 var5, var3, and rand are normalized by the corresponding values of Full Var. Thus, in our experiment, the values of sampling cost and variance of estimation for sampling procedure Full Var are all 1. Furthermore, Figure 1 c sampling procedures From Figure 1 a pared with sampling procedures Var7, Var5 and Var3, Full Var has the lowest estimation variance and the highest sampling cost. From sampling procedures Var7, Var5, and Var3, we can see a pattern that the variance of estimation increases, and the sampling cost decreases consistently with the decrease of the weight for variance of estimation. At the largest sample size of k = 10, the estimation variance of sampling procedure Var3 is increased by 27% and the sampling cost is decreased by 40 compared with sampling procedure Full Var. The experiment shows that our method decreases the sampling cost ef?ciently by trading off a percent of variance of estimation. Similar to variance of estimation, the sampling accuracy of these procedures also decreases with the decrease of the weight on variance of estimation. For the largest sample size at k = 10, we can see that the AER of sampling procedure Var3 is increased by 20 compared with sampling procedure Full Var. However, for many users, increase of the AER will be acceptable, since the sampling cost is decreased by 40%. By setting the weights for sampling variance and sampling cost, users would be able to control the trade-off between the variance of estimation, sampling cost, and estimation accuracy In addition, compared with sampling procedure of Full Var Var7, Var5, and Var3, sampling procedure Random, has higher estimation of variance, sampling cost and lower estimation accuracy. Thus, our approach clearly results in more effective methods than using simple random sampling for data mining on the deep web Figure 2 shows the experiment result of our proposed strati?ed 


sampling methods on the Yahoo! data set. The size of pilot sample on this data set is 2,000, and the ?xed value x for sample size is 200. The results are similar to those from the US census dataset. We can still see the pattern of the variance of estimation increasing with the decrease of its weight. Besides, the sampling accuracy is also similar to the variance of estimation. However although the variance estimation of sampling procedure Random is 60% larger than sampling procedure Full Var, the sampling cost of Random is 2% smaller than Full Var. This is because Full Var does not consider sampling cost. It is possible that Full Var assigns a large sample to a stratum with low ?, which denotes the probability of containing data records under the space of A = a, resulting the larger sampling cost than that of simple random sampling. Sampling procedures Var7, Var5, Var3 consider sampling cost as well, and have smaller variance estimation and sampling cost, compared with Random. Furthermore, Random has smaller sampling accuracy than Full Var, Var7 and Var5, but has larger sampling accuracy than Var3. This is because Var3 assigns much more importance to the sampling cost, and loses accuracy to a large extent To summarize, our results shows that our proposed strati?ed sampling are clearly more effective than simple random sampling on the deep web. Moreover, our approach allows users to tradeoff variance of estimation and sampling accuracy to some extent while achieving a large reduction in sampling costs B. Differential Rule Mining In this section, we present results from experiments based on differential rule mining. Particularly, we look at the rules of the form A = a ? D1\(t t categorical attribute and t is an output numerical attribute, while other categorical attributes in the data set are considered as input attributes In this experiment, we also evaluate our proposed method with different weights assigned to variance of estimation and sampling cost. Five sampling procedures, Full Var, Var7, Var5,Var3 and Random, have same meanings with those in the experiments of association rule mining. Similarly, 50 rules are randomly selected from the datasets, and each of the 50 differential rules are reprocessed 100 times using 100 different \(pilot sample, sample iterations 5000 runs First, we evaluated the performance of these procedures on 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


