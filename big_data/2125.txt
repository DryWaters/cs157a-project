A Sequential Pattern Mining Algorithm Based on Improved FP-tree Yi Sui 1 FengJing Shao 1 RenCheng Sun 1 JinLong Wang 2  1 College of Information and Engineering, Qingdao University sfj@qdu.edu.cn 2 School of Computer Engineering, Qingdao Technological University       Corresponding Author Abstract  Sequential pattern mining is an important data 
mining problem with broad application. Most of the previously developed sequential pattern mining methods need to scan the database many times. In this study, STMFP algorithm based on improved FP-tree is presented for sequential pattern mining. By improving the FP-tree structure, every node of the tree can store a set of items instead of one item. After scanning the sequential database once time, the tree can store all the sequences. In addition, a novel mining method combining nodes from leaf to root which helps mining sequential patterns, is proposed. The cost of mining pattern sequence is divided into two parts. One is to construct STMFP Tree. The cost of this part associates 
with the size of sequential database. Another one is to find random assembled nodes from leaf to root in every path of STMFP tree. Because the maximal length of path is bounded by the maximal length of one transaction, and there are exiting common nodes which help reduce the number of leaf nodes, so the cost of this part must be much less than the size of the database. Compared with other methods which need to scan the sequential database many times, the cost of our method must be less than two passes of the database. Through the whole mining process, it only needs scan the database once time  1. Introduction  
Sequential pattern mining, which discovers frequent subsequences as patterns in a sequence database, is an important data mining problem with broad applications. Sequential pattern mining is the extension of the association rules. Many sequential pattern mining algorithms, which are based on the methods used in the association rules, are proposed [1, 5, 6  T h e y are pres en t e d f o r m i n i n g  s e qu e n t i a l pat t e rns  on the basis of  the Apriori heuristic firstly put forward by Agrawal and Srikant in association mining. The 
Apriori heuristic is that any super-pattern of a nonfrequent pattern cannot be frequent. Though they perform well, the Apriori-based methods must scan the database frequently. When the length of each sequence grows, this bears a nontrivial cost. Zaki proposed the algorithm SPADE [8 w h i c h ge ne r a t e s nu me r o u s  candidates. Algorithm FreeSpan m s at in te g r ati ng the mining of frequent sequences with frequent patterns and using projected sequence databases to confine the search and the growth of subsequence fragments. It scans the database three times. Algorithm 
PrefixSpan m prov ed f r o m FreeSpan redu ce s th e  size of projected databases by using prefix-projection substantially. It scans database twice, which leads to efficient processing. However, the above-mentioned methods still need scan the database at least two times The cost is nontrivial 4 pr opos ed an al go ri t h m us i n g F P t r ee, w h i c h  is efficient and scalable for mining frequent patterns Many new methods [10, 11 h e bas i s of i m provi n g  FP-tree are presented in some practical application. We are inspired by [10  whic h proposed a combination method to mining frequent patterns opos ed s o me  
good ideas  on data mining. Thanks for what they have done  In this paper, we develop a new sequential mining method, called STMFP \(Sequence Tree for Mining Frequent Patterns\. By improving the FP-tree structure  ery n ode of th e tree can s t ore a s e t of ite m s  instead of one item. After scanning the database once time, the tree can store all sequences information. In addition, a novel mining method, combining nodes from leaf to root which helps mining sequential patterns, is proposed. The cost of mining sequential patterns is divided into two parts. The first part is 
constructing STMFP Tree. The cost of this part associates with the size of sequential database. The second part is to find all combinations from leaf to root node in every path of STMFP tree. Because the maximal length of path is bounded by the maximal length of one transaction, and there are exiting common nodes which help reduce the number of leaf nodes, so the cost of this part must be much less than the size of the database. By contrast with other 
Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing 978-0-7695-3263-9/08 $25.00 © 2008 IEEE DOI 10.1109/SNPD.2008.161 440 
Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing 978-0-7695-3263-9/08 $25.00 © 2008 IEEE DOI 10.1109/SNPD.2008.161 440 


methods, such as Apriori-like, FreeSpan, PrefixSpan which are needed to scan the database many times, the cost of our method must be less than two pass of the database. Through the whole mining process, it only needs scan the sequential database once time. Our performance study shows that STMFP algorithm mines the complete set of patterns The remaining of the paper is organized as follows In Section 2, we present our new method STMFP. In Section 3, we present the performance study and experiment results. We summarize our study in Section 4  2. STMFP Algorithm 2.1. Terminology Definition 1\(STMFP-tree STMFP-tree consists of one root node labeled as “root”, a set of itemset subtrees as children of “root”. Each node in the itemset subtree represents one itemset of sequence, we called it itemset node Definition 2 \(itemset node Itemset node in the itemset subtree consists of three fields: parent, itemset child, where parent links to its parent node in the STMFP-tree; itemset is pointing to a item chain table which consists of three fields: name, count and next where name represents item’s mark, count registers the number of transactions represented by the portion of the path reaching this itemset node, and next links to the next item which also belongs to the same node child is a pointer pointing to a chain table which consists of two fields: node and next, where node links to its one child node in the STMFP-tree, next links to its next child By adding field itemset, one node in this tree can indicate itemset of the sequence and number of this itemset Definition 3 Supposed that the number of item one itemset node includes is n, then we can denote the itemset node as \(name 1 name 2 name i name n  count 1 count 2 count i count n ame i and count i respectively stand for name and count of the item i in the itemset node. For example, one itemset is denoted as \(I1-I5:4-1\hich shows that the number of the itemset \(I1-I5\ing this node is one, the number of itemset I1 reaching this node is three Definition 4 \(combination of nodes in the STMFP-tree path It consists of  random assembled nodes from leaf node N to “root” but except “root” in the same path. We combine these nodes from top to bottom in the growth direction of the STMFP-tree. We denote one combination of nodes in the STMFP-tree path L as C\(node 1 node 2  node i  node n  count n<=length\(L\he node i registers the name of itemset node i in the path L. We set the value of count is node n s count. If one item’s count in node i is less than the count of C, we omit this item in the node i  Supposed that STMFP-tree shows in figure 1  Fig 1.  Structure of STMFP-tree Example 1 For leaf itemset node I4, it can generate a set of combinations  as follow: \(I1-I5 I2 I3 I4 2 I1-I5 I2 2 I1-I5 I3 2 I1-I5 I4 2 I2 I3 2 I2 I4 2 I3 I4 2 I1-I5 I2 I3 2 I2 I3 I4 2 I1-I5 I2 I4 2 I1-I5 I3 I4 2  Since the count of item I5 in the itemset node \(I1-I5:41\ is less than the count of combinations we omit item I5, then the set of  combinations in final follows as: \(I1 I2 I3 I4 2 I1 I2 2  I1 I3 2 I1 I4 2 I2 I3 2 I2 I4 2 I3 I4 2 I1 I2 I3 2 I2 I3 I4 2  I1 I2 I4 2 I1 I3 I4 2  Definition 5 \(decomposing C Supposed C\(node 1 node 2  node i  node n  count n<=length\(L\hen node i has no less than one item for example, node i can be denoted as name i1 name i2 name ij we decompose C to a set  as follows node 1 node 2  name i1  node n  count  node 1 node 2  name i2  node n  count     node 1 node 2  name ij  node n  count and then for each combination in this set ,we repeat this step until there is no node i having more than one item 2.2. The idea and process of STMFP algorithm 2.2.1. The construction of the STMFP-tree The mission in this phase is to construct the branches in the STMFP-tree by scanning the database once time. The STMFP-tree can be made in following steps. Getting the current itemset in the transaction, if current itemset node’s child has items which are a subset of the current itemset, we increment this child node’s count by 1 and add remaining items in the current itemset to the item chain table of this itemset node, otherwise we create a new node, with its count initialized to 1, its parent linked to current itemset node, and its item linked to chain table initialized by items belong to the curremt itemset. And we make the new node as current itemset node. Then we get next itemset in the transaction, and repeat until all itemsets in one transaction have been 
441 
441 


stored. We treat every transac tion in this way. So far STMFP-tree contains the complete information for frequent-pattern mining    Example 2 For sequence database DB in Table 1 with min_supp=2, firstly, the root of the tree is created and labeled with “root”. Secondly, scanning the first transaction leads to the construction of the first branch I2:1\, \(I3:1\ \(I4:1\Itemset node \(I1-I5:1-1\as item chain table which is composed of items I1 and I5. For the second transaction <{I1} {I3} {I4} {I3 I5}>, since its first itemset shares a common item name I1 in itemset node\(I1-I5:1-1\e increment the count of common item I1 by 1.Then we create a new nodes \(I3:1\d link it to node \(I1-I5:1-1\ as a child. As well, a new node \(I4:1\ created and linked as the child of \(I3:1 a new node \(I3-I5:1,1\s created and linked as the child of \(I4:1\ For the third transaction <{I1} {I2} {I3 I4}>, since there is existing path <\(I1-I5:2I3:1\,\(I4:1\at is to say, there are four shared nodes, we just need increment their counts by 1 respectively. The result is changing the path into <\(I1I3:2\,\(I4:2\When scanning of the fourth transaction, the common node \(I1-I5:3-1\s changed to \(I1-I5:4-1\d the common node \(I3:1\s changed to.\(I3:2\ An new node \(I5:1\ is created as a child node of \(I4:1\r the last transaction, we can generate another branch in the STMFP-tree I4:1\\(I5:1\The STMFP-tree is shown in Fig 2  Fig. 2.  STMFP-tree of Sequence DB 2.2.2. Mining the STMFP-tree For every leaf node in the STMFP-tree, we can get the path form it to root”. See from definition 4, we generate a set of combinations CF. See  from definition 5, we decompose some combinations and then we check the set CF. If there have been the same combinations in CF then we merge their count. Otherwise, we put them into the set CF. Finally, compared with the value of minsup, we can omit those combinations whose counts are less than minsupp, and then we can mine the complete patterns Example3 Mining the STMFP-tree\(shown in Figure 2\or leaf itemset node \(I3-I5:1-1\we generate combinations as follows: \(I1 I3 2 I1-I5 I3I5 1 I1-I5 I4 1 I3 I4 1 I3 I3-I5 1 I4 I3-I5 1 I1-I5 I3 I4 1 I1-I5 I4 I3-I5 1 I3 I4 I3-I5 1 I1-I5 I3 I4 I3I5 1 We put all of them into a set CF. For those combinations met definition 5, we decompose them The decomposed combinations are: \(I1 I3 1 I1 I5 1 I5 I3 1 I5 I5 1 I1 I4 1 I5 I4 1 I3 I3 1 I3 I5 1 I4 I3 1 I4 I5 1 I1 I3 I4 1 I5 I3 I4 1 I1 I4 I3 1 I1 I4 I5 1 I5 I4 I3 1 I5 I4 I5 1 I3 I4 I3 1 I3 I4 I5 1 I1 I3 I4 I3 1 I1 I3 I4 I5 1 I5 I3 I4 I3 1 I5 I3 I4 I5 1 For those having been in the set CF, we merge their count. For those not having been in the set CF, we put them into the set CF At last step, by comparing with the value of minsupp we delete those combinations whose counts are less than minsupp. For the remaining leaves, we repeat this step. In final, we can get the completed sequential patterns. For this example, the sequential patterns we can get are: \(I1 I2 2 I1 I3 4 I2 I3 2 I1 I2 I3 2 I1 I4 3  I2 I4 2 I3 I4 3 I1 I2 I4 2 I1 I3 I4 3 I2 I3 I4 2 I1 I2 I3 I4 2 I1 I5 2 I3 I5 2 I3 I5 2 I4 I5 3 I1 I3 I5 2   2.3. STMFP algorithm 1\ Algorithm 1\(STMFP-tree construction Input: A transaction database DB Output: The root of STMFP-tree Method: The STMFP-tree is constructed as follows Create the root of an STMFP-tree, and label it as root Open DB while\(not EOF  Let the Trans be [p|P w h ere p is th e firs t ite m s et  and P is the remaining list call insert-tree\([p|P  point the next trans  Procedure insert-tree\([p|P t _n od   While \(p   TABLE  I  S EQUENCE DB TID Customer Sequence T001 1 5} {2} {3} {4 T002 1} {3} {4} {3 5 T003 1} {2} {3} {4 T004 1} {3} {5  T005 4} {5 
442 
442 


flag=false  for\(every childi of root_nod\**denote child node of root_nod as childi  flag=Find\(childi, p\; /**if there is a childi node which has the same item with the item of p we increment the childi’s count by 1 and flag=true if\(flag InsertItem \(childi, p\*we add remainder in p to the item chain table of childi  else newNode=root_nod->InsertNode \(p create new node childi=newNode  p=the next itemset in P P=the remaining itemsets in P insert-tree p|P ildi    2\lgorithm 2 \(STMFP-growth: Mining sequential patterns with STMFP-tree in way of combination Input: The root of STMFP-tree and the minimum threshold  Output: The complete set of sequential patterns Method: call STMFP-growth \(root  Procedure STMFP-growth \(root   For\(every leaf node N in the STMFP-tree  Get random assembled combinations C 1 C 2  C m from the leaf node N to root  except root For\(i=1;i<=m;i  For\(for every itemset i in C i   Delete  i item\;/**if there exists item having less count than N, we delete this item in C i   C i count=N.count InsertCF\(CF C i if there exists the same C i in the set CF, we merge their count, otherwise we store C i  decompose C i into C i  InsertCF\(CF Ci if there exists the same C i  in the set CF, we merge their count otherwise we store C i in the set CF   delete those whose count is less than    3. Performance Study and Experimental Results In this section, we report our experimental results on the performance of STMFP in comparison with PrefixSpan, FreeSpan. It shows that STMFP outperforms other previously proposed methods and is efficient and scalable for mining sequential patterns in large databases 3.1. Demonstrating the validity of the better performance of STMFP algorithm Lemma Given a transaction database DB and a support threshold The cost of constructing STMFPtree is DB|\, the cost of generating combinations is m*height \(STMFP-tree\where the height of the tree is bounded by max  m is the number of the leave nodes in the tree\(m<<|DB|\, the cost of searching combinations can be reduced by distributed computing Rationale The cost of mining pattern sequence is divided into two parts. One is to construct STMFP Tree. The cost of this part associates with the size of sequential database. Another one is to find all combinations from leaf node to root node in every path of STMFP tree. Because the maximal length of path is bounded by the maximal length of one transaction, and there are exiting common nodes which help reduce the number of leaf nodes, so the cost of this part must be much less than the size of the database. Compared with other methods which need to scan the sequential database many times, the cost of our method must be less than two pass of the database. Through the whole mining process, it only needs scan the database once time 3.2. Experiment Results The synthetic datasets we used for our experiments were generated using standard procedure describe in l l t h e ex peri m e n t s are perf or m e d on 2.40GHZ Pentium PC machine with 256 megabytes 
443 
443 


main memory, running Microsoft Windows/NT STMFP algorithm is implemented using C language. The experiments show that STMFP-tree can mine complete set of patterns. We use C10T8S8I8 as input dataset and set the minsupp=0.5%. We can get complete set of patterns. Because of the huge result, we cull some results as example \(The part in front of colon is sequential pattern, and the other part is the support value 1937\ 0.010594 1772 6943\ 0.011463 7372\ 0.010606 1937 7372\ 0.010564 1772 6943\\(1937\ 0.010575 1772 6943\\(7372\ 0.010587 1772 6943\\(1937 7372\ 0.010547 1937 7372\ 0.011365 1937\ 0.010593 7372\ 0.010606 1937 7372\ 0.010562 1937\ 0.011085 7372\ 0.011108 1937 7372\ 0.011049 1258\ 0.013809 3279\ 0.014292 5577 5612\ 0.022377 6150\ 0.014085 7708\ 0.014006 8210\ 0.014053 5577 8526\ 0.022358  4. Conclusions  In this paper, we present a new sequential mining method, called STMFP \(Sequence Tree for Mining Frequent Patterns\. It bases on the improved FP-tree structure to store sequences after once scanning the database. And then a novel mining method in combination is proposed. After whole mining process we can mine the complete sequential patterns. The most important point is that we only need scan the database once time. Our performance study shows that STMFP algorithm has better performance when the database is large. STMFP algorithm is efficient and can mining the complete frequent patterns  5. References  1 R e k e s h A g r a w a l, R a m a kr is hna m Sr i k a n t Mi nin g  Sequential Patterns. Proceeding of 11th Conference of Data Engineering, Taipei, Taiwan, 1995. 3~14  we i Ha n, Jia n Pe i Fre e Spa n  Fre que n t Pa tte r n projected Sequential Pattern Mining[A  Pr oc 20 00  I n t  Conf Knowledge Discovery and Data Mining KDD 00 C  B o s t o n  MA A C M Pr e s s  20 00 3 5 5  3 59   3 J ia w e i H a n, J i a n  Pe i. Pr e f i x Spa n Mi ni ng Se que ntia l  Patterns Efficiently by Prefix-Projected Pattern Growth[J  I E E E Tr a n s a c tions  o n K now le dge a nd D a ta  Engineering, 2004.1~17 4 J ia w e i H a n, J i a n Pe i, Y i w e n Y i n Mini ng Fr e q ue nt  Patterns Without Candidate Generation. Proceedings of ACM SIGMOD Intl. Conferenceon Management of Data Weidong Chen, Jeffery Naughton, Philip A. Bernstein eds. ACM Press,2000. 1~12 5 R e k e s h A g r a w a l, R a m a kr is hna m Sr i k a n t Mi nin g  sequential patterns: Generalization and performance improvement C. In: Proc of the 5 th International Conference on Extending Database Technology \(EDB T96\. Berlin: Springer Verlog, 1996. 3~17 6 H m a nnila  H T o iv one n  a n d A  I  V e r k a m o. D i s c o v e r y of  frequent episodes in event sequences. Data Mining and Knowledge Discovery, 1:259~289,1997 7 R e k e s h A g r a w a l, R a m a kr is hna m Sr i k a n t Mi nin g  quantitative association rules in large relational tables. In proc. 1996 ACM-SIGMOD Int.Conf. Management of Data, pages 1~12,Montreal, Canada,june 1996 8 i S P ADE: An efficeien t alg o r it h m fo r min i n g  frequent sequences. Machine Learning \(J\, 2001, 42\(1~2 31~60 9 Sha o  Fe ng jin g, S un R e nc he ng, Y u Zh on gq ing  A n  Outlier-analysis Algorithm Based on the Reduction of Boundary Cells Influence Proc. Of the 8th Joint International Computer Conference,2002,536-540  G a o J un Shi B a iLe R e s e a r c h on Fa s t A s s o c i a tion R u le  Mining Algorithm. Computer Science Journal. 2005 Vol.32 No.3  Y u C hia n g Li, C h inC he n C h a ng. A N e w FPTr e e  Algorithm for Mining Frequent Itemsets. AWCC 2004 LNCS 3309, pages 26~277,2004  J  H a n J  Pe i, Y  Y i n a n d R  M a o: Mi nin g f r e q u e n t pa tte r n s  without candidate generation: A Frequent pattern tree approach. Data Mining and knowledge Discovery 8 2004\, 53~87 
444 
444 


B II*L\E#9I #&M\J+I%B  r r!0\n\n\rB\n\n\n   r  n    r"\n    n   n	\r\n	\n	\r  r"\r\r	\r!0\n r 0 r  n\r   r\r2\r r  r  0\n  n!"\r\r\n\r\n\n\r	BC n&*	70\r\r2\r7\n0\r n\r\r\n\n\r\n 7$\r"!\r	\r!%!\n\n    r r	\r$\r7\n   r"\r r$\r7\n   r"\r	7\n r\r%\n"7\n\n\r\n 2M  E	B n0\r\r\n"2\r0\r n\n\r  r  r    r 0\n2\r$\n\r\r 2M 2\r0 r r\n!\n\n r6\r"\r	\r r               B B  B      B B  C     B     B     B&B   B B&B   B&B    n\r\n\n0\r\r\r!27\n!1\n r\r\n	0	0\r\r\n\r  r r& \r$\r\r\n!2\n7\r\n r6\r"\r	%\r\n!27\n D  n  7\n  r\r  n  r\r2\r\n  r\r   r  n\r n!\r%\r\r\r6\r\r r r2\r\n0\r\r\n!&#"\r	\r  1\r\n\n\r\n"1\r r r 4\r&C&#\n\r\n2\r\r\r\r\n\r6\r r	\n\r!2\r$\r7 n\r\n7\r"!\r\n4\r\r\r\r  2\r7\n&#\n\n"7\n\r\r\r6\r r		\r!"!\n2\r\r\r\r r  r\n     2    r  r  7\n  n  0	0\r\n\r"!\r&907%7  r r r2\r\r\r	7\n"\r n\r\n%&&\r!2\r$\r7\n\n  r!2\r$\r7\n\n0&\n  r\n6"!\r\r7\r"!\r\n&C n!"7!\r\r\n!\r!$2\r    n\n2      r  r    77       r    r  n    r n&#\n"!\r\n\r\n!!	\n\r n r\r-"	\r!&#\n02 r:!"22;\n!\r\r!2 r$\r7\n&-\n	\r	\r\n\n  r0\n\r2\r\r	\r   J   n\r\n    r  6"!\r\n  7  r\n\n   2\r	\r!\n\r\r2\r\r"7\r\r\n	\r r\n\r	\n\r\r\n r\r\r\n2\n2/\r\r\n r!	!\r	\n""\r\r 2\n\n\r\r\n0\r n 7    n n\n  r\n  r    2    r r\r\n\r I6"!\r\n\r\n2\r	\r!\n\r2$\r\r n0\r\r\r\r2\r\r\n"\n\n20\r\r6\r  r	7	\n\r\n&907%\r  r\n\n2\r$\n\n\r\r!\n  r\n\r\r\r$\r  7\n&-\n\r0\n\n7\r6"!\r\n%\r r7\n\n!\r"\n\n\r	$\r\n"!\r\n r!"!\r\n\n7!$	7"\r!\n\r\n r\r6!\r\n7$1$\r\r r r\n\r\n0\r7\n\r"!\r\n\r 7	\r\r$\r 276"!\r\n\r5$\n\r	!\r n\r@J7\n\r\n\rA0\n\n&\r\n"\n\n2\r\r r\r\n\n\r2\r\r\r"\r\r\n\n 26"\r6"!\r\n 7%\r\n""\r\r\r r4\n\r\r	\n  IEI\(I I    0    n1    0    n\n r  n 2\r0\n\r\n\r!\nJ	*\r2\n\n  n\r\r\n   r\n  n\r\r\r  n  r\r  n    BB B       r  9\r\n  r  I\r"\n    r    r\r"3GG\n!&\r\r	\r&!G0\nG\rG%BB%\nV C B%&\r    1       r7  r  r\r  n  J2\r	!\r$\n\n  n\r\r n\r     r\n n\r\r\r n  n/\r\r  n!\r0  1 E	B3#"$2\r0\r\n\r#\n"!\r n\n\r6\r"\r	\r                  n n        n       r     


J+R-? ?-R3-J -#\\E J+#I\(L I#9\**-#- JI-L   n n\r%R&J!%W&S?:"\r  n\n\r 1	\n	17!\n  r+\n-\r\n*2 B%BB C       71    E2   n    r  n\r\r\n!\r	#\n1\n  n\r\r\n\r  3  r	\r9\r	\n*\r2\n\n0\r r n+\n	8\n\n#6\r!\r n\r\r\n r445	*'+,*$-\r\n\n\r\r\r\n*\r\r\n   BBB  I\n\r%9&R	%W&%S&S:-*\n\r n-	\r n7	 \n\r\nJ	"\r*\r2\n\n0\r n n\r\r\n\r r\n+\r\n	\n\r\r\r\n.\n/\r\r  n!\r0     E  R      I\n\r  E  E\n  9  r n\r	;\r\r"3GG000&\n&\n&GX\n\rG""\nGI    95  E      1    60    r\n  2\r r7\r  0.13  Y\n\n\n  B""&%3CB  R!2%&\(\n\n  r 8\r\n\r n\n29 r	\n\r\r  r\n-\r\n*\n\r  WZ\n%"\r 3BB  R!2%W& \n\r  r-8\r\n\r: \n\n29 r7\r  n    r  n\n    r!\r       W  Z  n  r  CBC  3 CCC B J%#&J	%,&J0:\r 3-10	2 n\r	\r r      n\r\r  n  r       r\n\n\r\r\r\n.\n/\r\r\n!\r0   BB    1    60  r\r  n  2\r\n 3BB  I&\(!%9&*:*\r 	&2!\n \r n   r\n\r\r	\n\r\r\n-:\r\r  MB&%*!2B C   71        9  L        J1    n\r\r  n  r    n\r\r\r  n r0\r\r\r  R&, :\r\n\r\r\n n\n n\r\r#6\r  n\r\r\n\r'\r\n	\n\r\r\r\n  r# \r\\n\r  W  22  n\n\r  n    r  n\n2  n     IA  J0  I2  n\n\r\n    3 CCC%B""%""BC    1  7  r\n  E    J1    n\r\r\n\r'\r\n\n'!\r0\r\r*\r\n  r  n\n  pp 467-472     1    r\r      J1    r   n 2!\n n\r\r\n\r'!\r0*\r\n'\r\n    n\r0\n  


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efÞcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efÞciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efÞciency When the clustering model is available it is a signiÞcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques SufÞcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efÞcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efÞcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conÞdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conÞdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conÞdence The sufÞcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efÞcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efÞciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufÞciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70Ð81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207Ð216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487Ð499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145Ð154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146Ð153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9Ð15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597Ð600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern ClassiÞcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155Ð162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512Ð521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1Ð12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265Ð278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283Ð304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476Ð482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559Ð563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220Ð231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13Ð24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10Ð17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549Ð550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188Ð201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259Ð283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909Ð921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305Ð345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432Ð444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conÞdence Intelligent Data Analysis  9\(4\:381Ð395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49Ð73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407Ð419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1Ð12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki EfÞciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642Ð658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483Ð490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194Ð203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344Ð353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efÞcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103Ð114 1996 
618 
618 


