TA6 11:00 USE OF LITERAL INFORMAlION IN MJULTI-TARGET DATA ASSOCIATION I.R Goodman Comuand  Control Department Code 421 Naval Ocean Systems Center San Diego California 92152 Abstract It has been shown that literal information can enhance geolocation information in the multi-target tracking and data association problem Thi's paper continues previous efforts in establishing a systematic approach to the combination of both types of information using membership functions based upon multiple-valued logic Filters are established for literal and non-numerical attributes somewhat analogous to the well-known Kalman filter 
The major result however is an improvement and clarification of a previous theorem establishimg asymptotic for ms for the posterior possibility distribution of the unknown data association parameter as information granularity decreases and as inference rule structures become more definitive 1 Introduction The multi-target tracking and data association or as comnnly called correlation problem still remains the center of much activity and interest In the past emphasis was placed upon the use of only geolocation data-i.e information containing reports on usually twoor threedimensional target positions together with possi 
ble velocities accelerations and related equations of motion parameters 1-31 More recently an effort has been carried out in utilizing in a more rigorous or systematic manner other types of infom ation  including various sorts of sensor systen parameters that could also be formally treated in terms of equations of motion In addition use of attribute information of radically different natures is also sought This includes visual sightings classifications and other,often non-numerical and linguistic-based or narrative which cannot be treated from traditional statistical analysis This work is presented 
in 4-6 based upon and related to,in general previous and ongoing researc?i in possibility and fuzzy set theory and its realtionships to classical probability theory and multi-valued logic and set theory 7-9 In this paper we continue to attempt to establish a unified approach to the integration of both types of Information for the multi-target tracking and data association problem However since the results are carried out on a general level with suitable modifications applications to medical diagnosis fault determination and other problems involving non-sequential 
knowledge-based systems can be established 9 As stated before this work still unfortunately requires both a sense of art as well as science in achieving its goal 2 Basic Problem The basic problem can be stated as follows Observed or reported data arrives,usually indexed by time t  attribute typo k and sensor source u For pWrposes of simplicity we will omit the last index and assume the multiple sensor source problem is resolved and plays no role in the analysis here Attributes A k=l,2,..,M can be either objective or statisdcal in nature or they can be subjective 
or linguistic in nature Denoting the natural domain of values which can be numbers vectors of nunbers lin2uistic labels etc of each attribute A as Dkdom\(A  statistical attributes typical19 have their dhmains D CRRnk with error distributions in the form of cldas ical probability functions typically being discretized versions of Gaussian or mixtures of Gaussian distributions Thus in general unless representing dirac or mass-point distributions the error prrobability functions are not normable,i.e they have maximal values less than unity due to the obvious constraint 
of probabilities adding up to one On the other hand subjective attributes Ak typically have D being some finite set of labels with no naturat spacial ordering present unless D represents a set of n tuples of measurement quintities where for examplbT,A7 large could have D7 consisting of pairs a,b with a being weight in lbs and b length in meters The error distributions corresponding to subjective attributes are often in the form of normable possibility functions-but not probability functions-obtained from a panel of experts in contrast 
to the geoblation-physical derivation of statistical attribute error distributions For motivation for such subJective distributions representing overlapping and/or vague compound events in contradistinction to statistical distributions representing disjoint exhaustive eventssee again 4 Each error distribution represents the conditional possibility possibilities include probabili-ties as special cases that a particular attribute domain value is actually present given an observed/reported domain value Whether statistical or subjective it is assused that at least theoretically each such distribution is obtainable 836 


In addition to error distributions and observed data a collection of inference rules is assumed to be present Each such rule R consists of an antecedent part ant and a condquent part conseqv connected by an impYication operation ant consists of the conjunction more generally s&me combination of conjunctions disjunctions and/or negations  only of modified matching tables for each attribute from some subset of all the attributes Each modification modv k of basic matching table M for is assumed  to be in the form of an ihtens ifcation or strengthening or an extensification or weakening More generally,modifications can involve positive and/or negative forms For simplicity we asstue throughout here that only positive forms are present.\(See 10 for more background on modifiers and hedges for attributes In ordinary English intensifications can be represented in a simple order as e.g  slightly very  very very,very  extresely  Extensifications can be expressed as e.g  very very little oft,..,"very little of",..,"more or less of",..,"moderate amount of The consequence consists of a single modification modv again either a positive intensification or extensification of the basic data association level Then as data arrives track histories are built up wlth some eventually discarded sequentially in time consisting of those data reports which are adjudged as belonging to the same target source The decision on associatinq together or not a given pair a previously established or tentative track history with a new reportis based upon the application of the set of inference rules and error distributions upon the two data vectors where it is understood that the previously established track history i is suitably updated for comparisons with the new report J Of course for a given track history the updating of the data history for a statistical attribute under linearMarkov or Gaussian assumptions,can be made using the now well-established Kalman filter 11 On the other hand the very different struetured subjective attributes require another approach For example A might be color of flag with D  red red-ora4n3,orange blue black str4 ed  or A might be class with D  C C  How 21 we filter"or predict aLong sub iJes Thus an attribute filter is sought for these cases But after a moment's thought the answer to this problem is relatively simple obtain the possibility analogues for the probability function situation This is spelled out in section 4 For a schematic view of one cycle of the data association process from one sampling time to the next see Figure 1 at the end of section 4 representing data assqgciatlon level between track histories i and j  CdAPs the attribute represent"'correlation between I dnd j AlIA2..IAMI Is the set of attributes which are statistical AMI+IsAMI+29...AAM is the set of subjective attr butes To indicate dependency and change due to sampling time tm where toc tl t2 t3   and track history i  denote Ak as Ak m with dlom\(A CiM DM  k,m k,m similarly replacing Dk D k=1 m ik,m Zk,m E Dk m are the possible true updated/smoothed-observed values for A4,respecti vely Ci Ci mC Z  iJi I  ii  7i k,,m Zk,m  ki,m Ooqti4 k   for any mi=O,1,2 i 0 etc ZM k,in k=l1,.M I ec Similar notation holds for the true possible attribute values We use primef to indicate the statistical part and double prime notation indicate the subjective part Zm inZm n Z i qz m 4 CZk,m   Z t DCI  Dm\(i DmI\(i  etc We use  to indicate possibility function OC-1 to Indicate conditional or dependent possibility function Pk M is the index denoting the conditional error possibility function for 1*;PCI D\(C CM k,m k,in k,,m A i  Thus  0.1 and 3 Definitions  Notation The notation used here is somewhat different from the previous and related paper 6 due to the ongoing effort to employ the simplest yet mnt accurate notation 10,1 A COYP Vaal N i E dom\(CP9j  0,11 is unknown parameter Zk m k\(m  Pki,m possibility ZkCi is true value for Aki given 1 is updated observed value Also P ismb 4\(pCi  Pk.i.M Pkim0 k\(5 k inn 0 k  k in in m k,k etc 837 


Four special possibility functions are indicated by nt 0,lJ  O,l decreasing or at least non-increasing function with boundary conditions coinciding with usual two-valued logic l=.nt\(O  260 nt  I representing negation or general set or attribute complement   0,1 240,1 a nondecreasing function in each of its arguments which is continuous,tmin associative syninetric and possesses the boundary conditions reducing to the two-valued logic ones 0=fa0 x0O  x x l  Cl x for a ll x E 0,1 This operatAr may bi unambiguously extended to an arbitrary finite number of arguments and represents conjunction or general set or attribute intersection or 2600,1 2+[,i a nondecreasing function in each of its arguments which is contlnuous max associative symmetric and possesses the boundary conditions reducing to the two-valued logic ones 1 or X,1 or l X  X=fOr X,O O,X for all x 243 0,1 This operator may be unambiguously extended to an arbitrary finite number of arguments and represents disjunction or general set or attribute union 0,1]2 a non-increasing function in its first argument and a nondecreasing function in its second argument with the two-valued logic boundary conditions l=#,C0,x  CX,l  0  1 0 for all x c 0,1 This operator represents logical implication  if then C i is also called in the literature a t-norm and  is called a t-conorm See 12 or r9 ChaptBF 2.3.6 for more details on these function classes Next,let Rv denote the vth inference rule with associated possibility function j*;R   XDC'J  0,1 v m somewhat abusing notation where there is an associated index set Jv fl,2 sM"}u where as usual Jy t n1 l sM 4v i f lM+l  M v Vvlml3 v 3v so that cev I Z\(V Rv 1 tbCant Zm j   Oys fante  Zis  O Omodk k\(Z m  kev Oconseqv eV mod OCO\(60 Denote also the index set R i Rv N tion Then we write for the overall error distribuW01j  i19 9 a n  mi J i,j P i,jf Z J  0j  m fdO 260\(Zk Zk kA  k=l De.,M Z k,m k,m km m k,m k,m with z\(Z ij defined similarly in tenns of O  For the overall inference rule effect we have C\(6ezIZ14R 4  CCe vIZ\(''j 2 v=l I,N Other notation and definitions will be introduced as needed From now on only the most important changes in t-norms and t-conormns will be specially indicated otherwi se the generic notation  or will be used 4 Basic Analysi s The following simple theorem is proven easily using the basic properties of conditional possibility functions as given in 6],[71 or 9 and can serve as the desired attribute filter Note alsp that in general the distinction\(omittin7 P ti',m   0  Z 242\(k,m Zk  k km z k  fo ZM\(i i fo k n k,m Theorem 1 Suppose omitting the p\(1 and4l for any k  kHM'+l,..,M  k,m Izki,m lk  k,m t k,m CI\(M-1 I Zk,m IZk,m-l SZk  CZk mZm  are known functions with Zk mlZk kv3-l  tc normable with respect Zk  m I tk  and without k,m k  a\(m loss of generality suppose that ZkmZk\(m  is normable Then Cl t*Uki sm I  X I Zck m m-l  k,m i k lm  1   ZI4 I m _   01 ZkO k,m-l Mi CZk,mI\(m is implicitly obtainable from k,m\(Z kl2  k,mlZ k   z I Z _l  1\(2k'm IZks 838 


Next for purpose of completeness the full PACT algorithm 4 fJ6 Chapter 9 will be presented For justification of the results see the above references The fi gI 3posteri gr 2ossibility function is e8|diagq,4 A R,PAi3   obtainable implicitly from the equation  again using simple notation dsa 011\(5 i  R P ttE jdi ag 2S  R jP diag    1jig univ diag t edia R,P 4 dia eIZ5J R,P C841A R,P ia e ij P;R,P 6 where 8j,,1cTiRI 6 e|Z\(i,gi i ij  8 z\(J l 2 J R aP O i Zf iJ 0\(j;RP  7 are obtainable as iln section 3 Furtherore in general  above breaks up into statisti cal and subjeBti ve components _\(1 i lorl in tl RP where ZmtZiJ J  10R  G\(a,Aij 0 8 8\(+H jPJ R P 1  j,l Zmj 1 tP 9  f esm  I i  ERsp  C,C!14Z\(i  Z'',\(ii i 12 where orl may be chosen as max Since in general ge\(a is a nondecreasing function in e with value 1 at o=l and if all modifiers are chosen ifn the positive sense as discussed in section 2 so that each inod kv\(x and mod Cx are nondecreasing functions of x with unity values at x1n such as is the case with exponentlals,then it follows from the property of  that the final pos teri or possi1bili 1ty function i s formall 1y i ke a distribution function except for not necessarily being 0 at a=0 Hence a reasonable measure of central tendency for e so described is  using formall y expectati on notati on E\(el diag,2J S0.d*\(ldiag,Z\(I R,P See also 6 for further discussion  A simplified outline of a data association procedure utril zng the PACT alngorthm as described above is given in Figure 1i Figure 1 Flow Chart for Data Association From One Data Sampling Time to the Next 5 Some Asymptotic Results Given the above scheme for obtaining the final posterior possibility function for the level of data association between two track histories the following natural question arises Can some analogue with the classical case of statistical consistency be established here In 6 it was shown that as the fineness of the domains of statistical attributes increasedand dually the granularity decreasedunder certain reasonable conditions,the posterior function for data association converged to a computable function of a statistical expectation In turn the behavior of this limiting expression as inference rules and error distributions became more precise was considered leading to under perhaps too stringent conditions a form of consistency In order to clarify this further a similar result under weakened conditions and with 839 


more specific applications will be presented Suppose the following additional assumptions hold relativeto the basic situation used to determinett posterior function of 8 a For all y c Rnk  fj  Iy Rnk Ri s a bounded continuous conditional probability density function for k=l,..,M',Lt-,j  for some chosen track histories i,j b For each integer p>l let D k\(p be the pth discretization and truncation of DRwlth A Ut  k S,m k,m,p denoting the mesh of D k\(mp i.e maximal length of any rectangle formed within the truncatlon area so that in any sense,,lim IkD\(tmp Rnk and 1 im A t  0 in addition replace each P+*k,m,p Zkm kt mt  by the approximating probability funct'ion 14 V Z JI LI1 Cz t Z\(L CL p Zk,m k,m k,m k k,m k m k,m Igp and denote accordingly all computations involving this replacement for k=l,..,M  t=i,ibwith p c e 3\(xk,t  is analytic about xk t=0 k=l t=l j d or is an Archimedean t-conorm with generator function h 0,1  R+U{4e}-whlch is continuous non-increasing with h\(l and h\(0   and is such that or\(x  x x min h\(t l x h\(0   or 1 q I w 15 for all x 243 0,1 q=1,2  See,e.g.,[12 and eq.\(23   e Referring to d h\(l-O x,y is analytic about x=y=0 in 0 t  f 61Z J  R is a continuous function in Zm  i allowed to be arbitrary in kpnkXXRnk This is guaranteed if  is continuous in its first argument and mod is continuous over 0,1 for k=l,..,M v=l Next define k=l  4 and  t 16 v\(x  d h\(s sC\(&Cx.y  17 for all x 243 0,1 Theorem 2 Assume the basic situation holds as established in the previous sections Suppose also that assumptions a f hold Then,assunlng or max f+=Lj 1 J 1 R P A v  eIC1'3  p4+m  l-h1\(mln\(p\(e O 18 where p\(e 4 max a\(e,Z"s b 19 c\(esZ i v+\(,s IO  20 a imi  CK'5E m~e where E v denotes ordinary statistical expectation with respect to random vqctor V replacing the argument non-random Z,kisi which has prob ability density function f where at any value O X for V,f has the conditional form f Si13 z\(i 9i s7 f Z m|f m m m Kk m'k,m'k,m 21 Proof Use the canonical expansion for orl as in 15 where q is replaced by a summation over Dm\(iJ and x i's replaced by Gp\(O,Z 9  In turn expand expand h\(l-G esZ h  in terms of variable fp 1 3 Ip  around 0 obtaining h\(1-G 2e 1'3  iJ Zm 4m p 1  H\(5Zm  m r  0f\(6  Z\(i j 1  rjl 2 Also expanding for A L small where mp  p k A 1 k p  M The result then follows from the definition of an integral  Remark An important famnily of t-norms and t-conorms is due to Frank 13 See also 12 These satisfy the basic modular relation or\(x,y  x  y x.y  22 for all x,y 243 0,1 Frank has shown that this family can be characterized by the Archimedean class i.e all  or such that for all x c 0,1 23 x,x and or\(x,x s which i's also DeMorgan i.e 840 


for\(X,Y  1 l-x l-y  24 for all x,y 243 0,13 and as wel by the class of all ordinal sums C typesof affine transfor involving the block diagonal parts of 0,1 see 9],[12 of subsets of the Archimedean class Indeed the Archimedean class of Frank contains many common t-norms and t-conorms and can be conveniently parameterized using parameter s  s\(x b b Xq log'\(lt SX-l  with or\(xl..xq determined from the DeNorgaA25 or modular relations for all x 1.,uxq e 0,1  with generator function hs given as h x  ogtsXl s-l x 243 0,1 26 vaa^;.ey8iC sl"f darG\260aenFIti"B ie ate St Syseus Academic hedt k dep ahSSr8N.b Processe and Fl ter3  Bar iTrachQgLnf.Mtl9gg 4 duf3^g rde,E EzySt Ss,Aaei p e8>E~\247 i,*aProb*^isticMetric Ri 7odau i o2&.4b kIVM c of 841 5 for all Ocsct  where the special cases are s=O non-Archimedean  O X1  xq  min\(x  xq Or,O\(x1'D sxq m max\(xf 6xq 27 ho\(x 40  if Ocxsl S=V O1\(x1 xq  x1 N xq O'or,l  s  xq P robsu\(xilb S xq 28 hl\(x  log\(x  all x 243 0,1 S=4e x1x...xq min\(xi+..+xqIl or co{xl 5  xq  xi  xq-\(q-l 29 h+.Cx  l-x  all x c 0,1 It follows that if all t-norms and t-conorms used in the computations for the posterior function of e are culled from Frank's family and the assumptions a f hold then Theorem 2 is valid and the key computations for K and v are For generator h5 of or si  Cd hs x _ 0<s'<c+Ws'#I CI\(s'=l   30 For sI sX-l s-l a  X.Y  O<S<ce,sOl s=l   31 For  Ps O&MS1  o0 s K O<s 1 4,n lg1 s'-l  32 Finally let us consider briefly the effect of the choice of inference rules and the accuracy of the error distributions upon the asymptotic posterior function for e as given in Theorem 2 So far iatching tables have not been discussed In general although the basic strutures of the inference rules do not depend upon time and the particular pair of track histories in question the matching table does For subjective attributes these could be chosen as some type of symmtrlzation of the corresponding error distributions while for statistical attributes the matching tables can be naturally chosen as M i'J  I-F cxcz\(iJ 33 where k,m nk k m 34 k,m k,m k,m km km k k,m and Fn is the probability di;tribution function nk of x corresponding to the Z\(t being r.v.'s with a connon expectation bqt oflerwise described by independent p.d.f.'s f kt as for V in Theorem 2 Note also by inspection that c e,z.iJ 1  non-hereasing f tion in the vaHablEs x\(Z,|ii 01 Zj1J   kZ;J provided that we cTooWe for example fmod x and mod x V'sk v for some constant positive ao  bv  all xe[O,l It then easily follows that if all error distributions are not in dirac-form but indicate some spread or confusion between observed/smoothed data then if we can legitimately obtain for the inference rules the consistency structure given by all avk approac hing the extreme extensification 0 with all b approachin he extreme intensification W  for O%6<l eIZ ii approaches O for fixed values of X  and imil3rly fort On the other hand for 8=1 the latter is maA mized The consequence of this,applying standard inequalities to the expectation E C in Theorem 2 and making the additional mild asdumption true for Frank's family for saf that v\(x is non-decreasing j  0,1 with v\(0 l that eI di ag,Z  R,P approaches the possibility function  representing complete data association between i afd  i.e.11 iff 8=1 W\(8 f 6~;f f 35 Future work will be directed toward extending further quantifying and establishing empirical bases for the above study References 5.Ge~u La"I  6p41c\243rportipp4f4uzlxseteorytCor B3 1jao-S ezI ranfkifl Stac3leg-2 Li 6 g 


DE&A Update BCD&A Update BCE&A Update BDE&A A,@D E AR AR AR Update CDE&A Rule R 6R A  B  C,@D Rule R 6R A,@B C,@E Rule R 6R A  B  D,@E Rule A,@C AR R 6R 8 9 IO 11 12 13 14 Update BCDE A Support  Count\(C,D,A Confidence8  Count\(C,D A  If Support8  S C,D  A 100 Count\(C,D  100 And Confidence  C Support  Count\(C,E,A Confidence  Count\(C,E A  If Support  S C,E  A 100 Count\(C,E  100 And Confidence 2 C Support  Count\(D,E,A Confidence  Count\(D,E A  If Support t S D,E  A 100 Count\(D,E  100 And Confidence C Support  Confidence Count\(B,C,D,A  If Support S B,C,D  Count\(B,C,D,A 100 Count\(B,C,D  100 And Confidence C A Support  Confidencel2 Count\(B,C,E A  If Supportl2 S B,C,E  Count\(B,C,E,A 100 Count\(B,C,E  100 And Confidencel2 C A Support  Confidence13 Count\(B,C,D A 1 If Support S B,D,E  Count\(B,C,D,A 100 Count\(B,C,D  100 And Confidence C A Support  Confidence,?= \(Count\(C,D,E A  If Supportl4 S C,D,E  Count\(C,D,E,A 100 Count\(C,D,E  100 And Confidencel4 C A A  B  C,@D,@E I I Tuple Tuple Tuple Tuple Tuple GR.A=@A and GR.D=@D and SR.E=@E If Select  from R where GR.A=@A and GR.B=@B and GR.C=@C and FR.D=@D If Select  from R where GR.A=@A and GR.B=@B and SR.C=@C and GR.E=@E If Select  from R where GR.A=@A and GR.B=@B and SR.D=@D and GR.E=@E If Select  from R where GR.A=@A and GR.C=@C and GR.D=@D and GR.E=@E If Select  from R where GR.A=@A and GR.B=@B and GR.C=@C and FR.D=@D and GR.E=@E set count\(D,E\=count\(D,E Set count\(D,E,A  count\(D,E,A where rule=lO Update AR set count\(B,C,D\=count\(B,C,D Set count\(B,C,D,A where rule=l 1 Update AR set count\(B,C,E\=count\(B,C,E Set count\(B,C,E,A  count\(B,C,E,A where rule=l2 Update AR set count\(B,D,E\=count\(B,D,E Set count\(B,D,E,A where rule=l3 Update AR set count\(C,D,E count\(C,D,E Set count C,D,E A count\(C,D,E,A where rule=14  count B C ,D A    count \(B D E A     Update AR set count B C,D ,E count\(B,C,D,E Set count\(B,C,D,E,A count\(B,C,D,E,A where rule=l5  127 


 Confidencel5 Count\(B,C,D,E If Supportl5 S B,C,D,E  A B C a1 bl Cl an bn cn al\222 bl\222 Cl\222    We can create an association rule table AR to hold its I Parameter count Association Rule table AR with timestamp at Date2 transaction occurrence count as follows I Rulecount I Potential association rule D E dl el dn en dl\222 el\222   We can use the same calculation as described in defined in Constraint class error message will be The situation is that an online shopping centre has a computing association rule continuously except returned to the user \(refer to Figure 3 the total transaction record count of source relation Date2 is n  m 5 Prototype     collection of commodities from television to fruits In Figure 4 we process web log file into Main Table Merge Cookie information Datez I bm cm I dm A prototype has been implemented for frame object agent model which is an Application program Interface developed to invoke the data operation in the frame model for data semantics management The API process handles the method defined by Constraint class during data modification The process defined and executed by RDBMS is to provide triggering event fired on the class When RDBMS process is invoked it will check from the Constraint class for any associated method If method is defined in the Constraint class the method definition will be queries The process will invoke the stored procedure defined by the queried method definition If executed result returned by stored procedure violated the rule Figure 4 Process M In figure 5 we transform Main Table into Transaction Table corresponding to procedure 0 in figure 3 In figure 6 we process Transaction Table into Concise Transaction Table corresponding to procedure  and 0 in figure 3 em leb Loe File 128 


 Figure 5 Process Main Table into Transaction Table D Figure 6 Process Transaction Table into Concise Figure 7 I f bl 411 significant association rules for the target attribute television confidence=20 support=0.2 In Figure 7 we select television as our target and get the rules based on confidence level 20 and support 0.2 6 Conclusion This paper presents a solution to the problem by using a frame metadata model to invoke record count program in a metadata whenever relevant new record is created in the target database As a result web mining association rules can be accomplished online or incrementally without the need to re-compute the historical web transaction counts The metadata thus contributes saving of computing time and also the automation of data mining association rules non-stop The future research of this paper is web mining web pages association rules via Internet References I Chen Q Hsu M and Dayal U A Data Warehouse/OLAP Framework for Scalable Telecommunication Tandem Traffic Analysis a report of Software Technology Laboratory HP Labs 1501 Page Mill Road MS 1U4 Palo Alto CA 94303 USA Robert Cooley, Bamshad Mobasher and Jaideep Srivastava Data Preparation for Mining World Wide Web Browsing Patterns Journal of Knowledge and Information Systems Vol 1 No 1 1999 Robert Cooley, Bamshad Mobasher and Jaideep Srivastava Grouping Web Page References into Transactions for Mining World Wide Web Browsing Patterns Proceedings of the 1997 IEEE Knowledge 2 3 41 I51 t61 I71 and Data Engineering Exchange Workshop \(KDEX 97 November 1997 A.G Buchner M.D Mulvenna S.S Anand, J.G Hughes An Internet-enabled Knowledge Discovery Process the 9th International Database Conference Hong Kong pp 13-27, July 1999 Cookie A.G. Buchner, M. Baumgarten S.S Anand, M.D Mulvenna J.G. Hughes Navigation Pattern Discoven from Internet Data ACM Workshop on Web Usage Analysis and User Profiling WebKDD San Diego CA pp. 25-30 1999 Mena Jesus Data Mining Your Website Digital Press 1999 pp 3 13-3 14 Moore J Han E Boley D Gini M Gross R Hstings K Karypis G Kumar V and Mobasher B Web Page Categorization and Feature Selection Using Association rule and Principal Component Clustering University of Minnesota M Mohania S Madria and Y Kambayashi Seu Maintainable Aggregate Views Proceedings of the 91h International Database Conference City University of Hong Kong Press ISBN 962-937-046-8 p.306-3 17 T.Griffin and L Libkin Incremental mainteriance of views with duplicates Proceedings of the International Conference on Management of Data 1995 Sunita Sarawagi Shiby Thomas and Rakesh Agrawal Integrating Association Rule Mining With Relational Database Systems Alternatives and Implications Takeshi Fukuda Yasuhiko Morimoto Shinichi Morishita and Takeshi Tokuyama Data Mining Using Two-Dimensional Optimized Association Rules: Scheme Algorithms and Visualization ACM 199 pp13-23 Eui-Hong Sam Han George Karypis and Vipin Kumar Scalable Parallel Data Mining for Association Rules ACM 1997, pp277-288 Pieter Adriaans and Dolf Zantinge Data Mining Addision Wesley ISDN 0-201-40380-3 Fong J Huang S Architecture of a Universal Database A Frame Model Approach lntemational Journal of Cooperative Information Systems Volume 8 Number 1 March 1999, pp.47-82 Fong J and Pang F Schema Evolution for New Database Applications A Frame Metadata model Approach Proceedings of Systems Cybernetics and Informatics, Volume 5 1999 pp 104-1 1 1 Fong J Huang S Information Systems Reengineering Springer Verlag ISBN 98 1-3083-15 0 R Zaiane M Xin, J Han Discovering Web Access Patterns arid Trends by Applying OLAP and Data Mining Technology on Web Logs Proceedings Advances in Digital Libraries Conference ADL\22298 Santa Barbara CA April 1998 pp 19-29 A.G Buchner M.D Mulvenna Discovering Internet Marketing Intelligence through Online Analytical Web Usage Mining ACM SIGMOD Record ISSN ACM 1998 343-354 8 1997 179-212 0163-5808 Vol. 27 NO 4 pp 54-61 1998 129 


ofimages 50 100 150 200 1 feature 50292 80777 127038 185080 2 obj identif 210 338 547 856 3 aux image 3847 6911 10756 13732 4 assoc rules 6 3 6 4 Table 2 Measured times in seconds for each Image Mining step with different image set sizes colors and contrast When the L was close to another shape its colors were merged making it dissimilar to other L shaped objects This suggests that irregular shapes in general make image mining dif\256cult We worked with color images but it is also possible to use black and white images Color and texture were important in mining the geometric shapes we created However we ignored shape as mentioned above Shape may be more important for black and white images but more accurate shape descriptors are needed than those provided by the blobs 4.3 Performance evaluation We ran our experiments on a Sun Multiprocessor forge.cc.gatech.edu computer with 4 processors each running at 100 MHz and 128 MB of RAM The image mining program was written in Matlab and C The 256rst three steps are performed in Matlab The feature extraction process is done in Matlab by the software we obtained from UCB Object identi\256cation and record creation were also done in Matlab by a program developed by us An html page is created in Matlab to interpret results The association rules were obtained by a program written in C In this section we examine the performance of the various components of the image mining process as shown in Table 2 for several image set sizes These times were obtained by averaging the ellapsed times of executing the image mining program 256ve times 4.4 Running time analysis Feature extraction although linear in the number of images is slow and there are several reasons for this If image size increases performance should degrade considerably since feature extraction is quadratic in image size Nevertheless this step is done only once and does not have to be repeated to run the image mining algorithm several times Object identi\256cation is fast This is because the algorithm only compares unmatched objects and the number of objects per image is bounded For our experimental results time for this step scales up well Auxiliary image creation is relatively slow but its time grows linearly since it is done on a per image basis The time it takes to 256nd rules is the lowest among all steps If the image mining program is run several times over the same image set only the times for the second and the fourth step should be considered since image features already exist and auxiliary images have already been created 5 Application Image mining could have an application with real images The current implementation could be used with a set of images having the following characteristics 017 Homogeneous The images should have the same type of image content For instance the program can give useless results if some images are landscapes other images contain only people and the remaining images have only cars 017 Simple image content If the images are complex they will produce blobs dif\256cult to match Also the association rules obtained will be harder to interpret A high number of colors blurred boundaries between objects large number of objects signi\256cant difference in object size make the image mining process more prone to errors 017 A few objects per image If the number of objects per image is greater than 10 then our current implementation would not give accurate results since Blobworld in most cases generates at most 12 blobs per image 017 New information The image itself should should give information not already known If all the information about the image is contained in associated alphanumeric data then that data could be mined directly 6 Future Work Results obtained so far look promising but we need to improve several aspects in our research effort We are currently working on the following tasks We also need to analyze images with repeated geometric shapes If we want to obtain simple association rules this can make our program more general This can be done without further modi\256cation to what is working However if we want to mine for more speci\256c rules then we would need to modify our algorithm For instance we could try to 


produce rules like the following if there are two rectangles and one square then we are likely to 256nd three triangles The issues are the combinatorial growth of all the possibilities to mine and also a more complex type of condition We will also study more deeply the problem of mining images with more complex shapes such as the irregular one similar to the letter L We need a systematic approach to determine an optimal similarity threshold or at least a close one A very high threshold means only perfect matches are accepted On the other hand a very low similarity threshold may mean any object is similar to any other object Finding the right similarity threshold for each image type l ooks like an interesting problem Right now it is provided by the user but it can be changed to be tuned by the algorithm itself Also there are many ways to tune the eleven parameters to match blobs and the optimal tuning may be speci\256c to image type There also exists the possibility of using other segmentation algorithms that could perform faster or better feature extraction It is important to note that these algorithms should give a means to compare segmented regions and provide suitable parameters to perform object matching in order to be useful for image mining From our experimental results it is clear that this step is a bottleneck for the overall performance of image mining We can change the object identi\256cation algorithms to generate overlapping object associations using more features Our algorithm currently generates partititons of objects that is if one object is considered similar To another one the latter one will not be compared again By generating overlapping associations we can 256nd even more rules For instance a red rectangular object may be considered similar to another rectangular object and at the same time be similar to another red object Mining by position is also possible for instance two objects in a certain position may imply another object to be in some other position Since the software we are using for feature extraction produces eleven parameters to describe blobs we have 2 11 possibilites to match objects 7 Conclusions We presented a new algorithm to perform data mining on images and an initial experimental and performance study The positive points about our algorithm to 256nd association rules in images and its implementation include the following It does not use domain knowledge it is reasonably fast it does not produce meaningless or false rules it is automated for the most part The negative points include some valid rules are discarded because of low s upport there are repeated rules because of different object id's unwanted matches because of blobs representing several objects slow feature extraction step a careful tuning of several parameters is needed it does not work well with complex images We studied this problem in the context of data mining for databases Our image mining algorithm has 4 major steps feature extraction object identi\256cation auxiliary image creation and identi\256ed object mining The slowest part of image mining is the feature extraction step which is really a part of the process of storing images in a CBIR system and is done only once The next slowest operation is creating the auxiliary blob images which is also done once Object identi\256cation and association rule 256nding are fairly fast and scale up well with image set size We also presented several improvements to our initial approach of image mining Our experimental results are promising and show some potential for future study Rules referring to speci\256c objects are obtained regardless of object position object orientation and even object shape when one object is partially hidden Image mining is feasible to obtain simple rules from not complex images with a few simple objects Nevertheless it requires human intervention and some domain knowledge to obtain better results Images contain a great deal of information and thus the amount of knowledge that we can extract from them is enormous This work is an attempt to combine association rules with automatically identi\256ed objects obtained from a matching process on segmented images Although our experimental results are far from perfect we show that it is better to discover some reliable knowledge automatically than not discovering any new knowledge at all Acknowledgments We thank Chad Carson from the University of California at Berkeley for helping us setup the Blobworld system We also thank Sham Navathe and Norberto Ezquerra for their comments to improve the presentation of this paper References 1 R  A g r a w a l  T  I m i e lin s k i a n d A  S w a m i  M in in g a s s o ciation rules between sets of items in large databases In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data  pages 207\261 216 Washington DC May 26-28 1993  R  A gra w a l a n d R  S ri ka nt  F a s t a l gori t h m s for m i n i n g association rules in large databases In Proceedings of the 20th International Conference on Very Large Data Bases  Santiago Chile August 29-September 1 1994  S  B e l ongi e  C Ca rs on H  G r e e n s p a n  a nd J  Ma lik Recognition of images in large databases using a learning framework Technical Report TR 97-939 U.C Berkeley CS Division 1997 


 C  C a r s on S  Be l ongi e  H  G r e e n s p a n  a nd J  Ma l i k  Region-based image querying In IEEE Workshop on Content-Based Access of Image and Video Libraries  1997 5 G  D u n n a n d B  S  E v e r itt An Introduction to Mathematical Taxonomy  Cambridge University Press New York 1982  U  F a yya d  D  H a u s s l e r  a nd P  S t orol t z  M i n i n g s c i e n ti\256c data Communications of the ACM  39\(11\51\26157 November 1996  U  F a yya d G  P i a t e t s k y-S h a p i r o a n d P  S m y t h  T he kdd process for extracting useful knowledge from volumes of data Communications of the ACM  39\(11\:27\261 34 November 1996 8 D  F o r s y t h J M a l i k  M F l e c k H G r e e n s p a n  T L e ung S Belongie C Carson and C Bregler Finding pictures of objects in large collections of images Technical report U.C Berkeley CS Division 1997  W  J  F ra wl e y  G  P i a t e t s k y S ha pi ro a nd C J  Ma t h e u s  Knowledge Discovery in Databases  chapter Knowledge Discovery in Databases An Overview pages 1 261 27 MIT Press 1991  V  G udi v a da a n d V  R a gha v a n Cont e n t ba s e d i m age retrieval systems IEEE Computer  28\(9\18\26122 September 1995 11 R  H a n s o n  J  S t u t z an d P  C h ees eman  B ay es i a n c l a s si\256cation theory Technical Report FIA-90-12-7-01 Arti\256cial Intelligence Research Branch NASA Ames Research Center Moffet Field CA 94035 1990  M H o l s he i m e r a n d A  S i e be s  D a t a m i ni ng T h e search for knowledge in databases Technical Report CS-R9406 CWI Amsterdam The Netherlands 1993  M H out s m a a nd A  S w a m i  S e t ori e nt e d m i ni ng of association rules Technical Report RJ 9567 IBM October 1993  C O r done z a nd E  O m i e c i ns ki  I m a ge m i ni ng A new approach for data mining Technical Report GITCC-98-12 Georgia Institute of Technology College of Computing 1998  J  R Q u i n l a n Induc t i o n o f d e c i s i on t r e e s  Machine Learning  1\(1\81\261106 1986  A  S a v a s e re  E  O m i e c i ns ki  a nd S  N a v a t h e  A n e f 256 cient algorithm for mining association rules In Proceedings of the VLDB Conference  pages 432 261 444 Zurich Switzerland September 1995  O  R Z a i a ne  J  H a n  Z  N  L i  J  Y  Chi a ng a n d S Chee Multimedia-miner A system prototype for multimedia data mining In Proc 1998 ACM-SIGMOD Conf on Management of Data  June 1998 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


