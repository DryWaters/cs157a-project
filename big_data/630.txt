COLLECTIVE INTELLIGENCE IN DISTRIBUTED SYSTEMS AND SEMANTIC DATA VISUALIZATION Sumalatha.M.R, Abishek Ravi, Aravind.M, Prasanna.N Department of Information Technology Madras Institute of Technology, Anna University, Chromepet, Chennai-44 E-mail rsumalatha@yahoo.com   Abstract  In the era of monumental development in all spheres of technology, the repository of huge amounts of raw data is but a necessity. Also with increasing availability of storage space, this factor is not a big problem. Hence instead of tackling issues related to reducing the amount of data to be stored, there is bigger need to implement the creation of a knowledge base and hence create semantic data visualization The work discussed presents an approach to integrate expert knowledge all along the data mining process in a coherent and uniform manner. A collective intelligence system plays a central role in this approach. The work primarily aims at converting the presence of large amounts of raw source data into a knowledge base and collective framework Heterogeneous web sources have been taken into consideration, which act as containers for the raw data source. Data visualization is the second major functionality Taking into consideration the patterns mined, the onus is on the developer to present the mined patterns in a format that is most appealing to the end-user. Creation of such data visualizations requires various dimensions of the pattern to be understood. The estimated end product creates visualization from the knowledge base created from heterogeneous web sources  KEY WORDS  Collective Intelligence, Apriori, Clusters, Semantic Visualization, Ontology  1 INTRODUCTION Collective intelligence is a form of intelligence that emerges from the collaboration and competition of many individuals  Knowledge that can be derived from a domain of various voting systems has the capacity for many individual unique perspectives to combine and converge. This convergence can be based on an assumption that the voting systems in use are uninformed. These systems are random to an extent. Knowledge can be obtained by filtering patterns from a decision based process which results in a well informed consensus. Ontology is then framed from this existing knowledge framework and is used as a structure in artificial   intelligence functions. Semantic data visualization and collective intelligence form the heart of the project A Collective Intelligence Information System consists of Instances Instances are the basic, "ground level" components of the system. Strictly speaking, the system need not include any instances, but one of the general purposes of ontology is to provide a means of classifying individuals, even if those individuals are not explicitly part of the ontology  Classes Ontology varies on whether classes can contain other classes, whether a class can belong to itself, or whether there is a universal class  Properties Objects in the ontology can be described by assigning values to their properties. Each property has at least a name and a value, and is used to store information that is specific to the object it is attached to  Relations An important use of attributes is to describe the relationships between objects in the ontology. Typically a relation is a property whose value is another object in the ontology  Events Events in a system play an important role in describing the nature, timing and effects of interaction between various objects Knowledge discovery from data is a process that aims to extract potentially useful knowledge hidden in large data sets However, it remains a crucial issue to elicit relevant models according domain expert knowledge. The KEOPS approach is based on a method which refers to expert knowledge all along a knowledge extraction process. Furthermore, KEOPS uses a part-way interestingness measure between objective and subjective measure in order to evaluate models relevance according to expert knowledge  The work has been proposed by Laurent Brisson and Martine Collar   It is more accurate to state that the approach has parameters that must  be tuned. When surveying the literature on this topic, it was noted that while there are many techniques for automatically tuning parameters, many of these techniques themselves have parameters, possibly resulting in an infinite regression. An additional problem of parameter-laden algorithms is that they make it difficult to reproduce published experimental results and to understand the contribution of a proposed algorithm These perils of parameter laden algorithms have been proposed by Eamonn Keogh  T h e s e pe r i l s  ha v e be e n de a l t  with accordingly in the proposed system of work  
First International Conference on Emerging Trends in Engineering and Technology 978-0-7695-3267-7/08 $25.00 © 2008 IEEE DOI 10.1109/ICETET.2008.183 292 


The proposed work aims at creating a knowledge base across heterogeneous web sources. The base hence created will perform the functions of a Recommendation System  Given a system of heterogeneous web sources the work first creates a knowledge base by mapping various datasets distributed geographically and reduces the sets to gain relevant patterns. Based on the association rules gathered from patterns a recommendation system is created. Finally the knowledge gathered from patterns is presented to the end user after implementation of the data visualization module. A method used to conserve bandwidth is the Compression Based Dissimilarity model. The dissimilarity model is described as follows Given two strings x and y the Compression based Dissimilarity Model is defined as follows CDM\(x, y\    C \(xy\ \(C\(x\ C\(y When two strings x and y are not related the CDM value is close to 1 and smaller than 1 by a larger value when x and y are related. But it is to be noted that CDM\(x, x\ is not zero  2. THE COLLECTIVE INTELLIGENCE SYSTEM 2.1 Implementation Issues For highly complex data with mutual relationships, the derived patterns tend to be complex. Thus implementation challenges are as follows  1. The patterns described by the existing knowledge extraction algorithms are still too abstract for general understanding. A pattern that is misinterpreted is of great danger. For example many algorithms do not distinguish between causality and cooccurrence. For an application that aims at finding the reason for a certain type of disease, there is a great difference between finding the origin of the disease and finding just an additional symptom. Developing systems which derive understandable patterns and making already derived patterns understandable is a challenge 2. As stated, current algorithms focus on a limited set of standard patterns. Deriving these patterns often does not yield a direct and complete solution to many problems Furthermore, with an increasing complexity of the analyzed data, it is likely that the derived patterns will increase in complexity as well. Thus, the challenge lies in finding richer patterns 3. The final challenge arises when working with future patterns due to the increased number of valid patterns Therefore, the number of potentially valid patterns will be too large to be handled by a human user, without a system organizing the results. Hence future systems deal with providing a platform for pattern exploration where users can browse for knowledge on a personalized basis 4 Storing Frequent Item-sets using tries is another major issue. In existing algorithms frequent item-sets that are not needed by the algorithm can be written to the disk, and memory can be freed. Frequent items of size y 001 only are needed for the candidate generation of  y\001 0011  item-sets, and later they are not used. Hence there is need for memory conservation  To conclude, systems should generate a large variety of well understandable patterns. Due to variations in the parameterizations, the number of possibly meaningful and useful patterns will dramatically increase and thus, an important aspect is managing and visualizing these patterns    Figure 2.1: Class Diagram for Implementation Issues  2.2 Overview of the proposed Collective Intelligence Architecture In the knowledge extraction and data visualization framework several sub modules like, implementation of distributed web sources, pattern mining in the compressed knowledge base, implementation of compression algorithms and creation of data visualization exist  2.2.1 Implementation of distributed web sources  Distributed web sources provide the user an advantage of having to distribute data geographically. Apart from data, even processes are distributed geographically thereby, making the execution process faster and easier from the resource requirement point of view. Hadoop distributed file system \(HDFS\ clusters are used to create heterogeneous web sources 2.2.2 Pattern mining in the compressed knowledge base  Form the compressed knowledge base that contains various related item sets from the given dataset, various patterns have to be extracted and irrelevant patterns have to be pruned. Various queries with and without parameters have be executed to implement this module. Parameter-light algorithms tend to produce results of higher accuracy 2.2.3 Implementation of compression algorithms  In order to conserve the bandwidth and relax certain other constraints the compression is implemented using runlength encoding. From here the dissimilarity and similarity measures are calculated 
293 


2.2.4 Implementation of trie-based apriori approach Once the trie data structure is constructed as per requirements the dataset and knowledge base are processed. Frequent items sets are scanned to generate various patterns. A recommendation system is created based on the Pearson and Euclidean mathematical models 2.2.5 Creation of data visualization The result set is parsed and an intermediate representation in the form of graphs and charts is created. The intermediate result is translated to a visualization structure. The visualization structure is classified based on recommendations created  3 System Architecture HDFS has master/slave architecture. An HDFS cluster consists of a single Namenode a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of Datanodes  usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files Internally, a file is split into one or more blocks and these blocks are stored in a set of Datanodes. The Namenode executes file system namespace operations like opening closing, and renaming files and directories. It also determines the mapping of blocks to Datanodes. The Datanodes are responsible for serving read and write requests from the file systems clients. The Datanodes also perform block creation deletion, and replication upon instruction from the Namenode  Hence is a brief description of system architecture. The same architecture will be deployed later with respect to an application which will bring out practical and commercial purposes involved in implementing the work proposed. The architecture helps in distributing processes across a group of systems NAME NODE METADATA CLIENT REPLICATION BLOCK DATA NODES BLACK OPERATIONS READ METADATA OPERATIONS WRITE RACK 1 RACK 2 CLIENT NAME NODE METADATA CLIENT REPLICATION BLOCK DATA NODES BLACK OPERATIONS READ METADATA OPERATIONS WRITE RACK 1 RACK 2 CLIENT  Figure 3.1:  System architecture 4 Real Time Analysis and Application e-Business \(electronic business\, is the conduct of business on the Internet, not only buying and selling but also servicing customers and collaborating with business partners Today, major corporations are rethinking their businesses in terms of the Internet and its new culture and capabilities Companies are using the Web to buy parts and supplies from other companies, to collaborate on sales promotions, and to do joint research. Exploiting the convenience, availability, and world-wide reach of the Internet, is the trend of recent times The following are the challenges or limitations in the eBusiness domain presently Complex parameters During various analysis operations like business process intelligence or customer relationship management queries include with them complex parameters that are hard to parse and many times they tend to produce malicious results because of their false and complex nature Integration into complete solution Using various techniques like knowledge extraction and analytical processing of multidimensional data, intermediate results should be deciphered into complete solutions in order to complete the analytical process The above challenges can be overcome by using the following solutions Text categorization The aim of this stage is to extract content and using data build topic hierarchy. Once, a hierarchy is built it becomes easier to track customer operations and hence analyze the customer behavior. Analysis operations performed are Extraction of key words and phrases, Transformation of documents and query log records into vectors, Creation of clusters hierarchically, labeling each cluster with significant words, phrases, normalization Catalog creation and service discovery This stage aims to achieve  Attribute extraction and Data formulation. Once a document structure is recognized attribute extraction and data formulation become easier Business Process Intelligence The goal of this stage is to improve the quality of business services and processes. The recommendation system by providing apt and complete recommendations improves the quality of the system  4.1 Application Oriented Architecture  Architecture for the recommendation system based on the collective intelligence system is described as follows The recommendation system is created to test the real-time application and feasibility of collective intelligence system 4.1.1 Proposed Methodology The proposed methodology operates on a distributed file system environment based on the SOAP \(Service oriented architecture protocol\ ontology. The triangular ontology deals with service registry, service provision and remote method invocation. Hadoop clusters are used for this purpose 
294 


4.1.2 User Layer Customer/Buyer Domain: The customer/buyer domain consists of various customers and buyers who represent the clients of the architecture. A customer or buyer is any remote device seeking to analyze various data sets based on certain queries. The customer seeks access to a remote service on the service domain that analyses datasets. These remote services have direct access to a remote database Hence the customers represented in this domain have indirect access to a remote database with service domain activity as an intermediary stage  4.1.3 Collective Intelligence Layer Application Server Proxy The application server proxy performs the function of a routing table. The customer in the user layer searches for various services that are available on the distributed file system. While doing so, the first component a customers service query will approach is the application server proxy. The application server proxy contains a table or list of various services available on the distributed file system. Hence all the analysis services have to first register themselves with the application proxy server in order for the customer to access these services. When a service wanted by the customer is available on the service domain, the proxy deploys a remote server where this service is available to the customer  Service Domain The service domain mainly consists of one primary service, namely the recommendation system The recommendation system used here deals with recommending items for users given a rated and uninformed data set. That is, the data set that serves as the base or input will contain a list of item sets that are ranked by users using the item set. The final output will be an intelligent recommendation of unused item sets for a particular user. The item sets can be any product like books, movies, gadgets services, techniques etc. based on the real time dataset provided as input Semantic Data Visualizer The output of the analysis service machines needs to be presented in a graphical understandable and user-friendly manner to the end user who is the customer or buyer. The semantic data visualizer involves a semantic index that is used in tandem with the analytical output. The semantic parameters involved vary with each analysis. Graphs and charts are created as intermediate results \(IR\. The IR is then used to map the output onto predetermined visualization structures 4.1.4 Data Layer The data layer is the core and heart of the entire architecture. The layer has one main component: the remote data set. The remote data set is a set of item sets that are linked to each other by way of ratings. The item sets consist of customer identities and various other e-Business customer fields like products purchased, time logs, costs incurred billing information and other logs. The analysis service provider servers have independent direct access to this remote data set  5 Algorithms  The collective intelligence system hence proposed uses a platter of assorted algorithms and following are the algorithms that are being put into use in the collective intelligence system  5.1 Compression based dissimilarity measure     5.2 Frequent Pattern mining  Function dist = cdm\(a,b Save a.txtaascii    Zip\(a.zip, a.txt    A_file = dir\(a.zip    Save b.txt b ascii    Zip\(b.zip, b.txt    B_file = dir\(b.zip    A_n_b = [a       Save a_n_b.txt a_n_b ascii   Zip\(a_n_b.zip, a_n_b.txt   A_n_b_file = dir \(a_n_b.zip   Dist = a_n_b_file.bytes / \(a_file.bytes + b_file.bytes Return Dist End 1. Select random sample, S, from transaction data set D 2. Split the random sample \(S\ into subset \(or partitions\, so each subset size is in accordance with computer memory 3. Read one subset 4. Store all the candidate 1-itemsets in the subset 5. Count the number of instances of each 1-itemsets to compose the candidate set called C1 6. Delete least frequent candidate 1-items to generate a large item set list called L1 7. Join L1 with itself to create candidate 2-itemsets C2. C2 consists of all combinations of 2-itemsets and check for the property that states all nonempty subsets of frequent itemsets must also be frequent 
295 


   Figure 4.1: e-Business Architecture                        5.3 Mapping & reducing algorithm                   APPLICATION SERVER PROXY   Service ServiceID Remote Address    SEMANTIC DATA VISUALIZER    Analyzer Charts Graphs Visualization Structures   The Recommendation System Service Domain    I N T E R N E T User Layer Collective Intelligence Layer Data Layer 8. Recursively perform steps 4 to 6 with the addition of an item-set every pass, until some L k becomes empty which means that the join operation \(step 6\ is not possible 9. Combine all frequent L n item-sets into one list called local frequent item-sets \(LL p\ for the given partition where n=1, 2  K 10. Recursively perform steps 2 to 8 for each partition 11. Combine all LL p to generate global candidate itemsets \(LG 12. Scan the whole transactional database to count the number of occurrences of each candidate K-item-set in LG 13. Delete least frequent candidate K-items to generate global frequent item-set in the data set \(all subsets  Function map \(string key, string value key: document name  value: document contents  For each word w in value  Emit-intermediate \(w, 1  Function reduce \(string key, iterator values  key: a word  values: a list of counts result=0  For each v in values  result+= parseint \(v  emit \(string \(result 
296 


 6 Results and graphs  The following are the two sections in which a few results have been inferred from the graphs plotted 6.1Construction and destruction time for stored filtered transactions Primarily no difference is observed when data sets are too small, as seen in Fig 6.1. However, the sorted lists tend to slow down as data set grows. RB-trees are always faster than tries, but the difference is not significant. In support count, each filtered transaction has to be visited to determine the contained candidates. If transactions are stored in a sorted list, then going through the elements is a very fast operation In the case of RB-trees and trie, this operation is slower, since a tree has to be traversed. Hence a sorted list is used despite the slowness during large data sets, due to easy traversal options  Fig 6.1: Time Vs Threshold  6.2 Writing frequent item-sets to disk The fastest results are obtained if frequent item-sets are not stored in memory but written to disk in the candidate generation process, as seen in Fig 6.2. In cases of determining small and medium sized candidates most edges lead to leaves hence removing other edges does not accelerate the algorithm too much. Hence, the memory needs can be significantly reduced if frequent item-sets are not stored in memory Consequently if frequent item-sets are needed to determine valid association rules and memory consumption is an important factor, then it is useful to write frequent item-sets to disk and read them back when the association rule discovery phase starts   7 Conclusion and future implementation The proposed collective intelligence system is hence implemented and its applications are tested using the real-time recommendation system in an e-business background. The    Fig 6.2: Time Vs Threshold recommendations hence created based on collective intelligence are accurate to a greater extent. The collective intelligence system can be extended in lines of semantic web searching to provide personalized results and optimized link retrievals. Furthermore, a mechanism or structure can be created to improve on the performance of sorted lists in case of large data sets to decrease the time complexity of creation of the filtered transactions from the data set  REFERENCES 1 Ea m o n n K e o g h  S t ef a n o Lo n a r d i Li Wei  Compression Based Data Mining of Sequential Data Springer, March 2007  e dr o D o m i n g o s   Towards Knowledge rich data mining  Springer, April 2007  r e g o r y P i a t e t s k i S h a p i r o   Data Mining and knowledge discovery from 1996-2005, Springer, January 2007 4 La u r en t B r i s s o n  an d M a r t in e C o l l ar d   An Ontology Driven data mining process, The KEOPS approach November 2006  a ur e n t B r i s s o n  Knowledge extraction using a conceptual information system ODBIS Workshop in VLDB Conference, Seoul, 2006 6 A b r a h a m B e r n s t e i n  F o s t e r P r o v o s t, a n d S h aw n d r a H ill   Toward intelligent assistance for a data mining process: An ontology-based approach for cost-sensitive classification  IEEE Transactions on Knowledge and Data Engineering 2005 7 B a r o n c h e ll i A  C a g l io ti E  L o r e to V   Artificial sequences and complexity measures J. Stat. Mech: Theory and Exp 2005 8  J e f f e r y D e an  S a n j ay G h e m aw at  MapReduce :Simplified  Data Processing on Large Clusters OSDI'04, San Francisco CA, December, 2004 
297 


 7 Product Units BI hours AF failures Other Failures Total hours T A C V CCA V V CCI V AX1000 129 1000 129000 125 1.8 3.6 AX1000 129 1000 129000 55 1.8 3.6 AX1000 77 1000 77000 125 1.6 3.6 AX1000 77 1000 77000 55 1.6 3.6 AX2000 38 1000 38000 125 1.6 3.6 AX2000 38 1000 38000 55 1.6 3.6 AX2000 22 1000 22000 125 1.65 3.6 AX2000 96 168 16128 110 1.8 3.6 AX2000 96 168 16128 110 1.9 3.6 AX2000 96 168 16128 110 1.9 3.6 AX2000 95 24 2280 110 2 3.6 Total 893 560664 AX Table 1 \226 AX Qualification Lot Summary     Table 2 \226 Aerospace Corporation Life Test \226 AX2000   


 8 Table 3 226 RTAXS Life Test Results  RTAXS Product Units Prog Yield BI hours AF failure s Other Failure s Total hours TA C Comments          RTAX2000 S 87  1000 0 2 85000 125 ESD RTAX1000 S 98  1000 0 4 94000 125 ESD RTAX1000 S 78  1000 0 1 77000 55 ESD RTAX1000 S 150  1000 0 6 144000 125 Cont failure due to BIB RTAX1000 S 28  1000 0 1 27000 125 improperly etched contact RTAX1000 S 120  6000 0 0 720000 125  RTAX2000 S 6 100% 1000 0 0 6000 125  RTAX2000 S 6 100% 1000 0 0 6000 125  RTAX1000 S 144  250 0 0 36000 55  RTAX1000 S 150  250 0 2 37000 55 Cont failure due to BIB RTAX2000 S 14 95 168 0 0 2352 125  RTAX2000 S 14 100 168 0 0 2352 125  RTAX2000 S 14 54 168 0 0 2352 125  RTAX1000 S 24  168 0 0 4032 125  RTAX1000 S 37 100% 1000 0 0 37000 125  RTAX1000 S 8 100% 1000 0 0 8000 125  RTAX2000 S 14 88 168 0 0 2352 125  RTAX2000 S 14 78 168 0 0 2352 125  RTAX2000 S 14  168 0 0 2352 125  RTAX2000 S 14 93 168 0 0 2352 125  RTAX250S 100 95 168 0 0 16800 125  RTAX1000 S 150  1000 0 2 148000 125 damage in metal layers only-EOS RTAX1000 S 148 250 0 0 37000 55  RTAX1000 S 150 250 0 0 37500 55  


 9 RTAX1000 S 150   1000 0 0 150000 125  RTAX2000 S 14 100% 168 0 0 2352 125  RTAX250S 6 100% 1000 0 0 6000 125  RTAX2000 S 58  1000 0 0 58000 125  RTAX2000 S 20  1000 0 0 20000 125  RTAX2000 S 58   168 0 0 9744 55  RTAX2000 S 20   168 0 0 3360 55  RTAX1000 S 24 100% 168 0 0 4032 125  RTAX2000 S 14 48% 168 0 0 2352 125  RTAX2000 S 6 86% 1000 0 0 6000 125  RTAX2000 6 100% 2000 0 0 12000 125  RTAX250S 100 93 168 0 0 16800 125  RTAX2000 S 79 100% 1000 0 0 79000 125  RTAX2000 S 14 88% 168 0 0 2352 125  RTAX2000 S 24 100% 1000 0 0 24000 125  RTAX2000 S 8 73% 2000 0 0 16000 125  RTAX2000 S 6 100% 2000 0 0 12000 125  RTAX2000 S 2 100 1000 0 1 1000 125 Gate oxide failure in buffer RTAX2000 S 8 100% 2000 0 0 16000 125  RTAX2000 S 14 93% 168 0 0 2352 125  RTAX2000 S 14 100% 168 0 0 2352 125  RTAX2000 S 11 92% 2000 0 0 22000 125  RTAX2000 S 11 92% 2000 0 0 22000 125  RTAX2000 S 8 93% 1000 0 0 8000 125  RTAX4000 S 75 72.5 5000 0 0 375000 125  RTAX4000 S 4 72.5 3000 0 0 12000 125  RTAX4000 S 24 80% 1000 0 0 24000 55  RTAX4000 S 24 80 1000 0 0 24000 125  RTAX2000 S 14 100% 168 0 0 2352 125  


 10 RTAX2000 S 14 88% 168 0 0 2352 125  RTAX4000 S 15 100% 1000 0 0 15000 125  RTAX2000 S 8 100% 2000 0 0 16000 125  RTAX1000 S 8 89% 1000 0 0 8000 125  RTAX1000 S 11 92% 2000 0 0 22000 125  RTAX2000 S 24 92% 1000 0 0 24000 125  RTAX2000 S 14 100% 168 0 0 2352 125  RTAX2000 S 14 88% 168 0 0 2352 125  RTAX4000 S 8 100% 168 0 0 1344 125  RTAX2000 S 82 95% 1500 0 0 123000 125  RTAX2000 S 82 95% 1500 0 0 123000 55  RTAX250S 82 98% 1500 0 0 123000 125  RTAX250S 82 98% 1500 0 0 123000 55           TOTAL 2842  TOTAL 3,057,244  LTOL 936  LTOL 507,604  HTOL 1906  HTOL 2,549,640    


 11 R EFERENCES     Reddy, M.K. and S.M. Reddy 223Detecting FET Stuck-Open Faults in CMOS Latc hes and Flip-Flops,\224 IEEE Design and Test of Computers Vol. 3 , No. 5 , pp. 17-26, October 1986   2 R Mad g e , M. Vilg is, a nd V. Bhide, "Achieving Ultra High Quality and Reliability in Deep Sub-Micron Technologies using Metal Layer Configurable Platform ASICs", MAPLD 2005    Kewal Sal u ja, \223Di g i t a l Sy st em Fundam e nt al s, Lect ure 11\224, Department of Electrical Engineering, University of Wisconsin Madison    Yu W e i  Papos ng  M oo Ki t Lee Peng W e ng Ng C h i n  Hu Ong, \223IDDQ Test Challenges in Nanotechnologies: A Manufacturing Test Strategy\224, Asian Test Symposium 2007. ATS apos;07. 16 th Volume , Issue , 8-11 Oct. 2007 Page\(s\211 \226 211  NASA GSFC Advi sory NA-GSFC 2004-06   Dan El ft m a nn, Sol o m on W o l d ay and M i nal  Sawant  New Burn In \(BI\ethodology for Testing of Blank and Programmed Actel 0.15 \265m RTAX-S FPGAs MAPLD 2005   M i nal Sawant Dan El ft m a nn,  W e rner van den Abeel an John McCollum, Solomon Wolday and Jonathan Alexander 223Post Programming Burn-in of Actel O.25um FPGA\222s\224 MAPLD 2002  B IOGRAPHY   John worked 2 years at Faichild R&D on bipolar switching performance specifically platinum dopedlife time control and the development of Ion Implantation.  He worked 15 years at Intel developing Intel's first bipolar PROM, Ion Implantation, the world's first 16K DRAM, as well as 64K and 256K DRAMs.  Mr. McCollum developed Intel's first dual layer metal CMOS technology for the 386 microprocessor.  He co-founded Actel and worked the last 20 years on process, antifuse and flash cell development and FPGA Architecture at Actel.  He holds over 50 patents   covering Process Technology, Antifuse and NVM technology, FPGA Architecture, Analog Processing and Radiation Hardening.  He has presented numerous papers at IEDM, MAPLD, CSME, SPWG, and the FPGA Symposium. He is currently a Fellow in the Technology Development Department 


Time Time 50 350   10 0                   10 1                   10 2 12.5 50 350   10 0                   10 1  12 expected from Figure 7, the width of the uncertainty region is compressed by the curvature of the monopulse response resulting in a detection-primitive with greater uncertainty than the variance admits.  A filter lag or so-called cluster tracking can easily result in a 5% or greater offset and degraded consistency.  After 300 seconds the curves peak up because the target is appr oaching a low-elevation beampoint limit.  This occurs anytime a target is tracked into the edge of the radar\222s field of re gard and can lead to radar-toradar handover difficulty         300 300 80  s D 2 k,1 k y    s D 2 k,1 k y   100 150 200 250 10 1                    10 5 1 0  Figure 8 - Consistency versus distance from beam center Monopulse Mismatch The next set of curves plotted in Figure 9 show the sensitivity of detection-primitive consistency to a mismatch in the monopulse slope.  All of these curves were generated using a linear monopulse response derived from the slope of the true monopulse response at beam center.  The slope of the 80% curve is 0.8 times th e beam-center slope; the 90 curve is 0.9 times the beam-center slope; and so on for 100%, 110% and 120%.  Again, the order of curves in the graph is the same as the legend order A steep slope tends to expand y I 222s uncertainty while a gentle slope tends to compress it.  An expanded uncertainty leads to a smaller consistency while a compressed uncertainty leads to a larger consistency.  This behavior can be observed in the family of curves in Figure 9.  Curves for the steeper slopes are on the botto m while curves for more gentle slopes are on top.  The notable feature of this set of curves is that the sensitivity to a mismatch in the monopulse slope is not very significant       100 150 200 250 10 1                    90 100 110 120  Figure 9 - Consistency versus monopulse mismatch Range-Bias Error The complex nature of the monopulse radar models presents ample opportunity to introduce errors in the software implementation.  One such e rror introduced in a \275 rangecell-width bias in the detection-primitive range which in turn resulted in a significant degradation to 2 9 k D The fact that 2 9 k D is measured in different coordinates compared to the bias made it difficult to determine which value or algorithm was to blame.  Examining the intermediate consistency values led directly to the error source A comparison between biased 2 1 k D  2 2 k D and 2 3 k D values and unbiased 2 2 k D values is shown in Figure 10.  The unbiased 2 2 k D is the bottom-most curve and the biased 2 3 k D is the top-most curve with a value around 80.  This large value for 2 3 k D indicates that there is a lot more uncertainty in the range measur ements compared to what is predicted by the range varian ce.  Since the range-variance calculation is easy to confirm, the problem must be in the algorithms that model or manipulate range A notable feature of Figure 10 is the sensitivity of the centroiding algorithm to range bias in the detection primitives.  The range bias is ba rely noticeable in the biased 2 1 k D and 2 2 k D curves.  Of course, if the unbiased 2 2 k D  curve existed as a baseline it would be relatively easy to spot the error 


Time Time Time 50 350   10 0                   10 1                   10 2 50 350   10 1                   10 0                   10 1 50 350   10 0                   10 1  13         Isolated No SNR Adjust  Figure 11 - Centroiding for isolated range cells Filter Tuning Now that the centroided m easurements are reasonably consistent, the parameters that govern track filtering can be examined.  As previously promised, the effects and corrections for atmospheric refr action and sensor bias have been disabled so that 2 8 k D can be analyzed using a sliding window.  Of course the full analysis would include these effects and 2 8 k D at each time step would be collected and averaged over many trials Plots of the effect of changing process noise in a nearlyconstant-velocity filter are shown in Figure 12 and Figure 13 for Cartesian position and velocity respectively.  In both figures, the plotted values have been divided by 3 so that the desired value is always 1.  Increasing the process noise up to a point should increase the updated uncertainty and reduce 2 8 k D values.  Except near th e end of the trajectory when the measurements are off of beam center, the curves in Figure 12 and Figure 13 appear inconclusive for this expected trend If 2 8 k D values are way out of range there are additional intermediate filter values that can be examined.  For example, the state extrapolati on algorithms can be examined by comparing the consistency of 1 210 Isolated With SNR Adjust 300 300 300 0.005 212 212 212 212 212 kkkk T kkkk D xhzSxhz 35        s D 2 k Range   D 2 k,2 biased D 2 k,1 biased D 2 k,2  Figure 10 - Range bias error in detection primitive Centroiding Algorithm From Section 3, assuming that the centroided-range uncertainty for an isolated range cell is the same as its detection-primitive uncertainty may be incorrect Collecting and plotting 2 3 k D values only from isolated range-cell measurements can be used to analyze such assumptions.  The plots in Figure 11 compare differences between the isolated-cell algorithm defined in Section 3, an algorithm that modifies the uncertainty based on the SNR in the isolated cell, and the 2 3 k D values from all measurements 34\was used to modify the range uncertainty for the upper line labeled Isolated with SNR Adjust    4 22  2 2  resRi o R R Rn bdp bm  s D 2 k,3 Range    s D 2 k,8 Position     212 1 can also be examined using \(35 The residual is also commonly used to determine the assignment cost  212 kk z  P  k  k1 with z k The consistency of the innovation covariance k T kkkkk RHPHS 100 150 200 250 10 1                    D 2 k,3 biased 100 150 200 250 10 2                    100 150 200 250 10 1                    0.5 50  Figure 12 \226 Position consistency, filter tuning example  r  t t 34 If the All Centroided curve \(middle\as the baseline doing nothing \(lower\imates the uncertainty and 33\imates the uncertainty.  Dividing by the square root of the observed SNR leads to a more consistent covariance; however, there is currently no statistical evaluation to justify it             210 210 1 1 1 2 All Centroided 


Time 50 350   10 1                   10 0                   10 1  14         300 0.005  s D 2 k,8 Velocity   100 150 200 250 10 2                    0.5 50  Figure 13 \226 Position consistency, filter tuning example 5  C ONCLUSION  Calculating and observing the behavior of covariance consistency at different levels  in the radar signal processing chain represents a very powerfu l tool that can be used to assess the accuracy and softwa re implementation of radar signal-processing algorithms.  Analyzing covariance consistency is applicable to radar systems both in the field and in simulations.  The primary challenge in both arenas comes down to properly accounting for the true target states that contribute to detections, detection primitives measurements, and state estimates For a fielded radar syst em, achieving covariance consistency is usually a s econdary consideration behind achieving and maintaining track s.  Indeed, until recently radar specifications did not even include requirements for covariance consistency.  Recent covariance consistency requirements stem from the fact that the use of radar systems in sensor netting applications is on the rise Currently the combined e ffects of off-beam-center measurements, atmospheric correction, bias correction clustering and centrioding, data association, and filtering on state covariance consistency throughout a target\222s trajectory are not well known.  This is particularly true for radars using wideband waveforms and multiple hypotheses or multiple frame trackers.  Numerical results presented here indicate that algorithms early in the radar signal processing chain can significantly degrad e covariance consistency and that some errors are better tolerated than others For a simulated target in a modeled system, truth relative to some global reference is known. However, transforming truth through different refere nce frames and accounting for changes that occur during various radar processing algorithms is not as simple as it appears.  The techniques in this paper help expose this hidden complexity and provide a framework for discussing and expanding the future development of covariance consistency techniques.  Such future developments include issues related to mapping truth through the convolution operation typically used to simulate wideband signal processing and the fast Fourier transforms typically used in pulse-Doppler processing.  Sophisticated tracking algorithms that carry multiple hypotheses, associate across multiple frames, or weight the association of multiple targets within a single frame pose significant challenges in properly associating truth with state estimates.  Additional work, including an investigation of track-to-truth assignment, is needed before covariance consistency techniques can be applied to these algorithms Another area that needs furthe r analysis is the use of a sliding window to approximate the covariance behavior expected during a set of Monte-Carlo trials.  Various timedependent variables such as the target\222s range and orientation, the transmit waveform, the radar\222s antenna patterns toward the target, missed detections, and false alarms could easily viol ate the assumption that measurement conditions are nearly stationary over the time of the window.  It is importa nt to understand the conditions when this assumption is violated Finally, the examples presented here included a relatively benign arrangement of targets.  Further analysis in dense target environments with the related increase in merged detections, merged measurements, and impure tracks is needed.  Further analysis for targets traveling over different trajectories is also needed Even so, the techniques presented here can be extended to many of these analyses R EFERENCES  1  S. Blackman and R. Popoli Design and Analysis of Modern Tracking Systems Artech House, 1999 2  Y. Bar-Shalom and X. R. Li Multitarget-Multisensor Tracking: Principles and  Techniques YBS Publishing, Storrs, CT, 1995 3  Y. Bar-Shalom, Editor Multi-target-Multi-sensor Tracking: Advanced Applications and  Vol. I Artech House, Norwood, MA, 1990 4  D. B. Reid, \223An Algorithm for Tracking Multiple Targets,\224 IEEE Trans. on Automatic Control Vol. 24 pp. 843-854, December 1979 5  T. Kurien, \223Issues in the Design of Practical Multitarget Tracking Algorithms,\224 in Multitarget-Multisensor Tracking Y. Bar-Shalom \(ed.\43-83, Artech House, 1990 6  R.P.S. Mahler, Statistical Multisource-Multitarget Information Fusion, Artech House, 2007 


 15 7  B.-N. Vo and W.-K. Ma, \223The Gaussian Mixture Probability Hypothesis Density Filter,\224 IEEE Trans Signal Processing Vol. 54, pp. 4091-4104, November 2006 8  B. Ristic, S. Arulampalam, and N. Gordon Beyond the Kalman Filter Artech House, 2004 9  Y. Bar-Shalom, X. Rong Li, and T. Kirubarajan Estimation with Applications to Tracking and Navigation, New York: John Wiley & Sons, pg. 166 2001 10  X. R. Li, Z. Zhao, and V. P. Jilkov, \223Estimator\222s Credibility and Its Measures,\224 Proc. IFAC 15th World Congress Barcelona, Spain, July 2002 11  M. Mallick and S. Arulampalam, \223Comparison of Nonlinear Filtering Algorithms in Ground Moving Target Indicator \(GMTI Proc Signal and Data Processing of Small Targets San Diego, CA, August 4-7, 2003 12  M. Skolnik, Radar Handbook, New York: McGrawHill, 1990 13  A. Gelb, Editor Applied Optimal Estimation The MIT Press, 1974 14  B. D. O. Anderson and J. B. Moore Optimal Filtering  Prentice Hall, 1979 15  A. B. Poore, \223Multidimensional assignment formulation of data ass ociation problems arising from multitarget and multisensor tracking,\224 Computational Optimization and Applications Vol. 3, pp. 27\22657 1994 16  A. B. Poore and R. Robertson, \223A New multidimensional data association algorithm for multisensor-multitarget tracking,\224 Proc. SPIE, Signal and Data Processing of Small Targets Vol. 2561,  p 448-459, Oliver E. Drummond; Ed., Sep. 1995 17  K. R. Pattipati, T. Kirubarajan, and R. L. Popp, \223Survey of assignment techniques for multitarget tracking,\224 Proc  on Workshop on Estimation  Tracking, and Fusion: A Tribute to Yaakov Bar-Shalom Monterey CA, May 17, 2001 18  P. Burns, W.D. Blair, \223Multiple Hypothesis Tracker in the BMD Benchmark Simulation,\224 Proceedings of the 2004 Multitarget Tracking ONR Workshop, June 2004 19  H. Hotelling, \223The generalization of Student's ratio,\224 Ann. Math. Statist., Vol. 2, pp 360\226378, 1931 20  Blair, W. D., and Brandt-Pearce, M., \223Monopulse DOA Estimation for Two Unresolved Rayleigh Targets,\224 IEEE Transactions Aerospace Electronic Systems  Vol. AES-37, No. 2, April 2001, pp. 452-469 21  H. A. P.  Blom, and Y. Bar-Shalom, The Interacting Multiple Model algorithm for systems with Markovian switching coefficients IEEE Transactions on Au tomatic Control 33\(8  780-783, August, 1988 22  M. Kendall, A. Stuart, and J. K. Ord, The Advanced Theory of Statistics, Vol. 3, 4th Edition, New York Macmillan Publishing, pg. 290, 1983 23  T.M. Cover and P.E. Hart, Nearest Neighbor Pattern Classification, IEEE Trans. on Inf. Theory, Volume IT-13\(1 24  C.D. Papanicolopoulos, W.D. Blair, D.L. Sherman, M Brandt-Pearce, Use of a Rician Distribution for Modeling Aspect-Dependent RCS Amplitude and Scintillation Proc. IEEE Radar Conf 2007 25  W.D. Blair and M. Brandt-Pearce, Detection of multiple unresolved Rayleigh targets using quadrature monopulse measurements, Proc. 28th IEEE SSST March 1996, pp. 285-289 26  W.D. Blair and M. Brandt-Pearce, Monopulse Processing For Tracking Unresolved Targets NSWCDD/TR-97/167, Sept., 1997 27  W.D. Blair and M. Brandt-Pearce, Statistical Description of Monopulse Parameters for Tracking Rayleigh Targets  IEEE AES Transactions, Vol. 34 Issue 2,  April 1998, pp. 597-611 28  Jonker and Volgenant, A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems, Computing, Vol. 38, 1987, pp. 325-340 29  V. Jain, L.M. Ehrman, and W.D. Blair, Estimating the DOA mean and variance of o ff-boresight targets using monopulse radar, IEEE Thirty-Eighth SSST Proceedings, 5-7 March 2006, pp. 85-88 30  Y. Bar-Shalom, T. Kirubarajan, and C. Gokberk 223Tracking with Classification-Aided Multiframe Data Association,\224 IEEE Trans. on Aerospace and Electronics Systems Vol. 41, pp. 868-878, July, 2005   


 16 B IOGRAPHY  Andy Register earned BS, MS, and Ph  D. degrees in Electrical Engineering from the Georgia Institute of Technology.  His doctoral research emphasized the simulation and realtime control of nonminimum phase mechanical systems.  Dr. Register has approximately 20 years of experience in R&D with his current employer, Georgia Tech, and product development at two early-phase startups. Dr. Register\222s work has been published in journals and conf erence proceedings relative to mechanical vibration, robotics, computer architecture programming techniques, and radar tracking.  More recently Dr. Register has b een developing advanced radar tracking algorithms and a software architecture for the MATLAB target-tracking benchmark.  This work led to the 2007 publication of his first book, \223A Guide to MATLAB Object Oriented Programming.\224  Mahendra Mallick is a Principal Research Scientist at the Georgia Tech Research Institute \(GTRI\. He has over 27 years of professional experience with employments at GTRI \(2008present\, Science Applications International Corporation \(SAIC Chief Scientist \(2007-2008\, Toyon Research Corporation, Chief Scientist 2005-2007\, Lockheed Martin ORINCON, Chief Scientist 2003-2005\, ALPHATECH Inc., Senior Research Scientist 1996-2002\, TASC, Principal MTS \(1985-96\, and Computer Sciences Corporation, MTS \(1981-85 Currently, he is working on multi-sensor and multi-target tracking and classification bas ed on multiple-hypothesis tracking, track-to-track association and fusion, distributed filtering and tracking, advanced nonlinear filtering algorithms, and track-before-detect \(TBD\ algorithms He received a Ph.D. degree in  Quantum Solid State Theory from the State University of New York at Albany in 1981 His graduate research was also based on Quantum Chemistry and Quantum Biophysics of large biological molecules. In 1987, he received an MS degree in Computer Science from the John Hopkins University He is a senior member of the IEEE and Associate Editor-inchief  of the Journal of Advances in Information Fusion of the International Society of Information Fusion \(ISIF\. He has organized and chaired special and regular sessions on target tracking and classific ation at the 2002, 2003, 2004 2006, 2007, and 2008 ISIF conferences. He was the chair of the International Program Committee and an invited speaker at the International Colloquium on Information Fusion \(ICIF '2007\, Xi\222an, China. He is a reviewer for the IEEE Transactions on Aerospa ce and Electronics Systems IEEE Transactions on Signal Pr ocessing, International Society of Information Fusion, IEEE Conference on Decision and Control, IEEE Radar Conference, IEEE Transactions on Systems, Man and Cybernetics, American Control Conference, European Signal Processing Journal and International Colloquium on Information Fusion ICIF '2007   William Dale Blair is a Principal Research Engineer at the Georgia Tech Research Institute in Atlanta, GA. He received the BS and MS degrees in electrical engineering from Tennessee Technological University in 1985 and 1987, and the Ph.D. degree in electrical engineering from the University of Virginia in 1998. From 1987 to 1990, he was with the Naval System Division of FMC Corporation in Dahlgren, Virginia. From 1990 to 1997, Dr Blair was with the Naval Surface Warfare Center, Dahlgren Division NSWCDD\ in Dahlgren, Virg inia. At NSWCDD, Dr Blair directed a real-time experiment that demonstrated that modern tracking algorithms can be used to improve the efficiency of phased array radars. Dr Blair is internationally recognized for conceptualizing and developing benchmarks for co mparison and evaluation of target tracking algorithms Dr Blair developed NSWC Tracking Benchmarks I and II and originated ONR/NSWC Tracking Benchmarks III and IV NSWC Tracking Benchmark II has been used in the United Kingdom France, Italy, and throughout the United States, and the results of the benchmark have been presented in numerous conference and journal articles. He joined the Georgia Institute of Technology as a Se nior Research Engineer in 1997 and was promoted to Principal Research Engineer in 2000. Dr Blair is co-editor of the Multitarg et-Multisensor Tracking: Applications and Advances III. He has coauthored 22 refereed journal articles, 16 refereed conference papers, 67 papers and reports, and two book chapters. Dr Blair's research interest include radar signal processing and control, resource allocation for multifunction radars, multisen sor resource allocation tracking maneuvering targets and multisensor integration and data fusion. His research at the University of Virginia involved monopulse tracking of unresolved targets. Dr Blair is the developer and coordinator of the short course Target Tracking in Sensor Systems for the Distance Learning and Professional Education Departmen t at the Georgia Institute of Technology. Recognition of Dr Blair as a technical expert has lead to his election to Fellow of the IEEE, his selection as the 2001 IEEE Y oung Radar Engineer of the Year, appointments of Editor for Radar Systems, Editor-InChief of the IEEE Transactions on Aerospace and Electronic Systems \(AES\, and Editor-in- Chief of the Journal for Advances in Information Fusion, and election to the Board of Governors of the IEEE AES Society,19982003, 2005-2007, and Board of Directors of the International Society of Information Fusion   


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


