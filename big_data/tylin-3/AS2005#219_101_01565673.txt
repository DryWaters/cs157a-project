Bifold Constraint-Based Mining by Simultaneous Monotone and Anti-Monotone Checking Mohammad El-Hajj Osmar R Za ane Paul Nalos Department of Computing Science University of Alberta Edmonton AB Canada mohammad zaiane nalos cs.ualberta.ca Abstract Mining for frequent itemsets can generate an overwhelming number of patterns often exceeding the size of the original transactional database One way to deal with this issue is to set lters and interestingness measures Others advocate the use of constraints to apply to the patterns either on the form of the patterns or on descriptors of the items in the patterns However typically the ltering of patterns based on these constraints is 
done as a post-processing phase Filtering the patterns postmining adds a signiìcant overhead still suffers from the sheer size of the pattern set and loses the opportunity to exploit those constraints In this paper we propose an approach that allows the efìcient mining of frequent itemsets patterns while pushing simultaneously both monotone and anti-monotone constraints during and at different strategic stages of the mining process Our implementation shows a signiìcant improvement when considering the constraints early and a better performance over Dualminer which also considers both types of constraints 1 Introduction Frequent Itemset Mining FIM is a key component of many algorithms which extract patterns from transactional databases For example FIM can be leveraged to produce association 
rules clusters classiìers or contrast sets This capability provides a strategic resource for decision support and is most commonly used for market basket analysis One challenge for frequent itemset mining is the potentially huge number of extracted patterns which can eclipse the original database in size In addition to increasing the cost of mining this makes it more difìcult for users to nd the valuable patterns Introducing constraints to the mining process helps mitigate both issues Decision makers can restrict discovered patterns according to speciìed rules By applying these restrictions as early as possible the cost of mining can be constrained For example users may be interested in purchases whose total price exceeds 100 or whose items cost between 50 and 100 Constraint based mining is an ongoing area of research 
Two important categories of constraints are monotone and antimonotone 13 Anti-monotone constraints are constraints that when valid for a pattern they are consequentially valid for any subset subsumed by the pattern Monotone constraints when valid for a pattern are inevitably valid for any superset subsuming that pattern The straightforward way to deal with constraints is to use them as a lter post-mining However it is more efìcient to consider the constraints during the mining process This is what is refereed to as  pushing the constraints  14 Most e xisting algorithms le v erage or push one of these types during mining and postpone the other to a post-processing phase New algorithms such as Dualminer apply both types of 
constraints at the same time It considers these tw o types of constraints in a double process one mirroring the other for each type of constraint hence its name However monotone and anti-monotone constraints do not necessarily apply in duality Especially when considering the mining process as a set of distinct phases such as the building of structures to compress the data and the mining of these structures the application of these constraints differ by type Moreover some constraints have different properties and should be considered separately For instance minimum support and maximum support are intricately tied to the mining process itself while constraints on item characteristics such as price are not There is no existing algorithm that pushes both types of constraints early in the mining process and neither traverses the lattice of patterns 
top-down nor bottom-up We introduce herein an algorithm that pushes both monotone and anti-monotone constraints by wisely jumping several levels in the pattern lattice from the bottom and top and cleverly reducing the unnecessary constraint checking while considering the intricacies and properties of the constraints and the patterns sought after 1.1 Problem Statement The problem of mining association rules over market basket analysis was introduced in 1 The problem consists of nding associations between items or itemsets in transactional data Formally the problem is stated as follows Let 
be a set of literals called items Each item is an object with some predeìned attributes such as price weight etc and is considered the dimensionality of the problem Let be a set of transactions where each transaction is a set of items such that  A transaction is said to contain  
a set of items in if  A constraint is a predicate on itemset that yields either true or false  An itemset satis1 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


es a constraint if and only if is true  An itemset has a support s in the transaction set if s of the transactions in contain  Two particular constraints pertain to the support of an itemset namely the minimum support constraint and the maximum support constraint An itemset is said to be infrequent if its support is smaller than a given minimum support threshold  is said to be too frequent if its support is greater than a given maximum support  and is said to be large or frequent if its support is greater or equal than and less or equal than  1.2 Motivation and Contribution The problem of discovering all frequent itemsets that satisfy constraints is a difìcult one The difìculty stems from the fact that rstly testing for minimum support and maximum support can not be done simultaneously since when valid one is always true for subsets while the other is always true for supersets Secondly despite their selective power some constraints cannot be checked to lter candidate itemsets until a very late stage of the mining process depending upon the type of constraint and the search space traversal strategy used We introduce a frequent itemset mining algorithm with the following properties A leap traversal strategy is used to apply constraints from selected nodes in the lattice in contrast to bottom-up or top-down traversals Both monotone and anti-monotone constraints are pushed efìciently by placing and timing their respective evaluation strategically Regions where one constraint needs not be evaluated are identiìed quickly using proven theorems Previously known data structures such as FP-tree and COFI-tree 10 are used b u t n e w algorithms e xploiting these structures are proposed Constraints are used not only to extract the valid frequent itemsets but also concurrently to obtain the valid frequent closed and maximal patterns along with their respective supports The remainder of the paper is organized as follows The relevant types of constraints monotone and anti-monotone are discussed in Section 2 with illustrative examples Section 3 presents a new algorithm for frequent itemset mining using the COFI-tree idea but instead of a bottom-up or top-down approach selectively jumps within the pattern lattice to nd those patterns that satisfy the minimum support threshold 17 How to push both monotone and anti-monotone constraints and where these constraints are evaluated in this new approach is presented in Section 4 A selection from a battery of tests for performance evaluation is presented in Section 5 In particular we compare our approach to Dualminer Section 6 presents related work Finally Section 7 concludes the paper MONOTONE ANTI MONOTONE Table 1 Commonly used monotone and antimonotone constraints 2 Constraints It is known that algorithms for discovering association rules generate an overwhelming number of those rules While many new efìcient algorithms were recently proposed to allow the mining of extremely large datasets the problem due to the sheer number of rules discovered still remains The set of discovered rules is often so large that it becomes useless Different measures of interestingness and lters have been proposed to reduce the discovered rules but one of the most realistic ways to nd only those interesting patterns is to express constraints on the rules we want to discover However ltering the rules post-mining adds a signiìcant overhead and misses the opportunity to reduce the search space using the constraints Ideally dealing with the constraints should be done as early as possible during the mining process 2.1 Categories of Constraints A number of types of constraints have been identiìed in the literature In this w ork we discuss tw o important categories of constraints  monotone and anti-monotone  Deìnition 1  Anti-monotone constraints  A constraint is anti-monotone if and only if an itemset violates  so does any superset of  That is if holds for an itemset then it holds for any subset of  Many constraints fall within the anti-monotone category The minimum support threshold is a typical anti-monotone constraint As an example is an anti-monotone constraint Assume that items   and have prices 100 150 and 200 respectively Given the constraint   then since itemset  with a total price of 250 violates the constraint there is no need to test any of its supersets e.g  as they also violate the constraint Deìnition 2  Monotone constraints  A constraint is monotone if and only if an itemset holds for  so does any superset of  That is if is violated for an itemset then it is violated for any subset of  An example of a monotone constraint is  Using the same items   and as before and with constraint   then knowing that violates the constraint is sufìcient to know that all subsets of ABC will violate as well 2 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


Table 1 presents commonly used constraints that are either anti-monotone or monotone  From the deìnition of both types of constraints we can conclude that anti-monotone constraints can be pushed when the mining-algorithm uses the bottom-up approach as we can prune any candidate superset if its subset violates the constraint Conversely the monotone constraints can be pushed efìciently when we are using algorithms that follow the top-down approach as we can prune any subset of patterns from the answer set once we nd that its superset violates the monotone constraint Algorithm 1 COFILeap Leap-Traversal with COFI-tree Input transactional database Support threshold Output Type patterns with their respective supports Scan to nd the set of frequent 1-itemsets Scan to build the FP-tree using for each item in Header  in increasing support do if Not then Build COFI-Tree for FindFrequentPathBases  for each pair do if is frequent and not then Add in else Add in IF not end if end for for each pattern in do if is not a subset of any then Add in end if end for end if end for GeneratePatterns  Output 2.2 Bi-directional Pushing of Constraints Pushing constraints early means considering constraints while mining for patterns rather than postponing the checking of constraints until after the mining process Given the intrinsic characteristics of existing algorithms for mining frequent itemsets either going over the lattice of candidate itemsets topdown or bottom-up considering all constraints while mining is difìcult Most algorithms attempt to push either type of constraints during the mining process hoping to reduce the search space in one direction from subsets to supersets or from supersets to subsets Dualminer pushes both types of constraints but at the expense of efìciency Focusing solely on reducing the search space by pruning the lattice of itemsets is not necessarily a winning strategy While pushing constraints early seems conceptually beneìcial in practice the testing of the constraints can add signiìcant overhead If the constraints are not selective enough checking the constraint predicates for each candidate can be onerous It is thus important that we also reduce the checking frequency While the primary beneìt of early constraint checking is the elimination of candidates which can not pass the constraint it can also be used to identify candidates which are guaranteed to pass the constraint and therefore do not need to be re-checked In summary the goal of pushing constraints early is to reduce the itemset search space eliminating unnecessary processing and memory consumption while at the same time limiting the amount of constraint checking performed 3 COFI with Leap Most existing algorithms traverse the itemset lattice topdown or bottom-up and search using a depth rst or breadth rst strategy In contrast we propose a leap traversal strategy that nds a superset of pertinent itemsets by leaping between promising nodes in the itemset lattice In addition to nding these relevant candidate itemsets sufìcient information is gathered to produce the frequent itemset patterns along with their supports Here we use leap traversal in conjunction with the complementary COFI idea 10 where the locally frequent itemsets of each frequent item are explored separately This creates additional opportunities for pruning What is the COFI idea and what is this set of pertinent itemsets with their additional information This set of pertinent itemsets is the set of maximals We will rst present the COFI tree structure then we will introduce our algorithm COFILeap which mines for frequent itemsets using COFI trees and jumping in the pattern lattice In the next section this same algorithm will be enhanced with constraint checking to produce our algorithm BifoldLeap  3.1 COFI-trees The COFI-tree idea was introduced in as a means to reduce the memory requirement and speed up the mining strategy of FP-growth Rather than recursi v ely b uilding conditional trees from the FP-tree the COFI strategy was to create COFI-trees for each frequent item and mine them separately Conditional trees are FP-trees conditioned on the existence of a given frequent item The FP-tree is a compact preìx-tree representation of the sub-transactions in the input data Here sub-transactions are the original transactions with infrequent items removed The FP-tree contains a header and inter-node links which facilitate downward traversal forward in the itemset pattern as well as lateral traversal next node representing a speciìc item Building COFI-trees based on the FP-tree For each frequent item in the FP-tree in order of increasing support one COFI-tree is built This tree is based on sub-transactions which contain the root item and are composed only of items locally frequent with the root item that have not already been 3 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


used as root items in earlier COFI-trees The COFI-tree is similar to the FP-tree but includes extra links for upward traversal earlier in the itemset pattern a new participation counter in each node and a data structure to allow traversal of all leaves in the tree This participation counter is used during the mining process to count up the participation of each node in the generation of a frequent pattern Algorithm 2 BifoldLeap Pushing P and Q Input transactional database  P  Q  Output Frequent patterns satisfying P Q Scan to nd the set of frequent P1-itemsets Scan to build FP-tree using and for each item in Header  do if Not  then break if   then Add to and break if Not then Build COFI-Tree for FindFrequentPathBases  and frequent for each pair do Add in and Break IF  AND is frequent and not  Delete and break IF Not  Intersection\(FPBs not in  delete and break IF Not  Do not check for in any subset of IF   end for for each pattern in do Add in IF  not subset of any  end for end if end for PQ-Patterns GPatternsQ  Output PQ-Patterns 3.2 COFILeap algorithm COFILeap is different than the algorithm presented in in the sense that it generates maximal patterns where a pattern is said to be maximal if there is no other frequent pattern that subsumes it COFILeap rather than traversing the pattern lattice top-down it leaps from one node to the other in search of the support border where maximals sit Once maximals are found with the extra information collected all other patterns can be generated with their respective support Following is a brief summary of the COFILeap algorithm First a frequent pattern FP-tree is created using tw o scans of the database Second for each frequent item a COFI-tree is created including all co-occurant frequent items to the right i.e in order of decreasing support Each COFI-tree is generated from the FP-tree without returning to the transactional database for scans Unique sub-transactions in the COFI-tree along with their count called branch support  are obtained from the COFI-tree These unique sub-transactions are called frequent path bases FPB These can be obtained by traversing upward from each leaf node in the COFI-tree updating the participation counter to avoid over-counting nodes Clearly there is at most one FPB for each sub-transaction in the COFI-tree Frequent FPBs are declared candidate maximals Infrequent FPBs are intersected iteratively producing subsets which may be frequent When an intersection of infrequent FPBs results in a frequent itemset that itemset is declared as a candidate maximal and is not subject to further intersections When an intersection is infrequent it participates in further intersections looking for maximals This is indeed how the leaps in the lattice are done The result of the intersection of FPBs indicates the next node to explore How is the support of a pattern calculated Given the set of frequent path bases along with their branch supports it is possible to count the support of any itemset This is done by nding all FPBs which are supersets of the target itemset and summing their branch supports For example if there are two FPBs and  each with branch support 1 has support 2 and has support 1 Algorithm 1 shows the main steps of COFILeap  Notice that COFI-trees are not generated systematically for all frequent 1itemsets There is no need to look for maximals locally with respect to an item if and its locally frequent items are already subset of known global maximals Finally in the function the set of candidate maximal patterns is used along with the frequent path bases to produce the set of all frequent itemsets that satisfy the constraints along with their supports Maximal itemsets can be found by ltering the candidate maximals to remove subsets Supports for the candidate maximal patterns were computed as part of the intersection process to discover that they were frequent and therefore do not need to be recomputed Once the maximal itemsets have been found the all frequent itemsets can be found by iterating over all subsets of the maximals suppressing duplicates resulting from overlap with other maximal patterns Support counting for the frequent itemsets is done as for the FPBs i.e by summing the branch supports of all FPBs which are supersets of the pattern 4 Leap with constraints The conjunction of all anti-monotone constraints comprises a predicate that we call  A second predicate contains the conjunction of the monotone constraints A common approach is to include the ubiquitous minimum support constraint as part of  Similarly the monotone maximum support constraint can be included as part of  In this way a frequent itemset mining algorithm can be extended to push deeply by replacing checks for minimum support with checks for  In our algorithm we separate the constraints on the support from other constraints Thus the minimum support constraint and the maximum support constraint are extracted from 4 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


and respectively This is because checking for support is an integral part of the frequent itemset enumeration while other constraints on item attributes are used for search space pruning The algorithm COFILeap offers a number of opportunities to push the monotone and anti-monotone predicates and respectively We start this process by deìning two terms which are head   and tail   where is a frequent path base or any subset generated from the intersection of frequent path bases and is the itemset generated from intersecting all remaining frequent path bases not used in the intersection of  The intersection of and   is the smallest subset of that may yet be considered Thus Leap focuses on nding frequent that can be declared as local maximals and candidate global maximals BifoldLeap extends this idea to nd local maximals that satisfy  We call these P-maximals Although we further constrain the P-maximals to itemsets which satisfy  not all subsets of these P-maximals are guaranteed to satisfy  To nd the itemsets which satisfy both constraints the subsets of each P-maximal are generated in order from long patterns to short When a subset is found to fail  further subsets do not need to be generated for that itemset as they are guaranteed to fail also There are three signiìcant places where constraints can be pushed a while building the FP-tree b while building the COFI-trees and c while intersecting the frequent path bases which is the main phase where both types of constraints are pushed at the same time Algorithm 2 Constraint pushing opportunities during FP-tree construction First is applied to each 1-itemset Items which fail this test are not included in FP-tree construction Second we use the idea from FP-Bonsai where sub-transactions which do not satisfy are not used in the second phase of the FP-tree building process The supports for the items within these transactions are decremented This may result in some previously frequent items becoming infrequent Such items will not be used to construct COFI-trees in the following phase Constraint pushing opportunities during COFI-tree construction Let be the set of all items that will be used to build the COFI-tree i.e the items which satisfy individually but have not been used as the root of a previous COFI-tree If fails  there is no need to build the COFI-tree as no subset of can satisfy  Alternatively if satisìes  there is also no need to build the COFI-tree as is a candidate Pmaximal Constraint pushing opportunities during intersection of Frequent-Path-Bases There are two high-level strategies for pushing constraints during the intersection phase First and can be used to eliminate an itemset or remove the need to evaluate its intersections with additional frequent path bases Second and can be applied to the head intersect tail   which is the smallest subset of the current itemset that can be produced by further intersections These strategies are detailed in the following four theorems Theorem 1  If an intersection of frequent path bases   fails  it can be discarded and there is no need to evaluate further intersections with  Proof  If an itemset fails  all of its subsets are guaranteed to fail based on the deìnition of monotone constraints Further intersecting will produce subsets all of which are guaranteed to violate  Theorem 2  If an intersection of frequent path bases   passes  it is a candidate P-maximal and there is no need to evaluate further intersections with  Proof  Further intersecting will produce subsets of  By deìnition no P-maximal is subsumed by another itemset which also satisìes  Therefore none of these subsets of are potential new P-maximals Theorem 3  If a nodeês fails  the node can be discarded and there is no need to evaluate further intersections with  Proof  If an itemset fails  then all of its supersets will also violate from the deìnition of anti-monotone constraints Since a nodeês represents the subset of that results from intersecting with all remaining frequent path bases and all combinations of intersections between and remaining frequent path bases are supersets of and therefore guaranteed to fail also Theorem 4 If a nodeês passes  is guaranteed to pass for any itemset resulting from the intersection of a subset of the frequent path bases used to generate plus the remaining frequent path bases yet to be intersected with  does not need to be checked in these cases Proof  is guaranteed to pass for all of these itemsets because they are generated from a subset of the intersections used to produce the and are therefore supersets of the  Price No 1 2 3 41 S 2 S 3 S 4 S 5 S 5H ACDE 2H ABCDE 1H ABCD 2H ABCE 2H ABDE 2 H A H AB H AB H AB H SSSS H ACDE 2H ACD 3H ACE 3H ADE 3 H A H H H SS S H ACD 3H ACE 3H ADE 3 H H H 60 1 1 450 200 Price 1 1 1 Price 310 1 Price 510 Price 1 Price 410 360 410 Price 1 Price 510 60 150 100 ABCE ABDE 360 310 1 Price Price 760 810 D E Price 510 510 Price Price 510 960 860 A Itemset B C Null ACDE ABCDE ABCD FPB SB 1 1 1  Figure 1 Pushing P and Q The following example shown in Figure 1 illustrates how BifoldLeap works An A-COFI-tree is made from ve items     and  with prices 60 450 200 150 and 100 respectively In our example this COFI-tree generates 5 frequent path bases     and  each with branch support one The anti-monotone predicate is  and the monotone predicate is  Intersecting the rst FPB with the second produces which has a price of 510 and therefore violates and passes  Next we examine the  the intersection of this node with the remaining three FPBs which yields with price 60 passing and failing  None of these constraint checks provide an opportunity for pruning so we continue intersecting this itemset with 5 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


the remaining frequent path bases The rst intersection is with the third FPB producing with price 410 which satisìes both the anti-monotone and monotone constraints The second intersection produces  which also satisìes both constraints The same thing occurs with the last intersection which produces  Going back to the second frequent path base ABCDE we nd that the   violates the antimonotone constraint with price 510 Therefore we do not need to consider or any further intersections with it The remaining nodes are eliminated in the same manner In total three candidate P-maximals were discovered We can generate all of their subsets while testing only against  Finally the support for these generated subsets can be computed from the existing frequent path bases 5 Performance Evaluation To evaluate our BifoldLeap algorithm we conducted a set of experiments to test the effect of pushing monotone and antimonotone constraints separately and then both in combination for the same datasets To quantify scalability we experimented with datasets of varying size We also measured the impact of pushing versus post-processing constraints on the number of evaluations of and  Like in we assigned prices to items using both uniform and zipf distributions Our constraints consisted of conjunctions of tests for aggregate minimum and maximum price in relation to speciìc threshholds We compared our algorithm with Dualminer Based on its authors recommendations we built the Dualminer framework on top of the MAFIA implementation provided by its original authors Our experiments were conducted on a 3GHz Intel P4 with 2 GB of memory running Linux 2.4.25 Red Hat Linux release 9 The times reported also include the time to output all matching itemsets We have tested these algorithms using both real datasets provided by and synthetic datasets generated using we used retail as our primary real dataset reported here A dataset with the same characteristics as the one reported in w a s also generated We received an FP-Bonsai code base on FP-Growth from its original authors Unfortunately  not all pruning and clever constraint considerations suggested in their FP-Bonsai paper were implemented in this code Moreover the implementation as received produced some false positives and false negatives This is why we opted not to add it to our comparison study Although with simple and only monotone constraints the received FP-Bonsai implementation was indeed fast FPBonsai as described in the paper has merit but because of lack of time we could not implement it ourselves albeit adding implementation bias or x the received code 5.1 Impact of P and Q Selectivity on BifoldLeap and Dualminer To differentiate between our novel BifoldLeap algorithm and Dualminer we experimented against the retail dataset In the rst experiment Figure 2.A we pushed  then  and nally  We used the zipf distribution to assign prices to items Both and consisted of constraints on the sum of the prices The constraint thresholds were chosen to not be very selective Figure 2.B presents the same experiment with more selective constraints Figure 2.C presents pushing extremely selective constraints using anti-monotone and monotone constraints on the sum of the prices and on the minimum and maximum item price In this experiment we found that BifoldLeap in most cases outperforms Dualminer and in some cases by more than one order of magnitude The most interesting observation we found from this experiment was that if we push one type of constraint e.g  that takes seconds and the other type of constraint  takes seconds where  in Dualminer pushing both constraints together will take seconds where is always between and  In contrast BifoldLeap always takes less time with the conjunction of the constraints than with either constraint in isolation Monotone and anti-monotone constraints can indeed be mutually assisting each other in the selectivity BifoldLeap took better advantage of this reciprocal assistance in the pruning 5.2 Scalability Tests Scalability is an important issue for frequent itemset mining algorithms Synthetic datasets were generated with 50K 100K 250K and 500K transactions with 5K or 10K distinct items In this experiment BifoldLeap demonstrated extremely good scalability versus increasing dataset size In contrast Dualminer reached a point where it consumed almost three orders of magnitude more time than that needed by BifoldLeap Figure 3.A depicts one of these results while mining datasets with only 5K unique items As another experiment example we tested both algorithms on datasets with up to 50 Million transactions and 100K items Dualminer nished the 1M dataset in 8534 seconds while BifoldLeap nished in 186s 190s 987s and 2034s for the 1M 5M 25M and 50M transactions datasets respectively 5.3 Constraint Checking Pushing Constraints versus Post-processing One of the major challenging issues for constraint mining is reducing the number of evaluations of and  In the following experiment we generated a synthetic dataset with the same characteristics as the one reported in Speciìcally  i t was generated with 10,000 transactions an average transaction length of 15 an average maximal pattern length of 10 1000 unique items and 10,000 patterns We found that Dualminer was indeed good on this dataset as reported in Ho we v e r  BiFoldLeap outperformed it with the same order of magnitude as the tests on timing This shows that the predicate checking is indeed a signiìcant overhead and BiFoldLeap outperforms Dualminer in time primarily because it does signiìcantly less predicate checks The goal of these experiments was to test the number of evaluations and the effect of pushing constraints early versus post-processing them We ran our experiments using this 6 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


   retail dataset 0 20 40 60 80 100 120 140 160 180 0.17 0.11 0.08 Support Time in seconds BifoldLeap \(Q only DualMiner \(Q only BifoldLeap \(P only DualMiner \(P only BifoldLeap \(P & Q DualMiner \(P & Q retail dataset 0 20 40 60 80 100 120 140 160 180 0.17 0.11 0.08 Support Time in seconds BifoldLeap \(Q only DualMiner \(Q only BifoldLeap \(P only DualMiner \(P only BifoldLeap \(P & Q DualMiner \(P & Q retail dataset 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 0.17 0.11 0.08 Support Time in seconds BifoldLeap \(P & Q DualMiner \(P & Q Figure 2 A Pushing   and  B More selective constraints C Extremely selective constraints dataset with absolute support equal to 25 50 and 75 using the two different distributions We used a modiìed version of MAFIA with post-processing as the post-processing counterpart to Dualminer Our implementation of Dualminer always tests minimum support and P together while BifoldLeapês minimum support checks occur at different times and do not contribute to the count for  Figure 4 depicts the results of these experiments Our rst observation is that Dualminer performs a huge number of constraint evaluations as compared to BifoldLeap Even in cases where we only generated 255 patterns Dualminer needed more than 50 thousand evaluations for both and  compared to almost 6 thousand needed by BifoldLeap Our second observation is that MAFIA with post-processing requires fewer constraint evaluations than Dualminer retail dataset 0 20 40 60 80 100 120 140 160 0.17 0.11 0.08 Support Time in seconds BifoldLeap \(z-distribution DualMiner \(z-distribution BifoldLeap \(uniform-distribution DualMiner \(uniform-distribution Figure 3 A Scalability test B Effect of changing the price distribution 5.4 Different Distributions All of our experiments were conducted using uniform and/or zipf price distributions In most of the experiments we found that the effect of changing the distribution on Dualminer was greater than for BifoldLeap This can be justiìed by the effectiveness of the pruning techniques used by BifoldLeap that also reduce the number of candidate checks which consequently affected its performance Figure 3.B depicts one of these results for the retail dataset 6 Related work Mining frequent patterns with constraints has been studied in where the concept of monotone and anti-monotone and Figure 4 No of and evaluations using constraint pushing vs post-processing succinct were introduced to prune the search space Jian Pei et al 14 h a v e also generalized these tw o classes of constraints and introduced a new convertible constraint class In their work they proposed a new algorithm called which is an FP-Growth based algorithm This algorithm generates most frequent patterns before pruning them Its main contribution is that it checks for monotone constraints early and once a frequent itemset is found to satisfy the monotone constraint then all itemsets having this item as a preìx are sure to satisfy the constraint and consequently there is no need to apply further checks Dualminer is the rst algorithm to mine both types of constraints at the same time Nonetheless it suffers from many practical limitations and performance issues First it is built on the top of the MAFIA algorithm which produces the set of maximal patterns and consequently all frequent patterns generated using this model do not have their support attached Second it assumes that the whole dataset can t in main memory which is not always the case FP-Growth and our approach use a very condensed representation namely FPTree which uses signiìcantly less memory Third their top-down computation exploiting the monotone constraint often performs many useless tests for relatively large datasets which raises doubts about the performance gained by pushing constraints in the Dualminer algorithm In a recent study of par7 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


allelizing Dualminer 16 the authors sho wed that by mining relatively small sparse datasets consisting of 10K transactions and 100K items the sequential version of Dualminer took an excessive amount of time Unfortunately the original authors of Dualminer did not show any single experiment to depict the execution time of their algorithm but only the reduction in predicate executions A recent strate gy dealing with monotone and anti-monotone constraints suggests reducing the transactional database input via pre-processing by successively eliminating transactions that violate the constraints and then applying any frequent itemset mining algorithm on the reduced transaction set 4 The main drawback of this approach is that it is highly I/O bound due to the iterative process needed in rewriting the reduced dataset to disk This algorithm is also sensitive to the results of the initial monotone constraint checking which is applied to full transactions In other words if a whole transaction satisìes the monotone constraint then no pruning is applied and consequently no gains are achieved even if parts of this transaction do not satisfy the same monotone constraint To overcome some of the issues in the same approach has been tested against the FP-Growth approach in with ne w effective pruning heuristics 7 Conclusion Since the introduction of association rules a decade ago and the launch of the research in efìcient frequent itemset mining the development of effective approaches for mining large transactional databases has been the focus of many research studies Furthermore it is widely recognized that mining for frequent items or association rules regardless of its efìciency usually yields an overwhelming crushing number of patterns This is one of the reasons it is argued that the integration of data mining and database management technologies is required These large sets of discovered patterns could be queried Expressing constraints using a query language could indeed help sift through the large pattern set to identify the useful ones We argue that pushing the consideration of these constraints at the mining process before discovering the patterns is an efìcient and effective way to solve the problem This does not exclude the integration of data mining and database systems but suggests the need for data mining query languages intricately integrated with the data mining process In this paper we address the issue of early consideration of monotone and anti-monotone constraints in the case of frequent itemset mining We propose a leap traversal approach BifoldLeap  that traverses the search space by jumping from relevant node to relevant node and simultaneously checking for constraint violations The approach we propose uses existing data structures FP-tree and COFI-tree but introduces new pruning techniques to reduce the search costs We conducted a battery of experiments to evaluate our constraint-based search and report a fraction of them herein for lack of space The experiments show the advantages of pushing both monotone and anti-monotone constraints as early as possible in the mining process despite the overhead of constraint checking We also compared our algorithm to Dualminer a state-of-the-art algorithm in constraint-based frequent itemset mining and showed how our algorithm outperforms it and can nd all frequent itemsets the closed and the maximal patterns that satisfy constraints along with their exact supports References  R Agra w al T  Imielinski and A Sw ami Mining association rules between sets of items in large databases In Proc 1993 ACM-SIGMOD Int Conf Management of Data  pages 207Ö216 Washington D.C May 1993  R Agra w a l and R Srikant F ast algorithms for mining association rules In Proc 1994 Int Conf Very Large Data Bases  pages 487Ö499 Santiago Chile September 1994  I Almaden Quest synthetic data generation code http://www.almaden.ibm.com/software/quest/Resources/index.shtml  F  Bonchi F  Giannotti A Mazzanti and D Pedreschi Examiner Optimized level-wise frequent pattern mining with monotone constraints In IEEE ICDM  Melbourne Florida November 2004  F  Bonchi and B Goethals Fp-bonsai the art of gro wing and pruning small fp-trees In Paciìc-Asia Conference on Knowledge Discovery and Data Mining PAKDDê04  pages 155Ö160 2004  F  Bonchi and C Lucchese On closed constrained frequent pattern mining In IEEE International Conference on Data Mining ICDMê04  Brighton UK November 2004  C Bucila J Gehrk e D Kifer  and W  White Dualminer A dual-pruning algorithm for itemsets with constraints In Eight ACM SIGKDD Internationa Conf on Knowledge Discovery and Data Mining  pages 42Ö51 Edmonton Alberta August 2002  D Burdick M Calimlim and J Gehrk e Maìa A maximal frequent itemset algorithm for transactional databases In ICDE  pages 443Ö452 2001  S Chaudhuri Data mining and database systems Where is the intersection Bulletin of the Technical Committee on Data Engineering  21 March 1998  M El-Hajj and O R Za  ane Non recursive generation of frequent k-itemsets from frequent pattern tree representations In Proc of 5th International Conference on Data Warehousing and Knowledge Discovery DaWakê2003  September 2003  Frequent itemset mining implementations repository  http://ìmi.cs.helsinki  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation In ACM-SIGMOD  Dallas 2000  L Lakshmanan R Ng J Han and A P ang Optimization of constrained frequent set queries with 2-variable constraints In ACM SIGMOD Conference on Management of Data  pages 157Ö168 1999  J Pie and J Han Can we push more constraints into frequent pattern mining In ACM SIGKDD Conference  pages 350Ö354 2000  J Pie J Han and L Lakshmanan Mining frequent itemsets with convertible constraints In IEEE ICDE Conference  pages 433Ö442 2001  R M T ing J Baile y  and K Ramamohanarao P aradualminer An efìcient parallel implementation of the dualminer algorithm In Eight Paciìc-Asia Conference PAKDD 2004  pages 96Ö105 Sydney Australia May 2004  O R Za  ane and M El-Hajj Pattern Lattice Traversal by Selective Jumps In In Proc 2005 Intêl Conf on Data Mining and Knowledge Discovery ACM SIGKDD Chicago IL USA  pages 729Ö735 August 2005 8 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


on the intention to use self- and e-assessment center as well as the influence of PQI, PCS and PTS on PU it might represent a single example for a specific economy, country or cultural region . There might be differences for the intention of different informnation systems with different settings. Future research should discuss this issue more detailed  Furthermore our questionnaire was just answered by large-scale corporations in Germany. So our results might not be directly transferred to small and medium-sized enterprises  In addition, as we collected our data from participants at the same time using the same survey our results may be affected by common method varian  6. Conclusion  Self-assessment and e-assessment as online games in staff recruiting are a valuable solution for companies to support on the one side their employer branding activities and on the other side the selection of the favored candidate. The case study of our papers shows that companies can generate more qualified applications and concurrently save time and money. Based on these findings our empirical research with 191 of the Top-1000 companies of Germany provide statistical evidence that perceived quality improvements, perceived cost savings and perceived time savings are the main reasons beside perceived usefulness and ease of use why companies intend to use self- and e-assessment in their recruiting processes  References   Bago zzi  R  P   T h e Ro l e  of M easu r e m en t i n T h eo ry Construction and Hypothesis Testing: Toward a Holistic Model", in: Conceptual and Theoretical Developments in Marketing, O.C. Ferrell, S.W. Brown and C.W. Lamb eds.\ago, pp. 15-32, 1979 2 Ba g o z z i R  P  a n d Y i  Y   O n t h e Ev a l ua tio n of  Structural Equation Models", Journal of the Academy of Marketing Science \(16\pp 74-94, 1988 3 Ba rtra m  D Testing on the Internet: Issues Challenges and Opportunities in the field of Occupational Assessments in: D. Bartram, and R.K. Hambleton \(eds Computer-Based Testing and the Internet: Issues and Advances, John Wiley & Sons, Ltd., Chichester, England 2006 4 B r o w n, S  A  a nd Ve nk a t e s h V  Model of Adoption of Technology in Households: A Baseline Model Test and Extension Incorporating Household Life Cycle MIS Quarterly \(29:3\pp.399-426, 2005  Bu zzetto M o r e N A   and A l ad e A  J   Best Practices in e-Assessment Journal of Information Technology Education, 2006, Vol. 5, pp. 251-269 6 Ca lig iuri P  M., a n d P h i llips J.M An application of self-assessment realistic job previews to expatriate assignments The International Journal of Human Resource Management, 2003, Vol. 14, No. 7, pp. 11021116 7 Ca rm ine s E. G  Ze lle r R A  1979 Re lia bil ity a nd Validity Assessment. Beverly Hills, CA: Sage Publications 8 Ca s c io, W  F A pplie d P s y c ho log y in H u m a n Re s ourc e  Management, Prentice Hall, Englewood, Cliffs, NJ, 1998 9 Cha p p e ll, D Ea t oug h V D a v i e s M.N  O  a n d  Griffiths, M EverQuest It s Just a Comupter Game Right? An Interpretative Phenomenological Analysis of Online Gaming Addicition International Journal of Mental Health and Addiction, Vol. 4, No. 3, pp. 205-216 10 C h in, W  W  T he P a rtia l L e a s t Squa re s A pproa c h to  Structural Equation Modelling", in: Modern methods for business research, G.A. Marcoluides \(ed.\, Lawrence Erlbaum Associates, London, pp. 295-336, 1998 11 Ch in, W  W  F re que ntly A s k e d Q u e s tions  Partial Least Squares & PLS-Graph", 2000 12 hill J. R a n d G ilbe r t, A  A P a ra dig m  f o r Developing Better Measures of Marketing Constructs  Journal of Marketing Research 15 \(2\pp. 77-94, 1979 13 C o n nol ly T a nd Sta n s f ie ld M  Using GamesBased eLearning Technologies in Overcoming Difficulties in Teaching Information Systems Journal of International Technology Education, 2006, Vol. 5, pp. 459-476 14 D a v i s  F. D   1 9 89  P e r c e i v e d U s ef ulne s s  P e r c e i v e d Ease of Use and User Acceptance of Information Technology. MIS Quarterly, 13:3, pp. 319-339 15 D o ll, W  J A  H e ndric k s on, a n d X  D e ng  Using Davis s Perceived Usefulness and Ease-of-use Instruments for Decision Making: A Confirmatory and Multigroup Invariance Analysis  Decision Sciences 29\(4\, 1998, pp 839-869 16 Eis e n h a r dt, K   M. \(1 98 9 ilding T h e o rie s f r om Ca s e  Study Research, Academy of Management Review, 14, 4 532-550 17  Fa e r be r, F  K e im T a nd W e itz e l T  An Automated Recommendation Approach to Personnel Selection  In: Proceedings of the 2003 Americas Conference on Information Systems; Tampa 18  Fe ng M H e ff e r na n, N  T a nd K o e d ing e r, K  R  Addressing the Testing Challenge with a Web-Based EAssessment System that Tutors as it Assesses  Proceedings of the 15th international conference on World Wide Web, Edinburgh, Scotland, 2006, pp. 307-316 19  For n e ll, C a n d L a rc ke r, D F E v a lua ting S t ruc tura l  Equitation Models with Unobservable Variables and Measurement Errors", Journal of Marketing Research, 18 pp. 39-50, 1981 20 G e org e  D I., a nd Sm ith, M.C An Empirical Comparison of Self-Assessment and Organizational Assessment in Personnel Selection Public Personnel Management, 1990, Vol. 19, No. 2, pp. 175-190 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


21 G r iff iths, M.D., Da v i e s  M.N.O., a nd Cha p p e ll, D  Breaking the Stereotype: The Case of Online Gaming  CyberPsychology & Behavior. January 1, 2003, Vol 6, No 1, pp. 81-91 22 Hu l l an d  J U se o f P a rtial L e a s t Sq u a res \(P L S  in   Strategic Management Research: A Review of Four Recent Studies. Strategic Management Journal, 20 \(4\, pp. 195204, 1999 23 J o ne s L a nd Fle t c h e r C  Self-assessment in a selection situation: An evaluation of different measurement approaches Journal of Occupational and Organizational Psychology, 2002, Vol. 75, pp. 145-161 24 K e im T  a nd W e itz e l  T  Strategies for Hiring IT Professionals: An Empirical Analysis of Employer and Job Seeker Behavior on the IT Labor market In: Proceedings of the 2006 Americas Conference on Information Systems AMCIS\ulco, Mexico 25 L a um e r S Ec k h a r dt, A a nd W e itz e l T   Recruiting IT Professionals in a Virtual World In: Proceedings of the 12th Pacific Asia Conference on Information Systems PACIS 2008\uzhou, China 2 L e e I   The Architecture for a Next-Generation Holistic E-Recruiting System Communications of the ACM, Vol. 50, No. 7, 81-85, 2007 27 L e v i ne E.L Flory  A   a nd A s h, R.A  Selfassessment in personnel selection Journal of Applied Psychology, 1977, Vol. 62, No. 4, pp. 428-435 28  Ma lho t ra Y., a n d G a lle tta D. F Extending the Technology Acceptance Model to Account for Social Influence Theoretical Bases and Empirical Validation  Proceedings of the 32 nd Hawaii International Conference on System Sciences 2 Lo u H Lu o  W  and St r ong D  2 00 0   Perceived Critical Mass Effect on Groupware Acceptance  European Journal of Information Systems 9\(2\p.91-103 30 N u nna lly J  C., a nd Be rns t e i n, I. H  Psychometric Theory 3rd ed.\ McGraw-Hill, New York, 1994 31 Phil lips  J.M  Effects of Realistic Job Previews on Multiple Organizational Outcomes: A Meta-Analysis  Academy of Management Journal, 1998, Vol. 41, No. 6 pp. 673-690 3  P o d s ako f f  P   M    M acKen zie S  B  L ee J Y  Podsakoff, N. P. \(2003\. Common Method Bias in Behavioral Research: A Critical Review of the Literature and Recommended Remedies. Journal of Applied Psychology, Vol. 88, No. 5, pp. 879-903 33 Ri dg w a y  J Mc Cus k e r, S., a nd P e a d D   Literature Review of e-Assessment Nesta Future Lab, Bristol, UK 2004 3 Ry n e s S  L   Recru i t m en t   Job Choice, and Post-Hire Consequences in: M.D. Dunnette, and L.M. Hough \(eds Handbook of Industrial and Organizational Psychology Consulting Psychologists Press, Palo Alto, CA, 1991 35 Ste i nk ue hle r   C.A  Learning in massively multiplayer online games Proceedings of the 6th international conference on Learning sciences, Santa Monica, California, 2004 36 U nde rw ood J  D  M Ba ny a r d P.E. a n d D a v i e s  M.N.O Students in digital worlds: Lost in Sin City or reaching Treasure Island BJEP Monograph Series II Number 5 - Learning through Digital Technologies Volume 1, Number 1, 1 July 2007 , pp. 83-99 36  Ve nk a t e s h V M o r r i s  M  G  un d A c k e r m a nn P  L   2000 A Longitudinal Field Investigation of Gender Differences in Individual Technology Adoption DecisionMaking Processes Organizational Behavior and Human Decision Processes, Vol. 83, Nr. 1, S. 33-60 3 W e i ss E  A    Self-Assessment Communications of the ACM, 1990, Vol. 33, No. 11, pp. 110-132 38 W a rd, M G r uppe n, L a n d Re g e hr, G  Measuring Self-assessment: Current State of the Art Advances in Health Sciences Education, 2002, Vol. 7, pp. 63-80 39 Y i n, R. K  20 03 s e Stu d y R e s e a r c h D e s i g n a nd Methods, Sage Publications, Inc., Thousand Oaks, London New Dehli Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


  11 could be improved by a simple modification of the feed by adding a small tuning vane to th e feed. Therefore, it can be stated that some improvement can be expected by modification of the feeds, and adaptation of the test antenna in such a way that surrounding Ku-band element are closed   Figure 28 Reflection coefficient of Ku-band stacked patch antenna element in dual-frequency antenna stack  Figure 29 shows the influence of the L-band slots on the return loss of the Ku-band antenna element. To this end, the four connectors of the L-band elements were alternately open and terminated by means of 50 loads. The deviations were measured with respect to the set-up where all connectors were terminated Apparently, the deviations are acceptable  Figure 29 Influence of L-band termination on return loss of Ku-band antenna element, with and without termination Figure 30 and Figure 31 show the isolation between the Lband and Ku-band elements in L-band and Ku-band respectively. To this end the S21-parameters have been measured. These figures reveal that the mutual coupling between the L-band and Ku-band elements is sufficiently small  Figure 30 Measured isolation between L-band and Ku-band antennas in L-band frequencies  Figure 31 Measured isolation between L-band and Ku-band antennas in Ku-band frequencies From these measurements it can be concluded that opportunities should be considered to improve the design of the dual-frequency antenna \(by optimizing the feeds of the Ku-band elements antenna and the measurement set-up \(closure of surrounding Ku-band ports and use of appropriate connectors for the open Ku-band ports 7  M ODIFIED DUAL FREQUENCY ANTENNA  In order to benefit the str ong points of the two separate designs as discussed in section 4, an alternative antenna is proposed that exploits the properties of a \221best of both worlds\222 solution employing ideas from both designs. The modified antenna possesses an aperture fed L-band patch of a similar form to first design, but situated towards the bottom of the stack. Ku band el ements are located within the L-band perforations and para sitic patches are situated above a foam spacer \(see Figure 32 and Figure 33\A measurement campaign is underway to assess the behaviour of this modified test antenna 


  12  Figure 32 Bottom view of dual frequency antenna tile with perforated L-band patc h in lower layer with Kuband patches  Figure 33 Layer stack with perforated L-band patch in lower layer with Ku-band patches  8  B EAM FORMING N ETWORK  A major keystone for the su ccess of phased array antenna onboard aircraft is the capability of steering the main beam in the direction of the geosta tionary satellites. This requires the inclusion of a broadband beam forming network. Beam steering can be realized by adding RF-phase shifters and LNA\222s to the antenna elements of the array. However traditional phase shifters in ge neral have a narrow band, and hence do not yield the re quired broadband capability Alternative technologies for broadband beam forming are switched beam networks \(using Butler matrices innovative designs for RF-compone nts such as phase shifter LNA components in \(M\IC technology, or beam forming by using opti cal ring resonators  The German SME IMST is involved in several projects for development of electronica lly steerable phased array antennas for satellite communication. In the NATALIA project \(New Automotive Track ing Antenna for Low-cost Innovative Applications\ ESA, IMST is investigating the possibility of realizing a compact costeffective solution for a recei ve-only full electronically steerable antenna for cars in Ku-band. This antenna is a planar array composed of approximately 150 patches circularly polarised by using a 90\260 hybrid, and arranged in a hexagonal fashion. Each patc h is equipped with a MMIC corechip containing a phase sh ifting unit, LNA and digital steering logic  In the Netherlands, a consortiu m \(consisting of University of Twente, Lionix BV, National Aerospace Laboratory NLR and Cyner Substrates developing in the national FlySmart project technology for a broadband optical beam forming network. For the steering of the beam of the conformal phased array a squi nt-free, continuously tunable mechanism is proposed that is based on a fully integrated optical beam forming network \(OBFN optical ring resonators \(ORRs as tunable delay elements. A narrowband continuously tunabl e optical TTD device is realized as a recirculating wa veguide coupled to a straight waveguide. This straight wave guide can behave as a bandpass filter with a periodic, bell-shaped tunable group delay response. The maximum group delay occurs at a tunable resonance frequency. A larger delay-bandwidth product can be achieved by cascading multiple ORR sections. A complete OBFN can be obtaine d by grouping several delays and combining elements in one optical circuit. Such an OBFN can be realized on a si ngle-chip. Electrical/Optical E/O O E by means of filter based single-sideband modulation suppressing the carrier lanced coherent optical detection. Further details of the optical beamforming network have been presented in Re The proof-ofconcept has been shown by manufacturing a chip for an 8x1 OBFN. Essential components of the OBFN are the optical modulators, which are used to modulate the light in the ORR system 9  C ONCLUSIONS  For enhanced communicati on on board aircraft, novel antenna systems with broa dband satellite-based capabilities are required. So far, existi ng L-band satellite based systems for communications are used primarily for passenger application \(APC\i nistrative communications AAC and now data are tending to evolve towards broadband dig ital applications \(Voice over IP\any studies are going on worldwide to employ Kuband TV geostationary sate llites for communication with mobile terminals on aircraft The inbound traffic is about 5 times higher than the outbound The inbound traffic requires the availability of a broadband Ku-band antenna in receive mode only. The outbound traffic services can be supplied by the Inmarsat SBB link, whic h requires the installation of an L-band transmit antenna. In order to avoid both the installation of L-band antenna and Ku-band antenna, the concept of a hybrid dual frequency antenna operating L 


  13 band and Ku-band with low aerodynamic profile has been investigated in this paper. Keyaspects of this research are 200  Design and testing dual-fre quency antenna elements operating in both L-band and Ku-band 200  Conformal aspects of Ku-band phased array antennas 200  Beam forming algorithms for planar and conformal phased array antennas Two designs for dual-frequency antenna tiles consisting of 8x8 Ku-band antenna elements and one L-band element The designs have been analysed by means of computer simulations. Both designs show promising performance both in L-band and Ku-band. The design with slotted Lband antenna has a resonant fre quency in receive mode with a bandwidth of about 1 GHz. The Ku-band antenna is a stacked patch configuration where a parasitic element is placed above a lower patch separated by dedicated space filler. The manufactured protot ype antennas indicate that the bandwidth is sufficiently large In order to be able to communicate with geostationary satellites also at high latitudes e.g. during inter-continental flights\stem should have sufficient performance at low elevation angles. The antenna Ku-band system is required to have a small beamwidth \(to discriminate between the satellite signals\gain 30 dB angles. The effects of these requirements on the size and positioning of the antenna on the aircraft fuselage have been investigated. These requirements can be best satisfi ed by installing two planar phased array antennas on both side s of the fuselage with at least 1600 Ku-band elements. Each element has two feed lines, one for each polarization Every feed line has to be connected to the beam formi ng network. This means that the connections cannot be routed to one of the four sides of the antenna. Instead the concept of vertical feed lines \(by means of vias in a sufficiently thick substrate recommended. These vertical f eed lines connect the L-band and Ku-band antenna elements in the upper layer with feed networks in multiple lower laye rs. This vertical feed line system was not available so far due to manufacturing problems The performances of one dua l-frequency antenna design have been investigated by manufacturing two test antennas without vertical feed line syst em. The first antenna contains only a multilayer structure with L-band slots and 8x8 Kuband stacked patches. The performances of the L-band slots and Ku-band stacked patches c ould be measured separately It was concluded that opportun ities should be considered to improve the design of the dual-frequency antenna \(by optimizing the feeds of the Ku-b and elements the dual frequency test-antenna and the measurement set-up More important, however, is the realization of a mechanically stable vertical feed line system, so that the properties of L-band and Ku-band elements can be measured adequately The second test antenna contains only a multilayer structure with 8x8 Ku-band stacked patches and a feed network with 8 combiners, where each comb iner coherently sums 8 antenna elements. In combination with a prototype 8x1 OBFN, a Ku-band phased arra y antenna is obtained of which the beam can be steered in one direction. This second test antenna is used to analyze the broadband properties of the 8x8 Ku-band antenna array and 8x1 OBFN. The measured performances of this antenna are presented in Ref   A CKNOWLEDGMENT  This work was part of the EU 6 th Framework project ANASTASIA., and the FlySmart project, supported by the Dutch Ministry of Economic A ffairs, SenterNovem project numbers ISO53030 The FlySmart project is part of the Eureka PIDEA  project SMART Cyner Substrates is acknowle dged for technical assistance during the fabrication of the prototype antennas 


  14 R EFERENCES  1  P. Jorna, H. Schippers, J. Verpoorte, \223Beam Synthesis for Conformal Array Antennas with Efficient Tapering\224 Proceedings of 5 th European Workshop on Conformal Antennas, Bristol, September 11-12, 2007 2  The Radio Regulations, editi on of 2004, contain the complete texts of the Radio Regulations as adopted by the World Radio-communication Conference \(Geneva WRC-95 tly revised and adopted by the World Radio-communication Conference WRC-97\RadioWRC2000\and the World Radio-communication Conference WRC-03 Resolutions, Recommendations and ITU-R Recommendations incorporat ed by reference 3  RECOMMENDATION ITU-R M.1643, Technical and operational requirements for ai rcraft earth stations of aeronautical mobile-satellite service including those using fixed satellite service network transponders in the band 14-14.5 GHz \(Earth-to-space 4  ETSI EN 302 186 v1.1.1 \(2004-01 Stations and Systems \(SES\onised European Norms for satellite mobile Aircraft Earth Stations AESs\the 11 12/14 GHz frequency bands covering essential requirement s under article 3.2 of the R&TTE directive 5  EUROCAE ED-14E; Environmental Conditions and Test procedures for Airbor ne Equipment, March 2005 6  F. Croq and D. M. Pozar, \223Millimeter wave design of wide-band aperture-coupled stacked microstrip antennas,\224 IEEE Trans. Antennas Propagation, vol. 39 pp. 1770\2261776, Dec. 1991 7  S. D. Targonski, R. B. Waterhouse, D. M. Pozar Design of wide-band aperture stacked patch microstrip antennas ", IEEE Transactions on Antennas and Propagation, vol. 46, no. 9, Sep. 1998, pp. 1245-1251 8  R. B. Waterhouse, "Design of probe-fed stacked patches", IEEE Transactions on Antennas and Propagation, vol. 47, no. 12, Dec. 1999, pp. 1780-1784 9  D.M. Pozar, S. D. Targonski, \223A shared aperture dualband dual-polarised microstrip array\224, IEEE Transactions on Antennas and Propagation,Vol. 49 no. 2,Feb. 2001, pp. 150-157 10  http://www.ansoft.com 11  J-F. Z\374rcher, F.E. Gardiol, \223Broadband patch antennas\224 Artech House, \(1995\N 0-89006-777-5 12  H. Schippers, J. Verpoorte, P. Jorna, A. Hulzinga, A Meijerink, C. G. H. Roeloffzen, L. Zhuang, D. A. I Marpaung, W. van Etten, R. G. Heideman, A. Leinse, A Borreman, M. Hoekman M. Wintels, \223Broadband Conformal Phased array with Optical Beamforming for Airborne Satellite Communication\224, Proc. of the IEEE Aerospace Conference, March 2008, Big Sky, Montana US 13  H. Schippers, J. Verpoorte, P. Jorna, A. Hulzinga, L Zhuang, A. Meijerink, C. G. H. Roeloffzen, D. A. I Marpaung, W. van Etten, R. G. Heideman, A. Leinse M. Wintels, \223Broadband Op tical Beam Forming for Airborne Phased Array An tenna\224, Proc. of the IEEE Aerospace Conference, March 2009, Big Sky, Montana US 


  15  B IOGRAPHIES  Harmen Schippers is senior scientist at the National Aerospace Laboratory NLR. He received his Ph. D. degree in applied mathematics from the University of Technology Delft in 1982. Since 1981 he has been employed at the National Aerospace laboratory NLR. He has research experience in computational methods for aero-eleastics, aeroacoustic and electromagnetic problems. His current research activities are development of technology for integration of smart antennas in aircraft structures, and development of computational tools for installed antenna analysis on aircraft and spacecraft  Jaco Verpoorte has more than 10 years research experience on antennas and propagation Electromagnetic compatibility \(EMC and radar and satellite navigation He is head of the EMC-laboratory of NLR. He is project manager on several projects concerning EMCanalysis and development of advanced airborne antennas    Adriaan Hulzinga received his BEng degree in electronics from the hogeschool Windesheim in Zwolle Since 1996 he has been employed at the National Aerospace laboratory \(NLR as a senior application engineer. He is involved in projects concerning antennas and Electromagnetic compatibility \(EMC  Pieter Jorna received the M.Sc degree in applied mathematics from the University of Twente in 1999 From 1999 to 2005 he was with the Laboratory of Electromagnetic Research at the University of Technology Delft. In 2005 he received the Ph.D. degree for his research on numerical computation of electromagnetic fields in strongly inhomogeneous media Since 2005 he is with the National Aerospace Laboratory NLR\ in the Netherlands as R&D engineer   Andrew Thain is a research engineer in the field of electromagnetic modelling of antennas. He specialises in the use of surface integral methods for the calculation of coupling and radiation patterns and works closely with Airbus on the topic of antenna positioning. He has experience in the field of electromagnetic modelling  Gilles Peres is head of the Electromagnetics group of EADS-IW He has a wide experience in computational EM modelling particularly the use of FDTD, integral and asymptotic techniques for antenna structure interactions. He has contributed with Airbus experts to the certification campaign of the A340/500 and A340/600. Dr Peres holds a PhD thesis from University of Toulouse \(1998\ on impulsive Electromagnetic Propagation effects through plasma   Hans van Gemeren has a BEng degree in electronics. From the beginning of Cyner substrates he is involved in development and production of prototyping and nonconventional Printed Circuit boards Working mainly for design and research centers Cyner got involved in many high tech projects and from this developed a great expertise in the use of different \(RF materials. In the FlySmart project Hans and his colleagues are able to do what they like most: In close cooperation with designers, creatively working on substrate solutions 


  16  


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


