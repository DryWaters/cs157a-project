Hiding Sensitive Patterns in Association Rules Mining Guanling Lee  Chien-Yu Chang  Arbee L.P Chen   Department of Computer Science  Department of computer  and Information Engineering                    Science National Dong Hwa University          National Cheng-chi University Email: guanling@mail.ndhu.edu.tw      Email: alpchen@cs.nthu.edu.tw Abstract Data mining techniques have been developed in many applications. However, it also causes a threat to privacy. We investigate to find an appropriate balance between a need for privacy and information discovery on association patterns. In this paper, we propose an innovative technique for hiding sensitive patterns. In our approach, a sanitization matrix is defined. By multiplying the original transaction database and the sanitization matrix, a new database which is sanitized for privacy concern, is gotten Moreover, a set of experiments is performed to show the effectiveness of our approach     Keywords association patterns, privacy preservation sanitized database, data mining 1.Introduction Data mining techniques have been developed in many applications and researches. However, it also brings the problem of privacy. A motivating example is discussed in Suppose we have a server and many clients in which each client has a set of data The clients want the server to gather statistical information about association among items in order to provide recommendations to the customers However, the clients do not want the server to know some sensitive patterns Sensitive pattern is the frequent itemset that contain highly sensitive knowledge. Thus, when a client sends its database to the server, some sensitive patterns are hidden from its database according to some specific privacy policies Therefore, the server only can gather statistical information from the modified database In recent years, more and more researchers emphasize the seriousness of the problem about privacy. The privacy problem can be classified into two classes data privacy problem and information privacy problem. Data privacy is to protect the privacy of sensitive data, while information privacy is investigated privacy of patterns that contain highly sensitive knowledge Privacy-preserving mining in the context of data privacy for classification rules has been investigated in using a ra ndom iz ing funct ion wit h Gaussian or Uniform perturbations, the sensitive values in userês record will be perturbed. Based on probabilistic distortion of user demonst rates a schem problem  of how to avoi d pri vacy breaches in privacy preserving data mining is introduced Information privacy preserving problem is to hide the sensitive patterns or rules by updating the original database and with as little effect on non-sensitive patterns as possible. This problem is Proceedings of the 28th Annual International Computer Software and Applications Conference \(COMPSACê04 0730-3157/04 $20.00 © 2004 IEEE 


proved to be NP-Hard ABE99 Similar to ABE99  the other heuristic method is proposed in h ey falsify some value or replace known values with unknown values such as question marks In a f r a m ewor k is pr opose d to e n force privacy in mining frequent itemsets. In the approach the victim items that should be eliminated for each restrictive pattern are selected. And transaction retrieval engine is used to identify sensitive transactions for each restrictive pattern. Based on the disclosure threshold, the number of sensitive transactions is computed and the victim items are removed from the select transactions In this paper we propose an innovative technique for hiding sensitive patterns. By observing the relationship between sensitive patterns and non-sensitive patterns a sanitization matrix is defined. By setting the entries in sanitization matrix to appropriate values and multiplying the original transaction database to the sanitization matrix, a sanitized database is gotten The sanitized database is the database that has been modified for hiding sensitive patterns with privacy concern The reminder of this paper is organized as follows The problem and the framework of our approach is presented in section 2 In section 3, the sanitizing algorithms are discussed. The metrics to estimate the performance of our approach is introduced in section 4. The experimental results are also reported in section 4. We conclude with a summary and directions for future work in section 5 2. Basic Concept 2.1 Problem Formulation In our approach, a transaction database D is represented as a matrix in which the rows represent transactions and the columns represent the items. If D contains m transactions and n kinds of items D is represented as an mxn matrix. The entry D t,i is set to 1 if item i is purchased in transaction t. Otherwise, it is set to 0 Our problem can be formulated as follows. Let D be a transaction database P be the set of frequent patterns that can be mined from D Let P h denote a set of sensitive patterns that need to be hidden according to some security policies, and P h P  P h is the set of non-sensitive patterns P h  P h P Our problem is to transform D into a sanitized database D such that only the patterns belong to P h can be mined from D  1  2  3 1  2   3 1  2  3 t1 t2 t3 t4 10  1 11 0 0  0  1 1  1 1 1  0   0 1 1  0 0   0  1 t1 t2 t3 t4 1  0  1 0  1 0 0  0  1 0  1 1 1 2 3 X  CM DD Figure 1 Setting r 21 1 in S 2 Proceedings of the 28th Annual International Computer Software and Applications Conference \(COMPSACê04 0730-3157/04 $20.00 © 2004 IEEE 


2.2 Sanitization matrix In our approach, original database D is multiplied by a sanitization matrix S  get a sanitized database D  By setting S ij where i j to appropriate value, a sanitized database D will be gotten In the following the basic concept of our approach is discussed 2.2.1 New definition for the matrix multiplication In our approach, the matrix multiplication method is defined as follows 1. If D ti equals zero, no multiplication proceeds on it. That is D ti is set to 0 directly. This is because our goal is to hide the sensitive pattern by decreasing its support. Moreover, if an entry with value zero can be converted to 1, new patterns may be produced 2 If the resulting value larger than 1, set it to 1 3 If the resulting value smaller than 0 set it to 0 2.2.2 The Setting of ç-1 A sensitive pattern can be hidden by decreasing its support. If D ti and D tj are both equal to 1, set D ti or D tj be 0 can reduce the support of {i, j}. Refer to Figure 1. Let minimum support be 50% and {1, 2} be a sensitive pattern. If S 21 is set to -1 D 21 D 41 will become 0. Oppositely, if S 12 is set to -1 D 22 D 42 will become 0.Therefore, the support of {1,2} can be decreased by setting S 21 or S 12 to -1. Moreover, if S ij is set to 1, then for a transaction t, where D ti and D tj are both equal to 1, D tj will be 0 2.2.3 The Setting of ç1 Setting appropriate entries in S to 1 can reduce the support of the sensitive patterns However it also leads to accidentally conceal the non-sensitive patterns. We remedy this defect by setting some entries in S to 1 to minimize the effect on losing non-sensitive patterns. Continue above example, the frequent patterns in D are {1,2} and {1,3 Let {1,2 and {1,3} be the sensitive and non-sensitive pattern respectively. If S 21 is set to Ö1, D 21 and D 41 will be 0 As a result, the support of {1, 3 will be decreased to 25% in D and no longer be a frequent pattern. To reserve the non-sensitive pattern {1,3} in D S 31 is set to 1 to make D t1 keep the same value as D t1 for those transaction t where D t1 1and D t3 1. The purpose of setting the specific entry to 1 is to reinforce the relation of {1 3 and avoid eliminating 1, 3} accidentally. Setting corresponding entries between any two items contained in non-sensitive patterns in S to 1 can preserve non-sensitive patterns after the sanitization process  3.The Sanitization Algorithms 3.1 Hidden-First Algorithm The main idea of Hidden-First algorithm denoted by HF, is to eliminate all patterns in P h from D by setting proper entries in S to Ö1. The entries of S are set to the proper values according to the following rules 1. S ii 1, diagonal entry 2. S ij  1 If P h such that {i, j and   P h i j  Moreover, the number of patterns containing {j} in P h is smaller than 3 Proceedings of the 28th Annual International Computer Software and Applications Conference \(COMPSACê04 0730-3157/04 $20.00 © 2004 IEEE 


that of the patterns containing {i} in P h The reason is that by setting S ij to 1, the support of item j will be reduced. Moreover, item j has smaller effect on P h than item i 3. S ij 0, otherwise Hidden-First Algorithm Input: Ph Ph, D, S Output: D Step 1: Set the values of the entries in S according to the rules Step 2: \(matrices multiplication For every transaction i in D do For j=1 to number of items do If \(D ij 0 ij 0 Else D ij max  0 items of number k kj ik 1 S D However, in this approach, some non-sensitive patterns may be accidentally hidden due to setting the value of some entries in S to Ö1 3.2 The Non-Hidden-First Algorithm \(NHF The main idea behind the Non-Hidden-First algorithm, denoted by NHF, is to reserve all non-sensitive patterns and endeavor to hide sensitive patterns from D at the same time The entries in S are set according to the following rules 1. S ii 1, diagonal entry 2. S ij  1 If P h such that {i, j and  P h i, j  Moreover the number of patterns containing {j in P h is smaller than that of the patterns containing {i} in P h The reason is that by setting S ij to 1, the support of item j will be reduced Moreover, item j has smaller effect on P h than item i 3. S ij 1, If P h i j  and  P h  such that {i, j  4. S ij 0, otherwise Non-Hidden-First Algorithm Input: P h P h D, S Output: D Step 1: Set the values of the entries in S according to the rules Step 2: \(matrices multiplication For every transaction i in D do For j=1 to number of items do If \(D ij 0 ij 0 Else Temp items of number k kj ik 1 S  D If \(Temp 1 D ij 1 Else D ij 0  However the sanitized database produced by NHF algorithm may contain sensitive patterns That is, not all the sensitive patterns can be hidden successfully by applying NHF algorithm 3.3 HPCME Algorithm The main idea of HPCME algorithm \(Hiding sensitive Patterns Completely with Minimum side Effect on non-sensitive patterns\bine the advantages in HF and NHF. All sensitive patterns will be hidden with minimal side effect on non-sensitive patterns Restoration probability 4 Proceedings of the 28th Annual International Computer Software and Applications Conference \(COMPSACê04 0730-3157/04 $20.00 © 2004 IEEE 


Based on HF algorithm, NHF algorithm set proper entries in S to 1 to avoid canceling non-sensitive patterns accidentally. However some sensitive patterns may be hidden unsuccessfully  Therefore, a new factor restoration probability 0 p r 1\d to decide whether the value of D tj would follow the multiplication result when the multiplication result is 1 and there exist a S kj 1 1 k number of items A higher value of p r will let HPCME algorithm tend to preserve non-sensitive patterns, and vice versa Because our goal is to hide all the sensitive patterns with minimum side effect on non-sensitive patterns p r is set to a small value in HCPME algorithm Moreover, the entries in S are set according to the rules defined in NHF algorithm HPCME Algorithm Input: P h P h D, S p r Output: D Step 1 Set the values of the entries in S according to the rules Step 2: \(matrices multiplication For every transaction i in D do For j=1 to number of items do If \(D ij 0 ij 0 Else Temp  items of number k kj ik 1 S  D If \(Temp 0 D ij 0 Else if S kj 1, 1 k numebr of items D ij 1 with probability p r D ij 0 with probability 1-p r  else D ij 1  4.Performance Evaluation 4.1The Metrics for Quantifying Performance Two metrics are introduced to evaluate the effectiveness of our algorithms Error 1 some sensitive patterns can still be discovered after sanitization process. The hiding accuracy is measured by  h h P P in patterns of number ly successful hidden are which in patterns of number Accuracy A sensitive pattern p s is said to be hidden successfully if there is not exist a pattern p, such that p can be discovered from D where p is a subpattern of p s and p is not a subpattern of any non-sensitive pattern Error 2 some non-sensitive patterns are hidden after sanitization process h P  in patterns of number h P  process on sanitizati the after d disappeare are which in patterns of number Wrongness Moreover overlap rate is defined as follows for evaluating our approach            h h h h P P P P item item item rate overlap Where item\(P\enotes the set of items contained by P and |X denotes the cardinality of set X 4.2 Experiment Results The test dataset is generated by the IBM synthetic data generator The test dataset contains 200 different items, with 100K transactions. Moreover, p r is set to 0.35 in our experiments Figure 2 shows the accuracy of algorithms HF HPCME and NHF As shown in the result, HF and 5 Proceedings of the 28th Annual International Computer Software and Applications Conference \(COMPSACê04 0730-3157/04 $20.00 © 2004 IEEE 


HPCME algorithm approach at 100% accuracy no matter what the values of overlap rate. NHF works like HF and HCME when the overlap rate is low However, as overlap rate increases, its accuracy decreases           Figure 2 Effect of overlapped rate on Accuracy Figure 3 shows the wrongness of algorithms for HF HPCME and NHF The more the sensitive patterns need to be hidden, the more the entries in CM are set to 1 As a result, non-sensitive patterns are missed easily     Figure 3 Effect of overlapped rate on Wrongness 5. Conclusion and Future Work s In this paper, a new framework is presented for enhancing privacy in mining frequent patterns. By setting the entries in the sanitization matrix to appropriate values, and multiplying original DB and sanitization matrix, a sanitized database is gotten According to different settings in sanitization matrix we bring up three sanitization algorithms for hiding sensitive patterns successfully or for no legitimate pattern missing Acknowledge This work was partially supported by the National Science Council in Republic of China under the Contract No 922213E259006 Reference  Atallah, E. Bertino  A  Elmagarmid  M. Ibrahim and V. S. Verykios. çDisclosure Limitation of Sensitive Rulesé. Proceeding of IEEE Knowledge and Data Engineering Exchange Workshop, November 1999  R. Agrawal and R. Srikant Fast algorithms for mining association rules. In Proc. 1994 Int. Conf. Very Large Data Bases 1994  R.Agrawal and R.Srik ant Privacy Preser ving Data Mining é, Proceeding of ACM SIGMOD, 2000  R Agrawal, R. Srikan t A Evfim ievski and J. Gehrke Privacy Preserving of Association Rules é, Proceeding of 8th ACM SIGMOD Conference on Knowledge Discovery and Data Mining \(KDD\ 2002  A. Evf imievski, J. Gehrk e and R  Srik ant çLimiting Privacy Breaches in Privacy Preserving Data Mining Proceeding of ACM PODS, 2003  Shariq J. Ri zvi and Jayant R. Har itsa , çMaint aining Data Privacy in Association Rule Miningé, Proceedings of the 28th VLDB Conference,2002  S t anle y R  M. Oliv eir a  O s m a r R Za ian e  Priv ac y Preserving Frequent Itemset Miningé , IEEE International Conference on Data Mining Workshop on Privacy  Security and Data Mining 2002  Y  Say gin V  V e rkios and C  Clif ton, çUsing Unknown to Prevent Discovery of Association Rulesé, ACM SIGMOD Records Vol.30, no.4 2001 6 Proceedings of the 28th Annual International Computer Software and Applications Conference \(COMPSACê04 0730-3157/04 $20.00 © 2004 IEEE 


 dem, Z M., çPincer Search A new algorithm for  discovering the maximum frequent set Intl Conf. on Extending database technology 1998  cessing for Association Rule Miningé, Working Paper Series 2002-19, Indian Institute of Management, Lucknow, 2002  r Mining Maximal Frequent Sets based on Dominancy of Transactionsé, To To Appear in Proc. of Intl. Conf. on Enterprise Information Systems Angers, France, 2003  i, M.J., Gouda, K., çFast vertical mining using diffsets  RPI Technical Report, 01-1, 2001 Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


121 T. Y. Lin  The Power and Limit of Neural Networks   Proceedings of the 1996 Engineering Systems Design and Analysis Conference, Montpellier, France, July 1-4, 1996 Vol. 7, 49-53 13] T. Y. Lin and N. Cercone, Rough Sets and Data Mining Analysis of Imprecise Data, T. Y. Lm and N. Cercone eds 2nd print Foundation of Database Mining  in: the Proceedings of 9   International Conference, RSFDGrC 2003, Chongqing China, May 2003, Lecture Notes on Artificial Intelligence LNAI 2639, Springer-Verlag, 403-405 15]. Z. Pawlak, Rough sets. Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, 1991 I61 A. Barr and E.A. Feigenbaum, The handbook of Artificial Intelligence, Willam Kauhann 1981 7] T. Y. Lin  Deductive Data Mining: Mathematical 8] Tsau Young Lin  Data Mining: Granular Computing IO] T. Y. Lin  Data Mining: Granular Computing 14] T. Y. Lin  Deductive Data Mining: Mathematical 62 pre></body></html 


100 34 33 34 DSD u u u                          u             Fig.6: Over Hiding problem of setting  1 in S No matter the left-hand or right-hand equation, the support of {1, 2} in D' is 0. That is, item 1 and item 2 never appear toge ther, and they are mutual exclusive! This situation almost never happens in the normal database. The attackers may interest in this situation and infer that {1, 2} is hidden deliberately. To hide the sensitive patterns, only need to make their supports smaller than minimum support and need not to decrease their support to 0. To solve the problem, we inject a probability ? which is called Distortion probability into this approach. Distortion probability is used only when the column j of the sanitization matrix S contains only one  1  i.e. Sjj = 1 0 1 d   m k k j i k  S D  m j n i j i  d d d d   1  1     D  i j  h a s   j probability to be set to 1 and 1  j probability to be set to 0 Lemma 1: Given a minimum support ?, and a level of confidence c. Let {i, j} be a pattern in Marked-Set, nij be the support count of {i, j}. ? is the Distortion probability of column j Without loss of generality, we assume that Sij  1. If ? satisfies    D n i j  u  u  V U   a n d    


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


