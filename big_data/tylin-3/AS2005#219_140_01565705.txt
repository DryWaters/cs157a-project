Generalizing the Notion of Conìdence Michael Steinbach and Vipin Kumar Department of Computer Science and Engineering University of Minnesota 4-192 EE/CSci Building 200 Union Street SE Minneapolis MN 55455  steinbach kumar  cs.umn.edu Abstract In this paper we explore extending association analysis to non-traditional types of patterns and non-binary data by generalizing the notion of conìdence The key idea is to regard conìdence as a measure of the extent to which the strength of one association pattern provides information about the strength of another This approach provides 
a framework that encompasses the traditional concept of conìdence as a special case and can be used as the basis for designing a variety of new conìdence measures Besides discussing such conìdence measures we provide examples that illustrate the potential usefulness of a generalized notion of conìdence In particular we describe an approach to deìning conìdence for error tolerant itemsets that preserves the interpretation of conìdence as a conditional probability and derive a conìdence measure for continuous data that agrees with the standard conìdence measure when applied to binary transaction data 1 Introduction 
Traditional association analysis focuses on binary transaction data 1 such as the data that results when customers purchase items in for example a grocery store Such market basket data can be represented as a collection of transactions where each transaction represents a customer and consists of the items purchased by the customer Alternatively as is more convenient for the discussion later in this paper this data can be represented as a binary matrix where there is one row for each transaction one column for each item and the ij th entry is 1 if the i 
th customer purchased the j th item but is 0 otherwise A key task of association analysis is nding frequent itemsets  which are sets of items that frequently occur together in a transaction For example baby formula and diapers are items that may often be purchased together The strength of a frequent itemset is measured by its support  1 This includes non-ordered categorical data which can be trivially converted to binary transaction data  which is the number or fraction of transactions in 
which all items of the itemset appear together Although frequent itemsets are interesting in their own right the end goal of association analysis is typically the efìcient generation of association rules 1 where an association rule is of the form A  B  A and B itemsets and represents the statement that the items of B occur in a transaction that contains the items of A  The strength of an association rule is measured by the conìdence of the rule conf A  B  
which is the fraction of transactions containing all the items of A that also contain all the items of B  This deìnition of conìdence is an estimate of the conditional probability of A given B  Although traditional association analysis has been very successful its current foundations assume binary transaction data and thus it cannot be directly applied to many types of data sets such as those involving non-traditional association patterns or continuous attributes As an example of non-traditional patterns consider Error-Tolerant Itemsets ETIs which are itemsets in which a s peciìed fraction 
of the items can be missing from a transaction Such patterns are useful when for example real association patterns are distorted by noise To illustrate if the speciìed fraction is 0.80 then for a set of 5 items a transaction supports this itemset if it contains at least 4 out of the 5 items The traditional notion of support can be applied to ETIs in a straightforward manner i.e the support of an ETI is the number of transactions that contain at least the speciìed fraction of the items in the ETI However as described later in this paper the traditional deìnition of conìdence is not appropriate for ETIs If the data set has continuous attributes 2 
then association analysis cannot be directly applied Nonetheless if discretization techniques are emplo yed then association analysis can be used for such data sets However discretization complicates the analysis procedure and the interpretation of its results as well as potentially causing a 2 We include count attributes in this category Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


loss of information An example of an approach that directly deals with continuous data is Min-Apriori which uses the anti-monotone property of the min function to produce a new version of support that also has the antimonotone property For binary transaction data the support computed by Min-Apriori matches that of the standard approach However as described later the conìdence that Min-Apriori computes for an association rule involving binary transaction data does not always match that of the standard approach Our goal is to provide a framework for association analysis that allows it to be applied directly to both non-binary data and non-traditional types of association patterns In this paper we explore approaches for generalizing the notion of conìdence The key idea is to regard conìdence as a measure of the extent to which the strength of one association pattern provides information about the strength of another Based on this idea we propose a framework for generalizing conìdence that views conìdence as the composition of two functions 1 a function that evaluates the strength or presence of an association pattern for each object transaction producing a pattern evaluation vector and 2 a function that measures the strength of the relationship between a pair of pattern evaluation vectors The strength of the relationship may be measured by the extent to which one pattern evaluation vector predicts the other or more generally by the proximity similarity or dissimilarity of the two pattern evaluation vectors Note that the traditional deìnition of conìdence as an estimate of the conditional probability of one set of items given another is an evaluation of how well one set of items predicts another The following are speciìc contributions of this paper 1 We introduce a framework for generalizing the notion of conìdence The traditional approach to conìdence is based on support which can be regarded as a summarization of the strength of a pattern over all transactions In contrast the proposed framework is based on evaluating the relationship between the pattern evaluation vectors of two association patterns This framework provides a conceptual basis for designing new conìdence measures and offers a standard approach for describing and discussing the current notion of conìdence 2 We describe how to modify the standard deìnition of conìdence for Boolean association patterns 3 including ETIs so that conìdence can be viewed as an estimate of conditional probability We also provide an example of how applying the standard deìnition of conìdence to ETIês yields a nonsensical result while the modiìed deìnition gives the intuitively desired result  3 A Boolean association pattern is either present in a transaction or not 3 We describe approaches to conìdence based on similarity or dissimilarity functions such as the cosine measure or Euclidean distance 4 We derive a new conìdence measure for continuous data that matches the standard deìnition of conìdence for binary transaction data This conìdence measure is based on a mathematical relationship that we derive between the conìdence measure for Boolean association patterns and the cosine measure 2 Background After introducing some notation we begin by stating the traditional deìnitions of support and conìdence We then brieîy review the results of our previous work in generalizing support 2.1 Notation Table 1 provides an overview of the notation used in this and later sections Throughout this document the terms row transaction and object are used interchangeably as are the terms column item variable and attribute Table 1 Summary of Notation Notation Description D Data matrix of M rows and N columns T   t 1   t M  Set of objects transactions rows of D I   i 1   i N  Set of attributes items columns of D t An object transaction row or its index i  j  k An attribute item column or its index X  Y A set of attributes items 2.2 Traditional Support and Conìdence Using the notation of Table 1 we brieîy summarize the traditional concepts of 1 the support of an itemset 2 a frequent itemset 3 the support of an association rule 4 the conìdence of an association rule and 5 the antimonotone property of support Deìnition 1 Support For a binary transaction data matrix D with transactions T and items I  the support 4 of an itemset i.e a set of binary attributes X I  is given by   X   t T  D  t i    i  X   which is simply the count of transactions containing all the items of X  4 A distinction is sometimes made between support count and support with the latter being deìned by the ratio   X    T   For simplicity however we will use support to mean support count in this paper unless otherwise explicitly indicated 2 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


Deìnition 2 Frequent Itemset Given an itemset X I and a speciìed minimum support threshold minsup  X is a frequent itemset if   X   minsup  Deìnition 3 Support of an Association Rule The support of the association rule X  Y  where X I  Y I  and X  Y    is given by   X  Y    X  Y  Deìnition 4 Conìdence The conìdence of the association rule X  Y  where X  I  Y I  and X  Y    is given by conf X  Y    X  Y     X     X  Y    X  Deìnition 5 Anti-monotone Property of Support If X and Y X  Y  are two itemsets then   Y     X   The downward closure or anti-monotone property of standard support provides an efìcient way to nd frequent itemsets and is the foundation of the well-known Apriori algorithm If a n e w support measure also possesses the anti-monotone property then we may be able to nd its corresponding patterns efìciently 2.3 Framework for Generalizing Support AReview In we proposed a f rame w o rk for generalizing support that views support as the composition of a pattern evaluation and summarization function We summarize the three key components of this framework pattern evaluation functions summarization functions and support functions 2.3.1 Pattern Evaluation Functions The evaluation of the strength of a pattern can take various forms Most commonly and this is the case for traditional association analysis the pattern is either present i.e the pattern strength is 1 or it is absent i.e the pattern strength is 0 We call such patterns Boolean association patterns  More generally the evaluation of strength can be a real number Note that a pattern evaluation function deìnes a pattern Formally an evaluation function eval  is a function that takes a set of of attributes X I as an argument and returns a pattern evaluation vector  v  whose i th component is the strength of the pattern in the i th object Thus we can write v  t  eval  t X    t T or v  eval  X  1 If there are several sets of attributes under consideration e.g X and Y  then we will distinguish between their pattern evaluation vectors by using subscripts e.g v X and v Y  Various eval functions are shown in Table 2 Note that X   i 1 i 2   i k I  Table 2 Pattern evaluation functions eval function Deìnition 1  eval   t X  D  t i 1    D  t i k  2  eval   t X  D  t i 1    D  t i k  3 min eval min  t X in 1  j  k D  t i j   4 max eval max  t X ax 1  j  k D  t i j   5 range eval range  t X  eval max  t X   eval min  t X  6 strong ETI eval eti  t X   i  X D  t,i   X   1   Table 3 Summarization functions norm function Deìnition 1 L k  v  k  k   M t 1  v  t   k 2 L 1  v  1   M t 1  v  t   3 L 2  v  2    M t 1  v  t   2 3 L 2 2  v  2 2   M t 1  v  t   2 4 weighted sum norm w  v  w   M t 1 w  t  v  t  2.3.2 Summarization Functions The pattern evaluation vector v can be summarized by a single number e.g by using a vector norm A common vector norm is the L k norm which is deìned in Table 3 along with two of its most useful speciìc versions the L 1 and L 2 norms Another useful norm is the squared L 2 norm L 2 2  which is the sum of the squares of the components of v  To refer to such norm functions we use the notation L k or norm L k  We can also consider norm functions that are weighted sums Table 3 shows these summarization functions Note that M is the length of the vector k is a parameter  1  012  w is a vector of weights and  is the absolute value function 2.3.3 Generalized Support Functions The support of a pattern among a set of attributes X can be deìned as a function   X   that is the composition of a pattern evaluation function eval  and function norm  which summarizes these evaluations with a single number 5   X   norm  eval   X  norm  eval  X  2 Given a support function the goal is to use it to nd sets of attributes that meet some support criterion For instance if the support function has the anti-monotone property then by setting a minimum support threshold minsup and using an algorithm such as Apriori we can nd a collection of strong pattern sets  6 i.e a collection of sets of attributes that have support greater than minsup  We adopt the following notation to refer to the different types of support functions that we have created  eval norm  norm  eval 3 5 More formally     I   where  is the set of real numbers and   I  is the set of all subsets of I  6 The name frequent itemset is not appropriate in the general case 3 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


For example the support function that is based on the eval min and norm L 2 2 functions is written as follows  min  L 2 2  norm L 2 2  eval min   eval min  2 2 4 2.3.4 Example Boolean Support Functions A Boolean support function  b L 1  is any support function that uses a Boolean pattern evaluation function eval b and the L 1 norm A Boolean pattern evaluation function returns either 0 or 1 An example is the traditional support of an itemset X  which can be expressed as    L 1 and which is equivalent to counting the number of transactions for which a conjunction of of the items binary attributes in X is true This approach can be generalizedÑsee for example 5 more general Boolean formulas that use the logical connectives   and  015  or  and   not  Also a Boolean pattern evaluation function need not represent a Boolean formula and the data need not be binary Table 4 shows the computation of support for the Boolean support function    L 1  Table 4 Computation of support for    L 1   transaction/attrbute i 1 i 2 i 1  i 2 t 1 1 0 1 t 1 0 0 0 t 1 1 1 1 t 1 0 1 1 Support 2 2 3 2.3.5 Example Support for Continuous Data An example of support for continuous data is provided by the Min-Apriori algorithm which corresponds to using the support function  min  L 1  To adjust for differences in the scales of attributes Min-Apriori rst normalizes the data in each column by dividing each column entry by the sum of the column entries Then the support of a set of attributes is computed by taking the minimum value in each row and summing the resultant values The Min-Apriori process of computing the support of already normalized data is illustrated in Table 5 Table 5 Computation of support for two normalized attributes using Min-Apriori transaction/attrbute i 1 i 2 min i 1 i 2  t 1 0.45 0.55 0.45 t 1 0.25 0.05 0.05 t 1 0.30 0.25 0.25 t 1 0.00 0.15 0.00 Support 1.00 1.00 0.75 3 A Framework for Conìdence We rst consider the traditional approach to conìdence which is based on support and some of its limitations We then present a general approach to deìning conìdence which is based on the pattern evaluation vectors of the two association patterns and provide some speciìc examples In the remainder of this section we present a couple of examples to show how a generalized approach to support can be used to 1 deìne support for Boolean association patterns including ETIs and 2 deìne a conìdence measure for continuous data that agrees with traditional conìdence for binary transaction data 3.1 Traditional Approach to Conìdence For binary transaction data the traditional approach to conìdence for two itemsets X and Y is based on support i.e conf X  Y    X  Y    X   Thus it seems reasonable to attempt to deìne conìdence for Boolean association patterns or continuous data in the same way realizing of course that the support functions in these cases may be different than standard support Indeed the Min-Apriori algorithm deìnes support in just this way However as indicated by the following two examples this straightforward approach does not yield the desired results Example Strong Error-Tolerant Itemsets Here we attempt to use traditional conìdence for a binary association pattern known as a strong ETI Deìnition 6 provides a formal deìnition of a strong ETI F o r bre vity  w e w ill use the term ETI instead of strong ETI Deìnition 6 Strong Error Tolerant Itemset A strong ETI consists of a set of items X I  such that there exists a subset of transactions R T consisting of at least   M transactions and for each t  R  the fraction of items in X which are present in t is at least 1     is the minimum support expressed as fraction M is the number of transactions and  is the fraction of missing items For this example we will use the data shown in Table 6 Also we require that each transaction contains at least 3/8 of the speciìed items   5  8  and that half the transactions support the pattern   0  5  If X   i 1 i 2 i 3 i 4  and Y   i 5 i 6 i 7 i 8   then both X and Y are ETIs with a support of 4 transactions Table 6 Sample data set to illustrate conìdence for ETIs i 1 i 2 i 3 i 4 i 5 i 6 i 7 i 8 t 1 0 1 1 1 0 0 0 0 t 2 1 0 1 1 0 0 0 0 t 3 1 1 0 1 0 0 0 0 t 4 1 1 1 0 0 0 0 0 t 5 0 0 0 0 0 1 1 1 t 6 0 0 0 0 1 0 1 1 t 7 0 0 0 0 1 1 0 1 t 8 0 0 0 0 1 1 1 1 4 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


Computing conf  X  Y  using the traditional deìnition of support yields conf X  Y    X  Y     X   8 4 2  This seems quite odd because 1 the conìdence is larger than 1 and 2 the ETI pattern in X never co-occurs with the ETI pattern in Y  Thus for ETIs the traditional notion of conìdence does not seem appropriate Later we will indicate how to deìne conìdence for ETIs and other Boolean association patterns Example Conìdence for Continuous Data Using MinApriori Although Min-Apriori also uses the traditional support-based deìnition of conìdence it rst normalizes the data and uses the support function  min  L 1  However if Min-Apriori is given binary transaction data it does not always yield the same conìdence values as the standard definition of conìdence We illustrate this with the data shown in Table 7 The original data is shown in Table 7\(a Table 7\(b shows the data after it has been normalized and Table 7\(c shows the evaluation vectors Using traditional conìdence we obtain conf CD  AB    ABCD    CD   4=0  5  However using  min  L 1 as our support function we get conf  CD  AB   min  L 1  ABCD   min  L 1  CD  2  6  4  5 5  12  0  42  a Sample data ABCD 1111 1111 1011 1011 1100 1010 b Normalized data ABCD 1/6 1/3 1/5 1/4 1/6 1/3 1/5 1/4 1/6 0 1/5 1/4 1/6 0 1/5 1/4 1/6 1/3 0 0 1/6 0 1/5 0 c Eval vectors AB CD ABCD 1/6 1/5 1/6 1/6 1/5 1/6 0 1/5 0 0 1/5 0 1/6 0 0 00 0 Table 7 Data for conìdence calculations of Min-Apriori 3.2 A More General Approach A support based approach to conìdence is based on a summarization of the evaluation vectors of the patterns A more general approach is to deìne conìdence based directly on the pattern evaluation vectors themselves We present the general framework and some speciìc examples 3.2.1 A General Deìnition of Conìdence Most generally we view conìdence as a measure of the information that one association pattern provides about another Speciìcally we view conìdence as a function that quantiìes the relationship between the evaluation vectors of a pair of association patterns This is captured by the following deìnition Deìnition 7 A General Deìnition of Conìdence If X I and Y I  X  Y    are sets of attributes then conìdence can be deìned as a function conf X Y onf eval  X  eval  Y   conf v X  v Y  5 that maps the evaluation vectors of the two association patterns to a real value 7 Of course we are not interested in just any function but rather those functions that 1 capture the extent to which the strength of one association pattern can be used to predict another or 2 capture the proximity similarity or dissimilarity between the two association patterns The next section illustrates the proximity-based approach 3.2.2 Proximity-Based Approach When the pattern evaluation vectors v X and v Y are continuous it seems reasonable to consider as conìdence measures some of the similarity or distance measures that have been developed for evaluating the strength of a connection between two continuous vectors We introduce four potential measures Note that the eval and norm functions can be those listed in Tables 2 and 3 respectively as well as other functions not listed in those tables Also note that unlike traditional conìdence these deìnitions of conìdence are symmetric with respect to the order of X and Y  Deìnition 8 Distance-based Conìdence of an Association Rule conf X  Y  norm  eval  X   eval  Y  6 Deìnition 9 Correlation-based Conìdence of an Association Rule conf X  Y  corr  eval  X  eval  Y  7 Deìnition 10 Cosine-based Conìdence of an Association Rule conf X  Y os eval  X  eval  Y  8 Deìnition 11 Bregman Conìdence of an Association Rule conf X  Y  D   eval  X  eval  Y  9 where D  represents a class of functions known as Bregman divergences whic h i ncludes pr oximity measur es suc h a s squared Euclidean distance and the Mahalanobis distance 7 More formally conf    M   M   where  is the set of real numbers and M is the number of objects 5 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


3.3 Conìdence for Boolean Evaluation Functions In this section we use the framework described above to generalize the interpretation of conìdence as conditional probability to the case of general Boolean support functions We show that this deìnition also works for ETIs Given two attributes X and Y  and a Boolean pattern evaluation function what we are seeking is a measure of the strength of the connection between the pattern evaluation vectors of X and Y  For Boolean pattern evaluation functions the resulting pattern evaluation vectors v X  eval b  X  and v Y  eval b  Y  are binary vectors and this simpliìes the task of selecting a measure with which to evaluate the strength of the connection since corresponding entries of v X and v Y either match or they donêt A further simpliìcation arises if we impose the additional assumption that only the presence of a pattern a value of 1 is important This reduces the problem of deìning conìdence for a general Boolean support function to be one of predicting whether an entry of v Y is 1 when the corresponding entry of v X is 1 Conceptually this is Prob  v Y  t   v X  t  whose empirical approximation is the fraction of entries where v X and v Y are both 1 i.e Prob  v Y  t   v X  t   divided by the fraction of entries where v X  t  is 1 i.e Prob  v X  t   Given this discussion we can provide the following general deìnitions of the support and conìdence of an association rule when using a Boolean support function Deìnition 12 Support of an Association Rule for Boolean Support Functions Given two disjoint sets of attributes X and Y  and a Boolean support function  b L 1  based on a Boolean pattern evaluation function eval b and the L 1 norm the support of X  Y is given by  b L 1  X  Y   t T  v X  t   v Y  t    Deìnition 13 Conìdence of an Association Rule for Boolean Support Functions Given two disjoint sets of attributes X and Y  and a Boolean support function  b L 1  based on a Boolean pattern evaluation function eval b and the L 1 norm the conìdence of X  Y is given by conf  X  Y  Prob  Y  X   Prob  X  Y   Prob  X    t T  v X  t   v Y  t    t T  v X  t     b L 1  X  Y   b L 1  X  where we have used Deìnition 12 and the fact that  t  T  v X  t     b L 1  X   Thus a view of conìdence as conditional probability is appropriate even in the case of a general Boolean support function However this does not mean that we can use the standard approach for computing conìdence in the general case In particular if we followed the approach in standard association analysis we would use  b L 1  X  Y   t T  v X  Y  t     b L 1  X  Y  instead of  b L 1  X  Y   t T  v X  t   v Y  t    as given in Deìnition 12 The equivalence of these two approaches is a special case and does not hold in general Example Strong Error-Tolerant Itemsets Continued To compute conf  X  Y   as given in Deìnition 13 we need to compute the support of X  Y using Deìnition 12 In turn this requires us to and the pattern evaluation vectors of X and Y and then sum the resultant vector The ETI pattern of X occurs only in the rst four transactions while that of Y occurs only in the last four transaction If  denotes the componentwise and function then            1 1 1 1 0 0 0 0           012             0 0 0 0 1 1 1 1           012             0 0 0 0 0 0 0 0           012  and consequently  eti L 1  X  Y   Therefore conf  X  Y   eti L 1  X  Y   eti L 1  X  0  4=0  which is much more intuitive than our previous result To summarize an approach that tries to measure the strength of the connection between the pattern evaluation vectors of two sets of attributes leads naturally to a deìnition of conìdence as a conditional probability for Boolean support functions with the assumption that only 1ês count Furthermore this approach yields a deìnition of support that is applicable to all Boolean support functions and is consistent with conìdence as traditionally deìned 3.4 A Conìdence Measure for Continuous Data that Agrees With the Standard Deìnition of Conìdence Here we derive a conìdence measure for continuous data that agrees with traditional conìdence when used on binary transaction data We begin by proving Theorem 3.1 which indicates that the conìdence of the association rule X  Y for Boolean support functions equals the cosine measure between the evaluation vectors of X and Y multiplied by a factor that depends on the relative support of the two itemsets This theorem serves as the basis for our approach as well as being interesting in its own right Note that if both association patterns have the same support then the conìdence simply reduces to the cosine of their pattern evaluation vectors i.e cos eval b  X  eval b  Y  Inthis case conìdence is also symmetric i.e conf X  Y  conf Y  X   6 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


Theorem 3.1 Given a Boolean support function  b L 1  conf X  Y os eval b  X  eval b  Y     b L 1  Y   b L 1  X  10 Proof conf X  Y   b L 1  X  Y   b L 1  X   eval b  X   eval b  Y   eval b  X   2 2  cos eval b  X  eval b  Y   eval b  Y   2  eval b  X   2  cos eval b  X  eval b  Y    b L 1  Y   b L 1  X  where eval b  X   eval b  Y  is the dot product of evaluation vectors of X and Y  respectively We have used the the deìnition of the cosine measure cos x  y  x  y   x  2  y  2  as well as the following pair of facts  b L 1  X  Y  eval b  X   eval b  Y  and  b L 1  X   eval b  X   1   eval b  X   2 2  A useful property of the cosine measure is that if two vectors are multiplied by potentially different non-zero constants their cosine measure is unchanged As a result if we normalize each attribute of a binary transaction matrix to have an L 1 norm of 1 then the cosine measure between attributes does not change Thus if we can express   b L 1  Y   b L 1  X  in terms of support based on the Min-Apriori support function  min  L 1  then we will have an alternative deìnition of conìdence that matches the standard deìnition of conìdence for binary transaction data and works for continuous data Theorem 3.2 For traditional binary transaction data whose attributes have been normalized to have an L 1 norm of 1 and an itemset X   i 1 i k  of size k   min  L 1  X  c    X   where c is the mean of the nonzero entries of eval min  X  and   X  is the traditional support of the attributes X for the original unnormalized data Proof First notice that when attribute i j is normalized its non-zero entries become 1   i j   Thus all the non-zero entries of eval min  X  have the value min\(1   i j  Trivially the average of the non-zero values of eval min  X  will be c min\(1   i j   Furthermore eval min  X  will have   X  such entries one for each transaction where all the attributes of X are all non-zero Thus  min  L 1  X  c  X   and the theorem follows Using the previous two results We get a new deìnition of conìdence for continuous data that is normalized to have an an L 1 norm of 1 Deìnition 14 New Deìnition of Conìdence for Continuous Data conf X  Y os eval min  X  eval min  Y    min  L 1  Y  c Y  min L 1  X  c X  where c X  c Y  is the mean of the non-zero values of eval min  X   eval min  Y   We emphasize that this is a mathematical result and we are not claiming that Min-Apriori with this conìdence measure should be preferred to the original Min-Apriori Indeed if cosine similarity were used as a measure of conìdence then it would work for both binary transaction data with traditional support and for continuous data using the Min-Apriori approach Regardless we feel that the ability to derive results such as this indicates the potential value of our framework for generalizing conìdence We provide a numerical illustration of the result that we have just derived using the data in Table 7 Recall that conf CD  AB   5 when we use traditional conìdence Using Table 7\(c we will apply Deìnition 14 with X   C D   and Y   A B   Summing column AB of Table 7\(c we get  min  L 1  AB   6  Summing column CD of Table 7\(c we get  min  L 1  CD   5 From Table 7\(c we see c AB 1  5 and c CD 1  6  Finally cos eval min  CD  eval min  AB   1   3  Thus the conìdence given by Deìnition 14 is conf CD  AB  1  3  3  6  1  6 4  5  1  5  1  3 015 3 4  015 1 4 0  5  4 Related Work While other work in association analysis does not provide a general framework for extending the notion of conìdence some work has considered other measures for evaluating association rules Most notably correlation has been proposed as a substitute for traditional conìdence More common is work that focuses on recognizing new patterns that is proposing new evaluation functions One example is error tolerant itemsets ETIs which were used in this paper As another example the approach to generalized itemsets presented in seeks to nd both sets of attributes and attribute values that are more probable than usual i.e that represent a bump in the probability density function describing the data As a nal e xample some recent work uses a Boolean retrieval model to generalize the notion of an itemset 5 In this approach the patterns of interest are any Boolean formulae that involve binary variables and the logical connectives   and  015  or  and   not  7 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


Quantitative association rules 3 13 15 try to predict the values of one set of attributes from another There has also been some work in integrating regression with association analysis Ho we v e r  both these approaches are distinct from our view of association rules and conìdence which involves only the prediction of the strength/presence of one pattern from another Our work is also different from that of constraint-based association analysis as typiìed by or 16 Although our pattern evaluation functions may seem similar to the constraints used in those approaches our pattern evaluation functions can be non-Boolean functions that return continuous values Conceptually this is a shift from evaluating whether a patterns occurs in an object to evaluating the strength of a pattern in a particular object Also most constraint-based association analysis focuses on added efìciency while our focus is on new types of data and patterns 5 Acknowledgments This work was partially supported by NASA grant NCC 2 1231 NSF grant ACI-0325949 and by the Army High Performance Computing Research Center under the auspices of the Department of the Army ARL cooperative agreement number DAAD19-01-2-0014 The content of this work does not necessarily reîect the position or policy of the government and no ofìcial endorsement should be inferred Access to computing facilities was provided by the AHPCRC and the Minnesota Supercomputing Institute 6 Conclusions and Future Work We have described a framework for generalizing the notion of conìdence and have shown that this framework can be used to express conìdence for general Boolean patterns including error tolerant itemsets We have also shown that this framework can be useful for continuous data by presenting an example of the use of a generalized notion of conìdence to derive a conìdence measure for continuous data that agrees with traditional conìdence for binary transaction data There is a need for efìcient algorithms for nding highconìdence association rules for non-traditional patterns and non-binary data sets Also experiments are needed to evaluate the usefulness of such association rules This includes a comparison to results obtained from converting the data into binary transaction data Areas for investigation in the theoretical area include exploring the appropriate deìnitions of conìdence to accompany various support functions that have been deìned It would also be worthwhile to explore using interestingness measures of association patterns  for the functions used to combine the pattern e v a luation vectors References  R Agra w al T  Imielinski and A N Sw ami Mining association rules between sets of items in large databases In SIGMOD 93  pages 207Ö216 Washington D.C May 1993  R Agra w a l and R Srikant F ast algorithms for mining association rules In VLDB 94  pages 487Ö499 1994  Y  Aumann and Y  Lindell A statistical theory for quantitative association rules In KDD99  pages 261Ö270 1999  A Banerjee S Merugu I S Dhillon and J Ghosh Clustering with bregman divergences In SIAM 2004  pages 234 245 Lake Buena Vista FL April 2004  P  Bollmann-Sdorra A Hafez and V  V  Ragha v a n A theoretical framework for association mining based on the boolean retrieval model In DaWaK 2001 Munich Germany  pages 21Ö30 2001  S Brin R Motw ani and C Silv erstein Be yond mark et baskets generalizing association rules to correlations In SIGMOD 97  pages 265Ö276 1997  J W  Demmel Applied Numerical Linear Algebra SIAM January 1997  J Friedman and N Fisher  B ump-hunting in highdimensional data Statistics and Computing  9\(2 April 1999  E.-H Han G Karypis and V  K umar  T r 97-068 Minapriori An algorithm for nding association rules in data with continuous attributes Technical report Department of Computer Science University of Minnesota Minneapolis MN 1997  T  Hastie R T ibshirani and J Friedman The Elements of Statistical Learning  Springer Verlag August 2001  S Jarose wicz D A Simo vici and I Rosenber g An inclusion-exclusion result for boolean polynomials and its applications in data mining In Proceedings of the Workshop on Discrete Mathematics and Data Mining DM&DM SDM02 Arlington VA  AAAI April 2002  R T  Ng L V  S Lakshmanan J Han and A P ang Exploratory mining and pruning optimizations of constrained associations rules In SIGMOD 98  1998  M Ok onie wski L Gancarz and P  Ga wrysiak Mining multi-dimensional quantitative associations In INAP 2001 October  volume 2543 of LCNS  Springer 2003  A Ozgur  P N T a n and V  K umar  rba An inte grated framework for regression based on association rules In SIAM 2004  April 22-24 2004  R Srikant and R Agra w a l Mining quantitati v e association rules in large relational tables In SIGMOD 96  1996  R Srikant Q V u  a nd R Agra w a l Mining association rules with item constraints In KDD 97  pages 67Ö73 1997  M Steinbach P N T a n H Xiong and V  K umar  Gener alizing the notion of support In KDD 04  pages 689Ö694 New York NY USA 2004 ACM Press  P N T an V  K umar  and J Sri v asta v a  Selecting the right interestingness measure for association patterns In KDD 2002  pages 32Ö41 New York NY 2002 ACM Press  P N T an M Steinbach and V  K umar  Introduction to Data Mining  Pearson Addison-Wesley 2005  G I W e bb  D isco v e ring associations with numeric v ariables In KDD 01  pages 383Ö388 2001  C Y a ng U M F a yyad and P  S Bradle y  Ef cient disco v e ry of error-tolerant frequent itemsets in high dimensions In KDD 2001  pages 194Ö203 2001  M J Zaki and M Ogihara Theoretical foundations of association rules In DMKD 98  pages 7:1Ö7:8 1998 8 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


8] M. Gahegan. Data mining and knowledge discovery in the geographical domain 9] J. R. Holton, J. A. Curry, and J. A. Pyle. Encyclopedia of atmospheric sciences. Academic Press, Boston, 2002 10] Z. Kou, W. W. Cohen, and R. F. Murphy. Extracting information from text and images for location proteomics. In Proceedings of the 3nd ACM SIGKDD Workshop on Data Mining in Bioinformatics, Washington, DC, 2003 11] V. Kumar, M. Steinbach, P. Tan, S. Klooster, C. Potter, and A. Torregrosa. Mining scienti?c data: Discovery of patterns in the global climate system. In Proceedings of the Joint Statistical Meetings, Athens, GA, 2001 12] P. Lynch. Weather forecasting from woolly art to solid science. Meteorology at the Millennium, pages 106ñ119, 2002 13] T. N. Palmer. Predicting uncertainty in numerical weather forecasts. Meteorology at the Millennium, pages 3ñ13, 2002 14] R. Srikant and R. Agrawal. Mining quantitative association rules in large relational tables. In Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data, pages 1ñ12, Montreal, Quebec, Canada, 1996 15] A. J. Stevermer. Recent advances and issues in meteorology Oryx Press, London, 2002 16] B. Zupan, E. T. Keravnou, and N. Lavrac. Intelligent Data Analysis in Medicine and Pharmacology. Kluwer Academic Publishers, 1997 Proceedings of the 28th Annual International Computer Software and Applications Conference \(COMPSACí04 0730-3157/04 $20.00 © 2004 IEEE 


variable names mapped by the where keyword to the relation in DataSchema and pv are variable names that appear in the StructureSchema At instantiation time pv is assigned values of the Structure component and dv is mapped to the relation appearing in Data component. The de?nition for the pattern well-formed formula is now straightforward De?nition 14 A pattern formula is of the form fp\(dv 2 where fp \(formula predicate variablesmapped by thewhere keyword to the relation appearing in Data component From the previous de?nitions the semantics of the where keyword become evident: we impose that the variables of the formula will take values from speci?c relations when the formula predicate is employed in queries Example 2 Let us consider the following formulas 1. f\(x x 2. f\(g\(x x In the ?rst formula variable x is mapped to R using the where keyword, thus the formula is well formed. Keep in mind that the formula predicate by itself is just the part f\(x is not well-formed since y is not mapped via where to any relation, or otherwise range restricted 5. Querying the Pattern Warehouse We de?ne queries to be posed over the pattern warehouse and not individually over its data- or patternbase components. Through this approach, we are able to sustain queries traversing from the pattern to the data space and vice-versa. At the same time, the consistency of the results is guaranteed by the pattern-data mapping De?nition 15 Let PW the set of all possible Pattern Warehouses. A query is a function with signature PW ? PW. Given a query q and a pattern warehouse pw = \(DB,PB D?B, P?B q\(pw DB?, PB   P?B? = ?[D?1, ..., D?m], [P?C1:PT1]?. We assume that tr, tp\(tr ? R1 ? tp ? PC1 tr, tp Note that, similarly to the relational case, the result of a query is always a pattern warehouse containing just one relation and one pattern class. It is also important to point out that, in practice, even if a query always involve both the data and pattern space, operations over patterns are executed in isolation, locally at the PBMS. The reference to the underlying data is activated only on-demand \(whenever the user speci?cally requests so stored intermediate mappings or the formula approximation 5.1. Query operators In this section we introduce query operators that allow basic queries over the the PW . Assuming that DB denotes the set of all possible database instances and PB the set of all possible pattern bases, we consider the following groups of operators  Database operators: they can be applied locally to the DBMS. op : DB ? DB. We denote the set of database operators with OD  Pattern base operators: they can be applied locally to the PBMS. op : PB ? PB. We denote the set of database operators with OP  Cross-over database operators: they involve evaluation on both the DBMS and the PBMS, the result is a database. op : DB  PB ? DB. We denote the set of database operators with OCD  Cross-over pattern base operators: they involve evaluation on both the DBMS and the PBMS, the 


evaluation on both the DBMS and the PBMS, the result is a pattern base. op : DB  PB ? PB. We denote the set of database operators with OCP In the following, we present examples of the last three classes of operators \(database operators coincide with usual relational operators operators, we introduce some examples of predicates de?ned over patterns Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE 5.1.1. Pattern predicates We identify two main classes of atomic predicates: predicates over patterns and predicates over pattern components. From those atomic predicates we can then construct complex predicates. In the following, we denote pattern components by using the dot notation. For example, the measure component of a pattern p is denoted by p.Measure Predicates over pattern components. They check properties of speci?c pattern components. Let p1 and p2 be two patterns, possibly selected by some queries. The general form of a predicate over pattern components is t1?t2, where t1 and t2 are path expressions that must de?ne components of patterns p1 and p2, of compatible type and ? must be an operator, de?ned for the type of t1 and t2. For example, if t1 and t2 are integer expressions, then ? can be a disequality operator e.g. one of &lt;,&gt cases  If t1 and t2 are pattern data for patterns p1 and p2, then ? ? {=,?}. t1 = t2 is true if and only if x x ?? p1 ? x ?? p2 and t1 ? t2 is true if and only if ?x x ?? p1 ? x ?? p2  If t1 and t2 are pattern formulas for patterns p1 a n d  p 2   t h e n             t 1    t 2  i s  t r u e  i f  a n d o n l y  i f  t 1    t 2  a n d  t 1    t 2  i s  t r u e  i f  a n d  o n l y  i f  t 1 logically implies t2 Predicates over patterns. We consider the following set of predicates  Identity if they have the same PID, i.e. p1.P ID = p2.P ID  Shallow equality \(=s are shallow equal if their corresponding components \(except for the PID component i.e. p1.Structure = p2.Structure, p1.Source p2.Source, p1.Measure = p2.Measure, and p1.formula = p2.formula. Note that, to check the equality for each component pair, the basic equality operator for the speci?c component type is used  Deep equality \(=d deep equal if their corresponding data are identical, i.e., ?x x ?? p1 ? x ?? p2   S u b s u m p t i o n       A  p a t t e r n  p 1  s u b s u m e s  a  p a t t e r n  p 2   p 1    p 2   i f  t h e y  h a v e  t h e  s a m e  s t r u c ture but p2 represents a smaller set of raw data i.e. p1.Structure = p2.Structure, p1.Source p 2  S o u r c e  a n d  p 1  f o r m u l a    p 2  f o r m u l a  Complex predicates. They are de?ned by applying usual logical connectives to atomic predicates. Thus, if F1 and F2 are predicates, then F1 ? F2,F1 ? F2  F1 are predicates. We make a closed world assumption, thus the calculation of  F is always ?nite 5.1.2. Pattern base operators OP In this subsection, we introduce several operators de?ned over patterns. Some of them, like set-based operators, renaming and selection are quite close to their relational counterparts; nevertheless, some others, like join and projection have signi?cant di?erences Set-based operators. Since classes are sets, usual operators such as union, di?erence and intersection are de 


tors such as union, di?erence and intersection are de?ned for pairs of classes of the same pattern type Renaming. Similarly to the relational context, we consider a renaming operator ? that takes a class and a renaming function and changes the names of the pattern attributes according to the speci?ed function Projection. The projection operator allows one to reduce the structure and the measures of the input patterns by projecting out some components. The new expression is obtained by projecting the formula de?ning the expression over the remaining attributes [12 Note that no projection is de?ned over the data source since in this case the structure and the measures would have to be recomputed Let c be a class of pattern type pt. Let ls be a non empty list of attributes appearing in pt.Structure and lm a list of attributes appearing in pt.Measure. Then the projection operator is de?ned as follows ls,lm c id s m f p ? c, p = \(pid, s, d,m, f In the previous de?nition, id ing new pids for patterns, ?mlm\(m projection of the measure component and ?sls\(s ned as follows: \(i s usual relational projection; \(ii sls\(s and removing the rest from set elements. The last component ?ls?lm\(f computed in certain cases, when the theory over which the formula is constructed admits projection. This happens for example for the polynomial constraint theory 12 Selection. The selection operator allows one to select the patterns belonging to one class that satisfy a certain predicate, involving any possible pattern component, chosen among the ones presented in Section 5.1.1 Let c be a class of pattern type pt. Let pr be a predicate. Then, the selection operator is de?ned as follows pr\(c p Join. The join operation provides a way to combine patterns belonging to two di?erent classes according to a join predicate and a composition function speci?ed by the user Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE Let c1 and c2 be two classes over two pattern types pt1 and pt2. A join predicate F is any predicate de?ned over a component of patterns in c1 and a component of patterns in c2. A composition function c pattern types pt1 and pt2 is a 4-tuple of functions c cStructureSchema, cDataSchema, cMeasureSchema, cFormula one for each pattern component. For example, function cStructureSchema takes as input two structure values of the right type and returns a new structure value, for a possible new pattern type, generated by the join. Functions for the other pattern components are similarly de?ned. Given two patterns p1 = \(pid1, s1, d1,m1, f1 p2 = \(pid2, s2, d2,m2, f2 p1, p2 ned as the pattern p with the following components Structure : cStructureSchema\(s1, s2 Data : cDataSchema\(d1, d2 Measure : cMeasureSchema\(m1,m2 Formula : cformula\(f1, f2 The join of c1 and c2 with respect to the join predicate F and the composition function c, denoted by c 1   F  c  c 2   i s  n o w  d e  n e d  a s  f o l l o w s    F  c  c 2     c  p 1   p 2   p 1    c 1  p 2    c 2  F   p 1   p 2     t r u e   5.1.3. Cross-over database operators OCD Drill-Through. The drill-through operator allows one to 


Drill-Through. The drill-through operator allows one to navigate from the pattern layer to the raw data layer Thus it takes as input a pattern class and it returns a raw data set. More formally, let c be a class of pattern type pt and let d be an instance of the data schema ds of pt. Then, the drill-through operator is denoted by c c Data-covering. Given a pattern p and a dataset D sometimes it is important to determine whether the pattern represents it or not. In other words, we wish to determine the subset S of D represented by p \(p can also be selected by some query the formula as a query on the dataset. Let p be a pattern, possibly selected by using query language operators, and D a dataset with schema \(a1, ..., an ible with the source schema of p. The data-covering operator, denoted by ?d\(p,D responding to all tuples in D represented by p. More formally d\(p,D t.a1, ..., t.an In the previous expression, t.ai denotes a speci?c component of tuple t belonging to D and p.formula\(t.a1, ..., t.an instantiated by replacing each variable corresponding to a pattern data component with values of the considered tuple t Note that, since the drill-though operator uses the intermediate mapping and the data covering operator uses the formula, the covering ?\(p,D D = ?\(p not be equal to D. This is due to the approximating nature of the pattern formula 5.1.4. Cross-over pattern base operators OCP Pattern-covering. Sometimes it can be useful to have an operator that, given a class of patterns and a dataset, returns all patterns in the class representing that dataset \(a sort of inverse data-covering operation Let c be a pattern class and D a dataset with schema a1, ..., an pattern type. The pattern-covering operator, denoted as ?p\(c,D all patterns in c representing D. More formally p\(c,D t.a1, ..., t.an true Note that: ?p\(c,D p,D 6. Related Work Although signi?cant e?ort has been invested in extending database models to deal with patterns, no coherent approach has been proposed and convincingly implemented for a generic model There exist several standardization e?orts for modeling patterns, like the Predictive Model Markup Language \(PMML  eling approach, the ISO SQL/MM standard [2], which is SQL-based, and the Common Warehouse Model CWM  ing e?ort. Also, the Java Data Mining API \(JDMAPI 3] addresses the need for a language-based management of patterns. Although these approaches try to represent a wide range of data mining result, the theoretical background of these frameworks is not clear. Most importantly, though, they do not provide a generic model capable of handling arbitrary cases of pattern types; on the contrary only a given list of prede?ned pattern types is supported To our knowledge, research has not dealt with the issue of pattern management per se, but, at best, with peripheral proximate problems. For example, the paper by Ganti et. al. [9] deals with the measurement 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


