A Memory-based Learning Approach to Reduce False Alarms in Intrusion Detection Ill-Young Weon+,Doo Heon Song*,Chang-Hoon Lee\222,Young-Jun Heo*,Ki-Young Kim 221Dept of Computer engineering Konkuk Wniversity 93-I,Mojin-dong Kwangiin-gu Seoul clcc chlee konkuk.ac kr 223Dept of Computer  Information Yong-in SongDam Colleage 577-1 Mapyong Dong Young-in Kpngki dsong@ysc.ac kr 223Network Security Department Electronic and Telecommunication Research Institute 161 kajong-dong Yusong-Gu Taejon 305-350 yjheo, kykim etri re kr 
Abstraci  Signature-based IDS is known io have acceptable accuracy but suffers from high rates of false alarms We show a behavior based alarm reducfion by using a memopbased machine learning iechniquc  insiance based learner. Our mended form of IBL WBL mamines SNORTalarm signals iftirat signal is worthy sending signals to secum manager A preliminav experiment shows that these exists an apparent difference between hue alarms and false alarms wiih respect to XIBL behavior and the full uperirnenf successfully ahibiis thepower of hybrid sysfem is there is a rich set of analyzed daia such 
as DARPA 1998 dafa set we used Keywords  instancebased learning false alnrm machine learning network security 1 Introduction AIthough it is valuable to use intrusion detection systems in order to protect the resource of computer systems and networks the security managers have another practical problem  how to manage thousands of alarms generated by the IDS per day A striking consumer report says 70 of IDS purchasers in Korea, mostly financial enterprises and public administration offices, want to purchase another IDS that has less false alarms with similar accuracy And in fact many security 
managers ignore alarms until other supporting evidences are found lo Another problem is that most commercial IDSs are signature-based which identify an attack by matching input with the rules or patterns generated from known attacks. While the accuracy \(identify attack if it truly is of signature-based IDS is more acceptable than that of anomaly-based IDS which labels any input as 223malicious\224 if it is far from 224normal\224 behavior it is known that signature-based systems tend to produce more false positives than anomaly based systems I41[51 The situation could be worse in that packets can be crafted to match attack signatures on a target signature-based IDS due to 
the vulnerability of signature-based systems 7 Two types of partial solutions have been proposed to reduce false alarms recently A knowledge-based pre-filter with data mining techniques that handles most frequent alarms significantly reduces the security managers\222  However this approach tends to ignore infrequent abnormal signals and related otherwise valuable information so that reducing the amount of alarms may cost the accuracy Another type of study reducing false alarms is to use association rules after an alarm is triggered and investigate if that is worthy sending signals to the security This behavior based approach. seems to be safer in that the signature-based IDS did its role 
with full information available to trigger an alarm and the data mining technique if appropriately chosen validates it However this post-hoc analysis assumes that the front IDS\(signature-based\has no false negative in that it can only find that an alarm signal is in fact 223normal\224 by association rules In other words, there is practically no chance to beat the accuracy of front IDS to their hybrid system since it can only response when the front IDS triggers an alarm We propose here a behavior-based approach that could reduce false alarms and also compensate the accuracy of front IDS with 
a memory-based supervised learning algorithm Our system is originally intended to effectively combine two heterogeneous IDSs developed separately  a system based on SNORT[XJ by Electronics and Telecommunication Research Institute\(ETR1\in Korea and another anomaly based engine developed by co-authors of this paper affiliated with universities  1 I We use a variant of instance based Iearning IBL algorithm I as a behavior investigator of SNORT not only if a SNORT alarm is truly worthy informing to the security manager but also generate an alarm with IBL\222s o\\\\n evaluation although SNORT 
has no action IBL stores a subset of real instances that are recognized as 223abnormal\224 or 223normal\224 from the training examples Thus the knowledge it has is the set 0f\224important\224 instances When a set of events occur in a network IBL algorithm searches the most similar instance from its knowledge base to current event and decides whether it is normal or not by following the label of the most similar knowledge For our case, the instance is a packet represented by 16 real and nominal attributes IO 241  


However IBL is noise-sensitive, real-valued attribute based learning engine as it originally was we extend it to support symbolic attnbute 9 and give leave-one-out analysis to exclude noise instances and attribute weighting procedure appears to be better than original IB4 I Hence we will call this engine as XIBL extended IBL\in this paper The remainder of this paper is organized as follows In Section 2 we give a short explanation of XIBL algorithm and its behavior over SNORT alarm in training set which gives a rationale of combining two systems. Then the result of full experiment on DARPA 1998 data will be reported in section 3 and we will conclude this laboratory scale research at the end 2 Behavior of IBL on SNORT aIarm 2.1 XIBL IBL learns by storing examples as points in a feature space and it requires some means of measuring distance between examples. An example is usually a vector of feature values plus a category label \(class When the features aie numeric normalized Euclidean distance can be used to compare examples. However, when he feature values have symbolic unordered values original IBL typically resort to much simpler metrics such as counting the features that match  11 D Aha\222s original IBL I has had series of developments  from its pure form called IB1 to most complex version of fB4 Briefly analyzing them the major difference of one version from another is as follows IB1 stores all training instances as its knowledge base whereas TB2 stores only important subset by an evaluation function However it is known to be noise sensitive Thus IB3 effectively eliminates noise from TB2 And IB4 gwes a penaltyhcentive algorithm in mining phase to discriminate relatively important features from others Our version of IB3 re-implemented in C called XiBL in this paper has three important changes from the original one First XIBL avoids overestimating symbolic attributes by applying Value Difference Metnc VDM  whereas original IB3 took the 221winner-takes-all\222 strategy IB3 has the normalization process in it in order to calculate the distance between two instances., Real-valued attributes usually have a distribution between 0 and 1 However IB3\222s strategy gives 1 if the attribute values are different and 0 if they are the same This inevitably overestimates symbolic attributes over real-valued attributes Value Difference Metric VDM takes into account the overall similarity of classification of all instances for each possible value of each feature Using this method a matrix defining the distance between all values of a feature is derived statistically based on the examples in the training set The distance 6 between two values for a specific feature is defined in below equation.191 where VI V2 denote two points in the example space and Ck and Cki denote the number of examples labeled as class k and i\222th value of class kin respectively Second IBL is noise sensitive. Aha\222s 183 showed how to exclude noise of IB2 I However leave-one-aut procedure as a noise filter is more statistically sound So we take leave-one-out as a noise filtering process Third original IB4 has weighting feature scheme but the perfomance of it rarely beats IB3 mainly due to its simple penaltyiincentive scheme XIBL takes a sort of backward stepwise attribute filtering technique based on stepwise regression in order to set the weights of attributes Among N attributes we have we compare the accuracy of full N attributes and those of when only N-k attributes are considered The difference of the two\222means the importance of k attributes in classification Hence we set the weights of attributes proportional to the difference in a training set In this paper we take k  2 only for the simplicity Figure 1 shows the pseudo-code of XIBL used in this experiment Key  T Training set TE Test set P Set of attributes VDM Symbolic value distance metrks k  Number of most similar instances used PCD Partial concept description W  Weight of each attributes Train\( T  P VDM  k,W 1 PCD 2 VDM  Mab-VDMfT P 3 for each xt E T do 3.1 classValue=NearstNeighbor\(P PCD  VDM,W,k x 3.2 if classVahe not equal targetvalue then 3.3 LerveOoeOut\(PCD k MM P NcarstNeighbor\(P, PCD, VDM W k  I 1 Update each attribute\222s mar min values for numeric type 2 for each yI E PCD do 2.1 Similarity&,a,W 3 KSET=k_most_simibr_ins~nce PCD  k 4 return Vote\(KSET Leaveoneout PCD  k  VDM  P 1 NOISE 2 for each I E PCDdo PCD  PCD  XI 2.1 classVatuc=NearstNefghbor\(P PCD  VDM xI 2.2 if classVnlue not equal targetvalue then NOISE  NOISE  XI 3 PCD  PCD  NOISE Test\(TE P VDM k 1 for each x4 E TE do 1.1 return NearstNelehbor\(P. PCD  VDM.k I Figure 1 XIBL Pseudo code 2.2 XIBL in preliminary experiment We used DARPA 1998 off-line intrusion detection evaluation  throughout this paper We are interested in the relative classification perfomance of XIBL to the types of SNORT alarms  true and false alarms XlBL IabeIs every packet as 223normal\224 and 223abnormal\224 which are meaningless if it is used alone. However, we test the classification power of XIBL over the training set explained below in order to find its characteristics Our preliminary experiment data set is organized as follows First we randomly collect 70 of real attacks from DARPA  242  


data set and part of normal data set in contiguous time frame with about the same size in packets In results, we have 44726 packets and 45453 normal packets Then we conduct a IO-fold cross validation on this data set We choose the best PCD which contains noise-filtered packet vector from above procedure The best PCD contains 365 normal packets and 294 abnormal packets or about 8 of training data with 99.4 fitting rate With this PCD we test 30 of attack packets and about the same size of normal packets The result shows that 88 of normal packets are classified into \223normal\224 but only 68 of attack packets are correctly classified Instances in XIBL PCD denote a representative polygon of each class in the vector space If a test packet is coming in XlBL finds the most similar instances from the PCD and labels the class of input as that of the 221closest instance from PCD In this way XIBL tends to mode1 224normal\224 packets more correctly than that of 223abnormal\224 since a stream of abnormal packets consist an attack Then, we examine the performance of SNORT 1.8.7 with the same training data SNORT identifies an attack if one or more alarms are triggered within the stream of 223attack\224 labeled data For every attack type we calculate the number of packets consisting that attack approximately and see how XIBL responses to that attack There are several false alarms from SNORT in this pilot data set We send all 223normal\224 labeled data to SNORT and it triggers alarms with known and unknown types Then we compare these false \(labeled with known attacks\alarms with true alarms with respect to XIBL behavior if there exists any pattem For some back attacks 2-fri-back 3_wed_back\the packet sequence has completely same subsequences so that we only analyze that subsequence instead of full sequence Figure 1 and Figure 2 clearly shows that XlBL behaves differently whether an alarm signals true attack or not In this pilot data set there are only four types of false alarms labeled with known attacks Figure 3 shows if we set a threshold according to the XIBL\222s performance within the attack sequence The number of packets in false alarm is induced from that of true alarm And also there were more than 200 false alarms as \223unknown attacks\224 In that case, we analyze 100 packets in front of alarmed packet and see how XIBL behaves From Figure 1 to Figure 3 we can set a threshold that if SNORT triggers an alarm, trace back to 100 packets\(if type is unknown or the average packet size of a known alarm and see if XIBL\222s 223abnormal\224 rate is more that 50 if yes then regard it as 223true\224 alarm 7 mon phf I I 2 7 wed phf 7 6 216 C ortswee 2-mongortswe ep Table 1 Activity of XIBL la True Alarm 5 71.43 1 14.28 216 100 Name of Attack satan 1-mon 4 4 50 1-thur IO 5\221 33.33 2-fI-i 3 5 62.5 3 mon 31  5 62.5 3  wed 31 5 62.5 33 336 10 monjortswe I Table 2 Activity of XIBL in False Alarms I 27.2 ortswee Another factor considered in this preliminary experiment is how long XIBL\222s 223abnormal\224 consecutive sequence is and if that is different between real normal and attack data Let\222s call it as 223picks\224 The rationale of this criterion is that if an attack is real, some number of packets with similar characteristics will flood in the network meanwhile normal data should not have that long sequence of incorrectly classified packets by XIBL For most of real attacks, this pick criterion turns out to be more than 50 if the number of packets in the attack sequence is sufficiently long say more than 100 packets Table 4 and Figure 2 shows that the distribution of picks over number of patterns detected in the sequence of normal data  243  


picks Patterns Accuracy 86.52 91.03 Missed attack 49 17 1 454 221 I 2d Figure 2 Distribution of picks and patterns 3 Full Experiment We conduct an experiment with full DAWA 1998 data The purposes of this experiment are as follows 1 If appropriately combined can we reduce the amount of alarms while preserving the accuracy 2 Will our second criterion number of consecutive XIBL abnormal packets play any role to detect another attacks that passes SNORT signature Recall that that data set of our preliminary experiment consists of a part of data and the distribution of training data was set to approximately 5050 of normal and abnormal packets In real situation 99 of packet flow is normal In DARPA data there are more than 3 million packets and only little more than 2  of them are known as attack packets Thus it is easy to expect that here will be more false alarms from SNORT Also in our preliminary experiment we count on only One alarm per attack data set In real situation for the same type of attack SNORT will trigger many more alarms regardless of its real figure Thus we set a decision rule of triggering an alarm in this hybrid architecture as follows Key AB XIBL rate of abnormality P XIBL picks K  average packet size of known type attack Decision Rule If SNORT triggers an alarm on input packet x Then Begin if the type of attack is known\224 Then trace back to k packets Else trace back to 100 pickets Examine k packets by XIBL Examine ID0 packets by XIBL Endif If AB  50 Then Send Alarm Endif End Else Begin Else pass that packet I Examine P with the same source IP If P  50 Then Send Alarm Endif Else Update P of that source IP End Figure 3 Decision Role Table 5 summarizes the experiment I SNORT I Hybrid I Rate 1 TrueAlarm I 83084 834 99.98 I I I I2945 ew True ew False Total 960251 91304 95 I The result shows that the hybrid system actually reduces SNORT false alarm to 60 level and retain most of SNORT true alarm Moreover hybrid system did find some number of new attacks that SNORT could not find However in that case\(new true alarm only 1-3 alarms are triggered since the decision rule reset P\(picks after it detects new attack\(n0 SNORT alarm and usually picks are very high Another noticeable finding is that while SNORT did not generates any alarm for 49 attack set in DARPA 1998 data the hybrid system misses only 17 of them which usually consists of less than 100 packets 244 


4 Conclusion I 11 Won I Song D Lee C Heo Y  Jang J A Machine Learning approach toward an environment-free network anomaly IDS  A primer report In Proc of 5th Intemational Conference on Advanced Comunication Technology 2o03 We have presented a series of experiment to set up a hybrid system of combining signature-based IDS SNORT with a machine learning based anomaly detector engine While many other systems have tried data mining techniques to reduce false alarms we have shown that if rich set of analyzed data is available memory based supervised leaming can be effectively used to improve the performance of signature based IDS The experiment shows that the reduction rate is though far from satisfactory This might be resulted by poor set of signature we have in SNORT in that too many alarms are triggered from the same attack set In that sense a careful alarm filter that does not omit valuable information could be helpful for this hybrid system At this point of time the implementation level of this hybrid system is just a taboratory style We are developing this hybrid system continuously and some other new ideas and test sets are ready to be included Acknowledgements This research is fully supported by Electronic and Telecommunication research Institute REFERENCES I Aha D  Kibler D Noise-tolerant instance-based learning algorithms Proceedings of the Eleventh International Joint Conference on Artificial Intelligence pp.794-799 1989 2 Julisch K Minin alarm clusters to improve alarm handling efficiency In 17 t Annual Computer Security Application Conference\(ACSAC pp 12-21,2000 3 Julisch K  Dacier M Mining Inmsion Detection Alarms for Actionable Knowledge In 8 ACM International Confemce on Knowledge Discovery and Data Mining 2002 Toth T Using decision trees to improve signaturebased detcction In 6th Symposium on Recent Advances in Intrusion Detection RAD Lecture Notes in Computer Science, Springer Verlag USA September 2003 5 Lippman R et Al Evaluation intrusion detection systems The 1998 DARPA Off-line intrusion detection evaluation Proc Of DARPA Information Survivability Conference and Exposition 6 Manganaris S Christensen, M Zerkle D  Henniz K A Data Mining Analysis of RTID Alarms In Znd Workshop on Recent Advances in intrusion Detection RAID99\1999 7 Patton S Yurcik W  Doss D An Achilles Heel in Signature-based IDS Squealing False Positives in SNORT Lecture Notes in Computer Science Springer Verlag USA 2003 SI SNORT http://w.snort.org 9 Stanfill C  Waltz D Toward memory-based reasoning IO 4 Kruegel C  pp 12-26,2000 Communications of the ACM 1986 Intrusion Detection Systems \(written in Korean Communication of the Korean Institute of Communication Sciences 19\(8 pp 41-51,2002 Won I Song D  Lee C The Architecture of Network   245  


                         


                      


            


100 34 33 34 DSD u u u                          u             Fig.6: Over Hiding problem of setting  1 in S No matter the left-hand or right-hand equation, the support of {1, 2} in D' is 0. That is, item 1 and item 2 never appear toge ther, and they are mutual exclusive! This situation almost never happens in the normal database. The attackers may interest in this situation and infer that {1, 2} is hidden deliberately. To hide the sensitive patterns, only need to make their supports smaller than minimum support and need not to decrease their support to 0. To solve the problem, we inject a probability ? which is called Distortion probability into this approach. Distortion probability is used only when the column j of the sanitization matrix S contains only one  1  i.e. Sjj = 1 0 1 d   m k k j i k  S D  m j n i j i  d d d d   1  1     D  i j  h a s   j probability to be set to 1 and 1  j probability to be set to 0 Lemma 1: Given a minimum support ?, and a level of confidence c. Let {i, j} be a pattern in Marked-Set, nij be the support count of {i, j}. ? is the Distortion probability of column j Without loss of generality, we assume that Sij  1. If ? satisfies    D n i j  u  u  V U   a n d    


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


