html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">2004 IEEE  International Conference on Systems, Man and Cybernetics Maintenance of Multiple-level Association Rules for Record Modification Tzung-Pei Hong Department of Electrical Engineering National University of Kaohsiung Kaohsiung, 81 1, Taiwan, R.O.C tphong@nnk.edu.tw Abstract - In the past, researchers usually assumed databases were static and items lay on the same level to simplify the mining problem. Modijcation of records with item taronomy is, however, commonly seen in real-world applications. In this paper, we thus attempt to extend Han and Fu  s approach and our previous concept of pre-large itemsets to maintain discovered multiple-level association rules for record modijication. The concept of pre-large itemsets is used to reduce the need for rescanning original databases and ta save maintenance costs. A pre-large itemset is not truly large, but promises ta be large in the firture. An algorithm is proposed based on the concept to achieve this purpose. The proposed algorithm doesn  t need to rescan the original database until a number of records have been modijied Keywords: Data mining, association rule, large itemset pre-large itemset, record modification 1 Introduction Due to the increasing use of very large databases and data warehouses, mining useful information and helpful knowledge from transactions is evolving into an important research area. In the past, researchers usually assumed databases were static to simplify data-mining problems Thus, most of the classic algorithms proposed focused on batch mining [1][3][5][9~[101[131[l71[181 Cheung and his co-workers proposed an incremental mining algorithm, called FUP \(Fast UF  date algorithm 6][8], for incrementally maintaining mined association rules. In [l I], Hong et. al. proposed an incremental miniig algorithm for inserted transactions, which can further reduce the number of rescanning the databases. It uses a lower support threshold and an upper support threshold to reduce the need for rescanning original databases and to save maintenance costs. Pre-large itemsets are not truly large. They act like buffers in the incremental miniig process and are used to reduce the movements of itemsets directly from large to small and vice-versa Most algorithms for association rule miniig focused on finding association rules on the single-concept level Tzu-Juog Huang and Chao-Sheng Chang Department of Information Management I-Shou University Kaohsiung ,84008, Taiwan, R.O.C m9122006@s!mail.isu.edu.tw and ccs@isu.edu.tw However, mining multiple-concept-level rules may lead to discovery of more specific and important knowledge from database. Relevant item taxonomies are usually predefied in real-world applications and can be represented using hierarchy trees. In addition to record insertion and deletion record modification is also commonly seen in real-world applications. Although maintenance of association rules for records modification can be performed by usage of the deletion and the insertion procedures, twice computation time of a single procedure is needed. Developing efficient maintenance algorithms for modification of records is thus necessary In this paper, we thus attempt to extend Han and Fu  s approach and the concept of pre-large itemsets to maintain discovered multiple-level association rules for record modification. A new algorithm is proposed to maintain discovered multiple-level association rules for modification of records. The proposed algorithm doesn  t 


modification of records. The proposed algorithm doesn  t need to rescan the original database until a number of records have been modified. If the database is large, then the number of modified records allowed will be large too 2 Multiple-Level Association Rules Most algorithms for association rule mining focused on finding association rules on the single-concept level However, mining multiple-concept-level rules may lead to discover more specific and important knowledge from data. For example, Wonder wheat bread is a wheat bread which is also a kind of bread. Relevant item taxonomies are usually predefined in real-world applications and can be represented using hierarchy trees. Terminal nodes on the trees represent actual items appearing in transactions internal nodes represent classes or concepts formed by lower-level nodes. A simple example is given in Figure 1 In Figure 1, the root node is on level 0, the internal nodes representing categories \(such as  milk   1, the internal nodes representing flavors \(such as  chocolate   representing brands \(such as  Foremost   Only terminal nodes appear in transactions 0-7803-8566-7/04/$20.00 0 2004 JEEE 31 40 Food Table 2: Two modified records Items 1000 In [Il l ,  Hang et. al. proposed a maintenance algorithm for record insertion. It can reduce the number of rescanning an original database by using the pre-large itemsets. A pre-large itemset is not truly large, but promises to be large in the future. A lower support threshold and an upper support threshold are used to realize this concept Although maintenance of rules for modification of records can be performed by wage of the deletion procedure and the insertion procedure, twice computation time of a single procedure is needed.Therefore, we attempt to use the pre-large concept to maintain mined association rules as records are modified. Processing record modification is, however, different from processing record insertion and deletion. Tbe former must consider the change of record contents. Let record difference be the changes between modified records and original records. For example, assume Table 1 is the original database and the minimum support is 50%. The originally large I-itemsets are {\(A B C D records before modification and after modification Table 3: Two modification records I I Figure 1 : An example of taxonomy Han and Fu proposed a method for fmding level crossing association rules at multiple levels [IO]. Nodes in predefined taxonomies are fust encoded using sequences of numbers and the symbol "*" according to their positioris in the hierarchy tree. For example, the intemal node "Milk " in Figure 1 would he represented by 1**, the internal node Chocolate" by 11*, and the terminal node "Dairyland' by 11 1. A top-down progressively deepening search approach is used and exploration of "level-crossing" association relationships is allowed. Candidate itemsets on cettain levels may contain items on other levels. For exarr.ple candidate 2-itemsets on level 2 are not limited to containing only pairs of large items on level 2. Instsad large items on level 2 may he paired with large item!: on level 1 to form candidate 2-itemsets on level 2 \(such as 11*, 2 3 Rule Maintenance When records in a database are modified, the original association rules may become invalid, or new impl6:itly valid rules may appear in the resulting updated database For example, assume a database has ten records as shown 


For example, assume a database has ten records as shown in Table 1 and assume the minimum support is 50%. The large 1-itemsets mined out from the database are {\(A B C D E in Table 1 are modified as shown in Table 2, the origit.ally large itemset \(D before modification after modification Items IIems ABF 1000 DEF Let k-itemset difference be the count difference ofk itemsets due to record modification. The difference of 1 itemset is shown in Table 4 Table 4: The difference between 1-itemsets. 1 Items 1 Difference A I 0 0 1 I B r This 1-itemset difference can be represented as \(C: -1 D: -2, F +I AC: -1 AD: -1, AF: +1, BC: -1, BD: -l,BF: +1, CD: -1, D E  -1 D F  -I ABC: -1, ABD: -1 ACD: -1, BCD -1, ABF: +1, DEF -1 difference derived from the modified records is then used to maintain the association rules 3141 In the above example, the 1-itemsets {A B] and E} have no count difference and are the same as before the modification. They are thus still large after record modification. Their supersets \(AB], {AE], {BE} and ABE have no count difference. The I-itemsets \(C} and {D] have negative count difference, causing some original large itemsets with \(C} or \(D the I-itemset {F] have positive count difference, causing some original small itemsets with {F} to become large Thus only the itemsets including items with count difference must be reconsidered Considering an original database and its itemset difference from some modified records, the following nine cases \(illustrated in Figure 2 of pre-large itemsets is used, where the negative itemsets represent those with count difference smaller than zero Case 4 Case 5 Origind Pre-large or Lorge Decidedfrom the existing information No chanze Figure 2: Nine cases arising from modifying records in existing databases Case 1 will keep the originally large itemsets still large since their final counts increase. Cases 2, 5 and 8 will keep the itemsets behave as they did before record modification since their fmal counts do not change. These three cases will not affect the final association rules. Cases 3 and 6 may remove existing large or pre-large itemsets, but cases 4 and 7 may add new large or pre-large itemsets. If we retain all large and pre-large itemsets with their counts, then cases 3 4 and 6 can be handled easily since the original counts and the difference counts of the itemsets in these cases are known. Also, in the maintenance phase, the ratio of the numher of modified records to the number of all records is usually very small. This is more apparent when the database is larger. An itemset in case 7 cannot possibly be large for the entire updated database as long as the number of modified records is small when compared to the number of records in the original database. Finally, the itemsets in Cases 9 cannot become large since their counts decrease. A summary of the nine cases and their results is given in Table 5 


Table 5 Table 5 :  Nine cases and their results Small or Pre-large or Large Pre-large or Small, Case 6 Case 7 Case8 I No chanze Case9 1 Always Small Let S, and S. be respectively the lower and the upper support thresholds, d and t be respectively the numbers of the original and modified records, and r denote the ratio of the number t of modified records to the numher d of all records. In [I I], we have shown that if r 5 \(Su- SI itemset that is small \(neither large nor pre-large original database won  t become large for the entire updated database. In this paper, we will generalize the approach to maintain the association rules with item taxonomies 4 Notation The notation used in this paper is defined below D : the original database T :  the set of modified records U: the updated database RD : record difference IDk : the k-itemset difference d : the number of records in D t : the number of record in T SI : the lower support threshold forpre-large itemsets S. : the upper support threshold for large itemsets. S  SI L f  : the set of large k-itemsefsfiom D L: : the set of large k-itemsetsfiom U p; : the set ofpre-large k-itemsets fiom D p i  : the set ofpre-large k-itemsets from U I : an itemset fm: the count of I in D pD\(I fer 5 The Proposed Algorithm The proposed maintenance algorithm for record modification integrates Hang et al  s pre-large concepts and Han and Fu  s multi-level miniig method. Assume d is the number of transactions in the original database. A variable 31 42 c, is used to record the number of modified transactions since the last re-scanning of the original database. D,:tails of the proposed maintenance algorithm are given below The proposed multi-level maintenance algorithm for record modijkation INPUT: A set of large and pre-large itemsets in the original database consisting of d records, a set of t modified records, a predefmed taxonomy, a lower support threshold SI, an upper support threrihold Su, and a predefmed confidence value h OUTPUT A set of multiple-level association rules for the STEP 1 : Calculate the safety number f of modified records updated database as follows STEP 2: Set 1 = 1, where 1 records the level of items in STEP 3: Set k =I, where k records the number of items in taxonomy itemsets STEP 4: Derive the k-itemset difference ID1 from the modified records STEP 5: Partition the itemsets in IDk into three parts according to whether they are large, pre-lar\(:e or small in the original database STEP 6: For each itemset I existing in both the originally large k-itemsets L; and in the k-itemset difference IDh do the following substeps Substep 6-1: If I exists in IDk from the modified records, then set the new c o u n t f f l otherwise, set pm = s"\(I 


otherwise, set pm = s"\(I I as a large itemset, set f \( r I with P\(I otherwise, if f\(l a pre-large itemset, set P\(I 0 and keep I with f\(l otherwise, just remove I from the large itemsets STEP 7: For each itemset I existing in both the originally pre-large itemset p; and in the k-ite.nset difference IDb do the following substeps Substep 7-1: If I exists in IDk fiom the modified records, then set the new count :f otherwise, set s"\(0 = F\(I F\(I 0 Substep 6-2: If f @ / d  2 S., assi f\(I 0 Substep 7-2: If pf l /d  2 S., assi I as a large itemset, set f\(I with P\(0 otherwise, if s"m/d Z S ,  assign I as a pre-large itemset, set F\(I and keep I with 90 otherwise, just remove I from the pre-large itemsets STEP 8: For each itemset I in IDk that is not in the originally large itemsets L? or pre-large itemsets pf  , do the following substeps Substep 8-1: If I has positive counts in IDk, then put it in the rescan-set R, which is used when rescanning in Step 9 is necessary otherwise, just neglect I STEP 9: If t+c 5 for  R is null, then do nothing; otherwise rescan the original database to determine whether the itemsets in the rescan-set R are large or pre large STEPlO:Setk=k+l STEP 11: Repeat STEPs 3 to 10 until no new large or pre large itemsets are found STEP 12: Prune the kept large and pre-large itemsets on the next level which are not the descendents of those found after STEP 1 1 STEP13:Set l=l+l STEP 14: Repeat STEPs 2 to 13 until all levels are proposed or there are no large and pre-large itemsets on level I STEP 15: Modify the association d e s  according to the STEP 16: Iff +c &gt;f; then set c=O; otherwise, set c=t+c After Step 16, the final multiple-level association rules for the updated database are found modified large itemsets 6 AnExample An example is given to illustrate the proposed maintenance algorithm for record modification. Assume the original database includes 10 transactions as shown in Table 6 Table 6. Encoded transaction data in the example ITEMS Assume SI = 30% and S. = 50%. The sets of large itemsets and pre-large itemsets on any level for the given original transaction database are shown in Tables 7 and 8 respectively. They a e  then kept for later maintenance 3143 Table 7: The large itemsets on all levels for the original database Table 8: The pre-large itemsets on all levels for the original database If the record with TZD = 900 in Table 6 is modified to be the one in Table 9, the proposed maintenance algorithm for record modification proceeds as follows. The variable c 


is initially set at 0 Table 9: The modified records Items 111,112,221 f =  \(Su- SJd= \(0.5-0.3 All candidate I-itemsets differenceand their counts on level 1 from the modification records are found. All candidate 1-itemsets are divided into three parts, according to they are large, pre-large or small in the original database The following substeps are then done for each of the originally large 1-itemsets {I 2   Steps 3 to IO are then executed to fmd all updated large and pre-large itemsets on level 1, with results shown in Table 10 Table 10: All the large and pre-large 2-itemsets on level 1 Large itemsets The large and pre-large itemsets on level 1 are used to prune the originally kept itemsets on level 2. If a itemset originally kept on level 2 is not a descendent of any one on level 1, it is pmed .  In this example, {31*} is pruned Steps 2 to 13 are then repeated to fmd all large and pre-large itemsets on level 2. Similarly, all large and pre large itemsets on level 3 are found. The f m l  results are shown in Table 1 1 Table 11. The laree itemsets on all levels for the uudated 7 Conclusions In this paper, we have proposed a maintenance algorithm to discover multiple-level association rules for modification of records. The proposed algorithm can efficiently and effectively maintain association rules with a taxonomy based on the pre-large concept. This process can further reduce the number of rescanning original databases and improve the mining performance when records are modified Acknowledgment This research was supported by the MOE Program for Promoting Academic Excellence of Universities under the grant number 89-E-FA04-1-4 References l] R. Agrawal, T. Imielinksi and A. Swami  Mining association rules between sets of items in large database  The ACM SZGMOD Conference, pp. 207-216 Washington, D.C., USA, 1993 2] R. Agrawal, T. Imielinksi and A. Swami  Database mining: a performance perspective  ZEEE Transactions on Knowledge andData Engineering, Vol. 5, No. 6, pp. 914 925,1993 3] R. Agrawal and R. Srikant  Fast algorithm for mining association rules  The International Conference on Very Large Data Bases, pp. 487-499, 1994 4] R. Agrawal and R. Srikant  Mining sequential patterns  The Eleventh IEEE Zntemational Conference on Data Engineering, pp. 3-14, 1995 5] R. Agrawal, R. Srikant and Q. Vu  Mining association rules with item constraints  The Third International Conference on Knowledge Discovery in 3144 Databases and Dafa Mining, pp. 67-73, Newport Brach California, 1997 6] D.W. Cheung, J. Han, V.T. Ng, and C.Y. Wong  Maintenance of discovered association rules in large databases: An incremental updating approach  The TweIfth IEEE Infemafional Conference on i%fa Engineering, pp. 106-1 14, 1996 7] D.W. Chenng, V.T. Ng, and B.W. Tam  Maintenance of discovered knowledge: a case in multi level association rules  The Second lntemafional Conference on Knowledge Discovey and Data Mining   D-96 8] D.W. Cheung, S.D. Lee, and B. Kao  A general 


8] D.W. Cheung, S.D. Lee, and B. Kao  A general incremental technique for maintaining discovered association rules  In Proceedings of Dofabase Sysfem.y for Advanced Applications, pp. 185-194, Melbonme, Australia 1997 9] T. Fuknda, Y. Morimoto, S .  Morishita and T Tokuyama  Mining optimized association rules for numeric attributes  The ACM SIGACT-SIGMOD-SIG4RT Symposium on Principles of Dafabase Systems, pp. 182 191,1996 IO] J. Han and Y. Fu  Discovery of multiple-level association rules from large database  The Twenfyfirsf Intemafionaf Conference on Very Large Dafa Bases, pp 420-431, Zurich, Switzerland, 1995 Il l  T. P. Hong, C. Y. Wang and Y. H. Tao  A new incremental data mining algorithm using pre-large itemsets  Infelligenf Data Analysis, Vol. 5, No. 2, pp. I1 1 129,2001 12] M. Y. Lin and S .  Y. Lee  Incremental updatt: on sequential patterns in large databases  The Tenth G E E Infematioiml Conference on Tools wifh Arfif;cia Intelligence, pp. 24-31, 1998 I31 H. Mannila, H. Toivonen, and A. I. Verkamo  Efficient algorithm for discovering association rules  The AAAI Workshop on Knowledge Discovey in Databases, pp 181-192,1994 1141 J. S .  Park, M. S .  Chen, P. S .  Yu  Using a hash-based method with transaction trimming for mining association rules  IEEE Transactions an Knowledge and Data Engineering, Vol. 9, No. 5, pp. 812-825, 1997 1151 W. Pedrycz  Data mining and fuzzy modeling  New Frontiers in Fuzzy Logic and So Conference of fhe Norfh American Fuzzy Informafian Processing Society, pp. 263-267, 1996 16] N. L. Sarda and N. V. Srinivas  An adaptive algorithm for incremental mining of association rules  The Ninth International Workshop on Database and Expert System, pp. 240-245, 1998 1171 R. Srikant and R. Agrawal  Mining generalized association rules  The Twenty-first Infemafional Conference on Vefy Lnrge Data Bases, pp. 407-419 Zurich, Switzerland, 1995 I81 R. Srikant and R. Agrawal  Mining quantitative association rules in large relational tables  The 1996 ACM SIGMOD Infemafional Conference on Managemenf of Data,pp. 1-12, Montreal, Canada, 1996 19] C. Y. Wang, T. P. Hong and S. S. Tseng  Maintenance of sequential patterns for record deletion   The 2001 IEEE Infemafional Conference on Dafa Mining pp. 536-541,2001 20] S .  Zhang  Aggregation and maintenance for database mining  InfeNigenf Data Analysis, Vol. 3, No. 6, pp. 475 490, 1999 3145 pre></body></html 


Algorithm 4.1 employs CA to return interesting association rules. The process considers fuzzy importance of items and involves fuzzy weighted support and confidence. This algorithm has been implemented and tested, the results are presented next in Section 5 5. EXPERIMENTAL RESULTS We used real-life dataset and conducted some experiments to assess the effectiveness of the CA-based fuzzy weighted mining approach presented in this paper All of the experiments were performed using a Pentium 111, 1.4GHz CPU with 512 MB of memory and running Windows 2000. As experimental data, we used lOOK transactions dataset taken from the adult data of United States census in 2000. In the experiments, we have used 6 quantitative attributes, each with three corresponding fuzzy sets. Finally, we have used three linguistic intervals for which random linguistic weights have been generated namely \(Important, Very-Important Ordinary Important Unimportant, Ordinary 0-1 and UI-0, respectively 500 2 400 300 &lt; 200 1 0  1 100 Very Low Medium High Very Low High Minimum Support Fig. 5 Number of large itemsets for linguistic terms of minimum support M i n .  cont A Fig. 6 Membership function for nunimum confidence The first experiment tests, for the above three different linguistic weight intervals, the correlation between expressing minimum support in linguistic terms and the number of large itemsets produced. The obtained results are reported in Figure 5, which shows that the number of large itemsets decreases as a function of the linguistic minimum support yI 100 d, 80 1 60 I 5 40 0 B 20 i o Very Low Medium High Very LOW High MininumConfidencc Fig. 7 Number of interesting rules for different linguistic terms of nunimumconfidence; nunimum support fixed as  nuddle   In the second experiment, the minimum support is fixed at the linguistic value  middle  and we tested, for the three linguistic weight intervals, the effect of using linguistic te rm to express minimum confidence, as shown in Figure 6, on the number of generated interesting 114 association rules. The achieved results are reported in Figure 7. The obtained results do meet our expectations i.e., more rules are generated for higher weights However, the number decreases, for all cases, as the linguistic confidence threshold increases 140 120 TI00 80 60 2 40 20 0 0 20 40 60 80 100 Number ofTransactions \(K 


Fig. 8 Runtime for GAS to find fuzzy sets for the three linguistic intervals The last experiment is dedicated to investigate the performance for the three linguistic intervals. In particular, we examined how the performance varies with the number of transactions. This is reflected in Figure 8 which shows the runtime as we increase the number of input records from 10K to 100K, for the three different cases. The results plotted in Figure 8 show that the method scales quite linearly for the census dataset used in the experiments 6. CONCLUSIONS In this paper, we proposed a clustering approach to solve the problem of interval partitioning in favor of the maximum number of large itemsets based on linguistic minimum support and confidence. The main achievement of the proposed approach is employing GAS to dynamically adjust and optimize membership functions which are essential in finding interesting weighted association rules from quantitative transactions, based on support and confidence specified as linguistic terms Compared to previous mining approaches, the proposed approach directly manages linguistic parameters, which are more natural and understandable to humans. Results of the experiments conducted on a real life census dataset demonstrated the effectiveness and applicability of the proposed approach r31 r41 r51 REFERENCES R. Agrawal, T. Imielinski and A. Swami  Mining association rules between sets of items in large databases  Proc. of ACM SIGMOD, pp.207-216, 1993 W.H. Au and K.C.C. Chan  An Effective Algorithm for Discovering Fuzzy Rules in Relational Databases  Proc C.H. Cai, et al  Mining Association Rules with Weighted Items  Proc. of IDEAS, pp.68-77, 1998 K.C.C. Chan and W.H. Au  Mining Fuzzy Association Rules  Proc. of ACM CIKM, pp.209-215, 1997 B.C. Chien, ZL. Lin and T.P. Hong  An Efficient Clustering Algorithm for Mining Fuzzy Quantitative Association Rules  IFSA World Congress and NAFIPS International Conference, Vo1.3, pp.1306-1311,2001 A.W.C. Fu, et al  Ending Fuzzy Sets for the Mining of Association Rules for Numerical Attributes  Proc. of the OfIEEE-FUZZ, pp.1314-1319,1998 115 International Symposium of Intelligent Data Engineering and Learning, pp.263-268, Oct. 1998 D.E. Goldberg, Genetic Algorithms in Search Optimization, and Machine Learning, Addison-Wesley Reading, MA, 1989 S. Guha, R. Rastogi and K. Shim  CURE: An Efficient Clustering Algorithm for Large Databases  Information Systems, Vo1.26, No.1, pp.35-58,2001 A. Gyenesei  A Fuzzy Approach for Mining Quantitative Association Rules  TUCS Technical Report No.336 2000 K. Hirota and W. Pedrycz  Linguistic Data Mining and Fuzzy Modelling  Proc. of IEEE-FUZZ, pp.1448-1496 1996 J.H. Holland, Adaptation in Natural and Artificial Systems, The MIT Press, Cambridge, MA, MIT Press edition, 1992. First edition: University of Michigan Press 1975 T.P. Hong, C.S. Kuo and S.C. Chi  A fuzzy data mining algorithm for quantitative values  Proc. of the International Conference on Knowledge-Based Intelligent Information Engineering Systems, pp.480483, 1999 T.P. Hong, C.S. Kuo and S.C. Chi  Mining Association 


Rules from Quantitative Data  Intelligent Data Analysis Vo1.3, pp.363-376, 1999 T. P. Hong, M. J. Chiang and S. L. Wang  Mining from Quantitative Data with Linguistic Minimum Supports and Confidences  Proc. of IEEE-FUZZ, pp. 494-499,2002 H. Ishibuchi, T. Nakashima and T. Yamamoto  Fuzzy Association Rules for Handling Continuous Attributes   Proc. of IEEE International Symposium on Industrial Electronics, pp. 1 1 8 - 12 1, 200 1 M. Kaya, R. Alhajj, F. Polat and A. Arslan  Efficient Automated Mining of Fuzzy Association Rules  Proc. of DEXA, 2002 C.M. Kuok, A.W. Fu and M.H. Wong  Mining fuzzy association rules in databases  SIGMOD Record, Vol. 17 No.1, pp.41-46, 1998 B. Lent, A. Swami and J. Widom  Clustering Association Rules  Proc. of IEEE ICDE!, pp.220-23 1 1997 R.J. Miller and Y. Yang  Association Rules over Interval Data  Proc. ofACM SIGMOD, pp.452-461, 1997 R. Ng and J. Han  Efficient and effective clustering methods for spatial data mining  Proc. of VLDB, 1994 W. Pedrycz  Fuzzy Sets Technology in Knowledge Discovery  Fuzzy Sets and Systems, 98, pp.279-290 1998 R. Srikant and R. Agrawal  Mining quantitative association rules in large relational tables  Proc. of ACM W. Wang and S.M. Bridges  Genetic Algorithm Optimization of Membership Functions for Mining Fuzzy Association Rules  Proc. of the International Conference on F m y  Theory &amp; Technology, pp. 13 1-134,2000 R.R. Yager  Fuzzy Summaries in Database Mining   Proc. of the Conference on Artificial Intelligence for Application, pp.265-269, 1995 S .  Yue, el al  Mining fuzzy association rules with weighted items  Proc. of IEEE SMC Conference pp.1906-1911,2000 L.A., Zadeh  Fuzzy Sets  Information and Control W. Zhang  Mining Fuzzy Quantitative Association Rules  Proc. of IEEE ICTRI, pp.99-102, 1999 SIGMOD, pp.1-12, 19 V01.8, pp.338-353, 1965 pre></body></html 


efficiency then AOFI. However utilization of fuzzy concept hierarchies provides more flexibility in reflecting expert knowledge and so allows better modeling of real-life dependencies among attribute values, which will lead to more satisfactory overall results for the induction process. The drawback of the computational cost may additionally decline when we notice that, in contrast to many other data mining algorithms, hierarchical induction algorithms need to run only once through the original \(i.e. massive dataset. We are continuing an investigation of computational costs of our approach for large datasets ACKNOWLEDGMENT Rafal Angryk would like to thank the Montana NASA EPSCoR Grant Consortium for sponsoring this research REFERENCES 1] J. Han , M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann, New York, NY 2000 2] J. Han, Y. Cai, and N. Cercone  Knowledge discovery in databases: An attribute-oriented approach  Proc. 18th Int. Conf. Ver y Large Data Bases, Vancouver, Canada, 1992, pp. 547-559 3] J. Han  Towards Efficient Induction Mechanisms in Database Systems  Theoretical Computing Science, 133, 1994, pp. 361-385 4] J. Han, Y. Fu  Discovery of Multiple-Level Association Rules from Large Databases  IEEE Trans.  on KD E, 11\(5 5] C.L. Carter, H.J. Hamilton  Efficient AttributeOriented Generalization for Knowledge Discovery from Large Databases  IEEE Trans. on KDE 10\(2 6] R.J. Hilderman, H.J. Hamilton, and N. Cercone  Data mining in large databases using domain generalization graphs  Journal of Intelligent Information Systems, 13\(3 7] C.-C. Hsu  Extending attribute-oriented induction algorithm for major values and numeric values   Expert Systems with Applications , 27, 2004, pp 187-202 8] D.H. Lee, M.H. Kim  Database summarization using fuzzy ISA hierarchies  IEEE Trans . on SMC - part B, 27\(1 9] K.-M. Lee  Mining generalized fuzzy quantitative association rules with fuzzy generalization hierarchies  20th NAFIPS Int'l Conf., Vancouver Canada, 2001, pp. 2977-2982 10] J. C. Cubero, J.M. Medina, O. Pons &amp; M.A. Vila  Data Summarization in Relational Databases through  Fuzzy Dependencies  Information Sciences, 121\(3-4 11] G. Raschia, N. Mouaddib  SAINTETIQ:a fuzzy set-based approach to database summarization   Fuzzy Sets and Systems, 129\(2 162 12] R. Angryk, F. Petry  Consistent fuzzy concept hierarchies for attribute generalization  Proceeding of the IASTED Int. Conf. on Information and Knowledge Sharing, Scottsdale AZ, USA, November 2003, pp. 158-163 13] Toxics Release Inventory \(TRI available EPA database hosted at http://www.epa.gov/tri/tridata/tri01/index.htm The 2005 IEEE International Conference on Fuzzy Systems790 pre></body></html 


the initial global candidate set would be similar to the set of global MFIs. As a result, during the global mining phase the communication and synchronization overhead is low  0 2 4 6 8 1 0 Number of Nodes Figure 5. Speedup of DMM 4.4.2 Sizeup For the sizeup test, we fixed the system to the 8-node con figuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local database sire at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remained the same as the local database size was increased. This is different from the speedup test, where the database repartitioning was per formed when the number of nodes was increased. The per formance of DMM is affected by the database repartitioning to some extent, although it is usually very small. During the sizeup test, the local mining result of DMM is not changed at all at each node The results shown in Figure 6 indicate that DMM has a very good sizeup property. Since increasing the size of local database did not affect the local mining result of DMM at each node, the total execution time increased just due to more disk U 0  and computation cost which scaled almost linearly with sizeup 5 Conclusions In this paper, we proposed a new parallel maximal fre quent itemset \(MFI Max-Miner \(DMM tems. DMM is a parallel version of Max-Miner, and it re quires low synchronization and communication overhead compared to other parallel algorithms. In DMM, Max Miner is applied on each database partition during the lo 0 45 90 135 180 225 270 Amwnt of Data per Node \(ME Figure 6. Sizeup of DMM cal mining phase. Only one synchronization is needed at thc end of this phase to construct thc initial global candi date set. In the global mining phase, a top-down search is performed on the candidate set, and a prefix tree is used to count the candidates with different length efficiently. Usu ally, just a few passes are needed to find all global maximal frequent itemsets. Thus, DMM largely reduces the number of synchronizations required between processing nodes Compared with Count Distribution, DMM shows a great improvement when some frequent itemscts are large \(i.e long patterns employed by DMM for efficient communication between nodes; and global support estimation, subset-infrequency based pruning, and superset-frequency based pruning are used to reduce the size of global candidate set. DMM has very good speedup and sizeup properties References I ]  R. Agrawal and R. Srikant  FdSt Algorithms for Mining As sociation Rules  Pmc. o f f h e  ZOrh VLDB Conf, 1994, pp 487499 2] R. Agrawal and I. C. Shafer  Parallel Mining of Association Rules  IEEE Trans. on Knowledge and Dura Engineering Vol. 8, No. 6, 1996, pp. 962-969 3] R. I. Bayardo  Efficient Mining Long Patlems from Databases  Proc. ofrhe ACM SIGMOD Inf  l Conf on Man ogemenr ofDara, 1998, pp. 85-91 4] S.  M. Chung and J. Yang  A Parallel Distributive Join Al gorithm for Cube-Connected Multiprocessors  IEEE Trans on Parallel and Disrribured Systems, Vol. 7, No. 2, 1996, pp 127-137 51 M. Snir, S. Otto. S. Huss-Lederman, D. Walker, and J. Don gana, MPI: The Complete Reference, The MIT Press, 1996 


gana, MPI: The Complete Reference, The MIT Press, 1996 6] R. Rymon  Search through Systematic Set Enumeralion   Pmc. of3rd Inr  l Con$ on Principles of Knowledge Repre sentation and Reasoning, 1992, pp. 539-550 507 pre></body></html 


sketch-index in answering aggregate queries. Then Section 5.2 studies the effect of approximating spatiotemporal data, while Section 5.3 presents preliminary results for mining association rules 5.1 Performance of sketch-indexes Due to the lack of real spatio-temporal datasets we generate synthetic data in a way similar to [SJLL00 TPS03] aiming at simulation of air traffic. We first adopt a real spatial dataset [Tiger] that contains 10k 2D points representing locations in the Long Beach county \(the data space is normalized to unit length on each dimension These points serve as the  airbases  At the initial timestamp 0, we generate 100k air planes, such that each plane \(i uniformly generated in [200,300], \(ii, iii destination that are two random different airbases, and iv  the velocity direction is determined by the orientation of the line segment connecting its source and destination airbases move continually according to their velocities. Once a plane reaches its destination, it flies towards another randomly selected also uniform in [0.02, 0.04 reports to its nearest airbase, or specifically, the database consists of tuples in the form &lt;time t, airbase b, plane p passenger # a&gt;, specifying that plane p with a passengers is closest to base b at time t A spatio-temporal count/sum query has two parameters the length qrlen of its query \(square number qtlen of timestamps covered by its interval. The actual extent of the window \(interval uniformly in the data space \(history, i.e., timestamps 0,100 air planes that report to airbases in qr during qt, while a sum query returns the sum of these planes  passengers. A workload consists of 100 queries with the same parameters qrlen and qtlen The disk page size is set to 1k in all cases \(the relatively small page size simulates situations where the database is much more voluminous specialized method for distinct spatio-temporal aggregation, we compare the sketch-index to the following relational approach that can be implemented in a DBMS. Specifically, we index the 4-tuple table lt;t,b,p,a&gt; using a B-tree on the time t column. Given a count query \(with window qr and interval qt SELECT distinct p FROM &lt;t,b,p,a&gt WHERE t?qt &amp; b contained in qr The performance of each method is measured as the average number of page accesses \(per query processing a workload. For the sketch-index, we also report the average \(relative Specifically, let acti and esti be the actual and estimated results of the i-th query in the workload; then the error equals \(1/100 set the number of bits in each sketch to 24, and vary the number of sketches The first experiment evaluates the space consumption Figure 5.1 shows the sketch index size as a function of the number of sketches used \(count- and sum-indexes have the same results more sketches are included, but is usually considerably smaller than the database size \(e.g., for 16 signatures, the size is only 40% the database size 0 20 40 60 80 


80 100 120 140 160 8 16 32 number of sketches size \(mega bytes database size Figure 5.1: Size comparison Next we demonstrate the superiority of the proposed sketch-pruning query algorithm, with respect to the na  ve one that applies only spatio-temporal predicates. Figure 5.2a illustrates the costs of both algorithms for countworkloads with qtlen=10 and various qrlen \(the index used in this case has 16 sketches also illustrate the performance of the relational method which, however, is clearly incomparable \(for qrlen?0.1, it is worse by an order of magnitude we omit this technique Sketch-pruning always outperforms na  ve \(e.g., eventually two times faster for qrlen=0.25 increases with qrlen, since queries returning larger results tend to set bits in the result sketch more quickly, thus enhancing the power of Heuristics 3.1 and 3.2. In Figure 5.2b, we compare the two methods by fixing qrlen to 0.15 and varying qtlen. Similar to the findings of [PTKZ02]4 both algorithms demonstrate  step-wise  growths in their costs, while sketch-pruning is again significantly faster The experiments with sum-workloads lead to the same observations, and therefore we evaluate sketch-indexes using sketch-pruning in the rest of the experiments 4 As explained in [PTKZ02], query processing accesses at most two paths from the root to the leaf level of each B-tree regardless the length of the query interval Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE sketch-pruning naive relational 0 100 200 300 400 500 600 700 800 900 0.05 0.1 0.15 0.2 0.25 number of disk accesses query rectangle length 300 0 100 200 400 500 600 1 5 10 15 20 number of disk accesses query interval length a qtlen=10 b qrlen=0.15 Figure 5.2: Superiority of sketch-pruning \(count As discussed in Section 2, a large number of sketches reduces the variance in the resulting estimate. To verify this, Figure 5.3a plots the count-workload error of indexes 


using 8-, 16-, and 32- sketches, as a function of qrlen qtlen=10 error \(below 10 it increases slowly with qrlen used, however, the error rate is much higher \(up to 30 and has serious fluctuation, indicating the prediction is not robust. The performance of 16-sketch is in between these two extremes, or specifically, its accuracy is reasonably high \(average error around 15 much less fluctuation than 8-sketch 32-sketch 16-sketch 8-sketch relative error 0 5 10 15 20 25 30 35 0.05 0.1 0.15 0.2 0.25 query rectangle length relative error 0 5 10 15 20 25 30 35 1 5 10 15 20 query interval length a qtlen=10, count b qrlen=0.15, count relative error query rectangle length 0 5 10 15 20 25 0.05 0.1 0.15 0.2 0.25 relative error query interval length 0 5 10 15 20 25 30 1 5 10 15 20 c qtlen=10, sum d qrlen=0.15, sum Figure 5.3: Accuracy of the approximate results The same phenomena are confirmed in Figures 5.3b where we fix qrlen to 0.15 and vary qtlen 5.3d \(results for sum-workloads number of sketches improves the estimation accuracy, it also leads to higher space requirements \(as shown in Figure 5.1 Figures 5.4a and 5.4b show the number of disk accesses for the settings of Figures 5.3a and 5.3b. All indexes have almost the same behavior, while the 32-sketch is clearly more expensive than the other two indexes. The interesting observation is that 8- and 16-sketches have 


interesting observation is that 8- and 16-sketches have almost the same overhead due to the similar heights of their B-trees. Since the diagrams for sum-workloads illustrate \(almost avoid redundancy 32-sketch 16-sketch 8-sketch number of disk accesses query rectangle length 0 50 100 150 200 250 300 350 400 0.05 0.1 0.15 0.2 0.25 number of disk accesses query interval length 0 50 100 150 200 250 300 350 1 5 10 15 20 a qtlen=10 b qrlen=0.15 Figure 5.4: Costs of indexes with various signatures Summary: The sketch index constitutes an effective method for approximate spatio-temporal \(distinct aggregate processing. Particularly, the best tradeoff between space, query time, and estimation accuracy obtained by 16 sketches, which leads to size around 40 the database, fast response time \(an order of magnitude faster than the relational method average relative error 5.2 Approximating spatio-temporal data We proceed to study the efficiency of using sketches to approximate spatio-temporal data \(proposed in Section 4.1 as in the last section, except that at each timestamp all airplanes report their locations to a central server \(instead of their respective nearest bases maintains a table in the form &lt;time t, plane p, x, y&gt;, where x,y with parameters qrlen and qtlen distinct planes satisfying the spatial and temporal conditions. For comparison, we index the table using a 3D R*-tree on the columns time, x, and y. Given a query, this tree facilitates the retrieval of all qualifying tuples, after which a post-processing step is performed to obtain the Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE number of distinct planes \(in the sequel, we refer to this method as 3DR method introduces a regular res  res grid of the data space, where the resolution res is a parameter. We adopt 16 sketches because, as mentioned earlier, this number gives the best overall performance Figure 5.5 compares the sizes of the resulting sketch indexes \(obtained with resolutions res=25, 50, 100 the database size. In all cases, we achieve high compression rate \(e.g., the rate is 25% for res=25 evaluate the query efficiency, we first set the resolution to the median value 50, and use the sketch index to answer workloads with various qrlen \(qtlen=10 


workloads with various qrlen \(qtlen=10 size \(mega bytes database size 0 20 40 60 80 100 120 140 160 25 50 100 resolution Figure 5.5: Size reduction Figure 5.6a shows the query costs \(together with the error in each case method. The sketch index is faster than 3DR by an order of magnitude \(note that the vertical axis is in logarithmic scale around 15% error observations using workloads with different qtlen Finally, we examine the effect of resolution res using a workload with qrlen=0.15 and qtlen=10. As shown in Figure 5.6c, larger res incurs higher query overhead, but improves the estimation accuracy Summary: The proposed sketch method can be used to efficiently approximate spatio-temporal data for aggregate processing. It consumes significantly smaller space, and answers a query almost in real-time with low error 3D Rsketch number of disk accesses query rectangle length 1 10 100 1k 10k 0.05 0.1 0.15 0.2 0.25 16 14% 15 15% 13 relative error number of disk accesses query interval length 1 10 100 1k 10k 1 5 10 15 20 16 15% 15% 12% 11 relative error a qtlen=10, res=25 b qrlen=0.15, res=25 0 500 1000 1500 2000 2500 25 50 100 number of disk accesses resolution 20% 15% 14 relative error c qrlen=0.15, qtlen=10 


c qrlen=0.15, qtlen=10 Figure 5.6: Query efficiency \(costs and error 5.3 Mining association rules To evaluate the proposed algorithm for mining spatiotemporal association rules, we first artificially formulate 1000 association rules in the form \(r1,T,90 with 90% confidence i randomly picked from 10k ones, \(ii in at most one rule, and \(iii Then, at each of the following 100 timestamps, we assign 100k objects to the 10k regions following these rules. We execute our algorithms \(using 16 sketches these rules, and measure \(i  correct  rules divided by the total number of discovered rules, and \(ii successfully mined Figures 5.7a and 5.7b illustrate the precision and recall as a function of T respectively. Our algorithm has good precision \(close to 90 majority of the rules discovered are correct. The recall however, is relatively low for short T, but gradually increases \(90% for T=25 evaluated in the previous sections, the estimation error decreases as the query result becomes larger \(i.e., the case for higher T 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 precision HT 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 recall HT a b Figure 5.7: Efficiency of the mining algorithm Summary: The preliminary results justify the usefulness of our mining algorithm, whose efficiency improves as T increases Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 6. Conclusions While efficient aggregation is the objective of most spatio-temporal applications in practice, the existing solutions either incur prohibitive space consumption and query time, or are not able to return useful aggregate results due to the distinct counting problem. In this paper we propose the sketch index that integrates traditional approximate counting techniques with spatio-temporal indexes. Sketch indexes use a highly optimized query algorithm resulting in both smaller database size and faster query time. Our experiments show that while a sketch index consumes only a fraction of the space required for a conventional database, it can process 


required for a conventional database, it can process queries an order of magnitude faster with average relative error less than 15 While we chose to use FM sketches, our methodology can leverage any sketches allowing union operations Comparing the efficiency of different sketches constitutes a direction for future work, as well as further investigation of more sophisticated algorithms for mining association rules. For example, heuristics similar to those used for searching sketch indexes may be applied to improve the brute-force implementation ACKNOWLEDGEMENTS Yufei Tao and Dimitris Papadias were supported by grant HKUST 6197/02E from Hong Kong RGC. George Kollios, Jeffrey Considine and were Feifei Li supported by NSF CAREER IIS-0133825 and NSF IIS-0308213 grants References BKSS90] Beckmann, N., Kriegel, H., Schneider, R Seeger, B. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD, 1990 CDD+01] Chaudhuri, S., Das, G., Datar, M., Motwani R., Narasayya, V. Overcoming Limitations of Sampling for Aggregation Queries. ICDE 2001 CLKB04] Jeffrey Considine, Feifei Li, George Kollios John Byers. Approximate aggregation techniques for sensor databases. ICDE, 2004 CR94] Chen, C., Roussopoulos, N. Adaptive Selectivity Estimation Using Query Feedback. SIGMOD, 1994 FM85] Flajolet, P., Martin, G. Probabilistic Counting Algorithms for Data Base Applications JCSS, 32\(2 G84] Guttman, A. R-Trees: A Dynamic Index Structure for Spatial Searching. SIGMOD 1984 GAA03] Govindarajan, S., Agarwal, P., Arge, L. CRBTree: An Efficient Indexing Scheme for Range Aggregate Queries. ICDT, 2003 GGR03] Ganguly, S., Garofalakis, M., Rastogi, R Processing Set Expressions Over Continuous Update Streams. SIGMOD, 2003 HHW97] Hellerstein, J., Haas, P., Wang, H. Online Aggregation. SIGMOD, 1997 JL99] Jurgens, M., Lenz, H. PISA: Performance Models for Index Structures with and without Aggregated Data. SSDBM, 1999 LM01] Lazaridis, I., Mehrotra, S. Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure. SIGMOD 2001 PGF02] Palmer, C., Gibbons, P., Faloutsos, C. ANF A Fast and Scalable Tool for Data Mining in Massive Graphs. SIGKDD, 2002 PKZT01] Papadias,  D., Kalnis, P.,  Zhang, J., Tao, Y Efficient OLAP Operations in Spatial Data Warehouses. SSTD, 2001 PTKZ02] Papadias, D., Tao, Y., Kalnis, P., Zhang, J Indexing Spatio-Temporal Data Warehouses ICDE, 2002 SJLL00] Saltenis, S., Jensen, C., Leutenegger, S Lopez, M.A. Indexing the Positions of Continuously Moving Objects. SIGMOD 2000 SRF87] Sellis, T., Roussopoulos, N., Faloutsos, C The R+-tree: A Dynamic Index for MultiDimensional Objects. VLDB, 1987 TGIK02] Thaper, N., Guha, S., Indyk, P., Koudas, N Dynamic Multidimensional Histograms 


SIGMOD, 2002 Tiger] www.census.gov/geo/www/tiger TPS03] Tao, Y., Papadias, D., Sun, J. The TPR*Tree: An Optimized Spatio-Temporal Access Method for Predictive Queries. VLDB, 2003 TPZ02] Tao, Y., Papadias, D., Zhang, J. Aggregate Processing of Planar Points. EDBT, 2002 TSP03] Tao, Y., Sun, J., Papadias, D. Analysis of Predictive Spatio-Temporal Queries. TODS 28\(4 ZMT+01] Zhang, D., Markowetz, A., Tsotras, V Gunopulos, D., Seeger, B. Efficient Computation of Temporal Aggregates with Range Predicates. PODS, 2001 ZTG02] Zhang, D., Tsotras, V., Gunopulos, D Efficient Aggregation over Objects with Extent PODS, 2002 Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE pre></body></html 


