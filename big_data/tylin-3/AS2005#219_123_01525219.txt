A Novel Method to Integrate Spatial Data Mining and Geographic Information System  Jin Xingxing, Cai Yingkun, *Xie Kunqin g, Ma Xiujun, Sun Yuxiang, Cai Cuo National Laboratory on Machine Perception Department of Intelligent Science, Peking University, Beijing, China, 100871 jinxx, yingkun, kunqing, maxj pekingsun, ccai}@cis.pku.edu.cn   Abstract\227GIS is a good spatial analysis tool, nevertheless, as the accumulation of spatial data, the functions that GIS offers are not enough. Spatial data mining, which can automatically discover implicit knowledge from spatial data, has recently received wide attentions. However, the spatial data mining 
systems are not competent for preprocessing and presenting spatial data. Hence, the idea to integrate spatial data mining and GIS is straight forward. In this paper, we propose a novel method to integrate spatial data mining and GIS. This method is implemented with SPMML, an XML-based language that extended from PMML. We build a prototype system with this method and it works well Keywords\227SPMML, spatial data mining, GIS, integration PMML I  I NTRODUCTION  At present, Geographic Information System \(GIS\is a main tool for storing, processing and displaying spatial data GIS is a good spatial analysis tool, however, with the 
accumulation of spatial data, the spatial analysis function that GIS offers is not enough. Spatial data mining, which can automatically discover implicit knowledge from spatial data has recently received wide attentions. Lots of spatial data mining methods and systems have been developed since the 1990s. However, spatial data mining systems are not competent for preprocessing and presenting spatial data. The availability of functions such as spatial and non-spatial query and selection, classification, map overlay, network analysis and map creation, make GIS a useful tool for spatial data mining [1 T h e i d e a t o i n t e g r a t e s p a t i a l  d a t a m i ni ng a nd G I S which will produce mature and practical spatial data mining systems, is straightforward 
Several methods that integrate GIS and data mining techniques have been proposed, such as SPIN  3 SP L U S  for ArcView GIS [4 an d t h e m eth od in  5    T h e ov e r all  objective of the European IST SPIN! project, which integrates spatial data mining and GIS seamlessly, is to develop a web-based spatial data mining system by integrating state of the art GIS and data mining functionality in a closely coupled open and extensible system architecture [3  It  c o n s is ts  of f i v e  components: Lava map viewer, Descartes, Geoprocessor GAM and Kepler, where Kepler is a spatial data mining component and the other four components provide the GIS functions. S-PLUS for ArcView GIS is developed by MathSoft and ESRI. Functionality includes seamless data export and 
import between ArcView GIS and S-PLUS, advanced spatial bar charts and pie charts of analysis results and powerful linear regression analysis directly from a menu embedded in ArcView GIS. In [5  s p ati o t e m poral dat a a r e  p r e p r o ces s e d by  GIS before being mined by an association rule mining tool The data preprocessing step and the mining step are separately accomplished by two different systems. This is not an ideal method. SPIN! is an excellent spatial data mining system However, the integration method is too specific. Besides S-PLUS is a good spatial statistic tool rather than a spatial data mining system In this paper, a novel method is proposed to integrate spatial data mining and GIS. This method is based on Spatial 
Predictive Model Markup Language \(SPMML\anguage we extend from Predictive Model Markup Language \(PMML   By using this method, spatial data mining and GIS systems can be integrated easily as long as they are conformed to SPMML specification This paper is organized as follows: Section 2 gives a short overview of spatial data mining and introduces PMML  Section 3 specifies the definition of SPMML. We present the general integration method in section 4 and a prototype system that integrates spatial data mining and GIS with this method is introduced in section 5. Section 6 summarizes our conclusion and gives some directions for our future work  
II  B ASIC C ONCEPTS  A  Spatial Data Mining Widespread use of spatial databases and the accumulation of spatial data have lead to the studies of spatial data mining and the development of spatial data mining techniques \(or geographic knowledge discovery\. Spatial data mining is the process of discovering interesting and previously unknown but potentially useful patterns from large spatial datasets [7   T h e spatial data mining patterns include spatial classification spatial clustering, spatial association rules and spatial outliers and so on Extracting interesting and useful patterns from spatial 
datasets is more difficult than extracting corresponding patterns from traditional numeric and categorical data due to the complexity of spatial data types and spatial relationships   T i l l no w  a  l ot  o f spa t i a l  da t a m i ni ng t e c hni q ue s h a ve  b e e n  This paper is supported by the Major Project of National Natural Science Foundation \(40235056  Corresponding author: Xie Kunqing Tel.: 86-10-62756920  764  0-7803-9050-4/05/$20.00 \2512005 IEEE 764 


proposed, such as Co-location Rule [8  Sp a t ia l O u t lie r  Detection [9  Pr ed i c t i ng Loc a t i o ns U s i ng M a p  Si m i l a r i t y  PLUMS\[10 S p a tia l A u to reg r essiv e M o d e ls   1 1  a n d  STING   The process of knowledge discovery in database \(KDD includes seven steps: data cleaning, data integration, data selection, data transformation, data mining, pattern evaluation and knowledge presentation  Si m i l a r l y   t he p r oc e s s o f  spatial data mining also consists of these seven steps According to [7 th e m o st d i f f icu lt t a sk o f s p ati a l d a ta m i n i n g  is to transform spatial features into non-spatial features then plug them into traditional knowledge discovery processes. The integration of spatial data mining and GIS focuses on this task and additionally, the result presentation problem B  Predictive Model Markup Language The Predictive Model Markup Language \(PMML developed by Data Mining Group \(DMG\s an XML-based language which provides a way for applications to define statistical and data mining models and to share models between PMML compliant applications [6   DM G  is an  independent, vendor led group which develops data mining standards. It has 12 full members including the world\222s most famous software vendors, such as IBM Corp., Microsoft Oracle Corporation, SAS Inc., SPSS Inc., and so on. The newest version of PMML is version 3.0 that was released in October 2004. Version 3.1 is currently under development The goal of PMML is to encapsulate a model in an application and system independent fashion so that two different applications \(the PMML Producer and Consumer can use it. There are two primary a dvantages\001 of\001 using\001 the\001 PMML\001 standard\001 in\001 extensible knowledge\001 discovery\001 support\001engines\001\(KDDSE\\001and\001implementations\001of\001specific\001 mining\001 methods\001 15 Fi rst  001 no 001 t o ols pe cifi c\001 parse r s\001 ar e 001 required\001which\001decreases\001time\001and\001effort\001during\001 integration\001 and\001 eliminates\001 many\001 potential\001 errors\001 during\001 parsing.\001 And\001 second\001 the\001 knowledge\001 discovered\001 can\001 be\001 separated\001from\001the\001tool\001that\001was\001used\001to\001discover\001this\001 knowledge.\001 PMML version 3.0 consists of the following components  Header, Mining Build Task, Data Dictionary, Transformation Dictionary and Mining Models. There are 11 Models in PMML version 3.0, including Association Model, Clustering Model, General Regression Model, Naive Bayes Model Neural Network, Regression Model, Rule Set Model Sequence Model, Support Vector Machine Model, Text Model and Tree Model  III  SPMML We propose SPMML to integrate Spatial Data Mining and GIS. SPMML is designed based on the newest version PMML 3.0 and inherits the advantages of PMML. We extend PMML to SPMML by adding three components which are specific to spatial data mining, including spatial data types, spatial data transformations and spatial data mining models. The definitions of these three components are presented in the following sections  A  Spatial Data Types Spatial data types are used in the Data Dictionary and Mining Models components of SPMML. Currently, there are two kinds of spatial data types in SPMML, for vector data model and raster data model respectively. The vector data types that we defined are POINT, CURVE and SURFACE PIXEL is for raster data model. Figure 1 is the definitions of POINT, CURVE, SURFACE and PIXEL   Figure 1  definitions of spatial data types  Figure 2  the definition of BI-ATTR transformation B  Spatial Transformation Dictionary PMML version 3.0 defines various kinds of simple data transformations: Normalization, Discretization, Value mapping, Functions and Aggregation. In SPMML, we add two kinds of spatial data transformation. The first one is SINGLE-ATTR transformation, i e. transform single spatial  xs:element name="BI-ATTR xs:complexType xs:sequence xs:value name="method" type="xs:string" use="required xs:element name="type" type="xs:string" use="required xs:element name="value" type="xs:string" use="required xs:sequence xs:complexType xs:element xs:simpleType name='POINT xs:restriction base='xs:long xs:restriction xs:simpleType  xs:simpleType name='CURVE xs:restriction base='xs:long xs:restriction xs:simpleType  xs:simpleType name='SURFACE xs:restriction base='xs:long xs:restriction xs:simpleType  xs:complexType name='PIXEL xs:sequence xs:element name='fatherid' type='xs:long xs:element name='rowid' type='xs:long xs:element name='colid' type='xs:long xs:sequence xs:complexType  765  0-7803-9050-4/05/$20.00 \2512005 IEEE 765 


attribute to non-spatial attribute \(e.g. derive area attribute from polygon attribute.\The other one is BI-ATTR transformation, i.e. transform two spatial attributes to a non-spatial attribute \(e.g. derive the \223distance between two points\224 attribute from point attribute.\To finish the definition of Spatial Transformation Dictionary, these two kinds of transformations are added to the \223EXPRESSION\224 part of SPMML. Figure 2 is the definition of BI-ATTR transformation C  Spatial Data Models Two spatial data models are added into SPMML. They are Co-location Rule Model and Outlier Detection Model. In this section, we mainly introduce the definition of Co-location Rule Model  Figure 3  the definition of COL-LOCATION Model A co-location is a subset of boolean spatial features. A co-location rule is of the form: C 1 C 2 p, cp\ where C 1 and C 2  are  co-locations, C 1 C 2  p is a number representing the prevalence measure, and cp is a number measuring conditional probability [9 C o l o cat io n Ru le is ex t e n d e d  b a s e d  o n  Association Rules. The differences between these two patterns are the interesting measure parameters. For Co-location Rule the interesting measure parameters are prevalence and conditional probability. Hence, the definition of Co-location Rule Model is similar to Association Model except two components. First, the \223AssociationRule\224 \(see [6   pa rt of  Association Model is modified to \223ColocationRule\224. Figure 3 presents the definition of ColocationRule. Second, we add three attributes into Co-location Model, including reference feature centric model, window centric model and event centric model D  Transformation between SPMML and PMML The PMML schema contains a mechanism for extending the content of a model. The definitions of spatial data types spatial data transformations and spatial data models can be placed in the Extension part of corresponding components Consequently, SPMML is transformed to PMML. SPMML is a superset of PMML, so PMML model is also an SPMML model IV  T HE G ENERAL I NTEGRATION M ETHOD  The original purpose of PMML is to describe statistical and data mining models. We extend the usage of SPMML to the stages of spatial data transformation, spatial data mining and result presentation. By using SPMML, the integration of spatial data mining and GIS becomes simple. Figure 4 presents the integration of spatial data mining and GIS   Figure 4  integration of spatial data mining and GIS A  Spatial Data Transformation Before the stage of spatial data transformation, an SPMML format instruction \(or file\ that describes the spatial data transformation requirements should be generated by the spatial data mining system. Then, the GIS spatial data transformation component transforms the spatial data attributes to user desired attributes according to the SPMML instruction \(or file\ The transformation results are described in SPMML format too B  Spatial Data Mining After the spatial data transformation and ordinary data preprocessing, this stage extracts spatial patterns from the preprocessed data. The data also can be loaded from the SPMML file. In the mean time, former spatial models can be loaded from the SPMML files too. Finally, the mining results are described in SPMML format  C  Result Presentation Most spatial data mining models such as Co-location Model and Outlier Detection Model can not be effectively viewed in other tools except GIS. By adding an XML parser and a spatial data mining model interpreter in GIS, the mining  xs:element name="ColocationRule xs:complexType xs:sequence xs:element minOccurs="0" maxOccurs="unbounded ref="Extension xs:element minOccurs="1" maxOccurs="unbounded ref="Item xs:sequence xs:attribute name="support" type="PROB-NUMBER use="required xs:attribute name="confidence" type="PROB-NUMBER use="required xs:complexType xs:element  xs:element name="Item xs:complexType xs:attribute name="id" type="xs:string" use="required xs:attribute name="value" type="xs:string use="required xs:attribute name="mappedValue" type="xs:string xs:attribute name="weight" type="REAL-NUMBER xs:complexType xs:element  766  0-7803-9050-4/05/$20.00 \2512005 IEEE 766 


results that described in SPMML format can be easily presented V  E XPERIMENTS AND RESULTS  We integrate our spatial data mining system and SISP \(a spatial database system we implemented based on PostgreSQL 16  w ith th is m e th o d  F ig u r e 5  p r esen ts th e sy ste m  architecture of the integrated system. The spatial data mining system consists of four components   Figure 5  software architecture of our prototype system 200  Task Manager. It consists of several parts, such as XML parser, SPMML interpreter and so on. It defines the spatial data mining flows and generates SPMML models. It is the most important component of the system which connects the spatial data mining system and SISP 200  Data Adapter. This component loads spatial and non-spatial data and preprocesses the non-spatial data  200  Data Mining Algorithms Library. It accomplishes the spatial data mining tasks  200  Graphic User Interface \(GUI\. It presents the mining results which need not to be viewed in GIS. It also provides a user friendly interface to setting the parameters of mining models  VI  C ONCLUSION AND F UTURE WORK  SPMML is an XML-based language that describes Spatial Data Mining Models. It is extended from PMML which is designed for sharing data mining models between different mining tools. Therefore, SPMML has the advantages of PMML. We propose a novel method to integrate spatial data mining system and GIS with SPMML. A prototype system integrated by using this method is implemented and works well In the future, SPMML will be extended to describe the spatio-temporal data models. The spatial data types and spatial data transformations in SPMML will be redesigned too. We also want to add more spatial data mining models into SPMML VII  R EFEREN CES 1  Hong Tang, and Simon McDonald, "Integrating GIS and spatial data mining technique for target marketing of university courses", ISPRS Commission IV, Symposium 2002, Ottawa Canada, July 9-12 2002 2  European IST SPIN!-roject web site http://www.ais.fraunhofer.de/KD/SPIN 3  M. May, and A. Savinov, "An integrated platform for spatial data mining and interactive visual analysis", Proceedings Data Mining 2002 3rd International Conference on DataMining Methods and Databases for Engineering, Finance and Other Fields, Bologna, Italy, 25-27 Sept 2002 4  S-PLUS\256 for ArcView GIS ttp://www.insightful.com/products/arcview/default.asp 5  Jeremy Mennis, and Jun Wei Liu, "Mining Association Rules in Spatio-Temporal Data: An Analysis of Urban Socioeconomic and Land Cover Change", Transactions in GIS, 2005, 9\(1\: 5\22617 6  Predictive Model Markup Language, http://www.dmg.org 7  Shashi Shekhar, Yan Huang, Jia-Wei Han, Sanjay Chawla, and Sucharita Gopal, Categorization of Spatial Data Mining Techniques Scientific Data Mining, working chapter, 2001 8  Shashi Shekhar, Chang-Tien Lu, and Pusheng Zhang, "A Unified Approach to Detecting Spatial Outliers" volume 7, issue 2 GeoInformatica, Kluwer Academic Publishers, 2003 9  Yan Huang, Shashi Shekhar, and Hui Xiong, "Discovering Co-location Patterns from Spatial Datasets: A General Approach", IEEE Transactions on Knowledge and Data Engineering \(TKDE\, VOL.16 NO.12, December 2004   Sanjay Chawla, Shashi Shekhar, Weili Wu, and Uygar Ozesmi Predicting Locations Using Map Similarity\(PLUMS\: A Framework for Spatial Data Mining", MDM/KDD 2000, 14-24   Baris Kazar, Shashi Shekhar, and David J. Lilja, "Parallel Formulation of Spatial Auto-Regression", Army High-Performance Computing Research Center \(AHPCRC\echnical Report no. 2003-125, August 2003   Wei Wang, Jiong Yang, and Richard R. Muntz, "STING+: An Approach to Active Spatial Data Mining", ICDE 1999, 116-125   Jiawei Han, Micheline Kamber, Data Mining Concepts and Techniques Morgan Kaufmann Publishers, August 2000. 550 pages   Extensible Markup Language, http://www.w3.org/XML   D. Wettschereck and S. M\374ller, "Exchanging Data Mining Models with the Predictive Modelling Markup Language", In Christophe Giraud-Carrier, Nada Lavrac, Steve Moyle, and Branko Kavsek, editors ECML/PKDD'01 workshop on Integrating Aspects of Data Mining Decision Support and Meta-Learning: Positions, Developments and Future Directions, pages 55-66, ECML/PKDD'01 workshop notes September 2001   PostgreSQL, http://www.postgresql.org   767  0-7803-9050-4/05/$20.00 \2512005 IEEE 767 


2i-1 This proposition demonstrates the ordered context can prune the search space of closed itemsets Proposition 3.4 For a frequent ordered context, there are frequent closed itemsets in the folding search sub-space of an attribute Proof: For a frequent ordered context \(O, I, R and aj E I\( i &lt; j F3S \( ai i &lt; j This property of frequent ordered context allows certain search sub-spaces of an attribute to be formed a partition and the partition is not empty We can conclude our analysis in order to anwer the three above-mentioned questions. We need to order the data con  text into frequent ordered context. And then we can divide the search space into sub-spaces that include some F3S of attributes. Such partitions are not empty. In order to bal  ance the partitions, according to propositions 3.3 and 3.4 the attributes that are bigger can be put together to form the partition. For example, a3, a2, a1 can be in one partition the other partition contains only F3S\( a4  titioning method and the relations of partitions will be pre  sented in the next setion 4 A decomposition algorithm: PFC We propose a new algorithm, called PFC, which has two steps : determining the partitions \(see algorithm 1  erating all frequent closed itemsets in each partition \(see al  gorithm 2 4.1 Determining the partitions First, the frequent ordered context should be generated from a data context according to the min-support \(MS our algorithm, we don't preprocess the data, we only need binary context with the name of the items and transactions We don't generate a data file for frequent ordered context the frequent ordered context is only generated and stored in main memory. CHARM and CLOSET + also preprocessed the data to improve their performance Then we determine the partitions. Each partition con  tains certain attributes. Let the arribute set of the fre  quent ordered context I = {am, ... , ai, ... , ad. We choose some attributes \(including am and aI ordered context to form an order set P. If the num  ber of the elements of P is T, we have a 1 = apl &lt ap2 &lt; ... &lt; apk &lt; &lt; apT = am. We denote u 1 ::; k ::; T - 1 ai,ai] = F3S\(ai 1::; i::; m apk' apk+l[ \(0 ::; k ::; T - 1  partitions. All these partitions cover the search space of fre  quent closed itemsets. We can mine frequent closed item  sets in each partition. So the search space of frequent closed itemsets is U [apk,apHl[ U [am, am O&lt;k&lt;T-I In the first step of the algorithm \(see algorithm 1 can decide the number of partitions by a parameter P P\(O P P &lt; 1 is used to approximately balance the size of each partition When P P = 0, we get only one partition that contains all the search space. From the algorithm 1, we get the following proposition easily Proposition 4.1 The smaller the value of P P is, the more the number of partitions is All elements of listPartitions correspond to all Pk listPartitions[O] --+ PT, listPartitions[l] --+ PT-I listPartitions[T - 1] --+ PI of listPartitions to form the partitions [apk' apk+l [ and am, am], where 0 ::; k ::; T - 1. Here Pk means the posi  tion of an attribute of the frequent ordered context, and we use it to represent the attribute apk of the frequent ordered context 


context 4.2 Generating frequent closed itemsets For each generated partition, we mine the frequent closed itemsets with algorithm 2. The main method is based on the following propositions Algorithm 1 Determining the partitions 1: input a parameter PP \(0::; PP &lt; 1 2: input a parameter M S \(min-support 3: input data context 4: generate the frequent ordered context 5: create a list listPartitions to store the position of each par  tition 6: m = cardinal of the attribute set of the frequent ordered context 7: min:= m 8: k:= 0 9: while \(min &gt; 0 10: add min to listPartitions 11: min:= \(int double 12: k := k + 1 13: end while 14: T:= k liT is the number of the partitions 15: output listPartitions Proposition 4.2 Each partition is independent Proof: When we find the frequent closed itemsets of a par  tition, we only need to know the first subset and the last sub  set of this partition. And then we build next closed itemsets from the first subset until the last subset, the computing is closed in this partition. So each partition is independent. In other words, we can find all closed item sets of one partition independently Proposition 4.3 For a data context \(O, I, R I, I ={ am, ... , ai, ... , aI}, A --+ A" is the closure opera  tor. All closed itemsets form a closure system. The smallest one of all closures that is larger than A, called next clo  sure \(next closed itemset  ical order is A EB ai , where EB is defined by A EB ai A n \(am,am-I, ... ,ai+d U {ad ai E I, ai being the smallest element of I with A &lt; A EB ai by the order: am &gt; ... &gt; ai &gt; ... &gt; al In other words, for ai E I\\A, from the smallest element to larger one of I\\A, we calculate A EB ai, until we find the first time A &lt; A EB ai, then A EB ai is the next closed itemset. The proof can be seen in [6 Proposition 4.4 Let A be an itemset, A = {bk, ... , bi bI}, bi E I, bk &gt; ... &gt; bi &gt; ... &gt; bI &gt; al . Let B A n \(am' am-I, ... , ai+d U {ad ai E A, support\(B A Proof: When ai &lt; bI or ai E A, A c B, so support\(B A This proposition gives us a method to prune the non  frequent closed item sets when we find the next closed item  set. From one item set, we can find its next closed itemset But if the support of a closed itemset :::; min-support, its next closed itemset can be omitted using the proposition 7 Definition 4.1 Given a set A c I, A = {bko ... , bi bl}, bi E I. The next candidate of frequent closed item  set is the set Al  Iai = \(An \(am' am-I, ... , ai+d U {ad where ai &gt; bl and ai ? A, ai being the smallest element of I with A &lt; A l  I ai by the order: am &gt; ... &gt; ai &gt; ... &gt al   So for each partition, we compute the next closed itemset from {apK } to {apk+l} \(see algorithm 2  lation between each partition. The partitions only share the same source data. We can deal with any partition indepen  dently Algorithm 2 Generating all frequent closed itemsets in each partition 1: input frequent ordered context 2: input the index \(1 ::::: index::::: T 3: get Pk and Pk+l from listPartitions with index 


3: get Pk and Pk+l from listPartitions with index 4: A f- {apk} Ilbegin of partition 5: END f- apk+1 Ilend of partition 6: EXI:= false 7: A f- A 8: while \(!EXIT 9: ifllA'l1 &lt;= MSthen 10: if IIA'II = MS then 11: output frequent closed itemsets 12: end if 13: A f- next candidate of frequent closed itemset 14: else 15: output frequent closed itemsets 16: A f- generate next frequent closed itemsets 17: end if 18: if END E A when searching the next closure then 19: EXIT := true 20: end if 21: end while 4.3 An example With our approach, we can dynamically generate parti  tions according to the size and density of the data, and our need. In figure 4, we demonstrate how to generate different partitions for the same data Using the data context of Figure 3, we show below an example \(see Figure 4 need to generate the frequent ordered context. Given the min-support M S = 2, a2 and ag are merged as a2. As support \(a5 frequent ordered context is: as, a6, a7, a4, a3, a2, al   And then, we can give a value to the parameter to deter  mine the partitions, for example, when P P = 0.5 we get 3 partitions: [aI, a7[, [a7, as[ and [as, as]. When PP = 0.6 there are 4 partitions to generate: [aI, a4[, [a4, a6 [, [a6, as and [as, as]. When PP = 0.8,6 partitions are generated aI, a3[, [a3, a4[, [a4, a7[, [a7, a6[, [a6, as[ and [as, as When P P = 0.0 there is only one partition: [aI, as At the end, using algorithm 2, we find separately all fre  quent closed item sets in each partition. For the example of 3 partitions: [aI, a7[, [a7, as[ and [as, as], the frequent closed itemsets of each partition can be generated by algo  rithm 2 \(in Figure 4, Ci is the label of frequent closed item  sets. We get 4 frequent closed itemsets in [aI, a7[: {ad with 8 objects, {a2' al} with 5 objects, {a3, al} with 5 ob  jects, {a3, a2, ad with 2 objects, {a4' ad with 4 objects a4' a3, adwith 3 objects 5 Experimental results We have implemented PFC algorithm in Java. The experiments are performed on a PIII900 computer with 512Mo RAM and Windows XP system. We have tested our algorithm on some datasets of the UCI repository and the worst case. The worst case means the case when the sizes of  and each attribute is verified by n - 1 different objects, each object possesses n - 1 different attributes In order to supply the performance references of our al  gorithm, CLOSET + is tested on the same data. This isn't a real comparison, because 1 but CLOSET + in C++. C++ can improve performance than Java. 2 certain time to generate frequent ordered context. If the size of data is large, it will increase time-space cost. However the comparison with CLOSET + algorithm shows that our algorithm is better for large and dense data Table 1 shows some examples of our experimental re  sults on real data. The results demonstrate CLOSET+ out  perform PFC in most case. But if the size of item set is large and min-support is low, PFC is faster than CLOSET +. From table 1, the results of audiology data, lung-cancer data and soybean-large data show that PFC is faster than CLOSET For the worst case, PFC is faster than CLOSET +. In 


For the worst case, PFC is faster than CLOSET +. In table 2, the data name shows the size of data, for exam  ple, worst16 means the worst case data with 16 objects and items. It's hard to generate frequent closed itemsets for the worst case, so we only test for high min-support. Symbol I" means the algorithm needs very long time to gener  ate frequent closed itemsets, so we didn't get the result Here we demonstrate the number of partitions and maxi  mum time for one partition for PFC In summary, PFC is efficient for large dense data, and is faster than CLOSET + when the number of items is much higher than the number of objects PP=O.O PP=O.S PP=0.6 PP=0.8 Total partitions: I Total partitions: 3 Total partitions: 4 Total partitions: 6 Partition 1 [al ,  as1 #Partition l [al, a7[ #Partition I [al , a4[ #Partition 1 [ al, a3 CO: al [8J CO: al [81 CO: al [81 CO: al [8J CI : a2, al [SJ Cl : a2, al [Sl CI : a2, al [SJ Cl : a2, al [Sl C2 : a3, al [Sl C2 : a3, al [SJ C2 : a3, al [SJ #Partition2 [ a3, a4 C3 : a3, a2, al [2J C3 : a3, a2, al [21 C3 : a3, a2, al [21 CO: a3, al [SJ C4 : a4, al [4J C4 : a4, al [41 #Partition2 [a4, a6[ Cl : a3, a2, al [21 CS : a4, a3, al [31 CS : a4, a3, al [3J CO: a4, al [4J #Partition3 [ a4, a7 C6 : a7, al [4J #Partition2 [ a7, as[ Cl : a4, a3, al [31 CO: a4, al [4J C7 : a7, a2, al [3J CO: a7, al [41 C2 : a7, al [4J Cl : a4, a3, al [31 C8 : a6, a4, al [31 CI : a7, a2, al [3J C3 : a7, a2, al [3J #Partition4 [ a7, a6 C9 : a6, a4, a2, al [2J C2 : a6, a4, al [31 #Partition3 [ a6, as[ CO: a7, al [4J CIO: a6, a4, a3, al [2J C3 : a6, a4, a2, al [21 CO: a6, a4, al [3J Cl : a7, a2, al [31 C11 : as, a7, al [31 C4 : a6, a4, a3, al [2J C I : a6, a4, a2, al [2J #PartitionS [ a6, as C12: as, a7, a2, al [2J #Partition3 [as, as1 C2 : a6, a4, a3, al [21 CO: a6, a4, al [3J C13 : as, a7, a3, al [21 CO: as, a7, al [31 #Partition4 [as, asJ Cl : a6, a4, a2, al [21 CI : as, a7, a2, al [2J CO: as, a7, al [3J C2 : a6, a4, a3, al [21 C2 : as, a7, a3, al [21 Cl : as, a7, a2, al [21 #Partition6 [as, asJ C2 : as, a7, a3, al [2J CO: as, a7, atf31 Cl : as, a7, a2, al [21 C2 : as, a7, a3, al [2J Figure 4. An example of the different partitions for the same data \(The min-support is 2. The  number in brackets [ ] after each frequent closed itemset means the its support 6 Conclusion In this paper we analyze the search space of frequent closed itemsets and propose a new lattice-based algorithm PFC for mining frequent closed itemsets from data. PFC can generate non-overlapping partitions of the search space and mine frequent closed item sets in each partition. There is no relation between each partition in this algorithm. The partitions only share the same source data. We can deal with any partition independently. So we can apply this algorithm for parallel, distributed, or network computing. PFC shows good performance when dealing with very large data. Pre  liminary experimental results show that PFC outperforms CLOSET + for dense data. It is an efficient algorithm to mine frequent closed itemsets for large and dense data The ongoing research is studying the problem of balance of all partitions. And we will implement this algorithm on a parallel or distributed platform Acknowledgements We are grateful to anonymous reviewers for helpful com  ments. This research benefits from the support of IUT de Lens and the region NordlPas de calais References IJ R. Agrawal, T. Imielinski, and A. N. Swami. Mining as  sociation rules between sets of items in large databases. In Proceedings of the 7993 ACM SIGMOD, pages 207-216 Washington, D.C., 26-28 1993 21 R. Agrawal and R. Srikant. Fast algorithms for mining as  sociation rules. In Proc. 1994 Intl. Conf Very Large Data Bases \(VLDB'94  ber 1994 31 J. Boulicaut, A. Bykowski, and C. Rigotti. Freesets: A con  densed representation of boolean data for the approximation of frequency queries. Data Mining and Knowledge Discov  ery, 7\(1 4J H. Fu and E. Mephu Nguifo. Partitioning large data to 


4J H. Fu and E. Mephu Nguifo. Partitioning large data to scale up lattice-based algorithm. In Proceedings ofICTAI03 pages S37-S41, Sacramento, CA, November 20 03. IEEE Press SJ H. Fu and E. Mephu Nguifo. How well go lattice algo  rithms on currently used machine learning testbeds? In 4emes journees d' Extraction et de Gestion des Connais  sances, pages 373-384, France, 20 04 61 B. Ganter and R. Wille. Formal Concept Analysis. Mathe  matical Foundations. Springer, 1999 7J J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In W. Chen, J. Naughton, and P. A Bernstein, editors, 2000 ACM SIGMOD Intl. Conference on Management of Data, pages 1-12. ACM Press, OS 2000 Data file Objects Items Min support FCI PFC \(msec msec audiology 26 110 1 30401 1302 8563 soybean-small 47 79 1 3541 516 431 lung-cancer 32 228 1 186092 21381 279689 promoters 106 228 3 298772 120426 111421 soybean-large 307 133 1 806030 357408 364524 dermatogogy 366 130 50 192203 20204 18387 breast-cancer-wis 699 110 1 9860 3529 1131 kr-vs-kp 3196 75 1100 2770846 1823092 483896 agaricus-Iepiota 8124 124 100 38347 34815 1462 connect-4bi.data 67557 126 1000 2447136 1165806 65084 Table 1. Experiments on real data.\(FCI means frequent closed itemsets. msec means milliseconds For Ref., + means PFC is faster than CLOSET Data file Min support FCI PFC \(msec msec Worst16 1 65534 571 470 271 9 Worst17 1 131070 1112 1002 541 9 Worst18 1 262142 2243 2174 1091 9 Worst19 1 524286 4576 4466 2213 10 Worst20 1 1048574 9243 9484 4606 10 Worst25 20 68405 2103 66916 451 11 Worst25 19 245505 6099 1095065 1552 11 Worst25 18 726205 15452 10235287 4486 11 Worst25 17 1807780 33348 / 10755 11 Worst25 15 7119515 102237 / 39296 11 Worst30 25 174436 6980 426964 1302 12 Worst30 20 53009101 1029771 / 344035 12 Worst50 47 20875 1132 1042 422 14 Worst50 45 2369935 227207 / 29102 14 Worst60 57 36050 7320 3205 821 15 Worst60 56 523685 82938 1665715 9123 15 Worst60 55 5985197 772210 / 92102 15 Worst70 68 2485 1102 190 121 15 Worst70 67 57225 18096 9483 1933 15 Worst70 66 974120 242138 / 26398 15 Table 2. Experiments on the worst case data 8] S. Kuznetsov and S. Obiedkov. Comparing performance of algorithms for generating concept lattices. lETAI Special Issue on Concept Lattice for KDD, 14\(2/3 9j E. Mephu Nguifo, M. Liquiere, and V. Duquenne. lETA Special Issue on Concept Lattice for KDD. Taylor and Fran  cis, 2002 IOj N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Efficient mining of association rules using closed itemsets lattices lournal of Information Systems, 24\(1 II j 1. Pei, 1. Han, and R. Mao. CLOSET: An efficient algo  rithm for mining frequent closed itemsets. InACM SIGMOD Workshop on Research Issues in Data Mining and Knowl  edge Discovery, pages 21-3 0, 200 0 12] 1. Wang, 1. Han, and 1. Pei. Closet+: Searching for the best strategies for mining frequent closed itelnsets. In In Pro  ceedings of the Ninth ACM SIGKDD International Confer  ence on Knowledge Discovery and Data Mining \(KDD'03 Washington, DC, USA, 2003 13] M. I. Zaki and C.-I. Hsiao. CHARM: An efficient algorithm for closed item set mining. Technical Report 99-10, Rensse  laer Polytechnic Institute, 1999 


laer Polytechnic Institute, 1999 pre></body></html 


efficiency then AOFI. However utilization of fuzzy concept hierarchies provides more flexibility in reflecting expert knowledge and so allows better modeling of real-life dependencies among attribute values, which will lead to more satisfactory overall results for the induction process. The drawback of the computational cost may additionally decline when we notice that, in contrast to many other data mining algorithms, hierarchical induction algorithms need to run only once through the original \(i.e. massive dataset. We are continuing an investigation of computational costs of our approach for large datasets ACKNOWLEDGMENT Rafal Angryk would like to thank the Montana NASA EPSCoR Grant Consortium for sponsoring this research REFERENCES 1] J. Han , M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann, New York, NY 2000 2] J. Han, Y. Cai, and N. Cercone  Knowledge discovery in databases: An attribute-oriented approach  Proc. 18th Int. Conf. Ver y Large Data Bases, Vancouver, Canada, 1992, pp. 547-559 3] J. Han  Towards Efficient Induction Mechanisms in Database Systems  Theoretical Computing Science, 133, 1994, pp. 361-385 4] J. Han, Y. Fu  Discovery of Multiple-Level Association Rules from Large Databases  IEEE Trans.  on KD E, 11\(5 5] C.L. Carter, H.J. Hamilton  Efficient AttributeOriented Generalization for Knowledge Discovery from Large Databases  IEEE Trans. on KDE 10\(2 6] R.J. Hilderman, H.J. Hamilton, and N. Cercone  Data mining in large databases using domain generalization graphs  Journal of Intelligent Information Systems, 13\(3 7] C.-C. Hsu  Extending attribute-oriented induction algorithm for major values and numeric values   Expert Systems with Applications , 27, 2004, pp 187-202 8] D.H. Lee, M.H. Kim  Database summarization using fuzzy ISA hierarchies  IEEE Trans . on SMC - part B, 27\(1 9] K.-M. Lee  Mining generalized fuzzy quantitative association rules with fuzzy generalization hierarchies  20th NAFIPS Int'l Conf., Vancouver Canada, 2001, pp. 2977-2982 10] J. C. Cubero, J.M. Medina, O. Pons &amp; M.A. Vila  Data Summarization in Relational Databases through  Fuzzy Dependencies  Information Sciences, 121\(3-4 11] G. Raschia, N. Mouaddib  SAINTETIQ:a fuzzy set-based approach to database summarization   Fuzzy Sets and Systems, 129\(2 162 12] R. Angryk, F. Petry  Consistent fuzzy concept hierarchies for attribute generalization  Proceeding of the IASTED Int. Conf. on Information and Knowledge Sharing, Scottsdale AZ, USA, November 2003, pp. 158-163 13] Toxics Release Inventory \(TRI available EPA database hosted at http://www.epa.gov/tri/tridata/tri01/index.htm The 2005 IEEE International Conference on Fuzzy Systems790 pre></body></html 


the initial global candidate set would be similar to the set of global MFIs. As a result, during the global mining phase the communication and synchronization overhead is low  0 2 4 6 8 1 0 Number of Nodes Figure 5. Speedup of DMM 4.4.2 Sizeup For the sizeup test, we fixed the system to the 8-node con figuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local database sire at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remained the same as the local database size was increased. This is different from the speedup test, where the database repartitioning was per formed when the number of nodes was increased. The per formance of DMM is affected by the database repartitioning to some extent, although it is usually very small. During the sizeup test, the local mining result of DMM is not changed at all at each node The results shown in Figure 6 indicate that DMM has a very good sizeup property. Since increasing the size of local database did not affect the local mining result of DMM at each node, the total execution time increased just due to more disk U 0  and computation cost which scaled almost linearly with sizeup 5 Conclusions In this paper, we proposed a new parallel maximal fre quent itemset \(MFI Max-Miner \(DMM tems. DMM is a parallel version of Max-Miner, and it re quires low synchronization and communication overhead compared to other parallel algorithms. In DMM, Max Miner is applied on each database partition during the lo 0 45 90 135 180 225 270 Amwnt of Data per Node \(ME Figure 6. Sizeup of DMM cal mining phase. Only one synchronization is needed at thc end of this phase to construct thc initial global candi date set. In the global mining phase, a top-down search is performed on the candidate set, and a prefix tree is used to count the candidates with different length efficiently. Usu ally, just a few passes are needed to find all global maximal frequent itemsets. Thus, DMM largely reduces the number of synchronizations required between processing nodes Compared with Count Distribution, DMM shows a great improvement when some frequent itemscts are large \(i.e long patterns employed by DMM for efficient communication between nodes; and global support estimation, subset-infrequency based pruning, and superset-frequency based pruning are used to reduce the size of global candidate set. DMM has very good speedup and sizeup properties References I ]  R. Agrawal and R. Srikant  FdSt Algorithms for Mining As sociation Rules  Pmc. o f f h e  ZOrh VLDB Conf, 1994, pp 487499 2] R. Agrawal and I. C. Shafer  Parallel Mining of Association Rules  IEEE Trans. on Knowledge and Dura Engineering Vol. 8, No. 6, 1996, pp. 962-969 3] R. I. Bayardo  Efficient Mining Long Patlems from Databases  Proc. ofrhe ACM SIGMOD Inf  l Conf on Man ogemenr ofDara, 1998, pp. 85-91 4] S.  M. Chung and J. Yang  A Parallel Distributive Join Al gorithm for Cube-Connected Multiprocessors  IEEE Trans on Parallel and Disrribured Systems, Vol. 7, No. 2, 1996, pp 127-137 51 M. Snir, S. Otto. S. Huss-Lederman, D. Walker, and J. Don gana, MPI: The Complete Reference, The MIT Press, 1996 


gana, MPI: The Complete Reference, The MIT Press, 1996 6] R. Rymon  Search through Systematic Set Enumeralion   Pmc. of3rd Inr  l Con$ on Principles of Knowledge Repre sentation and Reasoning, 1992, pp. 539-550 507 pre></body></html 


sketch-index in answering aggregate queries. Then Section 5.2 studies the effect of approximating spatiotemporal data, while Section 5.3 presents preliminary results for mining association rules 5.1 Performance of sketch-indexes Due to the lack of real spatio-temporal datasets we generate synthetic data in a way similar to [SJLL00 TPS03] aiming at simulation of air traffic. We first adopt a real spatial dataset [Tiger] that contains 10k 2D points representing locations in the Long Beach county \(the data space is normalized to unit length on each dimension These points serve as the  airbases  At the initial timestamp 0, we generate 100k air planes, such that each plane \(i uniformly generated in [200,300], \(ii, iii destination that are two random different airbases, and iv  the velocity direction is determined by the orientation of the line segment connecting its source and destination airbases move continually according to their velocities. Once a plane reaches its destination, it flies towards another randomly selected also uniform in [0.02, 0.04 reports to its nearest airbase, or specifically, the database consists of tuples in the form &lt;time t, airbase b, plane p passenger # a&gt;, specifying that plane p with a passengers is closest to base b at time t A spatio-temporal count/sum query has two parameters the length qrlen of its query \(square number qtlen of timestamps covered by its interval. The actual extent of the window \(interval uniformly in the data space \(history, i.e., timestamps 0,100 air planes that report to airbases in qr during qt, while a sum query returns the sum of these planes  passengers. A workload consists of 100 queries with the same parameters qrlen and qtlen The disk page size is set to 1k in all cases \(the relatively small page size simulates situations where the database is much more voluminous specialized method for distinct spatio-temporal aggregation, we compare the sketch-index to the following relational approach that can be implemented in a DBMS. Specifically, we index the 4-tuple table lt;t,b,p,a&gt; using a B-tree on the time t column. Given a count query \(with window qr and interval qt SELECT distinct p FROM &lt;t,b,p,a&gt WHERE t?qt &amp; b contained in qr The performance of each method is measured as the average number of page accesses \(per query processing a workload. For the sketch-index, we also report the average \(relative Specifically, let acti and esti be the actual and estimated results of the i-th query in the workload; then the error equals \(1/100 set the number of bits in each sketch to 24, and vary the number of sketches The first experiment evaluates the space consumption Figure 5.1 shows the sketch index size as a function of the number of sketches used \(count- and sum-indexes have the same results more sketches are included, but is usually considerably smaller than the database size \(e.g., for 16 signatures, the size is only 40% the database size 0 20 40 60 80 


80 100 120 140 160 8 16 32 number of sketches size \(mega bytes database size Figure 5.1: Size comparison Next we demonstrate the superiority of the proposed sketch-pruning query algorithm, with respect to the na  ve one that applies only spatio-temporal predicates. Figure 5.2a illustrates the costs of both algorithms for countworkloads with qtlen=10 and various qrlen \(the index used in this case has 16 sketches also illustrate the performance of the relational method which, however, is clearly incomparable \(for qrlen?0.1, it is worse by an order of magnitude we omit this technique Sketch-pruning always outperforms na  ve \(e.g., eventually two times faster for qrlen=0.25 increases with qrlen, since queries returning larger results tend to set bits in the result sketch more quickly, thus enhancing the power of Heuristics 3.1 and 3.2. In Figure 5.2b, we compare the two methods by fixing qrlen to 0.15 and varying qtlen. Similar to the findings of [PTKZ02]4 both algorithms demonstrate  step-wise  growths in their costs, while sketch-pruning is again significantly faster The experiments with sum-workloads lead to the same observations, and therefore we evaluate sketch-indexes using sketch-pruning in the rest of the experiments 4 As explained in [PTKZ02], query processing accesses at most two paths from the root to the leaf level of each B-tree regardless the length of the query interval Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE sketch-pruning naive relational 0 100 200 300 400 500 600 700 800 900 0.05 0.1 0.15 0.2 0.25 number of disk accesses query rectangle length 300 0 100 200 400 500 600 1 5 10 15 20 number of disk accesses query interval length a qtlen=10 b qrlen=0.15 Figure 5.2: Superiority of sketch-pruning \(count As discussed in Section 2, a large number of sketches reduces the variance in the resulting estimate. To verify this, Figure 5.3a plots the count-workload error of indexes 


using 8-, 16-, and 32- sketches, as a function of qrlen qtlen=10 error \(below 10 it increases slowly with qrlen used, however, the error rate is much higher \(up to 30 and has serious fluctuation, indicating the prediction is not robust. The performance of 16-sketch is in between these two extremes, or specifically, its accuracy is reasonably high \(average error around 15 much less fluctuation than 8-sketch 32-sketch 16-sketch 8-sketch relative error 0 5 10 15 20 25 30 35 0.05 0.1 0.15 0.2 0.25 query rectangle length relative error 0 5 10 15 20 25 30 35 1 5 10 15 20 query interval length a qtlen=10, count b qrlen=0.15, count relative error query rectangle length 0 5 10 15 20 25 0.05 0.1 0.15 0.2 0.25 relative error query interval length 0 5 10 15 20 25 30 1 5 10 15 20 c qtlen=10, sum d qrlen=0.15, sum Figure 5.3: Accuracy of the approximate results The same phenomena are confirmed in Figures 5.3b where we fix qrlen to 0.15 and vary qtlen 5.3d \(results for sum-workloads number of sketches improves the estimation accuracy, it also leads to higher space requirements \(as shown in Figure 5.1 Figures 5.4a and 5.4b show the number of disk accesses for the settings of Figures 5.3a and 5.3b. All indexes have almost the same behavior, while the 32-sketch is clearly more expensive than the other two indexes. The interesting observation is that 8- and 16-sketches have 


interesting observation is that 8- and 16-sketches have almost the same overhead due to the similar heights of their B-trees. Since the diagrams for sum-workloads illustrate \(almost avoid redundancy 32-sketch 16-sketch 8-sketch number of disk accesses query rectangle length 0 50 100 150 200 250 300 350 400 0.05 0.1 0.15 0.2 0.25 number of disk accesses query interval length 0 50 100 150 200 250 300 350 1 5 10 15 20 a qtlen=10 b qrlen=0.15 Figure 5.4: Costs of indexes with various signatures Summary: The sketch index constitutes an effective method for approximate spatio-temporal \(distinct aggregate processing. Particularly, the best tradeoff between space, query time, and estimation accuracy obtained by 16 sketches, which leads to size around 40 the database, fast response time \(an order of magnitude faster than the relational method average relative error 5.2 Approximating spatio-temporal data We proceed to study the efficiency of using sketches to approximate spatio-temporal data \(proposed in Section 4.1 as in the last section, except that at each timestamp all airplanes report their locations to a central server \(instead of their respective nearest bases maintains a table in the form &lt;time t, plane p, x, y&gt;, where x,y with parameters qrlen and qtlen distinct planes satisfying the spatial and temporal conditions. For comparison, we index the table using a 3D R*-tree on the columns time, x, and y. Given a query, this tree facilitates the retrieval of all qualifying tuples, after which a post-processing step is performed to obtain the Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE number of distinct planes \(in the sequel, we refer to this method as 3DR method introduces a regular res  res grid of the data space, where the resolution res is a parameter. We adopt 16 sketches because, as mentioned earlier, this number gives the best overall performance Figure 5.5 compares the sizes of the resulting sketch indexes \(obtained with resolutions res=25, 50, 100 the database size. In all cases, we achieve high compression rate \(e.g., the rate is 25% for res=25 evaluate the query efficiency, we first set the resolution to the median value 50, and use the sketch index to answer workloads with various qrlen \(qtlen=10 


workloads with various qrlen \(qtlen=10 size \(mega bytes database size 0 20 40 60 80 100 120 140 160 25 50 100 resolution Figure 5.5: Size reduction Figure 5.6a shows the query costs \(together with the error in each case method. The sketch index is faster than 3DR by an order of magnitude \(note that the vertical axis is in logarithmic scale around 15% error observations using workloads with different qtlen Finally, we examine the effect of resolution res using a workload with qrlen=0.15 and qtlen=10. As shown in Figure 5.6c, larger res incurs higher query overhead, but improves the estimation accuracy Summary: The proposed sketch method can be used to efficiently approximate spatio-temporal data for aggregate processing. It consumes significantly smaller space, and answers a query almost in real-time with low error 3D Rsketch number of disk accesses query rectangle length 1 10 100 1k 10k 0.05 0.1 0.15 0.2 0.25 16 14% 15 15% 13 relative error number of disk accesses query interval length 1 10 100 1k 10k 1 5 10 15 20 16 15% 15% 12% 11 relative error a qtlen=10, res=25 b qrlen=0.15, res=25 0 500 1000 1500 2000 2500 25 50 100 number of disk accesses resolution 20% 15% 14 relative error c qrlen=0.15, qtlen=10 


c qrlen=0.15, qtlen=10 Figure 5.6: Query efficiency \(costs and error 5.3 Mining association rules To evaluate the proposed algorithm for mining spatiotemporal association rules, we first artificially formulate 1000 association rules in the form \(r1,T,90 with 90% confidence i randomly picked from 10k ones, \(ii in at most one rule, and \(iii Then, at each of the following 100 timestamps, we assign 100k objects to the 10k regions following these rules. We execute our algorithms \(using 16 sketches these rules, and measure \(i  correct  rules divided by the total number of discovered rules, and \(ii successfully mined Figures 5.7a and 5.7b illustrate the precision and recall as a function of T respectively. Our algorithm has good precision \(close to 90 majority of the rules discovered are correct. The recall however, is relatively low for short T, but gradually increases \(90% for T=25 evaluated in the previous sections, the estimation error decreases as the query result becomes larger \(i.e., the case for higher T 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 precision HT 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 recall HT a b Figure 5.7: Efficiency of the mining algorithm Summary: The preliminary results justify the usefulness of our mining algorithm, whose efficiency improves as T increases Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 6. Conclusions While efficient aggregation is the objective of most spatio-temporal applications in practice, the existing solutions either incur prohibitive space consumption and query time, or are not able to return useful aggregate results due to the distinct counting problem. In this paper we propose the sketch index that integrates traditional approximate counting techniques with spatio-temporal indexes. Sketch indexes use a highly optimized query algorithm resulting in both smaller database size and faster query time. Our experiments show that while a sketch index consumes only a fraction of the space required for a conventional database, it can process 


required for a conventional database, it can process queries an order of magnitude faster with average relative error less than 15 While we chose to use FM sketches, our methodology can leverage any sketches allowing union operations Comparing the efficiency of different sketches constitutes a direction for future work, as well as further investigation of more sophisticated algorithms for mining association rules. For example, heuristics similar to those used for searching sketch indexes may be applied to improve the brute-force implementation ACKNOWLEDGEMENTS Yufei Tao and Dimitris Papadias were supported by grant HKUST 6197/02E from Hong Kong RGC. George Kollios, Jeffrey Considine and were Feifei Li supported by NSF CAREER IIS-0133825 and NSF IIS-0308213 grants References BKSS90] Beckmann, N., Kriegel, H., Schneider, R Seeger, B. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD, 1990 CDD+01] Chaudhuri, S., Das, G., Datar, M., Motwani R., Narasayya, V. Overcoming Limitations of Sampling for Aggregation Queries. ICDE 2001 CLKB04] Jeffrey Considine, Feifei Li, George Kollios John Byers. Approximate aggregation techniques for sensor databases. ICDE, 2004 CR94] Chen, C., Roussopoulos, N. Adaptive Selectivity Estimation Using Query Feedback. SIGMOD, 1994 FM85] Flajolet, P., Martin, G. Probabilistic Counting Algorithms for Data Base Applications JCSS, 32\(2 G84] Guttman, A. R-Trees: A Dynamic Index Structure for Spatial Searching. SIGMOD 1984 GAA03] Govindarajan, S., Agarwal, P., Arge, L. CRBTree: An Efficient Indexing Scheme for Range Aggregate Queries. ICDT, 2003 GGR03] Ganguly, S., Garofalakis, M., Rastogi, R Processing Set Expressions Over Continuous Update Streams. SIGMOD, 2003 HHW97] Hellerstein, J., Haas, P., Wang, H. Online Aggregation. SIGMOD, 1997 JL99] Jurgens, M., Lenz, H. PISA: Performance Models for Index Structures with and without Aggregated Data. SSDBM, 1999 LM01] Lazaridis, I., Mehrotra, S. Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure. SIGMOD 2001 PGF02] Palmer, C., Gibbons, P., Faloutsos, C. ANF A Fast and Scalable Tool for Data Mining in Massive Graphs. SIGKDD, 2002 PKZT01] Papadias,  D., Kalnis, P.,  Zhang, J., Tao, Y Efficient OLAP Operations in Spatial Data Warehouses. SSTD, 2001 PTKZ02] Papadias, D., Tao, Y., Kalnis, P., Zhang, J Indexing Spatio-Temporal Data Warehouses ICDE, 2002 SJLL00] Saltenis, S., Jensen, C., Leutenegger, S Lopez, M.A. Indexing the Positions of Continuously Moving Objects. SIGMOD 2000 SRF87] Sellis, T., Roussopoulos, N., Faloutsos, C The R+-tree: A Dynamic Index for MultiDimensional Objects. VLDB, 1987 TGIK02] Thaper, N., Guha, S., Indyk, P., Koudas, N Dynamic Multidimensional Histograms 


SIGMOD, 2002 Tiger] www.census.gov/geo/www/tiger TPS03] Tao, Y., Papadias, D., Sun, J. The TPR*Tree: An Optimized Spatio-Temporal Access Method for Predictive Queries. VLDB, 2003 TPZ02] Tao, Y., Papadias, D., Zhang, J. Aggregate Processing of Planar Points. EDBT, 2002 TSP03] Tao, Y., Sun, J., Papadias, D. Analysis of Predictive Spatio-Temporal Queries. TODS 28\(4 ZMT+01] Zhang, D., Markowetz, A., Tsotras, V Gunopulos, D., Seeger, B. Efficient Computation of Temporal Aggregates with Range Predicates. PODS, 2001 ZTG02] Zhang, D., Tsotras, V., Gunopulos, D Efficient Aggregation over Objects with Extent PODS, 2002 Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE pre></body></html 


