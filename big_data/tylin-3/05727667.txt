f g 
Xiaofeng Wang Jianbo Ou Xiaofeng Meng Yan Chen Renmin University of China zzuwxf,oujianbo,xfmeng,chenyan8 ruc.edu.cn 
244 
Abox Inference for Large Scale OWL-Lite Data 
Abox inference is an important part in OWL data management When involving large scale of instance data it can not be supported by existing inference engines In this paper we propose ef\002cient Abox inference algorithms for large scale OWL-Lite data The algorithms can be divided 
Abstract 
into two categories initial inference and incremental inference Initial inference is used in situation where only raw data exists in storage system and for this category we propose Rule Static Association Based RSAB Rule Dynamic Association Based RDAB and Rule Grouped-Sorted Based RGSB inference methods Incremental inference algorithm is used in situation where large volume inference data exists in storage system and for this category we extend the initial inference algorithm and propose Rule Pattern-Sharing Based\(RPSB method At last extensive 
experiments show that our methods are ef\002cient in practice 
O W eb Ontology Language is proposed by W3C and it is used to describe Web resource OWL-Lite is a sublanguage of OWL It was disigned for easy implementation and to provide users with a functional subset that will get them started in the OWL Inference for OWL-Lite can be divided into two categories Tbox and Abox Tbox inference concerns Ontology and generally can be supported by existing inference engines[6 7 12 Abox inference fo 
1 Introduction 
cuses on lar ge scale instance data and can not be supported by existing inference engines As the research of semantic web is becoming more and more popular,many semantic repositories have been  
Supported by the National Natural Science Foundation of China under Grant Nos.60073014 60273018 China National Basic Research and Development Program's Semantic Grid Project No 2003CB317000 the Key Project of Chinese Ministry of Education under Grant No.03044 Program for New Century Excellent Talents in University developed,which can be divided into three categories 
244 
based on the storage strategy they use RDB Relational Database 10 system-based[4 and  or the RDB has been studied e xtensively these years RDB-based systems are in the majority like DLDB-O WL[10 RStar[9 and so on Sesame provides a general storage interface and implements storage method on MySQL Oracle and so on Sesame aims at storage for RDF 002les and doesn't take the feature of OWL data into consideration.From experiment we observe that large number of rules are needed to express complete OWL semantics and loading process is also very inef 
002cient for doing complete inference based on such rules Therefore it can't be used to manage large scale OWL data DLDB-OWL uses MS Access as its persistent platform and uses inference engine F It declares high performance for large scale OWL data but has limited inference ability From experiment we observe that DLDBOWL cannot get any answers for some queries O is a typical memory-based system It supports more semantic rules than any other systems OWLim uses Sesame's general storage interface and has higher performance than 
Sesame's own memory-based storage module.But when do query and inference processing all data will be read from hard disk into memory Because OWLim supports most of the semantic rules in OWL Lite we use it as benchmark of query completeness in our experiment To support large scale semantic data management we develop HStar system which is built on 002le system and do query and inference processing on physical storage model In this paper we mainly concern the inference engine in HStar system This paper discusses Abox inference for large scale OWL-Lite data and divides the algorithms into two cat 
egories initial inference and incremental inference Initial inference is used in situation where only raw data exists in storage system Because inference about one single rule has high computation cost it is important to remove redundant rule inference From this point we propose Rule Static Association Based RSAB Rule Dynamic Association Based RDAB and Rule Grouped-Sorted Based 


Given the dataset D which is declared by users explicitly we call D the complete inference dataset after inference procedure if and only if it satis\002es the following condition no new data can be generated in D using any rule j j j f g f g onto Ins Ins onto onto Ins Ins n Ins x m Ins Ins n Ins x m 1 1 2 1 1 2 241 241 241 241 241 241 De\002nition 1 Complete Inference Dataset P P P P P P P P V V V P P P V V V RGSB inference algorithms.Compared to basic inference algorithm RSAB algorithm removes some redundant rule inference procedures But there still exist some redundant procedures RDAB takes data set and rules together into consideration any redundant procedures can be removed by this algorithm The whole Rules of OWL-Lite Abox inference has lattice-like relation,based on which we can group rules and sort such groups Do inferences based on GroupSorted Rules can make inference more focused and then reduce the number of data and pattern matching Incremental inference algorithm is used in situation where large volume inference data existing in storage system.So we propose Rule Pattern Sharing Based\(RPSB inference algorithm to reduce number of data and pattern matching Compared to old data new data is generally small in scale So buffering new data in the sharing patterns can avoid redundant inference and reduce number of data and pattern matching The contributions of this paper are 1 We summarize general Abox inference methods for large scale OWL-Lite data and divide the algorithms into two categories initial inference and incremental inference 2\or initial dataset we propose Rule Static Association Based RSAB Rule Dynamic Association Based RDAB and Rule Grouped-Sorted Based RGSB inference algorithms besides basic inference algorithm 3 We extend initial inference algorithm and design Pattern-Sharing Based incremental inference PSB algorithm to handle incremental inference 4 We present an extensive performance evaluation of different inference algorithms and analyze the results in detail 3.1 Basic Inference Method Before introducing any speci\002c inference algorithms 002rst we will explain some symbols used in this paper  the features of O WL-Lite according to the semantic of which we list rules related to Abox inference shown in table 1 The basic unit of rule is binary relation related to Ontology data and ternary relation related to Instance which are described by RDF syntax We call them rule pattern written by P whose formal format is P    S P O in which S P and O can be variables Rel\(Class Property Class Property in which Rel\(Class describes the relationship between Class Rel\(Property describes relationship between Property Rel\(Class Property describes the relationship between Class and Property Inference Rule in Abox can be formalized as below R         the part before symbol  is called rule premise and the part after symbol  is called rule conclusion If we use       to represent variable sets of      then the variables which belong to is in the subset of        To begin we give the de\002nition of complete inference dataset which de\002nes complete results that do not omit any inference procedures then we talk about basic rule inference at last we will explain rule association based inference algorithms Before introducing any inference methods,\002rst,we want to explain inference complete dataset,for it is critical to the 002nal results According to the above de\002nition we can get D from D using the rules shown in table 1 repeatedly We 002rst discuss inference methods focusing on single rule The rule premise consists of several patterns and the association between patterns is connected by the same variable For example the association between patterns a p b and b q c is connected by variable b Generally speaking there are two methods to compute all instance data from the rule premise One is to use join method and the other is navigation The former method is to match the pattern with the data then for each pattern we can get a result set at last these sets are joined to get the 002nal set The latter method is to match a certain pattern with the data then use its result set to query the pattern associated with it this 2 Preliminaries 3 Initial Abox Inference Algorithms 


De\002nition 2 Pattern Match i i i j i j j j i i Ins A Ins B i i i 0 241 241 J NULL the rule premise of R has been scanned once item e in D e doesn't exists in D in RuleSet hasNewData is FALSE a p b match x q y if and only if x is a variable or a=x and q is a variable or p=q and y is a variable or b=y conditional pattern match in WaitSet SingRuleReason D new generated data satis\002es conditional association rules WaitSet is NULL 1 2 1 2 3.2 Association Based Inference Methods f g f g\303 f g 303 f g f g 303 303 f g f g f g f g P P Query P P P P P Query P F ALSE R SingleRuleReason R  D P P R R R R R R R procedure continues until all the patterns have been handled In most cases,the navigation method is better than the join method.When many variables in the pattern are unknown,the join method would probably match all instances in the dataset,which is unaccepted for large scale data Based on de\002nition 1 we propose single rule inference algorithm using navigation method below I represents value sets of instance data matching the pattern    SingleRuleReason\(R,D  select rule R whose pattern has known element I  Select a pattern which has least unknown elements associated to  Substitute corresponding variables in with the value in instance sets I and obtain  J  delete corresponding data from I  combine I and J and get I,J    Substitute corresponding variable in rule conclusion of R with the value in instance set I and obtain result set D insert it into D return TRUE   return FALSE Based on Algorithm 1,it is easy to think of a method which do inference straightforward.When new data is generated by using certain rule in order to assure the completeness of the inference we need do inference for all rules over again for newly generated data may match the rule premise and the rule may lead to new data When we have no prior association in advance it is an effective way to do inference iteratively to ensure the completeness of the result   BasicReason\(D  BOOLEAN hasNewData  hasNewData    Algorithm 2 doesn't consider the association of rules so we may do redundant inference When a certain rule R generates new data which triggers other rules R  then R and R have association relationship If we can get all R and its association rules R  we only need to do inference on R to avoid redundant inference Above de\002nition ignores the case when pattern x p u matches pattern\(x type u for this kind of match is related to instance data we call it  If the rule conclusion of conditional matches the rule premise of  we call conditional association match  Based on the above discussion we can improve basic rule inference algorithm.The main idea is to provide a waiting rule set.In an iterative inference procedure if rule R generates new data,then we add rules that are associated to R to waiting rule set,after that we check whether new generated data satis\002es conditional association rules if so we add the rules to the waiting rule set too In next iterative inference procedure we do inference according to rules in the waiting rule set This method can avoid redundant inference Improved algorithm called Rule Static Association Based Inference RSAB is shown as Algorithm 3   ReasonAlgorithm  1\(D  Put all rules into WaitSet clear WaitSet  1 Add rules associated to to WaitSet  1 add rules to WaitSet  1    WaitSet  WaitSet  1 clear WaitSet  1  Algorithm 3 avoids some redundant inference procedures but not thorough.For example in table 2 new data generated by rule 1 can trigger rule 2 but when matching instance data the case is not always true Say rule 1 generates x type v  when it is applied to rule 2 we can get SubProperty\(type v x type v if SubProperty\(type v has no corresponding instance data rule 2 won't generate new inference procedure so we need not add rule 2 to WaitSet Determination of dynamic association between rules Algorithm 1 begin  repeat  if then  else  until foreach do  if then  end  Algorithm 2 begin  repeat  foreach do  until end  Algorithm 3 begin  repeat  foreach do  if then  if then  until end        


i i i i Ins Ins 1 4 Incremental Abox Inference Algorithms Algorithm 4 begin  repeat  foreach do  if then  if then  until end  Algorithm 5 begin  for to do  end  R R R i n R P P is similar to that of conditional association rule We use instance data to test if there exists matched data for nonmatched pattern.Now we give Rule Dynamic Association Based Inference algorithm shown as Algorithm 4   ReasonAlgorithm  2\(D  put all rules into WaitSet clear WaitSet  1 Add rules associated to which match instance data generated by Based on table 1 add rules to WaitSet  1    WaitSet  WaitSet  1 clear WaitSet  1  Algorithm 3 and Algorithm 4 describe association based inference algorithms which avoid redundant inference procedure by association relationship but do not consider inference order Let's come back to table 1 again and assume that there are only rule 5 6 and 9 if we do inference in such order  5,6 9   we can assure that rule 9 need only be e x ecuted once for newly generated data by rule 9 won't trigger rule 5 and rule 6 so we can put them in one group and do inference before rule 9 or else we should do inference for rule 9 repeatedly for example if we do inference in such order  5 9 6  and if rule 6 generates ne w data it will trigger rule 9  probably trigger rule 5 we need a new run of inference  5 9 6  we group rules in advance the inference iteration faces rule group not the whole rule sets.Figure 1 illustrates the difference between Rule Grouped-Sorted Based inference algorithm and Rule Association Based inference algorithm After the discussion we propose rule grouped-sorted based inference algorithm in Algorithm 5   ReasonAlgorithm  3\(D  assume the order after grouping is  R1 R2    Rn  do inference on rule set  using ReasonAlgorithm  2\(D  In section 3 we described Initial inference now let's consider another scenario inference complete data has already existed in storage system now new data comes and needs to be inserted into the system If we use the methods introduced in section 3 directly we will do redundant inference on old data repeatedly for those algorithms do not differentiate new data and old data When new data and old data coexist,obviously,the old data is inference complete and we want to avoid inference procedure on it But inference on new data may have relation with the old data for new data and old data probably match certain rule premise In 002gure 2,we show design pattern sharing structure we test each pattern on the new data if rules share the same pattern the times that rules match on new data can be greatly reduced Take rule 1,2,3,13 in table 1 for example in the rule premise  the common pattern is[\(x u y u y are v Box drawn with dashed line in 002gure 2 contain all the rule premise  we can see that rule 2 and rule 3 share the same pattern x u y When rules are represented in such structure we need only test the patters enclosed by the box in dashed line then we can complete the inference procedure along the pointer of the pattern.we propose Rule Pattern Sharing Based\(RPSB incremental inference algorithm shown as Algorithm 6 in WaitSet SingleRuleReason\(Ri,D new generated data satis\002es conditional association rules WaitSet is NULL f g  f g f g!f g!f g f g!f g!f g f g!f g f g 303 f g f g 


P P d P P P d P P P P m i x m i x x x x 1 1 in D in      matches new data exists in inference result   ReasonAlgorithm  4 D D  Assume the sharing patterns are       clear WaitSet;//waiting to be inferred Assign corresponding variables in and the pattern pointing to for each pattern pointing to query in new and old data and get inference result add association rules based on ReasonAlgorithm  2\(D to the WaitSet     We now experimentally evaluate the techniques presented in this paper First we present the performance of different algorithms in the initial dataset Second,In the case of incremental inference we compare the performance between RPSB and RGSB  We use de v eloped by LEHIGH university which provides test data and its corresponding Ontology and also de\002nes 14 typical queries re\003ecting OWL semantic features The details of the dataset will be discussed later Figure 3 and 002gure 4 separately depict the Class and Property hierarchy of the Benchmark All experiments were conducted on a Pentium IV 1.7GHz machine with 512M memory and 40 G hard disk  running the WindowXP We conducted our experiments on HStar system which is an extention of OrientX developed by Renmin university of China All experiments were repeated 10 times and the average processing time was calculated We design the following 6 queries in order to test the completeness of inference These queries cover all the inference rules  rdf:type Professor  rdf:type Student uri rdf:type  uri isFriendOf  uri hasSameHomeTownWith  uri love  In this experiment we use three instance datasets and their corresponding Ontology The number of triples and 002le size are shown in table 2   dataset  tripple numbers  002le size\(KB    Ontology  428  42    Instance 1  10694  992    Instance 2  19321  2483    Instance3  30181  4658   Table 2 Triple numbers and 002le size Because the basic inference algorithm is based on integrity de\002nition its results can be used as the standard sets table 3-5 shows the results of different inference algorithms on the above 6 queries from which we can make a conclusion that the initial inference algorithms are complete and correct.\(BI represents Basic Inference None represents No Inference   Query  None  BI  RSAC  RDAB  RGSB    Query-1  0  36  36  36  36    Query-2  0  666  666  666  666    Query-3  2  8  8  8  8    Query-4  2  5  5  5  5    Query-5  2  4  4  4  4    Query-6  1  1  1  1  1   242 242 Algorithm 6 begin  foreach do  foreach do  if then  if then  end  Dataset Environment Queries Query-1 Query-2 Query-3 Query-4 Query-5 Query-6 f g f g 5 Experiments 


Query  None  BI  RSAC  RDAB  RGSB    Query-1  0  44  44  44  44    Query-2  0  666  666  666  666    Query-3  2  18  18  18  18    Query-4  3  14  14  14  14    Query-5  5  70  70  70  70    Query-6  0  1  1  1  1   Table 4 completeness comparison on dataset 2   Query  None  BI  RSAC  RDAB  RGSB    Query-1  0  52  52  52  52    Query-2  0  666  666  666  666    Query-3  5  18  18  18  18    Query-4  9  730  730  730  730    Query-5  7  141  141  141  141    Query-6  0  18  18  18  18   Table 5 completeness comparison on dataset 3 Next we compare the ef\002ciency of the four inference algorithms 002gure 5 shows the performance of the different approaches RGSB performs best while the performance of basic inference declines sharply as the dataset increases At last we evaluate the performance two incremental inference algorithms which are RGSB and RPSB.We evaluate the performance from two aspects processing time and results completeness.We omit the completeness evaluation due to limited space Figure 6 is about the processing time between RGSB and RPSB Not surprisingly RPSB inference algorithm outperforms RGSB We have presented algorithms for Abox inference on large OWL-Lite data in initial dataset including basic inference RSAC RSAB and RGSB From the experiment we can see that these inference algorithms are complete among which RGSB performs best When large amount of inference data exists we provide incremental inference which not only ensure the inference completeness but also improve the ef\002ciency At last,extensive experiments show that our methods are ef\002cient in practice  Owl,web ontology language   Rdf,resource description frame w ork  2004  D O D M Atanas Kiryak o v  Owlim a pragmatic semantic repository for owl  2005  P  G Da vid W ood and T  Adams K o w ari A platform for semantic web storage and analysis  2005  V  Haarsle v and R Moller  High performance reasoning with very large knowledge bases A practical case study  pages 161\226166 2001  V  Haarsle v and R R Moller  A core inference engine for the semantic web  pages 27\22636 2003  I Horrocks The f act system  pages 27\22630 1998  A K J Broekstra and F  Harmelen Sesame A generic architecture for storing and querying rdf and rdf schema  pages 54\22668 2002  Y  P  L Z Li Ma Zhong Su and T  Liu Rstar An rdf storage and query system for enterprise resource management  2004  Z P an and He\003in J dldb Extending relational databases to support semantic web queries  2003  Z P  Y  Guo and J He\002n An e v aluation of kno wledge base systems for large owl datasets  T  F  Y ouyong Zou and H Chen F-o wl an inference engine for the semantic web  pages 238\226248 2004 Table 3 completeness comparison on dataset 1   6 Conclusion References http://www.w3.org/2004/OWL http://www.w3.org/RDF In Proc of Int Workshop on Scalable Semantic Web Knowledge Base Systems In WWW In B Nebel editor Proceedings of the Seventeenth International Joint Conference on Arti\002cial Intelligence In Workshop on Evaluation on Ontologybased Tools In Automated Reasoning with Analytic Tableaux and Related Methods International Conference In Proc of the 1st International Semantic Web Conference\(ISWC In CIKM In Workshop on Practical and Scalable Semantic Systems In Book Formal Approaches to AgentBased Systems 


Colinbowfive Research November 2000 J.S Shirabad T.C Lethbridge S Matwin 223Supporting Software Maintenance by Mining Software Update Records\224 Irzfe\221riiaiionui Cvizfeerence vn sofhvare Muinrenriiice ICSM\222OI November 2001 J Man and M Kamber Data Mining Conceprs atid Techniques Morgan KaUfnlaM 2000 8 Demeyer S Ducasse 0 Nierstrasz Object Oriented ReenggUreerIng Pultcmns Morgan Kaufmann 2003 liltD://www.xfig.org J Martin K Wong B Winter H.A.Miillcr 223Analyzing Xfig Using the Rigi Tool Suite\224 The Sevmth Working Uonjerence un Reverse Eqimeriq WCRE\221OU November 2000 Acknowledgement Wc would like to thank the Lahore University of Management Sciences for providing funding for this research Thanks also to Ash Qureshi for his help in the initial phascs of this work 395 


camp?on?the strongest?cell nR=1 Greedy Alg \(nR=1 b Figure 3: Performance of macroscopic scheduler versus transmit power at nR = 1, 2, 4. Path loss exponent = 3.5. ?L = ?4dB users to one base station even if the user is closer to another base station \(unless the user is very far away from the base station interference cancellation could be effective only for users associated with the base station. This is illustrated in figure 4 Figure 3\(b utility function at nR = 1, 2, 4. Similarly, significant gains are observed at all nR 6 Conclusions In this paper, we consider a generic multi-cell system with K client users \(each with single antenna each with nR antennas and multiuser detector time, a user could be associated with one base station only. Encoding and decoding terminates at the base station level. The problem is to find the optimal dynamic user association with the nB base Figure 4: Illustration of the advantage to associate users with one base station. It is better to associate mobile 1, 2, 3 to base station A \(even if mobile 3 is closer to base station B mobile 4 should be associated with base station B because it is very close to the base station stations such that the system performance is optimized. We proposed an analytical framework for the above macroscopic scheduling problem and introduce a greedy sub-optimal solution. We show that when multi-user detection is incorporated in base stations, the conventional camp-on-the-strongest-cell approach is far from optimal. For example, we illustrated that there are 4 time of throughput gain between the proposed greedy algorithm and the conventional algorithm at various nR. This is because with multi-user detection it is advantageous to associate users to a common base station due to interference cancellation unless the user is very far away from the base station. We found that the scheduling gain increases with decreasing path loss exponent because of the increase in the effective overlapping area between cells References 1] S. Verdu  The Capacity Region of the Symbol-Asynchronous Gaussian Multiple-Access Channel  IEEE Transactions on Information Theory., pp. 733  751, July 1989 2] G. J. Foschini and M. J. Gans  On the limits of wireless communications in a fading environment  Wireless Personal Communications., pp. 315  335, Nov 1998 3] J. M. Torrance and L. Hanzo  Latency Considerations for Adaptive Modulation in an Interference-Free Slow Rayleigh Fading Channel  Proc. VTC  97, Phoenix, USA, June 1997 4] T. M. Cover and J. A. Thomas, Elements of Information Theory John Wiley and Sons, second ed., 1991 5] T. S. Rappaport, Wireless Communications - Principles and Practice. Prentice Hall, second ed., 2002 WCNC 2004 / IEEE Communications Society 564 0-7803-8344-3/04/$20.00  2004 IEEE pre></body></html 


CBA and Neural Networks at 1% confidence level while they are all significantly better than C4.5 decision tree. Taking the interpretability of classification model into account, these two adapted CBA algorithm seem to be appropriate choices for credit scoring because they generated much more compact decision lists \(less sequential rules original CBA. They therefore favour the well-known Occam  s Razor theory and are more suitable for decision makers to understand. A deeper insight into the rules structures shows that original CBA and adapted CBA 1 both focus on generating classification rules that predict good clients \(with bad clients as the default class implication, numerous rules with high confidence but low support have lower ranks than they are in original CBA. These rules are finally discarded since they are not fired by any training samples, which are matched by these rules with higher intensity of implications thus making the decision lists generated by adapted CBA 1 more compact. Adapted CBA 2 mainly mines these classification rules for bad clients \(with good clients as the default class compact rule sets Original CBA Adapted CBA1 Adapted CBA2 C45 NN dataset accuracy no. of rules accuracy num. of rules accuracy no. of rules accuracy accuracy 1 Austr 85.65% 130.5 86.52% 26.4 86.96% 12.4 86.52% 85.07 2 Germ 73.30% 134 74.40% 56.5 73.20% 19.7 72.40% 74.90 3 Bene 72.92% 393 72.30% 186 73.51% 51 70.21% 72.63 Table 2. Experiment results Proceedings of the 2005 IEEE International Conference on e-Business Engineering \(ICEBE  05 0-7695-2430-3/05 $20.00  2005 IEEE In addition, decision makers in financial institution certainly pay more attentions to those rules that predict bad clients, which will be extraordinary costly if they are regarded as good ones 5. Conclusion Intensity of implication is proposed in the beginning as an interestingness measure for association rules Another novel interestingness measure called dilated chi-square is designed by us to reveal the statistical interdependence between the antecedents and consequents of association rules We then adapt CBA algorithm, which can be used to build classifiers based on class association rules, by coupling it with intensity of implication and dilated chi-square respectively. More concretely, Intensity of implication \(or dilated chi-square primary criterion to rank class association rules at the first step of the database coverage pruning procedure in CBA algorithm. Experiments on three credit scoring datasets proved that these two adapted algorithms compared with original CBA, classical C4.5 decision tree and neural network, achieve satisfactory performance and generates classifiers much more compact than CBA 6. Acknowledgement The work was partly supported by National Natural Science Foundation of China \(70231010/70321001 7. References 1] Wang, K. and S. Zhou, Growing decision trees on support-less association rules. in KDD'00. 2000. Boston,MA 2] Liu, B., W. Hsu, and Y. Ma, Integrating Classification and Association Rule Mining. in the 4th International Conference on Discovery and Data Mining. 1998. New 


Conference on Discovery and Data Mining. 1998. New York,U.S 3] Dong, G., et al, CAEP:Classification by aggregating emerging patterns. in 2nd International Conference on Discovery Science,\(DS'99 Artificial Intelligence. 1999. Tokyo,Japan: Springer-Verlag 4] Liu, W., J. Han, and J. Pei, CMAR: Accurate and efficient classification based on multiple class-association rules. in ICDM'01. 2001. San Jose, CA 5] Yin, X. and J. Han, CPAR:Classification based on predictive association rules. in 2003 SIAM International Conference on Data Mining \(SDM'03 Fransisco,CA 6] Agrawal, R. and R. Srikant, Fast algorithm for mining association rules. in the 20th International Conference on Very Large Data Bases. 1994. Santiago,Chile 7] Agrawal, R., T. Imielinski, and A. Swami, Mining association rules between sets of items in large databases. in the ACM SIGMOID Conference on Management of Data 1993. Washington,D.C 8] Guillaume, S., F. Guillet, and J. Philippe, Improving the discovery of association rules with intensity of implication Principles of Data Mining and Knowledge Discovery, 1998 1510: p. 318-327 9] Janssens, D., et al, Adapting the CBA-algorithm by means of intensity of implication. in the First International Conference on Fuzzy Information Processing Theories and Applications. 2003. Beijing, China 10] Gras, G. and A. Lahrer, L'implication statistique: une nouvelle methode d'analysis de donnees. Mathematiques Informatique et Sciences Humaines n 20, 1993 11] Suzuki, E. and Y. Kodratoff, Discovery of  surprising exception rules based on intensity of implication. in PKDD'98. 1998. Berlin: Springer 12] Mills, F., Statistical Methods. 1955: Pitman 13] Lan, Y., et al, Dilated Chi-square: A novel interestingness measure to build accurate and compact decision list. in International conference on intelligent information processing. 2004. Beijing,China 14] Quinlan, J.R., C4.5 programs for machine learning 1993: Morgan Kaufmann 15] Witten, I.H. and E. Frank, Data Mining: practical machine learning tools and techniques with Java implementations. 2000: Morgan Kaufmann, San Francisco 16] Fayyad, U.M. and K.B. Irani, Multi-interval discretization of continuous valued attributes for classification learning. in the Thirteenth International Joint Conference on Artificial Intelligence \(IJCAI Chambery,France: Morgan Kaufmann 17] Blake, C.L. and C.J. Merz, UCI repository of machine learning databases http://www.ics.uci.edu/~mlearn/mlrepository.htm]. 1998 Irvine,CA:University of California, Dept. of Information and Computor Science 18] Dietterich, T.G., Approximate statistical tests for comparing supervised classification learning algorithms Neural Computation, 1998. 10\(7 Proceedings of the 2005 IEEE International Conference on e-Business Engineering \(ICEBE  05 0-7695-2430-3/05 $20.00  2005 IEEE pre></body></html 


absolute values. The results can vary on other computers. But it can be guaranteed that performance ratio of the algorithms will remain the same After making the comparisons with sample data, we came to the conclusion that PD algorithm performs significantly better than the other two especially with larger datasets. PD outperforms DCP and PIP regarding running time. On the other hand, since PD reduces the dataset, mining time does not necessary increase as the number of transactions increases and experiments reveals that PD has better scalability than DCP and PIP. So, PD has the ability to handle the large data mine in practical field like market basket analysis and medical report documents mining 5. References 1] R. Agrawal and R. Srikant, "Fast algoritlnns for mining association rules", VLDB'94, pp. 487-499 2] R. J. Bayardo, "Efficiently mining long patterns from databases", SIGMOD'98, pp.85-93 3] J. Pei, J. Han, and R. Mao, "CLOSET: An Efficient Algorithm for Mining Frequent Closed Itemsets \(PDF Proc. 2000 ACM-SIGMOD International Workshop on Data Mining and Knowledge Discovery, Dallas, TX, May 2000 4] Qinghua Zou, Henry Chiu, Wesley Chu, David Johnson, "Using Pattern Decomposition\( PD Finding All Frequent Patterns in Large Datasets", Computer Science Department University of California - Los Angeles 5] J. Han, J. Pei, and Y. Yin, "Mining Frequent Patterns without Candidate Generation \(PDF  SIGMOD International Con! on Management of Data SIGMOD'OOj, Dallas, TX, May 2000 6] S. Orlando, P. Palmerini, and R. Perego, "The DCP algoritlnn for Frequent Set Counting", Technical Report CS2001-7, Dip. di Informatica, Universita di Venezia 2001.Available at http://www.dsi.unive.itl?orlando/TR017.pdf 7] MD. Mamun-Or-Rashid, MD.Rezaul Karim, "Predictive item pruning FP-tree algoritlnn", The Dhaka University  Journal of Science, VOL. 52, NO. 1, October,2003, pp. 3946 8] Park, J. S., Chen, M.-S., and Yu, P. S, "An Effective Hash Based Algoritlnn for Mining Association Rules", Proc ofthe 1995 ACM-SIGMOD Con! on Management of Data 175-186 9] Brin, S., Motwani, R., Ullman, J., and Tsur, S, "Dynamic Itemset Counting and Implication Rules for Market Basket Data", In Proc. of the 1997 ACM-SIGMOD Conf On Management of Data, 255-264 10] Zaki, M. J., Parthasarathy, S., Ogihara, M., and Li, W New Algoritlnns for Fast Discovery of Association Rules In Proc. of the Third Int'l Con! on Knowledge Discovery in Databases and Data Mining, 283-286 11] Lin, D.-I and Kedem, Z. M., "Pincer-Search: A New Algoritlnn for Discovering the Maximum Frequent Set", In Proc. of the Sixth European Conf on Extending DatabaseTechnology, 1998 12] R. Ramakrishnan, Database Management Systems University of Wisconsin, Madison, WI, USA; International Edition 1998 pre></body></html 


tors such as union, di?erence and intersection are de?ned for pairs of classes of the same pattern type Renaming. Similarly to the relational context, we consider a renaming operator ? that takes a class and a renaming function and changes the names of the pattern attributes according to the speci?ed function Projection. The projection operator allows one to reduce the structure and the measures of the input patterns by projecting out some components. The new expression is obtained by projecting the formula de?ning the expression over the remaining attributes [12 Note that no projection is de?ned over the data source since in this case the structure and the measures would have to be recomputed Let c be a class of pattern type pt. Let ls be a non empty list of attributes appearing in pt.Structure and lm a list of attributes appearing in pt.Measure. Then the projection operator is de?ned as follows ls,lm c id s m f p ? c, p = \(pid, s, d,m, f In the previous de?nition, id ing new pids for patterns, ?mlm\(m projection of the measure component and ?sls\(s ned as follows: \(i s usual relational projection; \(ii sls\(s and removing the rest from set elements. The last component ?ls?lm\(f computed in certain cases, when the theory over which the formula is constructed admits projection. This happens for example for the polynomial constraint theory 12 Selection. The selection operator allows one to select the patterns belonging to one class that satisfy a certain predicate, involving any possible pattern component, chosen among the ones presented in Section 5.1.1 Let c be a class of pattern type pt. Let pr be a predicate. Then, the selection operator is de?ned as follows pr\(c p Join. The join operation provides a way to combine patterns belonging to two di?erent classes according to a join predicate and a composition function speci?ed by the user Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE Let c1 and c2 be two classes over two pattern types pt1 and pt2. A join predicate F is any predicate de?ned over a component of patterns in c1 and a component of patterns in c2. A composition function c pattern types pt1 and pt2 is a 4-tuple of functions c cStructureSchema, cDataSchema, cMeasureSchema, cFormula one for each pattern component. For example, function cStructureSchema takes as input two structure values of the right type and returns a new structure value, for a possible new pattern type, generated by the join. Functions for the other pattern components are similarly de?ned. Given two patterns p1 = \(pid1, s1, d1,m1, f1 p2 = \(pid2, s2, d2,m2, f2 p1, p2 ned as the pattern p with the following components Structure : cStructureSchema\(s1, s2 Data : cDataSchema\(d1, d2 Measure : cMeasureSchema\(m1,m2 Formula : cformula\(f1, f2 The join of c1 and c2 with respect to the join predicate F and the composition function c, denoted by c 1   F  c  c 2   i s  n o w  d e  n e d  a s  f o l l o w s    F  c  c 2     c  p 1   p 2   p 1    c 1  p 2    c 2  F   p 1   p 2     t r u e   5.1.3. Cross-over database operators OCD Drill-Through. The drill-through operator allows one to 


Drill-Through. The drill-through operator allows one to navigate from the pattern layer to the raw data layer Thus it takes as input a pattern class and it returns a raw data set. More formally, let c be a class of pattern type pt and let d be an instance of the data schema ds of pt. Then, the drill-through operator is denoted by c c Data-covering. Given a pattern p and a dataset D sometimes it is important to determine whether the pattern represents it or not. In other words, we wish to determine the subset S of D represented by p \(p can also be selected by some query the formula as a query on the dataset. Let p be a pattern, possibly selected by using query language operators, and D a dataset with schema \(a1, ..., an ible with the source schema of p. The data-covering operator, denoted by ?d\(p,D responding to all tuples in D represented by p. More formally d\(p,D t.a1, ..., t.an In the previous expression, t.ai denotes a speci?c component of tuple t belonging to D and p.formula\(t.a1, ..., t.an instantiated by replacing each variable corresponding to a pattern data component with values of the considered tuple t Note that, since the drill-though operator uses the intermediate mapping and the data covering operator uses the formula, the covering ?\(p,D D = ?\(p not be equal to D. This is due to the approximating nature of the pattern formula 5.1.4. Cross-over pattern base operators OCP Pattern-covering. Sometimes it can be useful to have an operator that, given a class of patterns and a dataset, returns all patterns in the class representing that dataset \(a sort of inverse data-covering operation Let c be a pattern class and D a dataset with schema a1, ..., an pattern type. The pattern-covering operator, denoted as ?p\(c,D all patterns in c representing D. More formally p\(c,D t.a1, ..., t.an true Note that: ?p\(c,D p,D 6. Related Work Although signi?cant e?ort has been invested in extending database models to deal with patterns, no coherent approach has been proposed and convincingly implemented for a generic model There exist several standardization e?orts for modeling patterns, like the Predictive Model Markup Language \(PMML  eling approach, the ISO SQL/MM standard [2], which is SQL-based, and the Common Warehouse Model CWM  ing e?ort. Also, the Java Data Mining API \(JDMAPI 3] addresses the need for a language-based management of patterns. Although these approaches try to represent a wide range of data mining result, the theoretical background of these frameworks is not clear. Most importantly, though, they do not provide a generic model capable of handling arbitrary cases of pattern types; on the contrary only a given list of prede?ned pattern types is supported To our knowledge, research has not dealt with the issue of pattern management per se, but, at best, with peripheral proximate problems. For example, the paper by Ganti et. al. [9] deals with the measurement 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


