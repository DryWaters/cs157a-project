A SOM and Bayesian Network Architecture for Alert Filtering in Network Intrusion Detection Systems Ahmad Faour Laboratoire LITIS INSA Rouen France e-mail ahmad.faour@ul.edu.lb Philippe Leray Laboratoire LITIS INSA Rouen France e-mail philippe.leray@insa-rouen.fr Bassam Eter Laboratoire LPM Univ Libantaise Beyrouth Liban e-mail beter@ul.edu.lb Abstract With the ever growing deployment of networks and the Internet the inmportance of network security has increased Recently however systems that detect irntrusions which are inmportant irn security countermeasures have been unable to provide proper analysis or an effective defense mechaniism Instead they have overwhelmed human operators with a large 
volume of irntrusion detection alerts This paper presents a new approach for handlirng irntrusion detection alarms more efficieintly We propose here an architecture for automated alarm filterirng based on classical method of clusterirng Self-Organrizirng Maps coupled with probabilistic graphical model Bayesian belief networks for determirinirg if the network is really attacked Keywords Network Security Intrusion Detection Clusterirng Bayesian Networks and Alarms Filterirng 1 Introduction With the ever growing deployment of networks and the Internet the importance of network security has increased Over the past ten years the number as well as the severity of network-based 
computer attacks have significantly increased 1 As a consequence classic computer security technologies such as authentication and cryptography have gained in importance Simultaneously intrusion detection has emerged as a new and potent approach to protect computer systems 3 12 In this approach so-called Intrusion Detection Systems IDSs are used to monitor and analyze the events occurring in a computer system The principal problem which discover the administrators when they install the IDS is the significant quantity of generated alarms On a network of average size one or two public classes C several thousands of 
alarms are generated daily making almost impossible the analysis of the results The consequence is that the administrator is obliged to seriously re-examine with the rise his tolerance level which will lead it to pass beside many real problems This problem known as false positives is a big barrier for intrusion detection tools to cross before their deployment can be practical In this paper we suggest an approach for using data mining technology in the intrusion detection area We claim that the best positioning for a data mining technology within an intrusion detection system is not as a detection engine but 
rather as an analysis layer that will filter out the false positives The outcome for our research was an automatic process that consists of two steps The first step aims at grouping similar standard behavior for external machines with destination to internal machines using clustering algorithms like self-organizing maps 18 The second step uses these information for determining if the network is really attacked and filter out the false positives using probabilistic tools like Bayesian Networks 16 The outline of this paper is as follows Section 2 discusses related works Section 3 describes our approach Section 4 shows the experiments and the obtained results and finally section 5 concludes and 
presents our future works and perspectives 2 Related work The intuitively most appealing way of dealing with false positives is to build better IDSs which trigger less false positives This is a challenging endeavor because false positives are the result of multiple problems Examples of IDSs that are less prone to false positives include the embedded detectors technology by 31 a lightweight tool for detecting Web server attacks by 2 and a network-based IDS that focuses exclusively on low-level network attacks 27 Alarm correlation systems 10 11 13 29 30 try to group alarms so that the alarms of the same group 
pertain to the same type e.g the same attack In that way they offer a more condensed view on the security issues raised by an IDS 17 Other researches use data mining approaches in order to discover association rules over alarm bursts 21 Subsequently alarms that are consistent with these association rules are deemed normal and get discarded The risk of discarding true positives is not considered in this work whereas bounding this risk is central to our approach concerning the use of Bayesian networks for higher level analysis system resources logs of users let us cite the work of 
19 23 25 26 4 5 6 use the naive Bayesian network and decision tree for intrusion detection The data used in these works is a set of benchmark data from KDD'99 which are appropriate to evaluate an intrusion detection system For the temporal analysis of attacks 28 uses a probabilistic graphical model temporal tree 8 implements other probabilistic graphical models very close to dynamic Bayesian networks Hidden Markov models Closer to our work i.e to rather propose a postprocessing with a current NIDS 7 use a very simple bayesian decision rule on the logs of two NIDS SNORT and Shadow 0-7803-9521-2/06/$20.00 2472006 IEEE 3175 


Figure 1 General description of our approach 3 Our approach 3.1 General functionality The goal of our system is to start from the alarms generated by a NIDS and to try to filter these alarms to determine if there were an attack on the network during a fixed lapse of time Our system is composed of three components detailed in figure 1 14 In the first step temporal preprocessing we start by making a synthesis of the behavior of all the external machines to all the internal machines in a fixed temporal window Starting from the principle that this behavior can be similar for several external machines which would try the same kind of attack towards the same internal machine or for several internal machines we thus will group these behaviors in a given number of behavior types spatial preprocessing by using a Self-Organizing Map SOM an usual technique of unsupervised classification proposed by 18 This information is then used to determine if a given internal computer is really attacked classification 3.2 Temporal preprocessing An attack is often characterized by a series of consecutive events trying to violate the security policy of a machine or a network To detect such a scenario we place ourselves in a pseudo-reality detection mode by making a synthesis of the various types of alarms generated by the NIDS during a mobile window of time for every couple IPext Pint The length and the offset of the window were determined by an expert of the domain to have a good compromise between the minimal duration necessary to detect potential scenarios of attacks and a maximum duration beyond which the system is drowned by alarms Starting from the log issued by the NIDS we calculate for the considered window of time the number of alarms of each type for each value of the couple IPint  IPext We do not take account the value of the external port considered to be nonsignificant by our expert nor the internal port or the protocol very correlated with the type of alarm generated by the NIDS The logs we used in our experiments contain 406 different types of alerts generated by SNORT 24 At the end of this phase we obtain a summary window Pext IPint Nalert1  Nalert4O6 of the behavior of each internal machine in our network gathered for each external machine in connection To take account of the average traffic of each internal machine we propose to normalize the data First we calculate the average of every type of alert for every alert type t\(i,..T i as shown by the following formula nNalert-type\(wi'nd\(n t i average alert\(t i  N 1 where N is the number of mobile windows in which IPint is connected T is the number of type of alerts and I is the number of IPint Consequently we can then divide every alert\(t i wind\(n by the calculated average 3.3 Spatial preprocessing 3.3 1.Alarm Priority Given that the various types of alarms issued by the NIDS are not all on the same level of importance and the most indicative alarms are in general the least frequent we propose to classify these alarms under 3 levels of priority low medium and high With each level we associate a coefficient of priority in way to distinguish significant alarms from the others The coefficients of priority are indicated by an expert of the domain and associated with a weighting coefficient 3.3 2.Clustering In this part we work starting from the number of alarms of each type generated for each couple IPext IPint by supposing that this vector is a representative behavior of each IPexternal in connection to each Pinternai On the basis of the principle that this behavior can be similar for several external machines connecting to one or more internal machines we used a technique of unsupervised classification to cluster these behaviors in a given number of behavior-types We used a self-organizing map 18 as clustering algorithm In general clustering seeks to group objects so that the objects within a given group/cluster are similar whereas the objects of different group/cluster are dissimilar SOMs accomplish two things they reduce dimensions and display similarities The way that SOMs go about organizing themselves is by competition for representation of the samples Neurons are also allowed to change themselves by learning to become more like samples in hopes of winning the next competition The first step in constructing a SOM is to initialize the weight vectors From there you select a sample vector randomly and search the map of weight vectors to find which weight best represents that sample Since each weight vector has a location it also has neighboring weights that are close to it The weight that is chosen is rewarded by being able to become more like that randomly selected sample vector The quality of clustering using SOM are mainly influenced by three essential parameters 1 size of the map 2 initialization of weights codebook vectors and 3 neighborhood functions In this study 0-7803-9521-2/06/$20.00 2472006 IEEE 3176 


we used the Davies-Bouldin index DB as clustering validation measure This index is given by the following formula DB n a Sn\(Qi  Sn\(Qj 2 DR max 2 n iiA S\(QiQJ 1 where n is the number of clusters Sn average distance between the center of each cluster and the objects within this cluster S\(QiQj intra-cluster distance Consequently the ratio is small if the clusters are compact and far one from the other The DB index will have a small value for a good clustering After the choice of the best SOM we proceed according to two approaches In the first approach Approachl we calculate the distance between every vector obtained in the previous step and every cluster obtained in the map This distance is in fact the distance between this vector and the codebook vector for each cluster in the map At the end of this approach the behavior of each IPext in target with each IPint in a temporal window is represented by the distance vector calculated between this vector and the behavior-types clusters Consequently for every IPint we calculate the sum of the distances between the vectors that contain IPint and each of clusters We can thus make a synthesis window Pint dist2clust1  dist2clustn of the behavior-types clust1  clustn detected for each IPint independently of Pext to be able to compare the profile of two different internal machines In the second approach Approach2 and for every IPint we made a synthesis of the number of standard behaviors detected bound for this IPint in a temporal window We obtain a synthesis window IPint NBofclust  NBofclustn of the behavior-types clust1  clustn detected for each IPint independently of Pext to be able to compare the profile of two different internal machines 3.4 Classification The synthesis of the standard behaviors calculated for every IPint is supposed to be representative of the various types of potential attacks aiming each internal machine of the network during a fixed window of time We propose in this last phase to use these information to determine if the network were really attacked ATT  true or false To implement this task of classification we will restrict ourselves with Bayesian network models 22 20 Bayesian networks are tools to reason with uncertain information in the probability theory framework They use direct acyclic graphs to represent causal relations and conditional probabilities of each node given its parents to express uncertainty of causal relations The structure of the Bayesian network is fixed in advance like the Naive Bayesian network or given by an expert of domain or learned from data Similarly the conditional probabilities can be obtained from an expert or learned from data In this work we implemented different kinds of Bayesian networks for the two experts These structures are  Naive Bayesian Network NB  Tree Augmented Naive Bayesian Network TANB  Maximum Weighted Spanned Tree MWST  Multinet Naive Bayesian Networks NB 15 are very simple Bayesian networks which are composed of DAGs with only one parent representing the unobserved node and several children corresponding to observed nodes with the strong assumption of independence among child nodes in the context of their parent This model gave excellent results in many fields However such an assumption is not always valid and can sully the results The TANB 15 is another structure similar to the NB but more elaborate because it does not propose any independence between the child nodes The class node has no parents and the child nodes have at maximum two parents including the class node The third structure MWST proposed by Chow and Liu 9 This method seeks to find a maximum weighted spanning tree For the TANB and MWST the structure are completely learned from data For the parameters conditional probabilities we assume that every node is a continuous node that has a Gaussian\(i.e normal probability distribution which supposes a continued values of attributes We consider the characteristic vector of each IPinterne as input to determine the value of class normal or attacks as the maximum a posteriori of the class node probability conditionnally to the observation of all the attributes nodes This probability is directly obtained by using a classical inference algorithm in the network For the fourth kind of Bayesian network Multinet we created two Bayesian networks using the MWST algorithm The first is created from normal data and the other from abnormal data As the class node is not explicitely defined we calculate the maximum a posteriori MAP of the class probability by using the following formula P\(C  cjlA c P\(AIC  ci x P\(C  ci 3 where P\(A C  ci is the likelihood of the evidence A calculated for each structure normal and abnormal and P\(C  ci is the prior probability of each class 4 Experiments and results 4.1 Description of the Data We work starting from log files issued from NIDS SNORT These files include 32031 alarms generated over a duration of 20 days from 20/11/2004 to 10/12/2004 These alarms correspond to 4638 external machines trying to connect 288 internal machines Alarms generated during these 20 days are of 406 different types and they include 16 real attacks scenarios some are of a few minutes others lasting several days and the other are for normal scenario The scenarios of attack are distributed as  4 scenarios brute force on POP3  3 scenarios crawler Web  2 scenarios Web IIS 0-7803-9521-2/06/$20.00 2472006 IEEE 3177 


 2 scenarios scanner of vulnerability  1 scenario IIS attack against apache server  3 scenarios brute force against FTP server  1 scenario SNMP attack After implementing the temporal pre-processing phase we decomposed the base in two bases learning base and testing base The learning base contains 10 scenarios of attacks and the testing base contains six scenarios of attacks 4.2 Spatial preprocessing As mentioned above the data used for the experiments in this phase are characteristic vectors summarizing the profile of every couple IPext Pint in a fixed window of time We conducted a series of experiments in three stages 1 Normalization in this stage we made two tests one with normalization and other without normalization 2 Weighting as mentioned in the section 3.3 J.we classified the alerts with three level of importance and for every level we integrated a weighting coefficient Let a b and c be these three coefficients we made here four tests  Without weighting a 1 b 1 and c 1  Weighting level 1 a  1 b 2 and c 3  Weighting level 2 a  1 b  0 and c 10  Weighting level 3 a  1 b 10 and c 100 3 SOM parameters  Size we used several sizes 5*5 10*10 15*15 20*20 and 25*25  Initialization linear and random  Neighborhood function gaussian cutgauss ep and buble1 The first results obtained in this phase show that the best results correspond to un-normalized data with weighting level 1 and a map of size 5*5 with linear initialization and gaussian neighborhood function By admitting that the codebook vector of each cluster is representative of the data projected in this cluster we can try to interpret the map by determining the most significant variables in each cluster for instance the first top 5 variables As each variable corresponds to one specific alarm we obtain the characteristic alarms of each cluster Table 1 gives the top 5 significative alerts for two clusters 5 and 22 The scenarios 13 and 15 are two web attacks against an IIS server and Apache server The majority of points for these two scenarios are projected in the cluster 5 As we can see in the table 1 the first three top alerts of the cluster 5 are characteristics of Web attacks The scenario 14 is a brute force against POP3 server The majority of points are projected in the cluster 22 The first 1We admitted here the same conventions used mem bin in SOMTOOLBOX implemented in Matlab Table 1 Top 5 alarms of clusters 5 and 22 Alert rank  Cluster 5 1 WEB-IIS FTP anonymous login attempt Alert rank Cluster 22 1 User incorrect POP 2 WEB-MISC http directory traversal 3 VIRUS exe file attachment 4 VIRUS pif file attachment 5 VIRUS bat file attachment top alert in cluster 22 is User tncorrect POP which is a sign of POP attack 4.3 Classification We could directly use this map to perform a supervised classification by making the following assumption every data projected in a cluster is classified as the more frequent situation also projected in the same location during the learning phase With this strong assumption we obtained the following results a 84 of attack scenarios are mapped in relevant attack clusters and consequently are well detected b 85 of normal scenarios are mapped in normal clusters 15 false positives and consequently are filtered out These results show us that the SOM clustering is able to regroup similar data and consequently to discover some interesting codebooks representative to attack scenarios or normal situations But this clustering do not gives us performances that will be expected by an network security expert For this reason we think that the last stage of our architecture the classification phase is quite important As mentioned in the previous section  we led a serie of experiments on Bayesian networks for two approaches Approachl and Approach2 Tables 2 and 3 show the obtained results for the two approaches The hit rate is the percent of attacks classified as attacks the filtered alarms is the percent of normal data classified as normals and consequently filtered out and the PCC is the overall percent of correct classification The PCC is not very significant in the problem of intrusion detection because in the majority of cases the normal data are much more numerous than the attacked data for example in our case the normal data constitute 99 of the whole of data Thus the ideal system of filtering is not that it gets higher PCC but that can detect all the true attacks hit rate=100 and at the same time filter out most of all the normal alarms alarm filtered-_ 100 or false positive 0 Our results show that the Multirnet network is the most suitable for intrusion detection alarms filtering when it is applied for the second approach Expert2 Note that on these kind of structures Multinet the prior probability of Class is very important in special on the cases where the two like0-7803-9521-2/06/$20.00 2472006 IEEE 3178 access 2 WEB-FRONTPAGE shtml.dll access 3 WEB-FRONTPAGE  request 4 ATTACK-RESPONSES 403 Forbidden 5 POLICY 


Table 2 Results obtained for different structures of BN applied to the first approach Approachl PCC stands for Percent of Correct Classification Algorithm Hit Rate Filtered Alarms PCC Naive 46 88 87.44 MWST 36 97 96.18 TANB 74 89 88.80 Multinet 0 100 99.99 Table 3 Results obtained for different structures of BN applied to the second approach Approach2 Algorithm Hit Rate Filtered Alarms PCC Naive 62 88 87.65 MWST 12 98 96.85 TANB 62 83 82.72 Multinet 92 64 64.37 Table 4 Influence of the prior probability of the class on classification results for the Multinet Bayesian network lihoods for the evidence vector P\(A/C  normal and P\(A/C  attack have close values Table 4 shows the influence of the prior probability on the results It presents the classification results with the maximum a posteriori decision rule taking into account the prior probability of the class P\(C  normal  0.99 and P\(C  attack  0.01 and with the maximum of likelihood decision rule i.e by considering that the prior probability is uniform In this case we can notice that the results are better Hit Rate 100 and Filtered Alarms 76 5 Conclusion and future work We presented here an automated architecture for filtering the alarms issued from a network intrusion detection system NIDS Our architecture based on unsupervised classification and Bayesian networks allows to determine if the network is really attacked While the filter only keeps 24 of false positive Filtered Alarms=76 it could detect all the real attacks Hit Rate 100 In the future we intend to use a data base much richer on attack and some specialized Bayesian networks taking into account the network characteristics topology types of server in order to to improve the results We will also use other classification algorithms like Support Vector Machines to compare their results in the last of our architecture The last step will be the study of the adaptability of our architecture i.e being able to automatically detect evolution of the context new machines in our network new alarms new behaviors-type  and taking them into account Acknowledgement This work was supported in part by the IST Programme of the European Community under the PASCAL Network of Excellence IST-2002-506778 This publication only reflects the authors views The authors are thankful to Cedric Foll security engineer in our education board for his expertise in intrusion detection and for data and help he provided us 6 References 1 J Allen A Christie W Fithen J McHugh J Pickel and E Stoner State of the practice of Intrusion Detection Technologies Technical Report Carnegie Mellon University 2000 2 M Almegren H Debar and M Dacier A lightweight tool for detecting web server attacks Network and Distributed System Security Symposium NDSS 2000 p 157-170 2000 3 R Bace Intrusion Detection Macmiillan Technical Publishirng 2000 4 N Ben Amor S Benferhat and Z Elouedi Naive bayesian Networks irn Intrusion Detection Systems Workshop on Probabilistic Graphical Models for Classification 14th European Conference on Machirne Learniing 7th European Conference on prirnciples and Practice of Knowledge Discovery in Databases ECML/PKDD'2003 Dubrovnik Croatie 11-23 Septembre 2003 5 N Ben Amor S Benferhat Z Elouedi and K Mellouli Decisioin Trees and Qualitative Possibilistic Inference Application to the Intrusion Detection Problem European Conference of Symbolic and Quanrtitative Approaches to Reasoning and Uncertairnty ECSQARU'2003 Alborg Danemark pp 419-431 Juillet 2003 6 N Ben Amor S Benferhat Z Elouedi and F Cuppens Decisioin Trees and Qualitative Inference for Intrusion Detection Systems International Fuzzy Systems Association conference IFSA2A003 Turquie Juillet 2003 7 D J Burroughs L F Wilson and G V Cybenko Analysis of Distributed Intrusion Detection Systems Usirng Bayesian Methods Proceedings of IEEE International Performance Computirng and Communiication Conference April 2002 8 S Cho Incorporatirng Soft Computirng Techniques into a Probabilistic Intrusion Detection System IEEE Transactions on Systems Man and Cybernetics 32 2 pp 154-160 May 2002 9 C K Chow and C.N Liu Approximating discrete probability distributions with dependence trees IEEE Transactions on Information Theory 3\(14 pp 462-467 1968 0-7803-9521-2/06/$20.00 2472006 IEEE Max of likelihood Max a posteriori P\(AIC P\(AIC C Hit Filtered Hit Filtered Rate alarms Rate alarms Approachl 84 86 0 100 Approach2 100 76 92 64 3179 


10 F Cuppens Managing alerts irn a multiintrusion detection environment In 17th Annual Computer Security Applications Conference ACSAC p 22-31 2001 11 0 Dain and R K Cuningham Fusirng hetergenous alert streams irnto scenarios In Proceedirngs of the Eighth ACM Conference on Computer and Communiications Security pp 1-13 2001 12 H Debar M Dacier and A Wespi A Revised Taxonomy for Intrusion Detection Systems Annales des Te'le'communiications 55\(7-8 p 361378 2000 13 H Debar and A Wespi Aggregation and correlation of intrusion alerts In 4th Workshop on Recent Advances irn Intrusion Detection RAID 2001 LNCS Springer Verlag Berlin p 85-103 2001 14 A Faour P Leray and C Foll Re'seaux bayesieins pour le filtrage d'alarmes dans les systemes de de'tection d'intrusion In Atelier Modeles Graphiques Probabilistes 5emes journe'es d'Extraction et de Gestion des Connaissances EGC 2005 pages 25-33 Paris France 2005 15 N Freidman D Geiger and M Goldszmidt Bayesian network classifiers Machirne Learning 29 pp 131-163 1997 16 F V Jensen An introduction to Bayesian Networks Taylor and Francis London United Kingtom 1996 17 K Julish and M Dacier Miniing Intrusion Detection alarms for actionable knowledge In the 8th ACM International Conference on Knowledge Discovery and Data Miniing p 366-375 2002 18 T Kohonen Self-Organrizirng Maps Series irn Information Sciences Third Extended Edition Berlin Sprirnger 2001 19 C Kruegel D Mutz W Robertson and F Valeur Bayesian Event Classification for Intrusion Detection In the Proceedings of the 19th Annual Computer Security Applications Conference ACSAC03 IEEE Computer Society p14 Washington DC USA December 2003 20 P Leray and 0 Francois Re'seaux Bayesieins pour la classification Methodologie et Illustration dans le cadre du Diagnostic Me'dical Revue d'Intelligence Artificielle 18\(29 pp 169193 2004 21 S Manganaris M Christenen D Zerkleand and K Hermiz A Data Miniing analysis of RTID alarms Computer Networks 34-\(4 p 571-577 2000 22 J Pearl Probabilistic Reasoniing irn Intelligent Systems Networks of Plausible Inference Morgan Kaufmann 1988 tion for networks Proceedirngs of Thirteenth Systems Admiriistration Conference LISA 99 pp 229-238 1999 25 S L Scott A bayesian paradigm for designing Intrusion Detection System Computational Statistics and Data Analysis special issue on network irntrusion detection 45 pp 69-83 2004 26 A A Sebyala T Olukemi and L Sacks Active Platform Security through Intrusion Detection Usirng Nave Bayesian Network for Anomaly Detection In the proceedings of London Communications Symposium 2002 27 R Sekar Y Guang S Verma and T Shanbhag A high performance network intrusion detection system 6th ACM Conference on Computer and Communiications Security p 8-17 1999 28 A Seleznyov Temporal-Probabilistic Network Approach for Anomaly Intrusion 12th Annual Computer Security Inrcident Handlirng Conference Chicago 2000 29 S Staniford J.A Hoagland and J.M McAlernay Practical automated detection of stealthy portscans In the ACM Computer and Communications Security IDS Workshop pp 1-7 2000 30 A Valdes and K Skinner Probabilistic alert correlation In the 4th Workshop on Recent Advances in Intrusion Detection RAID LNCS Springer Verlag Berlin pp 54-86 2001 31 D Zamboni using internal sensors for computer intrusion detection PhD Thesis Purdue University 2001 23 R S Puttini Z Marrakchi and L Me A Bayesian Classification Model for RealTime Intrusion Detection AIP International Conference 659 1 pp 150-162 March 2003 24 M Roesch Snort lightweight intrusion detec0-7803-9521-2/06/$20.00 2472006 IEEE 3180 


        


     


nature of a k-ary predicate is a k-tuple of types. The signature of a k-ary function is a k + 1-tuple of types involving the types of the parameters and the result of the function. The set of terms of the language is the smallest set that contains the atomic constants and variables, and it is closed under the application of functions. Simple formulae consist of predicates applied to terms and formulae are combinations of atomic formulae through the combination of the connectives   and the quanti?ers Functions and Predicates. Functions and predicates are quite important in the PBMS setting, since the approximation of the data to patterns mapping, usually needs complex functions to be expressed. Functions and predicates can possibly appear both in the formula ?eld and in queries, associating relation names with the pattern structure. We believe that having interpreted functions is the best approach for the PBMS since we would like the formula to be informative to the user and we would like to be able to reason on it Safety andRangeRestriction. The formula is a predicate that we would ideally like to be true for all the data that are mapped to a pattern. Notice that the formula by itself does not contain a logical expression involving the pattern structure schema and the data schema i.e., it is not a query on the relations of the raw data The formula is merely a predicate to be used in queries We would like for example to use it in queries that navigate between the data and the pattern space like the following x | fp\(x where fp is a formula predicate and R is a relation appearing in the Data component. We require that fp is de?ned in such a way that we can construct queries like the previous, which are  safe  Safety is considered in terms of domain independence. Still, we cannot adopt the classical notion of domain independence \(which restricts values to the active domain of the database since even the simple functions can create new values not belonging to the domain of the database fore, we should consider a broader sense of domain independence similar to the one presented in [5, 17, 8 which allows the ?nite application of functions. For example, the n-depth domain independence as suggested in [5] considers domain independence with respect to the active domain closed under n application of functions. This includes the active domain and all the values that can be produced by applying the database functions n times, where n some ?nite integer The easiest way to ensure safety in these terms is to range restrict all variables appearing in a query. To this end, we introduce the where keyword in the formula which facilitates the mapping of the formula predicate free variables to the relation schema that appears in the DataSchema or Data component. More speci?Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE cally, we require that there are no free variables in the fp that are not mapped to the relation of the Data component by the use of thewhere keyword. This restriction guarantees that all the variables appearing in fp are either range restricted or that the system knows how to range restrict them to a ?nite set of values when fp is used in a query Now we can formally de?ne the well-formed formula for the pattern-type De?nition 13 A pattern type formula is of the form fp\(dv, pv 1 where fp \(formula predicate variable names mapped by the where keyword to the rela 


variable names mapped by the where keyword to the relation in DataSchema and pv are variable names that appear in the StructureSchema At instantiation time pv is assigned values of the Structure component and dv is mapped to the relation appearing in Data component. The de?nition for the pattern well-formed formula is now straightforward De?nition 14 A pattern formula is of the form fp\(dv 2 where fp \(formula predicate variablesmapped by thewhere keyword to the relation appearing in Data component From the previous de?nitions the semantics of the where keyword become evident: we impose that the variables of the formula will take values from speci?c relations when the formula predicate is employed in queries Example 2 Let us consider the following formulas 1. f\(x x 2. f\(g\(x x In the ?rst formula variable x is mapped to R using the where keyword, thus the formula is well formed. Keep in mind that the formula predicate by itself is just the part f\(x is not well-formed since y is not mapped via where to any relation, or otherwise range restricted 5. Querying the Pattern Warehouse We de?ne queries to be posed over the pattern warehouse and not individually over its data- or patternbase components. Through this approach, we are able to sustain queries traversing from the pattern to the data space and vice-versa. At the same time, the consistency of the results is guaranteed by the pattern-data mapping De?nition 15 Let PW the set of all possible Pattern Warehouses. A query is a function with signature PW ? PW. Given a query q and a pattern warehouse pw = \(DB,PB D?B, P?B q\(pw DB?, PB   P?B? = ?[D?1, ..., D?m], [P?C1:PT1]?. We assume that tr, tp\(tr ? R1 ? tp ? PC1 tr, tp Note that, similarly to the relational case, the result of a query is always a pattern warehouse containing just one relation and one pattern class. It is also important to point out that, in practice, even if a query always involve both the data and pattern space, operations over patterns are executed in isolation, locally at the PBMS. The reference to the underlying data is activated only on-demand \(whenever the user speci?cally requests so stored intermediate mappings or the formula approximation 5.1. Query operators In this section we introduce query operators that allow basic queries over the the PW . Assuming that DB denotes the set of all possible database instances and PB the set of all possible pattern bases, we consider the following groups of operators  Database operators: they can be applied locally to the DBMS. op : DB ? DB. We denote the set of database operators with OD  Pattern base operators: they can be applied locally to the PBMS. op : PB ? PB. We denote the set of database operators with OP  Cross-over database operators: they involve evaluation on both the DBMS and the PBMS, the result is a database. op : DB  PB ? DB. We denote the set of database operators with OCD  Cross-over pattern base operators: they involve evaluation on both the DBMS and the PBMS, the 


evaluation on both the DBMS and the PBMS, the result is a pattern base. op : DB  PB ? PB. We denote the set of database operators with OCP In the following, we present examples of the last three classes of operators \(database operators coincide with usual relational operators operators, we introduce some examples of predicates de?ned over patterns Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE 5.1.1. Pattern predicates We identify two main classes of atomic predicates: predicates over patterns and predicates over pattern components. From those atomic predicates we can then construct complex predicates. In the following, we denote pattern components by using the dot notation. For example, the measure component of a pattern p is denoted by p.Measure Predicates over pattern components. They check properties of speci?c pattern components. Let p1 and p2 be two patterns, possibly selected by some queries. The general form of a predicate over pattern components is t1?t2, where t1 and t2 are path expressions that must de?ne components of patterns p1 and p2, of compatible type and ? must be an operator, de?ned for the type of t1 and t2. For example, if t1 and t2 are integer expressions, then ? can be a disequality operator e.g. one of &lt;,&gt cases  If t1 and t2 are pattern data for patterns p1 and p2, then ? ? {=,?}. t1 = t2 is true if and only if x x ?? p1 ? x ?? p2 and t1 ? t2 is true if and only if ?x x ?? p1 ? x ?? p2  If t1 and t2 are pattern formulas for patterns p1 a n d  p 2   t h e n             t 1    t 2  i s  t r u e  i f  a n d o n l y  i f  t 1    t 2  a n d  t 1    t 2  i s  t r u e  i f  a n d  o n l y  i f  t 1 logically implies t2 Predicates over patterns. We consider the following set of predicates  Identity if they have the same PID, i.e. p1.P ID = p2.P ID  Shallow equality \(=s are shallow equal if their corresponding components \(except for the PID component i.e. p1.Structure = p2.Structure, p1.Source p2.Source, p1.Measure = p2.Measure, and p1.formula = p2.formula. Note that, to check the equality for each component pair, the basic equality operator for the speci?c component type is used  Deep equality \(=d deep equal if their corresponding data are identical, i.e., ?x x ?? p1 ? x ?? p2   S u b s u m p t i o n       A  p a t t e r n  p 1  s u b s u m e s  a  p a t t e r n  p 2   p 1    p 2   i f  t h e y  h a v e  t h e  s a m e  s t r u c ture but p2 represents a smaller set of raw data i.e. p1.Structure = p2.Structure, p1.Source p 2  S o u r c e  a n d  p 1  f o r m u l a    p 2  f o r m u l a  Complex predicates. They are de?ned by applying usual logical connectives to atomic predicates. Thus, if F1 and F2 are predicates, then F1 ? F2,F1 ? F2  F1 are predicates. We make a closed world assumption, thus the calculation of  F is always ?nite 5.1.2. Pattern base operators OP In this subsection, we introduce several operators de?ned over patterns. Some of them, like set-based operators, renaming and selection are quite close to their relational counterparts; nevertheless, some others, like join and projection have signi?cant di?erences Set-based operators. Since classes are sets, usual operators such as union, di?erence and intersection are de 


tors such as union, di?erence and intersection are de?ned for pairs of classes of the same pattern type Renaming. Similarly to the relational context, we consider a renaming operator ? that takes a class and a renaming function and changes the names of the pattern attributes according to the speci?ed function Projection. The projection operator allows one to reduce the structure and the measures of the input patterns by projecting out some components. The new expression is obtained by projecting the formula de?ning the expression over the remaining attributes [12 Note that no projection is de?ned over the data source since in this case the structure and the measures would have to be recomputed Let c be a class of pattern type pt. Let ls be a non empty list of attributes appearing in pt.Structure and lm a list of attributes appearing in pt.Measure. Then the projection operator is de?ned as follows ls,lm c id s m f p ? c, p = \(pid, s, d,m, f In the previous de?nition, id ing new pids for patterns, ?mlm\(m projection of the measure component and ?sls\(s ned as follows: \(i s usual relational projection; \(ii sls\(s and removing the rest from set elements. The last component ?ls?lm\(f computed in certain cases, when the theory over which the formula is constructed admits projection. This happens for example for the polynomial constraint theory 12 Selection. The selection operator allows one to select the patterns belonging to one class that satisfy a certain predicate, involving any possible pattern component, chosen among the ones presented in Section 5.1.1 Let c be a class of pattern type pt. Let pr be a predicate. Then, the selection operator is de?ned as follows pr\(c p Join. The join operation provides a way to combine patterns belonging to two di?erent classes according to a join predicate and a composition function speci?ed by the user Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE Let c1 and c2 be two classes over two pattern types pt1 and pt2. A join predicate F is any predicate de?ned over a component of patterns in c1 and a component of patterns in c2. A composition function c pattern types pt1 and pt2 is a 4-tuple of functions c cStructureSchema, cDataSchema, cMeasureSchema, cFormula one for each pattern component. For example, function cStructureSchema takes as input two structure values of the right type and returns a new structure value, for a possible new pattern type, generated by the join. Functions for the other pattern components are similarly de?ned. Given two patterns p1 = \(pid1, s1, d1,m1, f1 p2 = \(pid2, s2, d2,m2, f2 p1, p2 ned as the pattern p with the following components Structure : cStructureSchema\(s1, s2 Data : cDataSchema\(d1, d2 Measure : cMeasureSchema\(m1,m2 Formula : cformula\(f1, f2 The join of c1 and c2 with respect to the join predicate F and the composition function c, denoted by c 1   F  c  c 2   i s  n o w  d e  n e d  a s  f o l l o w s    F  c  c 2     c  p 1   p 2   p 1    c 1  p 2    c 2  F   p 1   p 2     t r u e   5.1.3. Cross-over database operators OCD Drill-Through. The drill-through operator allows one to 


Drill-Through. The drill-through operator allows one to navigate from the pattern layer to the raw data layer Thus it takes as input a pattern class and it returns a raw data set. More formally, let c be a class of pattern type pt and let d be an instance of the data schema ds of pt. Then, the drill-through operator is denoted by c c Data-covering. Given a pattern p and a dataset D sometimes it is important to determine whether the pattern represents it or not. In other words, we wish to determine the subset S of D represented by p \(p can also be selected by some query the formula as a query on the dataset. Let p be a pattern, possibly selected by using query language operators, and D a dataset with schema \(a1, ..., an ible with the source schema of p. The data-covering operator, denoted by ?d\(p,D responding to all tuples in D represented by p. More formally d\(p,D t.a1, ..., t.an In the previous expression, t.ai denotes a speci?c component of tuple t belonging to D and p.formula\(t.a1, ..., t.an instantiated by replacing each variable corresponding to a pattern data component with values of the considered tuple t Note that, since the drill-though operator uses the intermediate mapping and the data covering operator uses the formula, the covering ?\(p,D D = ?\(p not be equal to D. This is due to the approximating nature of the pattern formula 5.1.4. Cross-over pattern base operators OCP Pattern-covering. Sometimes it can be useful to have an operator that, given a class of patterns and a dataset, returns all patterns in the class representing that dataset \(a sort of inverse data-covering operation Let c be a pattern class and D a dataset with schema a1, ..., an pattern type. The pattern-covering operator, denoted as ?p\(c,D all patterns in c representing D. More formally p\(c,D t.a1, ..., t.an true Note that: ?p\(c,D p,D 6. Related Work Although signi?cant e?ort has been invested in extending database models to deal with patterns, no coherent approach has been proposed and convincingly implemented for a generic model There exist several standardization e?orts for modeling patterns, like the Predictive Model Markup Language \(PMML  eling approach, the ISO SQL/MM standard [2], which is SQL-based, and the Common Warehouse Model CWM  ing e?ort. Also, the Java Data Mining API \(JDMAPI 3] addresses the need for a language-based management of patterns. Although these approaches try to represent a wide range of data mining result, the theoretical background of these frameworks is not clear. Most importantly, though, they do not provide a generic model capable of handling arbitrary cases of pattern types; on the contrary only a given list of prede?ned pattern types is supported To our knowledge, research has not dealt with the issue of pattern management per se, but, at best, with peripheral proximate problems. For example, the paper by Ganti et. al. [9] deals with the measurement 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


