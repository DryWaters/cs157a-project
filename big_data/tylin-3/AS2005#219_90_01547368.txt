GUHA Method and Granular Computing Jan Rauch Faculty of Informatics and Statistics University of Economics Prague nam W Churchilla 4 130 67 Praha 3 Czech Republic Email rauch@vse.cz Abstract-GUHA method of exploratory data analysis is presented GUHA offers all interesting facts foflowing from the analysed data to the given problem Its development started about 40 years ago It is implemented in the form of GUHA-procedures hnplementation techniques called now granular computing are used The software system LISp-Miner containing six 
GUHA procedures mining for various types of patterns is presented Important theoretical results concerning GUHA method are mentioned I INTRODUCTION GUHA is an original Czech method of exploratory data analysis Its aim is to offer all interesting facts following from the analysed data to the given problem GUHA is realized by GUHA-procedures GUHA-procedure is a computer program the input of which consists of the analysed data and of a simple definition of relevant i.e potentially interesting patterns GUHA procedure automatically generates each particular pattern and tests if 
it is true in the analysed data The output of the procedure consists of all prime patterns see Fig 1 The pattern is prime if it is true in the analysed data and if it does not immediately follow from the other more simple output patterns 3 1 41 4J Fig 1 GUHA Method Principle Development of the GUHA method started about forty years ago 2 Important milestone in the GUHA method development is the book 3 The most important GUHA procedure is the procedure ASSOC 
3 that mines for association rules The association rules the procedure ASSOC mines for are more general than the classical alssociation rules defined in 1 Procedure ASSOC mines among other for association rules corresponding to Milan Siimunek Institute of Computer Science Academy of Sciences of the Czech Republic Pod vodarenskou vezf 2 182 07 Praha 8 Czech Republic Email simunek@vse.cz statistical hypothesis tests There are several implementations of the procedure ASSOC see e.g 4 5 8 The newest implementation of procedure ASSOC is procedure 4ft-Miner 
It has various new important features and it mines also for conditional association rules 17 18 There is academic software system LISp-Miner 18 that includes five new GUHA procedures in addition to the procedure 4ft-Miner They mine for large variety of patterns There are both simple patterns verified in one contingency table of two Boolean attributes and complex patterns corresponding to differences of two sets what concerns relation of two categorial attributes Such complex pattern is verified using a pair of contingency 
tables Implementation of all GUHA procedures of the LISp-Miner system is based on representation of analysed data by strings of bits 11 12 18 This approach is also called granular computing 10 There are important theoretical results related to the GUHA method that can be understood as theoretical contribution to the granular computing Mathematical theory related to the question Can computers formulate and justify scientific hypotheses was developed in 3 Observational calculi are defined and studied in 3 as a language in which statements 
concerning observed data are formulated Logical calculi formulae of which correspond to generalized association rules are special case of observational calculi Various theoretical results concerning observational calculi and namely association rules were achieved in 3 Some new results concerning logic of association rules are e.g in 13 14 15 16 Goal of this paper is to introduce GUHA procedures implemented in the LISp-Miner system Principles of their implementation are outlined in section II One of new procedures is the procedure SD4ft-Miner It 
is an example of the procedure dealing with a pair of contingency tables Main features of SD4ft-Miner are described in section m Overview of all GUHA procedures implemented in the LISp-Miner system is given in section IV Current experience is summarized in section V where EverMiner project is also introduced II BIT-STRING APPROACH Application of bit-strings in GUHA method were first described in 11 Here we only outline main principles Fore more details see e.g 12 18 19 20 0-7803-9017-2/05/$20.00 02005 IEEEE 630 


All six GUHA procedures deal with data matrices An example is data matrix M in Fig 2 Each column of the object i.e row of M 2601 02  A1 2 for the basic Boolean attribute A1 1,2 etc It is important that the bit-wise Boolean operations A V and 2 are carried out by very fast computer instructions Very fast computer instructions and some heuristics are also used to carry out a bit string function Count\(s returning the number of values 1 in the bit string  This function is used to compute all contingency tables An example of contingency tables is the 4ft-table of Boolean attributes  and 4 on data matrix M that is denoted by 4ft\(p 4 M It is a quadruple a b c d of natural numbers such that a is the number of rows of M satisfying both  and 4 b is the number of rows satisfying p and not satisfying 4 c is the number of rows not satisfying  and satisfying 4 and d is the number of rows satisfying neither  nor 4 see Fig 4 It is a  Count\(C A C\(4 b  Count\(C\(p a p a b c d Fig 4 4ft-table 4ft\(W b M of p and  on M c  Count\(C\(4 a and d  n a b c where n is the total number of rows in data matrix M Conditional contingency tables concerning a condition given by the Boolean attribute y are used in some GUHA procedures of the LISp-Miner system Conditional contingency table on data matrix M under the condition y concerns just the rows of data matrix M satisfying y Thus it can be understood as the contingency table on data matrix M/'y Here M/-y is a data matrix consisting of just all rows of M satisfying y We suppose that there is at least one such row III PROCEDURE SD4FT-MINER The procedure SD4ft-Miner mines for SD4ft-patterns of the form  4,/\(a y This SD4ft-pattern means that the subsets given by Boolean attributes a and  differ in what concerns the relation of the Boolean attributes  and 4 when the condition given by Boolean attribute y is satisfied Symbol is called SD4ft-quantifier It corresponds to a condition concerning two four-fold contingency tables of p and 4 The first second contingency table concern with the objects of the set defined by the Boolean attributes a 3 respectively satisfying the condition y We use medical data described in section Ill-A to demonstrate SD4ft-Miner procedure Tools to define the set of 631 L 1 i L i W 02 03 260n A1 1 4 2 3 Fig 3 cards of categories of A1 A1 1Il A1[2 A1 3 A1 4 1 0 0 0  U U O 0 1 1 0 0 O 1 0 Cards of categories of attribute A1 Various Boolean attributes play important role in patterns of all described GUHA procedures Thus cards of Boolean attributes are used Cards of Boolean attributes are analogous to the cards of categories Card C\(f of Boolean attribute  is a string of bits Each row of the data matrix corresponds to one bit of C\(f There is 1 in the i-th bit if and only if  is true in row oi It is evident that C A 4  C\(f A C\(4 C V 4  C\(f V C and C\(-"p  C\(f Here C\(f A C\(4 is a bit-wise conjunction of bit strings C\(f and C\(4 analogously for V and  Moreover it is C\(A1 1 2  A1 1 03 On columns of M i.e attributes A1 A2  A5o 1 4  4 4 3  6 2 6  7 3 1 36 examples of literals A1 1 2 A5o 6 T T F F T T F T Fig 2 Data matrix M data matrix corresponds to one attribute Each attribute has finite number of possible values called categories Thus these attributes are called categorial attributes here Attributes Al and A50 are examples of categorial attributes Attribute Al has categories 1,2,3,4 etc Particular patterns mined by the described GUHA procedures concern categorial attributes or Boolean attributes or both The Boolean attributes used in patterns are conjunctions of literals Literal is either basic Boolean attribute A\(a or negation iA\(a of basic Boolean attribute It is a C al ak where a ak is the set of all categories of column A Basic Boolean attribute A\(a is true in row o of M if it is a E a where a is the value of the attribute A in row o The set a is a coefficient of literals A\(a and A\(a Expressions A1 1 2 and 7A50 6 are examples of literals see Fig 2 All patterns are verified using one or two contingency tables concerning corresponding attributes Contingency tables are computed using bit string representation of analysed data Bit string representation is based on cards of categories The card of category is a string of bits For example the attribute A1 of data matrix M in Fig 2 has four categories 1,2,3,4 It means that the attribute A1 is represented by cards A1 1 A1 2 A1 3 A1 4 of categories 1,2,3,4 respectively The card A1 1 of category 1 is the string of bits Each row of data matrix M corresponds to one bit of the card A1 1 There is 1 in the bit corresponding to row oi if and only if there is the value 1 in row oi In other words there is 1 in the i-th bit of the card A1 1 if and only if the basic Boolean attribute Al\(1 is true in row oi The same is true for other categories and attributes see also Fig 3 row of M 2601 


relevant SD4ft-patterns are introduced in section III-B An example of solution of practically interesting analytical question is given in section III-C Some remarks to SD4ft-Miner procedure are also in section V A Analysed Data and its Transformations We use data matrix ENTRY of data set STULONG 1 Data matrix ENTRY concerns 1 419 patients each row corresponds to one patient There are 64 columns corresponding to attributes of patients There are 6 attributes concerning social characteristics 18 attributes concerning personal anamnesis etc We use three attributes concerning body of patient Weight kg 69 categories Skinfold above musculus triceps mm 30 categories and Skinfold above musculus subscapularis mm 48 categories three attributes concerning social characteristics of patient Education 4 categories Marital status 4 categories and Responsibility in job 4 categories two attributes concerning physical activities Physical activity in a job 4 categories and Physical activity after a job 3 categories two attributes concerning blood pressure Systolic mm Hg 48 categories and Diastolic mm Hg 30 categories three attributes concerning alcohol consumptions Bier Liquers and Vine each of them with 3 categories We also use attribute Group of patients with three categories normal risk and pathological We use the procedure DataSource 20 of LISp-Miner system to transform original columns such that the resulting attributes have categories suitable for further analysis We define new categories intervals 90,100 100,110   220 230 for attribute systolic This way we get new attribute SystoliclO Similarly we get new attributes Diastolicl1O Skinfold-triceps.S and Skinfold-subscapularisJO B Set of relevant SD4ft-patterns We use the procedure SD4ft-Miner mining for SD4ftpatterns W  0/\(a,3,7y to solve analytical question of the type What groups a and 3 of patients and under what condition y remarkable differ what concerns relation of Boolean attributes W and 0 The attribute S is called antecedent and the attribute O is called succedent Both antecedent and succedent are Boolean attributes we call them cedents Also a 3 and y are Boolean attributes and we call them also cedents We have to define the set of relevant SD4ft-patterns to be automatically generated and verified It is given by SD4ftquantifier sz and by five sets of relevant cedents There is one set of relevant cedents for antecedent called set of relevant antecedents one set of relevant cedents for succedent etc 1 The study STULONG was realized at the 2nd Department of Medicine 1st Faculty of Medicine of Charles University and Charles University Hospital U nemocnice 2 Prague 2 head Prof M Aschermann MD SDr FESC under the supervision of Prof E Boudik MD ScD with collaboration of M Tomekcova MD PhD and Ass Prof J Bultas MD PhD The data were transferred to the electronic form by the European Centre of Medical Informatics Statistics and Epidemiology of Charles University and Academy of Sciences head Prof RNDr J Zvarova DrSc The data resource is on the web pages http://euromise.vse.cz/challenge2OO4 Cedent is a conjunction apos apos apos 0-1 B pos B pos 0-1 B pos B pos Fig 5 Set of relevant antecedents The set of partial cedents is given by minimum and maximum lengths by a set of attributes from which literals will be generated and by simple definition of the set of relevant literals to be generated for each attribute There are some further parameters for a fine tuning of the set of partial cedent 18 The length of partial cedent Body will be 1 2 and this partial cedent will contain literals automatically created from attributes Weight Skinfold-triceps.5 and Skinfold-subscapularislO see Fig 5 Analogously for partial cedents Social characteristics and Activities see Fig 5 We would like to point out that the minimum length of the partial cedent can be 0 and this results in some conjunctions of length 0 The value of the conjunction of length 0 is identical to 1 and thus the partial cedent pi of the length 0 can be omitted from the cedent The length of the literal is the number of categories in its coefficient see Section II The set of all literals to be generated for a particular attribute is given by the type of coefficient there are seven types the minimum and the maximum lengths of the literal and by positive/negative literal option i.e generate only positive literals or generate only negative literals or generate both positive and negative literals We use the attribute X with categories 1 2 3 4 5 to give examples of types of coefficients used in this paper for the further types see 18  Subset definition of subsets of length 1-2 gives literals X\(1 X\(1,2 X\(1,3 X\(1,4 X\(1,5 X\(2  X\(2,5  X\(3  X\(4,5  Interval definition of intervals of length 2-3 gives literals X\(1,2 X\(2,3 X\(3,4 X\(4,5 X\(1,2,3 X\(2,3,4 and X\(3,4,5  Left cut definition of left cuts with a maximum length of 3 defines literals X\(l X\(1,2 and X\(1,2,3 632 I p  A 1 AfOk of partial cedents Partial cedent ci is a conjunction Wi  Ai,1 A  A it of literals see Section H The length of the partial cedent is the number of literals in this conjunction The set of relevant cedents is given by at least one definition of the set of relevant partial cedents In our case the set of relevant antecedents is given by definitions of three partial cedents Body Social characteristics and Physical activities see Fig 5  Weight  Skinfold_nTrceps_5  Skinfold_Subscapulart.j 0C Social cherectenstics  Education  MeritalStetus\(j  Responsibility-Jobn Activities  Activity_Afte rJob  ActivAty_Jab 


a is at least 0.4 greater than confidence of the rule  on the set 3 The task given by these parameters was solved in 23 minutes at the PC with 1.58 GHz and 512 MB RAM Due to various optimizations about 31.6 106 only of SD4ft-patterns were 633 4 daA.y dVA b6A7y dlA7y l means that if the condition a is satisfied then the confidence of the rule s us remark that coefficients cuts make possible to concentrate on the extreme values of attributes row of this attribute in Fig 6 Attribute Skinfold.subscapularislO has 8 categories intervals 0 10 10,20  70 80 It means that literals with coefficients 0 10 i.e left cut with length 1 0,10 10 20 i.e left cut with length 2 0 10 10 20 20 30 i.e left cut with length 3 50,60 60 70 70 80 i.e right cut with length 3 60,70 70,80 i.e right cut with length 2 70,80 i.e right cut with length 1 will be generated Let an analytical question of the type What groups of patients i.e normal risk pathological and under what Fig 7 4ft\(W 4 M 4 on the set  Right cut definition of right cuts with a maximum length of 4 defines literals X\(5 X\(5,4 X\(5,4,3 and X\(5,4,3,2  Cut means both left cut and right cut Definition of relevant literals for particular cedent Body is in Fig 6 Literals with coefficients Interval of the length 10 4ft 4 M/\(a A 4ft\(W 4 M/\(3 A y Y 4 s aaAy so Cc,,Ay a,9A7 CIAy Fig 6 Literals of partial cedent Body will be generated for attribute Weight see Length 10-10 in the row of attribute Weight in Fig 6 It means that literals Weight\(52-61  Weight\(124-133 of the form of sliding window will be generated for attribute Weight because of values of this attribute range from 52 to 133 Similarly literals with coefficients Interval of the length 1-2 will be generated for attribute Skinfoldtriceps-5 see the row of this attribute in Fig 6 Attribute Skinfolditriceps-5 has 8 categories intervals 0 5 5 10  35 40 It means that the literals Skinfoldjriceps.5\(\(O 5 SkinfoldJriceps.5\(\(O 5 5 10 Skinfold-triceps.5\(\(30,35 35,40 Skinfold-triceps5\(35 40 will be generated Literal Skinfold_triceps5\(\(0 5 5 10 of the attribute Skinfold-triceps-5 corresponds to the literal Skinfold above musculus triceps\(0   10 of the attribute Skinfold above musculus triceps see also section IE-A Literals with coefficients Cut of the length 1-3 will be generated for attribute Skinfold.subscapularis1O see the 1 created from one of attributes Bier Liquers and Vine The SD4ft-quantifier is given by three basic SD4ftquantifiers SD4ft-quantifier corresponds to a condition concerning two 4ft-tables The first one is the table 4ft\(s b M/\(a A y of and on M/\(aA7 see Fig 7 Thus aaAly is the number of rows of data matrix M/\(aA-y satisfying both and baAy a/A7  bj3Ay The last condition 4 etc O see section HI-A The set of relevant conditions is given as the empty conjunction or one single literal sets of the size see Section II The second 4ft-table is 4ft\(s 4 M/\(3 A y of and on M/\(13 A y see Fig also 7 SD4ft-quantifier is a conjunction of basic SD4ft-quantifiers Description of all basic SD4ft-quantifiers is out the scope of this paper Some examples are in the next section C Application Example We solve see Fig 9 Fig 9 SD4ft-quantifier BASE FirstSet  40.00 Abs i.e aaAY  40 BASE SecondSet  40.00 Abs i.e aA7y  40 and FUI DiffVal  0.40 Abs i.e ac,A7 a#A7y _  0.4 aftA7  A Ay and 4ft\(p  M A y condition remarkable differs what concerns relation of body characteristics and blood pressure It means that definitions of both cedents sets a and j3 are given by the attribute Group of patients see section EI-A with the set of relevant literals of the type Subset of the length 1 We use the set of relevant antecedents given in the way described in previous section see namely Fig 6 Further we use the set of relevant succedents given according to Fig 8 It Fig 8 Succedent Blood pressure is conjunction of one or two literals intervals of the length 13 generated from attributes SystoliclJO and Diastolicl 


generated and verified The amount of 13 true patterns were found The strongest one is the pattern shown in Fig 10 textual part and Fig 11 graphical part Fig 10 Strongest pattem text This pattern has the following properties antecedent is Skinfold-subscapularis\(l-30 2 A Education\(University succedent is Diastolic\(60-89 A Systolic\(1 10-139 first set is Group of patients\(normal second set is Group of patients\(risk condition is Beer\(up to 1 litre  day Fig 11 Strongest pattem graphs The contingency tables of antecedent and succedent on set of normal patients and on set of risk patients under the condition Beer\(up to 1 litre  day are in Fig 12 It is Diff Confidence  0.46 see Fig 10 i.e 48+6 48+65  0.46 It means that the confidence of the rule 2 There is Skinfold_subscapularis_10  20,30  in Fig 10 that is abbreviation of Skinfold-subscapularis-10 0,10 10 20 20 30  that means Skinfold-subscapularis\(1-30 Similarly for attributes of succedent normal patients succedent succedent antecedent 48 6 antecedent 71 30 risk patients succedent  succedent antecedent 48 65 antecedent 169 197 Fig 12 4ft tables for normal and risk patients antecedent P succedent on the set of normal patients is 0.46 greater than its confidence on the set of risk patients IV GUHA PROCEDURES OVERVIEW There are six GUHA procedures this time in the LISpMiner system Three of them mine for patterns concerning one contingency table and three mine for patterns concerning pairs of contingency tables Names of the last three procedures start with string SD that means Set Differs from Set An example of such procedures is the just described procedure SD4ft-Miner The implemented procedures are The procedure 4ft-Miner 17 18 mines for association rules 0 and a when the condition given by Boolean attribute y is satisfied see section Im The procedure SD4ft-Miner is a generalization of the procedure SDS-Miner 9 The procedure SDKL-Miner mines for SDKL-patterns of the form R C/\(a,13 y The intuitive meaning of SDKL-pattern R C/\(a 1 y is that the subsets a and 13 differ in what concerns the relation of the categorial attributes R and C when the condition given by Boolean attribute y is satisfied The procedure SDCF-Miner mines for SDCF-patterns of the form R/\(a 1 y The intuitive meaning of SDCFpattern R/\(a,13 y is that the subsets a and 3 differ in what concerns the frequencies of the particular categories of 634 o z b and for conditional association rules S\260 Zt The association rule R/-y The CF pattern R/y means that frequencies of categories of attribute R satisfy the condition given by symbol when an other condition given by Boolean attribute y is satisfied The procedure SD4ft-Miner mines for SD4ft-patterns of the form c,o 4O/\(a,13 y This SD4ft-pattern means that the subsets given by Boolean attributes a and 1 differ in what concerns the relation of the Boolean attributes p z  means that the Boolean attributes V and  are associated in the way given by symbol P The conditional association rule   b/fy means that f and 0,b are in the relation given by 4ft-quantifier  when the condition given by the Boolean attribute y is satisfied The procedure KL-Miner 19 mines for KL-patterns R C/-y The KL-pattern R C/-y means that the categorial attributes R and C are in a relation given by symbol when the condition given by Boolean attribute y is satisfied The procedure CF-Miner mines for CF-patterns of the form 


the attribute R when the condition given by Boolean attribute y is satisfied V CONCLUSIONS AND EVERMINER PROJECT Let us emphasize that all GUHA procedures of the LISpMiner are implemented using bit-string approach i.e by bit-maps or granular computing outlined in section II All analysed data are represented by cards of categories i.e strings of bits introduced in section II All contingency tables necessary for verification of particular patterns are computed using very fast bit-string operations Various data structures called traces of literals and traces of cedents built from cards of categories are maintained during generation and verification 18 It however means that the a-priori algorithm is not used The length of the cards of the category is the same as the number of rows in analysed data matrix Thus the time necessary to run all these GUHA procedures is approximately linearly dependent on the number of rows of the analysed data matrix This fact is confirmed by several experiments described in 18 19 Both published experiments and further experience show that this approach makes possible to analyze large variety of practically important data sets However there are lot of related open problems The first one is how to efficiently use large possibilities of fine tuning of the definition of the set of relevant patterns to be generated and verified It is e.g not clear what type of coefficients to use there are seven types subsets intervals etc and how to define partial cedents minimal and maximal length etc see section III-B Problem is also in the fact that particular partial cedents can be used in various analytical procedures Thus it is necessary to maintain a set of definitions of partial cedents related to the given area of application There is also problem wit the choice of suitable procedure and quantifier there are 6 procedures and dozens of quantifiers see section IV and 18 19 Even if some patterns are complex namely SD"-patterns they can be very useful if reasonable effort is related to interpretation and to explanation these patterns to user Particular procedures of the LISp-Miner system can be combined in various ways when solving a given data mining task The next step depends both on the goal of analysis and on results of previous runs of procedures Big challenge is automatized chaining of particular procedures of LISpMiner to solve given problem Let us remark that the project GUHA80 never realized was related to a similar goal 7 There are for sure interesting classes of particular patterns and this patterns must have its own logic Example of such a logic is logic of association rules see 3 15 There are also examples of interesting classes of association rules in 3 15 Our next research goals are  To build an open system called EverMiner of tools to facilitate solving real problems using all procedures implemented in the LISp-Miner system The EverMiner system will consists of typical tasks scenarios reposito To study logic of patterns mined by procedures implemented in LISp-Miner system Example of such a logic is logic of association rules see 15 ACKNOWLEDGMENT The work described here has been supported by project COST 274 TARSKI and by project IGA 17/04 of University of Economics Prague REFERENCES 1 Aggraval R et al 1996 Fast Discovery of Association Rules In Fayyad UM et al eds Advances in Knowledge Discovery and Data Mining AAAI Press Menlo Park California 2 Hajek P Havel I Chytil M 1966 The GUHA method of automatic hypotheses determination Computing 1 293-308 3 Hdjek P Havranek T 1978 Mechanising Hypothesis Formation Mathematical Foundations for a General Theory Springer Berlin Heidelberg New York 4 Hajek P guest editor 1978 International Journal of Man-Machine Studies special issue on GUHA 10 5 Hdjek P guest editor 1981 International Journal of Man-Machine Studies second special issue Hu eds Foundations and Novel Approaches in Data Mining Springer-Verlag 2005 pp 155 168 to appear 20 Simruonek M 2003 Academic KDD Project LISp-Miner In Abraham A et al eds Advances in Soft Computing Intelligent Systems Design and Applications Springer Berlin Heidelberg New York ries of partial cedents etc 635 to Data Analysis Computers and Artificial Intelligence Vol 1 pp 107-134 8 Hdjek P Sochorova A Zvarovd J 1995 GUHA for personal computers Computational Statistics  Data Analysis 19 149 153 9 Karban T Rauch J Simiunek M 2004 SDS-Rules and Association Rules In Haddad H M Omicini A Wainwright R L Liebrock L M eds Proceedings of the ACM SAC ACM Press New York 10 Louie E Lin T Y 2000 Finding Association Rules using Fast Bit Computation Machine-Oriented Modeling In Ras Z Ohsuga S eds Foundations of Intelligent Systems Springer Berlin Heidelberg New York 11 Rauch J 1971 Application of three-valued logic for GUHA method Thesis Faculty of Mathematics and Physics Charles University Prague in Czech 12 Rauch J 1978 Some Remarks on Computer Realisations of GUHA Procedures International Journal of Man-Machine Studies 10 23-28 13 Rauch J 1998 Classes of Four-Fold Table Quantifiers In Zytkow J Quafafou M eds Principles of Data Mining and Knowledge Discovery Springer-Verlag pp 203 211 14 Rauch J 2004 Definability of Association Rules and Tables of Critical Frequencies In Lin T Y Smale S Poggio T Liau C J eds Foundations and New Directions in Data Mining ICDM Workshop Proceedings IEEE Computer Society 15 Rauch J 2005 Logic of Association Rules Applied Intelligence 22 9-28 16 Rauch J 2005 Definability of Association Rules in Predicate Calculus In Lin T Y Ohsuga S Liau C J and Hu X eds Foundations and Novel Approaches in Data Mining Springer-Verlag 2005 pp 23 40 to appear 17 Rauch J Simfinek M 2000 Mining for 4ft Association Rules In Arikawa S Morishita eds Discovery Science Springer Verlag pp 268 272 18 Rauch J Simufnek M 2005 An Alternative Approach to Mining Association Rules In Lin T Y Ohsuga S Liau C J and Tsumoto S eds Data Mining Foundations Methods and Applications Springer-Verlag 2005 pp 219 238 to appear 19 Rauch J Sirmunek M Lfn V 2005 Mining for Patterns Based on Contingency Tables by KL-Miner First Experience In T Y Lin S Ohsuga,C J Liau and X on GUHA 15 6 Hajek P Havranek T Chytil M 1983 GUHA Method Academia Prague in Czech 7 Hafjek P Havranek T 1982 GUHA 80 An Application of Artificial Intelligence 


References Knowledge and Information Systems Proc of the 14th Intl Joint Conf on AI Proc of the 14th Intl Conf on Data Engineering Proc of the 9th Intl Conf on Cooperative Information Systems Comm of ACM Proc of the 15th Intl Symposium on Foundations of Intelligent Systems Proc of the 10th Intl Conf on Travel Behaviour Research Data Knowledge Engineering Journal of Information and Control  pages 924\320 929 1995 8 D  O la ru a n d B  S mith  M o d e llin g D a ily Ac ti v ity Sc h e d u le s with Fuzzy Logic In 1 R  C ool e y  B  M obasher  a nd J S r i v ast a v a  D at a P r e par a t i o n for Mining World Wide Web Browsing Patterns a c 5 0 0 0 Formal Concept Analysis Mathematical Foundations User Modeling and User-Adapted Interaction 0 1 2 4 Figure 4 Performance results based on satisfaction Proc of the 8th Intl Symposium on Temporal Representation and Reasoning     1\(1\:5\32032 1999 2 M  E irin a k i a n d M  V a z ir g ia n n is We b M in in g f o r We b P e r sonalization  3\(1\:1\32027 2003 3 B G a n te r a n d R W ille   40\(3\:77\32087 1997 5 D  L i a nd J S  D e ogun D i sco v e r i ng Par t i a l P er i odi c S equential Association Rules with Time Lag in Multiple Sequences for Prediction In  2003  B  O zden S  R amasw amy  and A S i l b erschat z C ycl i c Association Rules In  13\(4\:311\320372 2003  C S h ahabi  F  B Kashani  Y  S  C hen and D  M cL eod Yoda An Accurate and Scalable Web-Based Recommendation System In  42\(2\:189\320222 2002  L  A  Z a deh F u zzy S e t s   8:338\320353 1965 u13 u36 u48 u82 0 10 20 30 40 50 60 70 80 90 100 1 23456 Number of Personalized Resources Satisfaction u13 u36 u48 u82 0 10 20 30 40 50 60 70 80 90 100 1 23456 Number of Personalized Resources Satisfaction u13 u36 u48 u82 0 10 20 30 40 50 60 70 80 90 100 1 23456 Number of Personalized Resources Satisfaction u13 u36 u48 u82  Springer-Verlag 1997 4 J  A  K o n s ta n  B N M ille r  D M a ltz  J  H e r lo c k e r  L  G o r don and J Riedl Grouplens Applying Collaborative Filtering to Usenet News  pages 332\320341 2005 6 Y  L i  P  N i ng X  S  W a ng and S  J aj odi a D i sco v e r i ng Calendar-Based Temporal Association Rules In  pages 111\320118 2001 7 H  L ie b e rma n  L e tiz ia  A n A g e n t th a t Assists W e b Bro w sing In  pages 412\320421 1998  D  P i er r a k o s G  Pal i our as C  P apat heodor ou and C  D  Spyropoulos Web Usage Mining as a Tool for Personalization A Survey  pages 418\320432 2001  G  S t umme R  T aoui l  Y  B a st i d e N  Pasqui er  a nd L Lakhal Computing Iceberg Concept Lattices with TITANIC b d d  d  d  d  0 10 20 30 40 50 60 70 80 90 100 1 23456 Number of Personalized Resources Satisfaction ACM TOIT alization using periodic access p atterns of individual users Different from non-periodic approaches the proposed approach can ef\336ciently determine which resources a user is most probably interested in during a given period without the use of the user\325s current access information This makes it possible to perform more costly personalized resource preparation in advance rath er than in real-time The experimental results have shown that the proposed approach has achieved very effective web personalization evaluated by the applicability and satisfaction measures for the prede\336ned period conditions 


G f d O??Et c f d tb ? f} g ? h c h d h Figure 2. A frequency tree example After we built a frequency tree, we then use the Quick-split technique to calculate the maximal frequent sets The Quick-split algorithm is given here Input: A frequency tree r Output: An array of BitSets representing Itemsets Quick-split\(Tree r 1: if\(r is leaf   2: for all x E r.subs do 3: subres[x] = Quick-split\(x x 4: result =newBSO 5: for all x E r.subs do 6: result = result &amp; subres[x 7: remove b E result, b.size:::; k 8: return result To speed up calculation, an itemset is represented by a bitset with 0 and I for specifying the absence or presence of an item at a corresponding position respectively. The Quick-split performs a calculation on a frequency tree and returns an array of bitsets, which represent a group of decomposed itemsets. Splitting is accomplished by calculating bitset results in a bottom  up fashion in the tree. In the above example, we have 8 items a, b, . . .  , h corresponding to positions 0-7 in a 8bit bitset. So p.JS = abcdefgh = {I III I Ill}; abcd ll1 10000}; bcdefgh = {OllillII}. The size of the bitset is the number of items in p.JS which is usually much smaller than the total item size in the dataset For the frequency tree in Figure 2, for the itemset abcdefgh and infrequent 3-itemsets {aef, aeg, aeh, afg ajh, agh, abe, abf, abg, abh, ace, acf, acg, ach, ade adf, adg, adh} , Quick-split returns the possible maximal frequent sets {abcd, bcdefgh The PD Algorithm Input: transaction dataset T Output: frequent patterns PD \( transaction-set T I : D 1 = {&lt;t, 1&gt;1 t E T}; k= I 2: while \(Dk   3: for all p E Dk do II counting 4: for all k-itemset s c;;; p.JS do 5: Sup\(s IDk 6: decide Lk and ?Lk Ilbuild Dk 7: Dk+]= PD-rebuild\(Dk, Lk , ?Lk 8: k 9: end 10: Answer = u Lk As shown above, PD is the top-level function that accepts a transaction dataset as its input and returns the union of all frequent sets as the result. At the kth pass steps 3-6 count for every k itemset of each pattern in Dk and then determine the frequent and infrequent sets Lk and ? Lk; step 7 uses Dk , Lk and ?Lk to rebuild Dk PD stops when Dk is empty The PD-rebuild Algorithm Input: Dataset Db frequent Lk, infrequent ?Lk Output: Dataset DB PD-rebuild \(Dlo Llo ?Lk 1: Dk  ht = an empty hash table 2: for all p E Dk do begin 3: / / q k, ?q k can be taken from previous counting qk={sls E p. JS nLk}; ?qk={tltE p. JS n ?Lk 4: u = PD-decompose\(p.JS, ?qk 5: v ={s E ul s is k-item independent in u 


5: v ={s E ul s is k-item independent in u 6: add &lt;u-v, p.Occ&gt; to Dk+1 7: for all s E v do 8: if sin ht then ht. s. Occ += p. Occ 9: else put &lt;S,p.Occ&gt; to ht 10: end 11: Dk+] = Dk+] u {p E ht The PD-rebuild shown above is to determine DB by Dk, Lk and ?Lk' For each pattern p in Db step 3 computes its qk and ?qk; step 4 calls PD-decompose algorithm to decompose p by ?qk. Note that qk is not used here for decomposing p. In steps 5 to 9, we use pattern separation rule to separate p. In steps 7 to 9 PD-rebuild merges the patterns separated from p with their identical ones via a hash table ht. Since PD follows the pattern decomposition rule to decompose patterns and the pattern separation rule for merging identical patterns that yield same support, the answers generated by PD are correct 3. Comparative Study Our experiments were performed on a 600 MHz Pentium PC machine with 256 MB main memory running on Microsoft Windows XP. All three algorithms were written in C++ language. The test dataset was generated in the same fashion as the IBM Quest project . We used dataset T25.I10.DIOOK . In the dataset, the number of items N was set to 1000. The corruption level for a seed large itemset was fixed obtained from a normal distribution with mean 0.5 and variance 0.1. In the dataset, half of all items were corruptible. In the dataset, the average transaction size ITI and average maximal potentially frequent itemset size III are set to 25 and 10, respectively, while the number of transactions IDI in the dataset is set to lOOK 3.1. Comparison of PD with DCP  Pattern Decomposition algorithm does not need to generate candidate sets which are the main improvement on DCP; the reduced dataset contains only itemsets whose subsets are all frequent  PD generates all frequent sets whether it is not certain in DCP. PD significantly reduces the dataset in each pass by reducing the number of transactions and their size to give better performance. So counting time is clearly less than DCP in a reduced dataset  PD is much more scalable than the DCP 8000 7000 6000 5000 lt;.i OOO E f OOO 2000 Minimum Support PD DCP Figure 3. Performance comparison 8 6 4 2 PD DCP owa_L??_L??_L??_L??_L??_L?? o 50 100 150 200 250 Number of Transactions\(k Figure 4. Scalability comparison Figure 3 shows the execution times for different 


Figure 3 shows the execution times for different minimum support. We can see that PD is about 15 times faster than DCP with minimal support at 2% and about 8 times faster than DCP at 0.25%. In Figure 4, to test the scalability with the number of transactions experiments on dataset D are used. The support threshold is set to 0.75 3.2. Comparison of PD with PIP  Predictive item pruning FP-tree algorithm has a complicated data structure in comparison to PD algorithm  PD works by decomposing transactions into short itemsets. Then regular patterns are combined together. Hence data set is being reduced at every pass. As a result, space efficiency is improved. On the other hand, PIP FP-tree, although prunes infrequent items, its hit ratio is not as efficient as PD. So, obviously its space efficiency is not better than PD  As PD always concerns with the current data set less time is required in comparison to PIP FP-tree algorithm  PD is more scalable than the PIP 100r  E F  60 40 0 20 PD  PIP Minimum Support Figure 5. Performance comparison 500 r 400 300 E F  oo 0 100     Number of Transactions\(k   PD  PIP   Figure 6. Scalability comparison 160 In Figure 5, both PIP and PD have good performance on D. But PIP takes substantially more time when minimum support in the range from 0.6% to 2%. When minimum support is less than 0.6%, the number of frequent patterns increased quickly and thus the execution times are comparable. In Figure 6, we compared the scalability of PD with PIP on the dataset D with minimum support = 0.75%. PD was clearly more scalable than that of PIP 4. Conclusion The programs were implemented in C++ very efficiently. Since really large datasets or data warehouse was not available for us, we run the programs of these three algorithms by using test datasets. So performance comparisons are not the absolute values. The results can vary on other 


absolute values. The results can vary on other computers. But it can be guaranteed that performance ratio of the algorithms will remain the same After making the comparisons with sample data, we came to the conclusion that PD algorithm performs significantly better than the other two especially with larger datasets. PD outperforms DCP and PIP regarding running time. On the other hand, since PD reduces the dataset, mining time does not necessary increase as the number of transactions increases and experiments reveals that PD has better scalability than DCP and PIP. So, PD has the ability to handle the large data mine in practical field like market basket analysis and medical report documents mining 5. References 1] R. Agrawal and R. Srikant, "Fast algoritlnns for mining association rules", VLDB'94, pp. 487-499 2] R. J. Bayardo, "Efficiently mining long patterns from databases", SIGMOD'98, pp.85-93 3] J. Pei, J. Han, and R. Mao, "CLOSET: An Efficient Algorithm for Mining Frequent Closed Itemsets \(PDF Proc. 2000 ACM-SIGMOD International Workshop on Data Mining and Knowledge Discovery, Dallas, TX, May 2000 4] Qinghua Zou, Henry Chiu, Wesley Chu, David Johnson, "Using Pattern Decomposition\( PD Finding All Frequent Patterns in Large Datasets", Computer Science Department University of California - Los Angeles 5] J. Han, J. Pei, and Y. Yin, "Mining Frequent Patterns without Candidate Generation \(PDF  SIGMOD International Con! on Management of Data SIGMOD'OOj, Dallas, TX, May 2000 6] S. Orlando, P. Palmerini, and R. Perego, "The DCP algoritlnn for Frequent Set Counting", Technical Report CS2001-7, Dip. di Informatica, Universita di Venezia 2001.Available at http://www.dsi.unive.itl?orlando/TR017.pdf 7] MD. Mamun-Or-Rashid, MD.Rezaul Karim, "Predictive item pruning FP-tree algoritlnn", The Dhaka University  Journal of Science, VOL. 52, NO. 1, October,2003, pp. 3946 8] Park, J. S., Chen, M.-S., and Yu, P. S, "An Effective Hash Based Algoritlnn for Mining Association Rules", Proc ofthe 1995 ACM-SIGMOD Con! on Management of Data 175-186 9] Brin, S., Motwani, R., Ullman, J., and Tsur, S, "Dynamic Itemset Counting and Implication Rules for Market Basket Data", In Proc. of the 1997 ACM-SIGMOD Conf On Management of Data, 255-264 10] Zaki, M. J., Parthasarathy, S., Ogihara, M., and Li, W New Algoritlnns for Fast Discovery of Association Rules In Proc. of the Third Int'l Con! on Knowledge Discovery in Databases and Data Mining, 283-286 11] Lin, D.-I and Kedem, Z. M., "Pincer-Search: A New Algoritlnn for Discovering the Maximum Frequent Set", In Proc. of the Sixth European Conf on Extending DatabaseTechnology, 1998 12] R. Ramakrishnan, Database Management Systems University of Wisconsin, Madison, WI, USA; International Edition 1998 pre></body></html 


tors such as union, di?erence and intersection are de?ned for pairs of classes of the same pattern type Renaming. Similarly to the relational context, we consider a renaming operator ? that takes a class and a renaming function and changes the names of the pattern attributes according to the speci?ed function Projection. The projection operator allows one to reduce the structure and the measures of the input patterns by projecting out some components. The new expression is obtained by projecting the formula de?ning the expression over the remaining attributes [12 Note that no projection is de?ned over the data source since in this case the structure and the measures would have to be recomputed Let c be a class of pattern type pt. Let ls be a non empty list of attributes appearing in pt.Structure and lm a list of attributes appearing in pt.Measure. Then the projection operator is de?ned as follows ls,lm c id s m f p ? c, p = \(pid, s, d,m, f In the previous de?nition, id ing new pids for patterns, ?mlm\(m projection of the measure component and ?sls\(s ned as follows: \(i s usual relational projection; \(ii sls\(s and removing the rest from set elements. The last component ?ls?lm\(f computed in certain cases, when the theory over which the formula is constructed admits projection. This happens for example for the polynomial constraint theory 12 Selection. The selection operator allows one to select the patterns belonging to one class that satisfy a certain predicate, involving any possible pattern component, chosen among the ones presented in Section 5.1.1 Let c be a class of pattern type pt. Let pr be a predicate. Then, the selection operator is de?ned as follows pr\(c p Join. The join operation provides a way to combine patterns belonging to two di?erent classes according to a join predicate and a composition function speci?ed by the user Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE Let c1 and c2 be two classes over two pattern types pt1 and pt2. A join predicate F is any predicate de?ned over a component of patterns in c1 and a component of patterns in c2. A composition function c pattern types pt1 and pt2 is a 4-tuple of functions c cStructureSchema, cDataSchema, cMeasureSchema, cFormula one for each pattern component. For example, function cStructureSchema takes as input two structure values of the right type and returns a new structure value, for a possible new pattern type, generated by the join. Functions for the other pattern components are similarly de?ned. Given two patterns p1 = \(pid1, s1, d1,m1, f1 p2 = \(pid2, s2, d2,m2, f2 p1, p2 ned as the pattern p with the following components Structure : cStructureSchema\(s1, s2 Data : cDataSchema\(d1, d2 Measure : cMeasureSchema\(m1,m2 Formula : cformula\(f1, f2 The join of c1 and c2 with respect to the join predicate F and the composition function c, denoted by c 1   F  c  c 2   i s  n o w  d e  n e d  a s  f o l l o w s    F  c  c 2     c  p 1   p 2   p 1    c 1  p 2    c 2  F   p 1   p 2     t r u e   5.1.3. Cross-over database operators OCD Drill-Through. The drill-through operator allows one to 


Drill-Through. The drill-through operator allows one to navigate from the pattern layer to the raw data layer Thus it takes as input a pattern class and it returns a raw data set. More formally, let c be a class of pattern type pt and let d be an instance of the data schema ds of pt. Then, the drill-through operator is denoted by c c Data-covering. Given a pattern p and a dataset D sometimes it is important to determine whether the pattern represents it or not. In other words, we wish to determine the subset S of D represented by p \(p can also be selected by some query the formula as a query on the dataset. Let p be a pattern, possibly selected by using query language operators, and D a dataset with schema \(a1, ..., an ible with the source schema of p. The data-covering operator, denoted by ?d\(p,D responding to all tuples in D represented by p. More formally d\(p,D t.a1, ..., t.an In the previous expression, t.ai denotes a speci?c component of tuple t belonging to D and p.formula\(t.a1, ..., t.an instantiated by replacing each variable corresponding to a pattern data component with values of the considered tuple t Note that, since the drill-though operator uses the intermediate mapping and the data covering operator uses the formula, the covering ?\(p,D D = ?\(p not be equal to D. This is due to the approximating nature of the pattern formula 5.1.4. Cross-over pattern base operators OCP Pattern-covering. Sometimes it can be useful to have an operator that, given a class of patterns and a dataset, returns all patterns in the class representing that dataset \(a sort of inverse data-covering operation Let c be a pattern class and D a dataset with schema a1, ..., an pattern type. The pattern-covering operator, denoted as ?p\(c,D all patterns in c representing D. More formally p\(c,D t.a1, ..., t.an true Note that: ?p\(c,D p,D 6. Related Work Although signi?cant e?ort has been invested in extending database models to deal with patterns, no coherent approach has been proposed and convincingly implemented for a generic model There exist several standardization e?orts for modeling patterns, like the Predictive Model Markup Language \(PMML  eling approach, the ISO SQL/MM standard [2], which is SQL-based, and the Common Warehouse Model CWM  ing e?ort. Also, the Java Data Mining API \(JDMAPI 3] addresses the need for a language-based management of patterns. Although these approaches try to represent a wide range of data mining result, the theoretical background of these frameworks is not clear. Most importantly, though, they do not provide a generic model capable of handling arbitrary cases of pattern types; on the contrary only a given list of prede?ned pattern types is supported To our knowledge, research has not dealt with the issue of pattern management per se, but, at best, with peripheral proximate problems. For example, the paper by Ganti et. al. [9] deals with the measurement 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


