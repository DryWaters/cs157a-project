html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Performance  Prediction for Association Rule Mining Algorithms Renata Ivancsy Department of Automation Sandor Juhasz Department of Automation Ferenc Kovacs Department of Automation and Applied Informatics Budapest University of Technology and Economics Goldmann Gyllrgy ter 3, H-illi Budapest and Applied Informatics Budapest University of Technology and Economics Goldmann Gyllrgy ter 3, H-IJ11 Budapest and Applied Informatics Budapest University of Technology and Economics Goldmann GyBrgy ter 3, H-1111 Budapest Hungary renata.ivancsy@aut.bme.hu Hungary sandor.juhasz@aut.bme.hu Hungary ferenc.kovacs@aut.bme.hu Abstract - Execution time prediction is very important issue in job scheduling and resource allocation. Association rule mining algorithms are complex and their execution time depends on both the properties of the input data sources and on the mining parameters. In this paper an analytical model of the Apriori algorithm is introduced, which is based on statistical parameters of the input dataset \(average size of the transactions, number of transactions in the dataset the minimum support threshold. The developed analytical model has only few parameters therefore the predicted execution time can be calculated in a simple way. The investigated domain of the input parameters covers the most commonly used datasets, therefore the introduced model can be used widely in field of association rule mining. The constant parameters of the model can be identified in small npmber of test executions. The developed model allows predicting the execution time of the Apriori algorithm in a wide range of parameters. The suggested model was validated by several different datasets and the experimental results show that the overall average error rate of the model is less than 15 I. INTRODUCTION The execution time estimation is an important objective in many applications, and this is especially true for long running, resource intensive, costly data mining algoritluns Performance prediction not only allows estimating the execution time, but also helps to adjust the parameters the execution time is particularly sensitive to. It allows balancing the associated costs and the expected benefits of the execution. Good estimation of resource requirements is also important in distributed systems, where the balance of the running time and the amount of processing resources can be fine tuned The main purpose of data mining is to fmd hidden relationships in large datasets. The association rule mining I] is one of the most important fields in data mining. The process has two separate phases; the first is to find the frequent itemsets, and the second is to detennine the rules from these itemsets. Because the first step works with the original dataset, which can be terabytes in size, its computation cost is much higher, than that of the second step. So most improvements ofthe association rule mining 


step. So most improvements ofthe association rule mining algorithms focus on finding the frequent itemsets. The basic algoritlun for this purpose is the Apriori algorithm suggested by Agrawal et al in 1994 [2]. Several other methods where developed since that time [3], [4], [5], [6 but most of them are the enhancement of the Apriori algorithm 0-7803-8588-8/04/$20.00  2004 IEEE The estimation of execution time of an association rule mining algorithm can be essential when choosing the parameters because of the huge amount of data that is processed by the algorithm The organization of the rest of this paper is as follows Section 2 and 3 introduce association rules and basic mining algorithms respectively. Section 4 describes the general idea of performance modelling based on complexity analysis. In Section 5 we suggest a model for the Apriori algorithm and in Section 6 and 7 the introduced model is identified and validated by experimental results comparing the measured and the predicted data. Our work is summarized in Section 8 II. ASSOCIA nON RULES First we elaborate on some basic concepts of association rules using the formalism presented in [1]. Let I={iJ,i2    imJ be set of literals, called items. Let D={t"tz    t,,} be set of transactions, where each transaction t is a set of items such that t  1. The item set X has support s in the transaction set D if s% of transactions contains X, here we denote s=support\(X rule is an expression in the form of X -7 y whereX,Y  I, andXnY=0. Each rule has two measures of value, support and confidence. The support of the rule X -7 Y is support\(X U V rule X 7 Y in the transaction set D means that c"10 of transactions in D that contain X also contain Y, which can be written in c '" S \(X U Y X mining association rules is to find all the rules that satisfy a user specified minimum support and minimum confidence If support\(X support then the itemset X is called frequent item set III. BASIC ALGORITHMS The Apriori algorithm [2] uses the following theorem to reduce the search space: if an itemset is frequent then all of its subsets are frequent as welL This means it is possible to generate the potentially frequent i+ 1 itemset using frequent i item set. Each subset of candidate i+ 1 item set must be frequent. Hereby it is possible to find all frequent item set using database scan repeatedly. During the ith database scan the Apriori algorithm counts the occurrence of the i itemset and the end of the pass i, it generates the 267 candidates containing i+ 1 item. Figure 1 shows the pseudo code of the Apriori algorithm The Apriori algorithm is a level-wise algorithm thus it accesses the database as many times as the length of the longest frequent itemset is If the dataset is huge, the mUltiple database scan makes the execution of the Apriori algorithm very long. Therefore several algorithms were developed to speed up the Apriori algorithm. These improved algorithms reduce the I/O cost in different ways The Apriori-TID algorithm [2] prunes the dataset during the later steps. It deletes the items from a basket if they cannot be part of a frequent itemset. In this case it can reduce extremely the I/O cost of the algorithm, because it has to handle smaller and smaller datasets The Dynamic Itemset Counting \(Die  works in a different way. It tries to reduce the I/O cost via early candidate generation. The basic idea ofthis algorithm is to generate new candidates as early as possible. It scans the database and increments the counters of the candidates 


the database and increments the counters of the candidates when an itemset became frequent so it is possible to generate new candidates based on the new frequent itemset. The candidate generation is a complex task therefore the new candidates are generated only at checkpoints. The distance of the checkpoints is an important issue in this algorithm; the optimum is reached at about 10,000 read transactions as described in [4 The Direct Hashing and PIUning algorithm \(DHP collects some information about i+ 1 itemset during the i database scan. It does not generate i+ I itemset candidates in the ith step, but it keeps an extra counter array in the memory and an possible itemset is assigned to a counter by a hash function. In this manner multiple i+ 1 itemsets have the same counter. This counter can be used during the candidate generation step. An i+ 1 itemset cannot be a candidate if its counter value is not frequent. This technique heavily reduces the number of candidates hereby it is able to speed up the itemset counting phase The FP-growth \(Frequent Pattern-growth  two-phase mining algorithm. The FP-growth algorithm fITst reads the database and counts the occurrences of the items. Regarding the I-frequent items by the second database read it builds a so-called FP-tree in the memory LI ? {I f requ e n t  i te m s e ts C2 ? GenerateCandidate \(II i +-- 2 while \(Lj_1 *- 0  foreach \( t ED  Increm entCounter \(Cj, t  L, +-- {c e C, IcoC1;I'" &gt; min_,upp i+-- i+l Ci ? GenerateCandidate\(L  L +-- U Li Figure 1. Apriori algorithm 268 After that it recursively generates conditional FP-trees and generates the frequent patterns without generating candidates. The FP-tree contains all market basket in the memory, but it can handle some itemset overlapping among the baskets, hereby it can compress the entire dataset. The main disadvantage of this algorithm is that it needs a large amount of memory. Hence it can handle limited size of datasets, but it works faster than the level wise algorithms IV. PERFORMANCE MODELLING The complexity of an algorithm is handled by setting up a computational model. The constants in the complexity formula can be calculated with regression analysis using a small number of test execution data. After this the formula can provide the execution times for a wide range of parameters A number of performance evaluation and prediction methods exist including iterative algorithms, analytical approaches of asymptotical analysis of the communication and computation complexity [7], adaptive sampling statistics techniques [8] and queuing network models [9 In case of the analytical computational model: the relation between the problem size and computational time can be determined either based on the number of operations to complete or on the memory access pattern [\\0]. The former is applied in the domain of processing-intensive problems \(heavy use of floating point arithmetic is used in case of applications that deal with lots of data, on which they usually do limited amount of simple processing steps. Frequent itemset counting is the member of the second group where the overall performance is dominated by memory access time rather than computational power 


by memory access time rather than computational power By many association rule mining algorithms an additional relevant parameter can be the I/O access time The formula given for the execution times of the frequent itemset counting in Section 5 is based on counting the average number of memory accesses in function of the statistical parameters of the processed dataset V. PERFORMANCE MODEL FOR THE APRIORI ALGORITHM Due to the extremely high memory requirements of the FP-growth based algorithms, the level-wise association rule mining algorithms were in the focus of the execution time modelling. The Apriori algorithm is basis of all the level-wise algorithms therefore its analytical perfonnance model is a very important issue Association rule mining is an NP-complete problem [5 therefore the introduced execution time approximation is valid only for a part of the parameter space. During the parameter identification it is important to choose a parameter domain that covers the commonly used datasets The main parameters of the processed dataset, which can be identified before the association rule mining, are the number of the transactions \(N transactions \(1 the dataset parameters but also on the specified minimum support threshold. The minimum support threshold \(8 used for filtering out the significant itemsets therefore the minimum support value is independent of the dataset To develop an analytical model capturing the performance of the Apriori algorithm a detailed investigation of its operation is needed. The main features of its execution time behaviour are to be detennined. The investigated parameters are independent of each other thus their effects on the response time can be observed separately The evaluation of the ApTiori algorithm took place in a PC laboratory of our department. The algorithm was implemented in C#. The simulations were executed on a computer with Pentium 4 CPU, 2.40 GHz, and 1024MB of RAM. The datasets used by the algorithm are synthetic from [2]. The naming conventions of the datasets are described in Table 1 Figure 2 illustrates the relation between the execution time and the number of transactions in the database. This relation can be well approximated by a linear function The smaller the minimum support threshold is the more itemsets fulfil the minimum support condition, and the more time is needed to discover them. The execution time dependence on the minimum support threshold is shown in Figure 3. The relationship between the two parameters is an inverse proportionality The time as a function of the average size of the transactions can be calculated considering the following facts. The distribution of the transaction lengths can be described by Gaussian distribution with T as expected value. When the 2-candidates are counted, all the two sized subset of the transactions is generated, and its occurrence is counted. This means that processing one transaction generate ti choose two subsets, which are to be counted where 1;. is the size of the given transaction. In the t, level the j-sized subsets are to be generated. The number of these subsets depends on the number of the \(jol itemsets - and through this - also on the minimum support Experimental results show that the Apriori algorithm uses more than 40% of its time to discover the two frequent itemsets The reason for that is that the number of the candidates in the second level is more orders of magnitude higher than in the further levels. It is shown in Figure 4. In this way the execution time dependence from the average size of the transactions can be modelled with a polynomial of the degree three. The constants in the polynomial can be 


degree three. The constants in the polynomial can be calculated by least squares method. The general form of the formula describing the model is shown in Equation 1 4500 4000 3500 U' 3000 2500 .. 2CJOO S 1500 lCJOO 500 o  V-V  1     lOOK 200K 300K 400K SOOK 600K 700K 800K 900K l000K number of transactions IN __ minsup?o.7% ...-min&amp;up:l Figure 2. Execution time of the Apriori algorithm by the number of transactions 10 o  t   0.5% 0.6% 0.7% 0.8% 0.9% 1.0% 1.1% 1.2% 1.3% 1.4% 1.5 minimum wpport \(%I I--Measured ....... Predicted I Figure 3. The execution time It&lt;; a function of the minimwn support using Tl817DIOK 300000 250000 g 200000 .. ?150000 100a00 E 50000 " n c 0 0.5% 0.6% 0.7% o.S% 13.9% 1.11 minimum support {sl 11I1-candidate ils..camlidate II 2-candidato [] 3-:andidste [] 4-candidat&amp; II 5-candidate 01 II 7-cand',dat" olH:andidate II 9-candidatb .10-candidale Figure 4. The number of candidates in each level using the dataset T1817DIOK Table 1. Meaning oftlte synthetic dataset parameters T Average transaction length I Average size of frequent itemsets D Number of transactions VI. PARAMETER IDENTIFICATION The measurements for parameter identification were made by different dataset parameters and support thresholds. The plIliill\\eter T varied from 10 to 25 by 5, the parameter I was in the range of 2 to 8. The performance model does not contain the parameter I, because this parameter cannot be identified before the mining process Earlier experiences showed that there is a liner relationship between the response time and number of transaction hence this pllliill\\eter was fixed to 100.000. The maximal number of the hems that can occur in a transaction was 1000 The parameter of the performance model \(Equation 1 can be calculated by least square method. Table 2 contains the identified parameters ofthe modeL 


Table 2. Identified parameter of the model C1 2.62274E-05 C;! -0.001015643 C3 0.01436588 C4 -0.065519084 269 2500 2000 i - 1500  i 1000 500 o _Measured __ Predicted 9 17 25 33 41 49 57 65 73 81 89 97 105 113 121 129 137 numer Of measurements Figure 5. The calculated and the measured time by different database parameter settings To calculate the error of the model the following function was defmed \(also used in [9 e= ?i: lfi - l i n i=1 2 where n is the number of the measurements, fi is the measured time and!;" is the calculated time Figure 5 shows the calculated and the measured execution times by all the different parameter settings where the minimum support threshold changes from 0.5 tom 1 %. It is clearly visible in Figure 5 that the prediction function follows well the behaviour characteristics of the real execution times when varying the different parameters. The average relative error of the model calculated according to Equation 2 was 1 n If; -ll e=-L I I =0.123445 n i=1 f 3 VII. MODEL V ALIDA nON After the model identification process several measurements were taken to validate the model. Table 3 contains the synthetic database parameters that were used during the model validation Figure 6 shows the measured mining time and the predicted mining time by the different datasets. It is possible to realise that the model follows well the measured execution time In some cases, especially in case of low support values the relative error of the model can be the double \(max 30 Table 3. Synthetic dataset parameters during the model validation T 18,22 7,8 D 50K, lOOK, 250K 270 The average relative error of the model calculated according to Equation 2 was e=? tlh -ll n 1=4 h 0.133185 \(4 The average error ratio is not significantly different from the error ratio of the identified model. Therefore the introduced performance model can be used for predicting the execution time of the Apriori algorithm in the investigated dataset parameter and support value intervals VIII. CONCLUSION In this paper a basic level-wise association rule mining algorithm, the Apriori algorithm was developed and its analytical performance model was introduced. Three input parameters were taken into the consideration in this model Two parameters, namely the number of transactions and average size of transactions describes the input dataset The third parameter, the minimum support threshold is the input parameter of the mining process. The constant 


input parameter of the mining process. The constant parameters of the model can be identified by few measurements using least square method A simple formula was given to anticipate the performance of the algorithm, described welI the behaviour of the data mining algorithm for a wide range of parameters. The main contribution of the paper was predicting the behaviour of a complex probability based data mining algorithm in a relatively simple, closed numerical form allowing a good estimation of execution times The model was validated by comparing the calculated and measured performance. Experimental results showed that the suggested model is reasonably accurate in a wide domain having an average error below 15 percent 1 2 3 4 5- 6 1 S 9 10 11 12 13 14 15 16 17 1$ 19 2\(121 22 23 204 25  16 27 2S 29 30 I ....... M88$urai - p,.:lided I Figure 6_ Model validation: the C11lculated and the measured time The introduced model can be used not only for predicting execution times of the Apriori algorithm, but also for predicting the response time of the other level-wise association rule mining algorithms. The reason behind this is that these algorithms are based on the Apriori algorithm thus they inherit several properties from it. Of course the identification of the constants in the formula must be repeated when moving to different algorithm in the same way as when moving to different hardware environments IX. ACKNOWLEDGEMENTS This work was supported by IBM Hungary and the fund of the Hungarian Academy of Sciences for control research and the Hungarian National Research Fund \(grant number T042741 X.REFERENCES 1] R. Agrawal, T. Imielinski and A.Swami, "Mining association rules between sets of items of large databases" in Proc. of the ACM SIGMOD Int/'/ Conf On Management of Data, Washington, D.C.,USA 1993, pp 207-216 2] R. Agrawal and R. Srikant, "Fast algorithms for mining association rules" in Proc. 20'h Very Large Databases Conference, Santiago, Chile, 1994, pp 487-499 3] J. S. Park, M. Chen, and P. S. Yu, "An effective hash based algorithm for mining association rules" in Proc of the 1995 ACM Int. Con/. on Management of Data San Jose, California, USA, 1995, pp. 175-186 4] S.Brin, R. Motawani, J.D. Ullman and S. Tsur Dynamic Item set counting and implication rules for market basket data" in Froc. of the ACM SIGMOD Inti '1 Con/. On Management of Data, Tucson Arizona, USA, 1997, pp. 255-264 5] M. J. Zaki, "Scalable algorithms for association mining", IEEE Transaction on Knowledge and Data Engineering, Vol 12. No 3. May/June 2000, pp. 372390 6] J.Han, J. Pei and Y. Yin, "Mining frequent patterns without candidate generation" in Proc. of the 2000 ACM-SIGMOD Int'l Conf. On Management of Data Dallas, Texas, USA, 2000, pp. 1-12 71 David R. Helman, David A. Bader, and Joseph Jara A randomized parallel sorting algorithm with an experimental study", Journal of Parallel and Distributed Computing, 52\(1 8] J. Landrum, J. Hardwick, and Q.F. Stout, "Predicting algorithm performance", Computing Science and Statistics, 30, 1998, pp. 309  314 9] P. Cremonesi and C. Gennaro, "Integrated Performance Models for SPMD Applications and MIMD Architectures", IEEE Transactions on Parallel and Distributed Systems, Vol. 13, No.7., 2002, pp 745-757 


745-757 10] U. Meyer et aI., Algorithms for Memory Hierarchies LNCS 2625, Springer-Verlag, Berlin, 2003, pp. 32035 4 11] Mohammed J. Zaki, "Scalable Algorithms for 271 Association Mining", IEEE Transaction on Knowledge and Data Engineering, Vo1.l2, No.3 2000, pp. 372-390 pre></body></html 


support count, the smaller the best cutting level 0 5 10 15 20 25 30 35 40 1 2 3 4 5 6 Cutting level T im e s ec  Figure 8. Execution times of CBW for various  s with minsup count = 12 0 1 2 3 4 5 36912 Minimum support count B es t  Figure 9. Evolution of the best cutting level under different minsups 4.2 Synthetic database We next compared the four algorithms on the synthetic data sets under different minsups. The results were shown in Figures 10, 11 and 12. Our observations were as follows 1. CBW outperformed all other methods in all cases 2. While all algorithms suffered from the combinatorial exploration of itemsets due to low support constraints, our CBW exhibited the best in maintaining its performance 3. The longer the itemsets become, the worse all four algorithms performed. The reason is that the cost for candidate generation, support counting Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C and conditional pattern and FP-tree construction grows as the itemset length increases 0 400 800 1200 1600 2000 0.0 0.5 1.0 1.5 2.0 2.5 minsup T im e se c  CBW FP-growth Partition Apriori Figure 10. Execution times of Apriori, Partition 


Figure 10. Execution times of Apriori, Partition FP-growth and CBW on T15.I4.D200K 0 400 800 1200 1600 2000 2400 0.0 0.5 1.0 1.5 2.0 2.5 minsup T im e se c  CBW FP-growth Partition Apriori Figure 11. Execution times of Apriori, Partition FP-growth and CBW on T15.I6.D200K We also evaluated the execution times of CBW under different cutting levels \(minsup = 1.0 influence of minsup to the cutting levels. We only showed the results for T6.I4.D100K in Figure 13 and 14; similar results were observed for the other datasets The results conformed to those observed in Figures 8 and 9 Finally, we conducted an experiment to evaluate the scalability of the four algorithms. The results were shown in Figure 15, where we omitted Apriori because its performance was significantly inferior to the others As the figure showed, our CBW exhibited the best in scalability while FP-growth exhibited the worst 0 400 800 1200 1600 2000 2400 0.0 0.5 1.0 1.5 2.0 2.5 minsup T im e se c  CBW FP-growth Partition Apriori Figure 12. Execution times of Apriori, Partition FP-growth and CBW on T15.I8.D200K 0 20 40 60 80 100 120 1 2 3 4 5 6 Cutting level T im e s ec  


Figure 13. Execution times of CBW on T6.I4.D100K for various  s 5. Concluding remarks 5.1. Summary In this paper, we have described a new efficient algorithm for frequent itemsets mining. Unlike contemporary algorithms that either adopt a top-down or a bottom-up traversal throughout the itemset lattice to search for frequent itemsets, our algorithm employs a clever guess on the most promising itemset level cutting-level there. Then it performs a downward search, followed by an upward search to discover all other frequent itemsets. Empirical study showed that our algorithm is more than an order of magnitude faster than the Apriori variants Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C 01 2 3 4 5 0.25%0.50%0.75%1.00 minsup B es t cu tt in g l ev el Figure 14. Evolution of the best cutting level for CBW under different minsups, running on T6.I4.D100K Our CBW algorithm has been incorporated into an online multidimensional association rule mining system currently under development [10]. In the future we will incorporate into CBW the taxonomy information and extend it to allow multiple minimum support specification [13 0 100 200 300 400 500 600 700 800 2 4 6 8 10 12 14 16 18 20 Number of transactions \(x 10,000 T im e s ec  CBW FP-growth Partition Figure 15. Scalability evaluation of CBW, FPgrowth and Partition running on T15.I8.D200K with minsup = 1.0 5.2. Comparison with related work To our knowledge, [9] is the only work on combining the top-down and the bottom-up searches for association mining. But their approach and 


for association mining. But their approach and intention are quite different from ours First, rather than starting from the middle of the search space and progressively searching towards both ends, their approach proceeds from both ends of the search lattice and progressively searches towards the middle Second, their approach aims at discovering, instead of all frequent itemsets, the maximal frequent itemsets i.e., itemsets having no supersets, which work is quite simple compared to the work for discovering all frequent itemsets. Furthermore, on applying their approach to the work of frequent itemsets mining, the top-down pruning" technique on which their approach relies will become useless because the subsets of a frequent itemset found in top-down search still have to been counted to know their supports. In this way, the top-down search becomes unnecessary and their method will degenerate to the Apriori algorithm On the contrary, we believe that our method can be adapted to the problem of mining maximal frequent itemsets [1][3][9]. Indeed, we are currently working on applying our CBW to this problem and hope to have result in the near future References 1] R.C. Agarwal, C.C. Aggarwal, and V.V.V. Prasad Depth first generation of long patterns," in Proceedings of 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2000, pp 108?118 2] R. Agrawal and R. Srikant, "Fast Algorithms for Mining Association Rules," in Proceedings of the 20th VLDB Conference, 1994, pp. 487?499 3] R.J. Bayardo Jr., "Efficiently Mining Long Patterns from Databases," in Proceedings of 1998 ACM SIGMOD International Conference on Management of Data Seattle, Washington, USA, 1998, pp. 85?93 4] S. Brin, R. Motwani, J.D. Ullman, and S. Tsur Dynamic Itemset Counting and Implication Rules for Market Baseket Data," SIGMOD Record, Vol. 26, 1997 pp. 255?264 5] Database Research Group in the Department of Computer Science and Engineering at the Chinese University of Hong Kong http://www.cse.cuhk.edu.hk/~kdd/program.html 6] J. Han, J. Pei, and Y. Yin, "Mining Frequent Patterns Without Candidate Generation," in Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, Dallas, TX, USA, 2000, pp. 1?12 7] J. Hipp, U. Guntzer, and G. Nakhaeizadeh, "Algorithms for Association Rule Mining?A General Survey and Comparison," SIGKDD Explorations, Vol. 2, 2000, pp 58?64 8] J. Hipp, U. Guntzer, and G. Nakhaeizadeh, "Mining Association Rules: Deriving a Superior Algorithm by Analyzing Today  s Approaches," in Proceedings of 4th European Symposium on Principles of Data Mining and Knowledge Discovery \(PKDD  00 9] D. Lin and Z.M. Kedem, "Pincer-search: An Efficient Algorithm for Discovering the Maximum Frequent Set Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C IEEE Transactions on Knowledge and Data Engineering, Vol. 14, No. 3, 2002, pp. 553?566 10] W.Y. Lin, J.H. Su and M.C. Tseng, "OMARS: The Framework of an Online Multi-dimensional Association Rules Mining System," in Proceedings of the 2nd International Conference on Electronic Business, Taipei Taiwan, 2002, pp. 216?225 11] J.S. Park, M.S. Chen, and P.S. Yu, "An Effective HashBased Algorithm for Mining Association Rules," in Proceedings of the 1995 ACM SIGMOD International 


Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data, San Jose, CA USA, 1995, pp. 175?186 12] A. Savasere, E. Omiecinski, and S. Navathe, "An Efficient Algorithm for Mining Association Rules in Large Databases," in Proceedings of the 24th VLDB Conference, 1995, pp. 432?444 13] M.C. Tseng and W.Y. Lin, "Mining Generalized Association Rules with Multiple Minimum Supports," in Proceedings of International Conference on Data Warehousing and Knowledge Discovery, Munich Germany, 2001, pp. 11-20 14] M.J. Zaki, "Scalable Algorithms for Association Mining," IEEE Transactions on Knowledge and Data Engineering, Vol. 12, No. 2, 2000, pp. 372?390 Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C pre></body></html 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


