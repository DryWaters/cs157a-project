2004 IEEE International Conference on Multimedia and Expo ICME Multimedia Indexing and Retrieval with Features Association Rules Mining Anicet Kouomou-Choupo, Laure Berti-Equille Annie Morin IRISA University of Rennes I iakouomou,Laure.Berti-Equille,Annie.Morin}~irisa Abstract The administration of very large collections of images accentuates the classical problems of indexing and eBciently querying information 
This paper describes a new method applied to very large still image databases that combines two data mining techniques: clustering and association rules mining in order to better organize image collections and to improve the performance of queries The objective of our work is to exploit association rules discovered by mining global MPEG-7 features data and to adapt the query processing In our experiment we 
use five MPEG-7 features to describe several thousands of still images For each feature we initially determine several clusters of images by using a K-mean algorithm Then we generate association rules between different clusters of features and exploit these rules to rewrite he query and to optimize the query-by content processing 1 Introduction Because of the growing demand for 
database and information systems support in the area of modeling managing and processing digital media, there is a need to explicitly capture a fair amount of information content as well as application-specific semantics by mean of a variety of metadata e.g multimedia indexes attributes-based annotations and intentional descriptions in order to allow appropriate access to selection of and processing of digital media involving very large raw data volumes But most 
of the current practices in the context of multimedia data management are still quite ad hoc Content-based retrieval on raw data means that query capabilities are limited to the number of available matching algorithms Performance is lacking when queries are executed on large data sets Indirect retrieval and processing however that use abstract information or metadata seem to be a promising 
approach to enhance querying and processing Our approach is to exploit metadata such as association rules extracted from mining visual global features data and to better organize and index image collections according to these feature association rules We propose a new method combining two data mining techniques clustering and association rules mining applied to all kind of very large still image databases in order to optimize the organization of 
data and the query processing The paper is organized as follows In section 2 we present the new method including clustering and association rule mining we propose for bcner organizing very large image databases In section 3 we present and discuss our experimental results Finally Section 4 concludes the paper and presents OUT future work 2 Mining 
global features for query planning The method we proposed is described in Figure 1 and includes two steps 1 the off-line image indexing by clustering and association rule mining on global MPEG-7 feature data and 2 the on-line retrieval exploiting association rules metadata in order to accelerate access to queried images 0-7803-8603-5/04/$20.00 02004 IEEE 1299 


2.1 Indexing images with clustering and association rule mining Image database indexing consists of three modules see figure I the featuring module the classifier and the rule generator Data featuring module calculates MPEG-7 features for the whole image set For our experiment we work with two MPEG-7 color features CoiorLqvout; ScaiabieColor two MPEG-7 texture features HomogeneousTexture; EdgeHistogram and one form feature Regionshape 3 Our image indexing process has two steps 1 for each feature our system generates a XML tile describing all the stored images Each image is then represented as a multidimensional numeric vector for used features I1 indexation step in Figure 1 2 the automatic classifier and the rule generator described in next subsections cany out tasks of organizing image collection I2 indexation step 2.1.1 Clustering To reduce the dimensionality a clustering approach is used It reduces considerably the number of images subcollections by clustering similar images into similar groups The clustering algorithm is a variant of k-medoids Z The automatic classifier organizes the file of features in clusters and builds an access index We use the K-mean algorithm for clustering Each cluster contains images considered to be similar according to the distance associated to a given feature It is then identified by a number cluster and the name of the feature FeatureName Finally an image is described by a set of clusters in which it belongs for all features The rule generator then uses the files of clusters produced by the automatic classifier to extract relations named association rules between the clusters 2.1.2 Association rule mining Association rule mining has been intensively investigated in the data mining literature Many efficient algorithms have been proposed the most popular being Apriori I Association rule mining typically aims at discovering associations between items in a transactional database Given a set of transactions D  TI  Tn and a set of items I  il  im such that any transaction Tin D is a set of items in I an association rule is an implication A  B where the antecedent A and the consequent B 1300 


are subsets of I and A and B have no common items A _c I is called a k-itemset if the size of A is k For an association rule to be strong the conditional probability of B given A has to be higher than a threshold called minimum confidence The support is defined such as s  YhSlilDl and the confidence is defined such as c  4hS//JAI Association rules mining is normally a two-step process In the first step frequent item-sets i.e item sets whose support is no less than a minimum support are computed iteratively in the ascending order of their size using a level wise algorithm In the second step association rules in the form AI A  Az I Az CAI with confidence greater or equal to minimum confidence are derived from each frequent item-set A computed in the first step In our approach, we used the Apriori algorithm I in order to discover association rules among the clusters of global MPEG-7 features extracted from the image database Generated association rules are implications such as clusfed FeatureName cluster#>;<FeahrreNome   3 ciuster#>;<FearureName sz c with the following semantics clusfer is the cluster identifier for the feature FeatureName  and c are percentages indicating respectively the support and the confidence of the rule previously defined The lefl part of a rule is made up of one or several couples identifying the clusters such as clusted>;<FeahrreName The right part is limited to only one couple see Figure 1 for six instances of the extracted rules 2.2 Retrieving images While retrieving an image from an image database the goal of the user is to fmd all the images similar to the specified query Two cases can arise either the user selects the features whereby searching must be made or the user does not have any idea of the features to use We work with the second case we considered to be more general and the retrieval process consists of four main steps 1 image submitted as query is processed and all the global features managed by the procedure of indexing RI retrieval step in Figure 1 are produced for this image by the featuring module In our example we considered five MPEG-7 features two for the colour two for the texture and one for the form Each of the 7727 images is described by five variables noted CL ColorLqouf and SC ScalableColor HT HomogeneousTedwe and EH EdgeHistogram RS RegionShape 2 the cluster selector uses the file of clusters and thc description of the query to deduce for each feature the cluster in which the query could he the closest R2 retrieval step in Figure 1 3 we use association rules to reduce the number of clusters in which we make sequential search The rule selector chooses among rules available those which describe relations between clusters provided by the cluster selector R3 retrieval step In other words a rule is selected if all the clusters implied in the rule in the lefl and right pan are elements of the whole set of clusters selected in the R2 retrieval step 4 selected clusters and rules are transmitted to the searching module R4 retrieval step If no rule is selected then sequential search is made in all the selected clusters of images On the contrary case sequential search will he done only in the clusters not appearing in the right part ofthe rule Results of sequential search in the clusters are then merged according to the principle given in 4 3 Experiment and Discussion The method is implemented in C+t All the algorithms nm on a PC under Linux Its CPU is a Pentium 4 2.4 GHz with 1024 Mb of main memory and 80 Gb of local disk We worked with a base of 7727 still images For each of the five MPEG-7 features we gather the images into 5 clusters with the k-mean algorithm We chose a minimum support of 10 and a minium confidence of 50 for the calculation of association rules between clusters with the Apriori algorithm I Under these conditions the system produces 6 relevant rules whose support varies between 10 and 13 This relatively weak support is explained Indeed the value of the support is a decreasing hnction of the number of clusters chosen by feature If we suppose for example an uniform distribution of the images in each of the 5 clusters for each feature then the support of the rules is majored by 20 The use of association rules reduces the number of features to explore for on-line searching In this case the search time is lower than sequential search time including the results fusion Our CBIR system has been queried by 500 queries For 165 of them the 1301 


system makes use of the generated association rules that is to say the usage ratio of 33 Image retrieval guided by association rules offers an interesting perspective to explore for improving query by content performance The searching time is reduced with the proposed method compared to the sequential searching time including the fusion of results Table I Our objective is now to improve these performances by refining combination strategies ofassociation rules Table 1 Experimental results of searching time for 500 queries over 7727 images Queries using association rules Queries without using association des CL is extremely significant and allows to induce other values of features This permanence of associations implying CL led us to estimate the topology of a Bayesian network hetwccn thc five variables Figure 2 Indeed, we note that the position of the origin of the network is CL  Average Average Time sequential searching time speed For every kind of image databases we can build searching with the UP s off-line such Bayesian networks with global MPEG-7 features that can be used for improving the access 1.73 time to queried images time s method s 46.50 44.77 46.71 45.25 1.46 4 Conclusion In this article we describe a new method that can  Figure 2 Bayesian network of features In order to validate our approach we also compared it with statistical methods such as multiple correspondence analysis MCA Our objective was initially to check if we find results close to association rules and if we cannot find some others Most interesting results appear in the following Table 2 the numbers between brackets indicate the confidence of the rule Table 2 Experimental results I Strong relations between I Induced modalities I modalities of variables I 2,CL and 2,SC I,CL\or\(3,CL and I 0,HT 54.4 I OX 59.7 and/or 2,Hn I 4,EH\\(51.3 0.CL and 3.HT I OX 47.9 or The variables are noted CL ColorLoyout and SC ScalableColor HT HomogeneousTexture and EH EdgeHistogram FLY Regionshape This table is interesting because we find association rules showed in Figure 1 and generated by association rule mining and also more complicated rules with several variables obtained by multiple correspondence analysis We especially remark that the feature of color improve the global search time for a content-based information retrieval system The interesting point is this method can be applied to all kind of large image databases by exploiting association rules extracted from mining the clusters of global image features Our research perspective is to adaptively combine and interchange features data in order to build optimized query plans and also to rewrite queries by content and to improve the performances and the quality of results 5 References l R Agnwal T Imielinski A Swami 221\221Mining Association Rules Between Sets of Items in Large Databases\224 ACM SIGMOD Infernofional Conference on Management of Data Volume 22 pp 207-216 Washington ACM press 1993 2 S A Berrani L Amsaleg P Gms 223Approximate k Nearest Neighbor Searches A New Algorithm with Probabilistic Control of the Precision\224 Tech Rep INRL4 No 4675,2002 3 B S Manjunath P Salembier T Sikora Innoduction Io MPEG-7 John Wiley  Sons 2002 4 S Nepal M V Rmakrishna 223Query Processing Issues in Image Multimedia Databases\224 Proceedings of fhe 15\223 Infernafional Conference on Data Engineering \(ICDE 99 pp 22-29, Australia 1999 1302 


terns of itemsets which are the subsets of I  in L2 as shown below vr: = n\({vr; I VI; I 8 However, by utilizing I  s  repeating counts in Vr, V I  E L2 we can find the minimal set of candidate calendar patterns for each itemset in C,. From E   count of a candidate calendar pattern of an itemset I  in C3 uI;.count, will never be larger than any u p  .count  JI  c I  Therefore, the maximal value of uI;.count, GI:.,iount can be obtained by 9 Furthermore, for each I:, uI: is removed if GI;.cozmt 5 m x IvpI k &gt;= 4, &amp;e obtained by ur:.count = min\({vp.count I VI  c I vrL = n\({v,;-. I v1j-l c I IO In the final phase, to discover the frequent patterns from candidates, all candidate itemsets along with their candidate calendar patterns are counted in the database in one shot Note that a candidate itemset is only scanned in the time intervals covered by its candidate calendar patterns. As a result, a frequent itemset passes the match ratio of a calendar pattem can be found. Calendar-based temporal association rules are then obtained 4 Experimental Results We compare the performance of our method with that of the  Temporal-Apriori  algorithm proposed in [4] by run ning them on several experiments with a PC with 2.2 GHz CPU and 1.OG memory. The technique introduced in [ I is used to generate four synthetic datasets, T10.14.D400K T10.14.D600K, T10.14.D800K and T10.14.D1000K, to form input databases to the algorithms in the experiments where T is the mean size of a transaction, I is the mean size of potential maximal large itemsets, and D is the num her of transactions in units of K, i.e. 1000. The calendar schema \(year : [2001, ..., 20041, month : [ l ,  ..., 121, day l, ..., 311 is 0.8. These four datasets are divided into, 400, 600, 800 and 1000, partitions, respectively, where each partition con tains 1000 transactions and is corresponding to one basic time interval in the calendar schema. In the following, we firstly give one experiment to compare the performance of Temporal-Apriori and our method with different scales on data size and support threshold. Then, to demonstrate the effectiveness of our 1-star aggregation mechanism, another experiment is given to compare the performance of generat ing all k-star and only 1-star candidate calendar patterns of LZ in the first database scan 4.1 Experiment 1 In experiment 1, to compare Temporal-Apriori and our method, we use both methods to discover calendar-based temporal association rules in four datasets with different sizes mentioned above on different scales of support thresh old. In Figure 3, the execution time of Temporal-Apriori applying to different datasets with support thresholds, 0.05 0.06,0.07,0.08,0.09 and 1 ,  respectively, are drawn in dotted lines while our method are drawn in solid lines. Moreover in Figure 4 and Figure 5 ,  the black pillars represent the av erage number of candidate itemsets and candidate calendar patterns generated by Temporal-Apriori in these four differ ent databases, respectively. The gray pillars are those gen erated by our method. As for the white ones, they indicate the average number of frequent itemsets and frequent cal endar patterns discovered by both methods in the database From these three figures, we can see that though our method generates slightly more candidates than Temporal-Apriori we have better performance with smaller support thresholds For smaller support thresholds, the length of maximal fre quent itemsets will be longer. Therefore, the number of database scans in Temporal-Apriori increases when the sup 3125 


x lo 0.05, 0.06 0.07 0.m om 0.1 support t h m h o l d Figure 3: Performance comparison between Temporal Apriori and our method port threshold decreases. Meanwhile, our method always scans the database at most twice. As a result, our method is efficient in all kinds of support thresholds, but the perfor mance of Temporal-Apriori varies dramatically for different cases 4.2 Experiment 2 To demonstrate the effectiveness of our 1-star aggrega tion mechanism, we compare the difference between apply ing and not applying the mechanism into discovering candi date calendar patterns in the first scan of database. Figure 6 shows the average number of 1-star and all k-star calendar patterns discovered in the first database scan with four differ ent datasets. Since 1-star calendar patterns is the subset of all k-star patterns, it is clear that the number of 1-star calendar patterns will be smaller than that of all k-star patterns. For the calendar schema used in our experiments. the number of 1-star calendar patterns is close to 90% of all k-star pattems But the percentage of 1-star calendar patterns will be smaller if a more complex calendar schema, e.g., a schema with more time granularities or more time intervals in one granularity is used. Though the number of 1-star calendar patterns is ev idently smaller than that of all k-star calendar patterns, the efficacy of the mechanism can not be directly perceived eas ily. Thus, the execution time for discovering all candidate calendar patterns of large 2-itemsets by scanning 1-star and all star candidate calendar patterns in the database, respec tively, are given in Figure 7. The results obtained by utilizing 1-star calendar pattern mechanism are obviously better than those obtained by not applying the mechanism 5 Conclusion We have proposed an algorithm to discover all calendar based temporal association rules that occur over any time 1200 0 07 SUPP Figure 4: Average number of candidate itemsets and frequent itemsets generated by Temporal-Apriori and our method om 007 008 om 0 1 suppon threshold Figure 5: Average number of candidate calendar patterns and frequent calendar patterns generated by Temporal-Apriori and our method 31 26 io I I 1 star candidate calendar patterns I all 611, candtdats calendar p a t l e d  '7 n i om DO9 0 1  I uppod threshold Figure 6: Average number of 1-star and all star carididate calendar patterns generated in the first database scan Figure I: Execution time for discovering candidate calendru pattems of large 2-itemsets by scanning I-star and a l l  star candidate calendar patterns in the database, respectively interval in a temporal database. An example of a calendar based temporal association rule is "chocolates and flowers are frequently purchased together on February 14th in ev ery year." A user-given calendar schema, e.g., year, month and day, is firstly adopted to specify the interested time in tervals as calendar pattems. Then, in every time interval the frequent %itemsets are discovered along with their 1-star calendar patterns. After that, the information of the rest k star calendar pattems of the frequent 2-itemsets are aggre gated from their 1-star calendar patterns. Thus, the mini mal number of calendar pattems are generated and counted in the database. Further, to avoid multiple scans over the database, all candidate itemsets are generated from discov ered frequent 2-itemsets and the Apriori downward property 


ered frequent 2-itemsets and the Apriori downward property is utilized to generate the minimal number of their candidate calendar patterns. Finally, all frequent itemsets and their cal endar patterns are discovered in one shot. Calendar-based temporal association rules are then obtained. Experimental results have shown that our method is more efficient than others References 11 R. Agrawal and R. Srikant. Fast Algorithms for Min ing Association Rules. In Proceedings of the Inrema tional Very Large Database Conference , pages 487 499,1994 2 ]  J. Han, G. Dong, and Y. Yin. Efficient Mining of Par tial Periodic Patterns in Time Series Databases. In Pro ceedings of the Inremational Conference on Data En gineering, pages 106-1 15, 1999 3] C. H. Lee, C. R. Lin and M. S. Chen. Sliding-Window Filtering: An Efficient Algorithm for Incremental Min ing. In Pmceedings of the ACM 10th Intemational Conference on Information and Knowledge Manage ment, pages 263-270,2001 4] Y. Li, P. Ning, X. S. Wang and S .  Jajodia. Dsicovering Calendar-based Temporal Association Rules. Data and Knowledge Engineering, Vo1.44, No.2, pages 193-21 8 2003 5] B. Ozden, S. Ramaswamy, and A. Silberscbatz. Cyclic Association Rules. In Proceedings of the 15th Inter national Conference on Data Engineering, pages 41 2 421,1998 6] S. Ramaswamy, S. Mahajan, and A. Silberschatz. On the Discovery of Interesting Patterns in Association Rules. In Proceedings of the International Very Large Database Corference , pages 368-379,1998 7] J. F. Roddick and M. Spiliopoulou. A Survey of Tem poral Knowledge Discovery Paradigms and Methods IEEE Trans. Knowledge and Data Engineering, Vol 14, Issue 4., pages 75C!-767,2002 3127 pre></body></html 


decreased. However, refer to Fig.6, it brings the following problem  110 100 010 100 100 011 001 111 100 011 100 34 33 34 DSD u u u                         u              101 100 001 100 100 010 011 111 100 011 100 


100 34 33 34 DSD u u u                          u             Fig.6: Over Hiding problem of setting  1 in S No matter the left-hand or right-hand equation, the support of {1, 2} in D' is 0. That is, item 1 and item 2 never appear toge ther, and they are mutual exclusive! This situation almost never happens in the normal database. The attackers may interest in this situation and infer that {1, 2} is hidden deliberately. To hide the sensitive patterns, only need to make their supports smaller than minimum support and need not to decrease their support to 0. To solve the problem, we inject a probability ? which is called Distortion probability into this approach. Distortion probability is used only when the column j of the sanitization matrix S contains only one  1  i.e. Sjj = 1 0 1 d   m k k j i k  S D  m j n i j i  d d d d   1  1     D  i j  h a s   j probability to be set to 1 and 1  j probability to be set to 0 Lemma 1: Given a minimum support ?, and a level of confidence c. Let {i, j} be a pattern in Marked-Set, nij be the support count of {i, j}. ? is the Distortion probability of column j Without loss of generality, we assume that Sij  1. If ? satisfies    D n i j  u  u  V U   a n d    


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


