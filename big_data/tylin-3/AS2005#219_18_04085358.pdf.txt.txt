html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Paper  Identification Number: 1568965532   Abstract  A decision tree is an effective means of data classification from which rules can be both expressive and precise However decision tree is only applicable in the applications that the data is expressed with attribute-value pairs.  Since genetic data are not attribute-pairs, the only method that we know of to make decision-tree for them is based on a greedy graph-based data mining algorithm known as DT-GBI.  Due to its greedy nature some of the important rules may be missed.  Even though recently some attempts to make the algorithm complete has been presented, the computational complexity of algorithm increased so much that is not appropriate for practical purposes.  In this paper we present an approach to make decision tree for DNA based data which basically uses regular association-rule algorithms.  Thus it has computational complexity which is much more tractable. Our contribution in this paper is to convert DNA based problems to regular data mining methods and by this conversion we present a method that can be applied to all classes of classification based on association rules  Index Terms  DNA, decision support system, Sequence estimation  I. INTRODUCTION Analysis of DNA has been the center of attention of several researchers.  A lot of different form of tools and methods has been deployed to gain more understanding about DNA sequences.  Even though decision trees [1, 2] and induction rules [3, 4] have been deployed for classification purposes, we should consider each node as a categorical value and decision nodes in the tree only consist of one nucleotide.  In the last  This work was supported in part by the Japanese Munbokagokusho Ph.D. fellowship Ashkan Sami. is with Graduate School of Engineering, Tohoku University; 6-6-11-805, Aoba, Aramaki, Aoba-ku, Sendai 980-8579 JAPAN and Department of Computer Science and Engineering, Shiraz University; Shiraz, Iran \(corresponding author phone:+81-90-9747-5938; fax: +81-22-795-4847, e-mail ashkan.sami@most.tohoku.ac.jp Makoto Takahashi is with Graduate School of Engineering, Tohoku University; Sendai; JAPAN. email makoto.takahashi@most.tohoku.ac.jp decade, there has been extensive growth of known genetic information.  Different methods have been employed to reach a better understanding of DNA sequences.  As stated in regular deployment of regular decision tree we cannot have structures for decision making nodes To find interrelationships between DNA sequences, DNA sequences have been treated as graph structures.  Since graph structure is represented by proper relations, knowledge discovery from graph structured data poses a general problem for mining from structured data. Some examples amenable to graph mining are finding typical web browsing patterns identifying typical substructures of chemical compounds and discovering diagnostic rules from patient history records Majority of the methods widely used is designed for data that do not have structure and are represented by attribute-value pairs Decision trees are very expressive method of presenting relationship and at the same time classification rules.  Graph structures, by all means, are no exception.  However due to NP-completeness nature of graph isomorphism, the only approach to make a decision tree based on patterns is based on B-GBI [5] called DT-GBI [6] \(Based on best of our knowledge Even though, they have reached a good results as far as the performance is concerned they only applied their algorithm to DNA promoter data in UC-Irvine  s Machine Learning Repository [7] which only consists of 106 sequences of 


Repository [7] which only consists of 106 sequences of 57-nucleotide sequences Due to greedy nature of B-GBI, it is quite possible to miss some of the more important patterns.  Recently there has been an attempt to make GBI more complete by introduction of CI-BGI [8].  Even though the authors have presented the results based on same promoter data and found all the patterns.  They not only confess some restrictions for patterns that CI-BGI finds but also the exponential increase in computational time The rest of this paper is as follows, in section 2 we present a review of the graph-based data mining methods and classifier based on GBI.  Section 3 provides the definition and the algorithm.  Experimental results are given in section 4 and section 5 concludes the paper Decision Tree Construction for Genetic Applications Based on Association Rules Ashkan Sami, Student Member IEEE and Makoto Takahashi Graduate School of Engineering Tohoku University Sendai, Japan ashkan.sami@most.tohoku.ac.jp Paper Identification Number: 1568965532  II. REVIEW OF RELATED GRAPH-BASED DATA MINING Graph-Based Induction \(GBI devised for the purpose of discovering typical patterns in a general graph data by recursively chunking two adjoining nodes. There can be more than one link between any two nodes GBI is very efficient because of its greedy search GBI does not lose any information of graph structure after chunking, and it can use various evaluation functions in so far as they are based on frequency. It is not, however, suitable for graph structured data where many nodes share the same label because of its greedy recursive chunking without backtracking but it is still effective in extracting patterns from such graph structured data where each node has a distinct label or where some typical structures exist even if some nodes share the same labels B-GBI is improved version of GBI in three aspects: 1 incorporating a beam search, 2 function to extract patterns that are more discriminatory than those simply occurring frequently, and 3 labeling to enumerate identical patterns accurately GBI employs the idea of extracting typical patterns by stepwise pair expansion as shown in Fig. 1  Typicality  is characterized by the pattern  s frequency or the value of some evaluation function which is calculated by the pattern  s frequency. In Fig. 1 the shaded pattern consisting of nodes 1, 2 and 3 is thought typical because it occurs three times in the graph. GBI first finds the 1?3 pairs based on its frequency chunks them into a new node 10, then in the next iteration finds the 2?10 pairs, chunks them into a new node 11. The resulting node represents the shaded pattern It is possible to extract typical patterns of various sizes by repeating the stepwise pair expansion \(pairwise chunking Note that the search is greedy. No backtracking is made. This means that in enumerating pairs any pattern which has once been chunked into one node is never restored to the original pattern.  Because of this, all the  typical patterns  that exist in the input graph are not necessarily extracted Since the representation of decision tree is easy to understand it is often used as the representation of classifier for data which are expressed as attribute-value pairs. On the other hand graph-structure data are usually expressed as nodes and links and there is no obvious component which corresponds to attributes and their values. Thus, it is difficult to construct a decision tree for graph structured data in a straight forward manner. To cope with this issue B-GBI regards the existence of a subgraph in a graph as an attribute so that graph-structured data can be represented with attribute-value pairs according to the existence of particular subgraphs 


the existence of particular subgraphs However, it is difficult to extract subgraphs which are effective for classification task beforehand. If pairs are extended in a step-wise fashion by GBI and discriminative ones are selected and extended, discriminative patterns \(subgraphs can  be constructed simultaneously during the construction of a decision tree When constructing a decision tree, all the pairs in data are enumerated and one pair is selected. The data \(graphs divided into two groups, namely, the one with the pair and the other without the pair. The selected pair is then chunked in the former graphs. and these graphs are rewritten by replacing all the occurrence of the selected pair with a new node. This process is recursively applied at each node of a decision tree and a decision tree is constructed while attributes \(pairs classification task is created on the fly III. APRIORI DECISION TREE We present a new method that is complete and can find all the rules containing all the association rules.  The algorithm we propose has several steps 1 analysis can be performed on them 2  3  Ration [2] GINI index [4] to find outmost prominent rule for separation 4 into two sets.  Thus the procedure repeats itself by going back to stage 2, until there is no more data 5 A. Mapping the Nucleotides We concatenate type of each nucleotide with its position with respect to a specific place in DNA sequence to a new categorical value.  As an example an Adenine which is located at position -30 with respect to transcriptionalstartsite is mapped to  A-30   By this mapping regular Basket Analysis can be performed An immediate concern about usage of this method would be that in regular DNA sequences no specific ordering exist and most DNA sequences are not aligned with respect to a specific position.  Although this might be a challenge to applying the method to a DNA sequence that has not been aligned, several tools can do DNA alignment. Avid, BlastZ, Chaos, ClustalW DiAlign, Lagan, Needle, and WABA are the most famous alignment tools available Figure 1. The basic idea of GBI method Paper Identification Number: 1568965532  Thus by applying the alignment tools that are presented in the previous paragraph, the sequences are ready to apply the algorithm as stated.  Since in this paper we used aligned DNA sequences as the input, we only consider the alignment \(which is a challenge by itself method B. Review of CMAR Suppose a data object obj = \(a1  an A1  An  An attributes. Attributes here are categorical. Thus, we assume that all the possible values are mapped to a set of consecutive positive integers.  By doing so all the attributes are treated uniformly Let class labels be C = {c1  cm}. A training data set is a set of data objects such that, for each object obj, there exists a class label cobj ?  C associated with it. A classifier \( is a function from \(A1  An obj class label CMAR consists of two phases: rule generation and classification.  In the first phase, rule generation, CMAR computes the complete set of rules in the form of R : P ? c where P is a pattern in the training data set, and c  is a class label 


where P is a pattern in the training data set, and c  is a class label such that sup\(R R confidence thresholds, respectively. Furthermore, CMAR prunes some rules and only selects a subset of high quality rules for classification.  The second phase which is classification for a given data object obj, CMAR extracts a subset of rules matching the object and predicts the class label of the object by analyzing this subset of rules To make mining highly scalable and efficient, CMAR adopts a variant of FP-growth method [12] which is especially good in the situations where there exist large data sets, low support threshold, and/or long patterns.  CMAR scans the training data set T once to find the set of attribute values happening more than predefined support.  In other words, the frequent item sets are identified.  Then, it sorts attribute values in support descending order and scans the training data set again to construct an FP-tree.  The class label is attached to the last node in the path.  Based on F-list, the set of class-association rules can be divided into different groups.  CMAR finds these subsets one by one.  To find the subset of rules having a specific item CMAR traverses nodes having the attribute value and look upward to collect the projected database. We can mine the projected database recursively by constructing FP-trees and projected databases. After search for rules having the specified item, all nodes of the item are merged into their parent nodes respectively. That is, the class label information registered in the specified node is registered in its parent node.  In other words, CMAR finds the frequent items and generate rules in one step CMAR was chosen because conventional association rules must be mined in a two step fashion.  First, all the frequent patterns \(i.e., patterns passing support threshold Then, all the association rules satisfying the confidence threshold are generated based on the mined frequent patterns The difference of CMAR from other associative classification methods is that for every pattern, CMAR maintains the distribution of various class labels among data objects matching the pattern. This is done without any overhead in the procedure of counting \(conditional pattern \(i.e., pattern passing support threshold about the pattern can be generated immediately. Therefore CMAR has no separated rule generation step On the other hand, CMAR uses class label distribution to prune.  For any frequent pattern P , let c be the most dominant class in the set of data objects matching P.  If the number of objects having class label c and matching P is less than the support threshold, there is no need to search any superpattern superset  of P of , since any rule in the form of P ? c cannot satisfy the support threshold either After this stage CR-tree is built for the set of rules that were obtained.  CR-tree is a compact structure. It explores potential sharing among rules and thus can save a lot of space on storing rules. Experimental results show that, in many cases about 50-60% of space can be saved using CR-tree.  CR-tree itself is an index for rules. By this method rule retrieval becomes efficient. That facilitates the pruning of rules and using rules for classification dramatically.  For rule set shown in Fig 2 the CR-tree is presented in Fig. 3 Since the number of rules that are generated might be huge CMAR deploys a pruning method.  Basically the pruning method is base on the fact that rule provides a better accuracy more coverage and at the same time provide less attributes C. Decision Tree Construction We can use several ranking methods for the rules like information gain [1], Gain Ration [2] and GINI index [4] to find outmost prominent rule for separation.  Thus in this stage we can evaluate all the rules based on one of the above mentioned methods.  The idea behind this stage is to find the rule that has the most important contribution in dividing the data set.  In contrast to CART and C4.5 decision tree construction methods this stage of the process is not completely greedy.  In other 


this stage of the process is not completely greedy.  In other words the algorithm does go back after the decision is made However, only for combination of the rules to produce more expressible results Based on the node that has the most prominent effect on dividing the dataset, we divide the data set based on the data that matches the rule and the other set that does not match the rule Since we were using pre-pruning, if we cannot find a node that can split the data with higher values than the specified value, we should only grow the tree on the other side  Figure 2. An illustrative set of rules  Paper Identification Number: 1568965532   Figure 3:  An illustrative CR-tree of example in Fig. 2  It is a well-known fact that post-pruning will produce a more accurate result.  However since in this paper we only wanted to propose the feasibility of the method, we felt it is sufficient to use pre-pruning The modified CMAR algorithm that we used does not evaluate the strength of the rules based on statistical measures like weighted Chi-square test that is performed by CMAR.  In contrast, all the rules are generated without any pre-assumption The class labels are kept due to the fact that ranking methods require detail knowledge of class distribution that will occur due to division by that rule As an illustration, to calculate information gain, we must know exactly by using antecedents of the rule as a dividing factor each dataset \(The sequences that posses all the itemsets and the one that does not contain the whole itemset class distributions are.  In contrast confidence as defined by association rule literature does not have any significant impact on the importance or candidacy of the rule for becoming a node in the decision tree A performance adjustment was performed to increase the speed by deleting the itemsets that belonged to the antecedent of the rule in the dataset that obeyed the rule.  In other words, the set of data that has all the itemsets in the rule creates too many rules that one or more items of those rules are from the antecedents of the node prior to it.  Definitely these rules are not much interesting as far as ranking factors are concerned since division was based on all the sequences that have those items in them.  Moreover, since they exist in all the sequences, they do not posses much of ranking capabilities at later stages of the algorithm.  However if we do not delete them a lot of rules created will contain them.  Empirical results showed us that this performance adjustment reduced the size of the rules by 1/6 of the size without the performance adjustment. Thus it speeds up our calculations by six folds in the positive instances of the rules Another well known fact is that recursive partitioning of data until each subset in the partition contains data of a single class results in overfitting to the training data and thus degrades the predictive accuracy of decision trees. To improve the predictive accuracy, a pre-pruning is implemented by stop growing an overfitted tree IV. EXPERIMENTAL RESULTS We applied the proposed algorithm to splice-junction DNA sequence of UCI Machine Repository [7].  Splice junctions are points on a DNA sequence at which  superfluous  DNA is removed during the process of protein creation in higher organisms.  The problem posed in this dataset is to recognize given a sequence of DNA, the boundaries between exons \(the parts of the DNA sequence retained after splicing the parts of the DNA sequence that are spliced out problem consists of two subtasks: recognizing exon/intron boundaries \(referred to as EI sites boundaries \(IE sites In the biological community, IE borders are referred to a  acceptors  while EI borders are referred to as 


are referred to a  acceptors  while EI borders are referred to as  donors   Since the DNA sequences in addition to A, C, G and T, had other types of nucleotides.  In the first stage we omitted all the sequences that had other nucleotides.  Even though only 15 sequences had this property we omitted them so the algorithm runs smoothly Secondly, since all the nucleotides were aligned already, we considered the starting nucleotide as the first base and applied the mapping to the algorithm based on that position The modified CMAR algorithm that we used did not use statistical tests for evaluating  interestingness  of the rules.  In addition, we did not prone any rule after rule generation.  We obtained all the rules with their respective class frequencies That is, for each rule the number of sequences that would belong to each class if the rule becomes the dividing factor was found.  The calculated class distributions of the negative instances of the rule can be easily calculated by knowing the distribution of classes of the dataset.  This information was used to calculate information gain.  In other words, nonetheless we evaluate confidence, the class distribution of rule coverage was only used for entropy calculations The experimental results that are presented here are obtained based on Information Gain.  After construction of the rules Information Gain was used to evaluate the node for splitting By using the modified CMAR, we found the base rules.  As stated before, we map the nucleotides and for each class we assign a unique integer.  The constructed decision tree considering pre-pruning is illustrated in Fig. 4 of this paper The merging process of nodes only happens if the following node of positive instances has another rule that points to the class of the previous rule and the new node does not produce above a threshold value.  The only instance that rules were combined only happened at the root of the decision tree.  We noticed that rule  Base 31 = G Base 32 = T =&gt; EI  produced the highest information gain.  However after division the dataset complying with the rule had  Base 35 =&gt; EI  which did not produce a very high information gain.  Thus the algorithm combined the rules and only grow on the other side  V. SUMMARY In this paper, first we reviewed the only paper we know about constructing a decision tree for DNA sequence data.  Then based on a modified CMAR, we presented rule extractions that can be deployed.  By measuring the information gain we can rank the rules.  The rule that has most significant increase in Paper Identification Number: 1568965532  information gain is chosen as a node and the dataset is divided based on the antecedents of the rule.  The process repeats itself until the information gain by division is less than desired value of pre-pruning.  Since based on the simple mapping that we present the computational complexity of the finding sub-structures decrease very rapidly the process is feasible and accuracy, as can be seen, is very high.  Since decision tree is very expressive and at the same time very accurate for classification, we believe this method will be of high usage for bioinformatics scientists and researchers.  Moreover the method can be applied to other problems of data mining and the overall method is a general method ACKNOWLEDGMENT The authors would like to thank Professor Takashi Washio and Hiroshi Motoda for their suggestion at the early stage of this work  Base 31 = G   Base 32 = T Base 35 = G    =&gt;    EI EI 91 EI 95 Base 31 = A  Base 32 = T 


Base 31 = A  Base 32 = T Base 33 = A    =&gt;   EI Base 28 = C   Base 29 = A Base 30 = G     =&gt;    IE Base 28 = T   Base 29 = A gt;    IE Base 30 = G   =&gt;   IE Base 38 = T  Base 39 = G gt;    IE N 94 N 98 IE 92 EI 91 EI 100 Figure 4. Decision tree for spice junction DNA sequence based on DNA sequences Paper Identification Number: 1568965532   REFERENCES 1] J.R. Quinlan  Induction of decision trees  Machine Learning, vol. 1, pp. 81  106, 1986 2] J.R. Quinlan. C4.5:Programs For Machine Learning Morgan Kaufmann Publishers,1993 3] R.S. Michalski  Learning flexible concepts Fundamental ideas and a method based on two-tiered representation  Machine Learning, An Artificial Intelligence Approiach, 3:63  102, 1990 4] L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone  The cn2 induction algorithm  Machine Learning, vol 3, pp. 261  283, 1989 5] T. Matsuda, T. Horiuchi, H. Motoda, and T. Washio  Extension of graph-based induction for general graph structured data  Knowledge Discovery and Data Mining: Current Issues and New Applications, Springer Verlag, LNAI 1805, pages  420  431, 2000 6] W. Geamsakul, T. Matsuda, T. Yoshida, H. Motoda, T Washio  Classifier Construction by Graph-Based Induction for Graph-Structured Data  Advances in Knowledge Discovery and Data Mining: 7th Pacific-Asia Conference, PAKDD 2003, Seoul, Korea April 30 - May 2, 2003 7] C.L. Blake, E. Keogh, and C.J. Merz. UCI repository of machine leaning database, 1998 http://www.ics.uci.edu/mlearn/MLRepository.html 8] P.C. Nguyen, K. Ohara, H. Motoda and T. Washio  Cl-GBI: A Novel Approach for Extracting Typical Patterns from Graph-Structured Data  Advances in Knowledge Discovery and Data Mining: 9th Pacific-Asia Conference, PAKDD 2005, Hanoi Vietnam, May 18-20, 2005 9] K. Yoshida and H. Motoda  Clip: Concept learning from inference pattern  Journal of Artificial Intelligence, vol. 75\(1  92, 1995 10] T. Matsuda, T. Horiuchi, H. Motoda, and T. Washio  Extension of graph-based induction for general graph structured data  In Knowledge Discovery and Data Mining: Current Issues and New Applications, Springer Verlag, LNAI 1805, pages 420  431, 2000 11] W. Li, J. Han, and J. Pei  CMAR: Accurate and efficient classification based on multiple class-association rules  Proceedings of the 2001 IEEE International Conference on Data Mining  pp 369  376 2001  pre></body></html 





4J H. Fu and E. Mephu Nguifo. Partitioning large data to scale up lattice-based algorithm. In Proceedings ofICTAI03 pages S37-S41, Sacramento, CA, November 20 03. IEEE Press SJ H. Fu and E. Mephu Nguifo. How well go lattice algo  rithms on currently used machine learning testbeds? In 4emes journees d' Extraction et de Gestion des Connais  sances, pages 373-384, France, 20 04 61 B. Ganter and R. Wille. Formal Concept Analysis. Mathe  matical Foundations. Springer, 1999 7J J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In W. Chen, J. Naughton, and P. A Bernstein, editors, 2000 ACM SIGMOD Intl. Conference on Management of Data, pages 1-12. ACM Press, OS 2000 Data file Objects Items Min support FCI PFC \(msec msec audiology 26 110 1 30401 1302 8563 soybean-small 47 79 1 3541 516 431 lung-cancer 32 228 1 186092 21381 279689 promoters 106 228 3 298772 120426 111421 soybean-large 307 133 1 806030 357408 364524 dermatogogy 366 130 50 192203 20204 18387 breast-cancer-wis 699 110 1 9860 3529 1131 kr-vs-kp 3196 75 1100 2770846 1823092 483896 agaricus-Iepiota 8124 124 100 38347 34815 1462 connect-4bi.data 67557 126 1000 2447136 1165806 65084 Table 1. Experiments on real data.\(FCI means frequent closed itemsets. msec means milliseconds For Ref., + means PFC is faster than CLOSET Data file Min support FCI PFC \(msec msec Worst16 1 65534 571 470 271 9 Worst17 1 131070 1112 1002 541 9 Worst18 1 262142 2243 2174 1091 9 Worst19 1 524286 4576 4466 2213 10 Worst20 1 1048574 9243 9484 4606 10 Worst25 20 68405 2103 66916 451 11 Worst25 19 245505 6099 1095065 1552 11 Worst25 18 726205 15452 10235287 4486 11 Worst25 17 1807780 33348 / 10755 11 Worst25 15 7119515 102237 / 39296 11 Worst30 25 174436 6980 426964 1302 12 Worst30 20 53009101 1029771 / 344035 12 Worst50 47 20875 1132 1042 422 14 Worst50 45 2369935 227207 / 29102 14 Worst60 57 36050 7320 3205 821 15 Worst60 56 523685 82938 1665715 9123 15 Worst60 55 5985197 772210 / 92102 15 Worst70 68 2485 1102 190 121 15 Worst70 67 57225 18096 9483 1933 15 Worst70 66 974120 242138 / 26398 15 Table 2. Experiments on the worst case data 8] S. Kuznetsov and S. Obiedkov. Comparing performance of algorithms for generating concept lattices. lETAI Special Issue on Concept Lattice for KDD, 14\(2/3 9j E. Mephu Nguifo, M. Liquiere, and V. Duquenne. lETA Special Issue on Concept Lattice for KDD. Taylor and Fran  cis, 2002 IOj N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Efficient mining of association rules using closed itemsets lattices lournal of Information Systems, 24\(1 II j 1. Pei, 1. Han, and R. Mao. CLOSET: An efficient algo  rithm for mining frequent closed itemsets. InACM SIGMOD Workshop on Research Issues in Data Mining and Knowl  edge Discovery, pages 21-3 0, 200 0 12] 1. Wang, 1. Han, and 1. Pei. Closet+: Searching for the best strategies for mining frequent closed itelnsets. In In Pro  ceedings of the Ninth ACM SIGKDD International Confer  ence on Knowledge Discovery and Data Mining \(KDD'03 Washington, DC, USA, 2003 13] M. I. Zaki and C.-I. Hsiao. CHARM: An efficient algorithm for closed item set mining. Technical Report 99-10, Rensse  laer Polytechnic Institute, 1999 


laer Polytechnic Institute, 1999 pre></body></html 


efficiency then AOFI. However utilization of fuzzy concept hierarchies provides more flexibility in reflecting expert knowledge and so allows better modeling of real-life dependencies among attribute values, which will lead to more satisfactory overall results for the induction process. The drawback of the computational cost may additionally decline when we notice that, in contrast to many other data mining algorithms, hierarchical induction algorithms need to run only once through the original \(i.e. massive dataset. We are continuing an investigation of computational costs of our approach for large datasets ACKNOWLEDGMENT Rafal Angryk would like to thank the Montana NASA EPSCoR Grant Consortium for sponsoring this research REFERENCES 1] J. Han , M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann, New York, NY 2000 2] J. Han, Y. Cai, and N. Cercone  Knowledge discovery in databases: An attribute-oriented approach  Proc. 18th Int. Conf. Ver y Large Data Bases, Vancouver, Canada, 1992, pp. 547-559 3] J. Han  Towards Efficient Induction Mechanisms in Database Systems  Theoretical Computing Science, 133, 1994, pp. 361-385 4] J. Han, Y. Fu  Discovery of Multiple-Level Association Rules from Large Databases  IEEE Trans.  on KD E, 11\(5 5] C.L. Carter, H.J. Hamilton  Efficient AttributeOriented Generalization for Knowledge Discovery from Large Databases  IEEE Trans. on KDE 10\(2 6] R.J. Hilderman, H.J. Hamilton, and N. Cercone  Data mining in large databases using domain generalization graphs  Journal of Intelligent Information Systems, 13\(3 7] C.-C. Hsu  Extending attribute-oriented induction algorithm for major values and numeric values   Expert Systems with Applications , 27, 2004, pp 187-202 8] D.H. Lee, M.H. Kim  Database summarization using fuzzy ISA hierarchies  IEEE Trans . on SMC - part B, 27\(1 9] K.-M. Lee  Mining generalized fuzzy quantitative association rules with fuzzy generalization hierarchies  20th NAFIPS Int'l Conf., Vancouver Canada, 2001, pp. 2977-2982 10] J. C. Cubero, J.M. Medina, O. Pons &amp; M.A. Vila  Data Summarization in Relational Databases through  Fuzzy Dependencies  Information Sciences, 121\(3-4 11] G. Raschia, N. Mouaddib  SAINTETIQ:a fuzzy set-based approach to database summarization   Fuzzy Sets and Systems, 129\(2 162 12] R. Angryk, F. Petry  Consistent fuzzy concept hierarchies for attribute generalization  Proceeding of the IASTED Int. Conf. on Information and Knowledge Sharing, Scottsdale AZ, USA, November 2003, pp. 158-163 13] Toxics Release Inventory \(TRI available EPA database hosted at http://www.epa.gov/tri/tridata/tri01/index.htm The 2005 IEEE International Conference on Fuzzy Systems790 pre></body></html 


the initial global candidate set would be similar to the set of global MFIs. As a result, during the global mining phase the communication and synchronization overhead is low  0 2 4 6 8 1 0 Number of Nodes Figure 5. Speedup of DMM 4.4.2 Sizeup For the sizeup test, we fixed the system to the 8-node con figuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local database sire at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remained the same as the local database size was increased. This is different from the speedup test, where the database repartitioning was per formed when the number of nodes was increased. The per formance of DMM is affected by the database repartitioning to some extent, although it is usually very small. During the sizeup test, the local mining result of DMM is not changed at all at each node The results shown in Figure 6 indicate that DMM has a very good sizeup property. Since increasing the size of local database did not affect the local mining result of DMM at each node, the total execution time increased just due to more disk U 0  and computation cost which scaled almost linearly with sizeup 5 Conclusions In this paper, we proposed a new parallel maximal fre quent itemset \(MFI Max-Miner \(DMM tems. DMM is a parallel version of Max-Miner, and it re quires low synchronization and communication overhead compared to other parallel algorithms. In DMM, Max Miner is applied on each database partition during the lo 0 45 90 135 180 225 270 Amwnt of Data per Node \(ME Figure 6. Sizeup of DMM cal mining phase. Only one synchronization is needed at thc end of this phase to construct thc initial global candi date set. In the global mining phase, a top-down search is performed on the candidate set, and a prefix tree is used to count the candidates with different length efficiently. Usu ally, just a few passes are needed to find all global maximal frequent itemsets. Thus, DMM largely reduces the number of synchronizations required between processing nodes Compared with Count Distribution, DMM shows a great improvement when some frequent itemscts are large \(i.e long patterns employed by DMM for efficient communication between nodes; and global support estimation, subset-infrequency based pruning, and superset-frequency based pruning are used to reduce the size of global candidate set. DMM has very good speedup and sizeup properties References I ]  R. Agrawal and R. Srikant  FdSt Algorithms for Mining As sociation Rules  Pmc. o f f h e  ZOrh VLDB Conf, 1994, pp 487499 2] R. Agrawal and I. C. Shafer  Parallel Mining of Association Rules  IEEE Trans. on Knowledge and Dura Engineering Vol. 8, No. 6, 1996, pp. 962-969 3] R. I. Bayardo  Efficient Mining Long Patlems from Databases  Proc. ofrhe ACM SIGMOD Inf  l Conf on Man ogemenr ofDara, 1998, pp. 85-91 4] S.  M. Chung and J. Yang  A Parallel Distributive Join Al gorithm for Cube-Connected Multiprocessors  IEEE Trans on Parallel and Disrribured Systems, Vol. 7, No. 2, 1996, pp 127-137 51 M. Snir, S. Otto. S. Huss-Lederman, D. Walker, and J. Don gana, MPI: The Complete Reference, The MIT Press, 1996 


gana, MPI: The Complete Reference, The MIT Press, 1996 6] R. Rymon  Search through Systematic Set Enumeralion   Pmc. of3rd Inr  l Con$ on Principles of Knowledge Repre sentation and Reasoning, 1992, pp. 539-550 507 pre></body></html 


sketch-index in answering aggregate queries. Then Section 5.2 studies the effect of approximating spatiotemporal data, while Section 5.3 presents preliminary results for mining association rules 5.1 Performance of sketch-indexes Due to the lack of real spatio-temporal datasets we generate synthetic data in a way similar to [SJLL00 TPS03] aiming at simulation of air traffic. We first adopt a real spatial dataset [Tiger] that contains 10k 2D points representing locations in the Long Beach county \(the data space is normalized to unit length on each dimension These points serve as the  airbases  At the initial timestamp 0, we generate 100k air planes, such that each plane \(i uniformly generated in [200,300], \(ii, iii destination that are two random different airbases, and iv  the velocity direction is determined by the orientation of the line segment connecting its source and destination airbases move continually according to their velocities. Once a plane reaches its destination, it flies towards another randomly selected also uniform in [0.02, 0.04 reports to its nearest airbase, or specifically, the database consists of tuples in the form &lt;time t, airbase b, plane p passenger # a&gt;, specifying that plane p with a passengers is closest to base b at time t A spatio-temporal count/sum query has two parameters the length qrlen of its query \(square number qtlen of timestamps covered by its interval. The actual extent of the window \(interval uniformly in the data space \(history, i.e., timestamps 0,100 air planes that report to airbases in qr during qt, while a sum query returns the sum of these planes  passengers. A workload consists of 100 queries with the same parameters qrlen and qtlen The disk page size is set to 1k in all cases \(the relatively small page size simulates situations where the database is much more voluminous specialized method for distinct spatio-temporal aggregation, we compare the sketch-index to the following relational approach that can be implemented in a DBMS. Specifically, we index the 4-tuple table lt;t,b,p,a&gt; using a B-tree on the time t column. Given a count query \(with window qr and interval qt SELECT distinct p FROM &lt;t,b,p,a&gt WHERE t?qt &amp; b contained in qr The performance of each method is measured as the average number of page accesses \(per query processing a workload. For the sketch-index, we also report the average \(relative Specifically, let acti and esti be the actual and estimated results of the i-th query in the workload; then the error equals \(1/100 set the number of bits in each sketch to 24, and vary the number of sketches The first experiment evaluates the space consumption Figure 5.1 shows the sketch index size as a function of the number of sketches used \(count- and sum-indexes have the same results more sketches are included, but is usually considerably smaller than the database size \(e.g., for 16 signatures, the size is only 40% the database size 0 20 40 60 80 


80 100 120 140 160 8 16 32 number of sketches size \(mega bytes database size Figure 5.1: Size comparison Next we demonstrate the superiority of the proposed sketch-pruning query algorithm, with respect to the na  ve one that applies only spatio-temporal predicates. Figure 5.2a illustrates the costs of both algorithms for countworkloads with qtlen=10 and various qrlen \(the index used in this case has 16 sketches also illustrate the performance of the relational method which, however, is clearly incomparable \(for qrlen?0.1, it is worse by an order of magnitude we omit this technique Sketch-pruning always outperforms na  ve \(e.g., eventually two times faster for qrlen=0.25 increases with qrlen, since queries returning larger results tend to set bits in the result sketch more quickly, thus enhancing the power of Heuristics 3.1 and 3.2. In Figure 5.2b, we compare the two methods by fixing qrlen to 0.15 and varying qtlen. Similar to the findings of [PTKZ02]4 both algorithms demonstrate  step-wise  growths in their costs, while sketch-pruning is again significantly faster The experiments with sum-workloads lead to the same observations, and therefore we evaluate sketch-indexes using sketch-pruning in the rest of the experiments 4 As explained in [PTKZ02], query processing accesses at most two paths from the root to the leaf level of each B-tree regardless the length of the query interval Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE sketch-pruning naive relational 0 100 200 300 400 500 600 700 800 900 0.05 0.1 0.15 0.2 0.25 number of disk accesses query rectangle length 300 0 100 200 400 500 600 1 5 10 15 20 number of disk accesses query interval length a qtlen=10 b qrlen=0.15 Figure 5.2: Superiority of sketch-pruning \(count As discussed in Section 2, a large number of sketches reduces the variance in the resulting estimate. To verify this, Figure 5.3a plots the count-workload error of indexes 


using 8-, 16-, and 32- sketches, as a function of qrlen qtlen=10 error \(below 10 it increases slowly with qrlen used, however, the error rate is much higher \(up to 30 and has serious fluctuation, indicating the prediction is not robust. The performance of 16-sketch is in between these two extremes, or specifically, its accuracy is reasonably high \(average error around 15 much less fluctuation than 8-sketch 32-sketch 16-sketch 8-sketch relative error 0 5 10 15 20 25 30 35 0.05 0.1 0.15 0.2 0.25 query rectangle length relative error 0 5 10 15 20 25 30 35 1 5 10 15 20 query interval length a qtlen=10, count b qrlen=0.15, count relative error query rectangle length 0 5 10 15 20 25 0.05 0.1 0.15 0.2 0.25 relative error query interval length 0 5 10 15 20 25 30 1 5 10 15 20 c qtlen=10, sum d qrlen=0.15, sum Figure 5.3: Accuracy of the approximate results The same phenomena are confirmed in Figures 5.3b where we fix qrlen to 0.15 and vary qtlen 5.3d \(results for sum-workloads number of sketches improves the estimation accuracy, it also leads to higher space requirements \(as shown in Figure 5.1 Figures 5.4a and 5.4b show the number of disk accesses for the settings of Figures 5.3a and 5.3b. All indexes have almost the same behavior, while the 32-sketch is clearly more expensive than the other two indexes. The interesting observation is that 8- and 16-sketches have 


interesting observation is that 8- and 16-sketches have almost the same overhead due to the similar heights of their B-trees. Since the diagrams for sum-workloads illustrate \(almost avoid redundancy 32-sketch 16-sketch 8-sketch number of disk accesses query rectangle length 0 50 100 150 200 250 300 350 400 0.05 0.1 0.15 0.2 0.25 number of disk accesses query interval length 0 50 100 150 200 250 300 350 1 5 10 15 20 a qtlen=10 b qrlen=0.15 Figure 5.4: Costs of indexes with various signatures Summary: The sketch index constitutes an effective method for approximate spatio-temporal \(distinct aggregate processing. Particularly, the best tradeoff between space, query time, and estimation accuracy obtained by 16 sketches, which leads to size around 40 the database, fast response time \(an order of magnitude faster than the relational method average relative error 5.2 Approximating spatio-temporal data We proceed to study the efficiency of using sketches to approximate spatio-temporal data \(proposed in Section 4.1 as in the last section, except that at each timestamp all airplanes report their locations to a central server \(instead of their respective nearest bases maintains a table in the form &lt;time t, plane p, x, y&gt;, where x,y with parameters qrlen and qtlen distinct planes satisfying the spatial and temporal conditions. For comparison, we index the table using a 3D R*-tree on the columns time, x, and y. Given a query, this tree facilitates the retrieval of all qualifying tuples, after which a post-processing step is performed to obtain the Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE number of distinct planes \(in the sequel, we refer to this method as 3DR method introduces a regular res  res grid of the data space, where the resolution res is a parameter. We adopt 16 sketches because, as mentioned earlier, this number gives the best overall performance Figure 5.5 compares the sizes of the resulting sketch indexes \(obtained with resolutions res=25, 50, 100 the database size. In all cases, we achieve high compression rate \(e.g., the rate is 25% for res=25 evaluate the query efficiency, we first set the resolution to the median value 50, and use the sketch index to answer workloads with various qrlen \(qtlen=10 


workloads with various qrlen \(qtlen=10 size \(mega bytes database size 0 20 40 60 80 100 120 140 160 25 50 100 resolution Figure 5.5: Size reduction Figure 5.6a shows the query costs \(together with the error in each case method. The sketch index is faster than 3DR by an order of magnitude \(note that the vertical axis is in logarithmic scale around 15% error observations using workloads with different qtlen Finally, we examine the effect of resolution res using a workload with qrlen=0.15 and qtlen=10. As shown in Figure 5.6c, larger res incurs higher query overhead, but improves the estimation accuracy Summary: The proposed sketch method can be used to efficiently approximate spatio-temporal data for aggregate processing. It consumes significantly smaller space, and answers a query almost in real-time with low error 3D Rsketch number of disk accesses query rectangle length 1 10 100 1k 10k 0.05 0.1 0.15 0.2 0.25 16 14% 15 15% 13 relative error number of disk accesses query interval length 1 10 100 1k 10k 1 5 10 15 20 16 15% 15% 12% 11 relative error a qtlen=10, res=25 b qrlen=0.15, res=25 0 500 1000 1500 2000 2500 25 50 100 number of disk accesses resolution 20% 15% 14 relative error c qrlen=0.15, qtlen=10 


c qrlen=0.15, qtlen=10 Figure 5.6: Query efficiency \(costs and error 5.3 Mining association rules To evaluate the proposed algorithm for mining spatiotemporal association rules, we first artificially formulate 1000 association rules in the form \(r1,T,90 with 90% confidence i randomly picked from 10k ones, \(ii in at most one rule, and \(iii Then, at each of the following 100 timestamps, we assign 100k objects to the 10k regions following these rules. We execute our algorithms \(using 16 sketches these rules, and measure \(i  correct  rules divided by the total number of discovered rules, and \(ii successfully mined Figures 5.7a and 5.7b illustrate the precision and recall as a function of T respectively. Our algorithm has good precision \(close to 90 majority of the rules discovered are correct. The recall however, is relatively low for short T, but gradually increases \(90% for T=25 evaluated in the previous sections, the estimation error decreases as the query result becomes larger \(i.e., the case for higher T 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 precision HT 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 recall HT a b Figure 5.7: Efficiency of the mining algorithm Summary: The preliminary results justify the usefulness of our mining algorithm, whose efficiency improves as T increases Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 6. Conclusions While efficient aggregation is the objective of most spatio-temporal applications in practice, the existing solutions either incur prohibitive space consumption and query time, or are not able to return useful aggregate results due to the distinct counting problem. In this paper we propose the sketch index that integrates traditional approximate counting techniques with spatio-temporal indexes. Sketch indexes use a highly optimized query algorithm resulting in both smaller database size and faster query time. Our experiments show that while a sketch index consumes only a fraction of the space required for a conventional database, it can process 


required for a conventional database, it can process queries an order of magnitude faster with average relative error less than 15 While we chose to use FM sketches, our methodology can leverage any sketches allowing union operations Comparing the efficiency of different sketches constitutes a direction for future work, as well as further investigation of more sophisticated algorithms for mining association rules. For example, heuristics similar to those used for searching sketch indexes may be applied to improve the brute-force implementation ACKNOWLEDGEMENTS Yufei Tao and Dimitris Papadias were supported by grant HKUST 6197/02E from Hong Kong RGC. George Kollios, Jeffrey Considine and were Feifei Li supported by NSF CAREER IIS-0133825 and NSF IIS-0308213 grants References BKSS90] Beckmann, N., Kriegel, H., Schneider, R Seeger, B. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD, 1990 CDD+01] Chaudhuri, S., Das, G., Datar, M., Motwani R., Narasayya, V. Overcoming Limitations of Sampling for Aggregation Queries. ICDE 2001 CLKB04] Jeffrey Considine, Feifei Li, George Kollios John Byers. Approximate aggregation techniques for sensor databases. ICDE, 2004 CR94] Chen, C., Roussopoulos, N. Adaptive Selectivity Estimation Using Query Feedback. SIGMOD, 1994 FM85] Flajolet, P., Martin, G. Probabilistic Counting Algorithms for Data Base Applications JCSS, 32\(2 G84] Guttman, A. R-Trees: A Dynamic Index Structure for Spatial Searching. SIGMOD 1984 GAA03] Govindarajan, S., Agarwal, P., Arge, L. CRBTree: An Efficient Indexing Scheme for Range Aggregate Queries. ICDT, 2003 GGR03] Ganguly, S., Garofalakis, M., Rastogi, R Processing Set Expressions Over Continuous Update Streams. SIGMOD, 2003 HHW97] Hellerstein, J., Haas, P., Wang, H. Online Aggregation. SIGMOD, 1997 JL99] Jurgens, M., Lenz, H. PISA: Performance Models for Index Structures with and without Aggregated Data. SSDBM, 1999 LM01] Lazaridis, I., Mehrotra, S. Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure. SIGMOD 2001 PGF02] Palmer, C., Gibbons, P., Faloutsos, C. ANF A Fast and Scalable Tool for Data Mining in Massive Graphs. SIGKDD, 2002 PKZT01] Papadias,  D., Kalnis, P.,  Zhang, J., Tao, Y Efficient OLAP Operations in Spatial Data Warehouses. SSTD, 2001 PTKZ02] Papadias, D., Tao, Y., Kalnis, P., Zhang, J Indexing Spatio-Temporal Data Warehouses ICDE, 2002 SJLL00] Saltenis, S., Jensen, C., Leutenegger, S Lopez, M.A. Indexing the Positions of Continuously Moving Objects. SIGMOD 2000 SRF87] Sellis, T., Roussopoulos, N., Faloutsos, C The R+-tree: A Dynamic Index for MultiDimensional Objects. VLDB, 1987 TGIK02] Thaper, N., Guha, S., Indyk, P., Koudas, N Dynamic Multidimensional Histograms 


SIGMOD, 2002 Tiger] www.census.gov/geo/www/tiger TPS03] Tao, Y., Papadias, D., Sun, J. The TPR*Tree: An Optimized Spatio-Temporal Access Method for Predictive Queries. VLDB, 2003 TPZ02] Tao, Y., Papadias, D., Zhang, J. Aggregate Processing of Planar Points. EDBT, 2002 TSP03] Tao, Y., Sun, J., Papadias, D. Analysis of Predictive Spatio-Temporal Queries. TODS 28\(4 ZMT+01] Zhang, D., Markowetz, A., Tsotras, V Gunopulos, D., Seeger, B. Efficient Computation of Temporal Aggregates with Range Predicates. PODS, 2001 ZTG02] Zhang, D., Tsotras, V., Gunopulos, D Efficient Aggregation over Objects with Extent PODS, 2002 Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE pre></body></html 


